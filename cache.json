{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-12-01T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Where did you tweet from? Inferring the origin locations of tweets based on contextual information. (arXiv:2211.16506v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16506","description":"<p>Public conversations on Twitter comprise many pertinent topics including\ndisasters, protests, politics, propaganda, sports, climate change,\nepidemics/pandemic outbreaks, etc., that can have both regional and global\naspects. Spatial discourse analysis rely on geographical data. However, today\nless than 1% of tweets are geotagged; in both cases--point location or bounding\nplace information. A major issue with tweets is that Twitter users can be at\nlocation A and exchange conversations specific to location B, which we call the\nLocation A/B problem. The problem is considered solved if location entities can\nbe classified as either origin locations (Location As) or non-origin locations\n(Location Bs). In this work, we propose a simple yet effective framework--the\nTrue Origin Model--to address the problem that uses machine-level natural\nlanguage understanding to identify tweets that conceivably contain their origin\nlocation information. The model achieves promising accuracy at country (80%),\nstate (67%), city (58%), county (56%) and district (64%) levels with support\nfrom a Location Extraction Model as basic as the CoNLL-2003-based RoBERTa. We\nemploy a tweet contexualizer (locBERT) which is one of the core components of\nthe proposed model, to investigate multiple tweets' distributions for\nunderstanding Twitter users' tweeting behavior in terms of mentioning origin\nand non-origin locations. We also highlight a major concern with the currently\nregarded gold standard test set (ground truth) methodology, introduce a new\ndata set, and identify further research avenues for advancing the area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lamsal_R/0/1/0/all/0/1\">Rabindra Lamsal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwood_A/0/1/0/all/0/1\">Aaron Harwood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Read_M/0/1/0/all/0/1\">Maria Rodriguez Read</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proactive Moderation of Online Discussions: Existing Practices and the Potential for Algorithmic Support. (arXiv:2211.16525v1 [cs.CY])","link":"http://arxiv.org/abs/2211.16525","description":"<p>To address the widespread problem of uncivil behavior, many online discussion\nplatforms employ human moderators to take action against objectionable content,\nsuch as removing it or placing sanctions on its authors. This reactive paradigm\nof taking action against already-posted antisocial content is currently the\nmost common form of moderation, and has accordingly underpinned many recent\nefforts at introducing automation into the moderation process. Comparatively\nless work has been done to understand other moderation paradigms -- such as\nproactively discouraging the emergence of antisocial behavior rather than\nreacting to it -- and the role algorithmic support can play in these paradigms.\nIn this work, we investigate such a proactive framework for moderation in a\ncase study of a collaborative setting: Wikipedia Talk Pages. We employ a mixed\nmethods approach, combining qualitative and design components for a holistic\nanalysis. Through interviews with moderators, we find that despite a lack of\ntechnical and social support, moderators already engage in a number of\nproactive moderation behaviors, such as preemptively intervening in\nconversations to keep them on track. Further, we explore how automation could\nassist with this existing proactive moderation workflow by building a prototype\ntool, presenting it to moderators, and examining how the assistance it provides\nmight fit into their workflow. The resulting feedback uncovers both strengths\nand drawbacks of the prototype tool and suggests concrete steps towards further\ndeveloping such assisting technology so it can most effectively support\nmoderators in their existing proactive moderation workflow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schluger_C/0/1/0/all/0/1\">Charlotte Schluger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jonathan P. Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danescu_Niculescu_Mizil_C/0/1/0/all/0/1\">Cristian Danescu-Niculescu-Mizil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_K/0/1/0/all/0/1\">Karen Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Soft Alignment Objectives for Robust Adaptation in Machine Translation. (arXiv:2211.16550v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16550","description":"<p>Domain adaptation allows generative language models to address specific flaws\ncaused by the domain shift of their application. However, the traditional\nadaptation by further training on in-domain data rapidly weakens the model's\nability to generalize to other domains, making the open-ended deployments of\nthe adapted models prone to errors. This work introduces novel training\nobjectives built upon a semantic similarity of the predicted tokens to the\nreference.\n</p>\n<p>Our results show that (1) avoiding the common assumption of a single correct\nprediction by constructing the training target from tokens' semantic similarity\ncan mitigate catastrophic forgetting during domain adaptation, while (2)\npreserving the quality of the adaptation, (3) with negligible additions to\ncompute costs. In the broader perspective, the objectives grounded in a soft\ntoken alignment pioneer the exploration of the middle ground between the\nefficient but naive exact-match token-level objectives and expressive but\ncomputationally- and resource-intensive sequential objectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1\">Michal &#x160;tef&#xe1;nik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadlcik_M/0/1/0/all/0/1\">Marek Kadl&#x10d;&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1\">Petr Sojka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPARTAN: Sparse Hierarchical Memory for Parameter-Efficient Transformers. (arXiv:2211.16634v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16634","description":"<p>Fine-tuning pre-trained language models (PLMs) achieves impressive\nperformance on a range of downstream tasks, and their sizes have consequently\nbeen getting bigger. Since a different copy of the model is required for each\ntask, this paradigm is infeasible for storage-constrained edge devices like\nmobile phones. In this paper, we propose SPARTAN, a parameter efficient (PE)\nand computationally fast architecture for edge devices that adds hierarchically\norganized sparse memory after each Transformer layer. SPARTAN freezes the PLM\nparameters and fine-tunes only its memory, thus significantly reducing storage\ncosts by re-using the PLM backbone for different tasks. SPARTAN contains two\nlevels of memory, with only a sparse subset of parents being chosen in the\nfirst level for each input, and children cells corresponding to those parents\nbeing used to compute an output representation. This sparsity combined with\nother architecture optimizations improves SPARTAN's throughput by over 90%\nduring inference on a Raspberry Pi 4 when compared to PE baselines (adapters)\nwhile also outperforming the latter by 0.1 points on the GLUE benchmark.\nFurther, it can be trained 34% faster in a few-shot setting, while performing\nwithin 0.9 points of adapters. Qualitative analysis shows that different parent\ncells in SPARTAN specialize in different topics, thus dividing responsibility\nefficiently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1\">Ameet Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sultan_M/0/1/0/all/0/1\">Md Arafat Sultan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferritto_A/0/1/0/all/0/1\">Anthony Ferritto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1\">Ashwin Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1\">Avirup Sil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language Navigation. (arXiv:2211.16649v1 [cs.CV])","link":"http://arxiv.org/abs/2211.16649","description":"<p>Household environments are visually diverse. Embodied agents performing\nVision-and-Language Navigation (VLN) in the wild must be able to handle this\ndiversity, while also following arbitrary language instructions. Recently,\nVision-Language models like CLIP have shown great performance on the task of\nzero-shot object recognition. In this work, we ask if these models are also\ncapable of zero-shot language grounding. In particular, we utilize CLIP to\ntackle the novel problem of zero-shot VLN using natural language referring\nexpressions that describe target objects, in contrast to past work that used\nsimple language templates describing object classes. We examine CLIP's\ncapability in making sequential navigational decisions without any\ndataset-specific finetuning, and study how it influences the path that an agent\ntakes. Our results on the coarse-grained instruction following task of REVERIE\ndemonstrate the navigational capability of CLIP, surpassing the supervised\nbaseline in terms of both success rate (SR) and success weighted by path length\n(SPL). More importantly, we quantitatively show that our CLIP-based zero-shot\napproach generalizes better to show consistent performance across environments\nwhen compared to SOTA, fully supervised learning approaches when evaluated via\nRelative Change in Success (RCS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dorbala_V/0/1/0/all/0/1\">Vishnu Sashank Dorbala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigurdsson_G/0/1/0/all/0/1\">Gunnar Sigurdsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piramuthu_R/0/1/0/all/0/1\">Robinson Piramuthu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhatme_G/0/1/0/all/0/1\">Gaurav S. Sukhatme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Mismatch Doesn't Always Prevent Cross-Lingual Transfer Learning. (arXiv:2211.16671v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16671","description":"<p>Cross-lingual transfer learning without labeled target language data or\nparallel text has been surprisingly effective in zero-shot cross-lingual\nclassification, question answering, unsupervised machine translation, etc.\nHowever, some recent publications have claimed that domain mismatch prevents\ncross-lingual transfer, and their results show that unsupervised bilingual\nlexicon induction (UBLI) and unsupervised neural machine translation (UNMT) do\nnot work well when the underlying monolingual corpora come from different\ndomains (e.g., French text from Wikipedia but English text from UN\nproceedings). In this work, we show that a simple initialization regimen can\novercome much of the effect of domain mismatch in cross-lingual transfer. We\npre-train word and contextual embeddings on the concatenated domain-mismatched\ncorpora, and use these as initializations for three tasks: MUSE UBLI, UN\nParallel UNMT, and the SemEval 2017 cross-lingual word similarity task. In all\ncases, our results challenge the conclusions of prior work by showing that\nproper initialization can recover a large portion of the losses incurred by\ndomain mismatch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Edmiston_D/0/1/0/all/0/1\">Daniel Edmiston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keung_P/0/1/0/all/0/1\">Phillip Keung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Findings of the WMT 2022 Shared Task on Translation Suggestion. (arXiv:2211.16717v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16717","description":"<p>We report the result of the first edition of the WMT shared task on\nTranslation Suggestion (TS). The task aims to provide alternatives for specific\nwords or phrases given the entire documents generated by machine translation\n(MT). It consists two sub-tasks, namely, the naive translation suggestion and\ntranslation suggestion with hints. The main difference is that some hints are\nprovided in sub-task two, therefore, it is easier for the model to generate\nmore accurate suggestions. For sub-task one, we provide the corpus for the\nlanguage pairs English-German and English-Chinese. And only English-Chinese\ncorpus is provided for the sub-task two.\n</p>\n<p>We received 92 submissions from 5 participating teams in sub-task one and 6\nsubmissions for the sub-task 2, most of them covering all of the translation\ndirections. We used the automatic metric BLEU for evaluating the performance of\neach submission.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingxue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Ernan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A minor extension of the logistic equation for growth of word counts on online media: Parametric description of diversity of growth phenomena in society. (arXiv:2211.16733v1 [physics.soc-ph])","link":"http://arxiv.org/abs/2211.16733","description":"<p>To understand the growing phenomena of new vocabulary on nationwide online\nsocial media, we analyzed monthly word count time series extracted from\napproximately 1 billion Japanese blog articles from 2007 to 2019. In\nparticular, we first introduced the extended logistic equation by adding one\nparameter to the original equation and showed that the model can consistently\nreproduce various patterns of actual growth curves, such as the logistic\nfunction, linear growth, and finite-time divergence. Second, by analyzing the\nmodel parameters, we found that the typical growth pattern is not only a\nlogistic function, which often appears in various complex systems, but also a\nnontrivial growth curve that starts with an exponential function and\nasymptotically approaches a power function without a steady state. Furthermore,\nwe observed a connection between the functional form of growth and the\npeak-out. Finally, we showed that the proposed model and statistical properties\nare also valid for Google Trends data (English, French, Spanish, and Japanese),\nwhich is a time series of the nationwide popularity of search queries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Watanabe_H/0/1/0/all/0/1\">Hayafumi Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explicit Knowledge Transfer for Weakly-Supervised Code Generation. (arXiv:2211.16740v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16740","description":"<p>Large language models (LLMs) can acquire strong code-generation capabilities\nthrough few-shot learning. In contrast, supervised fine-tuning is still needed\nfor smaller models to achieve good performance. Such fine-tuning demands a\nlarge number of task-specific NL-code pairs, which are expensive to obtain. In\nthis paper, we attempt to transfer the code generation ability of an LLM to a\nsmaller model with the aid of weakly-supervised data. More specifically, we\npropose explicit knowledge transfer (EKT), which uses the few-shot capabilities\nof a teacher LLM to create NL-code pairs that we then filter for correctness\nand fine-tune the student on. We evaluate EKT on the task of generating code\nsolutions to math word problems from the GSM8k dataset. We find that EKT not\nonly yields better performance than training with expert iteration, but also\noutperforms knowledge distillation, another form of knowledge transfer. A\nGPT-Neo 1.3B model trained using EKT with a GPT-J teacher achieves a 12.4%\npass@100 on GSM8k, while the same student and teacher trained with knowledge\ndistillation yield only a 3.7% pass@100. We also show that it is possible for a\nstudent model to outperform the teacher using EKT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azerbayev_Z/0/1/0/all/0/1\">Zhangir Azerbayev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1\">Ansong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoelkopf_H/0/1/0/all/0/1\">Hailey Schoelkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforced Language Modeling for End-to-End Task Oriented Dialog. (arXiv:2211.16773v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16773","description":"<p>In task-oriented dialogs such as MultiWoZ (Budzianowski et al., 2018), an\ninformative and/or successful system response needs to include necessary key\ninformation such as the phone number of a hotel. Therefore, we hypothesize that\nby helping the model to focus more on learning key quantities in the dialog,\nthe model can generative more informative and helpful responses. In this paper,\nwe propose a new training algorithm, Reinforced Language Modeling (RLM), that\naims to use a fine-grained reward function and reinforcement learning to help\nthe model focus more on generating key quantities correctly during test time.\nEmpirical results show our proposed RLM achieves state-of-the-art performance\non the inform rate, success rate, and combined score in MultiWoZ.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qingyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1\">Kun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalised Spherical Text Embedding. (arXiv:2211.16801v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16801","description":"<p>This paper aims to provide an unsupervised modelling approach that allows for\na more flexible representation of text embeddings. It jointly encodes the words\nand the paragraphs as individual matrices of arbitrary column dimension with\nunit Frobenius norm. The representation is also linguistically motivated with\nthe introduction of a novel similarity metric. The proposed modelling and the\nnovel similarity metric exploits the matrix structure of embeddings. We then go\non to show that the same matrices can be reshaped into vectors of unit norm and\ntransform our problem into an optimization problem over the spherical manifold.\nWe exploit manifold optimization to efficiently train the matrix embeddings. We\nalso quantitatively verify the quality of our text embeddings by showing that\nthey demonstrate improved results in document classification, document\nclustering, and semantic textual similarity benchmark tests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1\">Souvik Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1\">Bamdev Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawanpuria_P/0/1/0/all/0/1\">Pratik Jawanpuria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1\">Manish Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Camelira: An Arabic Multi-Dialect Morphological Disambiguator. (arXiv:2211.16807v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16807","description":"<p>We present Camelira, a web-based Arabic multi-dialect morphological\ndisambiguation tool that covers four major variants of Arabic: Modern Standard\nArabic, Egyptian, Gulf, and Levantine. Camelira offers a user-friendly web\ninterface that allows researchers and language learners to explore various\nlinguistic information, such as part-of-speech, morphological features, and\nlemmas. Our system also provides an option to automatically choose an\nappropriate dialect-specific disambiguator based on the prediction of a dialect\nidentification component. Camelira is publicly accessible at\n<a href=\"http://camelira.camel-lab.com.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Obeid_O/0/1/0/all/0/1\">Ossama Obeid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inoue_G/0/1/0/all/0/1\">Go Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Probabilistic-Logic based Commonsense Representation Framework for Modelling Inferences with Multiple Antecedents and Varying Likelihoods. (arXiv:2211.16822v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16822","description":"<p>Commonsense knowledge-graphs (CKGs) are important resources towards building\nmachines that can 'reason' on text or environmental inputs and make inferences\nbeyond perception. While current CKGs encode world knowledge for a large number\nof concepts and have been effectively utilized for incorporating commonsense in\nneural models, they primarily encode declarative or single-condition\ninferential knowledge and assume all conceptual beliefs to have the same\nlikelihood. Further, these CKGs utilize a limited set of relations shared\nacross concepts and lack a coherent knowledge organization structure resulting\nin redundancies as well as sparsity across the larger knowledge graph.\nConsequently, today's CKGs, while useful for a first level of reasoning, do not\nadequately capture deeper human-level commonsense inferences which can be more\nnuanced and influenced by multiple contextual or situational factors.\n</p>\n<p>Accordingly, in this work, we study how commonsense knowledge can be better\nrepresented by -- (i) utilizing a probabilistic logic representation scheme to\nmodel composite inferential knowledge and represent conceptual beliefs with\nvarying likelihoods, and (ii) incorporating a hierarchical conceptual ontology\nto identify salient concept-relevant relations and organize beliefs at\ndifferent conceptual levels. Our resulting knowledge representation framework\ncan encode a wider variety of world knowledge and represent beliefs flexibly\nusing grounded concepts as well as free-text phrases. As a result, the\nframework can be utilized as both a traditional free-text knowledge graph and a\ngrounded logic-based inference system more suitable for neuro-symbolic\napplications. We describe how we extend the PrimeNet knowledge base with our\nframework through crowd-sourcing and expert-annotation, and demonstrate its\napplication for more interpretable passage-based semantic parsing and question\nanswering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_S/0/1/0/all/0/1\">Shantanu Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Liu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dongkyu Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwok_K/0/1/0/all/0/1\">Kenneth Kwok</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting text decomposition methods for NLI-based factuality scoring of summaries. (arXiv:2211.16853v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16853","description":"<p>Scoring the factuality of a generated summary involves measuring the degree\nto which a target text contains factual information using the input document as\nsupport. Given the similarities in the problem formulation, previous work has\nshown that Natural Language Inference models can be effectively repurposed to\nperform this task. As these models are trained to score entailment at a\nsentence level, several recent studies have shown that decomposing either the\ninput document or the summary into sentences helps with factuality scoring. But\nis fine-grained decomposition always a winning strategy? In this paper we\nsystematically compare different granularities of decomposition -- from\ndocument to sub-sentence level, and we show that the answer is no. Our results\nshow that incorporating additional context can yield improvement, but that this\ndoes not necessarily apply to all datasets. We also show that small changes to\npreviously proposed entailment-based scoring methods can result in better\nperformance, highlighting the need for caution in model and methodology\nselection for downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glover_J/0/1/0/all/0/1\">John Glover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fancellu_F/0/1/0/all/0/1\">Federico Fancellu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagannathan_V/0/1/0/all/0/1\">Vasudevan Jagannathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gormley_M/0/1/0/all/0/1\">Matthew R. Gormley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaaf_T/0/1/0/all/0/1\">Thomas Schaaf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Major Obstacle for NLP Research: Let's Talk about Time Allocation!. (arXiv:2211.16858v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16858","description":"<p>The field of natural language processing (NLP) has grown over the last few\nyears: conferences have become larger, we have published an incredible amount\nof papers, and state-of-the-art research has been implemented in a large\nvariety of customer-facing products. However, this paper argues that we have\nbeen less successful than we should have been and reflects on where and how the\nfield fails to tap its full potential. Specifically, we demonstrate that, in\nrecent years, subpar time allocation has been a major obstacle for NLP\nresearch. We outline multiple concrete problems together with their negative\nconsequences and, importantly, suggest remedies to improve the status quo. We\nhope that this paper will be a starting point for discussions around which\ncommon practices are -- or are not -- beneficial for NLP research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1\">Katharina Kann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudy_S/0/1/0/all/0/1\">Shiran Dudy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCarthy_A/0/1/0/all/0/1\">Arya D. McCarthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rephrasing the Reference for Non-Autoregressive Machine Translation. (arXiv:2211.16863v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16863","description":"<p>Non-autoregressive neural machine translation (NAT) models suffer from the\nmulti-modality problem that there may exist multiple possible translations of a\nsource sentence, so the reference sentence may be inappropriate for the\ntraining when the NAT output is closer to other translations. In response to\nthis problem, we introduce a rephraser to provide a better training target for\nNAT by rephrasing the reference sentence according to the NAT output. As we\ntrain NAT based on the rephraser output rather than the reference sentence, the\nrephraser output should fit well with the NAT output and not deviate too far\nfrom the reference, which can be quantified as reward functions and optimized\nby reinforcement learning. Experiments on major WMT benchmarks and NAT\nbaselines show that our approach consistently improves the translation quality\nof NAT. Specifically, our best variant achieves comparable performance to the\nautoregressive Transformer, while being 14.7 times more efficient in inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1\">Chenze Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logic and Commonsense-Guided Temporal Knowledge Graph Completion. (arXiv:2211.16865v1 [cs.AI])","link":"http://arxiv.org/abs/2211.16865","description":"<p>A temporal knowledge graph (TKG) stores the events derived from the data\ninvolving time. Predicting events is extremely challenging due to the\ntime-sensitive property of events. Besides, the previous TKG completion (TKGC)\napproaches cannot represent both the timeliness and the causality properties of\nevents, simultaneously. To address these challenges, we propose a Logic and\nCommonsense-Guided Embedding model (LCGE) to jointly learn the time-sensitive\nrepresentation involving timeliness and causality of events, together with the\ntime-independent representation of events from the perspective of commonsense.\nSpecifically, we design a temporal rule learning algorithm to construct a\nrule-guided predicate embedding regularization strategy for learning the\ncausality among events. Furthermore, we could accurately evaluate the\nplausibility of events via auxiliary commonsense knowledge. The experimental\nresults of TKGC task illustrate the significant performance improvements of our\nmodel compared with the existing approaches. More interestingly, our model is\nable to provide the explainability of the predicted results in the view of\ncausal inference. The source code and datasets of this paper are available at\nhttps://github.com/ngl567/LCGE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Guanglin Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers are Short Text Classifiers: A Study of Inductive Short Text Classifiers on Benchmarks and Real-world Datasets. (arXiv:2211.16878v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16878","description":"<p>Short text classification is a crucial and challenging aspect of Natural\nLanguage Processing. For this reason, there are numerous highly specialized\nshort text classifiers. However, in recent short text research, State of the\nArt (SOTA) methods for traditional text classification, particularly the pure\nuse of Transformers, have been unexploited. In this work, we examine the\nperformance of a variety of short text classifiers as well as the top\nperforming traditional text classifier. We further investigate the effects on\ntwo new real-world short text datasets in an effort to address the issue of\nbecoming overly dependent on benchmark datasets with a limited number of\ncharacteristics. Our experiments unambiguously demonstrate that Transformers\nachieve SOTA accuracy on short text classification tasks, raising the question\nof whether specialized short text techniques are necessary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karl_F/0/1/0/all/0/1\">Fabian Karl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"X-PuDu at SemEval-2022 Task 6: Multilingual Learning for English and Arabic Sarcasm Detection. (arXiv:2211.16883v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16883","description":"<p>Detecting sarcasm and verbal irony from people's subjective statements is\ncrucial to understanding their intended meanings and real sentiments and\npositions in social scenarios. This paper describes the X-PuDu system that\nparticipated in SemEval-2022 Task 6, iSarcasmEval - Intended Sarcasm Detection\nin English and Arabic, which aims at detecting intended sarcasm in various\nsettings of natural language understanding. Our solution finetunes pre-trained\nlanguage models, such as ERNIE-M and DeBERTa, under the multilingual settings\nto recognize the irony from Arabic and English texts. Our system ranked second\nout of 43, and ninth out of 32 in Task A: one-sentence detection in English and\nArabic; fifth out of 22 in Task B: binary multi-label classification in\nEnglish; first out of 16, and fifth out of 13 in Task C: sentence-pair\ndetection in English and Arabic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yaqian Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1\">Yekun Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hongyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yitong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VideoDubber: Machine Translation with Speech-Aware Length Control for Video Dubbing. (arXiv:2211.16934v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16934","description":"<p>Video dubbing aims to translate the original speech in a film or television\nprogram into the speech in a target language, which can be achieved with a\ncascaded system consisting of speech recognition, machine translation and\nspeech synthesis. To ensure the translated speech to be well aligned with the\ncorresponding video, the length/duration of the translated speech should be as\nclose as possible to that of the original speech, which requires strict length\ncontrol. Previous works usually control the number of words or characters\ngenerated by the machine translation model to be similar to the source\nsentence, without considering the isochronicity of speech as the speech\nduration of words/characters in different languages varies. In this paper, we\npropose a machine translation system tailored for the task of video dubbing,\nwhich directly considers the speech duration of each token in translation, to\nmatch the length of source and target speech. Specifically, we control the\nspeech length of generated sentence by guiding the prediction of each word with\nthe duration information, including the speech duration of itself as well as\nhow much duration is left for the remaining words. We design experiments on\nfour language directions (German -&gt; English, Spanish -&gt; English, Chinese &lt;-&gt;\nEnglish), and the results show that the proposed method achieves better length\ncontrol ability on the generated speech than baseline methods. To make up the\nlack of real-world datasets, we also construct a real-world test set collected\nfrom films to provide comprehensive evaluations on the video dubbing task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yihan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Junliang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Ruihua Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menezes_A/0/1/0/all/0/1\">Arul Menezes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AIONER: All-in-one scheme-based biomedical named entity recognition using deep learning. (arXiv:2211.16944v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16944","description":"<p>Biomedical named entity recognition (BioNER) seeks to automatically recognize\nbiomedical entities in natural language text, serving as a necessary foundation\nfor downstream text mining tasks and applications such as information\nextraction and question answering. Manually labeling training data for the\nBioNER task is costly, however, due to the significant domain expertise\nrequired for accurate annotation. The resulting data scarcity causes current\nBioNER approaches to be prone to overfitting, to suffer from limited\ngeneralizability, and to address a single entity type at a time (e.g., gene or\ndisease). We therefore propose a novel all-in-one (AIO) scheme that uses\nexternal data from existing annotated resources to improve generalization. We\nfurther present AIONER, a general-purpose BioNER tool based on cutting-edge\ndeep learning and our AIO schema. We evaluate AIONER on 14 BioNER benchmark\ntasks and show that AIONER is effective, robust, and compares favorably to\nother state-of-the-art approaches such as multi-task learning. We further\ndemonstrate the practical utility of AIONER in three independent tasks to\nrecognize entity types not previously seen in training data, as well as the\nadvantages of AIONER over existing methods for processing biomedical text at a\nlarge scale (e.g., the entire PubMed data).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Ling Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chih-Hsuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_P/0/1/0/all/0/1\">Po-Ting Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leaman_R/0/1/0/all/0/1\">Robert Leaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Pipeline for Generating, Annotating and Employing Synthetic Data for Real World Question Answering. (arXiv:2211.16971v1 [cs.CL])","link":"http://arxiv.org/abs/2211.16971","description":"<p>Question Answering (QA) is a growing area of research, often used to\nfacilitate the extraction of information from within documents.\nState-of-the-art QA models are usually pre-trained on domain-general corpora\nlike Wikipedia and thus tend to struggle on out-of-domain documents without\nfine-tuning. We demonstrate that synthetic domain-specific datasets can be\ngenerated easily using domain-general models, while still providing significant\nimprovements to QA performance. We present two new tools for this task: A\nflexible pipeline for validating the synthetic QA data and training downstream\nmodels on it, and an online interface to facilitate human annotation of this\ngenerated data. Using this interface, crowdworkers labelled 1117 synthetic QA\npairs, which we then used to fine-tune downstream models and improve\ndomain-specific QA performance by 8.75 F1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maufe_M/0/1/0/all/0/1\">Matthew Maufe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravenscroft_J/0/1/0/all/0/1\">James Ravenscroft</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1\">Rob Procter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liakata_M/0/1/0/all/0/1\">Maria Liakata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAFT: Rationale adaptor for few-shot abusive language detection. (arXiv:2211.17046v1 [cs.CL])","link":"http://arxiv.org/abs/2211.17046","description":"<p>Abusive language is a concerning problem in online social media. Past\nresearch on detecting abusive language covers different platforms, languages,\ndemographies, etc. However, models trained using these datasets do not perform\nwell in cross-domain evaluation settings. To overcome this, a common strategy\nis to use a few samples from the target domain to train models to get better\nperformance in that domain (cross-domain few-shot training). However, this\nmight cause the models to overfit the artefacts of those samples. A compelling\nsolution could be to guide the models toward rationales, i.e., spans of text\nthat justify the text's label. This method has been found to improve model\nperformance in the in-domain setting across various NLP tasks. In this paper,\nwe propose RAFT (Rationale Adaptor for Few-shoT classification) for abusive\nlanguage detection. We first build a multitask learning setup to jointly learn\nrationales, targets, and labels, and find a significant improvement of 6% macro\nF1 on the rationale detection task over training solely rationale classifiers.\nWe introduce two rationale-integrated BERT-based architectures (the RAFT\nmodels) and evaluate our systems over five different abusive language datasets,\nfinding that in the few-shot classification setting, RAFT-based models\noutperform baseline models by about 7% in macro F1 scores and perform\ncompetitively to models finetuned on other source domains. Furthermore,\nRAFT-based models outperform LIME/SHAP-based approaches in terms of\nplausibility and are close in performance in terms of faithfulness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_P/0/1/0/all/0/1\">Punyajoy Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_D/0/1/0/all/0/1\">Divyanshu Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kedia_K/0/1/0/all/0/1\">Kushal Kedia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_B/0/1/0/all/0/1\">Binny Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Animesh Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Transcription of UK Supreme Court Hearings. (arXiv:2211.17094v1 [eess.AS])","link":"http://arxiv.org/abs/2211.17094","description":"<p>Transcription of legal proceedings is very important to enable access to\njustice. However, speech transcription is an expensive and slow process. In\nthis paper we describe part of a combined research and industrial project for\nbuilding an automated transcription tool designed specifically for the Justice\nsector in the UK. We explain the challenges involved in transcribing court room\nhearings and the Natural Language Processing (NLP) techniques we employ to\ntackle these challenges. We will show that fine-tuning a generic off-the-shelf\npre-trained Automatic Speech Recognition (ASR) system with an in-domain\nlanguage model as well as infusing common phrases extracted with a collocation\ndetection model can improve not only the Word Error Rate (WER) of the\ntranscribed hearings but avoid critical errors that are specific of the legal\njargon and terminology commonly used in British courts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saadany_H/0/1/0/all/0/1\">Hadeel Saadany</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Orasan_C/0/1/0/all/0/1\">Constantin Or&#x103;san</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breslin_C/0/1/0/all/0/1\">Catherine Breslin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Handling and extracting key entities from customer conversations using Speech recognition and Named Entity recognition. (arXiv:2211.17107v1 [cs.CL])","link":"http://arxiv.org/abs/2211.17107","description":"<p>In this modern era of technology with e-commerce developing at a rapid pace,\nit is very important to understand customer requirements and details from a\nbusiness conversation. It is very crucial for customer retention and\nsatisfaction. Extracting key insights from these conversations is very\nimportant when it comes to developing their product or solving their issue.\nUnderstanding customer feedback, responses, and important details of the\nproduct are essential and it would be done using Named entity recognition\n(NER). For extracting the entities we would be converting the conversations to\ntext using the optimal speech-to-text model. The model would be a two-stage\nnetwork in which the conversation is converted to text. Then, suitable entities\nare extracted using robust techniques using a NER BERT transformer model. This\nwill aid in the enrichment of customer experience when there is an issue which\nis faced by them. If a customer faces a problem he will call and register his\ncomplaint. The model will then extract the key features from this conversation\nwhich will be necessary to look into the problem. These features would include\ndetails like the order number, and the exact problem. All these would be\nextracted directly from the conversation and this would reduce the effort of\ngoing through the conversation again.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Endait_S/0/1/0/all/0/1\">Sharvi Endait</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghatage_R/0/1/0/all/0/1\">Ruturaj Ghatage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadam_P/0/1/0/all/0/1\">Prof. DD Kadam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Emotion-guided Approach to Domain Adaptive Fake News Detection using Adversarial Learning. (arXiv:2211.17108v1 [cs.CL])","link":"http://arxiv.org/abs/2211.17108","description":"<p>Recent works on fake news detection have shown the efficacy of using emotions\nas a feature for improved performance. However, the cross-domain impact of\nemotion-guided features for fake news detection still remains an open problem.\nIn this work, we propose an emotion-guided, domain-adaptive, multi-task\napproach for cross-domain fake news detection, proving the efficacy of\nemotion-guided models in cross-domain settings for various datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Arkajyoti Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khatri_I/0/1/0/all/0/1\">Inder Khatri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhry_A/0/1/0/all/0/1\">Arjun Choudhry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Pankaj Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vishwakarma_D/0/1/0/all/0/1\">Dinesh Kumar Vishwakarma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_M/0/1/0/all/0/1\">Mukesh Prasad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"sEHR-CE: Language modelling of structured EHR data for efficient and generalizable patient cohort expansion. (arXiv:2211.17121v1 [cs.CL])","link":"http://arxiv.org/abs/2211.17121","description":"<p>Electronic health records (EHR) offer unprecedented opportunities for\nin-depth clinical phenotyping and prediction of clinical outcomes. Combining\nmultiple data sources is crucial to generate a complete picture of disease\nprevalence, incidence and trajectories. The standard approach to combining\nclinical data involves collating clinical terms across different terminology\nsystems using curated maps, which are often inaccurate and/or incomplete. Here,\nwe propose sEHR-CE, a novel framework based on transformers to enable\nintegrated phenotyping and analyses of heterogeneous clinical datasets without\nrelying on these mappings. We unify clinical terminologies using textual\ndescriptors of concepts, and represent individuals' EHR as sections of text. We\nthen fine-tune pre-trained language models to predict disease phenotypes more\naccurately than non-text and single terminology approaches. We validate our\napproach using primary and secondary care data from the UK Biobank, a\nlarge-scale research study. Finally, we illustrate in a type 2 diabetes use\ncase how sEHR-CE identifies individuals without diagnosis that share clinical\ncharacteristics with patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Munoz_Farre_A/0/1/0/all/0/1\">Anna Munoz-Farre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rose_H/0/1/0/all/0/1\">Harry Rose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cakiroglu_S/0/1/0/all/0/1\">Sera Aylin Cakiroglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BudgetLongformer: Can we Cheaply Pretrain a SotA Legal Language Model From Scratch?. (arXiv:2211.17135v1 [cs.CL])","link":"http://arxiv.org/abs/2211.17135","description":"<p>Pretrained transformer models have achieved state-of-the-art results in many\ntasks and benchmarks recently. Many state-of-the-art Language Models (LMs),\nhowever, do not scale well above the threshold of 512 input tokens. In\nspecialized domains though (such as legal, scientific or biomedical), models\noften need to process very long text (sometimes well above 10000 tokens). Even\nthough many efficient transformers have been proposed (such as Longformer,\nBigBird or FNet), so far, only very few such efficient models are available for\nspecialized domains. Additionally, since the pretraining process is extremely\ncostly in general - but even more so as the sequence length increases - it is\noften only in reach of large research labs. One way of making pretraining\ncheaper is the Replaced Token Detection (RTD) task, by providing more signal\nduring training, since the loss can be computed over all tokens. In this work,\nwe train Longformer models with the efficient RTD task on legal data to\nshowcase that pretraining efficient LMs is possible using much less compute. We\nevaluate the trained models on challenging summarization tasks requiring the\nmodel to summarize long texts to show to what extent the models can achieve\ngood performance on downstream tasks. We find that both the small and base\nmodels outperform their baselines on the in-domain BillSum and out-of-domain\nPubMed tasks in their respective parameter range. We publish our code and\nmodels for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niklaus_J/0/1/0/all/0/1\">Joel Niklaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giofre_D/0/1/0/all/0/1\">Daniele Giofr&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConvLab-3: A Flexible Dialogue System Toolkit Based on a Unified Data Format. (arXiv:2211.17148v1 [cs.CL])","link":"http://arxiv.org/abs/2211.17148","description":"<p>Diverse data formats and ontologies of task-oriented dialogue (TOD) datasets\nhinder us from developing general dialogue models that perform well on many\ndatasets and studying knowledge transfer between datasets. To address this\nissue, we present ConvLab-3, a flexible dialogue system toolkit based on a\nunified TOD data format. In ConvLab-3, different datasets are transformed into\none unified format and loaded by models in the same way. As a result, the cost\nof adapting a new model or dataset is significantly reduced. Compared to the\nprevious releases of ConvLab (Lee et al., 2019b; Zhu et al., 2020b), ConvLab-3\nallows developing dialogue systems with much more datasets and enhances the\nutility of the reinforcement learning (RL) toolkit for dialogue policies. To\nshowcase the use of ConvLab-3 and inspire future work, we present a\ncomprehensive study with various settings. We show the benefit of pre-training\non other datasets for few-shot fine-tuning and RL, and encourage evaluating\npolicy with diverse user simulators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geishauser_C/0/1/0/all/0/1\">Christian Geishauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hsien-chin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekerk_C/0/1/0/all/0/1\">Carel van Niekerk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heck_M/0/1/0/all/0/1\">Michael Heck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lubis_N/0/1/0/all/0/1\">Nurul Lubis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_D/0/1/0/all/0/1\">Dazhen Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaochen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasic_M/0/1/0/all/0/1\">Milica Ga&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Misogyny classification of German newspaper forum comments. (arXiv:2211.17163v1 [cs.CL])","link":"http://arxiv.org/abs/2211.17163","description":"<p>This paper presents work on detecting misogyny in the comments of a large\nAustrian German language newspaper forum. We describe the creation of a corpus\nof 6600 comments which were annotated with 5 levels of misogyny. The forum\nmoderators were involved as experts in the creation of the annotation\nguidelines and the annotation of the comments. We also describe the results of\ntraining transformer-based classification models for both binarized and\noriginal label classification of that corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petrak_J/0/1/0/all/0/1\">Johann Petrak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krenn_B/0/1/0/all/0/1\">Brigitte Krenn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Inference from Transformers via Speculative Decoding. (arXiv:2211.17192v1 [cs.LG])","link":"http://arxiv.org/abs/2211.17192","description":"<p>Inference from large autoregressive models like Transformers is slow -\ndecoding K tokens takes K serial runs of the model. In this work we introduce\nspeculative decoding - an algorithm to sample from autoregressive models faster\nwithout any changes to the outputs, by computing several tokens in parallel. At\nthe heart of our approach lie the observations that (1) hard language-modeling\ntasks often include easier subtasks that can be approximated well by more\nefficient models, and (2) using speculative execution and a novel sampling\nmethod, we can make exact decoding from the large models faster, by running\nthem in parallel on the outputs of the approximation models, potentially\ngenerating several tokens concurrently, and without changing the distribution.\nOur method supports existing off-the-shelf models without retraining or\narchitecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration\ncompared to the standard T5X implementation, with identical outputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leviathan_Y/0/1/0/all/0/1\">Yaniv Leviathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalman_M/0/1/0/all/0/1\">Matan Kalman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matias_Y/0/1/0/all/0/1\">Yossi Matias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EURO: ESPnet Unsupervised ASR Open-source Toolkit. (arXiv:2211.17196v1 [cs.CL])","link":"http://arxiv.org/abs/2211.17196","description":"<p>This paper describes the ESPnet Unsupervised ASR Open-source Toolkit (EURO),\nan end-to-end open-source toolkit for unsupervised automatic speech recognition\n(UASR). EURO adopts the state-of-the-art UASR learning method introduced by the\nWav2vec-U, originally implemented at FAIRSEQ, which leverages self-supervised\nspeech representations and adversarial training. In addition to wav2vec2, EURO\nextends the functionality and promotes reproducibility for UASR tasks by\nintegrating S3PRL and k2, resulting in flexible frontends from 27\nself-supervised models and various graph-based decoding strategies. EURO is\nimplemented in ESPnet and follows its unified pipeline to provide UASR recipes\nwith a complete setup. This improves the pipeline's efficiency and allows EURO\nto be easily applied to existing datasets in ESPnet. Extensive experiments on\nthree mainstream self-supervised models demonstrate the toolkit's effectiveness\nand achieve state-of-the-art UASR performance on TIMIT and LibriSpeech\ndatasets. EURO will be publicly available at https://github.com/espnet/espnet,\naiming to promote this exciting and emerging research area based on UASR\nthrough open-source activity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Dongji Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_S/0/1/0/all/0/1\">Shun-Po Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_L/0/1/0/all/0/1\">Leibny Paola Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khudanpur_S/0/1/0/all/0/1\">Sanjeev Khudanpur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ExtremeBERT: A Toolkit for Accelerating Pretraining of Customized BERT. (arXiv:2211.17201v1 [cs.CL])","link":"http://arxiv.org/abs/2211.17201","description":"<p>In this paper, we present ExtremeBERT, a toolkit for accelerating and\ncustomizing BERT pretraining. Our goal is to provide an easy-to-use BERT\npretraining toolkit for the research community and industry. Thus, the\npretraining of popular language models on customized datasets is affordable\nwith limited resources. Experiments show that, to achieve the same or better\nGLUE scores, the time cost of our toolkit is over $6\\times$ times less for BERT\nBase and $9\\times$ times less for BERT Large when compared with the original\nBERT paper. The documentation and code are released at\nhttps://github.com/extreme-bert/extreme-bert under the Apache-2.0 license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_R/0/1/0/all/0/1\">Rui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_S/0/1/0/all/0/1\">Shizhe Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianlin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topological Data Analysis for Speech Processing. (arXiv:2211.17223v1 [cs.SD])","link":"http://arxiv.org/abs/2211.17223","description":"<p>We apply topological data analysis (TDA) to speech classification problems\nand to the introspection of a pretrained speech model, HuBERT. To this end, we\nintroduce a number of topological and algebraic features derived from\nTransformer attention maps and embeddings. We show that a simple linear\nclassifier built on top of such features outperforms a fine-tuned\nclassification head. In particular, we achieve an improvement of about $9\\%$\naccuracy and $5\\%$ ERR on four common datasets; on CREMA-D, the proposed\nfeature set reaches a new state of the art performance with accuracy $80.155$.\nWe also show that topological features are able to reveal functional roles of\nspeech Transformer heads; e.g., we find the heads capable to distinguish\nbetween pairs of sample sources (natural/synthetic) or voices without any\ndownstream fine-tuning. Our results demonstrate that TDA is a promising new\napproach for speech analysis, especially for tasks that require structural\nprediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tulchinskii_E/0/1/0/all/0/1\">Eduard Tulchinskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_K/0/1/0/all/0/1\">Kristian Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kushnareva_L/0/1/0/all/0/1\">Laida Kushnareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherniavskii_D/0/1/0/all/0/1\">Daniil Cherniavskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1\">Serguei Barannikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1\">Irina Piontkovskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolenko_S/0/1/0/all/0/1\">Sergey Nikolenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CREPE: Open-Domain Question Answering with False Presuppositions. (arXiv:2211.17257v1 [cs.CL])","link":"http://arxiv.org/abs/2211.17257","description":"<p>Information seeking users often pose questions with false presuppositions,\nespecially when asking about unfamiliar topics. Most existing question\nanswering (QA) datasets, in contrast, assume all questions have well defined\nanswers. We introduce CREPE, a QA dataset containing a natural distribution of\npresupposition failures from online information-seeking forums. We find that\n25% of questions contain false presuppositions, and provide annotations for\nthese presuppositions and their corrections. Through extensive baseline\nexperiments, we show that adaptations of existing open-domain QA models can\nfind presuppositions moderately well, but struggle when predicting whether a\npresupposition is factually correct. This is in large part due to difficulty in\nretrieving relevant evidence passages from a large text corpus. CREPE provides\na benchmark to study question answering in the wild, and our analyses provide\navenues for future work in better modeling and further studying the task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinyan Velocity Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Context-Free Languages with Nondeterministic Stack RNNs. (arXiv:2010.04674v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.04674","description":"<p>We present a differentiable stack data structure that simultaneously and\ntractably encodes an exponential number of stack configurations, based on\nLang's algorithm for simulating nondeterministic pushdown automata. We call the\ncombination of this data structure with a recurrent neural network (RNN)\ncontroller a Nondeterministic Stack RNN. We compare our model against existing\nstack RNNs on various formal languages, demonstrating that our model converges\nmore reliably to algorithmic behavior on deterministic tasks, and achieves\nlower cross-entropy on inherently nondeterministic tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DuSell_B/0/1/0/all/0/1\">Brian DuSell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Hierarchical Structures with Differentiable Nondeterministic Stacks. (arXiv:2109.01982v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01982","description":"<p>Learning hierarchical structures in sequential data -- from simple\nalgorithmic patterns to natural language -- in a reliable, generalizable way\nremains a challenging problem for neural language models. Past work has shown\nthat recurrent neural networks (RNNs) struggle to generalize on held-out\nalgorithmic or syntactic patterns without supervision or some inductive bias.\nTo remedy this, many papers have explored augmenting RNNs with various\ndifferentiable stacks, by analogy with finite automata and pushdown automata\n(PDAs). In this paper, we improve the performance of our recently proposed\nNondeterministic Stack RNN (NS-RNN), which uses a differentiable data structure\nthat simulates a nondeterministic PDA, with two important changes. First, the\nmodel now assigns unnormalized positive weights instead of probabilities to\nstack actions, and we provide an analysis of why this improves training.\nSecond, the model can directly observe the state of the underlying PDA. Our\nmodel achieves lower cross-entropy than all previous stack RNNs on five\ncontext-free language modeling tasks (within 0.05 nats of the\ninformation-theoretic lower bound), including a task on which the NS-RNN\npreviously failed to outperform a deterministic stack RNN baseline. Finally, we\npropose a restricted version of the NS-RNN that incrementally processes\ninfinitely long sequences, and we present language modeling results on the Penn\nTreebank.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DuSell_B/0/1/0/all/0/1\">Brian DuSell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Uncertainty in Machine Translation Evaluation. (arXiv:2204.06546v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06546","description":"<p>Trainable evaluation metrics for machine translation (MT) exhibit strong\ncorrelation with human judgements, but they are often hard to interpret and\nmight produce unreliable scores under noisy or out-of-domain data. Recent work\nhas attempted to mitigate this with simple uncertainty quantification\ntechniques (Monte Carlo dropout and deep ensembles), however these techniques\n(as we show) are limited in several ways -- for example, they are unable to\ndistinguish between different kinds of uncertainty, and they are time and\nmemory consuming. In this paper, we propose more powerful and efficient\nuncertainty predictors for MT evaluation, and we assess their ability to target\ndifferent sources of aleatoric and epistemic uncertainty. To this end, we\ndevelop and compare training objectives for the COMET metric to enhance it with\nan uncertainty prediction output, including heteroscedastic regression,\ndivergence minimization, and direct uncertainty prediction. Our experiments\nshow improved results on uncertainty prediction for the WMT metrics task\ndatasets, with a substantial reduction in computational costs. Moreover, they\ndemonstrate the ability of these predictors to address specific uncertainty\ncauses in MT evaluation, such as low quality references and out-of-domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zerva_C/0/1/0/all/0/1\">Chrysoula Zerva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glushkova_T/0/1/0/all/0/1\">Taisiya Glushkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_R/0/1/0/all/0/1\">Ricardo Rei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Scaffold: Optimizing Model Explanations for Teaching. (arXiv:2204.10810v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.10810","description":"<p>Modern machine learning models are opaque, and as a result there is a\nburgeoning academic subfield on methods that explain these models' behavior.\nHowever, what is the precise goal of providing such explanations, and how can\nwe demonstrate that explanations achieve this goal? Some research argues that\nexplanations should help teach a student (either human or machine) to simulate\nthe model being explained, and that the quality of explanations can be measured\nby the simulation accuracy of students on unexplained examples. In this work,\nleveraging meta-learning techniques, we extend this idea to improve the quality\nof the explanations themselves, specifically by optimizing explanations such\nthat student models more effectively learn to simulate the original model. We\ntrain models on three natural language processing and computer vision tasks,\nand find that students trained with explanations extracted with our framework\nare able to simulate the teacher significantly more effectively than ones\nproduced with previous methods. Through human annotations and a user study, we\nfurther find that these learned explanations more closely align with how humans\nwould explain the required decisions in these tasks. Our code is available at\nhttps://github.com/coderpat/learning-scaffold\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1\">Patrick Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treviso_M/0/1/0/all/0/1\">Marcos Treviso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1\">Danish Pruthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are Few-Shot Clinical Information Extractors. (arXiv:2205.12689v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12689","description":"<p>A long-running goal of the clinical NLP community is the extraction of\nimportant variables trapped in clinical notes. However, roadblocks have\nincluded dataset shift from the general domain and a lack of public clinical\ncorpora and annotations. In this work, we show that large language models, such\nas InstructGPT, perform well at zero- and few-shot information extraction from\nclinical text despite not being trained specifically for the clinical domain.\nWhereas text classification and generation performance have already been\nstudied extensively in such models, here we additionally demonstrate how to\nleverage them to tackle a diverse set of NLP tasks which require more\nstructured outputs, including span identification, token-level sequence\nclassification, and relation extraction. Further, due to the dearth of\navailable data to evaluate these systems, we introduce new datasets for\nbenchmarking few-shot clinical information extraction based on a manual\nre-annotation of the CASI dataset for new tasks. On the clinical extraction\ntasks we studied, the GPT-3 systems significantly outperform existing zero- and\nfew-shot baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_M/0/1/0/all/0/1\">Monica Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegselmann_S/0/1/0/all/0/1\">Stefan Hegselmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_H/0/1/0/all/0/1\">Hunter Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1\">David Sontag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Component Contrastive Learning for Concept Relatedness Estimation. (arXiv:2206.12556v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.12556","description":"<p>Concept relatedness estimation (CRE) aims to determine whether two given\nconcepts are related. Existing methods only consider the pairwise relationship\nbetween concepts, while overlooking the higher-order relationship that could be\nencoded in a concept-level graph structure. We discover that this underlying\ngraph satisfies a set of intrinsic properties of CRE, including reflexivity,\ncommutativity, and transitivity. In this paper, we formalize the CRE properties\nand introduce a graph structure named ConcreteGraph. To address the data\nscarcity issue in CRE, we introduce a novel data augmentation approach to\nsample new concept pairs from the graph. As it is intractable for data\naugmentation to fully capture the structural information of the ConcreteGraph\ndue to a large amount of potential concept pairs, we further introduce a novel\nGraph Component Contrastive Learning framework to implicitly learn the complete\nstructure of the ConcreteGraph. Empirical results on three datasets show\nsignificant improvement over the state-of-the-art model. Detailed ablation\nstudies demonstrate that our proposed approach can effectively capture the\nhigh-order relationship among concepts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yueen Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zixing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Mandarin Speech Recogntion with Block-augmented Transformer. (arXiv:2207.11697v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.11697","description":"<p>Recently Convolution-augmented Transformer (Conformer) has shown promising\nresults in Automatic Speech Recognition (ASR), outperforming the previous best\npublished Transformer Transducer. In this work, we believe that the output\ninformation of each block in the encoder and decoder is not completely\ninclusive, in other words, their output information may be complementary. We\nstudy how to take advantage of the complementary information of each block in a\nparameter-efficient way, and it is expected that this may lead to more robust\nperformance. Therefore we propose the Block-augmented Transformer for speech\nrecognition, named Blockformer. We have implemented two block ensemble methods:\nthe base Weighted Sum of the Blocks Output (Base-WSBO), and the\nSqueeze-and-Excitation module to Weighted Sum of the Blocks Output (SE-WSBO).\nExperiments have proved that the Blockformer significantly outperforms the\nstate-of-the-art Conformer-based models on AISHELL-1, our model achieves a CER\nof 4.29\\% without using a language model and 4.05\\% with an external language\nmodel on the testset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaoming Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Huifeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Liuwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minghui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1\">Jie Hao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Cross-Lingual Generalisation in Visual Question Answering. (arXiv:2209.02982v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.02982","description":"<p>While several benefits were realized for multilingual vision-language\npretrained models, recent benchmarks across various tasks and languages showed\npoor cross-lingual generalisation when multilingually pre-trained\nvision-language models are applied to non-English data, with a large gap\nbetween (supervised) English performance and (zero-shot) cross-lingual\ntransfer. In this work, we explore the poor performance of these models on a\nzero-shot cross-lingual visual question answering (VQA) task, where models are\nfine-tuned on English visual-question data and evaluated on 7 typologically\ndiverse languages. We improve cross-lingual transfer with three strategies: (1)\nwe introduce a linguistic prior objective to augment the cross-entropy loss\nwith a similarity-based loss to guide the model during training, (2) we learn a\ntask-specific subnetwork that improves cross-lingual generalisation and reduces\nvariance without model modification, (3) we augment training examples using\nsynthetic code-mixing to promote alignment of embeddings between source and\ntarget languages. Our experiments on xGQA using the pretrained multilingual\nmultimodal transformers UC2 and M3P demonstrate the consistent effectiveness of\nthe proposed fine-tuning strategy for 7 languages, outperforming existing\ntransfer methods with sparse models. Code and data to reproduce our findings\nare publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nooralahzadeh_F/0/1/0/all/0/1\">Farhad Nooralahzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeking Diverse Reasoning Logic: Controlled Equation Expression Generation for Solving Math Word Problems. (arXiv:2209.10310v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.10310","description":"<p>To solve Math Word Problems, human students leverage diverse reasoning logic\nthat reaches different possible equation solutions. However, the mainstream\nsequence-to-sequence approach of automatic solvers aims to decode a fixed\nsolution equation supervised by human annotation. In this paper, we propose a\ncontrolled equation generation solver by leveraging a set of control codes to\nguide the model to consider certain reasoning logic and decode the\ncorresponding equations expressions transformed from the human reference. The\nempirical results suggest that our method universally improves the performance\non single-unknown (Math23K) and multiple-unknown (DRAW1K, HMWP) benchmarks,\nwith substantial improvements up to 13.2% accuracy on the challenging\nmultiple-unknown datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yibin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qianying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhuoyuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zhen Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1\">Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1\">Sadao Kurohashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WikiWhy: Answering and Explaining Cause-and-Effect Questions. (arXiv:2210.12152v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12152","description":"<p>As large language models (LLMs) grow larger and more sophisticated, assessing\ntheir \"reasoning\" capabilities in natural language grows more challenging.\nRecent question answering (QA) benchmarks that attempt to assess reasoning are\noften limited by a narrow scope of covered situations and subject matters. We\nintroduce WikiWhy, a QA dataset built around a novel auxiliary task: explaining\nwhy an answer is true in natural language. WikiWhy contains over 9,000 \"why\"\nquestion-answer-rationale triples, grounded on Wikipedia facts across a diverse\nset of topics. Each rationale is a set of supporting statements connecting the\nquestion to the answer. WikiWhy serves as a benchmark for the reasoning\ncapabilities of LLMs because it demands rigorous explicit rationales for each\nanswer to demonstrate the acquisition of implicit commonsense knowledge, which\nis unlikely to be easily memorized. GPT-3 baselines achieve only 38.7%\nhuman-evaluated correctness in the end-to-end answer &amp; explain condition,\nleaving significant room for future improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ho_M/0/1/0/all/0/1\">Matthew Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Aditya Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Justin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Sharon Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalized Dialogue Generation with Persona-Adaptive Attention. (arXiv:2210.15088v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15088","description":"<p>Persona-based dialogue systems aim to generate consistent responses based on\nhistorical context and predefined persona. Unlike conventional dialogue\ngeneration, the persona-based dialogue needs to consider both dialogue context\nand persona, posing a challenge for coherent training. Specifically, this\nrequires a delicate weight balance between context and persona. To achieve\nthat, in this paper, we propose an effective framework with Persona-Adaptive\nAttention (PAA), which adaptively integrates the weights from the persona and\ncontext information via our designed attention. In addition, a dynamic masking\nmechanism is applied to the PAA to not only drop redundant information in\ncontext and persona but also serve as a regularization mechanism to avoid\noverfitting. Experimental results demonstrate the superiority of the proposed\nPAA framework compared to the strong baselines in both automatic and human\nevaluation. Moreover, the proposed PAA approach can perform equivalently well\nin a low-resource regime compared to models trained in a full-data setting,\nwhich achieve a similar result with only 20% to 30% of data compared to the\nlarger models trained in the full-data setting. To fully exploit the\neffectiveness of our design, we designed several variants for handling the\nweighted information in different ways, showing the necessity and sufficiency\nof our weighting and masking designs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qiushi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_T/0/1/0/all/0/1\">Tom Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xubo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenwu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Lilian Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Use of Large Pre-Trained Models for Low Resource ASR. (arXiv:2210.15445v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2210.15445","description":"<p>Automatic speech recognition (ASR) has been established as a well-performing\ntechnique for many scenarios where lots of labeled data is available.\nAdditionally, unsupervised representation learning recently helped to tackle\ntasks with limited data. Following this, hardware limitations and applications\ngive rise to the question how to efficiently take advantage of large pretrained\nmodels and reduce their complexity for downstream tasks. In this work, we study\na challenging low resource conversational telephony speech corpus from the\nmedical domain in Vietnamese and German. We show the benefits of using\nunsupervised techniques beyond simple fine-tuning of large pre-trained models,\ndiscuss how to adapt them to a practical telephony task including bandwidth\ntransfer and investigate different data conditions for pre-training and\nfine-tuning. We outperform the project baselines by 22% relative using\npretraining techniques. Further gains of 29% can be achieved by refinements of\narchitecture and training and 6% by adding 0.8 h of in-domain adaptation data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vieting_P/0/1/0/all/0/1\">Peter Vieting</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luscher_C/0/1/0/all/0/1\">Christoph L&#xfc;scher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dierkes_J/0/1/0/all/0/1\">Julian Dierkes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stop Measuring Calibration When Humans Disagree. (arXiv:2210.16133v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.16133","description":"<p>Calibration is a popular framework to evaluate whether a classifier knows\nwhen it does not know - i.e., its predictive probabilities are a good\nindication of how likely a prediction is to be correct. Correctness is commonly\nestimated against the human majority class. Recently, calibration to human\nmajority has been measured on tasks where humans inherently disagree about\nwhich class applies. We show that measuring calibration to human majority given\ninherent disagreements is theoretically problematic, demonstrate this\nempirically on the ChaosNLI dataset, and derive several instance-level measures\nof calibration that capture key statistical properties of human judgements -\nclass frequency, ranking and entropy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baan_J/0/1/0/all/0/1\">Joris Baan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1\">Wilker Aziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_R/0/1/0/all/0/1\">Raquel Fern&#xe1;ndez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Chinese Word Segmentation and Span-based Constituency Parsing. (arXiv:2211.01638v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.01638","description":"<p>In constituency parsing, span-based decoding is an important direction.\nHowever, for Chinese sentences, because of their linguistic characteristics, it\nis necessary to utilize other models to perform word segmentation first, which\nintroduces a series of uncertainties and generally leads to errors in the\ncomputation of the constituency tree afterward. This work proposes a method for\njoint Chinese word segmentation and Span-based Constituency Parsing by adding\nextra labels to individual Chinese characters on the parse trees. Through\nexperiments, the proposed algorithm outperforms the recent models for joint\nsegmentation and constituency parsing on CTB 5.1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1\">Tianyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficiently Trained Mongolian Text-to-Speech System Based On FullConv. (arXiv:2211.01948v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.01948","description":"<p>Recurrent Neural Networks (RNNs) have become the standard modeling technique\nfor sequence data, and are used in a number of novel text-to-speech models.\nHowever, training a TTS model including RNN components has certain requirements\nfor GPU performance and takes a long time. In contrast, studies have shown that\nCNN-based sequence synthesis technology can greatly reduce training time in\ntext-to-speech models while ensuring a certain performance due to its high\nparallelism. We propose a new text-to-speech system based on deep convolutional\nneural networks that does not employ any RNN components (recurrent units). At\nthe same time, we improve the generality and robustness of our model through a\nseries of data augmentation methods such as Time Warping, Frequency Mask, and\nTime Mask. The final experimental results show that the TTS model using only\nthe CNN component can reduce the training time compared to the classic TTS\nmodels such as Tacotron while ensuring the quality of the synthesized speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">ZiQi Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GreenPLM: Cross-lingual pre-trained language models conversion with (almost) no cost. (arXiv:2211.06993v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.06993","description":"<p>While large pre-trained models have transformed the field of natural language\nprocessing (NLP), the high training cost and low cross-lingual availability of\nsuch models prevent the new advances from being equally shared by users across\nall languages, especially the less spoken ones. To promote equal opportunities\nfor all language speakers in NLP research and to reduce energy consumption for\nsustainability, this study proposes an effective and energy-efficient framework\nGreenPLM that uses bilingual lexicons to directly translate language models of\none language into other languages at (almost) no additional cost. We validate\nthis approach in 18 languages and show that this framework is comparable to, if\nnot better than, other heuristics trained with high cost. In addition, when\ngiven a low computational cost (2.5\\%), the framework outperforms the original\nmonolingual language models in six out of seven tested languages. We release\nlanguage models in 50 languages translated from English and the source code\nhere.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1\">Qingcheng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garay_L/0/1/0/all/0/1\">Lucas Garay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Peilin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_D/0/1/0/all/0/1\">Dading Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yining Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiageng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yikang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Han Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Adversarial Training with Robust Early-Bird Tickets. (arXiv:2211.07263v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07263","description":"<p>Adversarial training is one of the most powerful methods to improve the\nrobustness of pre-trained language models (PLMs). However, this approach is\ntypically more expensive than traditional fine-tuning because of the necessity\nto generate adversarial examples via gradient descent. Delving into the\noptimization process of adversarial training, we find that robust connectivity\npatterns emerge in the early training phase (typically $0.15\\sim0.3$ epochs),\nfar before parameters converge. Inspired by this finding, we dig out robust\nearly-bird tickets (i.e., subnetworks) to develop an efficient adversarial\ntraining method: (1) searching for robust tickets with structured sparsity in\nthe early stage; (2) fine-tuning robust tickets in the remaining time. To\nextract the robust tickets as early as possible, we design a ticket convergence\nmetric to automatically terminate the searching process. Experiments show that\nthe proposed efficient adversarial training method can achieve up to $7\\times\n\\sim 13 \\times$ training speedups while maintaining comparable or even better\nrobustness compared to the most competitive state-of-the-art adversarial\ntraining methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1\">Zhiheng Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Rui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrated Interpretation: Confidence Estimation in Semantic Parsing. (arXiv:2211.07443v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07443","description":"<p>Task-oriented semantic parsing is increasingly being used in user-facing\napplications, making measuring the calibration of parsing models especially\nimportant. We examine the calibration characteristics of six models across\nthree model families on two common English semantic parsing datasets, finding\nthat many models are reasonably well-calibrated and that there is a trade-off\nbetween calibration and performance. Based on confidence scores across three\nmodels, we propose and release new challenge splits of the two datasets we\nexamine. We then illustrate the ways a calibrated model can be useful in\nbalancing common trade-offs in task-oriented parsing. In a simulated\nannotator-in-the-loop experiment, we show that using model confidence allows us\nto improve the accuracy on validation programs by 9.6% (absolute) with\nannotator interactions on only 2.2% of tokens. Using sequence-level confidence\nscores, we then examine how we can optimize trade-off between a parser's\nusability and safety. We show that confidence-based thresholding can reduce the\nnumber of incorrect low-confidence programs executed by 76%; however, this\ncomes at a cost to usability. We propose the DidYouMean system which balances\nusability and safety. We conclude by calling for calibration to be included in\nthe evaluation of semantic parsing systems, and release a library for computing\ncalibration metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stengel_Eskin_E/0/1/0/all/0/1\">Elias Stengel-Eskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cracking Double-Blind Review: Authorship Attribution with Deep Learning. (arXiv:2211.07467v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07467","description":"<p>Double-blind peer review is considered a pillar of academic research because\nit is perceived to ensure a fair, unbiased, and fact-centered scientific\ndiscussion. Yet, experienced researchers can often correctly guess from which\nresearch group an anonymous submission originates, biasing the peer-review\nprocess. In this work, we present a transformer-based, neural-network\narchitecture that only uses the text content and the author names in the\nbibliography to atttribute an anonymous manuscript to an author. To train and\nevaluate our method, we created the largest authorship-identification dataset\nto date. It leverages all research papers publicly available on arXiv amounting\nto over 2 million manuscripts. In arXiv-subsets with up to 2,000 different\nauthors, our method achieves an unprecedented authorship attribution accuracy,\nwhere up to 95% of papers are attributed correctly. Thanks to our method, we\nare not only able to predict the author of an anonymous work but we also\nidentify weaknesses of the double-blind review process by finding the key\naspects that make a paper attributable. We believe that this work gives\nprecious insights into how a submission can remain anonymous in order to\nsupport an unbiased double-blind review process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bauersfeld_L/0/1/0/all/0/1\">Leonard Bauersfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_A/0/1/0/all/0/1\">Angel Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muglikar_M/0/1/0/all/0/1\">Manasi Muglikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMAUG: Sparse Masked Autoencoder for Efficient Video-Language Pre-training. (arXiv:2211.11446v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.11446","description":"<p>Video-language pre-training is crucial for learning powerful multi-modal\nrepresentation. However, it typically requires a massive amount of computation.\nIn this paper, we develop SMAUG, an efficient pre-training framework for\nvideo-language models. The foundation component in SMAUG is masked\nautoencoders. Different from prior works which only mask textual inputs, our\nmasking strategy considers both visual and textual modalities, providing a\nbetter cross-modal alignment and saving more pre-training costs. On top of\nthat, we introduce a space-time token sparsification module, which leverages\ncontext information to further select only \"important\" spatial regions and\ntemporal frames for pre-training. Coupling all these designs allows our method\nto enjoy both competitive performances on text-to-video retrieval and video\nquestion answering tasks, and much less pre-training costs by 1.9X or more. For\nexample, our SMAUG only needs about 50 NVIDIA A6000 GPU hours for pre-training\nto attain competitive performances on these two video-language tasks across six\npopular benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuanze Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huiyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cihang Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BotSIM: An End-to-End Bot Simulation Framework for Commercial Task-Oriented Dialog Systems. (arXiv:2211.11982v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.11982","description":"<p>We present BotSIM, a data-efficient end-to-end Bot SIMulation toolkit for\ncommercial text-based task-oriented dialog (TOD) systems. BotSIM consists of\nthree major components: 1) a Generator that can infer semantic-level dialog\nacts and entities from bot definitions and generate user queries via\nmodel-based paraphrasing; 2) an agenda-based dialog user Simulator (ABUS) to\nsimulate conversations with the dialog agents; 3) a Remediator to analyze the\nsimulated conversations, visualize the bot health reports and provide\nactionable remediation suggestions for bot troubleshooting and improvement. We\ndemonstrate BotSIM's effectiveness in end-to-end evaluation, remediation and\nmulti-intent dialog generation via case studies on two commercial bot\nplatforms. BotSIM's \"generation-simulation-remediation\" paradigm accelerates\nthe end-to-end bot evaluation and iteration process by: 1) reducing manual test\ncases creation efforts; 2) enabling a holistic gauge of the bot in terms of NLU\nand end-to-end performance via extensive dialog simulation; 3) improving the\nbot troubleshooting process with actionable suggestions. A demo of our system\ncan be found at https://tinyurl.com/mryu74cd and a demo video at\nhttps://youtu.be/qLi5iSoly30. We have open-sourced the toolkit at\nhttps://github.com/salesforce/botsim\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangsen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Samson Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Au_J/0/1/0/all/0/1\">Jimmy Au</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models. (arXiv:2211.15029v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.15029","description":"<p>We present DiffusionBERT, a new generative masked language model based on\ndiscrete diffusion models. Diffusion models and many pre-trained language\nmodels have a shared training objective, i.e., denoising, making it possible to\ncombine the two powerful models and enjoy the best of both worlds. On the one\nhand, diffusion models offer a promising training strategy that helps improve\nthe generation quality. On the other hand, pre-trained denoising language\nmodels (e.g., BERT) can be used as a good initialization that accelerates\nconvergence. We explore training BERT to learn the reverse process of a\ndiscrete diffusion process with an absorbing state and elucidate several\ndesigns to improve it. First, we propose a new noise schedule for the forward\ndiffusion process that controls the degree of noise added at each step based on\nthe information of each token. Second, we investigate several designs of\nincorporating the time step into BERT. Experiments on unconditional text\ngeneration demonstrate that DiffusionBERT achieves significant improvement over\nexisting diffusion models for text (e.g., D3PM and Diffusion-LM) and previous\ngenerative masked language models in terms of perplexity and BLEU score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhengfu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kuanning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BotSIM: An End-to-End Bot Simulation Toolkit for Commercial Task-Oriented Dialog Systems. (arXiv:2211.15916v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.15916","description":"<p>We introduce BotSIM, a modular, open-source Bot SIMulation environment with\ndialog generation, user simulation and conversation analytics capabilities.\nBotSIM aims to serve as a one-stop solution for large-scale data-efficient\nend-to-end evaluation, diagnosis and remediation of commercial task-oriented\ndialog (TOD) systems to significantly accelerate commercial bot development and\nevaluation, reduce cost and time-to-market. BotSIM adopts a layered design\ncomprising the infrastructure layer, the adaptor layer and the application\nlayer. The infrastructure layer hosts key models and components to support\nBotSIM's major functionalities via a streamlined\n\"generation-simulation-remediation\" pipeline. The adaptor layer is used to\nextend BotSIM to accommodate new bot platforms. The application layer provides\na suite of command line tools and a Web App to significantly lower the entry\nbarrier for BotSIM users such as bot admins or practitioners. In this report,\nwe focus on the technical designs of various system components. A detailed case\nstudy using Einstein BotBuilder is also presented to show how to apply BotSIM\npipeline for bot evaluation and remediation. The detailed system descriptions\ncan be found in our system demo paper. The toolkit is available at:\nhttps://github.com/salesforce/BotSIM .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangsen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer. (arXiv:2202.02113v6 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2202.02113","description":"<p>Knowledge graph completion aims to address the problem of extending a KG with\nmissing triples. In this paper, we provide an approach GenKGC, which converts\nknowledge graph completion to sequence-to-sequence generation task with the\npre-trained language model. We further introduce relation-guided demonstration\nand entity-aware hierarchical decoding for better representation learning and\nfast inference. Experimental results on three datasets show that our approach\ncan obtain better or comparable performance than baselines and achieve faster\ninference speed compared with previous methods with pre-trained language\nmodels. We also release a new large-scale Chinese knowledge graph dataset\nAliopenKG500 for research purpose. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKG/tree/main/GenKGC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion. (arXiv:2205.02357v4 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2205.02357","description":"<p>Multimodal Knowledge Graphs (MKGs), which organize visual-text factual\nknowledge, have recently been successfully applied to tasks such as information\nretrieval, question answering, and recommendation system. Since most MKGs are\nfar from complete, extensive knowledge graph completion studies have been\nproposed focusing on the multimodal entity, relation extraction and link\nprediction. However, different tasks and modalities require changes to the\nmodel architecture, and not all images/objects are relevant to text input,\nwhich hinders the applicability to diverse real-world scenarios. In this paper,\nwe propose a hybrid transformer with multi-level fusion to address those\nissues. Specifically, we leverage a hybrid transformer architecture with\nunified input-output for diverse multimodal knowledge graph completion tasks.\nMoreover, we propose multi-level fusion, which integrates visual and text\nrepresentation via coarse-grained prefix-guided interaction and fine-grained\ncorrelation-aware fusion modules. We conduct extensive experiments to validate\nthat our MKGformer can obtain SOTA performance on four datasets of multimodal\nlink prediction, multimodal RE, and multimodal NER. Code is available in\nhttps://github.com/zjunlp/MKGformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction. (arXiv:2205.03521v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2205.03521","description":"<p>Multimodal named entity recognition and relation extraction (MNER and MRE) is\na fundamental and crucial branch in information extraction. However, existing\napproaches for MNER and MRE usually suffer from error sensitivity when\nirrelevant object images incorporated in texts. To deal with these issues, we\npropose a novel Hierarchical Visual Prefix fusion NeTwork (HVPNeT) for\nvisual-enhanced entity and relation extraction, aiming to achieve more\neffective and robust performance. Specifically, we regard visual representation\nas pluggable visual prefix to guide the textual representation for error\ninsensitive forecasting decision. We further propose a dynamic gated\naggregation strategy to achieve hierarchical multi-scaled visual features as\nvisual prefix for fusion. Extensive experiments on three benchmark datasets\ndemonstrate the effectiveness of our method, and achieve state-of-the-art\nperformance. Code is available in https://github.com/zjunlp/HVPNeT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relphormer: Relational Graph Transformer for Knowledge Graph Representations. (arXiv:2205.10852v4 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2205.10852","description":"<p>Transformers have achieved remarkable performance in widespread fields,\nincluding natural language processing, computer vision and graph mining.\nHowever, vanilla Transformer architectures have not yielded promising\nimprovements in the Knowledge Graph (KG) representations, where the\ntranslational distance paradigm dominates this area. Note that vanilla\nTransformer architectures struggle to capture the intrinsically heterogeneous\nsemantic and structural information of knowledge graphs. To this end, we\npropose a new variant of Transformer for knowledge graph representations dubbed\nRelphormer. Specifically, we introduce Triple2Seq which can dynamically sample\ncontextualized sub-graph sequences as the input to alleviate the heterogeneity\nissue. We propose a novel structure-enhanced self-attention mechanism to encode\nthe relational information and keep the globally semantic information among\nsub-graphs. Moreover, we propose masked knowledge modeling as a new paradigm\nfor knowledge graph representation learning. We apply Relphormer to three\ntasks, namely, knowledge graph completion, KG-based question answering and\nKG-based recommendation for evaluation. Experimental results show that\nRelphormer can obtain better performance on benchmark datasets compared with\nbaselines. Code is available in https://github.com/zjunlp/Relphormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Analogical Reasoning over Knowledge Graphs. (arXiv:2210.00312v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2210.00312","description":"<p>Analogical reasoning is fundamental to human cognition and holds an important\nplace in various fields. However, previous studies mainly focus on single-modal\nanalogical reasoning and ignore taking advantage of structure knowledge.\nNotably, the research in cognitive psychology has demonstrated that information\nfrom multimodal sources always brings more powerful cognitive transfer than\nsingle modality sources. To this end, we introduce the new task of multimodal\nanalogical reasoning over knowledge graphs, which requires multimodal reasoning\nability with the help of background knowledge. Specifically, we construct a\nMultimodal Analogical Reasoning dataSet (MARS) and a multimodal knowledge graph\nMarKG. We evaluate with multimodal knowledge graph embedding and pre-trained\nTransformer baselines, illustrating the potential challenges of the proposed\ntask. We further propose a novel model-agnostic Multimodal analogical reasoning\nframework with Transformer (MarT) motivated by the structure mapping theory,\nwhich can obtain better performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing and Adversarial: Improve ASR with Speaker Labels. (arXiv:2211.06369v1 [eess.AS] CROSS LISTED)","link":"http://arxiv.org/abs/2211.06369","description":"<p>ASR can be improved by multi-task learning (MTL) with domain enhancing or\ndomain adversarial training, which are two opposite objectives with the aim to\nincrease/decrease domain variance towards domain-aware/agnostic ASR,\nrespectively. In this work, we study how to best apply these two opposite\nobjectives with speaker labels to improve conformer-based ASR. We also propose\na novel adaptive gradient reversal layer for stable and effective adversarial\ntraining without tuning effort. Detailed analysis and experimental verification\nare conducted to show the optimal positions in the ASR neural network (NN) to\napply speaker enhancing and adversarial training. We also explore their\ncombination for further improvement, achieving the same performance as\ni-vectors plus adversarial training. Our best speaker-based MTL achieves 7\\%\nrelative improvement on the Switchboard Hub5'00 set. We also investigate the\neffect of such speaker-based MTL w.r.t. cleaner dataset and weaker ASR NN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1\">Haotian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeineldeen_M/0/1/0/all/0/1\">Mohammad Zeineldeen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luscher_C/0/1/0/all/0/1\">Christoph L&#xfc;scher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-11-30T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}