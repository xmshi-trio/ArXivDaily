{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-12-22T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Ontologically Faithful Generation of Non-Player Character Dialogues. (arXiv:2212.10618v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10618","description":"<p>We introduce a language generation task grounded in a popular video game\nenvironment. KNUDGE (KNowledge Constrained User-NPC Dialogue GEneration)\ninvolves generating dialogue trees conditioned on an ontology captured in\nnatural language passages providing quest and entity specifications. KNUDGE is\nconstructed from side quest dialogues drawn directly from game data of Obsidian\nEntertainment's The Outer Worlds, leading to real-world complexities in\ngeneration: (1) dialogues are branching trees as opposed to linear chains of\nutterances; (2) utterances must remain faithful to the game lore--character\npersonas, backstories, and entity relationships; and (3) a dialogue must\naccurately reveal new quest-related details to the human player. We report\nresults for supervised and in-context learning techniques, finding there is\nsignificant room for future work on creating realistic game-quality dialogues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weir_N/0/1/0/all/0/1\">Nathaniel Weir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_R/0/1/0/all/0/1\">Ryan Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAmore_R/0/1/0/all/0/1\">Randolph D&#x27;Amore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_K/0/1/0/all/0/1\">Kellie Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jhamtani_H/0/1/0/all/0/1\">Harsh Jhamtani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mFACE: Multilingual Summarization with Factual Consistency Evaluation. (arXiv:2212.10622v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10622","description":"<p>Abstractive summarization has enjoyed renewed interest in recent years,\nthanks to pre-trained language models and the availability of large-scale\ndatasets. Despite promising results, current models still suffer from\ngenerating factually inconsistent summaries, reducing their utility for\nreal-world application. Several recent efforts attempt to address this by\ndevising models that automatically detect factual inconsistencies in machine\ngenerated summaries. However, they focus exclusively on English, a language\nwith abundant resources. In this work, we leverage factual consistency\nevaluation models to improve multilingual summarization. We explore two\nintuitive approaches to mitigate hallucinations based on the signal provided by\na multilingual NLI model, namely data filtering and controlled generation.\nExperimental results in the 45 languages from the XLSum dataset show gains over\nstrong baselines in both automatic and human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1\">Roee Aharoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_S/0/1/0/all/0/1\">Shashi Narayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maynez_J/0/1/0/all/0/1\">Joshua Maynez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1\">Jonathan Herzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_E/0/1/0/all/0/1\">Elizabeth Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KronA: Parameter Efficient Tuning with Kronecker Adapter. (arXiv:2212.10650v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10650","description":"<p>Fine-tuning a Pre-trained Language Model (PLM) on a specific downstream task\nhas been a well-known paradigm in Natural Language Processing. However, with\nthe ever-growing size of PLMs, training the entire model on several downstream\ntasks becomes very expensive and resource-hungry. Recently, different Parameter\nEfficient Tuning (PET) techniques are proposed to improve the efficiency of\nfine-tuning PLMs. One popular category of PET methods is the low-rank\nadaptation methods which insert learnable truncated SVD modules into the\noriginal model either sequentially or in parallel. However, low-rank\ndecomposition suffers from limited representation power. In this work, we\naddress this problem using the Kronecker product instead of the low-rank\nrepresentation. We introduce KronA, a Kronecker product-based adapter module\nfor efficient fine-tuning of Transformer-based PLMs. We apply the proposed\nmethods for fine-tuning T5 on the GLUE benchmark to show that incorporating the\nKronecker-based modules can outperform state-of-the-art PET methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Edalati_A/0/1/0/all/0/1\">Ali Edalati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tahaei_M/0/1/0/all/0/1\">Marzieh Tahaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobyzev_I/0/1/0/all/0/1\">Ivan Kobyzev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nia_V/0/1/0/all/0/1\">Vahid Partovi Nia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">James J. Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models. (arXiv:2212.10670v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10670","description":"<p>Given the success with in-context learning of large pre-trained language\nmodels, we introduce in-context learning distillation to transfer in-context\nfew-shot learning ability from large models to smaller models. We propose to\ncombine in-context learning objectives with language modeling objectives to\ndistill both the ability to read in-context examples and task knowledge to the\nsmaller models. We perform in-context learning distillation under two different\nfew-shot learning paradigms: Meta In-context Tuning (Meta-ICT) and Multitask\nIn-context Tuning (Multitask-ICT). Multitask-ICT performs better on multitask\nfew-shot learning but also requires more computation than Meta-ICT. Our method\nshows consistent improvements for both Meta-ICT and Multitask-ICT on two\nbenchmarks: LAMA and CrossFit. Our extensive experiments and analysis reveal\nthat in-context learning objectives and language modeling objectives are\ncomplementary under the Multitask-ICT paradigm. In-context learning objectives\nachieve the best performance when combined with language modeling objectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yukun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanda Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Stereotypes in Language Models: Towards Robust Measurement and Zero-Shot Debiasing. (arXiv:2212.10678v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10678","description":"<p>Generated texts from large pretrained language models have been shown to\nexhibit a variety of harmful, human-like biases about various demographics.\nThese findings prompted large efforts aiming to understand and measure such\neffects, with the goal of providing benchmarks that can guide the development\nof techniques mitigating these stereotypical associations. However, as recent\nresearch has pointed out, the current benchmarks lack a robust experimental\nsetup, consequently hindering the inference of meaningful conclusions from\ntheir evaluation metrics. In this paper, we extend these arguments and\ndemonstrate that existing techniques and benchmarks aiming to measure\nstereotypes tend to be inaccurate and consist of a high degree of experimental\nnoise that severely limits the knowledge we can gain from benchmarking language\nmodels based on them. Accordingly, we propose a new framework for robustly\nmeasuring and quantifying biases exhibited by generative language models.\nFinally, we use this framework to investigate GPT-3's occupational gender bias\nand propose prompting techniques for mitigating these biases without the need\nfor fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mattern_J/0/1/0/all/0/1\">Justus Mattern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"METEOR Guided Divergence for Video Captioning. (arXiv:2212.10690v1 [cs.CV])","link":"http://arxiv.org/abs/2212.10690","description":"<p>Automatic video captioning aims for a holistic visual scene understanding. It\nrequires a mechanism for capturing temporal context in video frames and the\nability to comprehend the actions and associations of objects in a given\ntimeframe. Such a system should additionally learn to abstract video sequences\ninto sensible representations as well as to generate natural written language.\nWhile the majority of captioning models focus solely on the visual inputs,\nlittle attention has been paid to the audiovisual modality. To tackle this\nissue, we propose a novel two-fold approach. First, we implement a\nreward-guided KL Divergence to train a video captioning model which is\nresilient towards token permutations. Second, we utilise a Bi-Modal\nHierarchical Reinforcement Learning (BMHRL) Transformer architecture to capture\nlong-term temporal dependencies of the input data as a foundation for our\nhierarchical captioning module. Using our BMHRL, we show the suitability of the\nHRL agent in the generation of content-complete and grammatically sound\nsentences by achieving $4.91$, $2.23$, and $10.80$ in BLEU3, BLEU4, and METEOR\nscores, respectively on the ActivityNet Captions dataset. Finally, we make our\nBMHRL framework and trained models publicly available for users and developers\nat https://github.com/d-rothen/bmhrl.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rothenpieler_D/0/1/0/all/0/1\">Daniel Lukas Rothenpieler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amiriparian_S/0/1/0/all/0/1\">Shahin Amiriparian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generation-Augmented Query Expansion For Code Retrieval. (arXiv:2212.10692v1 [cs.SE])","link":"http://arxiv.org/abs/2212.10692","description":"<p>Pre-trained language models have achieved promising success in code retrieval\ntasks, where a natural language documentation query is given to find the most\nrelevant existing code snippet. However, existing models focus only on\noptimizing the documentation code pairs by embedding them into latent space,\nwithout the association of external knowledge. In this paper, we propose a\ngeneration-augmented query expansion framework. Inspired by the human retrieval\nprocess - sketching an answer before searching, in this work, we utilize the\npowerful code generation model to benefit the code retrieval task.\nSpecifically, we demonstrate that rather than merely retrieving the target code\nsnippet according to the documentation query, it would be helpful to augment\nthe documentation query with its generation counterpart - generated code\nsnippets from the code generation model. To the best of our knowledge, this is\nthe first attempt that leverages the code generation model to enhance the code\nretrieval task. We achieve new state-of-the-art results on the CodeSearchNet\nbenchmark and surpass the baselines significantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Ruoming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Semantic Faithfulness of Language Models via Input Intervention on Conversational Question Answering. (arXiv:2212.10696v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10696","description":"<p>Transformer-based language models have been shown to be highly effective for\nseveral NLP tasks. In this paper, we consider three transformer models, BERT,\nRoBERTa, and XLNet, in both small and large version, and investigate how\nfaithful their representations are with respect to the semantic content of\ntexts. We formalize a notion of semantic faithfulness, in which the semantic\ncontent of a text should causally figure in a model's inferences in question\nanswering. We then test this notion by observing a model's behavior on\nanswering questions about a story after performing two novel semantic\ninterventions -- deletion intervention and negation intervention. While\ntransformer models achieve high performance on standard question answering\ntasks, we show that they fail to be semantically faithful once we perform these\ninterventions for a significant number of cases (~50% for deletion\nintervention, and ~20% drop in accuracy for negation intervention). We then\npropose an intervention-based training regime that can mitigate the undesirable\neffects for deletion intervention by a significant margin (from ~50% to ~6%).\nWe analyze the inner-workings of the models to better understand the\neffectiveness of intervention-based training for deletion intervention. But we\nshow that this training does not attenuate other aspects of semantic\nunfaithfulness such as the models' inability to deal with negation intervention\nor to capture the predicate-argument structure of texts. We also test\nInstructGPT, via prompting, for its ability to handle the two interventions and\nto capture predicate-argument structure. While InstructGPT models do achieve\nvery high performance on predicate-argument structure task, they fail to\nrespond adequately to our deletion and negation interventions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_A/0/1/0/all/0/1\">Akshay Chaturvedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhar_S/0/1/0/all/0/1\">Swarnadeep Bhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Soumadeep Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garain_U/0/1/0/all/0/1\">Utpal Garain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asher_N/0/1/0/all/0/1\">Nicholas Asher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extractive Text Summarization Using Generalized Additive Models with Interactions for Sentence Selection. (arXiv:2212.10707v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10707","description":"<p>Automatic Text Summarization (ATS) is becoming relevant with the growth of\ntextual data; however, with the popularization of public large-scale datasets,\nsome recent machine learning approaches have focused on dense models and\narchitectures that, despite producing notable results, usually turn out in\nmodels difficult to interpret. Given the challenge behind interpretable\nlearning-based text summarization and the importance it may have for evolving\nthe current state of the ATS field, this work studies the application of two\nmodern Generalized Additive Models with interactions, namely Explainable\nBoosting Machine and GAMI-Net, to the extractive summarization problem based on\nlinguistic features and binary classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1\">Vin&#xed;cius Camargo da Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Papa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_K/0/1/0/all/0/1\">Kelton Augusto Pontara da Costa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Triplet Extraction by Template Infilling. (arXiv:2212.10708v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10708","description":"<p>Triplet extraction aims to extract entities and their corresponding relations\nin unstructured text. Most existing methods train an extraction model on\nhigh-quality training data, and hence are incapable of extracting relations\nthat were not observed during training. Generalizing the model to unseen\nrelations typically requires fine-tuning on synthetic training data which is\noften noisy and unreliable. In this paper, we argue that reducing triplet\nextraction to a template filling task over a pre-trained language model can\nequip the model with zero-shot learning capabilities and enable it to leverage\nthe implicit knowledge in the language model. Embodying these ideas, we propose\na novel framework, ZETT (ZEro-shot Triplet extraction by Template infilling),\nthat is based on end-to-end generative transformers. Our experiments show that\nwithout any data augmentation or pipeline systems, ZETT can outperform previous\nstate-of-the-art models with 25% less parameters. We further show that ZETT is\nmore robust in detecting entities and can be incorporated with automatically\ngenerated templates for relations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Bosung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iso_H/0/1/0/all/0/1\">Hayate Iso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhutani_N/0/1/0/all/0/1\">Nikita Bhutani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hruschka_E/0/1/0/all/0/1\">Estevam Hruschka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakashole_N/0/1/0/all/0/1\">Ndapa Nakashole</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task Ambiguity in Humans and Language Models. (arXiv:2212.10711v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10711","description":"<p>Language models have recently achieved strong performance across a wide range\nof NLP benchmarks. However, unlike benchmarks, real world tasks are often\npoorly specified, and agents must deduce the user's intended behavior from a\ncombination of context, instructions, and examples. We investigate how both\nhumans and models behave in the face of such task ambiguity by proposing\nAmbiBench, a new benchmark of six ambiguously-specified classification tasks.\nWe evaluate humans and models on AmbiBench by seeing how well they identify the\nintended task using 1) instructions with varying degrees of ambiguity, and 2)\ndifferent numbers of labeled examples. We find that the combination of model\nscaling (to 175B parameters) and training with human feedback data enables\nmodels to approach or exceed the accuracy of human participants across tasks,\nbut that either one alone is not sufficient. In addition, we show how to\ndramatically improve the accuracy of language models trained without\nlarge-scale human feedback training by finetuning on a small number of\nambiguous in-context examples, providing a promising direction for teaching\nmodels to generalize well in the face of ambiguity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tamkin_A/0/1/0/all/0/1\">Alex Tamkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Handa_K/0/1/0/all/0/1\">Kunal Handa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrestha_A/0/1/0/all/0/1\">Avash Shrestha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah Goodman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Heterogeneous Domain Information into Relation Extraction: A Case Study on Drug-Drug Interaction Extraction. (arXiv:2212.10714v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10714","description":"<p>The development of deep neural networks has improved representation learning\nin various domains, including textual, graph structural, and relational triple\nrepresentations. This development opened the door to new relation extraction\nbeyond the traditional text-oriented relation extraction. However, research on\nthe effectiveness of considering multiple heterogeneous domain information\nsimultaneously is still under exploration, and if a model can take an advantage\nof integrating heterogeneous information, it is expected to exhibit a\nsignificant contribution to many problems in the world. This thesis works on\nDrug-Drug Interactions (DDIs) from the literature as a case study and realizes\nrelation extraction utilizing heterogeneous domain information. First, a deep\nneural relation extraction model is prepared and its attention mechanism is\nanalyzed. Next, a method to combine the drug molecular structure information\nand drug description information to the input sentence information is proposed,\nand the effectiveness of utilizing drug molecular structures and drug\ndescriptions for the relation extraction task is shown. Then, in order to\nfurther exploit the heterogeneous information, drug-related items, such as\nprotein entries, medical terms and pathways are collected from multiple\nexisting databases and a new data set in the form of a knowledge graph (KG) is\nconstructed. A link prediction task on the constructed data set is conducted to\nobtain embedding representations of drugs that contain the heterogeneous domain\ninformation. Finally, a method that integrates the input sentence information\nand the heterogeneous KG information is proposed. The proposed model is trained\nand evaluated on a widely used data set, and as a result, it is shown that\nutilizing heterogeneous domain information significantly improves the\nperformance of relation extraction from the literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asada_M/0/1/0/all/0/1\">Masaki Asada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoralDial: A Framework to Train and Evaluate Moral Dialogue Systems via Constructing Moral Discussions. (arXiv:2212.10720v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10720","description":"<p>Morality in dialogue systems has raised great attention in research recently.\nA moral dialogue system could better connect users and enhance conversation\nengagement by gaining users' trust. In this paper, we propose a framework,\nMoralDial to train and evaluate moral dialogue systems. In our framework, we\nfirst explore the communication mechanisms of morality and resolve expressed\nmorality into four sub-modules. The sub-modules indicate the roadmap for\nbuilding a moral dialogue system. Based on that, we design a simple yet\neffective method: constructing moral discussions from Rules of Thumb (RoTs)\nbetween simulated specific users and the dialogue system. The constructed\ndiscussion consists of expressing, explaining, and revising the moral views in\ndialogue exchanges, which makes conversational models learn morality well in a\nnatural manner. Furthermore, we propose a novel evaluation method in the\nframework. We evaluate the multiple aspects of morality by judging the relation\nbetween dialogue responses and RoTs in discussions, where the multifaceted\nnature of morality is particularly considered. Automatic and manual experiments\ndemonstrate that our framework is promising to train and evaluate moral\ndialogue systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhexin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jianwei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tracing and Removing Data Errors in Natural Language Generation Datasets. (arXiv:2212.10722v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10722","description":"<p>Recent work has identified noisy and misannotated data as a core cause of\nhallucinations and unfaithful outputs in Natural Language Generation (NLG)\ntasks. Consequently, identifying and removing these examples is a key open\nchallenge in creating reliable NLG systems. In this work, we introduce a\nframework to identify and remove low-quality training instances that lead to\nundesirable outputs, such as faithfulness errors in text summarization. We show\nthat existing approaches for error tracing, such as gradient-based influence\nmeasures, do not perform reliably for detecting faithfulness errors in\nsummarization. We overcome the drawbacks of existing error tracing methods\nthrough a new, contrast-based estimate that compares undesired generations to\nhuman-corrected outputs. Our proposed method can achieve a mean average\nprecision of 0.91 across synthetic tasks with known ground truth and can\nachieve a two-fold reduction in hallucinations on a real entity hallucination\nevaluation on the NYT dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ladhak_F/0/1/0/all/0/1\">Faisal Ladhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval. (arXiv:2212.10726v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10726","description":"<p>Contrastive learning has been successfully used for retrieval of semantically\naligned sentences, but it often requires large batch sizes or careful\nengineering to work well. In this paper, we instead propose a generative model\nfor learning multilingual text embeddings which can be used to retrieve or\nscore sentence pairs. Our model operates on parallel data in $N$ languages and,\nthrough an approximation we introduce, efficiently encourages source separation\nin this multilingual setting, separating semantic information that is shared\nbetween translations from stylistic or language-specific variation. We show\ncareful large-scale comparisons between contrastive and generation-based\napproaches for learning multilingual text embeddings, a comparison that has not\nbeen done to the best of our knowledge despite the popularity of these\napproaches. We evaluate this method on a suite of tasks including semantic\nsimilarity, bitext mining, and cross-lingual question retrieval -- the last of\nwhich we introduce in this paper. Overall, our Variational Multilingual\nSource-Separation Transformer (VMSST) model outperforms both a strong\ncontrastive and generative baseline on these tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wieting_J/0/1/0/all/0/1\">John Wieting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jonathan H. Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spoken Language Understanding for Conversational AI: Recent Advances and Future Direction. (arXiv:2212.10728v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10728","description":"<p>When a human communicates with a machine using natural language on the web\nand online, how can it understand the human's intention and semantic context of\ntheir talk? This is an important AI task as it enables the machine to construct\na sensible answer or perform a useful action for the human. Meaning is\nrepresented at the sentence level, identification of which is known as intent\ndetection, and at the word level, a labelling task called slot filling. This\ndual-level joint task requires innovative thinking about natural language and\ndeep learning network design, and as a result, many approaches and models have\nbeen proposed and applied.\n</p>\n<p>This tutorial will discuss how the joint task is set up and introduce Spoken\nLanguage Understanding/Natural Language Understanding (SLU/NLU) with Deep\nLearning techniques. We will cover the datasets, experiments and metrics used\nin the field. We will describe how the machine uses the latest NLP and Deep\nLearning techniques to address the joint task, including recurrent and\nattention-based Transformer networks and pre-trained models (e.g. BERT). We\nwill then look in detail at a network that allows the two levels of the task,\nintent classification and slot filling, to interact to boost performance\nexplicitly. We will do a code demonstration of a Python notebook for this model\nand attendees will have an opportunity to watch coding demo tasks on this joint\nNLU to further their understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Siqu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_H/0/1/0/all/0/1\">Henry Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_J/0/1/0/all/0/1\">Josiah Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ToL: A Tensor of List-Based Unified Computation Model. (arXiv:2212.10740v1 [cs.PL])","link":"http://arxiv.org/abs/2212.10740","description":"<p>Previous computation models either have equivalent abilities in representing\nall computations but fail to provide primitive operators for programming\ncomplex algorithms or lack generalized expression ability to represent\nnewly-added computations. This article presents a unified computation model\nwith generalized expression ability and a concise set of primitive operators\nfor programming high-level algorithms. We propose a unified data abstraction --\nTensor of List, and offer a unified computation model based on Tensor of List,\nwhich we call the ToL model (in short, ToL). ToL introduces five atomic\ncomputations that can represent any elementary computation by finite\ncomposition, ensured with strict formal proof. Based on ToL, we design a\npure-functional language -- ToLang. ToLang provides a concise set of primitive\noperators that can be used to program complex big data and AI algorithms. Our\nevaluations show ToL has generalized expression ability and a built-in\nperformance indicator, born with a strictly defined computation metric --\nelementary operation count (EOPs), consistent with FLOPs within a small error\nrange.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wanling Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1\">Jianfeng Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition. (arXiv:2212.10750v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10750","description":"<p>The widely studied task of Natural Language Inference (NLI) requires a system\nto recognize whether one piece of text is textually entailed by another, i.e.\nwhether the entirety of its meaning can be inferred from the other. In current\nNLI datasets and models, textual entailment relations are typically defined on\nthe sentence- or paragraph-level. However, even a simple sentence often\ncontains multiple propositions, i.e. distinct units of meaning conveyed by the\nsentence. As these propositions can carry different truth values in the context\nof a given premise, we argue for the need to recognize the textual entailment\nrelation of each proposition in a sentence individually.\n</p>\n<p>We propose PropSegmEnt, a corpus of over 35K propositions annotated by expert\nhuman raters. Our dataset structure resembles the tasks of (1) segmenting\nsentences within a document to the set of propositions, and (2) classifying the\nentailment relation of each proposition with respect to a different yet\ntopically-aligned document, i.e. documents describing the same event or entity.\nWe establish strong baselines for the segmentation and entailment tasks.\nThrough case studies on summary hallucination detection and document-level NLI,\nwe demonstrate that our conceptual framework is potentially useful for\nunderstanding and explaining the compositionality of NLI labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sihao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buthpitiya_S/0/1/0/all/0/1\">Senaka Buthpitiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabrikant_A/0/1/0/all/0/1\">Alex Fabrikant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CORRPUS: Detecting Story Inconsistencies via Codex-Bootstrapped Neurosymbolic Reasoning. (arXiv:2212.10754v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10754","description":"<p>Story generation and understanding -- as with all NLG/NLU tasks -- has seen a\nsurge in neurosymbolic work. Researchers have recognized that, while large\nlanguage models (LLMs) have tremendous utility, they can be augmented with\nsymbolic means to be even better and to make up for any flaws that the neural\nnetworks might have. However, symbolic methods are extremely costly in terms of\nthe amount of time and expertise needed to create them. In this work, we\ncapitalize on state-of-the-art Code-LLMs, such as Codex, to bootstrap the use\nof symbolic methods for tracking the state of stories and aiding in story\nunderstanding. We show that our CoRRPUS system and abstracted prompting\nprocedures can beat current state-of-the-art structured LLM techniques on\npre-existing story understanding tasks (bAbI task 2 and Re^3) with minimal hand\nengineering. We hope that this work can help highlight the importance of\nsymbolic representations and specialized prompting for LLMs as these models\nrequire some guidance for performing reasoning tasks properly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yijiang River Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1\">Lara J. Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JASMINE: Arabic GPT Models for Few-Shot Learning. (arXiv:2212.10755v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10755","description":"<p>Task agnostic generative pretraining (GPT) has recently proved promising for\nzero- and few-shot learning, gradually diverting attention from the expensive\nsupervised learning paradigm. Although the community is accumulating knowledge\nas to capabilities of English-language autoregressive models such as GPT-3\nadopting this generative approach, scholarship about these models remains\nacutely Anglocentric. Consequently, the community currently has serious gaps in\nits understanding of this class of models, their potential, and their societal\nimpacts in diverse settings, linguistic traditions, and cultures. To alleviate\nthis issue for Arabic, a collection of diverse languages and language varieties\nwith more than $400$ million population, we introduce JASMINE, a suite of\npowerful Arabic autoregressive Transformer language models ranging in size\nbetween 300 million-13 billion parameters. We pretrain our new models with\nlarge amounts of diverse data (400GB of text) from different Arabic varieties\nand domains. We evaluate JASMINE extensively in both intrinsic and extrinsic\nsettings, using a comprehensive benchmark for zero- and few-shot learning\nacross a wide range of NLP tasks. We also carefully develop and release a novel\nbenchmark for both automated and human evaluation of Arabic autoregressive\nmodels focused at investigating potential social biases, harms, and toxicity in\nthese models. We aim to responsibly release our models with interested\nresearchers, along with code for experimenting with them\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagoudi_E/0/1/0/all/0/1\">El Moatez Billah Nagoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1\">AbdelRahim Elmadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inciarte_A/0/1/0/all/0/1\">Alcides Alcoba Inciarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khondaker_M/0/1/0/all/0/1\">Md Tawkat Islam Khondaker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ORCA: A Challenging Benchmark for Arabic Language Understanding. (arXiv:2212.10758v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10758","description":"<p>Due to their crucial role in all NLP, several benchmarks have been proposed\nto evaluate pretrained language models. In spite of these efforts, no public\nbenchmark of diverse nature currently exists for evaluation of Arabic. This\nmakes it challenging to measure progress for both Arabic and multilingual\nlanguage models. This challenge is compounded by the fact that any benchmark\ntargeting Arabic needs to take into account the fact that Arabic is not a\nsingle language but rather a collection of languages and varieties. In this\nwork, we introduce ORCA, a publicly available benchmark for Arabic language\nunderstanding evaluation. ORCA is carefully constructed to cover diverse Arabic\nvarieties and a wide range of challenging Arabic understanding tasks exploiting\n60 different datasets across seven NLU task clusters. To measure current\nprogress in Arabic NLU, we use ORCA to offer a comprehensive comparison between\n18 multilingual and Arabic language models. We also provide a public\nleaderboard with a unified single-number evaluation metric (ORCA score) to\nfacilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1\">AbdelRahim Elmadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagoudi_E/0/1/0/all/0/1\">El Moatez Billah Nagoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning List-Level Domain-Invariant Representations for Ranking. (arXiv:2212.10764v1 [cs.IR])","link":"http://arxiv.org/abs/2212.10764","description":"<p>Domain adaptation aims to transfer the knowledge acquired by models trained\non (data-rich) source domains to (low-resource) target domains, for which a\npopular method is invariant representation learning. While they have been\nstudied extensively for classification and regression problems, how they apply\nto ranking problems, where the data and metrics have a list structure, is not\nwell understood. Theoretically, we establish a domain adaptation generalization\nbound for ranking under listwise metrics such as MRR and NDCG. The bound\nsuggests an adaptation method via learning list-level domain-invariant feature\nrepresentations, whose benefits are empirically demonstrated by unsupervised\ndomain adaptation experiments on real-world ranking tasks, including passage\nreranking. A key message is that for domain adaptation, the representations\nshould be analyzed at the same level at which the metric is computed, as we\nshow that learning invariant representations at the list level is most\neffective for adaptation on ranking problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xian_R/0/1/0/all/0/1\">Ruicheng Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_H/0/1/0/all/0/1\">Honglei Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1\">Hamed Zamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Ji Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1\">Kai Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Han Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuanhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1\">Michael Bendersky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Does Beam Search improve Span-Level Confidence Estimation in Generative Sequence Labeling?. (arXiv:2212.10767v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10767","description":"<p>Text-to-text generation models have increasingly become the go-to solution\nfor a wide variety of sequence labeling tasks (e.g., entity extraction and\ndialog slot filling). While most research has focused on the labeling accuracy,\na key aspect -- of vital practical importance -- has slipped through the\ncracks: understanding model confidence. More specifically, we lack a principled\nunderstanding of how to reliably gauge the confidence of a model in its\npredictions for each labeled span. This paper aims to provide some empirical\ninsights on estimating model confidence for generative sequence labeling. Most\nnotably, we find that simply using the decoder's output probabilities is not\nthe best in realizing well-calibrated confidence estimates. As verified over\nsix public datasets of different tasks, we show that our proposed approach --\nwhich leverages statistics from top-$k$ predictions by a beam search --\nsignificantly reduces calibration errors of the predictions of a generative\nsequence labeling model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kazuma Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naim_I/0/1/0/all/0/1\">Iftekhar Naim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_K/0/1/0/all/0/1\">Karthik Raman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncontrolled Lexical Exposure Leads to Overestimation of Compositional Generalization in Pretrained Models. (arXiv:2212.10769v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10769","description":"<p>Human linguistic capacity is often characterized by compositionality and the\ngeneralization it enables -- human learners can produce and comprehend novel\ncomplex expressions by composing known parts. Several benchmarks exploit\ndistributional control across training and test to gauge compositional\ngeneralization, where certain lexical items only occur in limited contexts\nduring training. While recent work using these benchmarks suggests that\npretrained models achieve impressive generalization performance, we argue that\nexposure to pretraining data may break the aforementioned distributional\ncontrol. Using the COGS benchmark of Kim and Linzen (2020), we test two\nmodified evaluation setups that control for this issue: (1) substituting\ncontext-controlled lexical items with novel character sequences, and (2)\nsubstituting them with special tokens represented by novel embeddings. We find\nthat both of these setups lead to lower generalization performance in T5\n(Raffel et al., 2020), suggesting that previously reported results have been\noverestimated due to uncontrolled lexical exposure during pretraining. The\nperformance degradation is more extreme with novel embeddings, and the\ndegradation increases with the amount of pretraining data, highlighting an\ninteresting case of inverse scaling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Najoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smolensky_P/0/1/0/all/0/1\">Paul Smolensky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ImPaKT: A Dataset for Open-Schema Knowledge Base Construction. (arXiv:2212.10770v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10770","description":"<p>Large language models have ushered in a golden age of semantic parsing. The\nseq2seq paradigm allows for open-schema and abstractive attribute and relation\nextraction given only small amounts of finetuning data. Language model\npretraining has simultaneously enabled great strides in natural language\ninference, reasoning about entailment and implication in free text. These\nadvances motivate us to construct ImPaKT, a dataset for open-schema information\nextraction, consisting of around 2500 text snippets from the C4 corpus, in the\nshopping domain (product buying guides), professionally annotated with\nextracted attributes, types, attribute summaries (attribute schema discovery\nfrom idiosyncratic text), many-to-one relations between compound and atomic\nattributes, and implication relations. We release this data in hope that it\nwill be useful in fine tuning semantic parsers for information extraction and\nknowledge base construction across a variety of domains. We evaluate the power\nof this approach by fine-tuning the open source UL2 language model on a subset\nof the dataset, extracting a set of implication relations from a corpus of\nproduct buying guides, and conducting human evaluations of the resulting\npredictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vilnis_L/0/1/0/all/0/1\">Luke Vilnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_Z/0/1/0/all/0/1\">Zach Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanagal_B/0/1/0/all/0/1\">Bhargav Kanagal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_P/0/1/0/all/0/1\">Patrick Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghai_S/0/1/0/all/0/1\">Sumit Sanghai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning. (arXiv:2212.10773v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10773","description":"<p>Instruction tuning, a new learning paradigm that fine-tunes pre-trained\nlanguage models on tasks specified through instructions, has shown promising\nzero-shot performance on various natural language processing tasks. However,\nit's still not explored for vision and multimodal tasks. In this work, we\nintroduce MultiInstruct, the first multimodal instruction tuning benchmark\ndataset that consists of 47 diverse multimodal tasks covering 11 broad\ncategories. Each task is designed at least with 5,000 instances (input-out\npairs) from existing open-source datasets and 5 expert-written instructions. We\ntake OFA as the base pre-trained model for multimodal instruction tuning, and\nto improve its performance, we explore multiple transfer learning strategies to\nleverage the large-scale Natural Instructions dataset. Experimental results\ndemonstrate its strong zero-shot performance on various unseen multimodal tasks\nand the benefit of transfer learning from text-only instructions. We also\ndesign a new evaluation metric: Sensitivity, to evaluate how sensitive the\nmodel is to the variety of instructions. Our results indicate that the model is\nless sensitive to the varying instructions after finetuning on a diverse set of\ntasks and instructions for each task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can NLI Provide Proper Indirect Supervision for Low-resource Biomedical Relation Extraction?. (arXiv:2212.10784v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10784","description":"<p>Two key obstacles in biomedical relation extraction (RE) are the scarcity of\nannotations and the prevalence of instances without explicitly pre-defined\nlabels due to low annotation coverage. Existing approaches, which treat\nbiomedical RE as a multi-class classification task, often result in poor\ngeneralization in low-resource settings and do not have the ability to make\nselective prediction on unknown cases but give a guess from seen relations,\nhindering the applicability of those approaches. We present NBR, which converts\nbiomedical RE as natural language inference formulation through indirect\nsupervision. By converting relations to natural language hypotheses, NBR is\ncapable of exploiting semantic cues to alleviate annotation scarcity. By\nincorporating a ranking-based loss that implicitly calibrates abstinent\ninstances, NBR learns a clearer decision boundary and is instructed to abstain\non uncertain instances. Extensive experiments on three widely-used biomedical\nRE benchmarks, namely ChemProt, DDI and GAD, verify the effectiveness of NBR in\nboth full-set and low-resource regimes. Our analysis demonstrates that indirect\nsupervision benefits biomedical RE even when a domain gap exists, and combining\nNLI knowledge with biomedical knowledge leads to the best performance gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiashu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingyu Derek Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SERENGETI: Massively Multilingual Language Models for Africa. (arXiv:2212.10785v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10785","description":"<p>Multilingual language models (MLMs) acquire valuable, generalizable\nlinguistic information during pretraining and have advanced the state of the\nart on task-specific finetuning. So far, only ~ 28 out of ~2,000 African\nlanguages are covered in existing language models. We ameliorate this\nlimitation by developing SERENGETI, a set of massively multilingual language\nmodel that covers 517 African languages and language varieties. We evaluate our\nnovel models on eight natural language understanding tasks across 20 datasets,\ncomparing to four MLMs that each cover any number of African languages.\nSERENGETI outperforms other models on 11 datasets across the eights tasks and\nachieves 82.27 average F-1. We also perform error analysis on our models'\nperformance and show the influence of mutual intelligibility when the models\nare applied under zero-shot settings. We will publicly release our models for\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adebara_I/0/1/0/all/0/1\">Ife Adebara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1\">AbdelRahim Elmadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inciarte_A/0/1/0/all/0/1\">Alcides Alcoba Inciarte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-hop Evidence Retrieval for Cross-document Relation Extraction. (arXiv:2212.10786v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10786","description":"<p>Relation Extraction (RE) has been extended to cross-document scenarios\nbecause many relations are not simply described in a single document. This\ninevitably brings the challenge of efficient open-space evidence retrieval to\nsupport the inference of cross-document relations, along with the challenge of\nmulti-hop reasoning on top of entities and evidence scattered in an open set of\ndocuments. To combat these challenges, we propose Mr.CoD, a multi-hop evidence\nretrieval method based on evidence path mining and ranking with adapted dense\nretrievers. We explore multiple variants of retrievers to show evidence\nretrieval is an essential part in cross-document RE. Experiments on CodRED show\nthat evidence retrieval with Mr.Cod effectively acquires cross-document\nevidence that essentially supports open-setting cross-document RE.\nAdditionally, we show that Mr.CoD facilitates evidence retrieval and boosts\nend-to-end RE performance with effective multi-hop reasoning in both closed and\nopen settings of RE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Keming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingyu Derek Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing. (arXiv:2212.10789v1 [cs.LG])","link":"http://arxiv.org/abs/2212.10789","description":"<p>There is increasing adoption of artificial intelligence in drug discovery.\nHowever, existing works use machine learning to mainly utilize the chemical\nstructures of molecules yet ignore the vast textual knowledge available in\nchemistry. Incorporating textual knowledge enables us to realize new drug\ndesign objectives, adapt to text-based instructions, and predict complex\nbiological activities. We present a multi-modal molecule structure-text model,\nMoleculeSTM, by jointly learning molecule's chemical structures and textual\ndescriptions via a contrastive learning strategy. To train MoleculeSTM, we\nconstruct the largest multi-modal dataset to date, namely PubChemSTM, with over\n280K chemical structure-text pairs. To demonstrate the effectiveness and\nutility of MoleculeSTM, we design two challenging zero-shot tasks based on text\ninstructions, including structure-text retrieval and molecule editing.\nMoleculeSTM possesses two main properties: open vocabulary and compositionality\nvia natural language. In experiments, MoleculeSTM obtains the state-of-the-art\ngeneralization ability to novel biochemical concepts across various benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengchao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1\">Weili Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiarui Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1\">Zhuoran Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpineSum: Entailment-based self-training for abstractive opinion summarization. (arXiv:2212.10791v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10791","description":"<p>A typical product or place often has hundreds of reviews, and summarization\nof these texts is an important and challenging problem. Recent progress on\nabstractive summarization in domains such as news has been driven by supervised\nsystems trained on hundreds of thousands of news articles paired with\nhuman-written summaries. However for opinion texts, such large scale datasets\nare rarely available. Unsupervised methods, self-training, and few-shot\nlearning approaches bridge that gap. In this work, we present a novel\nself-training approach, OpineSum, for abstractive opinion summarization. The\nsummaries in this approach are built using a novel application of textual\nentailment and capture the consensus of opinions across the various reviews for\nan item. This method can be used to obtain silver-standard summaries on a large\nscale and train both unsupervised and few-shot abstractive summarization\nsystems. OpineSum achieves state-of-the-art performance in both settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Louis_A/0/1/0/all/0/1\">Annie Louis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maynez_J/0/1/0/all/0/1\">Joshua Maynez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reconstruction Probing. (arXiv:2212.10792v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10792","description":"<p>We propose reconstruction probing, a new analysis method for contextualized\nrepresentations based on reconstruction probabilities in masked language models\n(MLMs). This method relies on comparing the reconstruction probabilities of\ntokens in a given sequence when conditioned on the representation of a single\ntoken that has been fully contextualized and when conditioned on only the\ndecontextualized lexical prior of the model. This comparison can be understood\nas quantifying the contribution of contextualization towards reconstruction --\nthe difference in the reconstruction probabilities can only be attributed to\nthe representational change of the single token induced by contextualization.\nWe apply this analysis to three MLMs and find that contextualization boosts\nreconstructability of tokens that are close to the token being reconstructed in\nterms of linear and syntactic distance. Furthermore, we extend our analysis to\nfiner-grained decomposition of contextualized representations, and we find that\nthese boosts are largely attributable to static and positional embeddings at\nthe input layer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Najoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khilnani_J/0/1/0/all/0/1\">Jatin Khilnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warstadt_A/0/1/0/all/0/1\">Alex Warstadt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qaddoumi_A/0/1/0/all/0/1\">Abed Qaddoumi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models. (arXiv:2212.10815v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10815","description":"<p>We explore the use of large language models (LLMs) for zero-shot semantic\nparsing. Semantic parsing involves mapping natural language utterances to\ntask-specific meaning representations. Language models are generally trained on\nthe publicly available text and code and cannot be expected to directly\ngeneralize to domain-specific parsing tasks in a zero-shot setting. In this\nwork, we propose ZEROTOP, a zero-shot task-oriented parsing method that\ndecomposes a semantic parsing problem into a set of abstractive and extractive\nquestion-answering (QA) problems, enabling us to leverage the ability of LLMs\nto zero-shot answer reading comprehension questions. For each utterance, we\nprompt the LLM with questions corresponding to its top-level intent and a set\nof slots and use the LLM generations to construct the target meaning\nrepresentation. We observe that current LLMs fail to detect unanswerable\nquestions; and as a result, cannot handle questions corresponding to missing\nslots. To address this problem, we fine-tune a language model on public QA\ndatasets using synthetic negative samples. Experimental results show that our\nQA-based decomposition paired with the fine-tuned LLM can correctly parse ~16%\nof utterances in the MTOP dataset without requiring any annotated data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mekala_D/0/1/0/all/0/1\">Dheeraj Mekala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_J/0/1/0/all/0/1\">Jason Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Subhro Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"4D ASR: Joint modeling of CTC, Attention, Transducer, and Mask-Predict decoders. (arXiv:2212.10818v1 [cs.SD])","link":"http://arxiv.org/abs/2212.10818","description":"<p>The network architecture of end-to-end (E2E) automatic speech recognition\n(ASR) can be classified into several models, including connectionist temporal\nclassification (CTC), recurrent neural network transducer (RNN-T), attention\nmechanism, and non-autoregressive mask-predict models. Since each of these\nnetwork architectures has pros and cons, a typical use case is to switch these\nseparate models depending on the application requirement, resulting in the\nincreased overhead of maintaining all models. Several methods for integrating\ntwo of these complementary models to mitigate the overhead issue have been\nproposed; however, if we integrate more models, we will further benefit from\nthese complementary models and realize broader applications with a single\nsystem. This paper proposes four-decoder joint modeling (4D) of CTC, attention,\nRNN-T, and mask-predict, which has the following three advantages: 1) The four\ndecoders are jointly trained so that they can be easily switched depending on\nthe application scenarios. 2) Joint training may bring model regularization and\nimprove the model robustness thanks to their complementary properties. 3) Novel\none-pass joint decoding methods using CTC, attention, and RNN-T further\nimproves the performance. The experimental results showed that the proposed\nmodel consistently reduced the WER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sudo_Y/0/1/0/all/0/1\">Yui Sudo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakeel_M/0/1/0/all/0/1\">Muhammad Shakeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attend to the Right Context: A Plug-and-Play Module for Content-Controllable Summarization. (arXiv:2212.10819v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10819","description":"<p>Content-Controllable Summarization generates summaries focused on the given\ncontrolling signals. Due to the lack of large-scale training corpora for the\ntask, we propose a plug-and-play module RelAttn to adapt any general\nsummarizers to the content-controllable summarization task. RelAttn first\nidentifies the relevant content in the source documents, and then makes the\nmodel attend to the right context by directly steering the attention weight. We\nfurther apply an unsupervised online adaptive parameter searching algorithm to\ndetermine the degree of control in the zero-shot setting, while such parameters\nare learned in the few-shot setting. By applying the module to three backbone\nsummarization models, experiments show that our method effectively improves all\nthe summarizers, and outperforms the prefix-based method and a widely used\nplug-and-play model in both zero- and few-shot settings. Tellingly, more\nbenefit is observed in the scenarios when more control is needed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wen Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miculicich_L/0/1/0/all/0/1\">Lesly Miculicich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1\">Giuseppe Carenini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Contrastive Finetuning Improves Low-Resource Relation Extraction. (arXiv:2212.10823v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10823","description":"<p>Relation extraction (RE), which has relied on structurally annotated corpora\nfor model training, has been particularly challenging in low-resource scenarios\nand domains. Recent literature has tackled low-resource RE by self-supervised\nlearning, where the solution involves pretraining the relation embedding by\nRE-based objective and finetuning on labeled data by classification-based\nobjective. However, a critical challenge to this approach is the gap in\nobjectives, which prevents the RE model from fully utilizing the knowledge in\npretrained representations. In this paper, we aim at bridging the gap and\npropose to pretrain and finetune the RE model using consistent objectives of\ncontrastive learning. Since in this kind of representation learning paradigm,\none relation may easily form multiple clusters in the representation space, we\nfurther propose a multi-center contrastive loss that allows one relation to\nform multiple clusters to better align with pretraining. Experiments on two\ndocument-level RE datasets, BioRED and Re-DocRED, demonstrate the effectiveness\nof our method. Particularly, when using 1% end-task training data, our method\noutperforms PLM-based RE classifier by 10.5% and 5.8% on the two datasets,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Automatic Speech Recognition model for the Sudanese Dialect. (arXiv:2212.10826v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10826","description":"<p>Designing a natural voice interface rely mostly on Speech recognition for\ninteraction between human and their modern digital life equipment. In addition,\nspeech recognition narrows the gap between monolingual individuals to better\nexchange communication. However, the field lacks wide support for several\nuniversal languages and their dialects, while most of the daily conversations\nare carried out using them. This paper comes to inspect the viability of\ndesigning an Automatic Speech Recognition model for the Sudanese dialect, which\nis one of the Arabic Language dialects, and its complexity is a product of\nhistorical and social conditions unique to its speakers. This condition is\nreflected in both the form and content of the dialect, so this paper gives an\noverview of the Sudanese dialect and the tasks of collecting represented\nresources and pre-processing performed to construct a modest dataset to\novercome the lack of annotated data. Also proposed end- to-end speech\nrecognition model, the design of the model was formed using Convolution Neural\nNetworks. The Sudanese dialect dataset would be a stepping stone to enable\nfuture Natural Language Processing research targeting the dialect. The designed\nmodel provided some insights into the current recognition task and reached an\naverage Label Error Rate of 73.67%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mansour_A/0/1/0/all/0/1\">Ayman Mansour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukhtar_W/0/1/0/all/0/1\">Wafaa F. Mukhtar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Multiple-Length Summaries via Reinforcement Learning for Unsupervised Sentence Summarization. (arXiv:2212.10843v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10843","description":"<p>Sentence summarization shortens given texts while maintaining core contents\nof the texts. Unsupervised approaches have been studied to summarize texts\nwithout human-written summaries. However, recent unsupervised models are\nextractive, which remove words from texts and thus they are less flexible than\nabstractive summarization. In this work, we devise an abstractive model based\non reinforcement learning without ground-truth summaries. We formulate the\nunsupervised summarization based on the Markov decision process with rewards\nrepresenting the summary quality. To further enhance the summary quality, we\ndevelop a multi-summary learning mechanism that generates multiple summaries\nwith varying lengths for a given text, while making the summaries mutually\nenhance each other. Experimental results show that the proposed model\nsubstantially outperforms both abstractive and extractive models, yet\nfrequently generating new words not contained in input texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hyun_D/0/1/0/all/0/1\">Dongmin Hyun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chanyoung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hwanjo Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-Augmented Linear Probing: Scaling Beyond The Limit of Few-shot In-Context Learners. (arXiv:2212.10873v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10873","description":"<p>Through in-context learning (ICL), large-scale language models are effective\nfew-shot learners without additional model fine-tuning. However, the ICL\nperformance does not scale well with the number of available training samples\nas it is limited by the inherent input length constraint of the underlying\nlanguage model. Meanwhile, many studies have revealed that language models are\nalso powerful feature extractors, allowing them to be utilized in a black-box\nmanner and enabling the linear probing paradigm, where lightweight\ndiscriminators are trained on top of the pre-extracted input representations.\nThis paper proposes prompt-augmented linear probing (PALP), a hybrid of linear\nprobing and ICL, which leverages the best of both worlds. PALP inherits the\nscalability of linear probing and the capability of enforcing language models\nto derive more meaningful representations via tailoring input into a more\nconceivable form. Throughout in-depth investigations on various datasets, we\nverified that PALP significantly enhances the input representations closing the\ngap between ICL in the data-hungry scenario and fine-tuning in the\ndata-abundant scenario with little training overhead, potentially making PALP a\nstrong alternative in a black-box scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyunsoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyuhng Joon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junyeob Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-goo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taeuk Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Linguistic Syntactic Difference in Multilingual BERT: How Good is It and How Does It Affect Transfer?. (arXiv:2212.10879v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10879","description":"<p>Multilingual BERT (mBERT) has demonstrated considerable cross-lingual\nsyntactic ability, whereby it enables effective zero-shot cross-lingual\ntransfer of syntactic knowledge. The transfer is more successful between some\nlanguages, but it is not well understood what leads to this variation and\nwhether it fairly reflects difference between languages. In this work, we\ninvestigate the distributions of grammatical relations induced from mBERT in\nthe context of 24 typologically different languages. We demonstrate that the\ndistance between the distributions of different languages is highly consistent\nwith the syntactic difference in terms of linguistic formalisms. Such\ndifference learnt via self-supervision plays a crucial role in the zero-shot\ntransfer performance and can be predicted by variation in morphosyntactic\nproperties between languages. These results suggest that mBERT properly encodes\nlanguages in a way consistent with linguistic diversity and provide insights\ninto the mechanism of cross-lingual transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Ningyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Ruotian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jingting Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Menghan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Mix-based Data Augmentation: Taxonomy, Methods, Applications, and Explainability. (arXiv:2212.10888v1 [cs.LG])","link":"http://arxiv.org/abs/2212.10888","description":"<p>Data augmentation (DA) is indispensable in modern machine learning and deep\nneural networks. The basic idea of DA is to construct new training data to\nimprove the model's generalization by adding slightly disturbed versions of\nexisting data or synthesizing new data. In this work, we review a small but\nessential subset of DA -- Mix-based Data Augmentation (MixDA) that generates\nnovel samples by mixing multiple examples. Unlike conventional DA approaches\nbased on a single-sample operation or requiring domain knowledge, MixDA is more\ngeneral in creating a broad spectrum of new data and has received increasing\nattention in the community. We begin with proposing a new taxonomy classifying\nMixDA into, Mixup-based, Cutmix-based, and hybrid approaches according to a\nhierarchical view of the data mix. Various MixDA techniques are then\ncomprehensively reviewed in a more fine-grained way. Owing to its\ngeneralization, MixDA has penetrated a variety of applications which are also\ncompletely reviewed in this work. We also examine why MixDA works from\ndifferent aspects of improving model performance, generalization, and\ncalibration while explaining the model behavior based on the properties of\nMixDA. Finally, we recapitulate the critical findings and fundamental\nchallenges of current MixDA studies, and outline the potential directions for\nfuture works. Different from previous related works that summarize the DA\napproaches in a specific domain (e.g., images or natural language processing)\nor only review a part of MixDA studies, we are the first to provide a\nsystematical survey of MixDA in terms of its taxonomy, methodology,\napplications, and explainability. This work can serve as a roadmap to MixDA\ntechniques and application reviews while providing promising directions for\nresearchers interested in this exciting area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Chengtai Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Fan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yurou Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training language models for deeper understanding improves brain alignment. (arXiv:2212.10898v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10898","description":"<p>Building systems that achieve a deeper understanding of language is one of\nthe central goals of natural language processing (NLP). Towards this goal,\nrecent works have begun to train language models on narrative datasets which\nrequire extracting the most critical information by integrating across long\ncontexts. However, it is still an open question whether these models are\nlearning a deeper understanding of the text, or if the models are simply\nlearning a heuristic to complete the task. This work investigates this further\nby turning to the one language processing system that truly understands complex\nlanguage: the human brain. We show that training language models for deeper\nnarrative understanding results in richer representations that have improved\nalignment to human brain activity. We further find that the improvements in\nbrain alignment are larger for character names than for other discourse\nfeatures, which indicates that these models are learning important narrative\nelements. Taken together, these results suggest that this type of training can\nindeed lead to deeper language understanding. These findings have consequences\nboth for cognitive neuroscience by revealing some of the significant factors\nbehind brain-NLP alignment, and for NLP by highlighting that understanding of\nlong-range context can be improved beyond language modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aw_K/0/1/0/all/0/1\">Khai Loong Aw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toneva_M/0/1/0/all/0/1\">Mariya Toneva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RECAP: Retrieval Augmented Music Captioner. (arXiv:2212.10901v1 [cs.SD])","link":"http://arxiv.org/abs/2212.10901","description":"<p>With the prevalence of stream media platforms serving music search and\nrecommendation, interpreting music by understanding audio and lyrics\ninteractively has become an important and challenging task. However, many\nprevious works focus on refining individual components of encoder-decoder\narchitecture mapping music to caption tokens, ignoring the potential usage of\naudio and lyrics correspondence. In this paper, we propose to explicitly learn\nthe multi-modal alignment with retrieval augmentation by contrastive learning.\nBy learning audio-lyrics correspondence, the model is guided to learn better\ncross-modal attention weights, thus generating high-quality caption words. We\nprovide both theoretical and empirical results that demonstrate the advantage\nof the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zihao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1\">Weituo Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xuchen Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models as Inductive Reasoners. (arXiv:2212.10923v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10923","description":"<p>Inductive reasoning is a core component of human intelligence. In the past\nresearch of inductive reasoning within computer science, logic language is used\nas representations of knowledge (facts and rules, more specifically). However,\nlogic language can cause systematic problems for inductive reasoning such as\ndisability of handling raw input such as natural language, sensitiveness to\nmislabeled data, and incapacity to handle ambiguous input. To this end, we\npropose a new task, which is to induce natural language rules from natural\nlanguage facts, and create a dataset termed DEER containing 1.2k rule-fact\npairs for the task, where rules and facts are written in natural language. New\nautomatic metrics are also proposed and analysed for the evaluation of this\ntask. With DEER, we investigate a modern approach for inductive reasoning where\nwe use natural language as representation for knowledge instead of logic\nlanguage and use pretrained language models as ''reasoners''. Moreover, we\nprovide the first and comprehensive analysis of how well pretrained language\nmodels can induce natural language rules from natural language facts. We also\npropose a new framework drawing insights from philosophy literature for this\ntask, which we show in the experiment section that surpasses baselines in both\nautomatic and human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zonglin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xinya Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPT: Semi-Parametric Prompt Tuning for Multitask Prompted Learning. (arXiv:2212.10929v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10929","description":"<p>Pre-trained large language models can efficiently interpolate human-written\nprompts in a natural way. Multitask prompted learning can help generalization\nthrough a diverse set of tasks at once, thus enhancing the potential for more\neffective downstream fine-tuning. To perform efficient multitask-inference in\nthe same batch, parameter-efficient fine-tuning methods such as prompt tuning\nhave been proposed. However, the existing prompt tuning methods may lack\ngeneralization. We propose SPT, a semi-parametric prompt tuning method for\nmultitask prompted learning. The novel component of SPT is a memory bank from\nwhere memory prompts are retrieved based on discrete prompts. Extensive\nexperiments, such as (i) fine-tuning a full language model with SPT on 31\ndifferent tasks from 8 different domains and evaluating zero-shot\ngeneralization on 9 heldout datasets under 5 NLP task categories and (ii)\npretraining SPT on the GLUE datasets and evaluating fine-tuning on the\nSuperGLUE datasets, demonstrate effectiveness of SPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1\">M Saiful Bari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xingjian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resolving Indirect Referring Expressions for Entity Selection. (arXiv:2212.10933v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10933","description":"<p>Recent advances in language modeling have enabled new conversational systems.\nIn particular, it is often desirable for people to make choices among specified\noptions when using such systems. We address the problem of reference\nresolution, when people use natural expressions to choose between real world\nentities. For example, given the choice `Should we make a Simnel cake or a\nPandan cake?' a natural response from a non-expert may be indirect: `let's make\nthe green one'. Reference resolution has been little studied with natural\nexpressions, thus robustly understanding such language has large potential for\nimproving naturalness in dialog, recommendation, and search systems. We create\nAltEntities (Alternative Entities), a new public dataset of entity pairs and\nutterances, and develop models for the disambiguation problem. Consisting of\n42K indirect referring expressions across three domains, it enables for the\nfirst time the study of how large language models can be adapted to this task.\nWe find they achieve 82%-87% accuracy in realistic settings, which while\nreasonable also invites further advances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1\">Mohammad Javad Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radlinski_F/0/1/0/all/0/1\">Filip Radlinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pareti_S/0/1/0/all/0/1\">Silvia Pareti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Louis_A/0/1/0/all/0/1\">Annie Louis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Esports Data-to-commentary Generation on Large-scale Data-to-text Dataset. (arXiv:2212.10935v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10935","description":"<p>Esports, a sports competition using video games, has become one of the most\nimportant sporting events in recent years. Although the amount of esports data\nis increasing than ever, only a small fraction of those data accompanies text\ncommentaries for the audience to retrieve and understand the plays. Therefore,\nin this study, we introduce a task of generating game commentaries from\nstructured data records to address the problem. We first build a large-scale\nesports data-to-text dataset using structured data and commentaries from a\npopular esports game, League of Legends. On this dataset, we devise several\ndata preprocessing methods including linearization and data splitting to\naugment its quality. We then introduce several baseline encoder-decoder models\nand propose a hierarchical model to generate game commentaries. Considering the\ncharacteristics of esports commentaries, we design evaluation metrics including\nthree aspects of the output: correctness, fluency, and strategic depth.\nExperimental results on our large-scale esports dataset confirmed the advantage\nof the hierarchical model, and the results revealed several challenges of this\nnovel task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshinaga_N/0/1/0/all/0/1\">Naoki Yoshinaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Critic-Guided Decoding for Controlled Text Generation. (arXiv:2212.10938v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10938","description":"<p>Steering language generation towards objectives or away from undesired\ncontent has been a long-standing goal in utilizing language models (LM). Recent\nwork has demonstrated reinforcement learning and weighted decoding as effective\napproaches to achieve a higher level of language control and quality with pros\nand cons. In this work, we propose a novel critic decoding method for\ncontrolled language generation (CriticControl) that combines the strengths of\nreinforcement learning and weighted decoding. Specifically, we adopt the\nactor-critic framework to train an LM-steering critic from non-differentiable\nreward models. And similar to weighted decoding, our method freezes the\nlanguage model and manipulates the output token distribution using called\ncritic, improving training efficiency and stability. Evaluation of our method\non three controlled generation tasks, namely topic control, sentiment control,\nand detoxification, shows that our approach generates more coherent and\nwell-controlled texts than previous methods. In addition, CriticControl\ndemonstrates superior generalization ability in zero-shot settings. Human\nevaluation studies also corroborate our findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minbeom Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwanhee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Joonsuk Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwaran Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1\">Kyomin Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallel Context Windows Improve In-Context Learning of Large Language Models. (arXiv:2212.10947v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10947","description":"<p>For applications that require processing large amounts of text at inference\ntime, Large Language Models (LLMs) are handicapped by their limited context\nwindows, which are typically 2048 tokens. In-context learning, an emergent\nphenomenon in LLMs in sizes above a certain parameter threshold, constitutes\none significant example because it can only leverage training examples that fit\ninto the context window. Existing efforts to address the context window\nlimitation involve training specialized architectures, which tend to be smaller\nthan the sizes in which in-context learning manifests due to the memory\nfootprint of processing long texts. We present Parallel Context Windows (PCW),\na method that alleviates the context window restriction for any off-the-shelf\nLLM without further training. The key to the approach is to carve a long\ncontext into chunks (``windows'') that fit within the architecture, restrict\nthe attention mechanism to apply only within each window, and re-use the\npositional embeddings among the windows. We test the PCW approach on in-context\nlearning with models that range in size between 750 million and 178 billion\nparameters, and show substantial improvements for tasks with diverse input and\noutput spaces. Our results motivate further investigation of Parallel Context\nWindows as a method for applying off-the-shelf LLMs in other settings that\nrequire long text sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ratner_N/0/1/0/all/0/1\">Nir Ratner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1\">Yoav Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ram_O/0/1/0/all/0/1\">Ori Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpas_E/0/1/0/all/0/1\">Ehud Karpas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leyton_Brown_K/0/1/0/all/0/1\">Kevin Leyton-Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoham_Y/0/1/0/all/0/1\">Yoav Shoham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computer says \"No\": The Case Against Empathetic Conversational AI. (arXiv:2212.10983v1 [cs.CL])","link":"http://arxiv.org/abs/2212.10983","description":"<p>Emotions are an integral part of human cognition and they guide not only our\nunderstanding of the world but also our actions within it. As such, whether we\nsoothe or flame an emotion is not inconsequential. Recent work in\nconversational AI has focused on responding empathetically to users, validating\nand soothing their emotions without a real basis. This AI-aided emotional\nregulation can have negative consequences for users and society, tending\ntowards a one-noted happiness defined as only the absence of \"negative\"\nemotions. We argue that we must carefully consider whether and how to respond\nto users' emotions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Curry_A/0/1/0/all/0/1\">Alba Curry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curry_A/0/1/0/all/0/1\">Amanda Cercas Curry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAViC: Multimodal Active Learning for Video Captioning. (arXiv:2212.11109v1 [cs.CV])","link":"http://arxiv.org/abs/2212.11109","description":"<p>A large number of annotated video-caption pairs are required for training\nvideo captioning models, resulting in high annotation costs. Active learning\ncan be instrumental in reducing these annotation requirements. However, active\nlearning for video captioning is challenging because multiple semantically\nsimilar captions are valid for a video, resulting in high entropy outputs even\nfor less-informative samples. Moreover, video captioning algorithms are\nmultimodal in nature with a visual encoder and language decoder. Further, the\nsequential and combinatorial nature of the output makes the problem even more\nchallenging. In this paper, we introduce MAViC which leverages our proposed\nMultimodal Semantics Aware Sequential Entropy (M-SASE) based acquisition\nfunction to address the challenges of active learning approaches for video\ncaptioning. Our approach integrates semantic similarity and uncertainty of both\nvisual and language dimensions in the acquisition function. Our detailed\nexperiments empirically demonstrate the efficacy of M-SASE for active learning\nfor video captioning and improve on the baselines by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_G/0/1/0/all/0/1\">Gyanendra Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_X/0/1/0/all/0/1\">Xavier Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_A/0/1/0/all/0/1\">Anant Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vikram Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A survey on text generation using generative adversarial networks. (arXiv:2212.11119v1 [cs.CL])","link":"http://arxiv.org/abs/2212.11119","description":"<p>This work presents a thorough review concerning recent studies and text\ngeneration advancements using Generative Adversarial Networks. The usage of\nadversarial learning for text generation is promising as it provides\nalternatives to generate the so-called \"natural\" language. Nevertheless,\nadversarial text generation is not a simple task as its foremost architecture,\nthe Generative Adversarial Networks, were designed to cope with continuous\ninformation (image) instead of discrete data (text). Thus, most works are based\non three possible options, i.e., Gumbel-Softmax differentiation, Reinforcement\nLearning, and modified training objectives. All alternatives are reviewed in\nthis survey as they present the most recent approaches for generating text\nusing adversarial-based techniques. The selected works were taken from renowned\ndatabases, such as Science Direct, IEEEXplore, Springer, Association for\nComputing Machinery, and arXiv, whereas each selected work has been critically\nanalyzed and assessed to present its objective, methodology, and experimental\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosa_G/0/1/0/all/0/1\">Gustavo Henrique de Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Papa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Religion and Spirituality on Social Media in the Aftermath of the Global Pandemic. (arXiv:2212.11121v1 [cs.CY])","link":"http://arxiv.org/abs/2212.11121","description":"<p>During the COVID-19 pandemic, the Church closed its physical doors for the\nfirst time in about 800 years, which is, arguably, a cataclysmic event. Other\nreligions have found themselves in a similar situation, and they were\npractically forced to move online, which is an unprecedented occasion. In this\npaper, we analyse this sudden change in religious activities twofold: we create\nand deliver a questionnaire, as well as analyse Twitter data, to understand\npeople's perceptions and activities related to religious activities online.\nImportantly, we also analyse the temporal variations in this process by\nanalysing a period of 3 months: July-September 2020. Additionally to the\nseparate analysis of the two data sources, we also discuss the implications\nfrom triangulating the results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aduragba_O/0/1/0/all/0/1\">Olanrewaju Tahir Aduragba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristea_A/0/1/0/all/0/1\">Alexandra I. Cristea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_P/0/1/0/all/0/1\">Pete Phillips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurlberg_J/0/1/0/all/0/1\">Jonas Kurlberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jialin Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chatbots in a Botnet World. (arXiv:2212.11126v1 [cs.CR])","link":"http://arxiv.org/abs/2212.11126","description":"<p>Question-and-answer formats provide a novel experimental platform for\ninvestigating cybersecurity questions. Unlike previous chatbots, the latest\nChatGPT model from OpenAI supports an advanced understanding of complex coding\nquestions. The research demonstrates thirteen coding tasks that generally\nqualify as stages in the MITRE ATT&amp;CK framework, ranging from credential access\nto defense evasion. With varying success, the experimental prompts generate\nexamples of keyloggers, logic bombs, obfuscated worms, and payment-fulfilled\nransomware. The empirical results illustrate cases that support the broad gain\nof functionality, including self-replication and self-modification, evasion,\nand strategic understanding of complex cybersecurity goals. One surprising\nfeature of ChatGPT as a language-only model centers on its ability to spawn\ncoding approaches that yield images that obfuscate or embed executable\nprogramming steps or links.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McKee_F/0/1/0/all/0/1\">Forrest McKee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1\">David Noever</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal versus system-specific features of punctuation usage patterns in~major Western~languages. (arXiv:2212.11182v1 [cs.CL])","link":"http://arxiv.org/abs/2212.11182","description":"<p>The celebrated proverb that \"speech is silver, silence is golden\" has a long\nmultinational history and multiple specific meanings. In written texts\npunctuation can in fact be considered one of its manifestations. Indeed, the\nvirtue of effectively speaking and writing involves - often decisively - the\ncapacity to apply the properly placed breaks. In the present study, based on a\nlarge corpus of world-famous and representative literary texts in seven major\nWestern languages, it is shown that the distribution of intervals between\nconsecutive punctuation marks in almost all texts can universally be\ncharacterised by only two parameters of the discrete Weibull distribution which\ncan be given an intuitive interpretation in terms of the so-called hazard\nfunction. The values of these two parameters tend to be language-specific,\nhowever, and even appear to navigate translations. The properties of the\ncomputed hazard functions indicate that among the studied languages, English\nturns out to be the least constrained by the necessity to place a consecutive\npunctuation mark to partition a sequence of words. This may suggest that when\ncompared to other studied languages, English is more flexible, in the sense of\nallowing longer uninterrupted sequences of words. Spanish reveals similar\ntendency to only a bit lesser extent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stanisz_T/0/1/0/all/0/1\">Tomasz Stanisz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdz_S/0/1/0/all/0/1\">Stanislaw Drozdz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwapien_J/0/1/0/all/0/1\">Jaroslaw Kwapien</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entropy- and Distance-Based Predictors From GPT-2 Attention Patterns Predict Reading Times Over and Above GPT-2 Surprisal. (arXiv:2212.11185v1 [cs.CL])","link":"http://arxiv.org/abs/2212.11185","description":"<p>Transformer-based large language models are trained to make predictions about\nthe next word by aggregating representations of previous tokens through their\nself-attention mechanism. In the field of cognitive modeling, such attention\npatterns have recently been interpreted as embodying the process of cue-based\nretrieval, in which attention over multiple targets is taken to generate\ninterference and latency during retrieval. Under this framework, this work\nfirst defines an entropy-based predictor that quantifies the diffuseness of\nself-attention, as well as distance-based predictors that capture the\nincremental change in attention patterns across timesteps. Moreover, following\nrecent studies that question the informativeness of attention weights, we also\nexperiment with alternative methods for incorporating vector norms into\nattention weights. Regression experiments using predictors calculated from the\nGPT-2 language model show that these predictors deliver a substantially better\nfit to held-out self-paced reading and eye-tracking data over a rigorous\nbaseline including GPT-2 surprisal. Additionally, the distance-based predictors\ngenerally demonstrated higher predictive power, with effect sizes of up to 6.59\nms per standard deviation on self-paced reading times (compared to 2.82 ms for\nsurprisal) and 1.05 ms per standard deviation on eye-gaze durations (compared\nto 3.81 ms for surprisal).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oh_B/0/1/0/all/0/1\">Byung-Doh Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuler_W/0/1/0/all/0/1\">William Schuler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Safe and Usable Chatbots for Promoting Voter Participation. (arXiv:2212.11219v1 [cs.HC])","link":"http://arxiv.org/abs/2212.11219","description":"<p>Chatbots, or bots for short, are multi-modal collaborative assistants that\ncan help people complete useful tasks. Usually, when chatbots are referenced in\nconnection with elections, they often draw negative reactions due to the fear\nof mis-information and hacking. Instead, in this paper, we explore how chatbots\nmay be used to promote voter participation in vulnerable segments of society\nlike senior citizens and first-time voters. In particular, we build a system\nthat amplifies official information while personalizing it to users' unique\nneeds transparently. We discuss its design, build prototypes with frequently\nasked questions (FAQ) election information for two US states that are low on an\nease-of-voting scale, and report on its initial evaluation in a focus group.\nOur approach can be a win-win for voters, election agencies trying to fulfill\ntheir mandate and democracy at large.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muppasani_B/0/1/0/all/0/1\">Bharath Muppasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pallagani_V/0/1/0/all/0/1\">Vishal Pallagani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_K/0/1/0/all/0/1\">Kausik Lakkaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Shuge Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_B/0/1/0/all/0/1\">Biplav Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robertson_B/0/1/0/all/0/1\">Brett Robertson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hickerson_A/0/1/0/all/0/1\">Andrea Hickerson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_V/0/1/0/all/0/1\">Vignesh Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Narrative Relationship Embeddings by Training with Additional Inverse-Relationship Constraints. (arXiv:2212.11234v1 [cs.CL])","link":"http://arxiv.org/abs/2212.11234","description":"<p>We consider the problem of embedding character-entity relationships from the\nreduced semantic space of narratives, proposing and evaluating the assumption\nthat these relationships hold under a reflection operation. We analyze this\nassumption and compare the approach to a baseline state-of-the-art model with a\nunique evaluation that simulates efficacy on a downstream clustering task with\nhuman-created labels. Although our model creates clusters that achieve\nSilhouette scores of -.084, outperforming the baseline -.227, our analysis\nreveals that the models approach the task much differently and perform well on\nvery different examples. We conclude that our assumption might be useful for\nspecific types of data and should be evaluated on a wider range of tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Figurski_M/0/1/0/all/0/1\">Mikolaj Figurski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias. (arXiv:2212.11261v1 [cs.CY])","link":"http://arxiv.org/abs/2212.11261","description":"<p>Nine language-vision AI models trained on web scrapes with the Contrastive\nLanguage-Image Pretraining (CLIP) objective are evaluated for evidence of a\nbias studied by psychologists: the sexual objectification of girls and women,\nwhich occurs when a person's human characteristics are disregarded and the\nperson is treated as a body or a collection of body parts. A first experiment\nuses standardized images of women from the Sexual OBjectification and EMotion\nDatabase, and finds that, commensurate with prior research in psychology, human\ncharacteristics are disassociated from images of objectified women: the model's\nrecognition of emotional state is mediated by whether the subject is fully or\npartially clothed. Embedding association tests (EATs) return significant effect\nsizes for both anger (d &gt;.8) and sadness (d &gt;.5). A second experiment measures\nthe effect in a representative application: an automatic image captioner\n(Antarctic Captions) includes words denoting emotion less than 50% as often for\nimages of partially clothed women than for images of fully clothed women. A\nthird experiment finds that images of female professionals (scientists,\ndoctors, executives) are likely to be associated with sexual descriptions\nrelative to images of male professionals. A fourth experiment shows that a\nprompt of \"a [age] year old girl\" generates sexualized images (as determined by\nan NSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17), and up to\n40% of the time for Stable Diffusion (ages 14 and 18); the corresponding rate\nfor boys never surpasses 9%. The evidence indicates that language-vision AI\nmodels trained on automatically collected web scrapes learn biases of sexual\nobjectification, which propagate to downstream applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_R/0/1/0/all/0/1\">Robert Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howe_B/0/1/0/all/0/1\">Bill Howe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caliskan_A/0/1/0/all/0/1\">Aylin Caliskan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Decoding for Pixel, Image, and Language. (arXiv:2212.11270v1 [cs.CV])","link":"http://arxiv.org/abs/2212.11270","description":"<p>We present X-Decoder, a generalized decoding model that can predict\npixel-level segmentation and language tokens seamlessly. X-Decodert takes as\ninput two types of queries: (i) generic non-semantic queries and (ii) semantic\nqueries induced from text inputs, to decode different pixel-level and\ntoken-level outputs in the same semantic space. With such a novel design,\nX-Decoder is the first work that provides a unified way to support all types of\nimage segmentation and a variety of vision-language (VL) tasks. Further, our\ndesign enables seamless interactions across tasks at different granularities\nand brings mutual benefits by learning a common and rich pixel-level\nvisual-semantic understanding space, without any pseudo-labeling. After\npretraining on a mixed set of a limited amount of segmentation data and\nmillions of image-text pairs, X-Decoder exhibits strong transferability to a\nwide range of downstream tasks in both zero-shot and finetuning settings.\nNotably, it achieves (1) state-of-the-art results on open-vocabulary\nsegmentation and referring segmentation on eight datasets; (2) better or\ncompetitive finetuned performance to other generalist and specialist models on\nsegmentation and VL tasks; and (3) flexibility for efficient finetuning and\nnovel task composition (e.g., referring captioning and image editing). Code,\ndemo, video, and visualization are available at https://x-decoder-vl.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1\">Xueyan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zi-Yi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behl_H/0/1/0/all/0/1\">Harkirat Behl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong Jae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Named Tensor Notation. (arXiv:2102.13196v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.13196","description":"<p>We propose a notation for tensors with named axes, which relieves the author,\nreader, and future implementers of machine learning models from the burden of\nkeeping track of the order of axes and the purpose of each. The notation makes\nit easy to lift operations on low-order tensors to higher order ones, for\nexample, from images to minibatches of images, or from an attention mechanism\nto multiple attention heads.\n</p>\n<p>After a brief overview and formal definition of the notation, we illustrate\nit through several examples from modern machine learning, from building blocks\nlike attention and convolution to full models like Transformers and LeNet. We\nthen discuss differential calculus in our notation and compare with some\nalternative notations. Our proposals build on ideas from many previous papers\nand software libraries. We hope that our notation will encourage more authors\nto use named tensors, resulting in clearer papers and more precise\nimplementations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barak_B/0/1/0/all/0/1\">Boaz Barak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual-Lexicon Approach for Abusive Language Detection. (arXiv:2104.12265v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.12265","description":"<p>Since a lexicon-based approach is more elegant scientifically, explaining the\nsolution components and being easier to generalize to other applications, this\npaper provides a new approach for offensive language and hate speech detection\non social media. Our approach embodies a lexicon of implicit and explicit\noffensive and swearing expressions annotated with contextual information. Due\nto the severity of the social media abusive comments in Brazil, and the lack of\nresearch in Portuguese, Brazilian Portuguese is the language used to validate\nthe models. Nevertheless, our method may be applied to any other language. The\nconducted experiments show the effectiveness of the proposed approach,\noutperforming the current baseline methods for the Portuguese language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vargas_F/0/1/0/all/0/1\">Francielle Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goes_F/0/1/0/all/0/1\">Fabiana Rodrigues de G&#xf3;es</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_I/0/1/0/all/0/1\">Isabelle Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benevenuto_F/0/1/0/all/0/1\">Fabr&#xed;cio Benevenuto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pardo_T/0/1/0/all/0/1\">Thiago Alexandre Salgueiro Pardo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Active Learning for Short Text Classification in User-Generated Data. (arXiv:2112.02611v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.02611","description":"<p>Mining user-generated data often suffers from the lack of enough labeled\ndata, short document lengths, and the informal user language. In this paper, we\npropose a novel active learning model to overcome these obstacles in the tasks\ntailored for query phrases--e.g., detecting positive reports of natural\ndisasters. Our model has three novelties: 1) It is the first approach to employ\nmulti-view active learning in this domain. 2) It uses the Parzen-Rosenblatt\nwindow method to integrate the representativeness measure into multi-view\nactive learning. 3) It employs a query-by-committee strategy, based on the\nagreement between predictors, to address the usually noisy language of the\ndocuments in this domain. We evaluate our model in four publicly available\nTwitter datasets with distinctly different applications. We also compare our\nmodel with a wide range of baselines including those with multiple classifiers.\nThe experiments testify that our model is highly consistent and outperforms\nexisting models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karisani_P/0/1/0/all/0/1\">Payam Karisani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karisani_N/0/1/0/all/0/1\">Negin Karisani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Li Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-view Graph Neural Networks for Knowledge Graph Completion. (arXiv:2112.09231v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.09231","description":"<p>We present an effective graph neural network (GNN)-based knowledge graph\nembedding model, which we name WGE, to capture entity- and relation-focused\ngraph structures. Given a knowledge graph, WGE builds a single undirected\nentity-focused graph that views entities as nodes. WGE also constructs another\nsingle undirected graph from relation-focused constraints, which views entities\nand relations as nodes. WGE then proposes a GNN-based architecture to better\nlearn vector representations of entities and relations from these two single\nentity- and relation-focused graphs. WGE feeds the learned entity and relation\nrepresentations into a weighted score function to return the triple scores for\nknowledge graph completion. Experimental results show that WGE outperforms\ncompetitive baselines, obtaining state-of-the-art performances on seven\nbenchmark datasets for knowledge graph completion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tong_V/0/1/0/all/0/1\">Vinh Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dai Quoc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat Quoc Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does human speech follow Benford's Law?. (arXiv:2203.13352v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.13352","description":"<p>Researchers have observed that the frequencies of leading digits in many\nman-made and naturally occurring datasets follow a logarithmic curve, with\ndigits that start with the number 1 accounting for $\\sim 30\\%$ of all numbers\nin the dataset and digits that start with the number 9 accounting for $\\sim\n5\\%$ of all numbers in the dataset. This phenomenon, known as Benford's Law, is\nhighly repeatable and appears in lists of numbers from electricity bills, stock\nprices, tax returns, house prices, death rates, lengths of rivers, and\nnaturally occurring images. In this paper we demonstrate that human speech\nspectra also follow Benford's Law on average. That is, when averaged over many\nspeakers, the frequencies of leading digits in speech magnitude spectra follow\nthis distribution, although with some variability at the individual sample\nlevel. We use this observation to motivate a new set of features that can be\nefficiently extracted from speech and demonstrate that these features can be\nused to classify between human speech and synthetic speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_L/0/1/0/all/0/1\">Leo Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berisha_V/0/1/0/all/0/1\">Visar Berisha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Hate Speech Detection from Bengali Memes and Texts. (arXiv:2204.10196v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10196","description":"<p>Numerous machine learning (ML) and deep learning (DL)-based approaches have\nbeen proposed to utilize textual data from social media for anti-social\nbehavior analysis like cyberbullying, fake news detection, and identification\nof hate speech mainly for highly-resourced languages such as English. However,\ndespite having a lot of diversity and millions of native speakers, some\nlanguages like Bengali are under-resourced, which is due to a lack of\ncomputational resources for natural language processing (NLP). Similar to other\nlanguages, Bengali social media contents also include images along with texts\n(e.g., multimodal memes are posted by embedding short texts into images on\nFacebook). Therefore, only the textual data is not enough to judge them since\nimages might give extra context to make a proper judgement. This paper is about\nhate speech detection from multimodal Bengali memes and texts. We prepared the\nonly multimodal hate speech dataset for-a-kind of problem for Bengali, which we\nuse to train state-of-the-art neural architectures (e.g., Bi-LSTM/Conv-LSTM\nwith word embeddings, ConvNets + pre-trained language models, e.g., monolingual\nBangla BERT, multilingual BERT-cased/uncased, and XLM-RoBERTa) to jointly\nanalyze textual and visual information for hate speech detection. Conv-LSTM and\nXLM-RoBERTa models performed best for texts, yielding F1 scores of 0.78 and\n0.82, respectively. As of memes, ResNet-152 and DenseNet-161 models yield F1\nscores of 0.78 and 0.79, respectively. As for multimodal fusion, XLM-RoBERTa +\nDenseNet-161 performed the best, yielding an F1 score of 0.83. Our study\nsuggests that text modality is most useful for hate speech detection, while\nmemes are moderately useful.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Md. Rezaul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1\">Sumon Kanti Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tanhim Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shajalal_M/0/1/0/all/0/1\">Md. Shajalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing Test-Time Query Representations for Dense Retrieval. (arXiv:2205.12680v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12680","description":"<p>Recent developments of dense retrieval rely on quality representations of\nqueries and contexts coming from pre-trained query and context encoders. In\nthis paper, we introduce TouR (test-time optimization of query\nrepresentations), which further optimizes instance-level query representations\nguided by signals from test-time retrieval results. We leverage a cross-encoder\nre-ranker to provide fine-grained pseudo labels over retrieval results and\niteratively optimize query representations with the gradient descent method.\nOur theoretical analysis reveals that TouR can be viewed as a generalization of\nthe classical Rocchio's algorithm for pseudo relevance feedback, and we present\ntwo variants leveraging psuedo labels as either hard binary or soft continuous\nlabels. We first apply TouR on phrase retrieval with our proposed phrase\nre-ranker. On passage retrieval, we demonstrate its effectiveness with an\noff-the-shelf re-ranker. TouR improves the end-to-end open-domain QA accuracy\nsignificantly, as well as passage retrieval performance. Compared to re-ranker,\nTouR requires a smaller number of candidates, and achieves consistently better\nperformance and runs up to 4x faster with our efficient implementation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1\">Mujeen Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jungsoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinhyuk Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can large language models reason about medical questions?. (arXiv:2207.08143v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.08143","description":"<p>Although large language models (LLMs) often produce impressive outputs, it\nremains unclear how they perform in real-world scenarios requiring strong\nreasoning skills and expert domain knowledge. We set out to investigate whether\nGPT-3.5 (Codex and InstructGPT) can be applied to answer and reason about\ndifficult real-world-based questions. We utilize two multiple-choice medical\nexam questions (USMLE and MedMCQA) and a medical reading comprehension dataset\n(PubMedQA). We investigate multiple prompting scenarios: Chain-of-Thought (CoT,\nthink step-by-step), zero- and few-shot (prepending the question with\nquestion-answer exemplars) and retrieval augmentation (injecting Wikipedia\npassages into the prompt). For a subset of the USMLE questions, a medical\nexpert reviewed and annotated the model's CoT. We found that InstructGPT can\noften read, reason and recall expert knowledge. Failure are primarily due to\nlack of knowledge and reasoning errors and trivial guessing heuristics are\nobserved, e.g.\\ too often predicting labels A and D on USMLE. Sampling and\ncombining many completions overcome some of these limitations. Using 100\nsamples, Codex 5-shot CoT not only gives close to well-calibrated predictive\nprobability but also achieves human-level performances on the three datasets.\nUSMLE: 60.2%, MedMCQA: 57.5% and PubMedQA: 78.2%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lievin_V/0/1/0/all/0/1\">Valentin Li&#xe9;vin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hother_C/0/1/0/all/0/1\">Christoffer Egeberg Hother</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1\">Ole Winther</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Your Model Sensitive? SPeDaC: A New Benchmark for Detecting and Classifying Sensitive Personal Data. (arXiv:2208.06216v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.06216","description":"<p>In recent years, there has been an exponential growth of applications,\nincluding dialogue systems, that handle sensitive personal information. This\nhas brought to light the extremely important issue of personal data protection\nin virtual environments. Sensitive Information Detection (SID) approaches\ndifferent domains and languages in literature. However, if we refer to the\npersonal data domain, a shared benchmark or the absence of an available labeled\nresource makes comparison with the state-of-the-art difficult. We introduce and\nrelease SPeDaC , a new annotated resource for the identification of sensitive\npersonal data categories in the English language. SPeDaC enables the evaluation\nof computational models for three different SID subtasks with increasing levels\nof complexity. SPeDaC 1 regards binary classification, a model has to detect if\na sentence contains sensitive information or not; whereas, in SPeDaC 2 we\ncollected labeled sentences using 5 categories that relate to macro-domains of\npersonal information; in SPeDaC 3, the labeling is fine-grained (61 personal\ndata categories). We conduct an extensive evaluation of the resource using\ndifferent state-of-the-art-classifiers. The results show that SPeDaC is\nchallenging, particularly with regard to fine-grained classification. The\ntransformer models achieve the best results (acc. RoBERTa on SPeDaC 1 = 98.20%,\nDeBERTa on SPeDaC 2 = 95.81% and SPeDaC 3 = 77.63%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gambarelli_G/0/1/0/all/0/1\">Gaia Gambarelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangemi_A/0/1/0/all/0/1\">Aldo Gangemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripodi_R/0/1/0/all/0/1\">Rocco Tripodi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In conversation with Artificial Intelligence: aligning language models with human values. (arXiv:2209.00731v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2209.00731","description":"<p>Large-scale language technologies are increasingly used in various forms of\ncommunication with humans across different contexts. One particular use case\nfor these technologies is conversational agents, which output natural language\ntext in response to prompts and queries. This mode of engagement raises a\nnumber of social and ethical questions. For example, what does it mean to align\nconversational agents with human norms or values? Which norms or values should\nthey be aligned with? And how can this be accomplished? In this paper, we\npropose a number of steps that help answer these questions. We start by\ndeveloping a philosophical analysis of the building blocks of linguistic\ncommunication between conversational agents and human interlocutors. We then\nuse this analysis to identify and formulate ideal norms of conversation that\ncan govern successful linguistic communication between humans and\nconversational agents. Furthermore, we explore how these norms can be used to\nalign conversational agents with human values across a range of different\ndiscursive domains. We conclude by discussing the practical implications of our\nproposal for the design of conversational agents that are aligned with these\nnorms and values.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasirzadeh_A/0/1/0/all/0/1\">Atoosa Kasirzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabriel_I/0/1/0/all/0/1\">Iason Gabriel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GROOT: Corrective Reward Optimization for Generative Sequential Labeling. (arXiv:2209.14694v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.14694","description":"<p>Sequential labeling is a fundamental NLP task, forming the backbone of many\napplications. Supervised learning of Seq2Seq models has shown great success on\nthese problems. However, the training objectives are still significantly\ndisconnected with the metrics and desiderata we care about in practice. For\nexample, a practical sequence tagging application may want to optimize for a\ncertain precision-recall trade-off (of the top-k predictions) which is quite\ndifferent from the standard objective of maximizing the likelihood of the gold\nlabeled sequence. Thus to bridge this gap, we propose GROOT -- a simple yet\neffective framework for Generative Reward Optimization Of Text sequences. GROOT\nworks by training a generative sequential labeling model to match the decoder\noutput distribution with that of the (black-box) reward function. Using an\niterative training regime, we first generate prediction candidates, then\ncorrect errors in them, and finally contrast those candidates (based on their\nreward values). As demonstrated via extensive experiments on four public\nbenchmarks, GROOT significantly improves all reward metrics. Furthermore, GROOT\nleads to improvements of the overall decoder distribution as evidenced by the\nquality gains of the top-$k$ candidates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kazuma Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_K/0/1/0/all/0/1\">Karthik Raman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation. (arXiv:2211.05719v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05719","description":"<p>Responding with multi-modal content has been recognized as an essential\ncapability for an intelligent conversational agent. In this paper, we introduce\nthe MMDialog dataset to better facilitate multi-modal conversation. MMDialog is\ncomposed of a curated set of 1.08 million real-world dialogues with 1.53\nmillion unique images across 4,184 topics. MMDialog has two main and unique\nadvantages. First, it is the largest multi-modal conversation dataset by the\nnumber of dialogues by 88x. Second, it contains massive topics to generalize\nthe open-domain. To build engaging dialogue system with this dataset, we\npropose and normalize two response producing tasks based on retrieval and\ngenerative scenarios. In addition, we build two baselines for above tasks with\nstate-of-the-art techniques and report their experimental performance. We also\npropose a novel evaluation metric MM-Relevance to measure the multi-modal\nresponses. Our dataset and scripts are available in\nhttps://github.com/victorsungo/MMDialog.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiazhan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qingfeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qingwei Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning over Different Types of Knowledge Graphs: Static, Temporal and Multi-Modal. (arXiv:2212.05767v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2212.05767","description":"<p>Knowledge graph reasoning (KGR), aiming to deduce new facts from existing\nfacts based on mined logic rules underlying knowledge graphs (KGs), has become\na fast-growing research direction. It has been proven to significantly benefit\nthe usage of KGs in many AI applications, such as question answering and\nrecommendation systems, etc. According to the graph types, the existing KGR\nmodels can be roughly divided into three categories, i.e., static models,\ntemporal models, and multi-modal models. The early works in this domain mainly\nfocus on static KGR and tend to directly apply general knowledge graph\nembedding models to the reasoning task. However, these models are not suitable\nfor more complex but practical tasks, such as inductive static KGR, temporal\nKGR, and multi-modal KGR. To this end, multiple works have been developed\nrecently, but no survey papers and open-source repositories comprehensively\nsummarize and discuss models in this important direction. To fill the gap, we\nconduct a survey for knowledge graph reasoning tracing from static to temporal\nand then to multi-modal KGs. Concretely, the preliminaries, summaries of KGR\nmodels, and typical datasets are introduced and discussed consequently.\nMoreover, we discuss the challenges and potential opportunities. The\ncorresponding open-source repository is shared on GitHub:\nhttps://github.com/LIANGKE23/Awesome-Knowledge-Graph-Reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1\">Ke Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lingyuan Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Meng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1\">Wenxuan Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sihang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMSMix: Sense-Maintained Sentence Mixup for Word Sense Disambiguation. (arXiv:2212.07072v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.07072","description":"<p>Word Sense Disambiguation (WSD) is an NLP task aimed at determining the\ncorrect sense of a word in a sentence from discrete sense choices. Although\ncurrent systems have attained unprecedented performances for such tasks, the\nnonuniform distribution of word senses during training generally results in\nsystems performing poorly on rare senses. To this end, we consider data\naugmentation to increase the frequency of these least frequent senses (LFS) to\nreduce the distributional bias of senses during training. We propose\nSense-Maintained Sentence Mixup (SMSMix), a novel word-level mixup method that\nmaintains the sense of a target word. SMSMix smoothly blends two sentences\nusing mask prediction while preserving the relevant span determined by saliency\nscores to maintain a specific word's sense. To the best of our knowledge, this\nis the first attempt to apply mixup in NLP while preserving the meaning of a\nspecific word. With extensive experiments, we validate that our augmentation\nmethod can effectively give more information about rare senses during training\nwith maintained target sense label.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1\">Hee Suk Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_E/0/1/0/all/0/1\">Eunseop Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harvill_J/0/1/0/all/0/1\">John Harvill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sunjae Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_Johnson_M/0/1/0/all/0/1\">Mark Hasegawa-Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1\">Chang D. Yoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Linguistically Informed Multi-Objective Pre-Training for Natural Language Inference. (arXiv:2212.07428v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.07428","description":"<p>We introduce a linguistically enhanced combination of pre-training methods\nfor transformers. The pre-training objectives include POS-tagging, synset\nprediction based on semantic knowledge graphs, and parent prediction based on\ndependency parse trees. Our approach achieves competitive results on the\nNatural Language Inference task, compared to the state of the art. Specifically\nfor smaller models, the method results in a significant performance boost,\nemphasizing the fact that intelligent pre-training can make up for fewer\nparameters and help building more efficient models. Combining POS-tagging and\nsynset prediction yields the overall best results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pielka_M/0/1/0/all/0/1\">Maren Pielka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_S/0/1/0/all/0/1\">Svetlana Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pucknat_L/0/1/0/all/0/1\">Lisa Pucknat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifa_R/0/1/0/all/0/1\">Rafet Sifa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Scales Data Augmentation Approach In Natural Language Inference For Artifacts Mitigation And Pre-Trained Model Optimization. (arXiv:2212.08756v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08756","description":"<p>Machine learning models can reach high performance on benchmark natural\nlanguage processing (NLP) datasets but fail in more challenging settings. We\nstudy this issue when a pre-trained model learns dataset artifacts in natural\nlanguage inference (NLI), the topic of studying the logical relationship\nbetween a pair of text sequences. We provide a variety of techniques for\nanalyzing and locating dataset artifacts inside the crowdsourced Stanford\nNatural Language Inference (SNLI) corpus. We study the stylistic pattern of\ndataset artifacts in the SNLI. To mitigate dataset artifacts, we employ a\nunique multi-scale data augmentation technique with two distinct frameworks: a\nbehavioral testing checklist at the sentence level and lexical synonym criteria\nat the word level. Specifically, our combination method enhances our model's\nresistance to perturbation testing, enabling it to continuously outperform the\npre-trained baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhenyuan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers. (arXiv:2212.10559v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10559","description":"<p>Large pretrained language models have shown surprising In-Context Learning\n(ICL) ability. With a few demonstration input-label pairs, they can predict the\nlabel for an unseen input without additional parameter updates. Despite the\ngreat success in performance, the working mechanism of ICL still remains an\nopen problem. In order to better understand how ICL works, this paper explains\nlanguage models as meta-optimizers and understands ICL as a kind of implicit\nfinetuning. Theoretically, we figure out that the Transformer attention has a\ndual form of gradient descent based optimization. On top of it, we understand\nICL as follows: GPT first produces meta-gradients according to the\ndemonstration examples, and then these meta-gradients are applied to the\noriginal GPT to build an ICL model. Experimentally, we comprehensively compare\nthe behavior of ICL and explicit finetuning based on real tasks to provide\nempirical evidence that supports our understanding. The results prove that ICL\nbehaves similarly to explicit finetuning at the prediction level, the\nrepresentation level, and the attention behavior level. Further, inspired by\nour understanding of meta-optimization, we design a momentum-based attention by\nanalogy with the momentum-based gradient descent algorithm. Its consistently\nbetter performance over vanilla attention supports our understanding again from\nanother aspect, and more importantly, it shows the potential to utilize our\nunderstanding for future model designing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yutao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yaru Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-12-21T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}