{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-01-24T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Is ChatGPT A Good Translator? A Preliminary Study. (arXiv:2301.08745v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08745","description":"<p>This report provides a preliminary evaluation of ChatGPT for machine\ntranslation, including translation prompt, multilingual translation, and\ntranslation robustness. We adopt the prompts advised by ChatGPT to trigger its\ntranslation ability and find that the candidate prompts generally work well and\nshow minor performance differences. By evaluating on a number of benchmark test\nsets, we find that ChatGPT performs competitively with commercial translation\nproducts (e.g., Google Translate) on high-resource European languages but lags\nbehind significantly on lowresource or distant languages. As for the\ntranslation robustness, ChatGPT does not perform as well as the commercial\nsystems on biomedical abstracts or Reddit comments but is potentially a good\ntranslator for spoken language. Scripts and data:\nhttps://github.com/wxjiao/Is-ChatGPT-A-Good-Translator\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jen-tse Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matching Exemplar as Next Sentence Prediction (MeNSP): Zero-shot Prompt Learning for Automatic Scoring in Science Education. (arXiv:2301.08771v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08771","description":"<p>Developing models to automatically score students' written responses to\nscience problems is critical for science education. However, collecting and\nlabeling sufficient student responses for training models is time and\ncost-consuming. Recent studies suggest that pre-trained language models (PLMs)\ncan be adapted to downstream tasks without fine-tuning with prompts. However,\nno research has employed such a prompt approach in science education. As\nstudent responses are presented with natural language, aligning the scoring\nprocedure as the next sentence prediction task using prompts can skip the\ncostly fine-tuning stage. In this study, we developed a zero-shot approach to\nautomatically score student responses via Matching Exemplars as Next Sentence\nPrediction (MeNSP). This approach employs no training samples. We first apply\nMeNSP in scoring three assessment tasks of scientific argumentation and found\nmachine-human scoring agreements, Cohen's Kappa ranges from 0.30 to 0.57, and\nF1 score ranges from 0.54 to 0.81. To improve the performance, we extend our\nresearch to the few-shots setting, either randomly selecting labeled student\nresponses or manually constructing responses to fine-tune the models. We find\nthat one task's performance is improved with more samples, Cohen's Kappa from\n0.30 to 0.38, and F1 score from 0.54 to 0.59; for the two others, scoring\nperformance is not improved. We also find that randomly selected few-shots\nperform better than the human expert-crafted approach. This study suggests that\nMeNSP can yield referable automatic scoring for student responses while\nsignificantly reducing the cost of model training. This method can benefit\nlow-stakes classroom assessment practices in science education. Future research\nshould further explore the applicability of the MeNSP in different types of\nassessment tasks in science education and improve the model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xuansheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xinyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ninghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaoming Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Semantic Relatedness Dataset for Image Captioning. (arXiv:2301.08784v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08784","description":"<p>Modern image captioning system relies heavily on extracting knowledge from\nimages to capture the concept of a static story. In this paper, we propose a\ntextual visual context dataset for captioning, in which the publicly available\ndataset COCO Captions (Lin et al., 2014) has been extended with information\nabout the scene (such as objects in the image). Since this information has a\ntextual form, it can be used to leverage any NLP task, such as text similarity\nor semantic relation methods, into captioning systems, either as an end-to-end\ntraining strategy or a post-processing based approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sabir_A/0/1/0/all/0/1\">Ahmed Sabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padro_L/0/1/0/all/0/1\">Llu&#xed;s Padr&#xf3;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phoneme-Level BERT for Enhanced Prosody of Text-to-Speech with Grapheme Predictions. (arXiv:2301.08810v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08810","description":"<p>Large-scale pre-trained language models have been shown to be helpful in\nimproving the naturalness of text-to-speech (TTS) models by enabling them to\nproduce more naturalistic prosodic patterns. However, these models are usually\nword-level or sup-phoneme-level and jointly trained with phonemes, making them\ninefficient for the downstream TTS task where only phonemes are needed. In this\nwork, we propose a phoneme-level BERT (PL-BERT) with a pretext task of\npredicting the corresponding graphemes along with the regular masked phoneme\npredictions. Subjective evaluations show that our phoneme-level BERT encoder\nhas significantly improved the mean opinion scores (MOS) of rated naturalness\nof synthesized speech compared with the state-of-the-art (SOTA) StyleTTS\nbaseline on out-of-distribution (OOD) texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghao Aaron Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Cong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xilin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mesgarani_N/0/1/0/all/0/1\">Nima Mesgarani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Document Summarization with Text Segmentation. (arXiv:2301.08817v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08817","description":"<p>In this paper, we exploit the innate document segment structure for improving\nthe extractive summarization task. We build two text segmentation models and\nfind the most optimal strategy to introduce their output predictions in an\nextractive summarization model. Experimental results on a corpus of scientific\narticles show that extractive summarization benefits from using a highly\naccurate segmentation method. In particular, most of the improvement is in\ndocuments where the most relevant information is not at the beginning thus, we\nconclude that segmentation helps in reducing the lead bias problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miculicich_L/0/1/0/all/0/1\">Lesly Miculicich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Benjamin Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review of the Trends and Challenges in Adopting Natural Language Processing Methods for Education Feedback Analysis. (arXiv:2301.08826v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08826","description":"<p>Artificial Intelligence (AI) is a fast-growing area of study that stretching\nits presence to many business and research domains. Machine learning, deep\nlearning, and natural language processing (NLP) are subsets of AI to tackle\ndifferent areas of data processing and modelling. This review article presents\nan overview of AI impact on education outlining with current opportunities. In\nthe education domain, student feedback data is crucial to uncover the merits\nand demerits of existing services provided to students. AI can assist in\nidentifying the areas of improvement in educational infrastructure, learning\nmanagement systems, teaching practices and study environment. NLP techniques\nplay a vital role in analyzing student feedback in textual format. This\nresearch focuses on existing NLP methodologies and applications that could be\nadapted to educational domain applications like sentiment annotations, entity\nannotations, text summarization, and topic modelling. Trends and challenges in\nadopting NLP in education were reviewed and explored. Contextbased challenges\nin NLP like sarcasm, domain-specific language, ambiguity, and aspect-based\nsentiment analysis are explained with existing methodologies to overcome them.\nResearch community approaches to extract the semantic meaning of emoticons and\nspecial characters in feedback which conveys user opinion and challenges in\nadopting NLP in education are explored.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaik_T/0/1/0/all/0/1\">Thanveer Shaik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1\">Xiaohui Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dann_C/0/1/0/all/0/1\">Christopher Dann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mcdonald_J/0/1/0/all/0/1\">Jacquie Mcdonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redmond_P/0/1/0/all/0/1\">Petrea Redmond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galligan_L/0/1/0/all/0/1\">Linda Galligan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Same Words, Different Meanings: Interpretable Predictions of Polarization Trends in Broadcast Media Language and Granger Causal Effects on Public Discourse. (arXiv:2301.08832v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08832","description":"<p>With the growth of online news over the past decade, empirical studies on\npolitical discourse and news consumption have focused on the phenomenon of\nfilter bubbles and echo chambers. Yet recently, scholars have revealed limited\nevidence around the impact of such phenomenon, leading some to argue that\npartisan segregation across news audiences cannot be fully explained by online\nnews consumption alone and that the role of traditional legacy media may be as\nsalient in polarizing public discourse around current events. In this work, we\nexpand the scope of analysis to include both online and more traditional media\nby investigating the relationship between broadcast news media language and\nsocial media discourse. By analyzing a decade's worth of closed captions (2\nmillion speaker turns) from CNN and Fox News along with topically corresponding\ndiscourse from Twitter, we provide a novel framework for measuring semantic\npolarization between America's two major broadcast networks to demonstrate how\nsemantic polarization between these outlets has evolved (Study 1), peaked\n(Study 2) and influenced partisan discussions on Twitter (Study 3) across the\nlast decade. Our results demonstrate a sharp increase in polarization in how\ntopically important keywords are discussed between the two channels, especially\nafter 2016, with overall highest peaks occurring in 2020. The two stations\ndiscuss identical topics in drastically distinct contexts in 2020, to the\nextent that there is barely any linguistic overlap in how identical keywords\nare contextually discussed. Further, we demonstrate at scale, how such partisan\ndivision in broadcast media language significantly shapes semantic polarity\ntrends on Twitter (and vice-versa), empirically linking for the first time, how\nonline discussions are influenced by televised media.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaohan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horning_M/0/1/0/all/0/1\">Mike Horning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rho_E/0/1/0/all/0/1\">Eugenia H. Rho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regeneration Learning: A Learning Paradigm for Data Generation. (arXiv:2301.08846v1 [cs.LG])","link":"http://arxiv.org/abs/2301.08846","description":"<p>Machine learning methods for conditional data generation usually build a\nmapping from source conditional data X to target data Y. The target Y (e.g.,\ntext, speech, music, image, video) is usually high-dimensional and complex, and\ncontains information that does not exist in source data, which hinders\neffective and efficient learning on the source-target mapping. In this paper,\nwe present a learning paradigm called regeneration learning for data\ngeneration, which first generates Y' (an abstraction/representation of Y) from\nX and then generates Y from Y'. During training, Y' is obtained from Y through\neither handcrafted rules or self-supervised learning and is used to learn\nX--&gt;Y' and Y'--&gt;Y. Regeneration learning extends the concept of representation\nlearning to data generation tasks, and can be regarded as a counterpart of\ntraditional representation learning, since 1) regeneration learning handles the\nabstraction (Y') of the target data Y for data generation while traditional\nrepresentation learning handles the abstraction (X') of source data X for data\nunderstanding; 2) both the processes of Y'--&gt;Y in regeneration learning and\nX--&gt;X' in representation learning can be learned in a self-supervised way\n(e.g., pre-training); 3) both the mappings from X to Y' in regeneration\nlearning and from X' to Y in representation learning are simpler than the\ndirect mapping from X to Y. We show that regeneration learning can be a\nwidely-used paradigm for data generation (e.g., text generation, speech\nrecognition, speech synthesis, music composition, image generation, and video\ngeneration) and can provide valuable insights into developing data generation\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProKD: An Unsupervised Prototypical Knowledge Distillation Network for Zero-Resource Cross-Lingual Named Entity Recognition. (arXiv:2301.08855v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08855","description":"<p>For named entity recognition (NER) in zero-resource languages, utilizing\nknowledge distillation methods to transfer language-independent knowledge from\nthe rich-resource source languages to zero-resource languages is an effective\nmeans. Typically, these approaches adopt a teacher-student architecture, where\nthe teacher network is trained in the source language, and the student network\nseeks to learn knowledge from the teacher network and is expected to perform\nwell in the target language. Despite the impressive performance achieved by\nthese methods, we argue that they have two limitations. Firstly, the teacher\nnetwork fails to effectively learn language-independent knowledge shared across\nlanguages due to the differences in the feature distribution between the source\nand target languages. Secondly, the student network acquires all of its\nknowledge from the teacher network and ignores the learning of target\nlanguage-specific knowledge. Undesirably, these limitations would hinder the\nmodel's performance in the target language. This paper proposes an unsupervised\nprototype knowledge distillation network (ProKD) to address these issues.\nSpecifically, ProKD presents a contrastive learning-based prototype alignment\nmethod to achieve class feature alignment by adjusting the distance among\nprototypes in the source and target languages, boosting the teacher network's\ncapacity to acquire language-independent knowledge. In addition, ProKD\nintroduces a prototypical self-training method to learn the intrinsic structure\nof the language by retraining the student network on the target data using\nsamples' distance information from prototypes, thereby enhancing the student\nnetwork's ability to acquire language-specific knowledge. Extensive experiments\non three benchmark cross-lingual NER datasets demonstrate the effectiveness of\nour approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_L/0/1/0/all/0/1\">Ling Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1\">Guanghui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jihong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dr.Spider: A Diagnostic Evaluation Benchmark towards Text-to-SQL Robustness. (arXiv:2301.08881v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08881","description":"<p>Neural text-to-SQL models have achieved remarkable performance in translating\nnatural language questions into SQL queries. However, recent studies reveal\nthat text-to-SQL models are vulnerable to task-specific perturbations. Previous\ncurated robustness test sets usually focus on individual phenomena. In this\npaper, we propose a comprehensive robustness benchmark based on Spider, a\ncross-domain text-to-SQL benchmark, to diagnose the model robustness. We design\n17 perturbations on databases, natural language questions, and SQL queries to\nmeasure the robustness from different angles. In order to collect more\ndiversified natural question perturbations, we utilize large pretrained\nlanguage models (PLMs) to simulate human behaviors in creating natural\nquestions. We conduct a diagnostic study of the state-of-the-art models on the\nrobustness set. Experimental results reveal that even the most robust model\nsuffers from a 14.0% performance drop overall and a 50.7% performance drop on\nthe most challenging perturbation. We also present a breakdown analysis\nregarding text-to-SQL model designs and provide insights for improving model\nrobustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuaichen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Mingwen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lin Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Henghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Alexander Hanbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_W/0/1/0/all/0/1\">Wuwei Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiarong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lilien_J/0/1/0/all/0/1\">Joseph Lilien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ash_S/0/1/0/all/0/1\">Steve Ash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiguo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castelli_V/0/1/0/all/0/1\">Vittorio Castelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_P/0/1/0/all/0/1\">Patrick Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rationalization for Explainable NLP: A Survey. (arXiv:2301.08912v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08912","description":"<p>Recent advances in deep learning have improved the performance of many\nNatural Language Processing (NLP) tasks such as translation,\nquestion-answering, and text classification. However, this improvement comes at\nthe expense of model explainability. Black-box models make it difficult to\nunderstand the internals of a system and the process it takes to arrive at an\noutput. Numerical (LIME, Shapley) and visualization (saliency heatmap)\nexplainability techniques are helpful; however, they are insufficient because\nthey require specialized knowledge. These factors led rationalization to emerge\nas a more accessible explainable technique in NLP. Rationalization justifies a\nmodel's output by providing a natural language explanation (rationale). Recent\nimprovements in natural language generation have made rationalization an\nattractive technique because it is intuitive, human-comprehensible, and\naccessible to non-technical users. Since rationalization is a relatively new\nfield, it is disorganized. As the first survey, rationalization literature in\nNLP from 2007-2022 is analyzed. This survey presents available methods,\nexplainable evaluations, code, and datasets used across various NLP tasks that\nuse rationalization. Further, a new subfield in Explainable AI (XAI), namely,\nRational AI (RAI), is introduced to advance the current state of\nrationalization. A discussion on observed insights, challenges, and future\ndirections is provided to point to promising research opportunities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gurrapu_S/0/1/0/all/0/1\">Sai Gurrapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Ajay Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1\">Ismini Lourentzou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_L/0/1/0/all/0/1\">Laura Freeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batarseh_F/0/1/0/all/0/1\">Feras A. Batarseh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unifying Structure Reasoning and Language Model Pre-training for Complex Reasoning. (arXiv:2301.08913v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08913","description":"<p>Recent knowledge enhanced pre-trained language models have shown remarkable\nperformance on downstream tasks by incorporating structured knowledge from\nexternal sources into language models. However, they usually suffer from a\nheterogeneous information alignment problem and a noisy knowledge injection\nproblem. For complex reasoning, the contexts contain rich knowledge that\ntypically exists in complex and sparse forms. In order to model structured\nknowledge in the context and avoid these two problems, we propose to unify\nstructure reasoning and language model pre-training. It identifies four types\nof elementary knowledge structures from contexts to construct structured\nqueries, and utilizes the box embedding method to conduct explicit structure\nreasoning along queries during language modeling. To fuse textual and\nstructured semantics, we utilize contextual language representations of\nknowledge structures to initialize their box embeddings for structure\nreasoning. We conduct experiments on complex language reasoning and knowledge\ngraph (KG) reasoning tasks. The results show that our model can effectively\nenhance the performance of complex reasoning of both language and KG\nmodalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiarong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhihao Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ExClaim: Explainable Neural Claim Verification Using Rationalization. (arXiv:2301.08914v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08914","description":"<p>With the advent of deep learning, text generation language models have\nimproved dramatically, with text at a similar level as human-written text. This\ncan lead to rampant misinformation because content can now be created cheaply\nand distributed quickly. Automated claim verification methods exist to validate\nclaims, but they lack foundational data and often use mainstream news as\nevidence sources that are strongly biased towards a specific agenda. Current\nclaim verification methods use deep neural network models and complex\nalgorithms for a high classification accuracy but it is at the expense of model\nexplainability. The models are black-boxes and their decision-making process\nand the steps it took to arrive at a final prediction are obfuscated from the\nuser. We introduce a novel claim verification approach, namely: ExClaim, that\nattempts to provide an explainable claim verification system with foundational\nevidence. Inspired by the legal system, ExClaim leverages rationalization to\nprovide a verdict for the claim and justifies the verdict through a natural\nlanguage explanation (rationale) to describe the model's decision-making\nprocess. ExClaim treats the verdict classification task as a question-answer\nproblem and achieves a performance of 0.93 F1 score. It provides subtasks\nexplanations to also justify the intermediate outcomes. Statistical and\nExplainable AI (XAI) evaluations are conducted to ensure valid and trustworthy\noutcomes. Ensuring claim verification systems are assured, rational, and\nexplainable is an essential step toward improving Human-AI trust and the\naccessibility of black-box systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gurrapu_S/0/1/0/all/0/1\">Sai Gurrapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batarseh_F/0/1/0/all/0/1\">Feras A. Batarseh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Methods for Building Dialects-Mandarin Code-Mixing Corpora: A Case Study in Taiwanese Hokkien. (arXiv:2301.08937v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08937","description":"<p>In natural language processing (NLP), code-mixing (CM) is a challenging task,\nespecially when the mixed languages include dialects. In Southeast Asian\ncountries such as Singapore, Indonesia, and Malaysia, Hokkien-Mandarin is the\nmost widespread code-mixed language pair among Chinese immigrants, and it is\nalso common in Taiwan. However, dialects such as Hokkien often have a scarcity\nof resources and the lack of an official writing system, limiting the\ndevelopment of dialect CM research. In this paper, we propose a method to\nconstruct a Hokkien-Mandarin CM dataset to mitigate the limitation, overcome\nthe morphological issue under the Sino-Tibetan language family, and offer an\nefficient Hokkien word segmentation method through a linguistics-based toolkit.\nFurthermore, we use our proposed dataset and employ transfer learning to train\nthe XLM (cross-lingual language model) for translation tasks. To fit the\ncode-mixing scenario, we adapt XLM slightly. We found that by using linguistic\nknowledge, rules, and language tags, the model produces good results on CM data\ntranslation while maintaining monolingual translation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Sin-En Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1\">Bo-Han Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chao-Yi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_R/0/1/0/all/0/1\">Richard Tzong-Han Tsai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting a Language Model While Preserving its General Knowledge. (arXiv:2301.08986v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08986","description":"<p>Domain-adaptive pre-training (or DA-training for short), also known as\npost-training, aims to train a pre-trained general-purpose language model (LM)\nusing an unlabeled corpus of a particular domain to adapt the LM so that\nend-tasks in the domain can give improved performances. However, existing\nDA-training methods are in some sense blind as they do not explicitly identify\nwhat knowledge in the LM should be preserved and what should be changed by the\ndomain corpus. This paper shows that the existing methods are suboptimal and\nproposes a novel method to perform a more informed adaptation of the knowledge\nin the LM by (1) soft-masking the attention heads based on their importance to\nbest preserve the general knowledge in the LM and (2) contrasting the\nrepresentations of the general and the full (both general and domain knowledge)\nto learn an integrated representation with both general and domain-specific\nknowledge. Experimental results will demonstrate the effectiveness of the\nproposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_Z/0/1/0/all/0/1\">Zixuan Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yijia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haowei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_L/0/1/0/all/0/1\">Lei Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REDAffectiveLM: Leveraging Affect Enriched Embedding and Transformer-based Neural Language Model for Readers' Emotion Detection. (arXiv:2301.08995v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08995","description":"<p>Technological advancements in web platforms allow people to express and share\nemotions towards textual write-ups written and shared by others. This brings\nabout different interesting domains for analysis; emotion expressed by the\nwriter and emotion elicited from the readers. In this paper, we propose a novel\napproach for Readers' Emotion Detection from short-text documents using a deep\nlearning model called REDAffectiveLM. Within state-of-the-art NLP tasks, it is\nwell understood that utilizing context-specific representations from\ntransformer-based pre-trained language models helps achieve improved\nperformance. Within this affective computing task, we explore how incorporating\naffective information can further enhance performance. Towards this, we\nleverage context-specific and affect enriched representations by using a\ntransformer-based pre-trained language model in tandem with affect enriched\nBi-LSTM+Attention. For empirical evaluation, we procure a new dataset REN-20k,\nbesides using RENh-4k and SemEval-2007. We evaluate the performance of our\nREDAffectiveLM rigorously across these datasets, against a vast set of\nstate-of-the-art baselines, where our model consistently outperforms baselines\nand obtains statistically significant results. Our results establish that\nutilizing affect enriched representation along with context-specific\nrepresentation within a neural architecture can considerably enhance readers'\nemotion detection. Since the impact of affect enrichment specifically in\nreaders' emotion detection isn't well explored, we conduct a detailed analysis\nover affect enriched Bi-LSTM+Attention using qualitative and quantitative model\nbehavior evaluation techniques. We observe that compared to conventional\nsemantic embedding, affect enriched embedding increases ability of the network\nto effectively identify and assign weightage to key terms responsible for\nreaders' emotion detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kadan_A/0/1/0/all/0/1\">Anoop Kadan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+P%2E_D/0/1/0/all/0/1\">Deepak P.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangan_M/0/1/0/all/0/1\">Manjary P. Gangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abraham_S/0/1/0/all/0/1\">Savitha Sam Abraham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+L_L/0/1/0/all/0/1\">Lajish V. L</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntax-guided Neural Module Distillation to Probe Compositionality in Sentence Embeddings. (arXiv:2301.08998v1 [cs.CL])","link":"http://arxiv.org/abs/2301.08998","description":"<p>Past work probing compositionality in sentence embedding models faces issues\ndetermining the causal impact of implicit syntax representations. Given a\nsentence, we construct a neural module net based on its syntax parse and train\nit end-to-end to approximate the sentence's embedding generated by a\ntransformer model. The distillability of a transformer to a Syntactic NeurAl\nModule Net (SynNaMoN) then captures whether syntax is a strong causal model of\nits compositional ability. Furthermore, we address questions about the geometry\nof semantic composition by specifying individual SynNaMoN modules' internal\narchitecture &amp; linearity. We find differences in the distillability of various\nsentence embedding models that broadly correlate with their performance, but\nobserve that distillability doesn't considerably vary by model size. We also\npresent preliminary evidence that much syntax-guided composition in sentence\nembedding models is linear, and that non-linearities may serve primarily to\nhandle non-compositional phrases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_R/0/1/0/all/0/1\">Rohan Pandey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blacks is to Anger as Whites is to Joy? Understanding Latent Affective Bias in Large Pre-trained Neural Language Models. (arXiv:2301.09003v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09003","description":"<p>Groundbreaking inventions and highly significant performance improvements in\ndeep learning based Natural Language Processing are witnessed through the\ndevelopment of transformer based large Pre-trained Language Models (PLMs). The\nwide availability of unlabeled data within human generated data deluge along\nwith self-supervised learning strategy helps to accelerate the success of large\nPLMs in language generation, language understanding, etc. But at the same time,\nlatent historical bias/unfairness in human minds towards a particular gender,\nrace, etc., encoded unintentionally/intentionally into the corpora harms and\nquestions the utility and efficacy of large PLMs in many real-world\napplications, particularly for the protected groups. In this paper, we present\nan extensive investigation towards understanding the existence of \"Affective\nBias\" in large PLMs to unveil any biased association of emotions such as anger,\nfear, joy, etc., towards a particular gender, race or religion with respect to\nthe downstream task of textual emotion detection. We conduct our exploration of\naffective bias from the very initial stage of corpus level affective bias\nanalysis by searching for imbalanced distribution of affective words within a\ndomain, in large scale corpora that are used to pre-train and fine-tune PLMs.\nLater, to quantify affective bias in model predictions, we perform an extensive\nset of class-based and intensity-based evaluations using various bias\nevaluation corpora. Our results show the existence of statistically significant\naffective bias in the PLM based emotion detection systems, indicating biased\nassociation of certain emotions towards a particular gender, race, and\nreligion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kadan_A/0/1/0/all/0/1\">Anoop Kadan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+P%2E_D/0/1/0/all/0/1\">Deepak P.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhadra_S/0/1/0/all/0/1\">Sahely Bhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangan_M/0/1/0/all/0/1\">Manjary P. Gangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+L_L/0/1/0/all/0/1\">Lajish V. L</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Poor Man's Quality Estimation: Predicting Reference-Based MT Metrics Without the Reference. (arXiv:2301.09008v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09008","description":"<p>Machine translation quality estimation (QE) predicts human judgements of a\ntranslation hypothesis without seeing the reference. State-of-the-art QE\nsystems based on pretrained language models have been achieving remarkable\ncorrelations with human judgements yet they are computationally heavy and\nrequire human annotations, which are slow and expensive to create. To address\nthese limitations, we define the problem of metric estimation (ME) where one\npredicts the automated metric scores also without the reference. We show that\neven without access to the reference, our model can estimate automated metrics\n($\\rho$=60% for BLEU, $\\rho$=51% for other metrics) at the sentence-level.\nBecause automated metrics correlate with human judgements, we can leverage the\nME task for pre-training a QE model. For the QE task, we find that pre-training\non TER is better ($\\rho$=23%) than training for scratch ($\\rho$=20%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zouhar_V/0/1/0/all/0/1\">Vil&#xe9;m Zouhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1\">Shehzaad Dhuliawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daheim_N/0/1/0/all/0/1\">Nico Daheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocmi_T/0/1/0/all/0/1\">Tom Kocmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuchen Eleanor Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Knowledge from Natural Language to Electrocardiography: Can We Detect Cardiovascular Disease Through Language Models?. (arXiv:2301.09017v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09017","description":"<p>Recent advancements in Large Language Models (LLMs) have drawn increasing\nattention since the learned embeddings pretrained on large-scale datasets have\nshown powerful ability in various downstream applications. However, whether the\nlearned knowledge by LLMs can be transferred to clinical cardiology remains\nunknown. In this work, we aim to bridge this gap by transferring the knowledge\nof LLMs to clinical Electrocardiography (ECG). We propose an approach for\ncardiovascular disease diagnosis and automatic ECG diagnosis report generation.\nWe also introduce an additional loss function by Optimal Transport (OT) to\nalign the distribution between ECG and language embedding. The learned\nembeddings are evaluated on two downstream tasks: (1) automatic ECG diagnosis\nreport generation, and (2) zero-shot cardiovascular disease detection. Our\napproach is able to generate high-quality cardiac diagnosis reports and also\nachieves competitive zero-shot classification performance even compared with\nsupervised baselines, which proves the feasibility of transferring knowledge\nfrom LLMs to the cardiac domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jielin Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">William Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiacheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengdi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_M/0/1/0/all/0/1\">Michael Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1\">Emerson Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_D/0/1/0/all/0/1\">Douglas Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Ding Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Data Selection for TTS: Using Arabic Broadcast News as a Case Study. (arXiv:2301.09099v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09099","description":"<p>Several high-resource Text to Speech (TTS) systems currently produce natural,\nwell-established human-like speech. In contrast, low-resource languages,\nincluding Arabic, have very limited TTS systems due to the lack of resources.\nWe propose a fully unsupervised method for building TTS, including automatic\ndata selection and pre-training/fine-tuning strategies for TTS training, using\nbroadcast news as a case study. We show how careful selection of data, yet\nsmaller amounts, can improve the efficiency of TTS system in generating more\nnatural speech than a system trained on a bigger dataset. We adopt to propose\ndifferent approaches for the: 1) data: we applied automatic annotations using\nDNSMOS, automatic vowelization, and automatic speech recognition (ASR) for\nfixing transcriptions' errors; 2) model: we used transfer learning from\nhigh-resource language in TTS model and fine-tuned it with one hour broadcast\nrecording then we used this model to guide a FastSpeech2-based Conformer model\nfor duration. Our objective evaluation shows 3.9% character error rate (CER),\nwhile the groundtruth has 1.3% CER. As for the subjective evaluation, where 1\nis bad and 5 is excellent, our FastSpeech2-based Conformer model achieved a\nmean opinion score (MOS) of 4.4 for intelligibility and 4.2 for naturalness,\nwhere many annotators recognized the voice of the broadcaster, which proves the\neffectiveness of our proposed unsupervised method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baali_M/0/1/0/all/0/1\">Massa Baali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_T/0/1/0/all/0/1\">Tomoki Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiti_S/0/1/0/all/0/1\">Soumi Maiti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Hajj_W/0/1/0/all/0/1\">Wassim El-Hajj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentially Private Natural Language Models: Recent Advances and Future Directions. (arXiv:2301.09112v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09112","description":"<p>Recent developments in deep learning have led to great success in various\nnatural language processing (NLP) tasks. However, these applications may\ninvolve data that contain sensitive information. Therefore, how to achieve good\nperformance while also protect privacy of sensitive data is a crucial challenge\nin NLP. To preserve privacy, Differential Privacy (DP), which can prevent\nreconstruction attacks and protect against potential side knowledge, is\nbecoming a de facto technique for private data analysis. In recent years, NLP\nin DP models (DP-NLP) has been studied from different perspectives, which\ndeserves a comprehensive review. In this paper, we provide the first systematic\nreview of recent advances on DP deep learning models in NLP. In particular, we\nfirst discuss some differences and additional challenges of DP-NLP compared\nwith the standard DP deep learning. Then we investigate some existing work on\nDP-NLP and present its recent developments from two aspects: gradient\nperturbation based methods and embedding vector perturbation based methods. We\nalso discuss some challenges and future directions of this topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lijie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habernal_I/0/1/0/all/0/1\">Ivan Habernal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representing Interlingual Meaning in Lexical Databases. (arXiv:2301.09169v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09169","description":"<p>In today's multilingual lexical databases, the majority of the world's\nlanguages are under-represented. Beyond a mere issue of resource\nincompleteness, we show that existing lexical databases have structural\nlimitations that result in a reduced expressivity on culturally-specific words\nand in mapping them across languages. In particular, the lexical meaning space\nof dominant languages, such as English, is represented more accurately while\nlinguistically or culturally diverse languages are mapped in an approximate\nmanner. Our paper assesses state-of-the-art multilingual lexical databases and\nevaluates their strengths and limitations with respect to their expressivity on\nlexical phenomena of linguistic diversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1\">Fausto Giunchiglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bella_G/0/1/0/all/0/1\">Gabor Bella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_N/0/1/0/all/0/1\">Nandu Chandran Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_Y/0/1/0/all/0/1\">Yang Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensemble Transfer Learning for Multilingual Coreference Resolution. (arXiv:2301.09175v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09175","description":"<p>Entity coreference resolution is an important research problem with many\napplications, including information extraction and question answering.\nCoreference resolution for English has been studied extensively. However, there\nis relatively little work for other languages. A problem that frequently occurs\nwhen working with a non-English language is the scarcity of annotated training\ndata. To overcome this challenge, we design a simple but effective\nensemble-based framework that combines various transfer learning (TL)\ntechniques. We first train several models using different TL methods. Then,\nduring inference, we compute the unweighted average scores of the models'\npredictions to extract the final set of predicted clusters. Furthermore, we\nalso propose a low-cost TL method that bootstraps coreference resolution models\nby utilizing Wikipedia anchor texts. Leveraging the idea that the coreferential\nlinks naturally exist between anchor texts pointing to the same article, our\nmethod builds a sizeable distantly-supervised dataset for the target language\nthat consists of tens of thousands of documents. We can pre-train a model on\nthe pseudo-labeled dataset before finetuning it on the final target dataset.\nExperimental results on two benchmark datasets, OntoNotes and SemEval, confirm\nthe effectiveness of our methods. Our best ensembles consistently outperform\nthe baseline approach of simple training by up to 7.68% in the F1 score. These\nensembles also achieve new state-of-the-art results for three languages:\nArabic, Dutch, and Spanish.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1\">Tuan Manh Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction. (arXiv:2301.09209v1 [cs.CV])","link":"http://arxiv.org/abs/2301.09209","description":"<p>We study the task of object interaction anticipation in egocentric videos.\nSuccessful prediction of future actions and objects requires an understanding\nof the spatio-temporal context formed by past actions and object relationships.\nWe propose TransFusion, a multimodal transformer-based architecture, that\neffectively makes use of the representational power of language by summarizing\npast actions concisely. TransFusion leverages pre-trained image captioning\nmodels and summarizes the caption, focusing on past actions and objects. This\naction context together with a single input frame is processed by a multimodal\nfusion module to forecast the next object interactions. Our model enables more\nefficient end-to-end learning by replacing dense video features with language\nrepresentations, allowing us to benefit from knowledge encoded in large\npre-trained models. Experiments on Ego4D and EPIC-KITCHENS-100 show the\neffectiveness of our multimodal fusion model and the benefits of using\nlanguage-based context summaries. Our method outperforms state-of-the-art\napproaches by 40.4% in overall mAP on the Ego4D test set. We show the\ngenerality of TransFusion via experiments on EPIC-KITCHENS-100. Video and code\nare available at: https://eth-ait.github.io/transfusion-proj/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pasca_R/0/1/0/all/0/1\">Razvan-George Pasca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavryushin_A/0/1/0/all/0/1\">Alexey Gavryushin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_Y/0/1/0/all/0/1\">Yen-Ling Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of Metrics to Measure Representational Harms in Pre-Trained Language Models. (arXiv:2301.09211v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09211","description":"<p>Large-scale Pre-Trained Language Models (PTLMs) capture knowledge from\nmassive human-written data which contains latent societal biases and toxic\ncontents. In this paper, we leverage the primary task of PTLMs, i.e., language\nmodeling, and propose a new metric to quantify manifested implicit\nrepresentational harms in PTLMs towards 13 marginalized demographics. Using\nthis metric, we conducted an empirical analysis of 24 widely used PTLMs. Our\nanalysis provides insights into the correlation between the proposed metric in\nthis work and other related metrics for representational harm. We observe that\nour metric correlates with most of the gender-specific metrics in the\nliterature. Through extensive experiments, we explore the connections between\nPTLMs architectures and representational harms across two dimensions: depth and\nwidth of the networks. We found that prioritizing depth over width, mitigates\nrepresentational harms in some PTLMs. Our code and data can be found at\nhttps://github.com/microsoft/SafeNLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1\">Saghar Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1\">Hamid Palangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-aware Contrastive Learning for Electroencephalography-to-Text Generation with Curriculum Learning. (arXiv:2301.09237v1 [cs.HC])","link":"http://arxiv.org/abs/2301.09237","description":"<p>Electroencephalography-to-Text generation (EEG-to-Text), which aims to\ndirectly generate natural text from EEG signals has drawn increasing attention\nin recent years due to the enormous potential for Brain-computer interfaces\n(BCIs). However, the remarkable discrepancy between the subject-dependent EEG\nrepresentation and the semantic-dependent text representation poses a great\nchallenge to this task. To mitigate this challenge, we devise a Curriculum\nSemantic-aware Contrastive Learning strategy (C-SCL), which effectively\nre-calibrates the subject-dependent EEG representation to the\nsemantic-dependent EEG representation, thus reducing the discrepancy.\nSpecifically, our C-SCL pulls semantically similar EEG representations together\nwhile pushing apart dissimilar ones. Besides, in order to introduce more\nmeaningful contrastive pairs, we carefully employ curriculum learning to not\nonly craft meaningful contrastive pairs but also make the learning\nprogressively. We conduct extensive experiments on the ZuCo benchmark and our\nmethod combined with diverse models and architectures shows stable improvements\nacross three types of metrics while achieving the new state-of-the-art. Further\ninvestigation proves not only its superiority in both the single-subject and\nlow-resource settings but also its robust generalizability in the zero-shot\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiachong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaocheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Encoders for Streaming Sequence Tagging. (arXiv:2301.09244v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09244","description":"<p>A naive application of state-of-the-art bidirectional encoders for streaming\nsequence tagging would require encoding each token from scratch for each new\ntoken in an incremental streaming input (like transcribed speech). The lack of\nre-usability of previous computation leads to a higher number of Floating Point\nOperations (or FLOPs) and higher number of unnecessary label flips. Increased\nFLOPs consequently lead to higher wall-clock time and increased label flipping\nleads to poorer streaming performance. In this work, we present a Hybrid\nEncoder with Adaptive Restart (HEAR) that addresses these issues while\nmaintaining the performance of bidirectional encoders over the offline (or\ncomplete) inputs while improving performance on streaming (or incomplete)\ninputs. HEAR has a Hybrid unidirectional-bidirectional encoder architecture to\nperform sequence tagging, along with an Adaptive Restart Module (ARM) to\nselectively guide the restart of bidirectional portion of the encoder. Across\nfour sequence tagging tasks, HEAR offers FLOP savings in streaming settings\nupto 71.1% and also outperforms bidirectional encoders for streaming\npredictions by upto +10% streaming exact match.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaushal_A/0/1/0/all/0/1\">Ayush Kaushal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aditya Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1\">Shyam Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faruqui_M/0/1/0/all/0/1\">Manaal Faruqui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StockEmotions: Discover Investor Emotions for Financial Sentiment Analysis and Multivariate Time Series. (arXiv:2301.09279v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09279","description":"<p>There has been growing interest in applying NLP techniques in the financial\ndomain, however, resources are extremely limited. This paper introduces\nStockEmotions, a new dataset for detecting emotions in the stock market that\nconsists of 10,000 English comments collected from StockTwits, a financial\nsocial media platform. Inspired by behavioral finance, it proposes 12\nfine-grained emotion classes that span the roller coaster of investor emotion.\nUnlike existing financial sentiment datasets, StockEmotions presents granular\nfeatures such as investor sentiment classes, fine-grained emotions, emojis, and\ntime series data. To demonstrate the usability of the dataset, we perform a\ndataset analysis and conduct experimental downstream tasks. For financial\nsentiment/emotion classification tasks, DistilBERT outperforms other baselines,\nand for multivariate time series forecasting, a Temporal Attention LSTM model\ncombining price index, text, and emotion features achieves the best performance\nthan using a single feature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jean Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Youn_H/0/1/0/all/0/1\">Hoyoul Luis Youn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_J/0/1/0/all/0/1\">Josiah Poon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sensemaking About Contraceptive Methods Across Online Platforms. (arXiv:2301.09295v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09295","description":"<p>Selecting a birth control method is a complex healthcare decision. While\nbirth control methods provide important benefits, they can also cause\nunpredictable side effects and be stigmatized, leading many people to seek\nadditional information online, where they can find reviews, advice, hypotheses,\nand experiences of other birth control users. However, the relationships\nbetween their healthcare concerns, sensemaking activities, and online settings\nare not well understood. We gather texts about birth control shared on Twitter,\nReddit, and WebMD -- platforms with different affordances, moderation, and\naudiences -- to study where and how birth control is discussed online. Using a\ncombination of topic modeling and hand annotation, we identify and characterize\nthe dominant sensemaking practices across these platforms, and we create\nlexicons to draw comparisons across birth control methods and side effects. We\nuse these to measure variations from survey reports of side effect experiences\nand method usage. Our findings characterize how online platforms are used to\nmake sense of difficult healthcare choices and highlight unmet needs of birth\ncontrol users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McDowall_L/0/1/0/all/0/1\">LeAnn McDowall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antoniak_M/0/1/0/all/0/1\">Maria Antoniak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mimno_D/0/1/0/all/0/1\">David Mimno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-scale fine-grained semantic indexing of biomedical literature based on weakly-supervised deep learning. (arXiv:2301.09350v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09350","description":"<p>Semantic indexing of biomedical literature is usually done at the level of\nMeSH descriptors, representing topics of interest for the biomedical community.\nSeveral related but distinct biomedical concepts are often grouped together in\na single coarse-grained descriptor and are treated as a single topic for\nsemantic indexing. This study proposes a new method for the automated\nrefinement of subject annotations at the level of concepts, investigating deep\nlearning approaches. Lacking labelled data for this task, our method relies on\nweak supervision based on concept occurrence in the abstract of an article. The\nproposed approach is evaluated on an extended large-scale retrospective\nscenario, taking advantage of concepts that eventually become MeSH descriptors,\nfor which annotations become available in MEDLINE/PubMed. The results suggest\nthat concept occurrence is a strong heuristic for automated subject annotation\nrefinement and can be further enhanced when combined with dictionary-based\nheuristics. In addition, such heuristics can be useful as weak supervision for\ndeveloping deep learning models that can achieve further improvement in some\ncases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nentidis_A/0/1/0/all/0/1\">Anastasios Nentidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzopoulos_T/0/1/0/all/0/1\">Thomas Chatzopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krithara_A/0/1/0/all/0/1\">Anastasia Krithara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1\">Grigorios Tsoumakas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1\">Georgios Paliouras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMDDH: Singleton Mention detection using Deep Learning in Hindi Text. (arXiv:2301.09361v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09361","description":"<p>Mention detection is an important component of coreference resolution system,\nwhere mentions such as name, nominal, and pronominals are identified. These\nmentions can be purely coreferential mentions or singleton mentions\n(non-coreferential mentions). Coreferential mentions are those mentions in a\ntext that refer to the same entities in a real world. Whereas, singleton\nmentions are mentioned only once in the text and do not participate in the\ncoreference as they are not mentioned again in the following text. Filtering of\nthese singleton mentions can substantially improve the performance of a\ncoreference resolution process. This paper proposes a singleton mention\ndetection module based on a fully connected network and a Convolutional neural\nnetwork for Hindi text. This model utilizes a few hand-crafted features and\ncontext information, and word embedding for words. The coreference annotated\nHindi dataset comprising of 3.6K sentences, and 78K tokens are used for the\ntask. In terms of Precision, Recall, and F-measure, the experimental findings\nobtained are excellent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lata_K/0/1/0/all/0/1\">Kusum Lata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Pardeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_K/0/1/0/all/0/1\">Kamlesh Dutta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Mental Health Dialogue System. (arXiv:2301.09412v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09412","description":"<p>Mental health counseling remains a major challenge in modern society due to\ncost, stigma, fear, and unavailability. We posit that generative artificial\nintelligence (AI) models designed for mental health counseling could help\nimprove outcomes by lowering barriers to access. To this end, we have developed\na deep learning (DL) dialogue system called Serena. The system consists of a\ncore generative model and post-processing algorithms. The core generative model\nis a 2.7 billion parameter Seq2Seq Transformer fine-tuned on thousands of\ntranscripts of person-centered-therapy (PCT) sessions. The series of\npost-processing algorithms detects contradictions, improves coherency, and\nremoves repetitive answers. Serena is implemented and deployed on\n\\url{https://serena.chat}, which currently offers limited free services. While\nthe dialogue system is capable of responding in a qualitatively empathetic and\nengaging manner, occasionally it displays hallucination and long-term\nincoherence. Overall, we demonstrate that a deep learning mental health\ndialogue system has the potential to provide a low-cost and effective\ncomplement to traditional human counselors with less barriers to access.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brocki_L/0/1/0/all/0/1\">Lennart Brocki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyer_G/0/1/0/all/0/1\">George C. Dyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gladka_A/0/1/0/all/0/1\">Anna G&#x142;adka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_N/0/1/0/all/0/1\">Neo Christopher Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Energy Worker Profiler from Technologies to Skills to Realize Energy Efficiency in Manufacturing. (arXiv:2301.09445v1 [cs.CL])","link":"http://arxiv.org/abs/2301.09445","description":"<p>In recent years, the manufacturing sector has been responsible for nearly 55\npercent of total energy consumption, inducing a major impact on the global\necosystem. Although stricter regulations, restrictions on heavy manufacturing\nand technological advances are increasing its sustainability, zero-emission and\nfuel-efficient manufacturing is still considered a utopian target. In\nparallel,companies that have invested in digital innovation now need to align\ntheir internal competencies to maximize their return on investment. Moreover, a\nprimary feature of Industry 4.0 is the digitization of production processes,\nwhich offers the opportunity to optimize energy consumption. However, given the\nspeed with which innovation manifests itself, tools capable of measuring the\nimpact that technology is having on digital and green professions and skills\nare still being designed. In light of the above, in this article we present the\nWorker Profiler, a software designed to map the skills currently possessed by\nworkers, identifying misalignment with those they should ideally possess to\nmeet the renewed demands that digital innovation and environmental preservation\nimpose. The creation of the Worker Profiler consists of two steps: first, the\nauthors inferred the key technologies and skills for the area of interest,\nisolating those with markedly increasing patent trends and identifying green\nand digital enabling skills and occupations. Thus, the software was designed\nand implemented at the user-interface level. The output of the self-assessment\nis the definition of the missing digital and green skills and the job roles\nclosest to the starting one in terms of current skills; both the results enable\nthe definition of a customized retraining strategy. The tool has shown evidence\nof being user-friendly, effective in identifying skills gaps and easily\nadaptable to other contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fareri_S/0/1/0/all/0/1\">Silvia Fareri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apreda_R/0/1/0/all/0/1\">Riccardo Apreda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulas_V/0/1/0/all/0/1\">Valentina Mulas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_R/0/1/0/all/0/1\">Ruben Alonso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaQA: Combining Expert Agents for Multi-Skill Question Answering. (arXiv:2112.01922v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.01922","description":"<p>The recent explosion of question answering (QA) datasets and models has\nincreased the interest in the generalization of models across multiple domains\nand formats by either training on multiple datasets or by combining multiple\nmodels. Despite the promising results of multi-dataset models, some domains or\nQA formats may require specific architectures, and thus the adaptability of\nthese models might be limited. In addition, current approaches for combining\nmodels disregard cues such as question-answer compatibility. In this work, we\npropose to combine expert agents with a novel, flexible, and training-efficient\narchitecture that considers questions, answer predictions, and\nanswer-prediction confidence scores to select the best answer among a list of\nanswer candidates. Through quantitative and qualitative experiments we show\nthat our model i) creates a collaboration between agents that outperforms\nprevious multi-agent and multi-dataset approaches in both in-domain and\nout-of-domain scenarios, ii) is highly data-efficient to train, and iii) can be\nadapted to any QA format. We release our code and a dataset of answer\npredictions from expert agents for 16 QA datasets to foster future developments\nof multi-agent systems on https://github.com/UKPLab/MetaQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Puerto_H/0/1/0/all/0/1\">Haritz Puerto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahin_G/0/1/0/all/0/1\">G&#xf6;zde G&#xfc;l &#x15e;ahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Defeat of the Winograd Schema Challenge. (arXiv:2201.02387v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.02387","description":"<p>The Winograd Schema Challenge - a set of twin sentences involving pronoun\nreference disambiguation that seem to require the use of commonsense knowledge\n- was proposed by Hector Levesque in 2011. By 2019, a number of AI systems,\nbased on large pre-trained transformer-based language models and fine-tuned on\nthese kinds of problems, achieved better than 90% accuracy. In this paper, we\nreview the history of the Winograd Schema Challenge and discuss the lasting\ncontributions of the flurry of research that has taken place on the WSC in the\nlast decade. We discuss the significance of various datasets developed for WSC,\nand the research community's deeper understanding of the role of surrogate\ntasks in assessing the intelligence of an AI system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocijan_V/0/1/0/all/0/1\">Vid Kocijan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_E/0/1/0/all/0/1\">Ernest Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcus_G/0/1/0/all/0/1\">Gary Marcus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgenstern_L/0/1/0/all/0/1\">Leora Morgenstern</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometry-Aware Supertagging with Heterogeneous Dynamic Convolutions. (arXiv:2203.12235v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12235","description":"<p>The syntactic categories of categorial grammar formalisms are structured\nunits made of smaller, indivisible primitives, bound together by the underlying\ngrammar's category formation rules. In the trending approach of constructive\nsupertagging, neural models are increasingly made aware of the internal\ncategory structure, which in turn enables them to more reliably predict rare\nand out-of-vocabulary categories, with significant implications for grammars\npreviously deemed too complex to find practical use. In this work, we revisit\nconstructive supertagging from a graph-theoretic perspective, and propose a\nframework based on heterogeneous dynamic graph convolutions aimed at exploiting\nthe distinctive structure of a supertagger's output space. We test our approach\non a number of categorial grammar datasets spanning different languages and\ngrammar formalisms, achieving substantial improvements over previous state of\nthe art scores. Code will be made available at\nhttps://github.com/konstantinosKokos/dynamic-graph-supertagging\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kogkalidis_K/0/1/0/all/0/1\">Konstantinos Kogkalidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moortgat_M/0/1/0/all/0/1\">Michael Moortgat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting Pre-trained Language Models with QA-Memory for Open-Domain Question Answering. (arXiv:2204.04581v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04581","description":"<p>Retrieval augmented language models have recently become the standard for\nknowledge intensive tasks. Rather than relying purely on latent semantics\nwithin the parameters of large neural models, these methods enlist a\nsemi-parametric memory to encode an index of knowledge for the model to\nretrieve over. Most prior work has employed text passages as the unit of\nknowledge, which has high coverage at the cost of interpretability,\ncontrollability, and efficiency. The opposite properties arise in other methods\nwhich have instead relied on knowledge base (KB) facts. At the same time, more\nrecent work has demonstrated the effectiveness of storing and retrieving from\nan index of Q-A pairs derived from text \\citep{lewis2021paq}. This approach\nyields a high coverage knowledge representation that maintains KB-like\nproperties due to its representations being more atomic units of information.\nIn this work we push this line of research further by proposing a\nquestion-answer augmented encoder-decoder model and accompanying pretraining\nstrategy. This yields an end-to-end system that not only outperforms prior QA\nretrieval methods on single-hop QA tasks but also enables compositional\nreasoning, as demonstrated by strong performance on two multi-hop QA datasets.\nTogether, these methods improve the ability to interpret and control the model\nwhile narrowing the performance gap with passage retrieval systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verga_P/0/1/0/all/0/1\">Pat Verga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jong_M/0/1/0/all/0/1\">Michiel de Jong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieting_J/0/1/0/all/0/1\">John Wieting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASQA: Factoid Questions Meet Long-Form Answers. (arXiv:2204.06092v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06092","description":"<p>An abundance of datasets and availability of reliable evaluation metrics have\nresulted in strong progress in factoid question answering (QA). This progress,\nhowever, does not easily transfer to the task of long-form QA, where the goal\nis to answer questions that require in-depth explanations. The hurdles include\n(i) a lack of high-quality data, and (ii) the absence of a well-defined notion\nof the answer's quality. In this work, we address these problems by (i)\nreleasing a novel dataset and a task that we call ASQA (Answer Summaries for\nQuestions which are Ambiguous); and (ii) proposing a reliable metric for\nmeasuring performance on ASQA. Our task focuses on factoid questions that are\nambiguous, that is, have different correct answers depending on interpretation.\nAnswers to ambiguous questions should synthesize factual information from\nmultiple sources into a long-form summary that resolves the ambiguity. In\ncontrast to existing long-form QA tasks (such as ELI5), ASQA admits a clear\nnotion of correctness: a user faced with a good summary should be able to\nanswer different interpretations of the original ambiguous question. We use\nthis notion of correctness to define an automated metric of performance for\nASQA. Our analysis demonstrates an agreement between this metric and human\njudgments, and reveals a considerable gap between human performance and strong\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stelmakh_I/0/1/0/all/0/1\">Ivan Stelmakh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_Y/0/1/0/all/0/1\">Yi Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1\">Bhuwan Dhingra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BanglaNLG and BanglaT5: Benchmarks and Resources for Evaluating Low-Resource Natural Language Generation in Bangla. (arXiv:2205.11081v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11081","description":"<p>This work presents BanglaNLG, a comprehensive benchmark for evaluating\nnatural language generation (NLG) models in Bangla, a widely spoken yet\nlow-resource language. We aggregate six challenging conditional text generation\ntasks under the BanglaNLG benchmark, introducing a new dataset on dialogue\ngeneration in the process. Then, using a clean corpus of 27.5 GB of Bangla\ndata, we pretrain BanglaT5, a sequence-to-sequence Transformer model for\nBangla. BanglaT5 achieves state-of-the-art performance in all of these tasks,\noutperforming several multilingual models by up to 9% absolute gain and 32%\nrelative gain. We are making the new dataset, the BanglaT5 language model, and\na leaderboard publicly available at https://github.com/csebuetnlp/BanglaNLG in\nthe hope of advancing future research and evaluation on Bangla NLG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Abhik Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_T/0/1/0/all/0/1\">Tahmid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahriyar_R/0/1/0/all/0/1\">Rifat Shahriyar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear Connectivity Reveals Generalization Strategies. (arXiv:2205.12411v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.12411","description":"<p>It is widely accepted in the mode connectivity literature that when two\nneural networks are trained similarly on the same data, they are connected by a\npath through parameter space over which test set accuracy is maintained. Under\nsome circumstances, including transfer learning from pretrained models, these\npaths are presumed to be linear. In contrast to existing results, we find that\namong text classifiers (trained on MNLI, QQP, and CoLA), some pairs of\nfinetuned models have large barriers of increasing loss on the linear paths\nbetween them. On each task, we find distinct clusters of models which are\nlinearly connected on the test loss surface, but are disconnected from models\noutside the cluster -- models that occupy separate basins on the surface. By\nmeasuring performance on specially-crafted diagnostic datasets, we find that\nthese clusters correspond to different generalization strategies: one cluster\nbehaves like a bag of words model under domain shift, while another cluster\nuses syntactic heuristics. Our work demonstrates how the geometry of the loss\nsurface can guide models towards different heuristic functions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Juneja_J/0/1/0/all/0/1\">Jeevesh Juneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_R/0/1/0/all/0/1\">Rachit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1\">Naomi Saphra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Questions Are All You Need to Train a Dense Passage Retriever. (arXiv:2206.10658v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.10658","description":"<p>We introduce ART, a new corpus-level autoencoding approach for training dense\nretrieval models that does not require any labeled training data. Dense\nretrieval is a central challenge for open-domain tasks, such as Open QA, where\nstate-of-the-art methods typically require large supervised datasets with\ncustom hard-negative mining and denoising of positive examples. ART, in\ncontrast, only requires access to unpaired inputs and outputs (e.g. questions\nand potential answer documents). It uses a new document-retrieval autoencoding\nscheme, where (1) an input question is used to retrieve a set of evidence\ndocuments, and (2) the documents are then used to compute the probability of\nreconstructing the original question. Training for retrieval based on question\nreconstruction enables effective unsupervised learning of both document and\nquestion encoders, which can be later incorporated into complete Open QA\nsystems without any further finetuning. Extensive experiments demonstrate that\nART obtains state-of-the-art results on multiple QA retrieval benchmarks with\nonly generic initialization from a pre-trained language model, removing the\nneed for labeled data and task-specific losses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sachan_D/0/1/0/all/0/1\">Devendra Singh Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1\">Joelle Pineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Manzil Zaheer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embedding Recycling for Language Models. (arXiv:2207.04993v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.04993","description":"<p>Real-world applications of neural language models often involve running many\ndifferent models over the same corpus. The high computational cost of these\nruns has led to interest in techniques that can reuse the contextualized\nembeddings produced in previous runs to speed training and inference of future\nones. We refer to this approach as embedding recycling (ER). While multiple ER\ntechniques have been proposed, their practical effectiveness is still unknown\nbecause existing evaluations consider very few models and do not adequately\naccount for overhead costs. We perform an extensive evaluation of ER across\neight different models (17 to 900 million parameters) and fourteen tasks in\nEnglish. We show how a simple ER technique that caches activations from an\nintermediate layer of a pretrained model, and learns task-specific adapters on\nthe later layers, is broadly effective. For the best-performing baseline in our\nexperiments (DeBERTa-v2 XL), adding a precomputed cache results in a &gt;90%\nspeedup during training and 87-91% speedup for inference, with negligible\nimpact on accuracy. Our analysis reveals important areas of future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saad_Falcon_J/0/1/0/all/0/1\">Jon Saad-Falcon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amanpreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldaini_L/0/1/0/all/0/1\">Luca Soldaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DArcy_M/0/1/0/all/0/1\">Mike D&#x27;Arcy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Task-specific Concept Knowledge into Script Learning. (arXiv:2209.00068v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.00068","description":"<p>In this paper, we present Tetris, a new task of Goal-Oriented Script\nCompletion. Unlike previous work, it considers a more realistic and general\nsetting, where the input includes not only the goal but also additional user\ncontext, including preferences and history. To address this problem, we propose\na novel approach, which uses two techniques to improve performance: (1) concept\nprompting, and (2) script-oriented contrastive learning that addresses step\nrepetition and hallucination problems. On our WikiHow-based dataset, we find\nthat both methods improve performance. The dataset, repository, and models will\nbe publicly available to facilitate further research on this new task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chenkai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ji_H/0/1/0/all/0/1\">Heng ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visualize Before You Write: Imagination-Guided Open-Ended Text Generation. (arXiv:2210.03765v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03765","description":"<p>Recent advances in text-to-image synthesis make it possible to visualize\nmachine imaginations for a given context. On the other hand, when generating\ntext, human writers are gifted at creative visualization, which enhances their\nwritings by forming imaginations as blueprints before putting down the stories\nin words. Inspired by such a cognitive process, we ask the natural question of\nwhether we can endow machines with the same ability to utilize visual\ninformation and construct a general picture of the context to guide text\ngeneration. In this work, we propose iNLG that uses machine-generated images to\nguide language models in open-ended text generation. The experiments and\nanalyses demonstrate the effectiveness of iNLG on open-ended text generation\ntasks, including text completion, story generation, and concept-to-text\ngeneration in both few-shot and full-data scenarios. Both automatic metrics and\nhuman evaluations verify that the text snippets generated by our iNLG are\ncoherent and informative while displaying minor degeneration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_A/0/1/0/all/0/1\">An Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenda Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_M/0/1/0/all/0/1\">Miguel Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are few(1)-shot Table Reasoners. (arXiv:2210.06710v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06710","description":"<p>Recent literature has shown that large language models (LLMs) are generally\nexcellent few-shot reasoners to solve text reasoning tasks. However, the\ncapability of LLMs on table reasoning tasks is yet to be explored. In this\npaper, we aim at understanding how well LLMs can perform table-related tasks\nwith few-shot in-context learning. Specifically, we evaluated LLMs on popular\ntable QA and fact verification datasets like WikiTableQuestion, FetaQA,\nTabFact, and FEVEROUS and found that LLMs are competent at complex reasoning\nover table structures, though these models are not pre-trained on any table\ncorpus. When combined with `chain of thoughts' prompting, LLMs can achieve very\nstrong performance with only a 1-shot demonstration, even on par with some SoTA\nmodels. We show that LLMs are even more competent at generating comprehensive\nlong-form answers on FetaQA than tuned T5-large. We further manually studied\nthe reasoning chains elicited from LLMs and found that these reasoning chains\nare highly consistent with the underlying semantic form. We believe that LLMs\ncan serve as a simple yet generic baseline for future research. The code and\ndata are released in https://github.com/wenhuchen/TableCoT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cascading Biases: Investigating the Effect of Heuristic Annotation Strategies on Data and Models. (arXiv:2210.13439v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13439","description":"<p>Cognitive psychologists have documented that humans use cognitive heuristics,\nor mental shortcuts, to make quick decisions while expending less effort. While\nperforming annotation work on crowdsourcing platforms, we hypothesize that such\nheuristic use among annotators cascades on to data quality and model\nrobustness. In this work, we study cognitive heuristic use in the context of\nannotating multiple-choice reading comprehension datasets. We propose tracking\nannotator heuristic traces, where we tangibly measure low-effort annotation\nstrategies that could indicate usage of various cognitive heuristics. We find\nevidence that annotators might be using multiple such heuristics, based on\ncorrelations with a battery of psychological tests. Importantly, heuristic use\namong annotators determines data quality along several dimensions: (1) known\nbiased models, such as partial input models, more easily solve examples\nauthored by annotators that rate highly on heuristic use, (2) models trained on\nannotators scoring highly on heuristic use don't generalize as well, and (3)\nheuristic-seeking annotators tend to create qualitatively less challenging\nexamples. Our findings suggest that tracking heuristic usage among annotators\ncan potentially help with collecting challenging datasets and diagnosing model\nbiases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malaviya_C/0/1/0/all/0/1\">Chaitanya Malaviya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1\">Sudeep Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yatskar_M/0/1/0/all/0/1\">Mark Yatskar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrimTail: Low-Latency Streaming ASR with Simple but Effective Spectrogram-Level Length Penalty. (arXiv:2211.00522v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2211.00522","description":"<p>In this paper, we present TrimTail, a simple but effective emission\nregularization method to improve the latency of streaming ASR models. The core\nidea of TrimTail is to apply length penalty (i.e., by trimming trailing frames,\nsee Fig. 1-(b)) directly on the spectrogram of input utterances, which does not\nrequire any alignment. We demonstrate that TrimTail is computationally cheap\nand can be applied online and optimized with any training loss or any model\narchitecture on any dataset without any extra effort by applying it on various\nend-to-end streaming ASR networks either trained with CTC loss [1] or\nTransducer loss [2]. We achieve 100 $\\sim$ 200ms latency reduction with equal\nor even better accuracy on both Aishell-1 and Librispeech. Moreover, by using\nTrimTail, we can achieve a 400ms algorithmic improvement of User Sensitive\nDelay (USD) with an accuracy loss of less than 0.2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingchen Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuekai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhendong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Fuping Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Changbao Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReInform: Selecting paths with reinforcement learning for contextualized link prediction. (arXiv:2211.10688v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.10688","description":"<p>We propose to use reinforcement learning to inform transformer-based\ncontextualized link prediction models by providing paths that are most useful\nfor predicting the correct answer. This is in contrast to previous approaches,\nthat either used reinforcement learning (RL) to directly search for the answer,\nor based their prediction on limited or randomly selected context. Our\nexperiments on WN18RR and FB15k-237 show that contextualized link prediction\nmodels consistently outperform RL-based answer search, and that additional\nimprovements (of up to 13.5% MRR) can be gained by combining RL with a link\nprediction model. The PyTorch implementation of the RL agent is available at\nhttps://github.com/marina-sp/reinform\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Speranskaya_M/0/1/0/all/0/1\">Marina Speranskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Methias_S/0/1/0/all/0/1\">Sameh Methias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Talking About Large Language Models. (arXiv:2212.03551v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.03551","description":"<p>Thanks to rapid progress in artificial intelligence, we have entered an era\nwhen technology and philosophy intersect in interesting ways. Sitting squarely\nat the centre of this intersection are large language models (LLMs). The more\nadept LLMs become at mimicking human language, the more vulnerable we become to\nanthropomorphism, to seeing the systems in which they are embedded as more\nhuman-like than they really are. This trend is amplified by the natural\ntendency to use philosophically loaded terms, such as \"knows\", \"believes\", and\n\"thinks\", when describing these systems. To mitigate this trend, this paper\nadvocates the practice of repeatedly stepping back to remind ourselves of how\nLLMs, and the systems of which they form a part, actually work. The hope is\nthat increased scientific precision will encourage more philosophical nuance in\nthe discourse around artificial intelligence, both within the field and in the\npublic sphere.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shanahan_M/0/1/0/all/0/1\">Murray Shanahan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Infinite Index: Information Retrieval on Generative Text-To-Image Models. (arXiv:2212.07476v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2212.07476","description":"<p>Conditional generative models such as DALL-E and Stable Diffusion generate\nimages based on a user-defined text, the prompt. Finding and refining prompts\nthat produce a desired image has become the art of prompt engineering.\nGenerative models do not provide a built-in retrieval model for a user's\ninformation need expressed through prompts. In light of an extensive literature\nreview, we reframe prompt engineering for generative models as interactive\ntext-based retrieval on a novel kind of \"infinite index\". We apply these\ninsights for the first time in a case study on image generation for game design\nwith an expert. Finally, we envision how active learning may help to guide the\nretrieval of generated images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deckers_N/0/1/0/all/0/1\">Niklas Deckers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frobe_M/0/1/0/all/0/1\">Maik Fr&#xf6;be</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiesel_J/0/1/0/all/0/1\">Johannes Kiesel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandolfo_G/0/1/0/all/0/1\">Gianluca Pandolfo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroder_C/0/1/0/all/0/1\">Christopher Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1\">Benno Stein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP. (arXiv:2212.14024v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.14024","description":"<p>Retrieval-augmented in-context learning has emerged as a powerful approach\nfor addressing knowledge-intensive tasks using frozen language models (LM) and\nretrieval models (RM). Existing work has combined these in simple\n\"retrieve-then-read\" pipelines in which the RM retrieves passages that are\ninserted into the LM prompt. To begin to fully realize the potential of frozen\nLMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that\nrelies on passing natural language texts in sophisticated pipelines between an\nLM and an RM. DSP can express high-level programs that bootstrap pipeline-aware\ndemonstrations, search for relevant passages, and generate grounded\npredictions, systematically breaking down problems into small transformations\nthat the LM and RM can handle more reliably. We have written novel DSP programs\nfor answering questions in open-domain, multi-hop, and conversational settings,\nestablishing in early evaluations new state-of-the-art in-context learning\nresults and delivering 37-120%, 8-39%, and 80-290% relative gains against the\nvanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a\ncontemporaneous self-ask pipeline, respectively. We release DSP at\nhttps://github.com/stanfordnlp/dsp\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1\">Omar Khattab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santhanam_K/0/1/0/all/0/1\">Keshav Santhanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Lisa Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_D/0/1/0/all/0/1\">David Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keyword Embeddings for Query Suggestion. (arXiv:2301.08006v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2301.08006","description":"<p>Nowadays, search engine users commonly rely on query suggestions to improve\ntheir initial inputs. Current systems are very good at recommending lexical\nadaptations or spelling corrections to users' queries. However, they often\nstruggle to suggest semantically related keywords given a user's query. The\nconstruction of a detailed query is crucial in some tasks, such as legal\nretrieval or academic search. In these scenarios, keyword suggestion methods\nare critical to guide the user during the query formulation. This paper\nproposes two novel models for the keyword suggestion task trained on scientific\nliterature. Our techniques adapt the architecture of Word2Vec and FastText to\ngenerate keyword embeddings by leveraging documents' keyword co-occurrence.\nAlong with these models, we also present a specially tailored negative sampling\napproach that exploits how keywords appear in academic publications. We devise\na ranking-based evaluation methodology following both known-item and ad-hoc\nsearch scenarios. Finally, we evaluate our proposals against the\nstate-of-the-art word and sentence embedding models showing considerable\nimprovements over the baselines for the tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gabin_J/0/1/0/all/0/1\">Jorge Gab&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ares_M/0/1/0/all/0/1\">M. Eduardo Ares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parapar_J/0/1/0/all/0/1\">Javier Parapar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-01-23T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}