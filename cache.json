{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-03-21T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Conversational Tree Search: A New Hybrid Dialog Task. (arXiv:2303.10227v1 [cs.CL])","link":"http://arxiv.org/abs/2303.10227","description":"<p>Conversational interfaces provide a flexible and easy way for users to seek\ninformation that may otherwise be difficult or inconvenient to obtain. However,\nexisting interfaces generally fall into one of two categories: FAQs, where\nusers must have a concrete question in order to retrieve a general answer, or\ndialogs, where users must follow a predefined path but may receive a\npersonalized answer. In this paper, we introduce Conversational Tree Search\n(CTS) as a new task that bridges the gap between FAQ-style information\nretrieval and task-oriented dialog, allowing domain-experts to define dialog\ntrees which can then be converted to an efficient dialog policy that learns\nonly to ask the questions necessary to navigate a user to their goal. We\ncollect a dataset for the travel reimbursement domain and demonstrate a\nbaseline as well as a novel deep Reinforcement Learning architecture for this\ntask. Our results show that the new architecture combines the positive aspects\nof both the FAQ and dialog system used in the baseline and achieves higher goal\ncompletion while skipping unnecessary questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vath_D/0/1/0/all/0/1\">Dirk V&#xe4;th</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanderlyn_L/0/1/0/all/0/1\">Lindsey Vanderlyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feedback Effect in User Interaction with Intelligent Assistants: Delayed Engagement, Adaption and Drop-out. (arXiv:2303.10255v1 [cs.HC])","link":"http://arxiv.org/abs/2303.10255","description":"<p>With the growing popularity of intelligent assistants (IAs), evaluating IA\nquality becomes an increasingly active field of research. This paper identifies\nand quantifies the feedback effect, a novel component in IA-user interactions:\nhow the capabilities and limitations of the IA influence user behavior over\ntime. First, we demonstrate that unhelpful responses from the IA cause users to\ndelay or reduce subsequent interactions in the short term via an observational\nstudy. Next, we expand the time horizon to examine behavior changes and show\nthat as users discover the limitations of the IA's understanding and functional\ncapabilities, they learn to adjust the scope and wording of their requests to\nincrease the likelihood of receiving a helpful response from the IA. Our\nfindings highlight the impact of the feedback effect at both the micro and meso\nlevels. We further discuss its macro-level consequences: unsatisfactory\ninteractions continuously reduce the likelihood and diversity of future user\nengagements in a feedback loop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiu_Z/0/1/0/all/0/1\">Zidi Xiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kai-Chen Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">David Q. Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiannan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotek_H/0/1/0/all/0/1\">Hadas Kotek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCarthy_P/0/1/0/all/0/1\">Paul McCarthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_C/0/1/0/all/0/1\">Christopher Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulman_S/0/1/0/all/0/1\">Stephen Pulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1\">Jason D. Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the rise of fear speech in online social media. (arXiv:2303.10311v1 [cs.SI])","link":"http://arxiv.org/abs/2303.10311","description":"<p>Recently, social media platforms are heavily moderated to prevent the spread\nof online hate speech, which is usually fertile in toxic words and is directed\ntoward an individual or a community. Owing to such heavy moderation, newer and\nmore subtle techniques are being deployed. One of the most striking among these\nis fear speech. Fear speech, as the name suggests, attempts to incite fear\nabout a target community. Although subtle, it might be highly effective, often\npushing communities toward a physical conflict. Therefore, understanding their\nprevalence in social media is of paramount importance. This article presents a\nlarge-scale study to understand the prevalence of 400K fear speech and over\n700K hate speech posts collected from Gab.com. Remarkably, users posting a\nlarge number of fear speech accrue more followers and occupy more central\npositions in social networks than users posting a large number of hate speech.\nThey can also reach out to benign users more effectively than hate speech users\nthrough replies, reposts, and mentions. This connects to the fact that, unlike\nhate speech, fear speech has almost zero toxic content, making it look\nplausible. Moreover, while fear speech topics mostly portray a community as a\nperpetrator using a (fake) chain of argumentation, hate speech topics hurl\ndirect multitarget insults, thus pointing to why general users could be more\ngullible to fear speech. Our findings transcend even to other platforms\n(Twitter and Facebook) and thus necessitate using sophisticated moderation\npolicies and mass awareness to combat fear speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_P/0/1/0/all/0/1\">Punyajoy Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garimella_K/0/1/0/all/0/1\">Kiran Garimella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_N/0/1/0/all/0/1\">Narla Komal Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_S/0/1/0/all/0/1\">Saurabh Kumar Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meher_P/0/1/0/all/0/1\">Pauras Mangesh Meher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_B/0/1/0/all/0/1\">Binny Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Animesh Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Automatic Question Summarization Evaluation in the Biomedical Domain. (arXiv:2303.10328v1 [cs.CL])","link":"http://arxiv.org/abs/2303.10328","description":"<p>Automatic evaluation metrics have been facilitating the rapid development of\nautomatic summarization methods by providing instant and fair assessments of\nthe quality of summaries. Most metrics have been developed for the general\ndomain, especially news and meeting notes, or other language-generation tasks.\nHowever, these metrics are applied to evaluate summarization systems in\ndifferent domains, such as biomedical question summarization. To better\nunderstand whether commonly used evaluation metrics are capable of evaluating\nautomatic summarization in the biomedical domain, we conduct human evaluations\nof summarization quality from four different aspects of a biomedical question\nsummarization task. Based on human judgments, we identify different noteworthy\nfeatures for current automatic metrics and summarization systems as well. We\nalso release a dataset of our human annotations to aid the research of\nsummarization evaluation metrics in the biomedical domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongyi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaoyun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Partial Knowledge Base Inference in Biomedical Entity Linking. (arXiv:2303.10330v1 [cs.CL])","link":"http://arxiv.org/abs/2303.10330","description":"<p>Biomedical entity linking (EL) consists of named entity recognition (NER) and\nnamed entity disambiguation (NED). EL models are trained on corpora labeled by\na predefined KB. However, it is a common scenario that only entities within a\nsubset of the KB are precious to stakeholders. We name this scenario partial\nknowledge base inference: training an EL model with one KB and inferring on the\npart of it without further training. In this work, we give a detailed\ndefinition and evaluation procedures for this practically valuable but\nsignificantly understudied scenario and evaluate methods from three\nrepresentative EL paradigms. We construct partial KB inference benchmarks and\nwitness a catastrophic degradation in EL performance due to dramatically\nprecision drop. Our findings reveal these EL paradigms can not correctly handle\nunlinkable mentions (NIL), so they are not robust to partial KB inference. We\nalso propose two simple-and-effective redemption methods to combat the NIL\nissue with little computational overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongyi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Keming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of Pre-trained Language Models in Simple Knowledge Graph Question Answering. (arXiv:2303.10368v1 [cs.CL])","link":"http://arxiv.org/abs/2303.10368","description":"<p>Large-scale pre-trained language models (PLMs) such as BERT have recently\nachieved great success and become a milestone in natural language processing\n(NLP). It is now the consensus of the NLP community to adopt PLMs as the\nbackbone for downstream tasks. In recent works on knowledge graph question\nanswering (KGQA), BERT or its variants have become necessary in their KGQA\nmodels. However, there is still a lack of comprehensive research and comparison\nof the performance of different PLMs in KGQA. To this end, we summarize two\nbasic KGQA frameworks based on PLMs without additional neural network modules\nto compare the performance of nine PLMs in terms of accuracy and efficiency. In\naddition, we present three benchmarks for larger-scale KGs based on the popular\nSimpleQuestions benchmark to investigate the scalability of PLMs. We carefully\nanalyze the results of all PLMs-based KGQA basic frameworks on these benchmarks\nand two other popular datasets, WebQuestionSP and FreebaseQA, and find that\nknowledge distillation techniques and knowledge enhancement methods in PLMs are\npromising for KGQA. Furthermore, we test ChatGPT, which has drawn a great deal\nof attention in the NLP community, demonstrating its impressive capabilities\nand limitations in zero-shot KGQA. We have released the code and benchmarks to\npromote the use of PLMs on KGQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_N/0/1/0/all/0/1\">Nan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yike Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guilin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_D/0/1/0/all/0/1\">Dehai Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_Z/0/1/0/all/0/1\">Zafar Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Powerful and Extensible WFST Framework for RNN-Transducer Losses. (arXiv:2303.10384v1 [eess.AS])","link":"http://arxiv.org/abs/2303.10384","description":"<p>This paper presents a framework based on Weighted Finite-State Transducers\n(WFST) to simplify the development of modifications for RNN-Transducer (RNN-T)\nloss. Existing implementations of RNN-T use CUDA-related code, which is hard to\nextend and debug. WFSTs are easy to construct and extend, and allow debugging\nthrough visualization. We introduce two WFST-powered RNN-T implementations: (1)\n\"Compose-Transducer\", based on a composition of the WFST graphs from acoustic\nand textual schema -- computationally competitive and easy to modify; (2)\n\"Grid-Transducer\", which constructs the lattice directly for further\ncomputations -- most compact, and computationally efficient. We illustrate the\nease of extensibility through introduction of a new W-Transducer loss -- the\nadaptation of the Connectionist Temporal Classification with Wild Cards.\nW-Transducer (W-RNNT) consistently outperforms the standard RNN-T in a\nweakly-supervised data setup with missing parts of transcriptions at the\nbeginning and end of utterances. All RNN-T losses are implemented with the k2\nframework and are available in the NeMo toolkit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Laptev_A/0/1/0/all/0/1\">Aleksandr Laptev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bataev_V/0/1/0/all/0/1\">Vladimir Bataev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gitman_I/0/1/0/all/0/1\">Igor Gitman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Graph-Guided Reasoning Approach for Open-ended Commonsense Question Answering. (arXiv:2303.10395v1 [cs.CL])","link":"http://arxiv.org/abs/2303.10395","description":"<p>Recently, end-to-end trained models for multiple-choice commonsense question\nanswering (QA) have delivered promising results. However, such\nquestion-answering systems cannot be directly applied in real-world scenarios\nwhere answer candidates are not provided. Hence, a new benchmark challenge set\nfor open-ended commonsense reasoning (OpenCSR) has been recently released,\nwhich contains natural science questions without any predefined choices. On the\nOpenCSR challenge set, many questions require implicit multi-hop reasoning and\nhave a large decision space, reflecting the difficult nature of this task.\nExisting work on OpenCSR sorely focuses on improving the retrieval process,\nwhich extracts relevant factual sentences from a textual knowledge base,\nleaving the important and non-trivial reasoning task outside the scope. In this\nwork, we extend the scope to include a reasoner that constructs a\nquestion-dependent open knowledge graph based on retrieved supporting facts and\nemploys a sequential subgraph reasoning process to predict the answer. The\nsubgraph can be seen as a concise and compact graphical explanation of the\nprediction. Experiments on two OpenCSR datasets show that the proposed model\nachieves great performance on benchmark OpenCSR datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhen Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yue Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mingming Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models. (arXiv:2303.10420v1 [cs.CL])","link":"http://arxiv.org/abs/2303.10420","description":"<p>GPT series models, such as GPT-3, CodeX, InstructGPT, ChatGPT, and so on,\nhave gained considerable attention due to their exceptional natural language\nprocessing capabilities. However, despite the abundance of research on the\ndifference in capabilities between GPT series models and fine-tuned models,\nthere has been limited attention given to the evolution of GPT series models'\ncapabilities over time. To conduct a comprehensive analysis of the capabilities\nof GPT series models, we select six representative models, comprising two GPT-3\nseries models (i.e., davinci and text-davinci-001) and four GPT-3.5 series\nmodels (i.e., code-davinci-002, text-davinci-002, text-davinci-003, and\ngpt-3.5-turbo). We evaluate their performance on nine natural language\nunderstanding (NLU) tasks using 21 datasets. In particular, we compare the\nperformance and robustness of different models for each task under zero-shot\nand few-shot scenarios. Our extensive experiments reveal that the overall\nability of GPT series models on NLU tasks does not increase gradually as the\nmodels evolve, especially with the introduction of the RLHF training strategy.\nWhile this strategy enhances the models' ability to generate human-like\nresponses, it also compromises their ability to solve some tasks. Furthermore,\nour findings indicate that there is still room for improvement in areas such as\nmodel robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuanting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Nuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zu_C/0/1/0/all/0/1\">Can Zu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zekai Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shichun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yuhan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zeyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NoisyHate: Benchmarking Content Moderation Machine Learning Models with Human-Written Perturbations Online. (arXiv:2303.10430v1 [cs.LG])","link":"http://arxiv.org/abs/2303.10430","description":"<p>Online texts with toxic content are a threat in social media that might cause\ncyber harassment. Although many platforms applied measures, such as machine\nlearning-based hate-speech detection systems, to diminish their effect, those\ntoxic content publishers can still evade the system by modifying the spelling\nof toxic words. Those modified words are also known as human-written text\nperturbations. Many research works developed certain techniques to generate\nadversarial samples to help the machine learning models obtain the ability to\nrecognize those perturbations. However, there is still a gap between those\nmachine-generated perturbations and human-written perturbations. In this paper,\nwe introduce a benchmark test set containing human-written perturbations online\nfor toxic speech detection models. We also recruited a group of workers to\nevaluate the quality of this test set and dropped low-quality samples.\nMeanwhile, to check if our perturbation can be normalized to its clean version,\nwe applied spell corrector algorithms on this dataset. Finally, we test this\ndata on state-of-the-art language models, such as BERT and RoBERTa, and black\nbox APIs, such as perspective API, to demonstrate the adversarial attack with\nreal human-written perturbations is still effective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yiran Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thai Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongwon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stop Words for Processing Software Engineering Documents: Do they Matter?. (arXiv:2303.10439v1 [cs.SE])","link":"http://arxiv.org/abs/2303.10439","description":"<p>Stop words, which are considered non-predictive, are often eliminated in\nnatural language processing tasks. However, the definition of uninformative\nvocabulary is vague, so most algorithms use general knowledge-based stop lists\nto remove stop words. There is an ongoing debate among academics about the\nusefulness of stop word elimination, especially in domain-specific settings. In\nthis work, we investigate the usefulness of stop word removal in a software\nengineering context. To do this, we replicate and experiment with three\nsoftware engineering research tools from related work. Additionally, we\nconstruct a corpus of software engineering domain-related text from 10,000\nStack Overflow questions and identify 200 domain-specific stop words using\ntraditional information-theoretic methods. Our results show that the use of\ndomain-specific stop words significantly improved the performance of research\ntools compared to the use of a general stop list and that 17 out of 19\nevaluation measures showed better performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yaohou Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1\">Chetan Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treude_C/0/1/0/all/0/1\">Christoph Treude</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GazeReader: Detecting Unknown Word Using Webcam for English as a Second Language (ESL) Learners. (arXiv:2303.10443v1 [cs.HC])","link":"http://arxiv.org/abs/2303.10443","description":"<p>Automatic unknown word detection techniques can enable new applications for\nassisting English as a Second Language (ESL) learners, thus improving their\nreading experiences. However, most modern unknown word detection methods\nrequire dedicated eye-tracking devices with high precision that are not easily\naccessible to end-users. In this work, we propose GazeReader, an unknown word\ndetection method only using a webcam. GazeReader tracks the learner's gaze and\nthen applies a transformer-based machine learning model that encodes the text\ninformation to locate the unknown word. We applied knowledge enhancement\nincluding term frequency, part of speech, and named entity recognition to\nimprove the performance. The user study indicates that the accuracy and\nF1-score of our method were 98.09% and 75.73%, respectively. Lastly, we\nexplored the design scope for ESL reading and discussed the findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jiexin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bowen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuntao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuanchun Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models. (arXiv:2303.10464v1 [cs.LG])","link":"http://arxiv.org/abs/2303.10464","description":"<p>The pre-training and fine-tuning paradigm has contributed to a number of\nbreakthroughs in Natural Language Processing (NLP). Instead of directly\ntraining on a downstream task, language models are first pre-trained on large\ndatasets with cross-domain knowledge (e.g., Pile, MassiveText, etc.) and then\nfine-tuned on task-specific data (e.g., natural language generation, text\nsummarization, etc.). Scaling the model and dataset size has helped improve the\nperformance of LLMs, but unfortunately, this also leads to highly prohibitive\ncomputational costs. Pre-training LLMs often require orders of magnitude more\nFLOPs than fine-tuning and the model capacity often remains the same between\nthe two phases. To achieve training efficiency w.r.t training FLOPs, we propose\nto decouple the model capacity between the two phases and introduce Sparse\nPre-training and Dense Fine-tuning (SPDF). In this work, we show the benefits\nof using unstructured weight sparsity to train only a subset of weights during\npre-training (Sparse Pre-training) and then recover the representational\ncapacity by allowing the zeroed weights to learn (Dense Fine-tuning). We\ndemonstrate that we can induce up to 75% sparsity into a 1.3B parameter GPT-3\nXL model resulting in a 2.5x reduction in pre-training FLOPs, without a\nsignificant loss in accuracy on the downstream tasks relative to the dense\nbaseline. By rigorously evaluating multiple downstream tasks, we also establish\na relationship between sparsity, task complexity, and dataset size. Our work\npresents a promising direction to train large GPT models at a fraction of the\ntraining FLOPs using weight sparsity while retaining the benefits of\npre-trained textual representations for downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thangarasa_V/0/1/0/all/0/1\">Vithursan Thangarasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshall_W/0/1/0/all/0/1\">William Marshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leong_K/0/1/0/all/0/1\">Kevin Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeCoste_D/0/1/0/all/0/1\">Dennis DeCoste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lie_S/0/1/0/all/0/1\">Sean Lie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_S/0/1/0/all/0/1\">Shreyas Saxena</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning. (arXiv:2303.10475v1 [cs.CL])","link":"http://arxiv.org/abs/2303.10475","description":"<p>Task semantics can be expressed by a set of input-to-output examples or a\npiece of textual instruction. Conventional machine learning approaches for\nnatural language processing (NLP) mainly rely on the availability of\nlarge-scale sets of task-specific examples. Two issues arise: first, collecting\ntask-specific labeled examples does not apply to scenarios where tasks may be\ntoo complicated or costly to annotate, or the system is required to handle a\nnew task immediately; second, this is not user-friendly since end-users are\nprobably more willing to provide task description rather than a set of examples\nbefore using the system. Therefore, the community is paying increasing interest\nin a new supervision-seeking paradigm for NLP: learning from task instructions.\nDespite its impressive progress, there are some common issues that the\ncommunity struggles with. This survey paper tries to summarize the current\nresearch on instruction learning, particularly, by answering the following\nquestions: (i) what is task instruction, and what instruction types exist? (ii)\nhow to model instructions? (iii) what factors influence and explain the\ninstructions' performance? (iv) what challenges remain in instruction learning?\nTo our knowledge, this is the first comprehensive survey about textual\ninstructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_R/0/1/0/all/0/1\">Renze Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenpeng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Learning System for Domain-specific speech Recognition. (arXiv:2303.10510v1 [cs.CL])","link":"http://arxiv.org/abs/2303.10510","description":"<p>As human-machine voice interfaces provide easy access to increasingly\nintelligent machines, many state-of-the-art automatic speech recognition (ASR)\nsystems are proposed. However, commercial ASR systems usually have poor\nperformance on domain-specific speech especially under low-resource settings.\nThe author works with pre-trained DeepSpeech2 and Wav2Vec2 acoustic models to\ndevelop benefit-specific ASR systems. The domain-specific data are collected\nusing proposed semi-supervised learning annotation with little human\nintervention. The best performance comes from a fine-tuned Wav2Vec2-Large-LV60\nacoustic model with an external KenLM, which surpasses the Google and AWS ASR\nsystems on benefit-specific speech. The viability of using error prone ASR\ntranscriptions as part of spoken language understanding (SLU) is also\ninvestigated. Results of a benefit-specific natural language understanding\n(NLU) task show that the domain-specific fine-tuned ASR system can outperform\nthe commercial ASR systems even when its transcriptions have higher word error\nrate (WER), and the results between fine-tuned ASR and human transcriptions are\nsimilar.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yanan Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning. (arXiv:2303.10512v1 [cs.CL])","link":"http://arxiv.org/abs/2303.10512","description":"<p>Fine-tuning large pre-trained language models on downstream tasks has become\nan important paradigm in NLP. However, common practice fine-tunes all of the\nparameters in a pre-trained model, which becomes prohibitive when a large\nnumber of downstream tasks are present. Therefore, many fine-tuning methods are\nproposed to learn incremental updates of pre-trained weights in a parameter\nefficient way, e.g., low-rank increments. These methods often evenly distribute\nthe budget of incremental updates across all pre-trained weight matrices, and\noverlook the varying importance of different weight parameters. As a\nconsequence, the fine-tuning performance is suboptimal. To bridge this gap, we\npropose AdaLoRA, which adaptively allocates the parameter budget among weight\nmatrices according to their importance score. In particular, AdaLoRA\nparameterizes the incremental updates in the form of singular value\ndecomposition. Such a novel approach allows us to effectively prune the\nsingular values of unimportant updates, which is essentially to reduce their\nparameter budget but circumvent intensive exact SVD computations. We conduct\nextensive experiments with several pre-trained models on natural language\nprocessing, question answering, and natural language generation to validate the\neffectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable\nimprovement over baselines, especially in the low budget settings. Our code is\npublicly available at https://github.com/QingruZhang/AdaLoRA .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Minshuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bukharin_A/0/1/0/all/0/1\">Alexander Bukharin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two Kinds of Recall. (arXiv:2303.10527v1 [cs.CL])","link":"http://arxiv.org/abs/2303.10527","description":"<p>It is an established assumption that pattern-based models are good at\nprecision, while learning based models are better at recall. But is that really\nthe case? I argue that there are two kinds of recall: d-recall, reflecting\ndiversity, and e-recall, reflecting exhaustiveness. I demonstrate through\nexperiments that while neural methods are indeed significantly better at\nd-recall, it is sometimes the case that pattern-based methods are still\nsubstantially better at e-recall. Ideal methods should aim for both kinds, and\nthis ideal should in turn be reflected in our evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How People Respond to the COVID-19 Pandemic on Twitter: A Comparative Analysis of Emotional Expressions from US and India. (arXiv:2303.10560v1 [cs.CL])","link":"http://arxiv.org/abs/2303.10560","description":"<p>The COVID-19 pandemic has claimed millions of lives worldwide and elicited\nheightened emotions. This study examines the expression of various emotions\npertaining to COVID-19 in the United States and India as manifested in over 54\nmillion tweets, covering the fifteen-month period from February 2020 through\nApril 2021, a period which includes the beginnings of the huge and disastrous\nincrease in COVID-19 cases that started to ravage India in March 2021.\nEmploying pre-trained emotion analysis and topic modeling algorithms, four\ndistinct types of emotions (fear, anger, happiness, and sadness) and their\ntime- and location-associated variations were examined. Results revealed\nsignificant country differences and temporal changes in the relative\nproportions of fear, anger, and happiness, with fear declining and anger and\nhappiness fluctuating in 2020 until new situations over the first four months\nof 2021 reversed the trends. Detected differences are discussed briefly in\nterms of the latent topics revealed and through the lens of appraisal theories\nof emotions, and the implications of the findings are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loh_B/0/1/0/all/0/1\">Brandon Siyuan Loh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Raj Kumar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vishwanath_A/0/1/0/all/0/1\">Ajay Vishwanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortony_A/0/1/0/all/0/1\">Andrew Ortony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinping Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Incidents, Effects, and Requested Advice from MeToo Posts. (arXiv:2303.10573v1 [cs.CL])","link":"http://arxiv.org/abs/2303.10573","description":"<p>Survivors of sexual harassment frequently share their experiences on social\nmedia, revealing their feelings and emotions and seeking advice. We observed\nthat on Reddit, survivors regularly share long posts that describe a\ncombination of (i) a sexual harassment incident, (ii) its effect on the\nsurvivor, including their feelings and emotions, and (iii) the advice being\nsought. We term such posts MeToo posts, even though they may not be so tagged\nand may appear in diverse subreddits. A prospective helper (such as a counselor\nor even a casual reader) must understand a survivor's needs from such posts.\nBut long posts can be time-consuming to read and respond to.\n</p>\n<p>Accordingly, we address the problem of extracting key information from a long\nMeToo post. We develop a natural language-based model to identify sentences\nfrom a post that describe any of the above three categories.\n</p>\n<p>On ten-fold cross-validation of a dataset, our model achieves a macro F1\nscore of 0.82.\n</p>\n<p>In addition, we contribute MeThree, a dataset comprising 8,947 labeled\nsentences extracted from Reddit posts. We apply the LIWC-22 toolkit on MeThree\nto understand how different language patterns in sentences of the three\ncategories can reveal differences in emotional tone, authenticity, and other\naspects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_V/0/1/0/all/0/1\">Vaibhav Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jiaqing Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_R/0/1/0/all/0/1\">Rujie Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Munindar P. Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Artificial Empathy for Human-Centered Design: A Framework. (arXiv:2303.10583v1 [cs.HC])","link":"http://arxiv.org/abs/2303.10583","description":"<p>In the early stages of the design process, designers explore opportunities by\ndiscovering unmet needs and developing innovative concepts as potential\nsolutions. From a human-centered design perspective, designers must develop\nempathy with people to truly understand their needs. However, developing\nempathy is a complex and subjective process that relies heavily on the\ndesigner's empathetic capability. Therefore, the development of empathetic\nunderstanding is intuitive, and the discovery of underlying needs is often\nserendipitous. This paper aims to provide insights from artificial intelligence\nresearch to indicate the future direction of AI-driven human-centered design,\ntaking into account the essential role of empathy. Specifically, we conduct an\ninterdisciplinary investigation of research areas such as data-driven user\nstudies, empathetic understanding development, and artificial empathy. Based on\nthis foundation, we discuss the role that artificial empathy can play in\nhuman-centered design and propose an artificial empathy framework for\nhuman-centered design. Building on the mechanisms behind empathy and insights\nfrom empathetic design research, the framework aims to break down the rather\ncomplex and subjective concept of empathy into components and modules that can\npotentially be modeled computationally. Furthermore, we discuss the expected\nbenefits of developing such systems and identify current research gaps to\nencourage future research efforts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jianxi Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CTRAN: CNN-Transformer-based Network for Natural Language Understanding. (arXiv:2303.10606v1 [cs.CL])","link":"http://arxiv.org/abs/2303.10606","description":"<p>Intent-detection and slot-filling are the two main tasks in natural language\nunderstanding. In this study, we propose CTRAN, a novel encoder-decoder\nCNN-Transformer-based architecture for intent-detection and slot-filling. In\nthe encoder, we use BERT, followed by several convolutional layers, and\nrearrange the output using window feature sequence. We use stacked Transformer\nencoders after the window feature sequence. For the intent-detection decoder,\nwe utilize self-attention followed by a linear layer. In the slot-filling\ndecoder, we introduce the aligned Transformer decoder, which utilizes a zero\ndiagonal mask, aligning output tags with input tokens. We apply our network on\nATIS and SNIPS, and surpass the current state-of-the-art in slot-filling on\nboth datasets. Furthermore, we incorporate the language model as word\nembeddings, and show that this strategy yields a better result when compared to\nthe language model as an encoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rafiepour_M/0/1/0/all/0/1\">Mehrdad Rafiepour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sartakhti_J/0/1/0/all/0/1\">Javad Salimi Sartakhti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bangla Grammatical Error Detection Using T5 Transformer Model. (arXiv:2303.10612v1 [cs.CL])","link":"http://arxiv.org/abs/2303.10612","description":"<p>This paper presents a method for detecting grammatical errors in Bangla using\na Text-to-Text Transfer Transformer (T5) Language Model, using the small\nvariant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were\nbracketed by the dedicated demarcation symbol. The T5 model was primarily\ndesigned for translation and is not specifically designed for this task, so\nextensive post-processing was necessary to adapt it to the task of error\ndetection. Our experiments show that the T5 model can achieve low Levenshtein\nDistance in detecting grammatical errors in Bangla, but post-processing is\nessential to achieve optimal performance. The final average Levenshtein\nDistance after post-processing the output of the fine-tuned model was 1.0394 on\na test set of 5000 sentences. This paper also presents a detailed analysis of\nthe errors detected by the model and discusses the challenges of adapting a\ntranslation model for grammar. Our approach can be extended to other languages,\ndemonstrating the potential of T5 models for detecting grammatical errors in a\nwide range of languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahgir_H/0/1/0/all/0/1\">H.A.Z. Sameen Shahgir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayeed_K/0/1/0/all/0/1\">Khondker Salman Sayeed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Makes Sentences Semantically Related: A Textual Relatedness Dataset and Empirical Study. (arXiv:2110.04845v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04845","description":"<p>The degree of semantic relatedness of two units of language has long been\nconsidered fundamental to understanding meaning. Additionally, automatically\ndetermining relatedness has many applications such as question answering and\nsummarization. However, prior NLP work has largely focused on semantic\nsimilarity, a subset of relatedness, because of a lack of relatedness datasets.\nIn this paper, we introduce a dataset for Semantic Textual Relatedness,\nSTR-2022, that has 5,500 English sentence pairs manually annotated using a\ncomparative annotation framework, resulting in fine-grained scores. We show\nthat human intuition regarding relatedness of sentence pairs is highly\nreliable, with a repeat annotation correlation of 0.84. We use the dataset to\nexplore questions on what makes sentences semantically related. We also show\nthe utility of STR-2022 for evaluating automatic methods of sentence\nrepresentation and for various downstream NLP tasks.\n</p>\n<p>Our dataset, data statement, and annotation questionnaire can be found at:\nhttps://doi.org/10.5281/zenodo.7599667\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdalla_M/0/1/0/all/0/1\">Mohamed Abdalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vishnubhotla_K/0/1/0/all/0/1\">Krishnapriya Vishnubhotla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning for Event Extraction with Memory-based Loss Prediction Model. (arXiv:2112.03073v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.03073","description":"<p>Event extraction (EE) plays an important role in many industrial application\nscenarios, and high-quality EE methods require a large amount of manual\nannotation data to train supervised learning models. However, the cost of\nobtaining annotation data is very high, especially for annotation of domain\nevents, which requires the participation of experts from corresponding domain.\nSo we introduce active learning (AL) technology to reduce the cost of event\nannotation. But the existing AL methods have two main problems, which make them\nnot well used for event extraction. Firstly, the existing pool-based selection\nstrategies have limitations in terms of computational cost and sample validity.\nSecondly, the existing evaluation of sample importance lacks the use of local\nsample information. In this paper, we present a novel deep AL method for EE. We\npropose a batch-based selection strategy and a Memory-Based Loss Prediction\nmodel (MBLP) to select unlabeled samples efficiently. During the selection\nprocess, we use an internal-external sample loss ranking method to evaluate the\nsample importance by using local information. Finally, we propose a delayed\ntraining strategy to train the MBLP model. Extensive experiments are performed\non three domain datasets, and our method outperforms other state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Shirong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guilin Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Semantic Embeddings for Ontology Subsumption Prediction. (arXiv:2202.09791v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2202.09791","description":"<p>Automating ontology construction and curation is an important but challenging\ntask in knowledge engineering and artificial intelligence. Prediction by\nmachine learning techniques such as contextual semantic embedding is a\npromising direction, but the relevant research is still preliminary especially\nfor expressive ontologies in Web Ontology Language (OWL). In this paper, we\npresent a new subsumption prediction method named BERTSubs for classes of OWL\nontology. It exploits the pre-trained language model BERT to compute contextual\nembeddings of a class, where customized templates are proposed to incorporate\nthe class context (e.g., neighbouring classes) and the logical existential\nrestriction. BERTSubs is able to predict multiple kinds of subsumers including\nnamed classes from the same ontology or another ontology, and existential\nrestrictions from the same ontology. Extensive evaluation on five real-world\nontologies for three different subsumption tasks has shown the effectiveness of\nthe templates and that BERTSubs can dramatically outperform the baselines that\nuse (literal-aware) knowledge graph embeddings, non-contextual word embeddings\nand the state-of-the-art OWL ontology embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1\">Yuxia Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jimenez_Ruiz_E/0/1/0/all/0/1\">Ernesto Jimenez-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horrocks_I/0/1/0/all/0/1\">Ian Horrocks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NELA-GT-2022: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles. (arXiv:2203.05659v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.05659","description":"<p>In this paper, we present the fifth installment of the NELA-GT datasets,\nNELA-GT-2022. The dataset contains 1,778,361 articles from 361 outlets between\nJanuary 1st, 2022 and December 31st, 2022. Just as in past releases of the\ndataset, NELA-GT-2022 includes outlet-level veracity labels from Media\nBias/Fact Check and tweets embedded in collected news articles. The\nNELA-GT-2022 dataset can be found at: https://doi.org/10.7910/DVN/AMCV2H\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gruppi_M/0/1/0/all/0/1\">Maur&#xed;cio Gruppi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horne_B/0/1/0/all/0/1\">Benjamin D. Horne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adali_S/0/1/0/all/0/1\">Sibel Adal&#x131;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward a realistic model of speech processing in the brain with self-supervised learning. (arXiv:2206.01685v2 [q-bio.NC] UPDATED)","link":"http://arxiv.org/abs/2206.01685","description":"<p>Several deep neural networks have recently been shown to generate activations\nsimilar to those of the brain in response to the same input. These algorithms,\nhowever, remain largely implausible: they require (1) extraordinarily large\namounts of data, (2) unobtainable supervised labels, (3) textual rather than\nraw sensory input, and / or (4) implausibly large memory (e.g. thousands of\ncontextual words). These elements highlight the need to identify algorithms\nthat, under these limitations, would suffice to account for both behavioral and\nbrain responses. Focusing on the issue of speech processing, we here\nhypothesize that self-supervised algorithms trained on the raw waveform\nconstitute a promising candidate. Specifically, we compare a recent\nself-supervised architecture, Wav2Vec 2.0, to the brain activity of 412\nEnglish, French, and Mandarin individuals recorded with functional Magnetic\nResonance Imaging (fMRI), while they listened to ~1h of audio books. Our\nresults are four-fold. First, we show that this algorithm learns brain-like\nrepresentations with as little as 600 hours of unlabelled speech -- a quantity\ncomparable to what infants can be exposed to during language acquisition.\nSecond, its functional hierarchy aligns with the cortical hierarchy of speech\nprocessing. Third, different training regimes reveal a functional\nspecialization akin to the cortex: Wav2Vec 2.0 learns sound-generic,\nspeech-specific and language-specific representations similar to those of the\nprefrontal and temporal cortices. Fourth, we confirm the similarity of this\nspecialization with the behavior of 386 additional participants. These\nelements, resulting from the largest neuroimaging benchmark to date, show how\nself-supervised learning can account for a rich organization of speech\nprocessing in the brain, and thus delineate a path to identify the laws of\nlanguage acquisition which shape the human brain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Millet_J/0/1/0/all/0/1\">Juliette Millet</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Caucheteux_C/0/1/0/all/0/1\">Charlotte Caucheteux</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Orhan_P/0/1/0/all/0/1\">Pierre Orhan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Boubenec_Y/0/1/0/all/0/1\">Yves Boubenec</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gramfort_A/0/1/0/all/0/1\">Alexandre Gramfort</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Dunbar_E/0/1/0/all/0/1\">Ewan Dunbar</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Pallier_C/0/1/0/all/0/1\">Christophe Pallier</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+King_J/0/1/0/all/0/1\">Jean-Remi King</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Maximum Linear Arrangement Problem for trees under projectivity and planarity. (arXiv:2206.06924v4 [cs.DS] UPDATED)","link":"http://arxiv.org/abs/2206.06924","description":"<p>A linear arrangement is a mapping $\\pi$ from the $n$ vertices of a graph $G$\nto $n$ distinct consecutive integers. Linear arrangements can be represented by\ndrawing the vertices along a horizontal line and drawing the edges as\nsemicircles above said line. In this setting, the length of an edge is defined\nas the absolute value of the difference between the positions of its two\nvertices in the arrangement, and the cost of an arrangement as the sum of all\nedge lengths. Here we study two variants of the Maximum Linear Arrangement\nproblem (MaxLA), which consists of finding an arrangement that maximizes the\ncost. In the planar variant for free trees, vertices have to be arranged in\nsuch a way that there are no edge crossings. In the projective variant for\nrooted trees, arrangements have to be planar and the root of the tree cannot be\ncovered by any edge. In this paper we present algorithms that are linear in\ntime and space to solve planar and projective MaxLA for trees. We also prove\nseveral properties of maximum projective and planar arrangements, and show that\ncaterpillar trees maximize planar MaxLA over all trees of a fixed size thereby\ngeneralizing a previous extremal result on trees.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alemany_Puig_L/0/1/0/all/0/1\">Llu&#xed;s Alemany-Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteban_J/0/1/0/all/0/1\">Juan Luis Esteban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraphCFC: A Directed Graph based Cross-modal Feature Complementation Approach for Multimodal Conversational Emotion Recognition. (arXiv:2207.12261v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.12261","description":"<p>Emotion Recognition in Conversation (ERC) plays a significant part in\nHuman-Computer Interaction (HCI) systems since it can provide empathetic\nservices. Multimodal ERC can mitigate the drawbacks of uni-modal approaches.\nRecently, Graph Neural Networks (GNNs) have been widely used in a variety of\nfields due to their superior performance in relation modeling. In multimodal\nERC, GNNs are capable of extracting both long-distance contextual information\nand inter-modal interactive information. Unfortunately, since existing methods\nsuch as MMGCN directly fuse multiple modalities, redundant information may be\ngenerated and diverse information may be lost. In this work, we present a\ndirected Graph based Cross-modal Feature Complementation (GraphCFC) module that\ncan efficiently model contextual and interactive information. GraphCFC\nalleviates the problem of heterogeneity gap in multimodal fusion by utilizing\nmultiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC)\nstrategy. We extract various types of edges from the constructed graph for\nencoding, thus enabling GNNs to extract crucial contextual and interactive\ninformation more accurately when performing message passing. Furthermore, we\ndesign a GNN structure called GAT-MLP, which can provide a new unified network\nframework for multimodal learning. The experimental results on two benchmark\ndatasets show that our GraphCFC outperforms the state-of-the-art (SOTA)\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_G/0/1/0/all/0/1\">Guoqing Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhigang Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EcoFormer: Energy-Saving Attention with Linear Complexity. (arXiv:2209.09004v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2209.09004","description":"<p>Transformer is a transformative framework that models sequential data and has\nachieved remarkable performance on a wide range of tasks, but with high\ncomputational and energy cost. To improve its efficiency, a popular choice is\nto compress the models via binarization which constrains the floating-point\nvalues into binary ones to save resource consumption owing to cheap bitwise\noperations significantly. However, existing binarization methods only aim at\nminimizing the information loss for the input distribution statistically, while\nignoring the pairwise similarity modeling at the core of the attention. To this\nend, we propose a new binarization paradigm customized to high-dimensional\nsoftmax attention via kernelized hashing, called EcoFormer, to map the original\nqueries and keys into low-dimensional binary codes in Hamming space. The\nkernelized hash functions are learned to match the ground-truth similarity\nrelations extracted from the attention map in a self-supervised way. Based on\nthe equivalence between the inner product of binary codes and the Hamming\ndistance as well as the associative property of matrix multiplication, we can\napproximate the attention in linear complexity by expressing it as a\ndot-product of binary codes. Moreover, the compact binary representations of\nqueries and keys enable us to replace most of the expensive multiply-accumulate\noperations in attention with simple accumulations to save considerable on-chip\nenergy footprint on edge devices. Extensive experiments on both vision and\nlanguage tasks show that EcoFormer consistently achieves comparable performance\nwith standard attentions while consuming much fewer resources. For example,\nbased on PVTv2-B0 and ImageNet-1K, Ecoformer achieves a 73% on-chip energy\nfootprint reduction with only a 0.33% performance drop compared to the standard\nattention. Code is available at https://github.com/ziplab/EcoFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zizheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Haoyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Construction and Applications of Billion-Scale Pre-Trained Multimodal Business Knowledge Graph. (arXiv:2209.15214v6 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2209.15214","description":"<p>Business Knowledge Graphs (KGs) are important to many enterprises today,\nproviding factual knowledge and structured data that steer many products and\nmake them more intelligent. Despite their promising benefits, building business\nKG necessitates solving prohibitive issues of deficient structure and multiple\nmodalities. In this paper, we advance the understanding of the practical\nchallenges related to building KG in non-trivial real-world systems. We\nintroduce the process of building an open business knowledge graph (OpenBG)\nderived from a well-known enterprise, Alibaba Group. Specifically, we define a\ncore ontology to cover various abstract products and consumption demands, with\nfine-grained taxonomy and multimodal facts in deployed applications. OpenBG is\nan open business KG of unprecedented scale: 2.6 billion triples with more than\n88 million entities covering over 1 million core classes/concepts and 2,681\ntypes of relations. We release all the open resources (OpenBG benchmarks)\nderived from it for the community and report experimental results of KG-centric\ntasks. We also run up an online competition based on OpenBG benchmarks, and has\nattracted thousands of teams. We further pre-train OpenBG and apply it to many\nKG- enhanced downstream tasks in business scenarios, demonstrating the\neffectiveness of billion-scale multimodal knowledge for e-commerce. All the\nresources with codes have been released at\n\\url{https://github.com/OpenBGBenchmark/OpenBG}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zelin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hehong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MTEB: Massive Text Embedding Benchmark. (arXiv:2210.07316v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07316","description":"<p>Text embeddings are commonly evaluated on a small set of datasets from a\nsingle task not covering their possible applications to other tasks. It is\nunclear whether state-of-the-art embeddings on semantic textual similarity\n(STS) can be equally well applied to other tasks like clustering or reranking.\nThis makes progress in the field difficult to track, as various models are\nconstantly being proposed without proper evaluation. To solve this problem, we\nintroduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding\ntasks covering a total of 58 datasets and 112 languages. Through the\nbenchmarking of 33 models on MTEB, we establish the most comprehensive\nbenchmark of text embeddings to date. We find that no particular text embedding\nmethod dominates across all tasks. This suggests that the field has yet to\nconverge on a universal text embedding method and scale it up sufficiently to\nprovide state-of-the-art results on all embedding tasks. MTEB comes with\nopen-source code and a public leaderboard at\nhttps://github.com/embeddings-benchmark/mteb.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1\">Niklas Muennighoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tazi_N/0/1/0/all/0/1\">Nouamane Tazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magne_L/0/1/0/all/0/1\">Lo&#xef;c Magne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PACIFIC: Towards Proactive Conversational Question Answering over Tabular and Textual Data in Finance. (arXiv:2210.08817v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08817","description":"<p>To facilitate conversational question answering (CQA) over hybrid contexts in\nfinance, we present a new dataset, named PACIFIC. Compared with existing CQA\ndatasets, PACIFIC exhibits three key features: (i) proactivity, (ii) numerical\nreasoning, and (iii) hybrid context of tables and text. A new task is defined\naccordingly to study Proactive Conversational Question Answering (PCQA), which\ncombines clarification question generation and CQA. In addition, we propose a\nnovel method, namely UniPCQA, to adapt a hybrid format of input and output\ncontent in PCQA into the Seq2Seq problem, including the reformulation of the\nnumerical reasoning process as code generation. UniPCQA performs multi-task\nlearning over all sub-tasks in PCQA and incorporates a simple ensemble strategy\nto alleviate the error propagation issue in the multi-task learning by\ncross-validating top-$k$ sampled Seq2Seq outputs. We benchmark the PACIFIC\ndataset with extensive baselines and provide comprehensive evaluations on each\nsub-task of PCQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privately Fine-Tuning Large Language Models with Differential Privacy. (arXiv:2210.15042v3 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2210.15042","description":"<p>Pre-trained Large Language Models (LLMs) are an integral part of modern AI\nthat have led to breakthrough performances in complex AI tasks. Major AI\ncompanies with expensive infrastructures are able to develop and train these\nlarge models with billions and millions of parameters from scratch. Third\nparties, researchers, and practitioners are increasingly adopting these\npre-trained models and fine-tuning them on their private data to accomplish\ntheir downstream AI tasks. However, it has been shown that an adversary can\nextract/reconstruct the exact training samples from these LLMs, which can lead\nto revealing personally identifiable information. The issue has raised deep\nconcerns about the privacy of LLMs. Differential privacy (DP) provides a\nrigorous framework that allows adding noise in the process of training or\nfine-tuning LLMs such that extracting the training data becomes infeasible\n(i.e., with a cryptographically small success probability). While the\ntheoretical privacy guarantees offered in most extant studies assume learning\nmodels from scratch through many training iterations in an asymptotic setting,\nthis assumption does not hold in fine-tuning scenarios in which the number of\ntraining iterations is significantly smaller. To address the gap, we present\n\\ewtune, a DP framework for fine-tuning LLMs based on Edgeworth accountant with\nfinite-sample privacy guarantees. Our results across four well-established\nnatural language understanding (NLU) tasks show that while \\ewtune~adds privacy\nguarantees to LLM fine-tuning process, it directly contributes to decreasing\nthe induced noise to up to 5.6\\% and improves the state-of-the-art LLMs\nperformance by up to 1.1\\% across all NLU tasks. We have open-sourced our\nimplementations for wide adoption and public testing purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Behnia_R/0/1/0/all/0/1\">Rouzbeh Behnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_M/0/1/0/all/0/1\">Mohamamdreza Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacheco_J/0/1/0/all/0/1\">Jason Pacheco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmanabhan_B/0/1/0/all/0/1\">Balaji Padmanabhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VieCap4H-VLSP 2021: ObjectAoA-Enhancing performance of Object Relation Transformer with Attention on Attention for Vietnamese image captioning. (arXiv:2211.05405v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.05405","description":"<p>Image captioning is currently a challenging task that requires the ability to\nboth understand visual information and use human language to describe this\nvisual information in the image. In this paper, we propose an efficient way to\nimprove the image understanding ability of transformer-based method by\nextending Object Relation Transformer architecture with Attention on Attention\nmechanism. Experiments on the VieCap4H dataset show that our proposed method\nsignificantly outperforms its original structure on both the public test and\nprivate test of the Image Captioning shared task held by VLSP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Nghia Hieu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_D/0/1/0/all/0/1\">Duong T.D. Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_M/0/1/0/all/0/1\">Minh-Quan Ha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models. (arXiv:2212.04088v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2212.04088","description":"<p>This study focuses on using large language models (LLMs) as a planner for\nembodied agents that can follow natural language instructions to complete\ncomplex tasks in a visually-perceived environment. The high data cost and poor\nsample efficiency of existing methods hinders the development of versatile\nagents that are capable of many tasks and can learn new tasks quickly. In this\nwork, we propose a novel method, LLM-Planner, that harnesses the power of large\nlanguage models to do few-shot planning for embodied agents. We further propose\na simple but effective way to enhance LLMs with physical grounding to generate\nand update plans that are grounded in the current environment. Experiments on\nthe ALFRED dataset show that our method can achieve very competitive few-shot\nperformance: Despite using less than 0.5% of paired training data, LLM-Planner\nachieves competitive performance with recent baselines that are trained using\nthe full training data. Existing methods can barely complete any task\nsuccessfully under the same few-shot setting. Our work opens the door for\ndeveloping versatile and sample-efficient embodied agents that can quickly\nlearn many tasks. Website: https://dki-lab.github.io/LLM-Planner\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chan Hee Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaman Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washington_C/0/1/0/all/0/1\">Clayton Washington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadler_B/0/1/0/all/0/1\">Brian M. Sadler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MN-DS: A Multilabeled News Dataset for News Articles Hierarchical Classification. (arXiv:2212.12061v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.12061","description":"<p>This article presents a dataset of 10,917 news articles with hierarchical\nnews categories collected between January 1st 2019, and December 31st 2019. We\nmanually labelled the articles based on a hierarchical taxonomy with 17\nfirst-level and 109 second-level categories. This dataset can be used to train\nmachine learning models for automatically classifying news articles by topic.\nThis dataset can be helpful for researchers working on news structuring,\nclassification, and predicting future events based on released news.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petukhova_A/0/1/0/all/0/1\">Alina Petukhova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fachada_N/0/1/0/all/0/1\">Nuno Fachada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Error-Guided Correction Model for Chinese Spelling Error Correction. (arXiv:2301.06323v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.06323","description":"<p>Although existing neural network approaches have achieved great success on\nChinese spelling correction, there is still room to improve. The model is\nrequired to avoid over-correction and to distinguish a correct token from its\nphonological and visually similar ones. In this paper, we propose an\nerror-guided correction model (EGCM) to improve Chinese spelling correction. By\nborrowing the powerful ability of BERT, we propose a novel zero-shot error\ndetection method to do a preliminary detection, which guides our model to\nattend more on the probably wrong tokens in encoding and to avoid modifying the\ncorrect tokens in generating. Furthermore, we introduce a new loss function to\nintegrate the error confusion set, which enables our model to distinguish\neasily misused tokens. Moreover, our model supports highly parallel decoding to\nmeet real application requirements. Experiments are conducted on widely used\nbenchmarks. Our model achieves superior performance against state-of-the-art\napproaches by a remarkable margin, on both the correction quality and\ncomputation speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Rui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiuyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine. (arXiv:2301.08745v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.08745","description":"<p>This report provides a preliminary evaluation of ChatGPT for machine\ntranslation, including translation prompt, multilingual translation, and\ntranslation robustness. We adopt the prompts advised by ChatGPT to trigger its\ntranslation ability and find that the candidate prompts generally work well and\nshow minor performance differences. By evaluating on a number of benchmark test\nsets, we find that ChatGPT performs competitively with commercial translation\nproducts (e.g., Google Translate) on high-resource European languages but lags\nbehind significantly on low-resource or distant languages. For distant\nlanguages, we explore an interesting strategy named $\\mathbf{pivot~prompting}$\nthat asks ChatGPT to translate the source sentence into a high-resource pivot\nlanguage before into the target language, which improves the translation\nperformance significantly. As for the translation robustness, ChatGPT does not\nperform as well as the commercial systems on biomedical abstracts or Reddit\ncomments but exhibits good results on spoken language. With the launch of the\nGPT-4 engine, the translation performance of ChatGPT is significantly boosted,\nbecoming comparable to commercial translation products, even for distant\nlanguages. In other words,\n$\\mathbf{ChatGPT~has~already~become~a~good~translator!}$ Scripts and data:\nhttps://github.com/wxjiao/Is-ChatGPT-A-Good-Translator\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jen-tse Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FGSI: Distant Supervision for Relation Extraction method based on Fine-Grained Semantic Information. (arXiv:2302.02078v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.02078","description":"<p>The main purpose of relation extraction is to extract the semantic\nrelationships between tagged pairs of entities in a sentence, which plays an\nimportant role in the semantic understanding of sentences and the construction\nof knowledge graphs. In this paper, we propose that the key semantic\ninformation within a sentence plays a key role in the relationship extraction\nof entities. We propose the hypothesis that the key semantic information inside\nthe sentence plays a key role in entity relationship extraction. And based on\nthis hypothesis, we split the sentence into three segments according to the\nlocation of the entity from the inside of the sentence, and find the\nfine-grained semantic features inside the sentence through the intra-sentence\nattention mechanism to reduce the interference of irrelevant noise information.\nThe proposed relational extraction model can make full use of the available\npositive semantic information. The experimental results show that the proposed\nrelation extraction model improves the accuracy-recall curves and P@N values\ncompared with existing methods, which proves the effectiveness of this model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chenghong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Weidong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guohui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zengxiang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yuqi Yue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Noisy Crowd Labels with Logics. (arXiv:2302.06337v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.06337","description":"<p>This paper explores the integration of symbolic logic knowledge into deep\nneural networks for learning from noisy crowd labels. We introduce Logic-guided\nLearning from Noisy Crowd Labels (Logic-LNCL), an EM-alike iterative logic\nknowledge distillation framework that learns from both noisy labeled data and\nlogic rules of interest. Unlike traditional EM methods, our framework contains\na ``pseudo-E-step'' that distills from the logic rules a new type of learning\ntarget, which is then used in the ``pseudo-M-step'' for training the\nclassifier. Extensive evaluations on two real-world datasets for text sentiment\nclassification and named entity recognition demonstrate that the proposed\nframework improves the state-of-the-art and provides a new solution to learning\nfrom noisy crowd labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhijun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hailong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Haoqian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pengpeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AugGPT: Leveraging ChatGPT for Text Data Augmentation. (arXiv:2302.13007v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.13007","description":"<p>Text data augmentation is an effective strategy for overcoming the challenge\nof limited sample sizes in many natural language processing (NLP) tasks. This\nchallenge is especially prominent in the few-shot learning scenario, where the\ndata in the target domain is generally much scarcer and of lowered quality. A\nnatural and widely-used strategy to mitigate such challenges is to perform data\naugmentation to better capture the data invariance and increase the sample\nsize. However, current text data augmentation methods either can't ensure the\ncorrect labeling of the generated data (lacking faithfulness) or can't ensure\nsufficient diversity in the generated data (lacking compactness), or both.\nInspired by the recent success of large language models, especially the\ndevelopment of ChatGPT, which demonstrated improved language comprehension\nabilities, in this work, we propose a text data augmentation approach based on\nChatGPT (named AugGPT). AugGPT rephrases each sentence in the training samples\ninto multiple conceptually similar but semantically different samples. The\naugmented samples can then be used in downstream model training. Experiment\nresults on few-shot learning text classification tasks show the superior\nperformance of the proposed AugGPT approach over state-of-the-art text data\naugmentation methods in terms of testing accuracy and distribution of the\naugmented samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Haixing Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">Wenxiong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaoke Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yihan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shaochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ninghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hongmin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Quanzheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uzbek text's correspondence with the educational potential of pupils: a case study of the School corpus. (arXiv:2303.00465v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.00465","description":"<p>One of the major challenges of an educational system is choosing appropriate\ncontent considering pupils' age and intellectual potential. In this article the\nexperiment of primary school grades (from 1st to 4th grades) is considered for\nautomatically determining the correspondence of an educational materials\nrecommended for pupils by using the School corpus where it includes the dataset\nof 25 school textbooks confirmed by the Ministry of preschool and school\neducation of the Republic of Uzbekistan. In this case, TF-IDF scores of the\ntexts are determined, they are converted into a vector representation, and the\ngiven educational materials are compared with the corresponding class of the\nSchool corpus using the cosine similarity algorithm. Based on the results of\nthe calculation, it is determined whether the given educational material is\nappropriate or not appropriate for the pupils' educational potential.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madatov_K/0/1/0/all/0/1\">Khabibulla Madatov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matlatipov_S/0/1/0/all/0/1\">Sanatbek Matlatipov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aripov_M/0/1/0/all/0/1\">Mersaid Aripov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Text Generation. (arXiv:2303.00908v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.00908","description":"<p>Users interact with text, image, code, or other editors on a daily basis.\nHowever, machine learning models are rarely trained in the settings that\nreflect the interactivity between users and their editor. This is\nunderstandable as training AI models with real users is not only slow and\ncostly, but what these models learn may be specific to user interface design\nchoices. Unfortunately, this means most of the research on text, code, and\nimage generation has focused on non-interactive settings, whereby the model is\nexpected to get everything right without accounting for any input from a user\nwho may be willing to help.\n</p>\n<p>We introduce a new Interactive Text Generation task that allows training\ngeneration models interactively without the costs of involving real users, by\nusing user simulators that provide edits that guide the model towards a given\ntarget text. We train our interactive models using Imitation Learning, and our\nexperiments against competitive non-interactive generation models show that\nmodels trained interactively are superior to their non-interactive\ncounterparts, even when all models are given the same budget of user inputs or\nedits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faltings_F/0/1/0/all/0/1\">Felix Faltings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1\">Michel Galley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brantley_K/0/1/0/all/0/1\">Kiant&#xe9; Brantley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weixin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolan_B/0/1/0/all/0/1\">Bill Dolan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Science of Detecting LLM-Generated Texts. (arXiv:2303.07205v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.07205","description":"<p>The emergence of large language models (LLMs) has resulted in the production\nof LLM-generated texts that is highly sophisticated and almost\nindistinguishable from texts written by humans. However, this has also sparked\nconcerns about the potential misuse of such texts, such as spreading\nmisinformation and causing disruptions in the education system. Although many\ndetection approaches have been proposed, a comprehensive understanding of the\nachievements and challenges is still lacking. This survey aims to provide an\noverview of existing LLM-generated text detection techniques and enhance the\ncontrol and regulation of language generation models. Furthermore, we emphasize\ncrucial considerations for future research, including the development of\ncomprehensive evaluation metrics and the threat posed by open-source LLMs, to\ndrive progress in the area of LLM-generated text detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Ruixiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yu-Neng Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential. (arXiv:2303.09038v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.09038","description":"<p>The large language model called ChatGPT has drawn extensively attention\nbecause of its human-like expression and reasoning abilities. In this study, we\ninvestigate the feasibility of using ChatGPT in experiments on using ChatGPT to\ntranslate radiology reports into plain language for patients and healthcare\nproviders so that they are educated for improved healthcare. Radiology reports\nfrom 62 low-dose chest CT lung cancer screening scans and 76 brain MRI\nmetastases screening scans were collected in the first half of February for\nthis study. According to the evaluation by radiologists, ChatGPT can\nsuccessfully translate radiology reports into plain language with an average\nscore of 4.27 in the five-point system with 0.08 places of information missing\nand 0.07 places of misinformation. In terms of the suggestions provided by\nChatGPT, they are general relevant such as keeping following-up with doctors\nand closely monitoring any symptoms, and for about 37% of 138 cases in total\nChatGPT offers specific suggestions based on findings in the report. ChatGPT\nalso presents some randomness in its responses with occasionally\nover-simplified or neglected information, which can be mitigated using a more\ndetailed prompt. Furthermore, ChatGPT results are compared with a newly\nreleased large model GPT-4, showing that GPT-4 can significantly improve the\nquality of translated reports. Our results show that it is feasible to utilize\nlarge language models in clinical education, and further efforts are needed to\naddress limitations and maximize their potential.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Josh Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zapadka_M/0/1/0/all/0/1\">Michael E. Zapadka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponnatapura_J/0/1/0/all/0/1\">Janardhana Ponnatapura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitlow_C/0/1/0/all/0/1\">Christopher T. Whitlow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-03-20T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}