{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-11-18T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Unified Question Answering in Slovene. (arXiv:2211.09159v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09159","description":"<p>Question answering is one of the most challenging tasks in language\nunderstanding. Most approaches are developed for English, while less-resourced\nlanguages are much less researched. We adapt a successful English\nquestion-answering approach, called UnifiedQA, to the less-resourced Slovene\nlanguage. Our adaptation uses the encoder-decoder transformer SloT5 and mT5\nmodels to handle four question-answering formats: yes/no, multiple-choice,\nabstractive, and extractive. We use existing Slovene adaptations of four\ndatasets, and machine translate the MCTest dataset. We show that a general\nmodel can answer questions in different formats at least as well as specialized\nmodels. The results are further improved using cross-lingual transfer from\nEnglish. While we produce state-of-the-art results for Slovene, the performance\nstill lags behind English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Logar_K/0/1/0/all/0/1\">Katja Logar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1\">Marko Robnik-&#x160;ikonja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Emotion Recognition in Textual Conversations: A Survey. (arXiv:2211.09172v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09172","description":"<p>While Emotion Recognition in Conversations (ERC) has seen a tremendous\nadvancement in the last few years, new applications and implementation\nscenarios present novel challenges and opportunities. These range from\nleveraging the conversational context, speaker and emotion dynamics modelling,\nto interpreting common sense expressions, informal language and sarcasm,\naddressing challenges of real time ERC and recognizing emotion causes. This\nsurvey starts by introducing ERC, elaborating on the challenges and\nopportunities pertaining to this task. It proceeds with a description of the\nmain emotion taxonomies and methods to deal with subjectivity in annotations.\nIt then describes Deep Learning methods relevant for ERC, word embeddings, and\nelaborates on the use of performance metrics for the task and methods to deal\nwith the typically unbalanced ERC datasets. This is followed by a description\nand benchmark of key ERC works along with comprehensive tables comparing\nseveral works regarding their methods and performance across different\ndatasets. The survey highlights the advantage of leveraging techniques to\naddress unbalanced data, the exploration of mixed emotions and the benefits of\nincorporating annotation subjectivity in the learning phase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pereira_P/0/1/0/all/0/1\">Patr&#xed;cia Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moniz_H/0/1/0/all/0/1\">Helena Moniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_J/0/1/0/all/0/1\">Joao Paulo Carvalho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Graph-Based Context-Aware Model to Understand Online Conversations. (arXiv:2211.09207v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09207","description":"<p>Online forums that allow for participatory engagement between users have been\ntransformative for the public discussion of many important issues. However,\nsuch conversations can sometimes escalate into full-blown exchanges of hate and\nmisinformation. Existing approaches in natural language processing (NLP), such\nas deep learning models for classification tasks, use as inputs only a single\ncomment or a pair of comments depending upon whether the task concerns the\ninference of properties of the individual comments or the replies between pairs\nof comments, respectively. But in online conversations, comments and replies\nmay be based on external context beyond the immediately relevant information\nthat is input to the model. Therefore, being aware of the conversations'\nsurrounding contexts should improve the model's performance for the inference\ntask at hand.\n</p>\n<p>We propose GraphNLI, a novel graph-based deep learning architecture that uses\ngraph walks to incorporate the wider context of a conversation in a principled\nmanner. Specifically, a graph walk starts from a given comment and samples\n\"nearby\" comments in the same or parallel conversation threads, which results\nin additional embeddings that are aggregated together with the initial\ncomment's embedding. We then use these enriched embeddings for downstream NLP\nprediction tasks that are important for online conversations. We evaluate\nGraphNLI on two such tasks - polarity prediction and misogynistic hate speech\ndetection - and found that our model consistently outperforms all relevant\nbaselines for both tasks. Specifically, GraphNLI with a biased root-seeking\nrandom walk performs with a macro-F1 score of 3 and 6 percentage points better\nthan the best-performing BERT-based baselines for the polarity prediction and\nhate speech detection tasks, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_V/0/1/0/all/0/1\">Vibhor Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_A/0/1/0/all/0/1\">Anthony P. Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joglekar_S/0/1/0/all/0/1\">Sagar Joglekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sastry_N/0/1/0/all/0/1\">Nishanth Sastry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Artificial Disfluency Detection, Uh No, Disfluency Generation for the Masses. (arXiv:2211.09235v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09235","description":"<p>Existing approaches for disfluency detection typically require the existence\nof large annotated datasets. However, current datasets for this task are\nlimited, suffer from class imbalance, and lack some types of disfluencies that\ncan be encountered in real-world scenarios. This work proposes LARD, a method\nfor automatically generating artificial disfluencies from fluent text. LARD can\nsimulate all the different types of disfluencies (repetitions, replacements and\nrestarts) based on the reparandum/interregnum annotation scheme. In addition,\nit incorporates contextual embeddings into the disfluency generation to produce\nrealistic context-aware artificial disfluencies. Since the proposed method\nrequires only fluent text, it can be used directly for training, bypassing the\nrequirement of annotated disfluent data. Our empirical evaluation demonstrates\nthat LARD can indeed be effectively used when no or only a few data are\navailable. Furthermore, our detailed analysis suggests that the proposed method\ngenerates realistic disfluencies and increases the accuracy of existing\ndisfluency detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Passali_T/0/1/0/all/0/1\">T. Passali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavropoulos_T/0/1/0/all/0/1\">T. Mavropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1\">G. Tsoumakas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meditskos_G/0/1/0/all/0/1\">G. Meditskos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vrochidis_S/0/1/0/all/0/1\">S. Vrochidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-aware Retrieval with Instructions. (arXiv:2211.09260v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09260","description":"<p>We study the problem of retrieval with instructions, where users of a\nretrieval system explicitly describe their intent along with their queries,\nmaking the system task-aware. We aim to develop a general-purpose task-aware\nretrieval systems using multi-task instruction tuning that can follow\nhuman-written instructions to find the best documents for a given query. To\nthis end, we introduce the first large-scale collection of approximately 40\nretrieval datasets with instructions, and present TART, a multi-task retrieval\nsystem trained on the diverse retrieval tasks with instructions. TART shows\nstrong capabilities to adapt to a new task via instructions and advances the\nstate of the art on two zero-shot retrieval benchmarks, BEIR and LOTTE,\noutperforming models up to three times larger. We further introduce a new\nevaluation setup to better reflect real-world scenarios, pooling diverse\ndocuments and tasks. In this setup, TART significantly outperforms competitive\nbaselines, further demonstrating the effectiveness of guiding retrieval with\ninstructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1\">Akari Asai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schick_T/0/1/0/all/0/1\">Timo Schick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izacard_G/0/1/0/all/0/1\">Gautier Izacard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reflect, Not Reflex: Inference-Based Common Ground Improves Dialogue Response Quality. (arXiv:2211.09267v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09267","description":"<p>Human communication relies on common ground (CG), the mutual knowledge and\nbeliefs shared by participants, to produce coherent and interesting\nconversations. In this paper, we demonstrate that current response generation\n(RG) models produce generic and dull responses in dialogues because they act\nreflexively, failing to explicitly model CG, both due to the lack of CG in\ntraining data and the standard RG training procedure. We introduce Reflect, a\ndataset that annotates dialogues with explicit CG (materialized as inferences\napproximating shared knowledge and beliefs) and solicits 9k diverse\nhuman-generated responses each following one common ground. Using Reflect, we\nshowcase the limitations of current dialogue data and RG models: less than half\nof the responses in current data are rated as high quality (sensible, specific,\nand interesting) and models trained using this data have even lower quality,\nwhile most Reflect responses are judged high quality. Next, we analyze whether\nCG can help models produce better-quality responses by using Reflect CG to\nguide RG models. Surprisingly, we find that simply prompting GPT3 to \"think\"\nabout CG generates 30% more quality responses, showing promising benefits to\nintegrating CG into the RG process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyundong Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jandaghi_P/0/1/0/all/0/1\">Pegah Jandaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-Ho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Execution-based Evaluation for Data Science Code Generation Models. (arXiv:2211.09374v1 [cs.SE])","link":"http://arxiv.org/abs/2211.09374","description":"<p>Code generation models can benefit data scientists' productivity by\nautomatically generating code from context and text descriptions. An important\nmeasure of the modeling progress is whether a model can generate code that can\ncorrectly execute to solve the task. However, due to the lack of an evaluation\ndataset that directly supports execution-based model evaluation, existing work\nrelies on code surface form similarity metrics (e.g., BLEU, CodeBLEU) for model\nselection, which can be inaccurate.\n</p>\n<p>To remedy this, we introduce ExeDS, an evaluation dataset for execution\nevaluation for data science code generation tasks. ExeDS contains a set of 534\nproblems from Jupyter Notebooks, each consisting of code context, task\ndescription, reference program, and the desired execution output. With ExeDS,\nwe evaluate the execution performance of five state-of-the-art code generation\nmodels that have achieved high surface-form evaluation scores. Our experiments\nshow that models with high surface-form scores do not necessarily perform well\non execution metrics, and execution-based metrics can better capture model code\ngeneration errors. Source code and data can be found at\nhttps://github.com/Jun-jie-Huang/ExeDS\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenglong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Cong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Haotian Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inala_J/0/1/0/all/0/1\">Jeevana Priya Inala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clement_C/0/1/0/all/0/1\">Colin Clement</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Training with Purpose Preserving Augmentation Improves Few-shot Generative Dialogue State Tracking. (arXiv:2211.09379v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09379","description":"<p>In dialogue state tracking (DST), labeling the dataset involves considerable\nhuman labor. We propose a new self-training framework for few-shot generative\nDST that utilize unlabeled data. Our self-training method iteratively improves\nthe model by pseudo labeling and employs Purpose Preserving Augmentation\n(PPAug) to prevent overfitting. We increaese the few-shot 10% performance by\napproximately 4% on MultiWOZ 2.1 and enhances the slot-recall 8.34% for unseen\nvalues compared to baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jihyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chaebin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yunsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gary Geunbae Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Efficient Autoregressive Document Retrieval for Fact Verification. (arXiv:2211.09388v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09388","description":"<p>Document retrieval is a core component of many knowledge-intensive natural\nlanguage processing task formulations such as fact verification and question\nanswering. Sources of textual knowledge, such as Wikipedia articles, condition\nthe generation of answers from the models. Recent advances in retrieval use\nsequence-to-sequence models to incrementally predict the title of the\nappropriate Wikipedia page given a query. However, this method requires\nsupervision in the form of human annotation to label which Wikipedia pages\ncontain appropriate context. This paper introduces a distant-supervision method\nthat does not require any annotation to train autoregressive retrievers that\nattain competitive R-Precision and Recall in a zero-shot setting. Furthermore\nwe show that with task-specific supervised fine-tuning, autoregressive\nretrieval performance for two Wikipedia-based fact verification tasks can\napproach or even exceed full supervision using less than $1/4$ of the annotated\ndata indicating possible directions for data-efficient autoregressive\nretrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1\">James Thorne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConNER: Consistency Training for Cross-lingual Named Entity Recognition. (arXiv:2211.09394v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09394","description":"<p>Cross-lingual named entity recognition (NER) suffers from data scarcity in\nthe target languages, especially under zero-shot settings. Existing\ntranslate-train or knowledge distillation methods attempt to bridge the\nlanguage gap, but often introduce a high level of noise. To solve this problem,\nconsistency training methods regularize the model to be robust towards\nperturbations on data or hidden states. However, such methods are likely to\nviolate the consistency hypothesis, or mainly focus on coarse-grain\nconsistency. We propose ConNER as a novel consistency training framework for\ncross-lingual NER, which comprises of: (1) translation-based consistency\ntraining on unlabeled target-language data, and (2) dropoutbased consistency\ntraining on labeled source-language data. ConNER effectively leverages\nunlabeled target-language data and alleviates overfitting on the source\nlanguage to enhance the cross-lingual adaptability. Experimental results show\nour ConNER achieves consistent improvement over various baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Ran Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Domain Conversational Question Answering with Historical Answers. (arXiv:2211.09401v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09401","description":"<p>Open-domain conversational question answering can be viewed as two tasks:\npassage retrieval and conversational question answering, where the former\nrelies on selecting candidate passages from a large corpus and the latter\nrequires better understanding of a question with contexts to predict the\nanswers. This paper proposes ConvADR-QA that leverages historical answers to\nboost retrieval performance and further achieves better answering performance.\nIn our proposed framework, the retrievers use a teacher-student framework to\nreduce noises from previous turns. Our experiments on the benchmark dataset,\nOR-QuAC, demonstrate that our model outperforms existing baselines in both\nextractive and generative reader settings, well justifying the effectiveness of\nhistorical answers for open-domain conversational question answering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Hung-Chieh Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_K/0/1/0/all/0/1\">Kuo-Han Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chao-Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun-Nung Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LongFNT: Long-form Speech Recognition with Factorized Neural Transducer. (arXiv:2211.09412v1 [cs.SD])","link":"http://arxiv.org/abs/2211.09412","description":"<p>Traditional automatic speech recognition~(ASR) systems usually focus on\nindividual utterances, without considering long-form speech with useful\nhistorical information, which is more practical in real scenarios. Simply\nattending longer transcription history for a vanilla neural transducer model\nshows no much gain in our preliminary experiments, since the prediction network\nis not a pure language model. This motivates us to leverage the factorized\nneural transducer structure, containing a real language model, the vocabulary\npredictor. We propose the {LongFNT-Text} architecture, which fuses the\nsentence-level long-form features directly with the output of the vocabulary\npredictor and then embeds token-level long-form features inside the vocabulary\npredictor, with a pre-trained contextual encoder RoBERTa to further boost the\nperformance. Moreover, we propose the {LongFNT} architecture by extending the\nlong-form speech to the original speech input and achieve the best performance.\nThe effectiveness of our LongFNT approach is validated on LibriSpeech and\nGigaSpeech corpora with 19% and 12% relative word error rate~(WER) reduction,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yanmin Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feedback is Needed for Retakes: An Explainable Poor Image Notification Framework for the Visually Impaired. (arXiv:2211.09427v1 [cs.CV])","link":"http://arxiv.org/abs/2211.09427","description":"<p>We propose a simple yet effective image captioning framework that can\ndetermine the quality of an image and notify the user of the reasons for any\nflaws in the image. Our framework first determines the quality of images and\nthen generates captions using only those images that are determined to be of\nhigh quality. The user is notified by the flaws feature to retake if image\nquality is low, and this cycle is repeated until the input image is deemed to\nbe of high quality. As a component of the framework, we trained and evaluated a\nlow-quality image detection model that simultaneously learns difficulty in\nrecognizing images and individual flaws, and we demonstrated that our proposal\ncan explain the reasons for flaws with a sufficient score. We also evaluated a\ndataset with low-quality images removed by our framework and found improved\nvalues for all four common metrics (e.g., BLEU-4, METEOR, ROUGE-L, CIDEr),\nconfirming an improvement in general-purpose image captioning capability. Our\nframework would assist the visually impaired, who have difficulty judging image\nquality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ohata_K/0/1/0/all/0/1\">Kazuya Ohata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitada_S/0/1/0/all/0/1\">Shunsuke Kitada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyatomi_H/0/1/0/all/0/1\">Hitoshi Iyatomi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature-augmented Machine Reading Comprehension with Auxiliary Tasks. (arXiv:2211.09438v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09438","description":"<p>While most successful approaches for machine reading comprehension rely on\nsingle training objective, it is assumed that the encoder layer can learn great\nrepresentation through the loss function we define in the predict layer, which\nis cross entropy in most of time, in the case that we first use neural networks\nto encode the question and paragraph, then directly fuse the encoding result of\nthem. However, due to the distantly loss backpropagating in reading\ncomprehension, the encoder layer cannot learn effectively and be directly\nsupervised. Thus, the encoder layer can not learn the representation well at\nany time. Base on this, we propose to inject multi granularity information to\nthe encoding layer. Experiments demonstrate the effect of adding multi\ngranularity information to the encoding layer can boost the performance of\nmachine reading comprehension system. Finally, empirical study shows that our\napproach can be applied to many existing MRC models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yifeng Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consultation Checklists: Standardising the Human Evaluation of Medical Note Generation. (arXiv:2211.09455v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09455","description":"<p>Evaluating automatically generated text is generally hard due to the\ninherently subjective nature of many aspects of the output quality. This\ndifficulty is compounded in automatic consultation note generation by differing\nopinions between medical experts both about which patient statements should be\nincluded in generated notes and about their respective importance in arriving\nat a diagnosis. Previous real-world evaluations of note-generation systems saw\nsubstantial disagreement between expert evaluators. In this paper we propose a\nprotocol that aims to increase objectivity by grounding evaluations in\nConsultation Checklists, which are created in a preliminary step and then used\nas a common point of reference during quality assessment. We observed good\nlevels of inter-annotator agreement in a first evaluation study using the\nprotocol; further, using Consultation Checklists produced in the study as\nreference for automatic metrics such as ROUGE or BERTScore improves their\ncorrelation with human judgements compared to using the original human note.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Savkov_A/0/1/0/all/0/1\">Aleksandar Savkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moramarco_F/0/1/0/all/0/1\">Francesco Moramarco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korfiatis_A/0/1/0/all/0/1\">Alex Papadopoulos Korfiatis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perera_M/0/1/0/all/0/1\">Mark Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belz_A/0/1/0/all/0/1\">Anya Belz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiter_E/0/1/0/all/0/1\">Ehud Reiter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Abstractive Summarization Guided by Latent Hierarchical Document Structure. (arXiv:2211.09458v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09458","description":"<p>Sequential abstractive neural summarizers often do not use the underlying\nstructure in the input article or dependencies between the input sentences.\nThis structure is essential to integrate and consolidate information from\ndifferent parts of the text. To address this shortcoming, we propose a\nhierarchy-aware graph neural network (HierGNN) which captures such dependencies\nthrough three main steps: 1) learning a hierarchical document structure through\na latent structure tree learned by a sparse matrix-tree computation; 2)\npropagating sentence information over this structure using a novel\nmessage-passing node propagation mechanism to identify salient information; 3)\nusing graph-level attention to concentrate the decoder on salient information.\nExperiments confirm HierGNN improves strong sequence models such as BART, with\na 0.55 and 0.75 margin in average ROUGE-1/2/L for CNN/DM and XSum. Further\nhuman evaluation demonstrates that summaries produced by our model are more\nrelevant and less redundant than the baselines, into which HierGNN is\nincorporated. We also find HierGNN synthesizes summaries by fusing multiple\nsource sentences more, rather than compressing a single source sentence, and\nthat it processes long inputs more effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yifu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Shay B. Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back-Translation-Style Data Augmentation for Mandarin Chinese Polyphone Disambiguation. (arXiv:2211.09495v1 [cs.SD])","link":"http://arxiv.org/abs/2211.09495","description":"<p>Conversion of Chinese Grapheme-to-Phoneme (G2P) plays an important role in\nMandarin Chinese Text-To-Speech (TTS) systems, where one of the biggest\nchallenges is the task of polyphone disambiguation. Most of the previous\npolyphone disambiguation models are trained on manually annotated datasets, and\npublicly available datasets for polyphone disambiguation are scarce. In this\npaper we propose a simple back-translation-style data augmentation method for\nmandarin Chinese polyphone disambiguation, utilizing a large amount of\nunlabeled text data. Inspired by the back-translation technique proposed in the\nfield of machine translation, we build a Grapheme-to-Phoneme (G2P) model to\npredict the pronunciation of polyphonic character, and a Phoneme-to-Grapheme\n(P2G) model to predict pronunciation into text. Meanwhile, a window-based\nmatching strategy and a multi-model scoring strategy are proposed to judge the\ncorrectness of the pseudo-label. We design a data balance strategy to improve\nthe accuracy of some typical polyphonic characters in the training set with\nimbalanced distribution or data scarcity. The experimental result shows the\neffectiveness of the proposed back-translation-style data augmentation method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiang_C/0/1/0/all/0/1\">Chunyu Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Peng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_H/0/1/0/all/0/1\">Hao Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jinba Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaorui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hey ASR System! Why Aren't You More Inclusive? Automatic Speech Recognition Systems' Bias and Proposed Bias Mitigation Techniques. A Literature Review. (arXiv:2211.09511v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09511","description":"<p>Speech is the fundamental means of communication between humans. The advent\nof AI and sophisticated speech technologies have led to the rapid proliferation\nof human-to-computer-based interactions, fueled primarily by Automatic Speech\nRecognition (ASR) systems. ASR systems normally take human speech in the form\nof audio and convert it into words, but for some users, it cannot decode the\nspeech, and any output text is filled with errors that are incomprehensible to\nthe human reader. These systems do not work equally for everyone and actually\nhinder the productivity of some users. In this paper, we present research that\naddresses ASR biases against gender, race, and the sick and disabled, while\nexploring studies that propose ASR debiasing techniques for mitigating these\ndiscriminations. We also discuss techniques for designing a more accessible and\ninclusive ASR technology. For each approach surveyed, we also provide a summary\nof the investigation and methods applied, the ASR systems and corpora used, and\nthe research findings, and highlight their strengths and/or weaknesses.\nFinally, we propose future opportunities for Natural Language Processing\nresearchers to explore in the next level creation of ASR technologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ngueajio_M/0/1/0/all/0/1\">Mikel K. Ngueajio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washington_G/0/1/0/all/0/1\">Gloria Washington</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ignore Previous Prompt: Attack Techniques For Language Models. (arXiv:2211.09527v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09527","description":"<p>Transformer-based large language models (LLMs) provide a powerful foundation\nfor natural language tasks in large-scale customer-facing applications.\nHowever, studies that explore their vulnerabilities emerging from malicious\nuser interaction are scarce. By proposing PromptInject, a prosaic alignment\nframework for mask-based iterative adversarial prompt composition, we examine\nhow GPT-3, the most widely deployed language model in production, can be easily\nmisaligned by simple handcrafted inputs. In particular, we investigate two\ntypes of attacks -- goal hijacking and prompt leaking -- and demonstrate that\neven low-aptitude, but sufficiently ill-intentioned agents, can easily exploit\nGPT-3's stochastic nature, creating long-tail risks. The code for PromptInject\nis available at https://github.com/agencyenterprise/PromptInject.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_F/0/1/0/all/0/1\">F&#xe1;bio Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_I/0/1/0/all/0/1\">Ian Ribeiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Building Text-To-Speech Systems for the Next Billion Users. (arXiv:2211.09536v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09536","description":"<p>Deep learning based text-to-speech (TTS) systems have been evolving rapidly\nwith advances in model architectures, training methodologies, and\ngeneralization across speakers and languages. However, these advances have not\nbeen thoroughly investigated for Indian language speech synthesis. Such\ninvestigation is computationally expensive given the number and diversity of\nIndian languages, relatively lower resource availability, and the diverse set\nof advances in neural TTS that remain untested. In this paper, we evaluate the\nchoice of acoustic models, vocoders, supplementary loss functions, training\nschedules, and speaker and language diversity for Dravidian and Indo-Aryan\nlanguages. Based on this, we identify monolingual models with FastPitch and\nHiFi-GAN V1, trained jointly on male and female speakers to perform the best.\nWith this setup, we train and evaluate TTS models for 13 languages and find our\nmodels to significantly improve upon existing models in all languages as\nmeasured by mean opinion scores. We open-source all models on the Bhashini\nplatform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1\">Gokul Karthik Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V_P/0/1/0/all/0/1\">Praveen S V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandakumar_K/0/1/0/all/0/1\">Karthik Nandakumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modal Adapter for Text-Video Retrieval. (arXiv:2211.09623v1 [cs.CV])","link":"http://arxiv.org/abs/2211.09623","description":"<p>Text-video retrieval is an important multi-modal learning task, where the\ngoal is to retrieve the most relevant video for a given text query. Recently,\npre-trained models, e.g., CLIP, show great potential on this task. However, as\npre-trained models are scaling up, fully fine-tuning them on text-video\nretrieval datasets has a high risk of overfitting. Moreover, in practice, it\nwould be costly to train and store a large model for each task. To overcome the\nabove issues, we present a novel $\\textbf{Cross-Modal Adapter}$ for\nparameter-efficient fine-tuning. Inspired by adapter-based methods, we adjust\nthe pre-trained model with a few parameterization layers. However, there are\ntwo notable differences. First, our method is designed for the multi-modal\ndomain. Secondly, it allows early cross-modal interactions between CLIP's two\nencoders. Although surprisingly simple, our approach has three notable\nbenefits: (1) reduces $\\textbf{99.6}\\%$ of fine-tuned parameters, and\nalleviates the problem of overfitting, (2) saves approximately 30% of training\ntime, and (3) allows all the pre-trained parameters to be fixed, enabling the\npre-trained model to be shared across datasets. Extensive experiments\ndemonstrate that, without bells and whistles, it achieves superior or\ncomparable performance compared to fully fine-tuned methods on MSR-VTT, MSVD,\nVATEX, ActivityNet, and DiDeMo datasets. The code will be available at\n\\url{https://github.com/LeapLabTHU/Cross-Modal-Adapter}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haojun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianke Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1\">Chunjiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Z/0/1/0/all/0/1\">Zanlin Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyse der Entwicklungstreiber milit\\\"arischer Schwarmdrohnen durch Natural Language Processing. (arXiv:2211.09680v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09680","description":"<p>Military drones are taking an increasingly prominent role in armed conflict,\nand the use of multiple drones in a swarm can be useful. Who the drivers of the\nresearch are and what sub-domains exist is analyzed and visually presented in\nthis research using NLP techniques based on 946 studies. Most research is\nconducted in the Western world, led by the United States, the United Kingdom,\nand Germany. Through Tf-idf scoring, it is shown that countries have\nsignificant differences in the subdomains studied. Overall, 2019 and 2020 saw\nthe most works published, with significant interest in military swarm drones as\nearly as 2008. This study provides a first glimpse into research in this area\nand prompts further investigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mundt_M/0/1/0/all/0/1\">Manuel Mundt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Effectiveness of Bidirectional Generative Patent Language Models. (arXiv:2211.09690v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09690","description":"<p>Generative patent language models can assist humans to write patent text more\neffectively. The question is how to measure effectiveness from a human-centric\nperspective and how to improve effectiveness. In this manuscript, a simplified\ndesign of the autocomplete function is proposed to increase effectiveness by\nmore than 10%. With the new design, the effectiveness of autocomplete can reach\nmore than 60%, which means that more than 60% of keystrokes can be saved by\nautocomplete. Since writing patent text does not necessarily start from the\nbeginning to the end, a question is whether the generative model can assist a\nuser no matter where to start writing. To answer the question, the generative\nmodels in this manuscript are pre-trained with training data in both\ndirections. The generative models become bidirectional. Since text generation\nis bidirectional, the calculation of autocomplete effectiveness can be\nbidirectional and starts from anywhere in the text. After thorough experiments,\na key finding is that the autocomplete effectiveness of a model for the same\ntext remains similar no matter where the calculation starts. The finding\nindicates that such bidirectional models can assist a user at a similar level,\nno matter where the user starts to write.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jieh-Sheng Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptCap: Prompt-Guided Task-Aware Image Captioning. (arXiv:2211.09699v1 [cs.CV])","link":"http://arxiv.org/abs/2211.09699","description":"<p>Image captioning aims to describe an image with a natural language sentence,\nallowing powerful language models to understand images. The framework of\ncombining image captioning with language models has been successful on various\nvision-language tasks. However, an image contains much more information than a\nsingle sentence, leading to underspecification of which visual entities should\nbe described in the caption sentence. For example, when performing visual\nquestioning answering (VQA), generic image captions often miss visual details\nthat are essential for the language model to answer correctly. To address this\nchallenge, we propose PromptCap, a captioning model that takes a\nnatural-language prompt to control the contents of the generated caption. The\nprompt contains a question that the caption should help to answer, and also\nsupports taking auxiliary text inputs such as scene text within the image\nitself. To finetune a general image caption model for prompt-guided captioning,\nwe propose a pipeline to synthesize and filter training examples with GPT-3 and\nexisting VQA datasets. For evaluation, we start with an existing pipeline in\nwhich a language model is prompted with image captions to carry out VQA. With\nthe same language model, a higher QA accuracy shows that our generated captions\nare more relevant to the question prompts. PromptCap outperforms generic\ncaptions by a large margin on a variety of VQA tasks and achieves the\nstate-of-the-art accuracy of 58.8 % on OK-VQA and 58.0 % on A-OKVQA. Zero-shot\nexperiments on WebQA show that PromptCap generalizes well to unseen domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yushi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_H/0/1/0/all/0/1\">Hang Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weijia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style Classification of Rabbinic Literature for Detection of Lost Midrash Tanhuma Material. (arXiv:2211.09710v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09710","description":"<p>Midrash collections are complex rabbinic works that consist of text in\nmultiple languages, which evolved through long processes of unstable oral and\nwritten transmission. Determining the origin of a given passage in such a\ncompilation is not always straightforward and is often a matter of dispute\namong scholars, yet it is essential for scholars' understanding of the passage\nand its relationship to other texts in the rabbinic corpus.\n</p>\n<p>To help solve this problem, we propose a system for classification of\nrabbinic literature based on its style, leveraging recently released pretrained\nTransformer models for Hebrew. Additionally, we demonstrate how our method can\nbe applied to uncover lost material from Midrash Tanhuma.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tannor_S/0/1/0/all/0/1\">Shlomo Tannor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dershowitz_N/0/1/0/all/0/1\">Nachum Dershowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavee_M/0/1/0/all/0/1\">Moshe Lavee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Design Considerations For Hypothesis Rejection Modules In Spoken Language Understanding Systems. (arXiv:2211.09711v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09711","description":"<p>Spoken Language Understanding (SLU) systems typically consist of a set of\nmachine learning models that operate in conjunction to produce an SLU\nhypothesis. The generated hypothesis is then sent to downstream components for\nfurther action. However, it is desirable to discard an incorrect hypothesis\nbefore sending it downstream. In this work, we present two designs for SLU\nhypothesis rejection modules: (i) scheme R1 that performs rejection on domain\nspecific SLU hypothesis and, (ii) scheme R2 that performs rejection on\nhypothesis generated from the overall SLU system. Hypothesis rejection modules\nin both schemes reject/accept a hypothesis based on features drawn from the\nutterance directed to the SLU system, the associated SLU hypothesis and SLU\nconfidence score. Our experiments suggest that both the schemes yield similar\nresults (scheme R1: 2.5% FRR @ 4.5% FAR, scheme R2: 2.5% FRR @ 4.6% FAR), with\nthe best performing systems using all the available features. We argue that\nwhile either of the rejection schemes can be chosen over the other, they carry\nsome inherent differences which need to be considered while making this choice.\nAdditionally, we incorporate ASR features in the rejection module (obtaining an\n1.9% FRR @ 3.8% FAR) and analyze the improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alok_A/0/1/0/all/0/1\">Aman Alok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananthakrishnan_S/0/1/0/all/0/1\">Shankar Ananthakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Numerical Optimizations for Weighted Low-rank Estimation on Language Model. (arXiv:2211.09718v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09718","description":"<p>Singular value decomposition (SVD) is one of the most popular compression\nmethods that approximate a target matrix with smaller matrices. However,\nstandard SVD treats the parameters within the matrix with equal importance,\nwhich is a simple but unrealistic assumption. The parameters of a trained\nneural network model may affect task performance unevenly, which suggests\nnon-equal importance among the parameters. Compared to SVD, the decomposition\nmethod aware of parameter importance is the more practical choice in real\ncases. Unlike standard SVD, weighted value decomposition is a non-convex\noptimization problem that lacks a closed-form solution. We systematically\ninvestigated multiple optimization strategies to tackle the problem and\nexamined our method by compressing Transformer-based language models. Further,\nwe designed a metric to predict when the SVD may introduce a significant\nperformance drop, for which our method can be a rescue strategy. The extensive\nevaluations demonstrate that our method can perform better than current SOTA\nmethods in compressing Transformer-based language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_T/0/1/0/all/0/1\">Ting Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yen-Chang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Felicity Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Q/0/1/0/all/0/1\">Qian Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yilin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hongxia Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Multilingual Models for Medical Transcript Analysis. (arXiv:2211.09722v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09722","description":"<p>Federated Learning (FL) is a novel machine learning approach that allows the\nmodel trainer to access more data samples, by training the model across\nmultiple decentralized data sources, while data access constraints are in\nplace. Such trained models can achieve significantly higher performance beyond\nwhat can be done when trained on a single data source. As part of FL's\npromises, none of the training data is ever transmitted to any central\nlocation, ensuring that sensitive data remains local and private. These\ncharacteristics make FL perfectly suited for large-scale applications in\nhealthcare, where a variety of compliance constraints restrict how data may be\nhandled, processed, and stored. Despite the apparent benefits of federated\nlearning, the heterogeneity in the local data distributions pose significant\nchallenges, and such challenges are even more pronounced in the case of\nmultilingual data providers. In this paper we present a federated learning\nsystem for training a large-scale multi-lingual model suitable for fine-tuning\non downstream tasks such as medical entity tagging. Our work represents one of\nthe first such production-scale systems, capable of training across multiple\nhighly heterogeneous data providers, and achieving levels of accuracy that\ncould not be otherwise achieved by using central training with public data.\nFinally, we show that the global model performance can be further improved by a\ntraining step performed locally.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manoel_A/0/1/0/all/0/1\">Andre Manoel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_M/0/1/0/all/0/1\">Mirian Hipolito Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumel_T/0/1/0/all/0/1\">Tal Baumel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shize Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jialei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_D/0/1/0/all/0/1\">Dan Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karmon_D/0/1/0/all/0/1\">Danny Karmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_R/0/1/0/all/0/1\">Robert Sim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitriadis_D/0/1/0/all/0/1\">Dimitrios Dimitriadis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Adversarial Training Can Improve Neural Language Models. (arXiv:2211.09728v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09728","description":"<p>While deep learning in the form of recurrent neural networks (RNNs) has\ncaused a significant improvement in neural language modeling, the fact that\nthey are extremely prone to overfitting is still a mainly unresolved issue. In\nthis paper we propose a regularization method based on generative adversarial\nnetworks (GANs) and adversarial training (AT), that can prevent overfitting in\nneural language models. Unlike common adversarial training methods such as the\nfast gradient sign method (FGSM) that require a second back-propagation through\ntime, and therefore effectively require at least twice the amount of time for\nregular training, the overhead of our method does not exceed more than 20% of\nthe training of the baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Movahedi_S/0/1/0/all/0/1\">Sajad Movahedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakery_A/0/1/0/all/0/1\">Azadeh Shakery</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stutter-TTS: Controlled Synthesis and Improved Recognition of Stuttered Speech. (arXiv:2211.09731v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09731","description":"<p>Stuttering is a speech disorder where the natural flow of speech is\ninterrupted by blocks, repetitions or prolongations of syllables, words and\nphrases. The majority of existing automatic speech recognition (ASR) interfaces\nperform poorly on utterances with stutter, mainly due to lack of matched\ntraining data. Synthesis of speech with stutter thus presents an opportunity to\nimprove ASR for this type of speech. We describe Stutter-TTS, an end-to-end\nneural text-to-speech model capable of synthesizing diverse types of stuttering\nutterances. We develop a simple, yet effective prosody-control strategy whereby\nadditional tokens are introduced into source text during training to represent\nspecific stuttering characteristics. By choosing the position of the stutter\ntokens, Stutter-TTS allows word-level control of where stuttering occurs in the\nsynthesized utterance. We are able to synthesize stutter events with high\naccuracy (F1-scores between 0.63 and 0.84, depending on stutter type). By\nfine-tuning an ASR model on synthetic stuttered speech we are able to reduce\nword error by 5.7% relative on stuttered utterances, with only minor (&lt;0.2%\nrelative) degradation for fluent utterances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valles_Perez_I/0/1/0/all/0/1\">Iv&#xe1;n Vall&#xe9;s-P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1\">Andreas Stolcke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chengzhu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droppo_J/0/1/0/all/0/1\">Jasha Droppo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shonibare_O/0/1/0/all/0/1\">Olabanji Shonibare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barra_Chicote_R/0/1/0/all/0/1\">Roberto Barra-Chicote</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_V/0/1/0/all/0/1\">Venkatesh Ravichandran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extending Logic Explained Networks to Text Classification. (arXiv:2211.09732v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09732","description":"<p>Recently, Logic Explained Networks (LENs) have been proposed as\nexplainable-by-design neural models providing logic explanations for their\npredictions. However, these models have only been applied to vision and tabular\ndata, and they mostly favour the generation of global explanations, while local\nones tend to be noisy and verbose. For these reasons, we propose LENp,\nimproving local explanations by perturbing input words, and we test it on text\nclassification. Our results show that (i) LENp provides better local\nexplanations than LIME in terms of sensitivity and faithfulness, and (ii) logic\nexplanations are more useful and user-friendly than feature scoring provided by\nLIME as attested by a human survey.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rishabh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciravegna_G/0/1/0/all/0/1\">Gabriele Ciravegna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1\">Pietro Barbiero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1\">Francesco Giannini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buffelli_D/0/1/0/all/0/1\">Davide Buffelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Lio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT-Deep CNN: State-of-the-Art for Sentiment Analysis of COVID-19 Tweets. (arXiv:2211.09733v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09733","description":"<p>The free flow of information has been accelerated by the rapid development of\nsocial media technology. There has been a significant social and psychological\nimpact on the population due to the outbreak of Coronavirus disease (COVID-19).\nThe COVID-19 pandemic is one of the current events being discussed on social\nmedia platforms. In order to safeguard societies from this pandemic, studying\npeople's emotions on social media is crucial. As a result of their particular\ncharacteristics, sentiment analysis of texts like tweets remains challenging.\nSentiment analysis is a powerful text analysis tool. It automatically detects\nand analyzes opinions and emotions from unstructured data. Texts from a wide\nrange of sources are examined by a sentiment analysis tool, which extracts\nmeaning from them, including emails, surveys, reviews, social media posts, and\nweb articles. To evaluate sentiments, natural language processing (NLP) and\nmachine learning techniques are used, which assign weights to entities, topics,\nthemes, and categories in sentences or phrases. Machine learning tools learn\nhow to detect sentiment without human intervention by examining examples of\nemotions in text. In a pandemic situation, analyzing social media texts to\nuncover sentimental trends can be very helpful in gaining a better\nunderstanding of society's needs and predicting future trends. We intend to\nstudy society's perception of the COVID-19 pandemic through social media using\nstate-of-the-art BERT and Deep CNN models. The superiority of BERT models over\nother deep models in sentiment analysis is evident and can be concluded from\nthe comparison of the various research studies mentioned in this article.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joloudari_J/0/1/0/all/0/1\">Javad Hassannataj Joloudari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_S/0/1/0/all/0/1\">Sadiq Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nematollahi_M/0/1/0/all/0/1\">Mohammad Ali Nematollahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagheri_R/0/1/0/all/0/1\">Rouhollah Bagheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazl_F/0/1/0/all/0/1\">Fatemeh Fazl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alizadehsani_R/0/1/0/all/0/1\">Roohallah Alizadehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lashgari_R/0/1/0/all/0/1\">Reza Lashgari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Dynamic Quantization for Transformer Inference. (arXiv:2211.09744v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09744","description":"<p>We introduce a novel run-time method for significantly reducing the accuracy\nloss associated with quantizing BERT-like models to 8-bit integers. Existing\nmethods for quantizing models either modify the training procedure,or they\nrequire an additional calibration step to adjust parameters that also requires\na selected held-out dataset. Our method permits taking advantage of\nquantization without the need for these adjustments. We present results on\nseveral NLP tasks demonstrating the usefulness of this technique.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+El_Kurdi_Y/0/1/0/all/0/1\">Yousef El-Kurdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quinn_J/0/1/0/all/0/1\">Jerry Quinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1\">Avirup Sil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing for Incremental Parse States in Autoregressive Language Models. (arXiv:2211.09748v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09748","description":"<p>Next-word predictions from autoregressive neural language models show\nremarkable sensitivity to syntax. This work evaluates the extent to which this\nbehavior arises as a result of a learned ability to maintain implicit\nrepresentations of incremental syntactic structures. We extend work in\nsyntactic probing to the incremental setting and present several probes for\nextracting incomplete syntactic structure (operationalized through parse states\nfrom a stack-based parser) from autoregressive language models. We find that\nour probes can be used to predict model preferences on ambiguous sentence\nprefixes and causally intervene on model representations and steer model\nbehavior. This suggests implicit incremental syntactic inferences underlie\nnext-word predictions in autoregressive neural language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eisape_T/0/1/0/all/0/1\">Tiwalayo Eisape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangireddy_V/0/1/0/all/0/1\">Vineet Gangireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger P. Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Transformers with Dynamic Token Pooling. (arXiv:2211.09761v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09761","description":"<p>Transformers achieve unrivalled performance in modelling language, but remain\ninefficient in terms of memory and time complexity. A possible remedy is to\nreduce the sequence length in the intermediate layers by pooling fixed-length\nsegments of tokens. Nevertheless, natural units of meaning, such as words or\nphrases, display varying sizes. To address this mismatch, we equip language\nmodels with a dynamic-pooling mechanism, which predicts segment boundaries in\nan autoregressive fashion. We compare several methods to infer boundaries,\nincluding end-to-end learning through stochastic re-parameterisation,\nsupervised learning (based on segmentations from subword tokenizers or spikes\nin conditional entropy), as well as linguistically motivated boundaries. We\nperform character-level evaluation on texts from multiple datasets and\nmorphologically diverse languages. The results demonstrate that dynamic\npooling, which jointly segments and models language, is often both faster and\nmore accurate than vanilla Transformers and fixed-length pooling within the\nsame computational budget.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nawrot_P/0/1/0/all/0/1\">Piotr Nawrot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chorowski_J/0/1/0/all/0/1\">Jan Chorowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lancucki_A/0/1/0/all/0/1\">Adrian &#x141;a&#x144;cucki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo M. Ponti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniSumm: Unified Few-shot Summarization with Multi-Task Pre-Training and Prefix-Tuning. (arXiv:2211.09783v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09783","description":"<p>The diverse demands of different summarization tasks and their high\nannotation costs are driving a need for few-shot summarization. However,\ndespite the emergence of many summarization tasks and datasets, the current\ntraining paradigm for few-shot summarization systems ignores potentially\nshareable knowledge in heterogeneous datasets. To this end, we propose\n\\textsc{UniSumm}, a unified few-shot summarization model pre-trained with\nmultiple summarization tasks and can be prefix-tuned to excel at any few-shot\nsummarization datasets. Meanwhile, to better evaluate few-shot summarization\nsystems, under the principles of diversity and robustness, we assemble and\npublicize a new benchmark \\textsc{SummZoo}. It consists of $8$ diverse\nsummarization tasks with multiple sets of few-shot samples for each task,\ncovering both monologue and dialogue domains. Experimental results and ablation\nstudies show that \\textsc{UniSumm} outperforms strong baseline systems by a\nlarge margin across all tasks in \\textsc{SummZoo} under both automatic and\nhuman evaluations. We release our code and benchmark at\n\\url{https://github.com/microsoft/UniSumm}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InstructPix2Pix: Learning to Follow Image Editing Instructions. (arXiv:2211.09800v1 [cs.CV])","link":"http://arxiv.org/abs/2211.09800","description":"<p>We propose a method for editing images from human instructions: given an\ninput image and a written instruction that tells the model what to do, our\nmodel follows these instructions to edit the image. To obtain training data for\nthis problem, we combine the knowledge of two large pretrained models -- a\nlanguage model (GPT-3) and a text-to-image model (Stable Diffusion) -- to\ngenerate a large dataset of image editing examples. Our conditional diffusion\nmodel, InstructPix2Pix, is trained on our generated data, and generalizes to\nreal images and user-written instructions at inference time. Since it performs\nedits in the forward pass and does not require per example fine-tuning or\ninversion, our model edits images quickly, in a matter of seconds. We show\ncompelling editing results for a diverse collection of input images and written\ninstructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brooks_T/0/1/0/all/0/1\">Tim Brooks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holynski_A/0/1/0/all/0/1\">Aleksander Holynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1\">Alexei A. Efros</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improve Cross-lingual Voice Cloning Using Low-quality Code-switched Data. (arXiv:2110.07210v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2110.07210","description":"<p>Recently, sequence-to-sequence (seq-to-seq) models have been successfully\napplied in text-to-speech (TTS) to synthesize speech for single-language text.\nTo synthesize speech for multiple languages usually requires multi-lingual\nspeech from the target speaker. However, it is both laborious and expensive to\ncollect high-quality multi-lingual TTS data for the target speakers. In this\npaper, we proposed to use low-quality code-switched found data from the\nnon-target speakers to achieve cross-lingual voice cloning for the target\nspeakers. Experiments show that our proposed method can generate high-quality\ncode-switched speech in the target voices in terms of both naturalness and\nspeaker consistency. More importantly, we find that our method can achieve a\ncomparable result to the state-of-the-art (SOTA) performance in cross-lingual\nvoice cloning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haitong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yue Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Pretrained Models of Source Code. (arXiv:2202.08975v3 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2202.08975","description":"<p>Deep learning models are widely used for solving challenging code processing\ntasks, such as code generation or code summarization. Traditionally, a specific\nmodel architecture was carefully built to solve a particular code processing\ntask. However, recently general pretrained models such as CodeBERT or CodeT5\nhave been shown to outperform task-specific models in many applications. While\npretrained models are known to learn complex patterns from data, they may fail\nto understand some properties of source code. To test diverse aspects of code\nunderstanding, we introduce a set of diagnosting probing tasks. We show that\npretrained models of code indeed contain information about code syntactic\nstructure and correctness, the notions of identifiers, data flow and\nnamespaces, and natural language naming. We also investigate how probing\nresults are affected by using code-specific pretraining objectives, varying the\nmodel size, or finetuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Troshin_S/0/1/0/all/0/1\">Sergey Troshin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1\">Nadezhda Chirkova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating the Uncertainty in Emotion Class Labels with Utterance-Specific Dirichlet Priors. (arXiv:2203.04443v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.04443","description":"<p>Emotion recognition is a key attribute for artificial intelligence systems\nthat need to naturally interact with humans. However, the task definition is\nstill an open problem due to the inherent ambiguity of emotions. In this paper,\na novel Bayesian training loss based on per-utterance Dirichlet prior\ndistributions is proposed for verbal emotion recognition, which models the\nuncertainty in one-hot labels created when human annotators assign the same\nutterance to different emotion classes. An additional metric is used to\nevaluate the performance by detection test utterances with high labelling\nuncertainty. This removes a major limitation that emotion classification\nsystems only consider utterances with labels where the majority of annotators\nagree on the emotion class. Furthermore, a frequentist approach is studied to\nleverage the continuous-valued \"soft\" labels obtained by averaging the one-hot\nlabels. We propose a two-branch model structure for emotion classification on a\nper-utterance basis, which achieves state-of-the-art classification results on\nthe widely used IEMOCAP dataset. Based on this, uncertainty estimation\nexperiments were performed. The best performance in terms of the area under the\nprecision-recall curve when detecting utterances with high uncertainty was\nachieved by interpolating the Bayesian training loss with the Kullback-Leibler\ndivergence training loss for the soft labels. The generality of the proposed\napproach was verified using the MSP-Podcast dataset which yielded the same\npattern of results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xixin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT-ASC: Implicit Aspect Representation Learning through Auxiliary-Sentence Construction for Sentiment Analysis. (arXiv:2203.11702v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.11702","description":"<p>Aspect-based sentiment analysis (ABSA) task aim at associating a piece of\ntext with a set of aspects and meanwhile infer their respective sentimental\npolarities. The state-of-the-art approaches are built upon fine-tuning of\nvarious pre-trained language models. They commonly attempt to learn\naspect-specific representation from the corpus. Unfortunately, the aspect is\noften expressed implicitly through a set of representatives and thus renders\nimplicit mapping process unattainable unless sufficient labeled examples are\navailable. However, high-quality labeled examples may not be readily available\nin real-world scenarios. In this paper, we propose to jointly address aspect\ncategorization and aspect-based sentiment subtasks in a unified framework.\nSpecifically, we first introduce a simple but effective mechanism to construct\nan auxiliary-sentence for the implicit aspect based on the semantic information\nin the corpus. Then, we encourage BERT to learn the aspect-specific\nrepresentation in response to the automatically constructed auxiliary-sentence\ninstead of the aspect itself. Finally, we empirically evaluate the performance\nof the proposed solution by a comparative study on real benchmark datasets for\nboth ABSA and Targeted-ABSA tasks. Our extensive experiments show that it\nconsistently achieves state-of-the-art performance in terms of aspect\ncategorization and aspect-based sentiment across all datasets and the\nimprovement margins are considerable. The code of BERT-ASC is available in\nGitHub: https://github.com/amurtadha/BERT-ASC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1\">Murtadha Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shengfeng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jianlin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xinxin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenze Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bo Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunfeng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Speech Recognition for Speech Assessment of Persian Preschool Children. (arXiv:2203.12886v9 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12886","description":"<p>Preschool evaluation is crucial because it gives teachers and parents\ninfluential knowledge about children's growth and development. The COVID-19\npandemic has highlighted the necessity of online assessment for preschool\nchildren. One of the areas that should be tested is their ability to speak.\nEmploying an Automatic Speech Recognition (ASR) system would not help since\nthey are pre-trained on voices that differ from children's in terms of\nfrequency and amplitude. Because most of these are pre-trained with data in a\nspecific range of amplitude, their objectives do not make them ready for voices\nin different amplitudes. To overcome this issue, we added a new objective to\nthe masking objective of the Wav2Vec 2.0 model called Random Frequency Pitch\n(RFP). In addition, we used our newly introduced dataset to fine-tune our model\nfor Meaningless Words (MW) and Rapid Automatic Naming (RAN) tests. Using\nmasking in concatenation with RFP outperforms the masking objective of Wav2Vec\n2.0 by reaching a Word Error Rate (WER) of 1.35. Our new approach reaches a WER\nof 6.45 on the Persian section of the CommonVoice dataset. Furthermore, our\nnovel methodology produces positive outcomes in zero- and few-shot scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abaskohi_A/0/1/0/all/0/1\">Amirhossein Abaskohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortazavi_F/0/1/0/all/0/1\">Fatemeh Mortazavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1\">Hadi Moradi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Simultaneous Speech Translation need Simultaneous Models?. (arXiv:2204.03783v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.03783","description":"<p>In simultaneous speech translation (SimulST), finding the best trade-off\nbetween high translation quality and low latency is a challenging task. To meet\nthe latency constraints posed by the different application scenarios, multiple\ndedicated SimulST models are usually trained and maintained, generating high\ncomputational costs. In this paper, motivated by the increased social and\nenvironmental impact caused by these costs, we investigate whether a single\nmodel trained offline can serve not only the offline but also the simultaneous\ntask without the need for any additional training or adaptation. Experiments on\nen-&gt;{de, es} indicate that, aside from facilitating the adoption of\nwell-established offline techniques and architectures without affecting\nlatency, the offline solution achieves similar or better translation quality\ncompared to the same model trained in simultaneous settings, as well as being\ncompetitive with the SimulST state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Contamination Helps Explain the Cross-lingual Capabilities of English Pretrained Models. (arXiv:2204.08110v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08110","description":"<p>English pretrained language models, which make up the backbone of many modern\nNLP systems, require huge amounts of unlabeled training data. These models are\ngenerally presented as being trained only on English text but have been found\nto transfer surprisingly well to other languages. We investigate this\nphenomenon and find that common English pretraining corpora actually contain\nsignificant amounts of non-English text: even when less than 1% of data is not\nEnglish (well within the error rate of strong language classifiers), this leads\nto hundreds of millions of foreign language tokens in large-scale datasets. We\nthen demonstrate that even these small percentages of non-English data\nfacilitate cross-lingual transfer for models trained on them, with target\nlanguage performance strongly correlated to the amount of in-language data seen\nduring pretraining. In light of these findings, we argue that no model is truly\nmonolingual when pretrained at scale, which should be considered when\nevaluating cross-lingual transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blevins_T/0/1/0/all/0/1\">Terra Blevins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Language Models with Language Feedback. (arXiv:2204.14146v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.14146","description":"<p>Pretrained language models often do not perform tasks in ways that are in\nline with our preferences, e.g., generating offensive text or factually\nincorrect summaries. Recent work approaches the above issue by learning from a\nsimple form of human evaluation: comparisons between pairs of model-generated\ntask outputs. Comparison feedback conveys limited information about human\npreferences per human evaluation. Here, we propose to learn from natural\nlanguage feedback, which conveys more information per human evaluation. We\nlearn from language feedback on model outputs using a three-step learning\nalgorithm. First, we condition the language model on the initial output and\nfeedback to generate many refinements. Second, we choose the refinement with\nthe highest similarity to the feedback. Third, we finetune a language model to\nmaximize the likelihood of the chosen refinement given the input. In synthetic\nexperiments, we first evaluate whether language models accurately incorporate\nfeedback to produce refinements, finding that only large language models (175B\nparameters) do so. Using only 100 samples of human-written feedback, our\nlearning algorithm finetunes a GPT-3 model to roughly human-level summarization\nability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scheurer_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;my Scheurer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_J/0/1/0/all/0/1\">Jon Ander Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1\">Jun Shern Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Angelica Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Distributional Properties Drive Emergent In-Context Learning in Transformers. (arXiv:2205.05055v6 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.05055","description":"<p>Large transformer-based models are able to perform in-context few-shot\nlearning, without being explicitly trained for it. This observation raises the\nquestion: what aspects of the training regime lead to this emergent behavior?\nHere, we show that this behavior is driven by the distributions of the training\ndata itself. In-context learning emerges when the training data exhibits\nparticular distributional properties such as burstiness (items appear in\nclusters rather than being uniformly distributed over time) and having large\nnumbers of rarely occurring classes. In-context learning also emerges more\nstrongly when item meanings or interpretations are dynamic rather than fixed.\nThese properties are exemplified by natural language, but are also inherent to\nnaturalistic data in a wide range of other domains. They also depart\nsignificantly from the uniform, i.i.d. training distributions typically used\nfor standard supervised learning. In our initial experiments, we found that\nin-context learning traded off against more conventional weight-based learning,\nand models were unable to achieve both simultaneously. However, our later\nexperiments uncovered that the two modes of learning could co-exist in a single\nmodel when it was trained on data following a skewed Zipfian distribution --\nanother common property of naturalistic data, including language. In further\nexperiments, we found that naturalistic data distributions were only able to\nelicit in-context learning in transformers, and not in recurrent models. In\nsum, our findings indicate how the transformer architecture works together with\nparticular properties of the training data to drive the intriguing emergent\nin-context learning behaviour of large language models, and how future work\nmight encourage both in-context and in-weights learning in domains beyond\nlanguage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">Stephanie C.Y. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santoro_A/0/1/0/all/0/1\">Adam Santoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1\">Andrew K. Lampinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jane X. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aaditya Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richemond_P/0/1/0/all/0/1\">Pierre H. Richemond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McClelland_J/0/1/0/all/0/1\">Jay McClelland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Learning of Hierarchical Conversation Structure. (arXiv:2205.12244v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12244","description":"<p>Human conversations can evolve in many different ways, creating challenges\nfor automatic understanding and summarization. Goal-oriented conversations\noften have meaningful sub-dialogue structure, but it can be highly\ndomain-dependent. This work introduces an unsupervised approach to learning\nhierarchical conversation structure, including turn and sub-dialogue segment\nlabels, corresponding roughly to dialogue acts and sub-tasks, respectively. The\ndecoded structure is shown to be useful in enhancing neural models of language\nfor three conversation-level understanding tasks. Further, the learned\nfinite-state sub-dialogue network is made interpretable through automatic\nsummarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1\">Bo-Ru Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yushi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1\">Mari Ostendorf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quark: Controllable Text Generation with Reinforced Unlearning. (arXiv:2205.13636v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.13636","description":"<p>Large-scale language models often learn behaviors that are misaligned with\nuser expectations. Generated text may contain offensive or toxic language,\ncontain significant repetition, or be of a different sentiment than desired by\nthe user. We consider the task of unlearning these misalignments by fine-tuning\nthe language model on signals of what not to do. We introduce Quantized Reward\nKonditioning (Quark), an algorithm for optimizing a reward function that\nquantifies an (un)wanted property, while not straying too far from the original\nmodel. Quark alternates between (i) collecting samples with the current\nlanguage model, (ii) sorting them into quantiles based on reward, with each\nquantile identified by a reward token prepended to the language model's input,\nand (iii) using a standard language modeling loss on samples from each quantile\nconditioned on its reward token, while remaining nearby the original language\nmodel via a KL-divergence penalty. By conditioning on a high-reward token at\ngeneration time, the model generates text that exhibits less of the unwanted\nproperty. For unlearning toxicity, negative sentiment, and repetition, our\nexperiments show that Quark outperforms both strong baselines and\nstate-of-the-art reinforcement learning methods like PPO (Schulman et al.\n2017), while relying only on standard language modeling primitives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Lianhui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NatiQ: An End-to-end Text-to-Speech System for Arabic. (arXiv:2206.07373v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.07373","description":"<p>NatiQ is end-to-end text-to-speech system for Arabic. Our speech synthesizer\nuses an encoder-decoder architecture with attention. We used both\ntacotron-based models (tacotron-1 and tacotron-2) and the faster transformer\nmodel for generating mel-spectrograms from characters. We concatenated\nTacotron1 with the WaveRNN vocoder, Tacotron2 with the WaveGlow vocoder and\nESPnet transformer with the parallel wavegan vocoder to synthesize waveforms\nfrom the spectrograms. We used in-house speech data for two voices: 1) neutral\nmale \"Hamza\"- narrating general content and news, and 2) expressive female\n\"Amina\"- narrating children story books to train our models. Our best systems\nachieve an average Mean Opinion Score (MOS) of 4.21 and 4.40 for Amina and\nHamza respectively. The objective evaluation of the systems using word and\ncharacter error rate (WER and CER) as well as the response time measured by\nreal-time factor favored the end-to-end architecture ESPnet. NatiQ demo is\navailable on-line at https://tts.qcri.org\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelali_A/0/1/0/all/0/1\">Ahmed Abdelali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1\">Nadir Durrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demiroglu_C/0/1/0/all/0/1\">Cenk Demiroglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_F/0/1/0/all/0/1\">Fahim Dalvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darwish_K/0/1/0/all/0/1\">Kareem Darwish</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not Cheating on the Turing Test: Towards Grounded Language Learning in Artificial Intelligence. (arXiv:2206.14672v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.14672","description":"<p>Recent hype surrounding the increasing sophistication of language processing\nmodels has renewed optimism regarding machines achieving a human-like command\nof natural language. Research in the area of natural language understanding\n(NLU) in artificial intelligence claims to have been making great strides in\nthis area, however, the lack of conceptual clarity/consistency in how\n'understanding' is used in this and other disciplines makes it difficult to\ndiscern how close we actually are. In this interdisciplinary research thesis, I\nintegrate insights from cognitive science/psychology, philosophy of mind, and\ncognitive linguistics, and evaluate it against a critical review of current\napproaches in NLU to explore the basic requirements--and remaining\nchallenges--for developing artificially intelligent systems with human-like\ncapacities for language use and comprehension.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alberts_L/0/1/0/all/0/1\">Lize Alberts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Vs. MLP-Mixer: Exponential Expressive Gap For NLP Problems. (arXiv:2208.08191v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.08191","description":"<p>Vision-Transformers are widely used in various vision tasks. Meanwhile, there\nis another line of works starting with the MLP-mixer trying to achieve similar\nperformance using mlp-based architectures. Interestingly, until now those\nmlp-based architectures have not been adapted for NLP tasks. Additionally,\nuntil now, mlp-based architectures have failed to achieve state-of-the-art\nperformance in vision tasks. In this paper, we analyze the expressive power of\nmlp-based architectures in modeling dependencies between multiple different\ninputs simultaneously, and show an exponential gap between the attention and\nthe mlp-based mechanisms. Our results suggest a theoretical explanation for the\nmlp inability to compete with attention-based mechanisms in NLP problems, they\nalso suggest that the performance gap in vision tasks may be due to the mlp\nrelative weakness in modeling dependencies between multiple different\nlocations, and that combining smart input permutations with mlp architectures\nmay not be enough to close the performance gap alone.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Navon_D/0/1/0/all/0/1\">Dan Navon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bronstein_A/0/1/0/all/0/1\">Alex M. Bronstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text to Image Generation: Leaving no Language Behind. (arXiv:2208.09333v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.09333","description":"<p>One of the latest applications of Artificial Intelligence (AI) is to generate\nimages from natural language descriptions. These generators are now becoming\navailable and achieve impressive results that have been used for example in the\nfront cover of magazines. As the input to the generators is in the form of a\nnatural language text, a question that arises immediately is how these models\nbehave when the input is written in different languages. In this paper we\nperform an initial exploration of how the performance of three popular\ntext-to-image generators depends on the language. The results show that there\nis a significant performance degradation when using languages other than\nEnglish, especially for languages that are not widely used. This observation\nleads us to discuss different alternatives on how text-to-image generators can\nbe improved so that performance is consistent across different languages. This\nis fundamental to ensure that this new technology can be used by non-native\nEnglish speakers and to preserve linguistic diversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reviriego_P/0/1/0/all/0/1\">Pedro Reviriego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merino_Gomez_E/0/1/0/all/0/1\">Elena Merino-G&#xf3;mez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dodging the Data Bottleneck: Automatic Subtitling with Automatically Segmented ST Corpora. (arXiv:2209.10608v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.10608","description":"<p>Speech translation for subtitling (SubST) is the task of automatically\ntranslating speech data into well-formed subtitles by inserting subtitle breaks\ncompliant to specific displaying guidelines. Similar to speech translation\n(ST), model training requires parallel data comprising audio inputs paired with\ntheir textual translations. In SubST, however, the text has to be also\nannotated with subtitle breaks. So far, this requirement has represented a\nbottleneck for system development, as confirmed by the dearth of publicly\navailable SubST corpora. To fill this gap, we propose a method to convert\nexisting ST corpora into SubST resources without human intervention. We build a\nsegmenter model that automatically segments texts into proper subtitles by\nexploiting audio and text in a multimodal fashion, achieving high segmentation\nquality in zero-shot conditions. Comparative experiments with SubST systems\nrespectively trained on manual and automatic segmentations result in similar\nperformance, showing the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karakanta_A/0/1/0/all/0/1\">Alina Karakanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transparency Helps Reveal When Language Models Learn Meaning. (arXiv:2210.07468v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07468","description":"<p>Many current NLP systems are built from language models trained to optimize\nunsupervised objectives on large amounts of raw text. Under what conditions\nmight such a procedure acquire meaning? Our systematic experiments with\nsynthetic data reveal that, with languages where all expressions have\ncontext-independent denotations (i.e., languages with strong transparency),\nboth autoregressive and masked language models successfully learn to emulate\nsemantic relations between expressions. However, when denotations are changed\nto be context-dependent with the language otherwise unmodified, this ability\ndegrades. Turning to natural language, our experiments with a specific\nphenomenon -- referential opacity -- add to the growing body of evidence that\ncurrent language models do not well-represent natural language semantics. We\nshow this failure relates to the context-dependent nature of natural language\nform-meaning mappings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhaofeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Word Meaning Disambiguation using TimeLMs. (arXiv:2210.08207v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08207","description":"<p>Meaning of words constantly changes given the events in modern civilization.\nLarge Language Models use word embeddings, which are often static and thus\ncannot cope with this semantic change. Thus,it is important to resolve\nambiguity in word meanings. This paper is an effort in this direction, where we\nexplore methods for word sense disambiguation for the EvoNLP shared task. We\nconduct rigorous ablations for two solutions to this problem. We see that an\napproach using time-aware language models helps this task. Furthermore, we\nexplore possible future directions to this problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Godbole_M/0/1/0/all/0/1\">Mihir Godbole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandavate_P/0/1/0/all/0/1\">Parth Dandavate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kane_A/0/1/0/all/0/1\">Aditya Kane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SIT at MixMT 2022: Fluent Translation Built on Giant Pre-trained Models. (arXiv:2210.11670v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.11670","description":"<p>This paper describes the Stevens Institute of Technology's submission for the\nWMT 2022 Shared Task: Code-mixed Machine Translation (MixMT). The task\nconsisted of two subtasks, subtask $1$ Hindi/English to Hinglish and subtask\n$2$ Hinglish to English translation. Our findings lie in the improvements made\nthrough the use of large pre-trained multilingual NMT models and in-domain\ndatasets, as well as back-translation and ensemble techniques. The translation\noutput is automatically evaluated against the reference translations using\nROUGE-L and WER. Our system achieves the $1^{st}$ position on subtask $2$\naccording to ROUGE-L, WER, and human evaluation, $1^{st}$ position on subtask\n$1$ according to WER and human evaluation, and $3^{rd}$ position on subtask $1$\nwith respect to ROUGE-L metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Abdul Rafae Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanade_H/0/1/0/all/0/1\">Hrishikesh Kanade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budhrani_G/0/1/0/all/0/1\">Girish Amar Budhrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jhanglani_P/0/1/0/all/0/1\">Preet Jhanglani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jia Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autoregressive Structured Prediction with Language Models. (arXiv:2210.14698v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.14698","description":"<p>Recent years have seen a paradigm shift in NLP towards using pretrained\nlanguage models ({PLM}) for a wide range of tasks.\n</p>\n<p>However, there are many difficult design decisions to represent structures\n(e.g. tagged text, coreference chains) in a way such that they can be captured\nby PLMs.\n</p>\n<p>Prior work on structured prediction with PLMs typically flattens the\nstructured output into a sequence, which limits the quality of structural\ninformation being learned and leads to inferior performance compared to classic\ndiscriminative models.\n</p>\n<p>In this work, we describe an approach to model structures as sequences of\nactions in an autoregressive manner with PLMs, allowing in-structure\ndependencies to be learned without any loss.\n</p>\n<p>Our approach achieves the new state-of-the-art on all the structured\nprediction tasks we looked at, namely, named entity recognition, end-to-end\nrelation extraction, and coreference resolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuchen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monath_N/0/1/0/all/0/1\">Nicholas Monath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chronic pain patient narratives allow for the estimation of current pain intensity. (arXiv:2210.17473v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.17473","description":"<p>Chronic pain is a multi-dimensional experience, and pain intensity plays an\nimportant part, impacting the patients emotional balance, psychology, and\nbehaviour. Standard self-reporting tools, such as the Visual Analogue Scale for\npain, fail to capture this burden. Moreover, this type of tools is susceptible\nto a degree of subjectivity, dependent on the patients clear understanding of\nhow to use it, social biases, and their ability to translate a complex\nexperience to a scale. To overcome these and other self-reporting challenges,\npain intensity estimation has been previously studied based on facial\nexpressions, electroencephalograms, brain imaging, and autonomic features.\nHowever, to the best of our knowledge, it has never been attempted to base this\nestimation on the patient narratives of the personal experience of chronic\npain, which is what we propose in this work. Indeed, in the clinical assessment\nand management of chronic pain, verbal communication is essential to convey\ninformation to physicians that would otherwise not be easily accessible through\nstandard reporting tools, since language, sociocultural, and psychosocial\nvariables are intertwined. We show that language features from patient\nnarratives indeed convey information relevant for pain intensity estimation,\nand that our computational models can take advantage of that. Specifically, our\nresults show that patients with mild pain focus more on the use of verbs,\nwhilst moderate and severe pain patients focus on adverbs, and nouns and\nadjectives, respectively, and that these differences allow for the distinction\nbetween these three pain classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nunes_D/0/1/0/all/0/1\">Diogo A.P. Nunes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_Gomes_J/0/1/0/all/0/1\">Joana Ferreira-Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1\">Daniela Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaz_C/0/1/0/all/0/1\">Carlos Vaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimenta_S/0/1/0/all/0/1\">Sofia Pimenta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neto_F/0/1/0/all/0/1\">Fani Neto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matos_D/0/1/0/all/0/1\">David Martins de Matos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Knowledge-Enhanced Pre-trained Language Models. (arXiv:2211.05994v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05994","description":"<p>Pre-trained Language Models (PLMs) which are trained on large text corpus via\nself-supervised learning method, have yielded promising performance on various\ntasks in Natural Language Processing (NLP). However, though PLMs with huge\nparameters can effectively possess rich knowledge learned from massive training\ntext and benefit downstream tasks at the fine-tuning stage, they still have\nsome limitations such as poor reasoning ability due to the lack of external\nknowledge. Research has been dedicated to incorporating knowledge into PLMs to\ntackle these issues. In this paper, we present a comprehensive review of\nKnowledge-Enhanced Pre-trained Language Models (KE-PLMs) to provide a clear\ninsight into this thriving field. We introduce appropriate taxonomies\nrespectively for Natural Language Understanding (NLU) and Natural Language\nGeneration (NLG) to highlight these two main tasks of NLP. For NLU, we divide\nthe types of knowledge into four categories: linguistic knowledge, text\nknowledge, knowledge graph (KG), and rule knowledge. The KE-PLMs for NLG are\ncategorized into KG-based and retrieval-based methods. Finally, we point out\nsome promising future directions of KE-PLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Linmei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zeyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziwang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrated Interpretation: Confidence Estimation in Semantic Parsing. (arXiv:2211.07443v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07443","description":"<p>Task-oriented semantic parsing is increasingly being used in user-facing\napplications, making measuring the calibration of parsing models especially\nimportant. We examine the calibration characteristics of six models across\nthree model families on two common English semantic parsing datasets, finding\nthat many models are reasonably well-calibrated and that there is a trade-off\nbetween calibration and performance. Based on confidence scores across three\nmodels, we propose and release new challenge splits of the two datasets we\nexamine. We then illustrate the ways a calibrated model can be useful in\nbalancing common trade-offs in task-oriented parsing. In a simulated\nannotator-in-the-loop experiment, we show that using model confidence allows us\nto improve performance by 9.6% (absolute) with interactions on only 2.2% of\ntokens. Using sequence-level confidence scores, we then examine how we can\noptimize trade-off between a parser's usability and safety. We show that\nconfidence-based thresholding can reduce the number of incorrect low-confidence\nprograms executed by 76%; however, this comes at a cost to usability. We\npropose the DidYouMean system which balances usability and safety. We conclude\nby calling for calibration to be included in the evaluation of semantic parsing\nsystems, and release a library for computing calibration metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stengel_Eskin_E/0/1/0/all/0/1\">Elias Stengel-Eskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Persian Emotion Detection using ParsBERT and Imbalanced Data Handling Approaches. (arXiv:2211.08029v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08029","description":"<p>Emotion recognition is one of the machine learning applications which can be\ndone using text, speech, or image data gathered from social media spaces.\nDetecting emotion can help us in different fields, including opinion mining.\nWith the spread of social media, different platforms like Twitter have become\ndata sources, and the language used in these platforms is informal, making the\nemotion detection task difficult. EmoPars and ArmanEmo are two new\nhuman-labeled emotion datasets for the Persian language. These datasets,\nespecially EmoPars, are suffering from inequality between several samples\nbetween two classes. In this paper, we evaluate EmoPars and compare them with\nArmanEmo. Throughout this analysis, we use data augmentation techniques, data\nre-sampling, and class-weights with Transformer-based Pretrained Language\nModels(PLMs) to handle the imbalance problem of these datasets. Moreover,\nfeature selection is used to enhance the models' performance by emphasizing the\ntext's specific features. In addition, we provide a new policy for selecting\ndata from EmoPars, which selects the high-confidence samples; as a result, the\nmodel does not see samples that do not have specific emotion during training.\nOur model reaches a Macro-averaged F1-score of 0.81 and 0.76 on ArmanEmo and\nEmoPars, respectively, which are new state-of-the-art results in these\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abaskohi_A/0/1/0/all/0/1\">Amirhossein Abaskohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabri_N/0/1/0/all/0/1\">Nazanin Sabri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahrak_B/0/1/0/all/0/1\">Behnam Bahrak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An FNet based Auto Encoder for Long Sequence News Story Generation. (arXiv:2211.08295v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08295","description":"<p>In this paper, we design an auto encoder based off of Google's FNet\nArchitecture in order to generate text from a subset of news stories contained\nin Google's C4 dataset. We discuss previous attempts and methods to generate\ntext from autoencoders and non LLM Models. FNET poses multiple advantages to\nBERT based encoders in the realm of efficiency which train 80% faster on GPUs\nand 70% faster on TPUs. We then compare outputs of how this autencoder perfroms\non different epochs. Finally, we analyze what outputs the encoder produces with\ndifferent seed text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mandal_P/0/1/0/all/0/1\">Paul K. Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahto_R/0/1/0/all/0/1\">Rakeshkumar Mahto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SexWEs: Domain-Aware Word Embeddings via Cross-lingual Semantic Specialisation for Chinese Sexism Detection in Social Media. (arXiv:2211.08447v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08447","description":"<p>The goal of sexism detection is to mitigate negative online content targeting\ncertain gender groups of people. However, the limited availability of labeled\nsexism-related datasets makes it problematic to identify online sexism for\nlow-resource languages. In this paper, we address the task of automatic sexism\ndetection in social media for one low-resource language -- Chinese. Rather than\ncollecting new sexism data or building cross-lingual transfer learning models,\nwe develop a cross-lingual domain-aware semantic specialisation system in order\nto make the most of existing data. Semantic specialisation is a technique for\nretrofitting pre-trained distributional word vectors by integrating external\nlinguistic knowledge (such as lexico-semantic relations) into the specialised\nfeature space. To do this, we leverage semantic resources for sexism from a\nhigh-resource language (English) to specialise pre-trained word vectors in the\ntarget language (Chinese) to inject domain knowledge. We demonstrate the\nbenefit of our sexist word embeddings (SexWEs) specialised by our framework via\nintrinsic evaluation of word similarity and extrinsic evaluation of sexism\ndetection. Compared with other specialisation approaches and Chinese baseline\nword vectors, our SexWEs shows an average score improvement of 0.033 and 0.064\nin both intrinsic and extrinsic evaluations, respectively. The ablative results\nand visualisation of SexWEs also prove the effectiveness of our framework on\nretrofitting word vectors in low-resource languages. Our code and\nsexism-related word vectors will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1\">Aiqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching for Carriers of the Diffuse Interstellar Bands Across Disciplines, using Natural Language Processing. (arXiv:2211.08513v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08513","description":"<p>The explosion of scientific publications overloads researchers with\ninformation. This is even more dramatic for interdisciplinary studies, where\nseveral fields need to be explored. A tool to help researchers overcome this is\nNatural Language Processing (NLP): a machine-learning (ML) technique that\nallows scientists to automatically synthesize information from many articles.\nAs a practical example, we have used NLP to conduct an interdisciplinary search\nfor compounds that could be carriers for Diffuse Interstellar Bands (DIBs), a\nlong-standing open question in astrophysics. We have trained a NLP model on a\ncorpus of 1.5 million cross-domain articles in open access, and fine-tuned this\nmodel with a corpus of astrophysical publications about DIBs. Our analysis\npoints us toward several molecules, studied primarily in biology, having\ntransitions at the wavelengths of several DIBs and composed of abundant\ninterstellar atoms. Several of these molecules contain chromophores, small\nmolecular groups responsible for the molecule's colour, that could be promising\ncandidate carriers. Identifying viable carriers demonstrates the value of using\nNLP to tackle open scientific questions, in an interdisciplinary manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+dObrenan_C/0/1/0/all/0/1\">Corentin van den Broek d&#x27;Obrenan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galliano_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Galliano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minton_J/0/1/0/all/0/1\">Jeremy Minton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botev_V/0/1/0/all/0/1\">Viktor Botev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ronin Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lesion Guided Explainable Few Weak-shot Medical Report Generation. (arXiv:2211.08732v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.08732","description":"<p>Medical images are widely used in clinical practice for diagnosis.\nAutomatically generating interpretable medical reports can reduce radiologists'\nburden and facilitate timely care. However, most existing approaches to\nautomatic report generation require sufficient labeled data for training. In\naddition, the learned model can only generate reports for the training classes,\nlacking the ability to adapt to previously unseen novel diseases. To this end,\nwe propose a lesion guided explainable few weak-shot medical report generation\nframework that learns correlation between seen and novel classes through visual\nand semantic feature alignment, aiming to generate medical reports for diseases\nnot observed in training. It integrates a lesion-centric feature extractor and\na Transformer-based report generation module. Concretely, the lesion-centric\nfeature extractor detects the abnormal regions and learns correlations between\nseen and novel classes with multi-view (visual and lexical) embeddings. Then,\nfeatures of the detected regions and corresponding embeddings are concatenated\nas multi-view input to the report generation module for explainable report\ngeneration, including text descriptions and corresponding abnormal regions\ndetected in the images. We conduct experiments on FFA-IR, a dataset providing\nexplainable annotations, showing that our framework outperforms others on\nreport generation for novel diseases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jinghan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding. (arXiv:2203.08481v2 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2203.08481","description":"<p>Visual grounding, i.e., localizing objects in images according to natural\nlanguage queries, is an important topic in visual language understanding. The\nmost effective approaches for this task are based on deep learning, which\ngenerally require expensive manually labeled image-query or patch-query pairs.\nTo eliminate the heavy dependence on human annotations, we present a novel\nmethod, named Pseudo-Q, to automatically generate pseudo language queries for\nsupervised training. Our method leverages an off-the-shelf object detector to\nidentify visual objects from unlabeled images, and then language queries for\nthese objects are obtained in an unsupervised fashion with a pseudo-query\ngeneration module. Then, we design a task-related query prompt module to\nspecifically tailor generated pseudo language queries for visual grounding\ntasks. Further, in order to fully capture the contextual relationships between\nimages and language queries, we develop a visual-language model equipped with\nmulti-level cross-modality attention mechanism. Extensive experimental results\ndemonstrate that our method has two notable benefits: (1) it can reduce human\nannotation costs significantly, e.g., 31% on RefCOCO without degrading original\nmodel's performance under the fully supervised setting, and (2) without bells\nand whistles, it achieves superior or comparable performance compared to\nstate-of-the-art weakly-supervised visual grounding methods on all the five\ndatasets we have experimented. Code is available at\nhttps://github.com/LeapLabTHU/Pseudo-Q.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haojun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuanze Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dongchen Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-11-17T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}