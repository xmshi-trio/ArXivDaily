{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-06-21T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A Pairing Enhancement Approach for Aspect Sentiment Triplet Extraction. (arXiv:2306.10042v1 [cs.IR])","link":"http://arxiv.org/abs/2306.10042","description":"<p>Aspect Sentiment Triplet Extraction (ASTE) aims to extract the triplet of an\naspect term, an opinion term, and their corresponding sentiment polarity from\nthe review texts. Due to the complexity of language and the existence of\nmultiple aspect terms and opinion terms in a single sentence, current models\noften confuse the connections between an aspect term and the opinion term\ndescribing it. To address this issue, we propose a pairing enhancement approach\nfor ASTE, which incorporates contrastive learning during the training stage to\ninject aspect-opinion pairing knowledge into the triplet extraction model.\nExperimental results demonstrate that our approach performs well on four ASTE\ndatasets (i.e., 14lap, 14res, 15res and 16res) compared to several related\nclassical and state-of-the-art triplet extraction methods. Moreover, ablation\nstudies conduct an analysis and verify the advantage of contrastive learning\nover other pairing enhancement approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1\">Gongzhen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiabing Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Practical Entity Linking System for Tables in Scientific Literature. (arXiv:2306.10044v1 [cs.IR])","link":"http://arxiv.org/abs/2306.10044","description":"<p>Entity linking is an important step towards constructing knowledge graphs\nthat facilitate advanced question answering over scientific documents,\nincluding the retrieval of relevant information included in tables within these\ndocuments. This paper introduces a general-purpose system for linking entities\nto items in the Wikidata knowledge base. It describes how we adapt this system\nfor linking domain-specific entities, especially for those entities embedded\nwithin tables drawn from COVID-19-related scientific literature. We describe\nthe setup of an efficient offline instance of the system that enables our\nentity-linking approach to be more feasible in practice. As part of a broader\napproach to infer the semantic meaning of scientific tables, we leverage the\nstructural and semantic characteristics of the tables to improve overall entity\nlinking performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mulwad_V/0/1/0/all/0/1\">Varish Mulwad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finin_T/0/1/0/all/0/1\">Tim Finin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vijay S. Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1\">Jenny Weisenberg Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixit_S/0/1/0/all/0/1\">Sharad Dixit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Anupam Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generate to Understand for Representation. (arXiv:2306.10056v1 [cs.CL])","link":"http://arxiv.org/abs/2306.10056","description":"<p>In recent years, a significant number of high-quality pretrained models have\nemerged, greatly impacting Natural Language Understanding (NLU), Natural\nLanguage Generation (NLG), and Text Representation tasks. Traditionally, these\nmodels are pretrained on custom domain corpora and finetuned for specific\ntasks, resulting in high costs related to GPU usage and labor. Unfortunately,\nrecent trends in language modeling have shifted towards enhancing performance\nthrough scaling, further exacerbating the associated costs.\n</p>\n<p>Introducing GUR: a pretraining framework that combines language modeling and\ncontrastive learning objectives in a single training step. We select similar\ntext pairs based on their Longest Common Substring (LCS) from raw unlabeled\ndocuments and train the model using masked language modeling and unsupervised\ncontrastive learning. The resulting model, GUR, achieves impressive results\nwithout any labeled training data, outperforming all other pretrained baselines\nas a retriever at the recall benchmark in a zero-shot setting. Additionally,\nGUR maintains its language modeling ability, as demonstrated in our ablation\nexperiment. Our code is available at \\url{https://github.com/laohur/GUR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1\">Changshang Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1\">Xiande Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoqing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EM-Network: Oracle Guided Self-distillation for Sequence Learning. (arXiv:2306.10058v1 [cs.LG])","link":"http://arxiv.org/abs/2306.10058","description":"<p>We introduce EM-Network, a novel self-distillation approach that effectively\nleverages target information for supervised sequence-to-sequence (seq2seq)\nlearning. In contrast to conventional methods, it is trained with oracle\nguidance, which is derived from the target sequence. Since the oracle guidance\ncompactly represents the target-side context that can assist the sequence model\nin solving the task, the EM-Network achieves a better prediction compared to\nusing only the source input. To allow the sequence model to inherit the\npromising capability of the EM-Network, we propose a new self-distillation\nstrategy, where the original sequence model can benefit from the knowledge of\nthe EM-Network in a one-stage manner. We conduct comprehensive experiments on\ntwo types of seq2seq models: connectionist temporal classification (CTC) for\nspeech recognition and attention-based encoder-decoder (AED) for machine\ntranslation. Experimental results demonstrate that the EM-Network significantly\nadvances the current state-of-the-art approaches, improving over the best prior\nwork on speech recognition and establishing state-of-the-art performance on\nWMT'14 and IWSLT'14.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Ji Won Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sunghwan Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyeonseung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minchan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seok Min Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Nam Soo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revealing the structure of language model capabilities. (arXiv:2306.10062v1 [cs.CL])","link":"http://arxiv.org/abs/2306.10062","description":"<p>Building a theoretical understanding of the capabilities of large language\nmodels (LLMs) is vital for our ability to predict and explain the behavior of\nthese systems. Here, we investigate the structure of LLM capabilities by\nextracting latent capabilities from patterns of individual differences across a\nvaried population of LLMs. Using a combination of Bayesian and frequentist\nfactor analysis, we analyzed data from 29 different LLMs across 27 cognitive\ntasks. We found evidence that LLM capabilities are not monolithic. Instead,\nthey are better explained by three well-delineated factors that represent\nreasoning, comprehension and core language modeling. Moreover, we found that\nthese three factors can explain a high proportion of the variance in model\nperformance. These results reveal a consistent structure in the capabilities of\ndifferent LLMs and demonstrate the multifaceted nature of these capabilities.\nWe also found that the three abilities show different relationships to model\nproperties such as model size and instruction tuning. These patterns help\nrefine our understanding of scaling laws and indicate that changes to a model\nthat improve one ability might simultaneously impair others. Based on these\nfindings, we suggest that benchmarks could be streamlined by focusing on tasks\nthat tap into each broad model ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burnell_R/0/1/0/all/0/1\">Ryan Burnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_H/0/1/0/all/0/1\">Han Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conway_A/0/1/0/all/0/1\">Andrew R. A. Conway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orallo_J/0/1/0/all/0/1\">Jose Hernandez Orallo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-specific ChatBots for Science using Embeddings. (arXiv:2306.10067v1 [cs.CL])","link":"http://arxiv.org/abs/2306.10067","description":"<p>Large language models (LLMs) have emerged as powerful machine-learning\nsystems capable of handling a myriad of tasks. Tuned versions of these systems\nhave been turned into chatbots that can respond to user queries on a vast\ndiversity of topics, providing informative and creative replies. However, their\napplication to physical science research remains limited owing to their\nincomplete knowledge in these areas, contrasted with the needs of rigor and\nsourcing in science domains. Here, we demonstrate how existing methods and\nsoftware tools can be easily combined to yield a domain-specific chatbot. The\nsystem ingests scientific documents in existing formats, and uses text\nembedding lookup to provide the LLM with domain-specific contextual information\nwhen composing its reply. We similarly demonstrate that existing image\nembedding methods can be used for search and retrieval across publication\nfigures. These results confirm that LLMs are already suitable for use by\nphysical scientists in accelerating their research efforts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yager_K/0/1/0/all/0/1\">Kevin G. Yager</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health. (arXiv:2306.10070v1 [cs.CY])","link":"http://arxiv.org/abs/2306.10070","description":"<p>ChatGPT has drawn considerable attention from both the general public and\ndomain experts with its remarkable text generation capabilities. This has\nsubsequently led to the emergence of diverse applications in the field of\nbiomedicine and health. In this work, we examine the diverse applications of\nlarge language models (LLMs), such as ChatGPT, in biomedicine and health.\nSpecifically we explore the areas of biomedical information retrieval, question\nanswering, medical text summarization, information extraction, and medical\neducation, and investigate whether LLMs possess the transformative power to\nrevolutionize these tasks or whether the distinct complexities of biomedical\ndomain presents unique challenges. Following an extensive literature survey, we\nfind that significant advances have been made in the field of text generation\ntasks, surpassing the previous state-of-the-art methods. For other\napplications, the advances have been modest. Overall, LLMs have not yet\nrevolutionized the biomedicine, but recent rapid progress indicates that such\nmethods hold great potential to provide valuable means for accelerating\ndiscovery and improving health. We also find that the use of LLMs, like\nChatGPT, in the fields of biomedicine and health entails various risks and\nchallenges, including fabricated information in its generated responses, as\nwell as legal and privacy concerns associated with sensitive patient data. We\nbelieve this first-of-its-kind survey can provide a comprehensive overview to\nbiomedical researchers and healthcare practitioners on the opportunities and\nchallenges associated with using ChatGPT and other LLMs for transforming\nbiomedicine and health.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shubo Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qiao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeganova_L/0/1/0/all/0/1\">Lana Yeganova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_P/0/1/0/all/0/1\">Po-Ting Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qingqing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yifan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1\">Won Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comeau_D/0/1/0/all/0/1\">Donald C. Comeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islamaj_R/0/1/0/all/0/1\">Rezarta Islamaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1\">Aadit Kapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Error correction and extraction in request dialogs. (arXiv:2004.04243v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.04243","description":"<p>We propose a dialog system utility component that gets the last two\nutterances of a user and can detect whether the last utterance is an error\ncorrection of the second last utterance. If yes, it corrects the second last\nutterance according to the error correction in the last utterance and outputs\nthe extracted pairs of reparandum and repair entity. This component offers two\nadvantages, learning the concept of corrections to avoid collecting corrections\nfor every new domain and extracting reparandum and repair pairs, which offers\nthe possibility to learn out of it.\n</p>\n<p>For the error correction one sequence labeling and two sequence to sequence\napproaches are presented. For the error correction detection these three error\ncorrection approaches can also be used and in addition, we present a sequence\nclassification approach. One error correction detection and one error\ncorrection approach can be combined to a pipeline or the error correction\napproaches can be trained and used end-to-end to avoid two components. We\nmodified the EPIC-KITCHENS-100 dataset to evaluate the approaches for\ncorrecting entity phrases in request dialogs. For error correction detection\nand correction, we got an accuracy of 96.40 % on synthetic validation data and\nan accuracy of 77.81 % on human-created real-world test data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Constantin_S/0/1/0/all/0/1\">Stefan Constantin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alex Waibel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Discovery in Visually Grounded, Self-Supervised Speech Models. (arXiv:2203.15081v5 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2203.15081","description":"<p>We present a method for visually-grounded spoken term discovery. After\ntraining either a HuBERT or wav2vec2.0 model to associate spoken captions with\nnatural images, we show that powerful word segmentation and clustering\ncapability emerges within the model's self-attention heads. Our experiments\nreveal that this ability is not present to nearly the same extent in the base\nHuBERT and wav2vec2.0 models, suggesting that the visual grounding task is a\ncrucial component of the word discovery capability we observe. We also evaluate\nour method on the Buckeye word segmentation and ZeroSpeech spoken term\ndiscovery tasks, where we perform on par with or better than currently\npublished methods on several metrics. Code and model weights are available at\nhttps://github.com/jasonppy/word-discovery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_P/0/1/0/all/0/1\">Puyuan Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual AMR Aligner: Paying Attention to Cross-Attention. (arXiv:2206.07587v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.07587","description":"<p>This paper introduces a novel aligner for Abstract Meaning Representation\n(AMR) graphs that can scale cross-lingually, and is thus capable of aligning\nunits and spans in sentences of different languages. Our approach leverages\nmodern Transformer-based parsers, which inherently encode alignment information\nin their cross-attention weights, allowing us to extract this information\nduring parsing. This eliminates the need for English-specific rules or the\nExpectation Maximization (EM) algorithm that have been used in previous\napproaches. In addition, we propose a guided supervised method using alignment\nto further enhance the performance of our aligner. We achieve state-of-the-art\nresults in the benchmarks for AMR alignment and demonstrate our aligner's\nability to obtain them across multiple languages. Our code will be available at\n\\href{https://www.github.com/Babelscape/AMR-alignment}{github.com/Babelscape/AMR-alignment}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lorenzo_A/0/1/0/all/0/1\">Abelardo Carlos Mart&#xed;nez Lorenzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabot_P/0/1/0/all/0/1\">Pere-Llu&#xed;s Huguet Cabot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navigli_R/0/1/0/all/0/1\">Roberto Navigli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-aspect Multilingual and Cross-lingual Parliamentary Speech Analysis. (arXiv:2207.01054v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.01054","description":"<p>Parliamentary and legislative debate transcripts provide informative insight\ninto elected politicians' opinions, positions, and policy preferences. They are\ninteresting for political and social sciences as well as linguistics and\nnatural language processing (NLP) research. While existing research studied\nindividual parliaments, we apply advanced NLP methods to a joint and\ncomparative analysis of six national parliaments (Bulgarian, Czech, French,\nSlovene, Spanish, and United Kingdom) between 2017 and 2020. We analyze\nemotions and sentiment in the transcripts from the ParlaMint dataset collection\nand assess if the age, gender, and political orientation of speakers can be\ndetected from their speeches. The results show some commonalities and many\nsurprising differences among the analyzed countries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miok_K/0/1/0/all/0/1\">Kristian Miok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hidalgo_Tenorio_E/0/1/0/all/0/1\">Encarnacion Hidalgo-Tenorio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osenova_P/0/1/0/all/0/1\">Petya Osenova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benitez_Castro_M/0/1/0/all/0/1\">Miguel-Angel Benitez-Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1\">Marko Robnik-Sikonja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Classifiers are Unreliable for Concept Removal and Detection. (arXiv:2207.04153v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2207.04153","description":"<p>Neural network models trained on text data have been found to encode\nundesirable linguistic or sensitive concepts in their representation. Removing\nsuch concepts is non-trivial because of a complex relationship between the\nconcept, text input, and the learnt representation. Recent work has proposed\npost-hoc and adversarial methods to remove such unwanted concepts from a\nmodel's representation. Through an extensive theoretical and empirical\nanalysis, we show that these methods can be counter-productive: they are unable\nto remove the concepts entirely, and in the worst case may end up destroying\nall task-relevant features. The reason is the methods' reliance on a probing\nclassifier as a proxy for the concept. Even under the most favorable conditions\nfor learning a probing classifier when a concept's relevant features in\nrepresentation space alone can provide 100% accuracy, we prove that a probing\nclassifier is likely to use non-concept features and thus post-hoc or\nadversarial methods will fail to remove the concept correctly. These\ntheoretical implications are confirmed by experiments on models trained on\nsynthetic, Multi-NLI, and Twitter datasets. For sensitive applications of\nconcept removal such as fairness, we recommend caution against using these\nmethods and propose a spuriousness metric to gauge the quality of the final\nclassifier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhinav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Amit Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Cognitive Study on Semantic Similarity Analysis of Large Corpora: A Transformer-based Approach. (arXiv:2207.11716v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.11716","description":"<p>Semantic similarity analysis and modeling is a fundamentally acclaimed task\nin many pioneering applications of natural language processing today. Owing to\nthe sensation of sequential pattern recognition, many neural networks like RNNs\nand LSTMs have achieved satisfactory results in semantic similarity modeling.\nHowever, these solutions are considered inefficient due to their inability to\nprocess information in a non-sequential manner, thus leading to the improper\nextraction of context. Transformers function as the state-of-the-art\narchitecture due to their advantages like non-sequential data processing and\nself-attention. In this paper, we perform semantic similarity analysis and\nmodeling on the U.S Patent Phrase to Phrase Matching Dataset using both\ntraditional and transformer-based techniques. We experiment upon four different\nvariants of the Decoding Enhanced BERT - DeBERTa and enhance its performance by\nperforming K-Fold Cross-Validation. The experimental results demonstrate our\nmethodology's enhanced performance compared to traditional techniques, with an\naverage Pearson correlation score of 0.79.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nemani_P/0/1/0/all/0/1\">Praneeth Nemani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vollala_S/0/1/0/all/0/1\">Satyanarayana Vollala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting as Probing: Using Language Models for Knowledge Base Construction. (arXiv:2208.11057v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.11057","description":"<p>Language Models (LMs) have proven to be useful in various downstream\napplications, such as summarisation, translation, question answering and text\nclassification. LMs are becoming increasingly important tools in Artificial\nIntelligence, because of the vast quantity of information they can store. In\nthis work, we present ProP (Prompting as Probing), which utilizes GPT-3, a\nlarge Language Model originally proposed by OpenAI in 2020, to perform the task\nof Knowledge Base Construction (KBC). ProP implements a multi-step approach\nthat combines a variety of prompting techniques to achieve this. Our results\nshow that manual prompt curation is essential, that the LM must be encouraged\nto give answer sets of variable lengths, in particular including empty answer\nsets, that true/false questions are a useful device to increase precision on\nsuggestions generated by the LM, that the size of the LM is a crucial factor,\nand that a dictionary of entity aliases improves the LM score. Our evaluation\nstudy indicates that these proposed techniques can substantially enhance the\nquality of the final predictions: ProP won track 2 of the LM-KBC competition,\noutperforming the baseline by 36.4 percentage points. Our implementation is\navailable on https://github.com/HEmile/iswc-challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alivanistos_D/0/1/0/all/0/1\">Dimitrios Alivanistos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santamaria_S/0/1/0/all/0/1\">Selene B&#xe1;ez Santamar&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cochez_M/0/1/0/all/0/1\">Michael Cochez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalo_J/0/1/0/all/0/1\">Jan-Christoph Kalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krieken_E/0/1/0/all/0/1\">Emile van Krieken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thanapalasingam_T/0/1/0/all/0/1\">Thiviyan Thanapalasingam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity. (arXiv:2209.12106v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.12106","description":"<p>Large Language Models (LLMs) have demonstrated impressive capabilities in\ngenerating fluent text, as well as tendencies to reproduce undesirable social\nbiases. This study investigates whether LLMs reproduce the moral biases\nassociated with political groups in the United States, an instance of a broader\ncapability herein termed moral mimicry. This hypothesis is explored in the\nGPT-3/3.5 and OPT families of Transformer-based LLMs. Using tools from Moral\nFoundations Theory, it is shown that these LLMs are indeed moral mimics. When\nprompted with a liberal or conservative political identity, the models generate\ntext reflecting corresponding moral biases. This study also explores the\nrelationship between moral mimicry and model size, and similarity between human\nand LLM moral word use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Simmons_G/0/1/0/all/0/1\">Gabriel Simmons</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting. (arXiv:2210.08964v4 [stat.ME] UPDATED)","link":"http://arxiv.org/abs/2210.08964","description":"<p>This paper presents a new perspective on time series forecasting. In existing\ntime series forecasting methods, the models take a sequence of numerical values\nas input and yield numerical values as output. The existing SOTA models are\nlargely based on the Transformer architecture, modified with multiple encoding\nmechanisms to incorporate the context and semantics around the historical data.\nInspired by the successes of pre-trained language foundation models, we pose a\nquestion about whether these models can also be adapted to solve time-series\nforecasting. Thus, we propose a new forecasting paradigm: prompt-based time\nseries forecasting (PromptCast). In this novel task, the numerical input and\noutput are transformed into prompts and the forecasting task is framed in a\nsentence-to-sentence manner, making it possible to directly apply language\nmodels for forecasting purposes. To support and facilitate the research of this\ntask, we also present a large-scale dataset (PISA) that includes three\nreal-world forecasting scenarios. We evaluate different SOTA numerical-based\nforecasting methods and language generation models. The benchmark results with\nvarious forecasting settings demonstrate the proposed PromptCast with language\ngeneration models is a promising research direction. Additionally, in\ncomparison to conventional numerical-based forecasting, PromptCast shows a much\nbetter generalization ability under the zero-shot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Xue_H/0/1/0/all/0/1\">Hao Xue</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Salim_F/0/1/0/all/0/1\">Flora D. Salim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Textual Entailment Recognition with Semantic Features from Empirical Text Representation. (arXiv:2210.09723v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.09723","description":"<p>Textual entailment recognition is one of the basic natural language\nunderstanding(NLU) tasks. Understanding the meaning of sentences is a\nprerequisite before applying any natural language processing(NLP) techniques to\nautomatically recognize the textual entailment. A text entails a hypothesis if\nand only if the true value of the hypothesis follows the text. Classical\napproaches generally utilize the feature value of each word from word embedding\nto represent the sentences. In this paper, we propose a novel approach to\nidentifying the textual entailment relationship between text and hypothesis,\nthereby introducing a new semantic feature focusing on empirical\nthreshold-based semantic text representation. We employ an element-wise\nManhattan distance vector-based feature that can identify the semantic\nentailment relationship between the text-hypothesis pair. We carried out\nseveral experiments on a benchmark entailment classification(SICK-RTE) dataset.\nWe train several machine learning(ML) algorithms applying both semantic and\nlexical features to classify the text-hypothesis pair as entailment, neutral,\nor contradiction. Our empirical sentence representation technique enriches the\nsemantic information of the texts and hypotheses found to be more efficient\nthan the classical ones. In the end, our approach significantly outperforms\nknown methods in understanding the meaning of the sentences for the textual\nentailment classification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shajalal_M/0/1/0/all/0/1\">Md Shajalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atabuzzaman_M/0/1/0/all/0/1\">Md Atabuzzaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baby_M/0/1/0/all/0/1\">Maksuda Bilkis Baby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Md Rezaul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boden_A/0/1/0/all/0/1\">Alexander Boden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks. (arXiv:2210.10040v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10040","description":"<p>How reliably can we trust the scores obtained from social bias benchmarks as\nfaithful indicators of problematic social biases in a given language model? In\nthis work, we study this question by contrasting social biases with non-social\nbiases stemming from choices made during dataset construction that might not\neven be discernible to the human eye. To do so, we empirically simulate various\nalternative constructions for a given benchmark based on innocuous\nmodifications (such as paraphrasing or random-sampling) that maintain the\nessence of their social bias. On two well-known social bias benchmarks\n(Winogender and BiasNLI) we observe that these shallow modifications have a\nsurprising effect on the resulting degree of bias across various models. We\nhope these troubling observations motivate more robust measures of social\nbiases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Selvam_N/0/1/0/all/0/1\">Nikil Roashan Selvam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1\">Tushar Khot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoNET: Tackle State Momentum via Noise-Enhanced Training for Dialogue State Tracking. (arXiv:2211.05503v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05503","description":"<p>Dialogue state tracking (DST) aims to convert the dialogue history into\ndialogue states which consist of slot-value pairs. As condensed structural\ninformation memorizing all history information, the dialogue state in the last\nturn is typically adopted as the input for predicting the current state by DST\nmodels. However, these models tend to keep the predicted slot values unchanged,\nwhich is defined as state momentum in this paper. Specifically, the models\nstruggle to update slot values that need to be changed and correct wrongly\npredicted slot values in the last turn. To this end, we propose MoNET to tackle\nstate momentum via noise-enhanced training. First, the previous state of each\nturn in the training data is noised via replacing some of its slot values.\nThen, the noised previous state is used as the input to learn to predict the\ncurrent state, improving the model's ability to update and correct slot values.\nFurthermore, a contrastive context matching framework is designed to narrow the\nrepresentation distance between a state and its corresponding noised variant,\nwhich reduces the impact of noised state and makes the model better understand\nthe dialogue history. Experimental results on MultiWOZ datasets show that MoNET\noutperforms previous DST methods. Ablations and analysis verify the\neffectiveness of MoNET in alleviating state momentum and improving anti-noise\nability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haipeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenye Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are Better Dense Retrievers. (arXiv:2212.07841v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.07841","description":"<p>Pre-trained Transformers (\\eg BERT) have been commonly used in existing dense\nretrieval methods for parameter initialization, and recent studies are\nexploring more effective pre-training tasks for further improving the quality\nof dense vectors. Although various novel and effective tasks have been\nproposed, their different input formats and learning objectives make them hard\nto be integrated for jointly improving the model performance. In this work, we\naim to unify a variety of pre-training tasks into the bottlenecked masked\nautoencoder manner, and integrate them into a multi-task pre-trained model,\nnamely MASTER. Concretely, MASTER utilizes a shared-encoder multi-decoder\narchitecture that can construct a representation bottleneck to compress the\nabundant semantic information across tasks into dense vectors. Based on it, we\nintegrate three types of representative pre-training tasks: corrupted passages\nrecovering, related passages recovering and PLMs outputs recovering, to\ncharacterize the inner-passage information, inter-passage relations and PLMs\nknowledge. Extensive experiments have shown that our approach outperforms\ncompetitive dense retrieval methods. Our code and data are publicly released in\n\\url{https://github.com/microsoft/SimXNS}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extrinsic Evaluation of Machine Translation Metrics. (arXiv:2212.10297v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10297","description":"<p>Automatic machine translation (MT) metrics are widely used to distinguish the\ntranslation qualities of machine translation systems across relatively large\ntest sets (system-level evaluation). However, it is unclear if automatic\nmetrics are reliable at distinguishing good translations from bad translations\nat the sentence level (segment-level evaluation). In this paper, we investigate\nhow useful MT metrics are at detecting the success of a machine translation\ncomponent when placed in a larger platform with a downstream task. We evaluate\nthe segment-level performance of the most widely used MT metrics (chrF, COMET,\nBERTScore, etc.) on three downstream cross-lingual tasks (dialogue state\ntracking, question answering, and semantic parsing). For each task, we only\nhave access to a monolingual task-specific model. We calculate the correlation\nbetween the metric's ability to predict a good/bad translation with the\nsuccess/failure on the final task for the Translate-Test setup. Our experiments\ndemonstrate that all metrics exhibit negligible correlation with the extrinsic\nevaluation of the downstream outcomes. We also find that the scores provided by\nneural metrics are not interpretable mostly because of undefined ranges. We\nsynthesise our analysis into recommendations for future MT metrics to produce\nlabels rather than scores for more informative interaction between machine\ntranslation and multilingual language understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moghe_N/0/1/0/all/0/1\">Nikita Moghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sherborne_T/0/1/0/all/0/1\">Tom Sherborne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1\">Mark Steedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1\">Alexandra Birch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MULTI3NLU++: A Multilingual, Multi-Intent, Multi-Domain Dataset for Natural Language Understanding in Task-Oriented Dialogue. (arXiv:2212.10455v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10455","description":"<p>Task-oriented dialogue (TOD) systems have been widely deployed in many\nindustries as they deliver more efficient customer support. These systems are\ntypically constructed for a single domain or language and do not generalise\nwell beyond this. To support work on Natural Language Understanding (NLU) in\nTOD across multiple languages and domains simultaneously, we constructed\nMULTI3NLU++, a multilingual, multi-intent, multi-domain dataset. MULTI3NLU++\nextends the English only NLU++ dataset to include manual translations into a\nrange of high, medium, and low resource languages (Spanish, Marathi, Turkish\nand Amharic), in two domains (BANKING and HOTELS). Because of its multi-intent\nproperty, MULTI3NLU++ represents complex and natural user goals, and therefore\nallows us to measure the realistic performance of TOD systems in a varied set\nof the world's languages. We use MULTI3NLU++ to benchmark state-of-the-art\nmultilingual models for the NLU tasks of intent detection and slot labelling\nfor TOD systems in the multilingual setting. The results demonstrate the\nchallenging nature of the dataset, particularly in the low-resource language\nsetting, offering ample room for future experimentation in multi-domain\nmultilingual TOD setups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moghe_N/0/1/0/all/0/1\">Nikita Moghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razumovskaia_E/0/1/0/all/0/1\">Evgeniia Razumovskaia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guillou_L/0/1/0/all/0/1\">Liane Guillou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1\">Alexandra Birch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata. (arXiv:2301.04647v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2301.04647","description":"<p>We learn a visual representation that captures information about the camera\nthat recorded a given photo. To do this, we train a multimodal embedding\nbetween image patches and the EXIF metadata that cameras automatically insert\ninto image files. Our model represents this metadata by simply converting it to\ntext and then processing it with a transformer. The features that we learn\nsignificantly outperform other self-supervised and supervised features on\ndownstream image forensics and calibration tasks. In particular, we\nsuccessfully localize spliced image regions \"zero shot\" by clustering the\nvisual embeddings for all of the patches within an image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chenhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Ayush Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owens_A/0/1/0/all/0/1\">Andrew Owens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Exemplars for In-context Learning. (arXiv:2302.05698v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.05698","description":"<p>Large pretrained language models (LMs) have shown impressive In-Context\nLearning (ICL) ability, where the model learns to do an unseen task via a\nprompt consisting of input-output examples as the demonstration, without any\nparameter updates. The performance of ICL is highly dominated by the quality of\nthe selected in-context examples. However, previous selection methods are\nmostly based on simple heuristics, leading to sub-optimal performance. In this\nwork, we formulate in-context example selection as a subset selection problem.\nWe propose CEIL (Compositional Exemplars for In-context Learning), which is\ninstantiated by Determinantal Point Processes (DPPs) to model the interaction\nbetween the given input and in-context examples, and optimized through a\ncarefully-designed contrastive learning objective to obtain preference from\nLMs. We validate CEIL on 12 classification and generation datasets from 7\ndistinct NLP tasks, including sentiment analysis, paraphrase detection, natural\nlanguage inference, commonsense reasoning, open-domain question answering, code\ngeneration, and semantic parsing. Extensive experiments demonstrate not only\nthe state-of-the-art performance but also the transferability and\ncompositionality of CEIL, shedding new light on effective and efficient\nin-context learning. Our code is released at\nhttps://github.com/HKUNLP/icl-ceil.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiacheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiangtao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MarioGPT: Open-Ended Text2Level Generation through Large Language Models. (arXiv:2302.05981v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2302.05981","description":"<p>Procedural Content Generation (PCG) algorithms provide a technique to\ngenerate complex and diverse environments in an automated way. However, while\ngenerating content with PCG methods is often straightforward, generating\nmeaningful content that reflects specific intentions and constraints remains\nchallenging. Furthermore, many PCG algorithms lack the ability to generate\ncontent in an open-ended manner. Recently, Large Language Models (LLMs) have\nshown to be incredibly effective in many diverse domains. These trained LLMs\ncan be fine-tuned, re-using information and accelerating training for new\ntasks. In this work, we introduce MarioGPT, a fine-tuned GPT2 model trained to\ngenerate tile-based game levels, in our case Super Mario Bros levels. We show\nthat MarioGPT can not only generate diverse levels, but can be text-prompted\nfor controllable level generation, addressing one of the key challenges of\ncurrent PCG techniques. As far as we know, MarioGPT is the first text-to-level\nmodel. We also combine MarioGPT with novelty search, enabling it to generate\ndiverse levels with varying play-style dynamics (i.e. player paths). This\ncombination allows for the open-ended generation of an increasingly diverse\nrange of content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sudhakaran_S/0/1/0/all/0/1\">Shyam Sudhakaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Duque_M/0/1/0/all/0/1\">Miguel Gonz&#xe1;lez-Duque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glanois_C/0/1/0/all/0/1\">Claire Glanois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freiberger_M/0/1/0/all/0/1\">Matthias Freiberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najarro_E/0/1/0/all/0/1\">Elias Najarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risi_S/0/1/0/all/0/1\">Sebastian Risi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-efficient Modularised Bias Mitigation via AdapterFusion. (arXiv:2302.06321v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.06321","description":"<p>Large pre-trained language models contain societal biases and carry along\nthese biases to downstream tasks. Current in-processing bias mitigation\napproaches (like adversarial training) impose debiasing by updating a model's\nparameters, effectively transferring the model to a new, irreversible debiased\nstate. In this work, we propose a novel approach to develop stand-alone\ndebiasing functionalities separate from the model, which can be integrated into\nthe model on-demand, while keeping the core model untouched. Drawing from the\nconcept of AdapterFusion in multi-task learning, we introduce DAM (Debiasing\nwith Adapter Modules) - a debiasing approach to first encapsulate arbitrary\nbias mitigation functionalities into separate adapters, and then add them to\nthe model on-demand in order to deliver fairness qualities. We conduct a large\nset of experiments on three classification tasks with gender, race, and age as\nprotected attributes. Our results show that DAM improves or maintains the\neffectiveness of bias mitigation, avoids catastrophic forgetting in a\nmulti-attribute scenario, and maintains on-par task performance, while granting\nparameter-efficiency and easy switching between the original and debiased\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1\">Deepak Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lesota_O/0/1/0/all/0/1\">Oleg Lesota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zerveas_G/0/1/0/all/0/1\">George Zerveas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_D/0/1/0/all/0/1\">Daniel Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schedl_M/0/1/0/all/0/1\">Markus Schedl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekabsaz_N/0/1/0/all/0/1\">Navid Rekabsaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NL2CMD: An Updated Workflow for Natural Language to Bash Commands Translation. (arXiv:2302.07845v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.07845","description":"<p>Translating natural language into Bash Commands is an emerging research field\nthat has gained attention in recent years. Most efforts have focused on\nproducing more accurate translation models. To the best of our knowledge, only\ntwo datasets are available, with one based on the other. Both datasets involve\nscraping through known data sources (through platforms like stack overflow,\ncrowdsourcing, etc.) and hiring experts to validate and correct either the\nEnglish text or Bash Commands. This paper provides two contributions to\nresearch on synthesizing Bash Commands from scratch. First, we describe a\nstate-of-the-art translation model used to generate Bash Commands from the\ncorresponding English text. Second, we introduce a new NL2CMD dataset that is\nautomatically generated, involves minimal human intervention, and is over six\ntimes larger than prior datasets. Since the generation pipeline does not rely\non existing Bash Commands, the distribution and types of commands can be custom\nadjusted. We evaluate the performance of ChatGPT on this task and discuss the\npotential of using it as a data generator. Our empirical results show how the\nscale and diversity of our dataset can offer unique opportunities for semantic\nparsing researchers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Quchen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1\">Zhongwei Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgaklis_M/0/1/0/all/0/1\">Marco Georgaklis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1\">Jules White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_D/0/1/0/all/0/1\">Douglas C. Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference. (arXiv:2303.06182v2 [cs.DC] UPDATED)","link":"http://arxiv.org/abs/2303.06182","description":"<p>Mixture-of-Experts (MoE) models have gained popularity in achieving\nstate-of-the-art performance in a wide range of tasks in computer vision and\nnatural language processing. They effectively expand the model capacity while\nincurring a minimal increase in computation cost during training. However,\ndeploying such models for inference is difficult due to their large size and\ncomplex communication pattern. In this work, we provide a characterization of\ntwo MoE workloads, namely Language Modeling (LM) and Machine Translation (MT)\nand identify their sources of inefficiencies at deployment. We propose three\noptimization techniques to mitigate sources of inefficiencies, namely (1)\nDynamic gating, (2) Expert Buffering, and (3) Expert load balancing. We show\nthat dynamic gating improves maximum throughput by 6.21-11.23$\\times$ for LM,\n5.75-10.98$\\times$ for MT Encoder and 2.58-5.71$\\times$ for MT Decoder. It also\nreduces memory usage by up to 1.36$\\times$ for LM and up to 1.1$\\times$ for MT.\nWe further propose Expert Buffering, a new caching mechanism that only keeps\nhot, active experts in GPU memory while buffering the rest in CPU memory. This\nreduces static memory allocation by up to 1.47$\\times$. We finally propose a\nload balancing methodology that provides additional scalability to the\nworkload.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haiyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardalani_N/0/1/0/all/0/1\">Newsha Ardalani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Anna Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1\">Liu Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsien-Hsin S. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1\">Anjali Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhosale_S/0/1/0/all/0/1\">Shruti Bhosale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Carole-Jean Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Benjamin Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2304.04370","description":"<p>Human intelligence excels at combining basic skills to solve complex tasks.\nThis capability is vital for Artificial Intelligence (AI) and should be\nembedded in comprehensive intelligent models, enabling them to harness expert\nmodels for complex task-solving towards Artificial General Intelligence (AGI).\nLarge Language Models (LLMs) show promising learning and reasoning abilities,\nand can effectively use external models to tackle complex problems. In this\nwork, we introduce OpenAGI, an open-source AGI research platform designed for\nmulti-step, real-world tasks. Specifically, OpenAGI uses a dual strategy,\nintegrating standard benchmark tasks for benchmarking and evaluation, and\nopen-ended tasks including more expandable models for creative problem-solving.\nTasks are presented as natural language queries to the LLM, which then selects\nand executes appropriate models. We also propose a Reinforcement Learning from\nTask Feedback (RLTF) mechanism that uses task results to improve the LLM's\nability, which creates a self-improving AI feedback loop. While we acknowledge\nthat AGI is a broad and multifaceted research challenge with no singularly\ndefined solution path, the integration of LLMs with domain-specific expert\nmodels, inspired by mirroring the blend of general and specialized intelligence\nin humans, offers a promising approach towards AGI. We are open-sourcing the\nOpenAGI project's code, dataset, benchmarks, evaluation methods, and demo to\nfoster community involvement in AGI advancement:\nhttps://github.com/agiresearch/OpenAGI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wenyue Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_K/0/1/0/all/0/1\">Kai Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jianchao Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Juntao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zelong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASL Citizen: A Community-Sourced Dataset for Advancing Isolated Sign Language Recognition. (arXiv:2304.05934v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2304.05934","description":"<p>Sign languages are used as a primary language by approximately 70 million\nD/deaf people world-wide. However, most communication technologies operate in\nspoken and written languages, creating inequities in access. To help tackle\nthis problem, we release ASL Citizen, the first crowdsourced Isolated Sign\nLanguage Recognition (ISLR) dataset, collected with consent and containing\n83,399 videos for 2,731 distinct signs filmed by 52 signers in a variety of\nenvironments. We propose that this dataset be used for sign language dictionary\nretrieval for American Sign Language (ASL), where a user demonstrates a sign to\ntheir webcam to retrieve matching signs from a dictionary. We show that\ntraining supervised machine learning classifiers with our dataset advances the\nstate-of-the-art on metrics relevant for dictionary retrieval, achieving 63%\naccuracy and a recall-at-10 of 91%, evaluated entirely on videos of users who\nare not present in the training or validation sets. An accessible PDF of this\narticle is available at the following link:\nhttps://aashakadesai.github.io/research/ASLCitizen_arxiv_updated.pdf\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Desai_A/0/1/0/all/0/1\">Aashaka Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berger_L/0/1/0/all/0/1\">Lauren Berger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minakov_F/0/1/0/all/0/1\">Fyodor O. Minakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milan_V/0/1/0/all/0/1\">Vanessa Milan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1\">Chinmay Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pumphrey_K/0/1/0/all/0/1\">Kriston Pumphrey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ladner_R/0/1/0/all/0/1\">Richard E. Ladner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daume_H/0/1/0/all/0/1\">Hal Daum&#xe9; III</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1\">Alex X. Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caselli_N/0/1/0/all/0/1\">Naomi Caselli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bragg_D/0/1/0/all/0/1\">Danielle Bragg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting k-NN for Fine-tuning Pre-trained Language Models. (arXiv:2304.09058v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.09058","description":"<p>Pre-trained Language Models (PLMs), as parametric-based eager learners, have\nbecome the de-facto choice for current paradigms of Natural Language Processing\n(NLP). In contrast, k-Nearest-Neighbor (kNN) classifiers, as the lazy learning\nparadigm, tend to mitigate over-fitting and isolated noise. In this paper, we\nrevisit kNN classifiers for augmenting the PLMs-based classifiers. From the\nmethodological level, we propose to adopt kNN with textual representations of\nPLMs in two steps: (1) Utilize kNN as prior knowledge to calibrate the training\nprocess. (2) Linearly interpolate the probability distribution predicted by kNN\nwith that of the PLMs' classifier. At the heart of our approach is the\nimplementation of kNN-calibrated training, which treats predicted results as\nindicators for easy versus hard examples during the training process. From the\nperspective of the diversity of application scenarios, we conduct extensive\nexperiments on fine-tuning, prompt-tuning paradigms and zero-shot, few-shot and\nfully-supervised settings, respectively, across eight diverse end-tasks. We\nhope our exploration will encourage the community to revisit the power of\nclassical methods for efficient NLP. Code and datasets are available in\nhttps://github.com/zjunlp/Revisit-KNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1\">Bozhong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Development of a Trust-Aware User Simulator for Statistical Proactive Dialog Modeling in Human-AI Teams. (arXiv:2304.11913v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2304.11913","description":"<p>The concept of a Human-AI team has gained increasing attention in recent\nyears. For effective collaboration between humans and AI teammates, proactivity\nis crucial for close coordination and effective communication. However, the\ndesign of adequate proactivity for AI-based systems to support humans is still\nan open question and a challenging topic. In this paper, we present the\ndevelopment of a corpus-based user simulator for training and testing proactive\ndialog policies. The simulator incorporates informed knowledge about proactive\ndialog and its effect on user trust and simulates user behavior and personal\ninformation, including socio-demographic features and personality traits. Two\ndifferent simulation approaches were compared, and a task-step-based approach\nyielded better overall results due to enhanced modeling of sequential\ndependencies. This research presents a promising avenue for exploring and\nevaluating appropriate proactive strategies in a dialog game setting for\nimproving Human-AI teams.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kraus_M/0/1/0/all/0/1\">Matthias Kraus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riekenbrauck_R/0/1/0/all/0/1\">Ron Riekenbrauck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minker_W/0/1/0/all/0/1\">Wolfgang Minker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models. (arXiv:2305.01146v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01146","description":"<p>We systematically investigate lightweight strategies to adapt large language\nmodels (LLMs) for the task of radiology report summarization (RRS).\nSpecifically, we focus on domain adaptation via pretraining (on natural\nlanguage, biomedical text, or clinical text) and via discrete prompting or\nparameter-efficient fine-tuning. Our results consistently achieve best\nperformance by maximally adapting to the task via pretraining on clinical text\nand fine-tuning on RRS examples. Importantly, this method fine-tunes a mere\n0.32% of parameters throughout the model, in contrast to end-to-end fine-tuning\n(100% of parameters). Additionally, we study the effect of in-context examples\nand out-of-distribution (OOD) training before concluding with a radiologist\nreader study and qualitative analysis. Our findings highlight the importance of\ndomain adaptation in RRS and provide valuable insights toward developing\neffective natural language processing solutions for clinical tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Veen_D/0/1/0/all/0/1\">Dave Van Veen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uden_C/0/1/0/all/0/1\">Cara Van Uden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attias_M/0/1/0/all/0/1\">Maayane Attias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pareek_A/0/1/0/all/0/1\">Anuj Pareek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bluethgen_C/0/1/0/all/0/1\">Christian Bluethgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polacin_M/0/1/0/all/0/1\">Malgorzata Polacin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1\">Wah Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delbrouck_J/0/1/0/all/0/1\">Jean-Benoit Delbrouck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaves_J/0/1/0/all/0/1\">Juan Manuel Zambrano Chaves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhari_A/0/1/0/all/0/1\">Akshay S. Chaudhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauly_J/0/1/0/all/0/1\">John Pauly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Working Memory Capacity of ChatGPT: An Empirical Study. (arXiv:2305.03731v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2305.03731","description":"<p>Working memory is a critical aspect of both human intelligence and artificial\nintelligence, serving as a workspace for the temporary storage and manipulation\nof information. In this paper, we systematically assess the working memory\ncapacity of ChatGPT (gpt-3.5-turbo), a large language model developed by\nOpenAI, by examining its performance in verbal and spatial n-back tasks under\nvarious conditions. Our experiments reveal that ChatGPT experiences significant\ndeclines in performance as n increases (which necessitates more information to\nbe stored in working memory), suggesting a limit to the working memory capacity\nstrikingly similar to that of humans. Furthermore, we investigate the impact of\ndifferent instruction strategies on ChatGPT's performance and observe that the\nfundamental patterns of a capacity limit persist. From our empirical findings,\nwe propose that n-back tasks may serve as tools for benchmarking the working\nmemory capacity of large language models and hold potential for informing\nfuture efforts aimed at enhancing AI working memory and deepening our\nunderstanding of human working memory through AI models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1\">Dongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xingchen Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dingmin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advancing Neural Encoding of Portuguese with Transformer Albertina PT-*. (arXiv:2305.06721v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06721","description":"<p>To advance the neural encoding of Portuguese (PT), and a fortiori the\ntechnological preparation of this language for the digital age, we developed a\nTransformer-based foundation model that sets a new state of the art in this\nrespect for two of its variants, namely European Portuguese from Portugal\n(PT-PT) and American Portuguese from Brazil (PT-BR).\n</p>\n<p>To develop this encoder, which we named Albertina PT-*, a strong model was\nused as a starting point, DeBERTa, and its pre-training was done over data sets\nof Portuguese, namely over data sets we gathered for PT-PT and PT-BR, and over\nthe brWaC corpus for PT-BR. The performance of Albertina and competing models\nwas assessed by evaluating them on prominent downstream language processing\ntasks adapted for Portuguese.\n</p>\n<p>Both Albertina PT-PT and PT-BR versions are distributed free of charge and\nunder the most permissive license possible and can be run on consumer-grade\nhardware, thus seeking to contribute to the advancement of research and\ninnovation in language technology for Portuguese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_J/0/1/0/all/0/1\">Jo&#xe3;o Rodrigues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomes_L/0/1/0/all/0/1\">Lu&#xed;s Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1\">Jo&#xe3;o Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Branco_A/0/1/0/all/0/1\">Ant&#xf3;nio Branco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_R/0/1/0/all/0/1\">Rodrigo Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_H/0/1/0/all/0/1\">Henrique Lopes Cardoso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osorio_T/0/1/0/all/0/1\">Tom&#xe1;s Os&#xf3;rio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Hidden Mystery of OCR in Large Multimodal Models. (arXiv:2305.07895v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.07895","description":"<p>Large models have recently played a dominant role in natural language\nprocessing and multimodal vision-language learning. It remains less explored\nabout their efficacy in text-related visual tasks. We conducted a comprehensive\nstudy of existing publicly available multimodal models, evaluating their\nperformance in text recognition (document text, artistic text, handwritten\ntext, scene text), text-based visual question answering (document text, scene\ntext, and bilingual text), key information extraction (receipts, documents, and\nnutrition facts) and handwritten mathematical expression recognition. Our\nfindings reveal strengths and weaknesses in these models, which primarily rely\non semantic understanding for word recognition and exhibit inferior perception\nof individual character shapes. They also display indifference towards text\nlength and have limited capabilities in detecting finegrained features in\nimages. Consequently, these results demonstrate that even the current most\npowerful large multimodal models cannot match domain-specific methods in\ntraditional text tasks and face greater challenges in more complex tasks. Most\nimportantly, the baseline results showcased in this study could provide a\nfoundational framework for the conception and assessment of innovative\nstrategies targeted at enhancing zero-shot multimodal techniques. Evaluation\npipeline is available at https://github.com/Yuliang-Liu/MultimodalOCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenwen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Biao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Mingxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Dezhi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingrui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xucheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cheng-lin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bring More Attention to Syntactic Symmetry for Automatic Postediting of High-Quality Machine Translations. (arXiv:2305.10557v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10557","description":"<p>Automatic postediting (APE) is an automated process to refine a given machine\ntranslation (MT). Recent findings present that existing APE systems are not\ngood at handling high-quality MTs even for a language pair with abundant data\nresources, English-to-German: the better the given MT is, the harder it is to\ndecide what parts to edit and how to fix these errors. One possible solution to\nthis problem is to instill deeper knowledge about the target language into the\nmodel. Thus, we propose a linguistically motivated method of regularization\nthat is expected to enhance APE models' understanding of the target language: a\nloss function that encourages symmetric self-attention on the given MT. Our\nanalysis of experimental results demonstrates that the proposed method helps\nimproving the state-of-the-art architecture's APE quality for high-quality MTs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_B/0/1/0/all/0/1\">Baikjin Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Myungji Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jong-Hyeok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yunsu Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models can be Guided to Evade AI-Generated Text Detection. (arXiv:2305.10847v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10847","description":"<p>Large Language Models (LLMs) have demonstrated exceptional performance in a\nvariety of tasks, including essay writing and question answering. However, it\nis crucial to address the potential misuse of these models, which can lead to\ndetrimental outcomes such as plagiarism and spamming. Recently, several\ndetectors have been proposed, including fine-tuned classifiers and various\nstatistical methods. In this study, we reveal that with the aid of carefully\ncrafted prompts, LLMs can effectively evade these detection systems. We propose\na novel Substitution-based In-Context example Optimization method (SICO) to\nautomatically generate such prompts. On three real-world tasks where LLMs can\nbe misused, SICO successfully enables ChatGPT to evade six existing detectors,\ncausing a significant 0.54 AUC drop on average. Surprisingly, in most cases\nthese detectors perform even worse than random classifiers. These results\nfirmly reveal the vulnerability of existing detectors. Finally, the strong\nperformance of SICO suggests itself as a reliable evaluation protocol for any\nnew detector in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_N/0/1/0/all/0/1\">Ning Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengcai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Rui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Viability of Synthetic Query Generation for Relevance Prediction. (arXiv:2305.11944v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2305.11944","description":"<p>Query-document relevance prediction is a critical problem in Information\nRetrieval systems. This problem has increasingly been tackled using\n(pretrained) transformer-based models which are finetuned using large\ncollections of labeled data. However, in specialized domains such as e-commerce\nand healthcare, the viability of this approach is limited by the dearth of\nlarge in-domain data. To address this paucity, recent methods leverage these\npowerful models to generate high-quality task and domain-specific synthetic\ndata. Prior work has largely explored synthetic data generation or query\ngeneration (QGen) for Question-Answering (QA) and binary (yes/no) relevance\nprediction, where for instance, the QGen models are given a document, and\ntrained to generate a query relevant to that document. However in many\nproblems, we have a more fine-grained notion of relevance than a simple yes/no\nlabel. Thus, in this work, we conduct a detailed study into how QGen approaches\ncan be leveraged for nuanced relevance prediction. We demonstrate that --\ncontrary to claims from prior works -- current QGen approaches fall short of\nthe more conventional cross-domain transfer-learning approaches. Via empirical\nstudies spanning 3 public e-commerce benchmarks, we identify new shortcomings\nof existing QGen approaches -- including their inability to distinguish between\ndifferent grades of relevance. To address this, we introduce label-conditioned\nQGen models which incorporates knowledge about the different relevance. While\nour experiments demonstrate that these modifications help improve performance\nof QGen techniques, we also find that QGen approaches struggle to capture the\nfull nuance of the relevance label space and as a result the generated queries\nare not faithful to the desired relevance label.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1\">Aditi Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_K/0/1/0/all/0/1\">Karthik Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_K/0/1/0/all/0/1\">Krishna Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kazuma Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1\">Mike Bendersky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najork_M/0/1/0/all/0/1\">Marc Najork</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptNER: Prompting For Named Entity Recognition. (arXiv:2305.15444v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.15444","description":"<p>In a surprising turn, Large Language Models (LLMs) together with a growing\narsenal of prompt-based heuristics now offer powerful off-the-shelf approaches\nproviding few-shot solutions to myriad classic NLP problems. However, despite\npromising early results, these LLM-based few-shot methods remain far from the\nstate of the art in Named Entity Recognition (NER), where prevailing methods\ninclude learning representations via end-to-end structural understanding and\nfine-tuning on standard labeled corpora. In this paper, we introduce PromptNER,\na new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to\nany new NER task PromptNER requires a set of entity definitions in addition to\nthe standard few-shot examples. Given a sentence, PromptNER prompts an LLM to\nproduce a list of potential entities along with corresponding explanations\njustifying their compatibility with the provided entity type definitions.\nRemarkably, PromptNER achieves state-of-the-art performance on few-shot NER,\nachieving a 4% (absolute) improvement in F1 score on the ConLL dataset, a 9%\n(absolute) improvement on the GENIA dataset, and a 4% (absolute) improvement on\nthe FewNERD dataset. PromptNER also moves the state of the art on Cross Domain\nNER, outperforming prior methods (including those not limited to the few-shot\nsetting), setting a new mark on 3/5 CrossNER target domains, with an average F1\ngain of 3%, despite using less than 2% of the available data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ashok_D/0/1/0/all/0/1\">Dhananjay Ashok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Simultaneous Speech Translation with Differentiable Segmentation. (arXiv:2305.16093v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16093","description":"<p>End-to-end simultaneous speech translation (SimulST) outputs translation\nwhile receiving the streaming speech inputs (a.k.a. streaming speech\ntranslation), and hence needs to segment the speech inputs and then translate\nbased on the current received speech. However, segmenting the speech inputs at\nunfavorable moments can disrupt the acoustic integrity and adversely affect the\nperformance of the translation model. Therefore, learning to segment the speech\ninputs at those moments that are beneficial for the translation model to\nproduce high-quality translation is the key to SimulST. Existing SimulST\nmethods, either using the fixed-length segmentation or external segmentation\nmodel, always separate segmentation from the underlying translation model,\nwhere the gap results in segmentation outcomes that are not necessarily\nbeneficial for the translation process. In this paper, we propose\nDifferentiable Segmentation (DiSeg) for SimulST to directly learn segmentation\nfrom the underlying translation model. DiSeg turns hard segmentation into\ndifferentiable through the proposed expectation training, enabling it to be\njointly trained with the translation model and thereby learn\ntranslation-beneficial segmentation. Experimental results demonstrate that\nDiSeg achieves state-of-the-art performance and exhibits superior segmentation\ncapability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages. (arXiv:2305.16307v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16307","description":"<p>India has a rich linguistic landscape with languages from 4 major language\nfamilies spoken by over a billion people. 22 of these languages are listed in\nthe Constitution of India (referred to as scheduled languages) are the focus of\nthis work. Given the linguistic diversity, high-quality and accessible Machine\nTranslation (MT) systems are essential in a country like India. Prior to this\nwork, there was (i) no parallel training data spanning all the 22 languages,\n(ii) no robust benchmarks covering all these languages and containing content\nrelevant to India, and (iii) no existing translation models which support all\nthe 22 scheduled languages of India. In this work, we aim to address this gap\nby focusing on the missing pieces required for enabling wide, easy, and open\naccess to good machine translation systems for all 22 scheduled Indian\nlanguages. We identify four key areas of improvement: curating and creating\nlarger training datasets, creating diverse and high-quality benchmarks,\ntraining multilingual models, and releasing models with open access. Our first\ncontribution is the release of the Bharat Parallel Corpus Collection (BPCC),\nthe largest publicly available parallel corpora for Indic languages. BPCC\ncontains a total of 230M bitext pairs, of which a total of 126M were newly\nadded, including 644K manually translated sentence pairs created as part of\nthis work. Our second contribution is the release of the first n-way parallel\nbenchmark covering all 22 Indian languages, featuring diverse domains,\nIndian-origin content, and source-original test sets. Next, we present\nIndicTrans2, the first model to support all 22 languages, surpassing existing\nmodels on multiple existing and new benchmarks created as a part of this work.\nLastly, to promote accessibility and collaboration, we release our models and\nassociated data with permissive licenses at\nhttps://github.com/ai4bharat/IndicTrans2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+AI4Bharat/0/1/0/all/0/1\">AI4Bharat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gala_J/0/1/0/all/0/1\">Jay Gala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitale_P/0/1/0/all/0/1\">Pranjal A. Chitale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AK_R/0/1/0/all/0/1\">Raghavan AK</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doddapaneni_S/0/1/0/all/0/1\">Sumanth Doddapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gumma_V/0/1/0/all/0/1\">Varun Gumma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aswanth Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nawale_J/0/1/0/all/0/1\">Janki Nawale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sujatha_A/0/1/0/all/0/1\">Anupama Sujatha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puduppully_R/0/1/0/all/0/1\">Ratish Puduppully</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1\">Vivek Raghavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoding the Underlying Meaning of Multimodal Hateful Memes. (arXiv:2305.17678v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17678","description":"<p>Recent studies have proposed models that yielded promising performance for\nthe hateful meme classification task. Nevertheless, these proposed models do\nnot generate interpretable explanations that uncover the underlying meaning and\nsupport the classification output. A major reason for the lack of explainable\nhateful meme methods is the absence of a hateful meme dataset that contains\nground truth explanations for benchmarking or training. Intuitively, having\nsuch explanations can educate and assist content moderators in interpreting and\nremoving flagged hateful memes. This paper address this research gap by\nintroducing Hateful meme with Reasons Dataset (HatReD), which is a new\nmultimodal hateful meme dataset annotated with the underlying hateful\ncontextual reasons. We also define a new conditional generation task that aims\nto automatically generate underlying reasons to explain hateful memes and\nestablish the baseline performance of state-of-the-art pre-trained language\nmodels on this task. We further demonstrate the usefulness of HatReD by\nanalyzing the challenges of the new conditional generation task in explaining\nmemes in seen and unseen domains. The dataset and benchmark models are made\navailable here: https://github.com/Social-AI-Studio/HatRed\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hee_M/0/1/0/all/0/1\">Ming Shan Hee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_W/0/1/0/all/0/1\">Wen-Haw Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating GPT-3 Generated Explanations for Hateful Content Moderation. (arXiv:2305.17680v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17680","description":"<p>Recent research has focused on using large language models (LLMs) to generate\nexplanations for hate speech through fine-tuning or prompting. Despite the\ngrowing interest in this area, these generated explanations' effectiveness and\npotential limitations remain poorly understood. A key concern is that these\nexplanations, generated by LLMs, may lead to erroneous judgments about the\nnature of flagged content by both users and content moderators. For instance,\nan LLM-generated explanation might inaccurately convince a content moderator\nthat a benign piece of content is hateful. In light of this, we propose an\nanalytical framework for examining hate speech explanations and conducted an\nextensive survey on evaluating such explanations. Specifically, we prompted\nGPT-3 to generate explanations for both hateful and non-hateful content, and a\nsurvey was conducted with 2,400 unique respondents to evaluate the generated\nexplanations. Our findings reveal that (1) human evaluators rated the\nGPT-generated explanations as high quality in terms of linguistic fluency,\ninformativeness, persuasiveness, and logical soundness, (2) the persuasive\nnature of these explanations, however, varied depending on the prompting\nstrategy employed, and (3) this persuasiveness may result in incorrect\njudgments about the hatefulness of the content. Our study underscores the need\nfor caution in applying LLM-generated explanations for content moderation. Code\nand results are available at https://github.com/Social-AI-Studio/GPT3-HateEval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Han Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hee_M/0/1/0/all/0/1\">Ming Shan Hee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awal_M/0/1/0/all/0/1\">Md Rabiul Awal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_K/0/1/0/all/0/1\">Kenny Tsu Wei Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MedNgage: A Dataset for Understanding Engagement in Patient-Nurse Conversations. (arXiv:2305.19981v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.19981","description":"<p>Patients who effectively manage their symptoms often demonstrate higher\nlevels of engagement in conversations and interventions with healthcare\npractitioners. This engagement is multifaceted, encompassing cognitive and\nsocio-affective dimensions. Consequently, it is crucial for AI systems to\nunderstand the engagement in natural conversations between patients and\npractitioners to better contribute toward patient care. In this paper, we\npresent a novel dataset (MedNgage), which consists of patient-nurse\nconversations about cancer symptom management. We manually annotate the dataset\nwith a novel framework of categories of patient engagement from two different\nangles, namely: i) socio-affective (3.1K spans), and ii) cognitive use of\nlanguage (1.8K spans). Through statistical analysis of the data that is\nannotated using our framework, we show a positive correlation between patient\nsymptom management outcomes and their engagement in conversations.\nAdditionally, we demonstrate that pre-trained transformer models fine-tuned on\nour dataset can reliably predict engagement classes in patient-nurse\nconversations. Lastly, we use LIME (Ribeiro et al., 2016) to analyze the\nunderlying challenges of the tasks that state-of-the-art transformer models\nencounter. The de-identified data is available for research purposes upon\nrequest.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donovan_H/0/1/0/all/0/1\">Heidi Ann Scharf Donovan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_S/0/1/0/all/0/1\">Sabit Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Mailhe Alikhani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beam Tree Recursive Cells. (arXiv:2305.19999v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.19999","description":"<p>We propose Beam Tree Recursive Cell (BT-Cell) - a backpropagation-friendly\nframework to extend Recursive Neural Networks (RvNNs) with beam search for\nlatent structure induction. We further extend this framework by proposing a\nrelaxation of the hard top-k operators in beam search for better propagation of\ngradient signals. We evaluate our proposed models in different\nout-of-distribution splits in both synthetic and realistic data. Our\nexperiments show that BTCell achieves near-perfect performance on several\nchallenging structure-sensitive synthetic tasks like ListOps and logical\ninference while maintaining comparable performance in realistic data against\nother RvNN-based models. Additionally, we identify a previously unknown failure\ncase for neural models in generalization to unseen number of arguments in\nListOps. The code is available at:\nhttps://github.com/JRC1995/BeamTreeRecursiveCells.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_J/0/1/0/all/0/1\">Jishnu Ray Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts. (arXiv:2306.02207v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2306.02207","description":"<p>Large language models (LLMs) have gained considerable attention for\nArtificial Intelligence Generated Content (AIGC), particularly with the\nemergence of ChatGPT. However, the direct adaptation of continuous speech to\nLLMs that process discrete tokens remains an unsolved challenge, hindering the\napplication of LLMs for speech generation. The advanced speech LMs are in the\ncorner, as that speech signals encapsulate a wealth of information, including\nspeaker and emotion, beyond textual data alone. Prompt tuning has demonstrated\nnotable gains in parameter efficiency and competitive performance on some\nspeech classification tasks. However, the extent to which prompts can\neffectively elicit generation tasks from speech LMs remains an open question.\nIn this paper, we present pioneering research that explores the application of\nprompt tuning to stimulate speech LMs for various generation tasks, within a\nunified framework called SpeechGen, with around 10M trainable parameters. The\nproposed unified framework holds great promise for efficiency and\neffectiveness, particularly with the imminent arrival of advanced speech LMs,\nwhich will significantly enhance the capabilities of the framework. The code\nand demos of SpeechGen will be available on the project website:\n\\url{https://ga642381.github.io/SpeechPrompt/speechgen}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1\">Haibin Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yuan-Kuei Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Cloud-based Machine Learning Pipeline for the Efficient Extraction of Insights from Customer Reviews. (arXiv:2306.07786v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.07786","description":"<p>The efficiency of natural language processing has improved dramatically with\nthe advent of machine learning models, particularly neural network-based\nsolutions. However, some tasks are still challenging, especially when\nconsidering specific domains. In this paper, we present a cloud-based system\nthat can extract insights from customer reviews using machine learning methods\nintegrated into a pipeline. For topic modeling, our composite model uses\ntransformer-based neural networks designed for natural language processing,\nvector embedding-based keyword extraction, and clustering. The elements of our\nmodel have been integrated and further developed to meet better the\nrequirements of efficient information extraction, topic modeling of the\nextracted information, and user needs. Furthermore, our system can achieve\nbetter results than this task's existing topic modeling and keyword extraction\nsolutions. Our approach is validated and compared with other state-of-the-art\nmethods using publicly available datasets for benchmarking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lakatos_R/0/1/0/all/0/1\">Robert Lakatos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogacsovics_G/0/1/0/all/0/1\">Gergo Bogacsovics</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harangi_B/0/1/0/all/0/1\">Balazs Harangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakatos_I/0/1/0/all/0/1\">Istvan Lakatos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiba_A/0/1/0/all/0/1\">Attila Tiba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toth_J/0/1/0/all/0/1\">Janos Toth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szabo_M/0/1/0/all/0/1\">Marianna Szabo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajdu_A/0/1/0/all/0/1\">Andras Hajdu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unifying Large Language Models and Knowledge Graphs: A Roadmap. (arXiv:2306.08302v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.08302","description":"<p>Large language models (LLMs), such as ChatGPT and GPT4, are making new waves\nin the field of natural language processing and artificial intelligence, due to\ntheir emergent ability and generalizability. However, LLMs are black-box\nmodels, which often fall short of capturing and accessing factual knowledge. In\ncontrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are\nstructured knowledge models that explicitly store rich factual knowledge. KGs\ncan enhance LLMs by providing external knowledge for inference and\ninterpretability. Meanwhile, KGs are difficult to construct and evolving by\nnature, which challenges the existing methods in KGs to generate new facts and\nrepresent unseen knowledge. Therefore, it is complementary to unify LLMs and\nKGs together and simultaneously leverage their advantages. In this article, we\npresent a forward-looking roadmap for the unification of LLMs and KGs. Our\nroadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs,\nwhich incorporate KGs during the pre-training and inference phases of LLMs, or\nfor the purpose of enhancing understanding of the knowledge learned by LLMs; 2)\nLLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding,\ncompletion, construction, graph-to-text generation, and question answering; and\n3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a\nmutually beneficial way to enhance both LLMs and KGs for bidirectional\nreasoning driven by both data and knowledge. We review and summarize existing\nefforts within these three frameworks in our roadmap and pinpoint their future\nresearch directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Linhao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiapu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xindong Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiPlomat: A Dialogue Dataset for Situated Pragmatic Reasoning. (arXiv:2306.09030v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.09030","description":"<p>Pragmatic reasoning plays a pivotal role in deciphering implicit meanings\nthat frequently arise in real-life conversations and is essential for the\ndevelopment of communicative social agents. In this paper, we introduce a novel\nchallenge, DiPlomat, aiming at benchmarking machines' capabilities on pragmatic\nreasoning and situated conversational understanding. Compared with previous\nworks that treat different figurative expressions (e.g. metaphor, sarcasm) as\nindividual tasks, DiPlomat provides a cohesive framework towards general\npragmatic understanding. Our dataset is created through the utilization of\nAmazon Mechanical Turk ( AMT ), resulting in a total of 4, 177 multi-turn\ndialogues. In conjunction with the dataset, we propose two tasks, Pragmatic\nIdentification and Reasoning (PIR) and Conversational Question Answering (CQA).\nExperimental results with state-of-the-art (SOTA) neural architectures reveal\nseveral significant findings: 1) large language models ( LLMs) exhibit poor\nperformance in tackling this subjective domain; 2) comprehensive comprehension\nof context emerges as a critical factor for establishing benign human-machine\ninteractions; 3) current models defect in the application of pragmatic\nreasoning. As a result, we call on more attention to improve the ability of\ncontext understanding, reasoning, and implied meaning modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hengli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zilong Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RED$^{\\rm FM}$: a Filtered and Multilingual Relation Extraction Dataset. (arXiv:2306.09802v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.09802","description":"<p>Relation Extraction (RE) is a task that identifies relationships between\nentities in a text, enabling the acquisition of relational facts and bridging\nthe gap between natural language and structured knowledge. However, current RE\nmodels often rely on small datasets with low coverage of relation types,\nparticularly when working with languages other than English. In this paper, we\naddress the above issue and provide two new resources that enable the training\nand evaluation of multilingual RE systems. First, we present SRED$^{\\rm FM}$,\nan automatically annotated dataset covering 18 languages, 400 relation types,\n13 entity types, totaling more than 40 million triplet instances. Second, we\npropose RED$^{\\rm FM}$, a smaller, human-revised dataset for seven languages\nthat allows for the evaluation of multilingual RE systems. To demonstrate the\nutility of these novel datasets, we experiment with the first end-to-end\nmultilingual RE model, mREBEL, that extracts triplets, including entity types,\nin multiple languages. We release our resources and model checkpoints at\nhttps://www.github.com/babelscape/rebel\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cabot_P/0/1/0/all/0/1\">Pere-Llu&#xed;s Huguet Cabot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tedeschi_S/0/1/0/all/0/1\">Simone Tedeschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1\">Axel-Cyrille Ngonga Ngomo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navigli_R/0/1/0/all/0/1\">Roberto Navigli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Demystifying GPT Self-Repair for Code Generation. (arXiv:2306.09896v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.09896","description":"<p>Large Language Models (LLMs) have shown remarkable aptitude in code\ngeneration but still struggle on challenging programming tasks. Self-repair --\nin which the model debugs and fixes mistakes in its own code -- has recently\nbecome a popular way to boost performance in these settings. However, only very\nlimited studies on how and when self-repair works effectively exist in the\nliterature, and one might wonder to what extent a model is really capable of\nproviding accurate feedback on why the code is wrong when that code was\ngenerated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's\nability to perform self-repair on APPS, a challenging dataset consisting of\ndiverse coding challenges. To do so, we first establish a new evaluation\nstrategy dubbed pass@t that measures the pass rate of the tasks against the\ntotal number of tokens sampled from the model, enabling a fair comparison to\npurely sampling-based approaches. With this evaluation strategy, we find that\nthe effectiveness of self-repair is only seen in GPT-4. We also observe that\nself-repair is bottlenecked by the feedback stage; using GPT-4 to give feedback\non the programs generated by GPT-3.5 and using expert human programmers to give\nfeedback on the programs generated by GPT-4, we unlock significant performance\ngains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Olausson_T/0/1/0/all/0/1\">Theo X. Olausson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inala_J/0/1/0/all/0/1\">Jeevana Priya Inala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenglong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solar_Lezama_A/0/1/0/all/0/1\">Armando Solar-Lezama</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts. (arXiv:2306.04723v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2306.04723","description":"<p>Rapidly increasing quality of AI-generated content makes it difficult to\ndistinguish between human and AI-generated texts, which may lead to undesirable\nconsequences for society. Therefore, it becomes increasingly important to study\nthe properties of human texts that are invariant over text domains and various\nproficiency of human writers, can be easily calculated for any language, and\ncan robustly separate natural and AI-generated texts regardless of the\ngeneration model and sampling method. In this work, we propose such an\ninvariant of human texts, namely the intrinsic dimensionality of the manifold\nunderlying the set of embeddings of a given text sample. We show that the\naverage intrinsic dimensionality of fluent texts in natural language is\nhovering around the value $9$ for several alphabet-based languages and around\n$7$ for Chinese, while the average intrinsic dimensionality of AI-generated\ntexts for each language is $\\approx 1.5$ lower, with a clear statistical\nseparation between human-generated and AI-generated distributions. This\nproperty allows us to build a score-based artificial text detector. The\nproposed detector's accuracy is stable over text domains, generator models, and\nhuman writer proficiency levels, outperforming SOTA detectors in model-agnostic\nand cross-domain scenarios by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tulchinskii_E/0/1/0/all/0/1\">Eduard Tulchinskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_K/0/1/0/all/0/1\">Kristian Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kushnareva_L/0/1/0/all/0/1\">Laida Kushnareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherniavskii_D/0/1/0/all/0/1\">Daniil Cherniavskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1\">Serguei Barannikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1\">Irina Piontkovskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolenko_S/0/1/0/all/0/1\">Sergey Nikolenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-06-20T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}