{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-03-27T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A comprehensive evaluation of ChatGPT's zero-shot Text-to-SQL capability. (arXiv:2303.13547v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13547","description":"<p>This paper presents the first comprehensive analysis of ChatGPT's Text-to-SQL\nability. Given the recent emergence of large-scale conversational language\nmodel ChatGPT and its impressive capabilities in both conversational abilities\nand code generation, we sought to evaluate its Text-to-SQL performance. We\nconducted experiments on 12 benchmark datasets with different languages,\nsettings, or scenarios, and the results demonstrate that ChatGPT has strong\ntext-to-SQL abilities. Although there is still a gap from the current\nstate-of-the-art (SOTA) model performance, considering that the experiment was\nconducted in a zero-shot scenario, ChatGPT's performance is still impressive.\nNotably, in the ADVETA (RPL) scenario, the zero-shot ChatGPT even outperforms\nthe SOTA model that requires fine-tuning on the Spider dataset by 4.1\\%,\ndemonstrating its potential for use in practical applications. To support\nfurther research in related fields, we have made the data generated by ChatGPT\npublicly available at https://github.com/THU-BPM/chatgpt-sql.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optical Character Recognition and Transcription of Berber Signs from Images in a Low-Resource Language Amazigh. (arXiv:2303.13549v1 [cs.CV])","link":"http://arxiv.org/abs/2303.13549","description":"<p>The Berber, or Amazigh language family is a low-resource North African\nvernacular language spoken by the indigenous Berber ethnic group. It has its\nown unique alphabet called Tifinagh used across Berber communities in Morocco,\nAlgeria, and others. The Afroasiatic language Berber is spoken by 14 million\npeople, yet lacks adequate representation in education, research, web\napplications etc. For instance, there is no option of translation to or from\nAmazigh / Berber on Google Translate, which hosts over 100 languages today.\nConsequently, we do not find specialized educational apps, L2 (2nd language\nlearner) acquisition, automated language translation, and remote-access\nfacilities enabled in Berber. Motivated by this background, we propose a\nsupervised approach called DaToBS for Detection and Transcription of Berber\nSigns. The DaToBS approach entails the automatic recognition and transcription\nof Tifinagh characters from signs in photographs of natural environments. This\nis achieved by self-creating a corpus of 1862 pre-processed character images;\ncurating the corpus with human-guided annotation; and feeding it into an OCR\nmodel via the deployment of CNN for deep learning based on computer vision\nmodels. We deploy computer vision modeling (rather than language models)\nbecause there are pictorial symbols in this alphabet, this deployment being a\nnovel aspect of our work. The DaToBS experimentation and analyses yield over 92\npercent accuracy in our research. To the best of our knowledge, ours is among\nthe first few works in the automated transcription of Berber signs from\nroadside images with deep learning, yielding high accuracy. This can pave the\nway for developing pedagogical applications in the Berber language, thereby\naddressing an important goal of outreach to underrepresented communities via AI\nin education.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Corallo_L/0/1/0/all/0/1\">Levi Corallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varde_A/0/1/0/all/0/1\">Aparna S. Varde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Unsupervised Speech Recognition with Diffusion GANs. (arXiv:2303.13559v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13559","description":"<p>We enhance the vanilla adversarial training method for unsupervised Automatic\nSpeech Recognition (ASR) by a diffusion-GAN. Our model (1) injects instance\nnoises of various intensities to the generator's output and unlabeled reference\ntext which are sampled from pretrained phoneme language models with a length\nconstraint, (2) asks diffusion timestep-dependent discriminators to separate\nthem, and (3) back-propagates the gradients to update the generator.\nWord/phoneme error rate comparisons with wav2vec-U under Librispeech (3.1% for\ntest-clean and 5.6% for test-other), TIMIT and MLS datasets, show that our\nenhancement strategies work effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xianchao Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Return of the RNN: Residual Recurrent Networks for Invertible Sentence Embeddings. (arXiv:2303.13570v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13570","description":"<p>This study presents a novel model for invertible sentence embeddings using a\nresidual recurrent network trained on an unsupervised encoding task. Rather\nthan the probabilistic outputs common to neural machine translation models, our\napproach employs a regression-based output layer to reconstruct the input\nsequence's word vectors. The model achieves high accuracy and fast training\nwith the ADAM optimizer, a significant finding given that RNNs typically\nrequire memory units, such as LSTMs, or second-order optimization methods. We\nincorporate residual connections and introduce a \"match drop\" technique, where\ngradients are calculated only for incorrect words. Our approach demonstrates\npotential for various natural language processing applications, particularly in\nneural network-based systems that require high-quality sentence embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilkerson_J/0/1/0/all/0/1\">Jeremy Wilkerson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages. (arXiv:2303.13592v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13592","description":"<p>While code-mixing is a common linguistic practice in many parts of the world,\ncollecting high-quality and low-cost code-mixed data remains a challenge for\nnatural language processing (NLP) research. The proliferation of Large Language\nModels (LLMs) in recent times compels one to ask: can these systems be used for\ndata generation? In this article, we explore prompting LLMs in a zero-shot\nmanner to create code-mixed data for five languages in South East Asia (SEA) --\nIndonesian, Malay, Chinese, Tagalog, Vietnamese, as well as the creole language\nSinglish. We find that ChatGPT shows the most potential, capable of producing\ncode-mixed text 68% of the time when the term \"code-mixing\" is explicitly\ndefined. Moreover, both ChatGPT and InstructGPT's (davinci-003) performances in\ngenerating Singlish texts are noteworthy, averaging a 96% success rate across a\nvariety of prompts. The code-mixing proficiency of ChatGPT and InstructGPT,\nhowever, is dampened by word choice errors that lead to semantic inaccuracies.\nOther multilingual models such as BLOOMZ and Flan-T5-XXL are unable to produce\ncode-mixed texts altogether. By highlighting the limited promises of LLMs in a\nspecific form of low-resource data generation, we call for a measured approach\nwhen applying similar techniques to other data-scarce NLP contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yong_Z/0/1/0/all/0/1\">Zheng-Xin Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruochen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forde_J/0/1/0/all/0/1\">Jessica Zosa Forde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Skyler Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutawika_L/0/1/0/all/0/1\">Lintang Sutawika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1\">Jan Christian Blaise Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_L/0/1/0/all/0/1\">Long Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yin Lin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Authorship attribution for Differences between Literary Texts by Bilingual Russian-French and Non-Bilingual French Authors. (arXiv:2303.13622v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13622","description":"<p>Do bilingual Russian-French authors of the end of the twentieth century such\nas Andre\\\"i Makine, Val\\'ery Afanassiev, Vladimir F\\'edorovski, Iegor Gran,\nLuba Jurgenson have common stylistic traits in the novels they wrote in French?\nCan we distinguish between them and non-bilingual French writers' texts? Is the\nphenomenon of interference observable in French texts of Russian authors? This\npaper applies authorship attribution methods including Support Vector Machine\n(SVM), $K$-Nearest Neighbors (KNN), Ridge classification, and Neural Network to\nanswer these questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Makarova_M/0/1/0/all/0/1\">Margarita Makarova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-depth analysis of music structure as a self-organized network. (arXiv:2303.13631v1 [cs.SD])","link":"http://arxiv.org/abs/2303.13631","description":"<p>Words in a natural language not only transmit information but also evolve\nwith the development of civilization and human migration. The same is true for\nmusic. To understand the complex structure behind the music, we introduced an\nalgorithm called the Essential Element Network (EEN) to encode the audio into\ntext. The network is obtained by calculating the correlations between scales,\ntime, and volume. Optimizing EEN to generate Zipfs law for the frequency and\nrank of the clustering coefficient enables us to generate and regard the\nsemantic relationships as words. We map these encoded words into the\nscale-temporal space, which helps us organize systematically the syntax in the\ndeep structure of music. Our algorithm provides precise descriptions of the\ncomplex network behind the music, as opposed to the black-box nature of other\ndeep learning approaches. As a result, the experience and properties\naccumulated through these processes can offer not only a new approach to the\napplications of Natural Language Processing (NLP) but also an easier and more\nobjective way to analyze the evolution and development of music.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsai_P/0/1/0/all/0/1\">Ping-Rui Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1\">Yen-Ting Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nathan-Christopher Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui-Ling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hong-Yue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zih-Jia Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_T/0/1/0/all/0/1\">Tzay-Ming Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT or Grammarly? Evaluating ChatGPT on Grammatical Error Correction Benchmark. (arXiv:2303.13648v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13648","description":"<p>ChatGPT is a cutting-edge artificial intelligence language model developed by\nOpenAI, which has attracted a lot of attention due to its surprisingly strong\nability in answering follow-up questions. In this report, we aim to evaluate\nChatGPT on the Grammatical Error Correction(GEC) task, and compare it with\ncommercial GEC product (e.g., Grammarly) and state-of-the-art models (e.g.,\nGECToR). By testing on the CoNLL2014 benchmark dataset, we find that ChatGPT\nperforms not as well as those baselines in terms of the automatic evaluation\nmetrics (e.g., $F_{0.5}$ score), particularly on long sentences. We inspect the\noutputs and find that ChatGPT goes beyond one-by-one corrections. Specifically,\nit prefers to change the surface expression of certain phrases or sentence\nstructure while maintaining grammatical correctness. Human evaluation\nquantitatively confirms this and suggests that ChatGPT produces less\nunder-correction or mis-correction issues but more over-corrections. These\nresults demonstrate that ChatGPT is severely under-estimated by the automatic\nevaluation metrics and could be a promising tool for GEC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haoran Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yuxuan Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mordecai 3: A Neural Geoparser and Event Geocoder. (arXiv:2303.13675v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13675","description":"<p>Mordecai3 is a new end-to-end text geoparser and event geolocation system.\nThe system performs toponym resolution using a new neural ranking model to\nresolve a place name extracted from a document to its entry in the Geonames\ngazetteer. It also performs event geocoding, the process of linking events\nreported in text with the place names where they are reported to occur, using\nan off-the-shelf question-answering model. The toponym resolution model is\ntrained on a diverse set of existing training data, along with several thousand\nnewly annotated examples. The paper describes the model, its training process,\nand performance comparisons with existing geoparsers. The system is available\nas an open source Python library, Mordecai 3, and replaces an earlier\ngeoparser, Mordecai v2, one of the most widely used text geoparsers (Halterman\n2017).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Halterman_A/0/1/0/all/0/1\">Andrew Halterman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReCOGS: How Incidental Details of a Logical Form Overshadow an Evaluation of Semantic Interpretation. (arXiv:2303.13716v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13716","description":"<p>Compositional generalization benchmarks seek to assess whether models can\naccurately compute meanings for novel sentences, but operationalize this in\nterms of logical form (LF) prediction. This raises the concern that\nsemantically irrelevant details of the chosen LFs could shape model\nperformance. We argue that this concern is realized for the COGS benchmark (Kim\nand Linzen, 2020). COGS poses generalization splits that appear impossible for\npresent-day models, which could be taken as an indictment of those models.\nHowever, we show that the negative results trace to incidental features of COGS\nLFs. Converting these LFs to semantically equivalent ones and factoring out\ncapabilities unrelated to semantic interpretation, we find that even baseline\nmodels get traction. A recent variable-free translation of COGS LFs suggests\nsimilar conclusions, but we observe this format is not semantically equivalent;\nit is incapable of accurately representing some COGS meanings. These findings\ninform our proposal for ReCOGS, a modified version of COGS that comes closer to\nassessing the target semantic capabilities while remaining very challenging.\nOverall, our results reaffirm the importance of compositional generalization\nand careful benchmark task design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhengxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural language processing to automatically extract the presence and severity of esophagitis in notes of patients undergoing radiotherapy. (arXiv:2303.13722v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13722","description":"<p>Radiotherapy (RT) toxicities can impair survival and quality-of-life, yet\nremain under-studied. Real-world evidence holds potential to improve our\nunderstanding of toxicities, but toxicity information is often only in clinical\nnotes. We developed natural language processing (NLP) models to identify the\npresence and severity of esophagitis from notes of patients treated with\nthoracic RT. We fine-tuned statistical and pre-trained BERT-based models for\nthree esophagitis classification tasks: Task 1) presence of esophagitis, Task\n2) severe esophagitis or not, and Task 3) no esophagitis vs. grade 1 vs. grade\n2-3. Transferability was tested on 345 notes from patients with esophageal\ncancer undergoing RT.\n</p>\n<p>Fine-tuning PubmedBERT yielded the best performance. The best macro-F1 was\n0.92, 0.82, and 0.74 for Task 1, 2, and 3, respectively. Selecting the most\ninformative note sections during fine-tuning improved macro-F1 by over 2% for\nall tasks. Silver-labeled data improved the macro-F1 by over 3% across all\ntasks. For the esophageal cancer notes, the best macro-F1 was 0.73, 0.74, and\n0.65 for Task 1, 2, and 3, respectively, without additional fine-tuning.\n</p>\n<p>To our knowledge, this is the first effort to automatically extract\nesophagitis toxicity severity according to CTCAE guidelines from clinic notes.\nThe promising performance provides proof-of-concept for NLP-based automated\ndetailed toxicity monitoring in expanded domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guevara_M/0/1/0/all/0/1\">Marco Guevara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_N/0/1/0/all/0/1\">Nicolas Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_A/0/1/0/all/0/1\">Arpi Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warner_J/0/1/0/all/0/1\">Jeremy L. Warner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aerts_H/0/1/0/all/0/1\">Hugo JWL Aerts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_T/0/1/0/all/0/1\">Timothy A. Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savova_G/0/1/0/all/0/1\">Guergana K. Savova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mak_R/0/1/0/all/0/1\">Raymond H. Mak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitterman_D/0/1/0/all/0/1\">Danielle S. Bitterman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Making the Most of ChatGPT for Machine Translation. (arXiv:2303.13780v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13780","description":"<p>ChatGPT shows remarkable capabilities for machine translation (MT). Several\nprior studies have shown that it achieves comparable results to commercial\nsystems for high-resource languages, but lags behind in complex tasks, e.g,\nlow-resource and distant-language-pairs translation. However, they usually\nadopt simple prompts which can not fully elicit the capability of ChatGPT. In\nthis report, we aim to further mine ChatGPT's translation ability by revisiting\nseveral aspects: temperature, task information, and domain information, and\ncorrespondingly propose two (simple but effective) prompts: Task-Specific\nPrompts (TSP) and Domain-Specific Prompts (DSP). We show that: 1) The\nperformance of ChatGPT depends largely on temperature, and a lower temperature\nusually can achieve better performance; 2) Emphasizing the task information\nfurther improves ChatGPT's performance, particularly in complex MT tasks; 3)\nIntroducing domain information can elicit ChatGPT's generalization ability and\nimprove its performance in the specific domain; 4) ChatGPT tends to generate\nhallucinations for non-English-centric MT tasks, which can be partially\naddressed by our proposed prompts but still need to be highlighted for the\nMT/NLP community. We also explore the effects of advanced in-context learning\nstrategies and find a (negative but interesting) observation: the powerful\nchain-of-thought prompt leads to word-by-word translation behavior, thus\nbringing significant translation degradation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Keqin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1\">Qihuang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_Y/0/1/0/all/0/1\">Yuanxin Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalizing Task-oriented Dialog Systems via Zero-shot Generalizable Reward Function. (arXiv:2303.13797v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13797","description":"<p>Task-oriented dialog systems enable users to accomplish tasks using natural\nlanguage. State-of-the-art systems respond to users in the same way regardless\nof their personalities, although personalizing dialogues can lead to higher\nlevels of adoption and better user experiences. Building personalized dialog\nsystems is an important, yet challenging endeavor and only a handful of works\ntook on the challenge. Most existing works rely on supervised learning\napproaches and require laborious and expensive labeled training data for each\nuser profile. Additionally, collecting and labeling data for each user profile\nis virtually impossible. In this work, we propose a novel framework, P-ToD, to\npersonalize task-oriented dialog systems capable of adapting to a wide range of\nuser profiles in an unsupervised fashion using a zero-shot generalizable reward\nfunction. P-ToD uses a pre-trained GPT-2 as a backbone model and works in three\nphases. Phase one performs task-specific training. Phase two kicks off\nunsupervised personalization by leveraging the proximal policy optimization\nalgorithm that performs policy gradients guided by the zero-shot generalizable\nreward function. Our novel reward function can quantify the quality of the\ngenerated responses even for unseen profiles. The optional final phase\nfine-tunes the personalized model using a few labeled training examples. We\nconduct extensive experimental analysis using the personalized bAbI dialogue\nbenchmark for five tasks and up to 180 diverse user profiles. The experimental\nresults demonstrate that P-ToD, even when it had access to zero labeled\nexamples, outperforms state-of-the-art supervised personalization models and\nachieves competitive performance on BLEU and ROUGE metrics when compared to a\nstrong fully-supervised GPT-2 baseline\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siddique_A/0/1/0/all/0/1\">A.B. Siddique</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maqbool_M/0/1/0/all/0/1\">M.H. Maqbool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taywade_K/0/1/0/all/0/1\">Kshitija Taywade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foroosh_H/0/1/0/all/0/1\">Hassan Foroosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Open-domain Slot Filling via Self-supervised Co-training. (arXiv:2303.13801v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13801","description":"<p>Slot filling is one of the critical tasks in modern conversational systems.\nThe majority of existing literature employs supervised learning methods, which\nrequire labeled training data for each new domain. Zero-shot learning and weak\nsupervision approaches, among others, have shown promise as alternatives to\nmanual labeling. Nonetheless, these learning paradigms are significantly\ninferior to supervised learning approaches in terms of performance. To minimize\nthis performance gap and demonstrate the possibility of open-domain slot\nfilling, we propose a Self-supervised Co-training framework, called SCot, that\nrequires zero in-domain manually labeled training examples and works in three\nphases. Phase one acquires two sets of complementary pseudo labels\nautomatically. Phase two leverages the power of the pre-trained language model\nBERT, by adapting it for the slot filling task using these sets of pseudo\nlabels. In phase three, we introduce a self-supervised cotraining mechanism,\nwhere both models automatically select highconfidence soft labels to further\nimprove the performance of the other in an iterative fashion. Our thorough\nevaluations show that SCot outperforms state-of-the-art models by 45.57% and\n37.56% on SGD and MultiWoZ datasets, respectively. Moreover, our proposed\nframework SCot achieves comparable performance when compared to\nstate-of-the-art fully supervised models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mosharrof_A/0/1/0/all/0/1\">Adib Mosharrof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fereidouni_M/0/1/0/all/0/1\">Moghis Fereidouni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddique_A/0/1/0/all/0/1\">A.B. Siddique</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT. (arXiv:2303.13809v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13809","description":"<p>Generative large language models (LLMs), e.g., ChatGPT, have demonstrated\nremarkable proficiency across several NLP tasks such as machine translation,\nquestion answering, text summarization, and natural language understanding.\nRecent research has shown that utilizing ChatGPT for assessing the quality of\nmachine translation (MT) achieves state-of-the-art performance at the system\nlevel but performs poorly at the segment level. To further improve the\nperformance of LLMs on MT quality assessment, we conducted an investigation\ninto several prompting methods. Our results indicate that by combining\nChain-of-Thoughts and Error Analysis, a new prompting method called\n\\textbf{\\texttt{Error Analysis Prompting}}, LLMs like ChatGPT can\n\\textit{generate human-like MT evaluations at both the system and segment\nlevel}. Additionally, we discovered some limitations of ChatGPT as an MT\nevaluator, such as unstable scoring and biases when provided with multiple\ntranslations in a single query. Our findings aim to provide a preliminary\nexperience for appropriately evaluating translation quality on ChatGPT while\noffering a variety of tricks in designing prompts for in-context learning. We\nanticipate that this report will shed new light on advancing the field of\ntranslation evaluation with LLMs by enhancing both the accuracy and reliability\nof metrics. The project can be found in\n\\url{https://github.com/Coldmist-Lu/ErrorAnalysis_Prompt}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qingyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_B/0/1/0/all/0/1\">Baopu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Liping Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$k$NN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference. (arXiv:2303.13824v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13824","description":"<p>In-Context Learning (ICL), which formulates target tasks as prompt completion\nconditioned on in-context demonstrations, has become the prevailing utilization\nof LLMs. In this paper, we first disclose an actual predicament for this\ntypical usage that it can not scale up with training data due to context length\nrestriction. Besides, existing works have shown that ICL also suffers from\nvarious biases and requires delicate calibration treatment. To address both\nchallenges, we advocate a simple and effective solution, $k$NN Prompting, which\nfirst queries LLM with training data for distributed representations, then\npredicts test instances by simply referring to nearest neighbors. We conduct\ncomprehensive experiments to demonstrate its two-fold superiority: 1)\nCalibration-Free: $k$NN Prompting does not directly align LLM output\ndistribution with task-specific label space, instead leverages such\ndistribution to align test and training instances. It significantly outperforms\nstate-of-the-art calibration-based methods under comparable few-shot scenario.\n2) Beyond-Context: $k$NN Prompting can further scale up effectively with as\nmany training data as are available, continually bringing substantial\nimprovements. The scaling trend holds across 10 orders of magnitude ranging\nfrom 2 shots to 1024 shots as well as different LLMs scales ranging from 0.8B\nto 30B. It successfully bridges data scaling into model scaling, and brings new\npotentials for the gradient-free paradigm of LLM deployment. Code is publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Benfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhendong Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yajuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qiaoqiao She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongdong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HRDoc: Dataset and Baseline Method Toward Hierarchical Reconstruction of Document Structures. (arXiv:2303.13839v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13839","description":"<p>The problem of document structure reconstruction refers to converting digital\nor scanned documents into corresponding semantic structures. Most existing\nworks mainly focus on splitting the boundary of each element in a single\ndocument page, neglecting the reconstruction of semantic structure in\nmulti-page documents. This paper introduces hierarchical reconstruction of\ndocument structures as a novel task suitable for NLP and CV fields. To better\nevaluate the system performance on the new task, we built a large-scale dataset\nnamed HRDoc, which consists of 2,500 multi-page documents with nearly 2 million\nsemantic units. Every document in HRDoc has line-level annotations including\ncategories and relations obtained from rule-based extractors and human\nannotators. Moreover, we proposed an encoder-decoder-based hierarchical\ndocument structure parsing system (DSPS) to tackle this problem. By adopting a\nmulti-modal bidirectional encoder and a structure-aware GRU decoder with\nsoft-mask operation, the DSPS model surpass the baseline method by a large\nmargin. All scripts and datasets will be made publicly available at\nhttps://github.com/jfma-USTC/HRDoc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiefeng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Pengfei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenrong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianshu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Huihui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overview of the ICASSP 2023 General Meeting Understanding and Generation Challenge (MUG). (arXiv:2303.13932v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13932","description":"<p>ICASSP2023 General Meeting Understanding and Generation Challenge (MUG)\nfocuses on prompting a wide range of spoken language processing (SLP) research\non meeting transcripts, as SLP applications are critical to improve users'\nefficiency in grasping important information in meetings. MUG includes five\ntracks, including topic segmentation, topic-level and session-level extractive\nsummarization, topic title generation, keyphrase extraction, and action item\ndetection. To facilitate MUG, we construct and release a large-scale meeting\ndataset, the AliMeeting4MUG Corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhijie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MUG: A General Meeting Understanding and Generation Benchmark. (arXiv:2303.13939v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13939","description":"<p>Listening to long video/audio recordings from video conferencing and online\ncourses for acquiring information is extremely inefficient. Even after ASR\nsystems transcribe recordings into long-form spoken language documents, reading\nASR transcripts only partly speeds up seeking information. It has been observed\nthat a range of NLP applications, such as keyphrase extraction, topic\nsegmentation, and summarization, significantly improve users' efficiency in\ngrasping important information. The meeting scenario is among the most valuable\nscenarios for deploying these spoken language processing (SLP) capabilities.\nHowever, the lack of large-scale public meeting datasets annotated for these\nSLP tasks severely hinders their advancement. To prompt SLP advancement, we\nestablish a large-scale general Meeting Understanding and Generation Benchmark\n(MUG) to benchmark the performance of a wide range of SLP tasks, including\ntopic segmentation, topic-level and session-level extractive summarization and\ntopic title generation, keyphrase extraction, and action item detection. To\nfacilitate the MUG benchmark, we construct and release a large-scale meeting\ndataset for comprehensive long-form SLP development, the AliMeeting4MUG Corpus,\nwhich consists of 654 recorded Mandarin meeting sessions with diverse topic\ncoverage, with manual annotations for SLP tasks on manual transcripts of\nmeeting recordings. To the best of our knowledge, the AliMeeting4MUG Corpus is\nso far the largest meeting corpus in scale and facilitates most SLP tasks. In\nthis paper, we provide a detailed introduction of this corpus, SLP tasks and\nevaluation methods, baseline systems and their performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhijie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. (arXiv:2303.13988v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13988","description":"<p>Large language models (LLMs) are currently at the forefront of intertwining\nAI systems with human communication and everyday life. Due to rapid\ntechnological advances and their extreme versatility, LLMs nowadays have\nmillions of users and are at the cusp of being the main go-to technology for\ninformation retrieval, content generation, problem-solving, etc. Therefore, it\nis of great importance to thoroughly assess and scrutinize their capabilities.\nDue to increasingly complex and novel behavioral patterns in current LLMs, this\ncan be done by treating them as participants in psychology experiments that\nwere originally designed to test humans. For this purpose, the paper introduces\na new field of research called \"machine psychology\". The paper outlines how\ndifferent subfields of psychology can inform behavioral tests for LLMs. It\ndefines methodological standards for machine psychology research, especially by\nfocusing on policies for prompt designs. Additionally, it describes how\nbehavioral patterns discovered in LLMs are to be interpreted. In sum, machine\npsychology aims to discover emergent abilities in LLMs that cannot be detected\nby most traditional natural language processing benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagendorff_T/0/1/0/all/0/1\">Thilo Hagendorff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paraphrase Detection: Human vs. Machine Content. (arXiv:2303.13989v1 [cs.CL])","link":"http://arxiv.org/abs/2303.13989","description":"<p>The growing prominence of large language models, such as GPT-4 and ChatGPT,\nhas led to increased concerns over academic integrity due to the potential for\nmachine-generated content and paraphrasing. Although studies have explored the\ndetection of human- and machine-paraphrased content, the comparison between\nthese types of content remains underexplored. In this paper, we conduct a\ncomprehensive analysis of various datasets commonly employed for paraphrase\ndetection tasks and evaluate an array of detection methods. Our findings\nhighlight the strengths and limitations of different detection methods in terms\nof performance on individual datasets, revealing a lack of suitable\nmachine-generated datasets that can be aligned with human expectations. Our\nmain finding is that human-authored paraphrases exceed machine-generated ones\nin terms of difficulty, diversity, and similarity implying that automatically\ngenerated texts are not yet on par with human-level performance. Transformers\nemerged as the most effective method across datasets with TF-IDF excelling on\nsemantically diverse corpora. Additionally, we identify four datasets as the\nmost diverse and challenging for paraphrase detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Becker_J/0/1/0/all/0/1\">Jonas Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1\">Jan Philip Wahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPEC: Summary Preference Decomposition for Low-Resource Abstractive Summarization. (arXiv:2303.14011v1 [cs.CL])","link":"http://arxiv.org/abs/2303.14011","description":"<p>Neural abstractive summarization has been widely studied and achieved great\nsuccess with large-scale corpora. However, the considerable cost of annotating\ndata motivates the need for learning strategies under low-resource settings. In\nthis paper, we investigate the problems of learning summarizers with only few\nexamples and propose corresponding methods for improvements. First, typical\ntransfer learning methods are prone to be affected by data properties and\nlearning objectives in the pretext tasks. Therefore, based on pretrained\nlanguage models, we further present a meta learning framework to transfer\nfew-shot learning processes from source corpora to the target corpus. Second,\nprevious methods learn from training examples without decomposing the content\nand preference. The generated summaries could therefore be constrained by the\npreference bias in the training set, especially under low-resource settings. As\nsuch, we propose decomposing the contents and preferences during learning\nthrough the parameter modulation, which enables control over preferences during\ninference. Third, given a target application, specifying required preferences\ncould be non-trivial because the preferences may be difficult to derive through\nobservations. Therefore, we propose a novel decoding method to automatically\nestimate suitable preferences and generate corresponding summary candidates\nfrom the few training examples. Extensive experiments demonstrate that our\nmethods achieve state-of-the-art performance on six diverse corpora with\n30.11%/33.95%/27.51% and 26.74%/31.14%/24.48% average improvements on\nROUGE-1/2/L under 10- and 100-example settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Syuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yun-Zhu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuai_H/0/1/0/all/0/1\">Hong-Han Shuai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge. (arXiv:2303.14070v1 [cs.CL])","link":"http://arxiv.org/abs/2303.14070","description":"<p>Recent large language models (LLMs) in the general domain, such as ChatGPT,\nhave shown remarkable success in following instructions and producing\nhuman-like responses. However, such language models have not been learned\nindividually and carefully for the medical domain, resulting in poor diagnostic\naccuracy and inability to give correct recommendations for medical diagnosis,\nmedications, etc. To address this issue, we collected more than 700 diseases\nand their corresponding symptoms, recommended medications, and required medical\ntests, and then generated 5K doctor-patient conversations. By fine-tuning\nmodels of doctor-patient conversations, these models emerge with great\npotential to understand patients' needs, provide informed advice, and offer\nvaluable assistance in a variety of medical-related fields. The integration of\nthese advanced language models into healthcare can revolutionize the way\nhealthcare professionals and patients communicate, ultimately improving the\noverall quality of care and patient outcomes. In addition, we will open all\nsource code, datasets and model weights to advance the further development of\ndialogue models in the medical field. In addition, the training data, code, and\nweights of this project are available at:\nhttps://github.com/Kent0n-Li/ChatDoctor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yunxiang_L/0/1/0/all/0/1\">Li Yunxiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zihan_L/0/1/0/all/0/1\">Li Zihan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kai_Z/0/1/0/all/0/1\">Zhang Kai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruilong_D/0/1/0/all/0/1\">Dan Ruilong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1\">Zhang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Prediction Performance and Model Interpretability through Attention Mechanisms from Basic and Applied Research Perspectives. (arXiv:2303.14116v1 [cs.LG])","link":"http://arxiv.org/abs/2303.14116","description":"<p>With the dramatic advances in deep learning technology, machine learning\nresearch is focusing on improving the interpretability of model predictions as\nwell as prediction performance in both basic and applied research. While deep\nlearning models have much higher prediction performance than traditional\nmachine learning models, the specific prediction process is still difficult to\ninterpret and/or explain. This is known as the black-boxing of machine learning\nmodels and is recognized as a particularly important problem in a wide range of\nresearch fields, including manufacturing, commerce, robotics, and other\nindustries where the use of such technology has become commonplace, as well as\nthe medical field, where mistakes are not tolerated. This bulletin is based on\nthe summary of the author's dissertation. The research summarized in the\ndissertation focuses on the attention mechanism, which has been the focus of\nmuch attention in recent years, and discusses its potential for both basic\nresearch in terms of improving prediction performance and interpretability, and\napplied research in terms of evaluating it for real-world applications using\nlarge data sets beyond the laboratory environment. The dissertation also\nconcludes with a summary of the implications of these findings for subsequent\nresearch and future prospects in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kitada_S/0/1/0/all/0/1\">Shunsuke Kitada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The crime of being poor. (arXiv:2303.14128v1 [cs.CL])","link":"http://arxiv.org/abs/2303.14128","description":"<p>The criminalization of poverty has been widely denounced as a collective bias\nagainst the most vulnerable. NGOs and international organizations claim that\nthe poor are blamed for their situation, are more often associated with\ncriminal offenses than the wealthy strata of society and even incur criminal\noffenses simply as a result of being poor. While no evidence has been found in\nthe literature that correlates poverty and overall criminality rates, this\npaper offers evidence of a collective belief that associates both concepts.\nThis brief report measures the societal bias that correlates criminality with\nthe poor, as compared to the rich, by using Natural Language Processing (NLP)\ntechniques in Twitter. The paper quantifies the level of crime-poverty bias in\na panel of eight different English-speaking countries. The regional differences\nin the association between crime and poverty cannot be justified based on\ndifferent levels of inequality or unemployment, which the literature correlates\nto property crimes. The variation in the observed rates of crime-poverty bias\nfor different geographic locations could be influenced by cultural factors and\nthe tendency to overestimate the equality of opportunities and social mobility\nin specific countries. These results have consequences for policy-making and\nopen a new path of research for poverty mitigation with the focus not only on\nthe poor but on society as a whole. Acting on the collective bias against the\npoor would facilitate the approval of poverty reduction policies, as well as\nthe restoration of the dignity of the persons affected.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Curto_G/0/1/0/all/0/1\">Georgina Curto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiritchenko_S/0/1/0/all/0/1\">Svetlana Kiritchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nejadgholi_I/0/1/0/all/0/1\">Isar Nejadgholi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_K/0/1/0/all/0/1\">Kathleen C. Fraser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Expert Language Models with Unsupervised Domain Discovery. (arXiv:2303.14177v1 [cs.CL])","link":"http://arxiv.org/abs/2303.14177","description":"<p>Large language models are typically trained densely: all parameters are\nupdated with respect to all inputs. This requires synchronization of billions\nof parameters across thousands of GPUs. We introduce a simple but effective\nmethod to asynchronously train large, sparse language models on arbitrary text\ncorpora. Our method clusters a corpus into sets of related documents, trains a\nseparate expert language model on each cluster, and combines them in a sparse\nensemble for inference. This approach generalizes embarrassingly parallel\ntraining by automatically discovering the domains for each expert, and\neliminates nearly all the communication overhead of existing sparse language\nmodels. Our technique outperforms dense baselines on multiple corpora and\nfew-shot tasks, and our analysis shows that specializing experts to meaningful\nclusters is key to these gains. Performance also improves with the number of\nexperts and size of training data, suggesting this is a highly efficient and\naccessible approach to training large language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1\">Suchin Gururangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Margaret Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weijia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Althoff_T/0/1/0/all/0/1\">Tim Althoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing. (arXiv:2111.09543v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.09543","description":"<p>This paper presents a new pre-trained language model, DeBERTaV3, which\nimproves the original DeBERTa model by replacing mask language modeling (MLM)\nwith replaced token detection (RTD), a more sample-efficient pre-training task.\nOur analysis shows that vanilla embedding sharing in ELECTRA hurts training\nefficiency and model performance. This is because the training losses of the\ndiscriminator and the generator pull token embeddings in different directions,\ncreating the \"tug-of-war\" dynamics. We thus propose a new gradient-disentangled\nembedding sharing method that avoids the tug-of-war dynamics, improving both\ntraining efficiency and the quality of the pre-trained model. We have\npre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its\nexceptional performance on a wide range of downstream natural language\nunderstanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an\nexample, the DeBERTaV3 Large model achieves a 91.37% average score, which is\n1.37% over DeBERTa and 1.91% over ELECTRA, setting a new state-of-the-art\n(SOTA) among the models with a similar structure. Furthermore, we have\npre-trained a multi-lingual model mDeBERTa and observed a larger improvement\nover strong baselines compared to English models. For example, the mDeBERTa\nBase achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6%\nimprovement over XLM-R Base, creating a new SOTA on this benchmark. We have\nmade our pre-trained models and inference code publicly available at\nhttps://github.com/microsoft/DeBERTa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Penguins Don't Fly: Reasoning about Generics through Instantiations and Exceptions. (arXiv:2205.11658v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11658","description":"<p>Generics express generalizations about the world (e.g., birds can fly) that\nare not universally true (e.g., newborn birds and penguins cannot fly).\nCommonsense knowledge bases, used extensively in NLP, encode some generic\nknowledge but rarely enumerate such exceptions and knowing when a generic\nstatement holds or does not hold true is crucial for developing a comprehensive\nunderstanding of generics. We present a novel framework informed by linguistic\ntheory to generate exemplars -- specific cases when a generic holds true or\nfalse. We generate ~19k exemplars for ~650 generics and show that our framework\noutperforms a strong GPT-3 baseline by 12.8 precision points. Our analysis\nhighlights the importance of linguistic theory-based controllability for\ngenerating exemplars, the insufficiency of knowledge bases as a source of\nexemplars, and the challenges exemplars pose for the task of natural language\ninference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Allaway_E/0/1/0/all/0/1\">Emily Allaway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Causal Effects of Data Statistics on Language Model's `Factual' Predictions. (arXiv:2207.14251v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.14251","description":"<p>Large amounts of training data are one of the major reasons for the high\nperformance of state-of-the-art NLP models. But what exactly in the training\ndata causes a model to make a certain prediction? We seek to answer this\nquestion by providing a language for describing how training data influences\npredictions, through a causal framework. Importantly, our framework bypasses\nthe need to retrain expensive models and allows us to estimate causal effects\nbased on observational data alone. Addressing the problem of extracting factual\nknowledge from pretrained language models (PLMs), we focus on simple data\nstatistics such as co-occurrence counts and show that these statistics do\ninfluence the predictions of PLMs, suggesting that such models rely on shallow\nheuristics. Our causal framework and our results demonstrate the importance of\nstudying datasets and the benefits of causality for understanding NLP models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1\">Yanai Elazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kassner_N/0/1/0/all/0/1\">Nora Kassner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1\">Amir Feder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichander_A/0/1/0/all/0/1\">Abhilasha Ravichander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosbach_M/0/1/0/all/0/1\">Marius Mosbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When and why vision-language models behave like bags-of-words, and what to do about it?. (arXiv:2210.01936v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.01936","description":"<p>Despite the success of large vision and language models (VLMs) in many\ndownstream applications, it is unclear how well they encode compositional\ninformation. Here, we create the Attribution, Relation, and Order (ARO)\nbenchmark to systematically evaluate the ability of VLMs to understand\ndifferent types of relationships, attributes, and order. ARO consists of Visual\nGenome Attribution, to test the understanding of objects' properties; Visual\nGenome Relation, to test for relational understanding; and COCO &amp;\nFlickr30k-Order, to test for order sensitivity. ARO is orders of magnitude\nlarger than previous benchmarks of compositionality, with more than 50,000 test\ncases. We show where state-of-the-art VLMs have poor relational understanding,\ncan blunder when linking objects to their attributes, and demonstrate a severe\nlack of order sensitivity. VLMs are predominantly trained and evaluated on\nlarge datasets with rich compositional structure in the images and captions.\nYet, training on these datasets has not been enough to address the lack of\ncompositional understanding, and evaluating on these datasets has failed to\nsurface this deficiency. To understand why these limitations emerge and are not\nrepresented in the standard tests, we zoom into the evaluation and training\nprocedures. We demonstrate that it is possible to perform well on retrieval\nover existing datasets without using the composition and order information.\nGiven that contrastive pretraining optimizes for retrieval on datasets with\nsimilar shortcuts, we hypothesize that this can explain why the models do not\nneed to learn to represent compositional information. This finding suggests a\nnatural solution: composition-aware hard negative mining. We show that a\nsimple-to-implement modification of contrastive learning significantly improves\nthe performance on tasks requiring understanding of order and compositionality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuksekgonul_M/0/1/0/all/0/1\">Mert Yuksekgonul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalluri_P/0/1/0/all/0/1\">Pratyusha Kalluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mathematically Modeling the Lexicon Entropy of Emergent Language. (arXiv:2211.15783v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.15783","description":"<p>We formulate a stochastic process, FiLex, as a mathematical model of lexicon\nentropy in deep learning-based emergent language systems. Defining a model\nmathematically allows it to generate clear predictions which can be directly\nand decisively tested. We empirically verify across four different environments\nthat FiLex predicts the correct correlation between hyperparameters (training\nsteps, lexicon size, learning rate, rollout buffer size, and Gumbel-Softmax\ntemperature) and the emergent language's entropy in 20 out of 20\nenvironment-hyperparameter combinations. Furthermore, our experiments reveal\nthat different environments show diverse relationships between their\nhyperparameters and entropy which demonstrates the need for a model which can\nmake well-defined predictions at a precise level of granularity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boldt_B/0/1/0/all/0/1\">Brendon Boldt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortensen_D/0/1/0/all/0/1\">David Mortensen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-world Story Generation with Structured Knowledge Enhancement: A Comprehensive Survey. (arXiv:2212.04634v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.04634","description":"<p>Storytelling and narrative are fundamental to human experience, intertwined\nwith our social and cultural engagement. As such, researchers have long\nattempted to create systems that can generate stories automatically. In recent\nyears, powered by deep learning and massive data resources, automatic story\ngeneration has shown significant advances. However, considerable challenges,\nlike the need for global coherence in generated stories, still hamper\ngenerative models from reaching the same storytelling ability as human\nnarrators. To tackle these challenges, many studies seek to inject structured\nknowledge into the generation process, which is referred to as structured\nknowledge-enhanced story generation. Incorporating external knowledge can\nenhance the logical coherence among story events, achieve better knowledge\ngrounding, and alleviate over-generalization and repetition problems in\nstories. This survey provides the latest and comprehensive review of this\nresearch field: (i) we present a systematical taxonomy regarding how existing\nmethods integrate structured knowledge into story generation; (ii) we summarize\ninvolved story corpora, structured knowledge datasets, and evaluation metrics;\n(iii) we give multidimensional insights into the challenges of\nknowledge-enhanced story generation and cast light on promising directions for\nfuture study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jieru Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_B/0/1/0/all/0/1\">B&#xf6;rje F. Karlsson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"POTATO: The Portable Text Annotation Tool. (arXiv:2212.08620v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08620","description":"<p>We present POTATO, the Portable text annotation tool, a free, fully\nopen-sourced annotation system that 1) supports labeling many types of text and\nmultimodal data; 2) offers easy-to-configure features to maximize the\nproductivity of both deployers and annotators (convenient templates for common\nML/NLP tasks, active learning, keypress shortcuts, keyword highlights,\ntooltips); and 3) supports a high degree of customization (editable UI,\ninserting pre-screening questions, attention and qualification tests).\nExperiments over two annotation tasks suggest that POTATO improves labeling\nspeed through its specially-designed productivity features, especially for long\ndocuments and complex tasks. POTATO is available at\nhttps://github.com/davidjurgens/potato and will continue to be updated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jiaxin Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananthasubramaniam_A/0/1/0/all/0/1\">Aparna Ananthasubramaniam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_N/0/1/0/all/0/1\">Naitian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sargent_J/0/1/0/all/0/1\">Jackson Sargent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dedeloudis_A/0/1/0/all/0/1\">Apostolos Dedeloudis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent Analogical Reasoning in Large Language Models. (arXiv:2212.09196v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2212.09196","description":"<p>The recent advent of large language models has reinvigorated debate over\nwhether human cognitive capacities might emerge in such generic models given\nsufficient training data. Of particular interest is the ability of these models\nto reason about novel problems zero-shot, without any direct training. In human\ncognition, this capacity is closely tied to an ability to reason by analogy.\nHere, we performed a direct comparison between human reasoners and a large\nlanguage model (the text-davinci-003 variant of GPT-3) on a range of analogical\ntasks, including a novel text-based matrix reasoning task closely modeled on\nRaven's Progressive Matrices. We found that GPT-3 displayed a surprisingly\nstrong capacity for abstract pattern induction, matching or even surpassing\nhuman capabilities in most settings. Our results indicate that large language\nmodels such as GPT-3 have acquired an emergent ability to find zero-shot\nsolutions to a broad range of analogy problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Webb_T/0/1/0/all/0/1\">Taylor Webb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holyoak_K/0/1/0/all/0/1\">Keith J. Holyoak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongjing Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.10410","description":"<p>Cross-domain NER is a challenging task to address the low-resource problem in\npractical scenarios. Previous typical solutions mainly obtain a NER model by\npre-trained language models (PLMs) with data from a rich-resource domain and\nadapt it to the target domain. Owing to the mismatch issue among entity types\nin different domains, previous approaches normally tune all parameters of PLMs,\nending up with an entirely new NER model for each domain. Moreover, current\nmodels only focus on leveraging knowledge in one general source domain while\nfailing to successfully transfer knowledge from multiple sources to the target.\nTo address these issues, we introduce Collaborative Domain-Prefix Tuning for\ncross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically,\nwe present text-to-text generation grounding domain-related instructors to\ntransfer knowledge to new domain NER tasks without structural modifications. We\nutilize frozen PLMs and conduct collaborative domain-prefix tuning to stimulate\nthe potential of PLMs to handle NER tasks across various domains. Experimental\nresults on the Cross-NER benchmark show that the proposed approach has flexible\ntransfer ability and performs better on both one-source and multiple-source\ncross-domain NER tasks. Codes will be available in\nhttps://github.com/zjunlp/DeepKE/tree/main/example/ner/cross.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1\">Shuofei Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fillers in Spoken Language Understanding: Computational and Psycholinguistic Perspectives. (arXiv:2301.10761v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.10761","description":"<p>Disfluencies (i.e. interruptions in the regular flow of speech), are\nubiquitous to spoken discourse. Fillers (\"uh\", \"um\") are disfluencies that\noccur the most frequently compared to other kinds of disfluencies. Yet, to the\nbest of our knowledge, there isn't a resource that brings together the research\nperspectives influencing Spoken Language Understanding (SLU) on these speech\nevents. This aim of this article is to survey a breadth of perspectives in a\nholistic way; i.e. from considering underlying (psycho)linguistic theory, to\ntheir annotation and consideration in Automatic Speech Recognition (ASR) and\nSLU systems, to lastly, their study from a generation standpoint. This article\naims to present the perspectives in an approachable way to the SLU and\nConversational AI community, and discuss moving forward, what we believe are\nthe trends and challenges in each area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dinkar_T/0/1/0/all/0/1\">Tanvi Dinkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1\">Chlo&#xe9; Clavel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilescu_I/0/1/0/all/0/1\">Ioana Vasilescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4. (arXiv:2303.12712v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.12712","description":"<p>Artificial intelligence (AI) researchers have been developing and refining\nlarge language models (LLMs) that exhibit remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. The latest model developed by OpenAI, GPT-4, was trained using an\nunprecedented scale of compute and data. In this paper, we report on our\ninvestigation of an early version of GPT-4, when it was still in active\ndevelopment by OpenAI. We contend that (this early version of) GPT-4 is part of\na new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that\nexhibit more general intelligence than previous AI models. We discuss the\nrising capabilities and implications of these models. We demonstrate that,\nbeyond its mastery of language, GPT-4 can solve novel and difficult tasks that\nspan mathematics, coding, vision, medicine, law, psychology and more, without\nneeding any special prompting. Moreover, in all of these tasks, GPT-4's\nperformance is strikingly close to human-level performance, and often vastly\nsurpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's\ncapabilities, we believe that it could reasonably be viewed as an early (yet\nstill incomplete) version of an artificial general intelligence (AGI) system.\nIn our exploration of GPT-4, we put special emphasis on discovering its\nlimitations, and we discuss the challenges ahead for advancing towards deeper\nand more comprehensive versions of AGI, including the possible need for\npursuing a new paradigm that moves beyond next-word prediction. We conclude\nwith reflections on societal influences of the recent technological leap and\nfuture research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1\">S&#xe9;bastien Bubeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_V/0/1/0/all/0/1\">Varun Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eldan_R/0/1/0/all/0/1\">Ronen Eldan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrke_J/0/1/0/all/0/1\">Johannes Gehrke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamar_E/0/1/0/all/0/1\">Ece Kamar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1\">Peter Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yin Tat Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundberg_S/0/1/0/all/0/1\">Scott Lundberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nori_H/0/1/0/all/0/1\">Harsha Nori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1\">Hamid Palangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Marco Tulio Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph Question Answering. (arXiv:2303.13284v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.13284","description":"<p>In this work, we present an end-to-end Knowledge Graph Question Answering\n(KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text\npre-trained language model. The model takes a question in natural language as\ninput and produces a simpler form of the intended SPARQL query. In the simpler\nform, the model does not directly produce entity and relation IDs. Instead, it\nproduces corresponding entity and relation labels. The labels are grounded to\nKG entity and relation IDs in a subsequent step. To further improve the\nresults, we instruct the model to produce a truncated version of the KG\nembedding for each entity. The truncated KG embedding enables a finer search\nfor disambiguation purposes. We find that T5 is able to learn the truncated KG\nembeddings without any change of loss function, improving KGQA performance. As\na result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata\ndatasets on end-to-end KGQA over Wikidata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1\">Debayan Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_P/0/1/0/all/0/1\">Pranav Ajit Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usbeck_R/0/1/0/all/0/1\">Ricardo Usbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biemann_C/0/1/0/all/0/1\">Chris Biemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-03-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/"}}]}]}