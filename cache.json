{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-01-27T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering. (arXiv:2301.10799v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10799","description":"<p>Providing explanations for visual question answering (VQA) has gained much\nattention in research. However, most existing systems use separate models for\npredicting answers and providing explanations. We argue that training\nexplanation models independently of the QA model makes the explanations less\ngrounded and limits performance. To address this, we propose a multitask\nlearning approach towards a Unified Model for more grounded and consistent\ngeneration of both Answers and Explanations (UMAE). To achieve this, we add\nartificial prompt tokens to training instances and finetune a multimodal\nencoder-decoder model on various VQA tasks. In our experiments, UMAE models\nsurpass the prior SOTA answer accuracy on A-OKVQA by 10~15%, show competitive\nresults on OK-VQA, achieve new SOTA explanation scores on A-OKVQA and VCR, and\ndemonstrate promising out-of-domain performance on VQA-X.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Whitehouse_C/0/1/0/all/0/1\">Chenxi Whitehouse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weyde_T/0/1/0/all/0/1\">Tillman Weyde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhyastha_P/0/1/0/all/0/1\">Pranava Madhyastha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the inconsistency of separable losses for structured prediction. (arXiv:2301.10810v1 [cs.LG])","link":"http://arxiv.org/abs/2301.10810","description":"<p>In this paper, we prove that separable negative log-likelihood losses for\nstructured prediction are not necessarily Bayes consistent, or, in other words,\nminimizing these losses may not result in a model that predicts the most\nprobable structure in the data distribution for a given input. This fact opens\nthe question of whether these losses are well-adapted for structured prediction\nand, if so, why.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Corro_C/0/1/0/all/0/1\">Caio Corro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram. (arXiv:2301.10856v1 [cs.CY])","link":"http://arxiv.org/abs/2301.10856","description":"<p>In response to disinformation and propaganda from Russian online media\nfollowing the Russian invasion of Ukraine, Russian outlets including Russia\nToday and Sputnik News were banned throughout Europe. Many of these Russian\noutlets, in order to reach their audiences, began to heavily promote their\ncontent on messaging services like Telegram. In this work, to understand this\nphenomenon, we study how 16 Russian media outlets have interacted with and\nutilized 732 Telegram channels throughout 2022. To do this, we utilize a\nmultilingual version of the foundational model MPNet to embed articles and\nTelegram messages in a shared embedding space and semantically compare content.\nLeveraging a parallelized version of DP-Means clustering, we perform\nparagraph-level topic/narrative extraction and time-series analysis with Hawkes\nProcesses. With this approach, across our websites, we find between 2.3%\n(ura.news) and 26.7% (ukraina.ru) of their content originated/resulted from\nactivity on Telegram. Finally, tracking the spread of individual narratives, we\nmeasure the rate at which these websites and channels disseminate content\nwithin the Russian media ecosystem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hanley_H/0/1/0/all/0/1\">Hans W. A. Hanley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durumeric_Z/0/1/0/all/0/1\">Zakir Durumeric</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Qualitative Analysis of a Graph Transformer Approach to Addressing Hate Speech: Adapting to Dynamically Changing Content. (arXiv:2301.10871v1 [cs.LG])","link":"http://arxiv.org/abs/2301.10871","description":"<p>Our work advances an approach for predicting hate speech in social media,\ndrawing out the critical need to consider the discussions that follow a post to\nsuccessfully detect when hateful discourse may arise. Using graph transformer\nnetworks, coupled with modelling attention and BERT-level natural language\nprocessing, our approach can capture context and anticipate upcoming\nanti-social behaviour. In this paper, we offer a detailed qualitative analysis\nof this solution for hate speech detection in social networks, leading to\ninsights into where the method has the most impressive outcomes in comparison\nwith competitors and identifying scenarios where there are challenges to\nachieving ideal performance. Included is an exploration of the kinds of posts\nthat permeate social media today, including the use of hateful images. This\nsuggests avenues for extending our model to be more comprehensive. A key\ninsight is that the focus on reasoning about the concept of context positions\nus well to be able to support multi-modal analysis of online posts. We conclude\nwith a reflection on how the problem we are addressing relates especially well\nto the theme of dynamic change, a critical concern for all AI solutions for\nsocial impact. We also comment briefly on how mental health well-being can be\nadvanced with our work, through curated content attuned to the extent of hate\nin posts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hebert_L/0/1/0/all/0/1\">Liam Hebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Yi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_R/0/1/0/all/0/1\">Robin Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golab_L/0/1/0/all/0/1\">Lukasz Golab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Break It Down: Evidence for Structural Compositionality in Neural Networks. (arXiv:2301.10884v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10884","description":"<p>Many tasks can be described as compositions over subroutines. Though modern\nneural networks have achieved impressive performance on both vision and\nlanguage tasks, we know little about the functions that they implement. One\npossibility is that neural networks implicitly break down complex tasks into\nsubroutines, implement modular solutions to these subroutines, and compose them\ninto an overall solution to a task -- a property we term structural\ncompositionality. Or they may simply learn to match new inputs to memorized\nrepresentations, eliding task decomposition entirely. Here, we leverage model\npruning techniques to investigate this question in both vision and language,\nacross a variety of architectures, tasks, and pretraining regimens. Our results\ndemonstrate that models oftentimes implement solutions to subroutines via\nmodular subnetworks, which can be ablated while maintaining the functionality\nof other subroutines. This suggests that neural networks may be able to learn\nto exhibit compositionality, obviating the need for specialized symbolic\nmechanisms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lepori_M/0/1/0/all/0/1\">Michael A. Lepori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1\">Thomas Serre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Text-based Early Prediction by Distillation from Privileged Time-Series Text. (arXiv:2301.10887v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10887","description":"<p>Modeling text-based time-series to make prediction about a future event or\noutcome is an important task with a wide range of applications. The standard\napproach is to train and test the model using the same input window, but this\napproach neglects the data collected in longer input windows between the\nprediction time and the final outcome, which are often available during\ntraining. In this study, we propose to treat this neglected text as privileged\ninformation available during training to enhance early prediction modeling\nthrough knowledge distillation, presented as Learning using Privileged\ntIme-sEries Text (LuPIET). We evaluate the method on clinical and social media\ntext, with four clinical prediction tasks based on clinical notes and two\nmental health prediction tasks based on social media posts. Our results show\nLuPIET is effective in enhancing text-based early predictions, though one may\nneed to consider choosing the appropriate text representation and windows for\nprivileged text to achieve optimal performance. Compared to two other methods\nusing transfer learning and mixed training, LuPIET offers more stable\nimprovements over the baseline, standard training. As far as we are concerned,\nthis is the first study to examine learning using privileged information for\ntime-series in the NLP context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capurro_D/0/1/0/all/0/1\">Daniel Capurro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anthony Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verspoor_K/0/1/0/all/0/1\">Karin Verspoor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Reasoning of Entities and Events in Procedural Texts. (arXiv:2301.10896v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10896","description":"<p>Entities and events have long been regarded as the crux of machine reasoning.\nSpecifically, procedural texts have received increasing attention due to the\ndynamic nature of involved entities and events. Existing work has exclusively\nfocused on entity state tracking (e.g., the temperature of a pan) or\ncounterfactual event reasoning (e.g., how likely am I to burn myself by\ntouching the pan), while these two tasks are tightly intertwined. In this work,\nwe propose CREPE, the first benchmark on causal reasoning about event\nplausibility based on entity states. We experiment with strong large language\nmodels and show that most models including GPT3 perform close to chance of .30\nF1, lagging far behind the human performance of .87 F1. Inspired by the finding\nthat structured representations such as programming languages benefits event\nreasoning as a prompt to code language models such as Codex, we creatively\ninject the causal relations between entities and events through intermediate\nvariables and boost the performance to .67 to .72 F1. Our proposed event\nrepresentation not only allows for knowledge injection, but also marks the\nfirst successful attempt of chain-of-thought reasoning with code language\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hainiu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_W/0/1/0/all/0/1\">Weiqiu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_M/0/1/0/all/0/1\">Manni Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning. (arXiv:2301.10915v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10915","description":"<p>Dialogue state tracking (DST) is an important step in dialogue management to\nkeep track of users' beliefs. Existing works fine-tune all language model (LM)\nparameters to tackle the DST task, which requires significant data and\ncomputing resources for training and hosting. The cost grows exponentially in\nthe real-world deployment where dozens of fine-tuned LM are used for different\ndomains and tasks. To reduce parameter size and better utilize cross-task\nshared information, we propose to use soft prompt token embeddings to learn\ntask properties. Without tuning LM parameters, our method drastically reduces\nthe number of parameters needed to less than 0.5% of prior works while achieves\nbetter low-resource DST performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingyu Derek Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_J/0/1/0/all/0/1\">Jiun-Yu Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shuyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Arpit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1\">Tagyoung Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Affective Faces for Goal-Driven Dyadic Communication. (arXiv:2301.10939v1 [cs.CV])","link":"http://arxiv.org/abs/2301.10939","description":"<p>We introduce a video framework for modeling the association between verbal\nand non-verbal communication during dyadic conversation. Given the input speech\nof a speaker, our approach retrieves a video of a listener, who has facial\nexpressions that would be socially appropriate given the context. Our approach\nfurther allows the listener to be conditioned on their own goals,\npersonalities, or backgrounds. Our approach models conversations through a\ncomposition of large language models and vision-language models, creating\ninternal representations that are interpretable and controllable. To study\nmultimodal communication, we propose a new video dataset of unscripted\nconversations covering diverse topics and demographics. Experiments and\nvisualizations show our approach is able to output listeners that are\nsignificantly more socially appropriate than baselines. However, many\nchallenges remain, and we release our dataset publicly to spur further\nprogress. See our website for video results, data, and code:\nhttps://realtalk.cs.columbia.edu.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Scott Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teotia_R/0/1/0/all/0/1\">Revant Teotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tendulkar_P/0/1/0/all/0/1\">Purva Tendulkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_S/0/1/0/all/0/1\">Sachit Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1\">Carl Vondrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross Modal Global Local Representation Learning from Radiology Reports and X-Ray Chest Images. (arXiv:2301.10951v1 [cs.CV])","link":"http://arxiv.org/abs/2301.10951","description":"<p>Deep learning models can be applied successfully in real-work problems;\nhowever, training most of these models requires massive data. Recent methods\nuse language and vision, but unfortunately, they rely on datasets that are not\nusually publicly available. Here we pave the way for further research in the\nmultimodal language-vision domain for radiology. In this paper, we train a\nrepresentation learning method that uses local and global representations of\nthe language and vision through an attention mechanism and based on the\npublicly available Indiana University Radiology Report (IU-RR) dataset.\nFurthermore, we use the learned representations to diagnose five lung\npathologies: atelectasis, cardiomegaly, edema, pleural effusion, and\nconsolidation. Finally, we use both supervised and zero-shot classifications to\nextensively analyze the performance of the representation learning on the IU-RR\ndataset. Average Area Under the Curve (AUC) is used to evaluate the accuracy of\nthe classifiers for classifying the five lung pathologies. The average AUC for\nclassifying the five lung pathologies on the IU-RR test set ranged from 0.85 to\n0.87 using the different training datasets, namely CheXpert and CheXphoto.\nThese results compare favorably to other studies using UI-RR. Extensive\nexperiments confirm consistent results for classifying lung pathologies using\nthe multimodal global local representations of language and vision information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hadjiyski_N/0/1/0/all/0/1\">Nathan Hadjiyski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_A/0/1/0/all/0/1\">Ali Vosoughi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wismueller_A/0/1/0/all/0/1\">Axel Wismueller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Dynamic Focused Topic Model. (arXiv:2301.10988v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10988","description":"<p>Topic models and all their variants analyse text by learning meaningful\nrepresentations through word co-occurrences. As pointed out by Williamson et\nal. (2010), such models implicitly assume that the probability of a topic to be\nactive and its proportion within each document are positively correlated. This\ncorrelation can be strongly detrimental in the case of documents created over\ntime, simply because recent documents are likely better described by new and\nhence rare topics. In this work we leverage recent advances in neural\nvariational inference and present an alternative neural approach to the dynamic\nFocused Topic Model. Indeed, we develop a neural model for topic evolution\nwhich exploits sequences of Bernoulli random variables in order to track the\nappearances of topics, thereby decoupling their activities from their\nproportions. We evaluate our model on three different datasets (the UN general\ndebates, the collection of NeurIPS papers, and the ACL Anthology dataset) and\nshow that it (i) outperforms state-of-the-art topic models in generalization\ntasks and (ii) performs comparably to them on prediction tasks, while employing\nroughly the same number of parameters, and converging about two times faster.\nSource code to reproduce our experiments is available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cvejoski_K/0/1/0/all/0/1\">Kostadin Cvejoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_R/0/1/0/all/0/1\">Rams&#xe9;s J. S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojeda_C/0/1/0/all/0/1\">C&#xe9;sar Ojeda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLP as a Lens for Causal Analysis and Perception Mining to Infer Mental Health on Social Media. (arXiv:2301.11004v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11004","description":"<p>Interactions among humans on social media often convey intentions behind\ntheir actions, yielding a psychological language resource for Mental Health\nAnalysis (MHA) of online users. The success of Computational Intelligence\nTechniques (CIT) for inferring mental illness from such social media resources\npoints to NLP as a lens for causal analysis and perception mining. However, we\nargue that more consequential and explainable research is required for optimal\nimpact on clinical psychology practice and personalized mental healthcare. To\nbridge this gap, we posit two significant dimensions: (1) Causal analysis to\nillustrate a cause and effect relationship in the user generated text; (2)\nPerception mining to infer psychological perspectives of social effects on\nonline users intentions. Within the scope of Natural Language Processing (NLP),\nwe further explore critical areas of inquiry associated with these two\ndimensions, specifically through recent advancements in discourse analysis.\nThis position paper guides the community to explore solutions in this space and\nadvance the state of practice in developing conversational agents for inferring\nmental health from social media. We advocate for a more explainable approach\ntoward modeling computational psychology problems through the lens of language\nas we observe an increased number of research contributions in dataset and\nproblem formulation for causal relation extraction and perception enhancements\nwhile inferring mental states.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1\">Muskan Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_C/0/1/0/all/0/1\">Chandni Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naseem_U/0/1/0/all/0/1\">Usman Naseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorr_B/0/1/0/all/0/1\">Bonnie J Dorr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paraphrase Acquisition from Image Captions. (arXiv:2301.11030v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11030","description":"<p>We propose to use captions from the Web as a previously underutilized\nresource for paraphrases (i.e., texts with the same \"message\") and to create\nand analyze a corresponding dataset. When an image is reused on the Web, an\noriginal caption is often assigned. We hypothesize that different captions for\nthe same image naturally form a set of mutual paraphrases. To demonstrate the\nsuitability of this idea, we analyze captions in the English Wikipedia, where\neditors frequently relabel the same image for different articles. The paper\nintroduces the underlying mining technology and compares known paraphrase\ncorpora with respect to their syntactic and semantic paraphrase similarity to\nour new resource. In this context, we introduce characteristic maps along the\ntwo similarity dimensions to identify the style of paraphrases coming from\ndifferent sources. An annotation study demonstrates the high reliability of the\nalgorithmically determined characteristic maps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gohsen_M/0/1/0/all/0/1\">Marcel Gohsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagen_M/0/1/0/all/0/1\">Matthias Hagen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1\">Benno Stein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A benchmark for toxic comment classification on Civil Comments dataset. (arXiv:2301.11125v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11125","description":"<p>Toxic comment detection on social media has proven to be essential for\ncontent moderation. This paper compares a wide set of different models on a\nhighly skewed multi-label hate speech dataset. We consider inference time and\nseveral metrics to measure performance and bias in our comparison. We show that\nall BERTs have similar performance regardless of the size, optimizations or\nlanguage used to pre-train the models. RNNs are much faster at inference than\nany of the BERT. BiLSTM remains a good compromise between performance and\ninference time. RoBERTa with Focal Loss offers the best performance on biases\nand AUROC. However, DistilBERT combines both good AUROC and a low inference\ntime. All models are affected by the bias of associating identities. BERT, RNN,\nand XLNet are less sensitive than the CNN and Compact Convolutional\nTransformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duchene_C/0/1/0/all/0/1\">Corentin Duchene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamet_H/0/1/0/all/0/1\">Henri Jamet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guillaume_P/0/1/0/all/0/1\">Pierre Guillaume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehak_R/0/1/0/all/0/1\">Reda Dehak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Image Captioning by Adversarially Propagating Labeled Data. (arXiv:2301.11174v1 [cs.CV])","link":"http://arxiv.org/abs/2301.11174","description":"<p>We present a novel data-efficient semi-supervised framework to improve the\ngeneralization of image captioning models. Constructing a large-scale labeled\nimage captioning dataset is an expensive task in terms of labor, time, and\ncost. In contrast to manually annotating all the training samples, separately\ncollecting uni-modal datasets is immensely easier, e.g., a large-scale image\ndataset and a sentence dataset. We leverage such massive unpaired image and\ncaption data upon standard paired data by learning to associate them. To this\nend, our proposed semi-supervised learning method assigns pseudo-labels to\nunpaired samples in an adversarial learning fashion, where the joint\ndistribution of image and caption is learned. Our method trains a captioner to\nlearn from a paired data and to progressively associate unpaired data. This\napproach shows noticeable performance improvement even in challenging scenarios\nincluding out-of-task data (i.e., relational captioning, where the target task\nis different from the unpaired data) and web-crawled data. We also show that\nour proposed method is theoretically well-motivated and has a favorable global\noptimal property. Our extensive and comprehensive empirical results both on (1)\nimage-based and (2) dense region-based captioning datasets followed by\ncomprehensive analysis on the scarcely-paired COCO dataset demonstrate the\nconsistent effectiveness of our semisupervised learning method with unpaired\ndata compared to competing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dong-Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1\">Tae-Hyun Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinsoo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing the Entities in Harmful Memes: Who is the Hero, the Villain, the Victim?. (arXiv:2301.11219v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11219","description":"<p>Memes can sway people's opinions over social media as they combine visual and\ntextual information in an easy-to-consume manner. Since memes instantly turn\nviral, it becomes crucial to infer their intent and potentially associated\nharmfulness to take timely measures as needed. A common problem associated with\nmeme comprehension lies in detecting the entities referenced and characterizing\nthe role of each of these entities. Here, we aim to understand whether the meme\nglorifies, vilifies, or victimizes each entity it refers to. To this end, we\naddress the task of role identification of entities in harmful memes, i.e.,\ndetecting who is the 'hero', the 'villain', and the 'victim' in the meme, if\nany. We utilize HVVMemes - a memes dataset on US Politics and Covid-19 memes,\nreleased recently as part of the CONSTRAINT@ACL-2022 shared-task. It contains\nmemes, entities referenced, and their associated roles: hero, villain, victim,\nand other. We further design VECTOR (Visual-semantic role dEteCToR), a robust\nmulti-modal framework for the task, which integrates entity-based contextual\ninformation in the multi-modal representation and compare it to several\nstandard unimodal (text-only or image-only) or multi-modal (image+text) models.\nOur experimental results show that our proposed model achieves an improvement\nof 4% over the best baseline and 1% over the best competing stand-alone\nsubmission from the shared-task. Besides divulging an extensive experimental\nsetup with comparative analyses, we finally highlight the challenges\nencountered in addressing the complex task of semantic role labeling within\nmemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shivam Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Atharva Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_T/0/1/0/all/0/1\">Tharun Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_H/0/1/0/all/0/1\">Himanshi Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md. Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Molecular Language Model as Multi-task Generator. (arXiv:2301.11259v1 [cs.LG])","link":"http://arxiv.org/abs/2301.11259","description":"<p>Molecule generation with desired properties has grown immensely in popularity\nby disruptively changing the way scientists design molecular structures and\nproviding support for chemical and materials design. However, despite the\npromising outcome, previous machine learning-based deep generative models\nsuffer from a reliance on complex, task-specific fine-tuning, limited\ndimensional latent spaces, or the quality of expert rules. In this work, we\npropose MolGen, a pre-trained molecular language model that effectively learns\nand shares knowledge across multiple generation tasks and domains.\nSpecifically, we pre-train MolGen with the chemical language SELFIES on more\nthan 100 million unlabelled molecules. We further propose multi-task molecular\nprefix tuning across several molecular generation tasks and different molecular\ndomains (synthetic &amp; natural products) with a self-feedback mechanism.\nExtensive experiments show that MolGen can obtain superior performances on\nwell-known molecular generation benchmark datasets. The further analysis\nillustrates that MolGen can accurately capture the distribution of molecules,\nimplicitly learn their structural characteristics, and efficiently explore the\nchemical space with the guidance of multi-task molecular prefix tuning. Codes,\ndatasets, and the pre-trained model will be available in\nhttps://github.com/zjunlp/MolGen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiaohui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BayesSpeech: A Bayesian Transformer Network for Automatic Speech Recognition. (arXiv:2301.11276v1 [eess.AS])","link":"http://arxiv.org/abs/2301.11276","description":"<p>Recent developments using End-to-End Deep Learning models have been shown to\nhave near or better performance than state of the art Recurrent Neural Networks\n(RNNs) on Automatic Speech Recognition tasks. These models tend to be lighter\nweight and require less training time than traditional RNN-based approaches.\nHowever, these models take frequentist approach to weight training. In theory,\nnetwork weights are drawn from a latent, intractable probability distribution.\nWe introduce BayesSpeech for end-to-end Automatic Speech Recognition.\nBayesSpeech is a Bayesian Transformer Network where these intractable\nposteriors are learned through variational inference and the local\nreparameterization trick without recurrence. We show how the introduction of\nvariance in the weights leads to faster training time and near state-of-the-art\nperformance on LibriSpeech-960.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rieger_W/0/1/0/all/0/1\">Will Rieger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Finetuning for Factual Knowledge Extraction from Language Models. (arXiv:2301.11293v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11293","description":"<p>Language models (LMs) pretrained on large corpora of text from the web have\nbeen observed to contain large amounts of various types of knowledge about the\nworld. This observation has led to a new and exciting paradigm in knowledge\ngraph construction where, instead of manual curation or text mining, one\nextracts knowledge from the parameters of an LM. Recently, it has been shown\nthat finetuning LMs on a set of factual knowledge makes them produce better\nanswers to queries from a different set, thus making finetuned LMs a good\ncandidate for knowledge extraction and, consequently, knowledge graph\nconstruction. In this paper, we analyze finetuned LMs for factual knowledge\nextraction. We show that along with its previously known positive effects,\nfinetuning also leads to a (potentially harmful) phenomenon which we call\nFrequency Shock, where at the test time the model over-predicts rare entities\nthat appear in the training set and under-predicts common entities that do not\nappear in the training set enough times. We show that Frequency Shock leads to\na degradation in the predictions of the model and beyond a point, the harm from\nFrequency Shock can even outweigh the positive effects of finetuning, making\nfinetuning harmful overall. We then consider two solutions to remedy the\nidentified negative effect: 1- model mixing and 2- mixture finetuning with the\nLM's pre-training task. The two solutions combined lead to significant\nimprovements compared to vanilla finetuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kazemi_M/0/1/0/all/0/1\">Mehran Kazemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1\">Sid Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_D/0/1/0/all/0/1\">Deepak Ramachandran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature. (arXiv:2301.11305v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11305","description":"<p>The fluency and factual knowledge of large language models (LLMs) heightens\nthe need for corresponding systems to detect whether a piece of text is\nmachine-written. For example, students may use LLMs to complete written\nassignments, leaving instructors unable to accurately assess student learning.\nIn this paper, we first demonstrate that text sampled from an LLM tends to\noccupy negative curvature regions of the model's log probability function.\nLeveraging this observation, we then define a new curvature-based criterion for\njudging if a passage is generated from a given LLM. This approach, which we\ncall DetectGPT, does not require training a separate classifier, collecting a\ndataset of real or generated passages, or explicitly watermarking generated\ntext. It uses only log probabilities computed by the model of interest and\nrandom perturbations of the passage from another generic pre-trained language\nmodel (e.g, T5). We find DetectGPT is more discriminative than existing\nzero-shot methods for model sample detection, notably improving detection of\nfake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the\nstrongest zero-shot baseline to 0.95 AUROC for DetectGPT. See\nhttps://ericmitchell.ai/detectgpt for code, data, and other project\ninformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_E/0/1/0/all/0/1\">Eric Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yoonho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khazatsky_A/0/1/0/all/0/1\">Alexander Khazatsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemSup-XC: Semantic Supervision for Zero and Few-shot Extreme Classification. (arXiv:2301.11309v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11309","description":"<p>Extreme classification (XC) involves predicting over large numbers of classes\n(thousands to millions), with real-world applications like news article\nclassification and e-commerce product tagging. The zero-shot version of this\ntask requires generalization to novel classes without additional supervision.\nIn this paper, we develop SemSup-XC, a model that achieves state-of-the-art\nzero-shot and few-shot performance on three XC datasets derived from legal,\ne-commerce, and Wikipedia data. To develop SemSup-XC, we use automatically\ncollected semantic class descriptions to represent classes and facilitate\ngeneralization through a novel hybrid matching module that matches input\ninstances to class descriptions using a combination of semantic and lexical\nsimilarity. Trained with contrastive learning, SemSup-XC significantly\noutperforms baselines and establishes state-of-the-art performance on all three\ndatasets considered, gaining up to 12 precision points on zero-shot and more\nthan 10 precision points on one-shot tests, with similar gains for recall@10.\nOur ablation studies highlight the relative importance of our hybrid matching\nmodule and automatically collected class descriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_P/0/1/0/all/0/1\">Pranjal Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1\">Ameet Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LoRaLay: A Multilingual and Multimodal Dataset for Long Range and Layout-Aware Summarization. (arXiv:2301.11312v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11312","description":"<p>Text Summarization is a popular task and an active area of research for the\nNatural Language Processing community. By definition, it requires to account\nfor long input texts, a characteristic which poses computational challenges for\nneural models. Moreover, real-world documents come in a variety of complex,\nvisually-rich, layouts. This information is of great relevance, whether to\nhighlight salient content or to encode long-range interactions between textual\npassages. Yet, all publicly available summarization datasets only provide plain\ntext content. To facilitate research on how to exploit visual/layout\ninformation to better capture long-range dependencies in summarization models,\nwe present LoRaLay, a collection of datasets for long-range summarization with\naccompanying visual/layout information. We extend existing and popular English\ndatasets (arXiv and PubMed) with layout information and propose four novel\ndatasets -- consistently built from scholar resources -- covering French,\nSpanish, Portuguese, and Korean languages. Further, we propose new baselines\nmerging layout-aware and long-range models -- two orthogonal approaches -- and\nobtain state-of-the-art results, showing the importance of combining both lines\nof research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Laura Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piwowarski_B/0/1/0/all/0/1\">Benjamin Piwowarski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staiano_J/0/1/0/all/0/1\">Jacopo Staiano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextualizing Emerging Trends in Financial News Articles. (arXiv:2301.11318v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11318","description":"<p>Identifying and exploring emerging trends in the news is becoming more\nessential than ever with many changes occurring worldwide due to the global\nhealth crises. However, most of the recent research has focused mainly on\ndetecting trends in social media, thus, benefiting from social features (e.g.\nlikes and retweets on Twitter) which helped the task as they can be used to\nmeasure the engagement and diffusion rate of content. Yet, formal text data,\nunlike short social media posts, comes with a longer, less restricted writing\nformat, and thus, more challenging. In this paper, we focus our study on\nemerging trends detection in financial news articles about Microsoft, collected\nbefore and during the start of the COVID-19 pandemic (July 2019 to July 2020).\nWe make the dataset accessible and propose a strong baseline (Contextual\nLeap2Trend) for exploring the dynamics of similarities between pairs of\nkeywords based on topic modelling and term frequency. Finally, we evaluate\nagainst a gold standard (Google Trends) and present noteworthy real-world\nscenarios regarding the influence of the pandemic on Microsoft.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Nhu Khoa Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delahaut_T/0/1/0/all/0/1\">Thierry Delahaut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boros_E/0/1/0/all/0/1\">Emanuela Boros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1\">Antoine Doucet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lejeune_G/0/1/0/all/0/1\">Ga&#xeb;l Lejeune</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Automated Construction of Food Composition Knowledge Base. (arXiv:2301.11322v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11322","description":"<p>A food composition knowledge base, which stores the essential phyto-, micro-,\nand macro-nutrients of foods is useful for both research and industrial\napplications. Although many existing knowledge bases attempt to curate such\ninformation, they are often limited by time-consuming manual curation\nprocesses. Outside of the food science domain, natural language processing\nmethods that utilize pre-trained language models have recently shown promising\nresults for extracting knowledge from unstructured text. In this work, we\npropose a semi-automated framework for constructing a knowledge base of food\ncomposition from the scientific literature available online. To this end, we\nutilize a pre-trained BioBERT language model in an active learning setup that\nallows the optimal use of limited training data. Our work demonstrates how\nhuman-in-the-loop models are a step toward AI-assisted food systems that scale\nwell to the ever-increasing big data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Youn_J/0/1/0/all/0/1\">Jason Youn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fangzhou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagkopoulos_I/0/1/0/all/0/1\">Ilias Tagkopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Marpa, A practical general parser: the recognizer. (arXiv:1910.08129v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1910.08129","description":"<p>The Marpa recognizer is described. Marpa is a practical and fully implemented\nalgorithm for the recognition, parsing and evaluation of context-free grammars.\nThe Marpa recognizer is the first to unite the improvements to Earley's\nalgorithm found in Joop Leo's 1991 paper to those in Aycock and Horspool's 2002\npaper. Marpa tracks the full state of the parse, at it proceeds, in a form\nconvenient for the application. This greatly improves error detection and\nenables event-driven parsing. One such technique is \"Ruby Slippers\" parsing, in\nwhich the input is altered in response to the parser's expectations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kegler_J/0/1/0/all/0/1\">Jeffrey Kegler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dual Prompt Learning Framework for Few-Shot Dialogue State Tracking. (arXiv:2201.05780v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05780","description":"<p>Dialogue state tracking (DST) module is an important component for\ntask-oriented dialog systems to understand users' goals and needs. Collecting\ndialogue state labels including slots and values can be costly, especially with\nthe wide application of dialogue systems in more and more new-rising domains.\nIn this paper, we focus on how to utilize the language understanding and\ngeneration ability of pre-trained language models for DST. We design a dual\nprompt learning framework for few-shot DST. Specifically, we consider the\nlearning of slot generation and value generation as dual tasks, and two prompts\nare designed based on such a dual structure to incorporate task-related\nknowledge of these two tasks respectively. In this way, the DST task can be\nformulated as a language modeling task efficiently under few-shot settings.\nExperimental results on two task-oriented dialogue datasets show that the\nproposed method not only outperforms existing state-of-the-art few-shot\nmethods, but also can generate unseen slots. It indicates that DST-related\nknowledge can be probed from PLM and utilized to address low-resource DST\nefficiently with the help of prompt learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuting Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Pei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jintao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversational Information Seeking. (arXiv:2201.08808v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2201.08808","description":"<p>Conversational information seeking (CIS) is concerned with a sequence of\ninteractions between one or more users and an information system. Interactions\nin CIS are primarily based on natural language dialogue, while they may include\nother types of interactions, such as click, touch, and body gestures. This\nmonograph provides a thorough overview of CIS definitions, applications,\ninteractions, interfaces, design, implementation, and evaluation. This\nmonograph views CIS applications as including conversational search,\nconversational question answering, and conversational recommendation. Our aim\nis to provide an overview of past research related to CIS, introduce the\ncurrent state-of-the-art in CIS, highlight the challenges still being faced in\nthe community. and suggest future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1\">Hamed Zamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trippas_J/0/1/0/all/0/1\">Johanne R. Trippas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalton_J/0/1/0/all/0/1\">Jeff Dalton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radlinski_F/0/1/0/all/0/1\">Filip Radlinski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALBETO and DistilBETO: Lightweight Spanish Language Models. (arXiv:2204.09145v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.09145","description":"<p>In recent years there have been considerable advances in pre-trained language\nmodels, where non-English language versions have also been made available. Due\nto their increasing use, many lightweight versions of these models (with\nreduced parameters) have also been released to speed up training and inference\ntimes. However, versions of these lighter models (e.g., ALBERT, DistilBERT) for\nlanguages other than English are still scarce. In this paper we present ALBETO\nand DistilBETO, which are versions of ALBERT and DistilBERT pre-trained\nexclusively on Spanish corpora. We train several versions of ALBETO ranging\nfrom 5M to 223M parameters and one of DistilBETO with 67M parameters. We\nevaluate our models in the GLUES benchmark that includes various natural\nlanguage understanding tasks in Spanish. The results show that our lightweight\nmodels achieve competitive results to those of BETO (Spanish-BERT) despite\nhaving fewer parameters. More specifically, our larger ALBETO model outperforms\nall other models on the MLDoc, PAWS-X, XNLI, MLQA, SQAC and XQuAD datasets.\nHowever, BETO remains unbeaten for POS and NER. As a further contribution, all\nmodels are publicly available to the community for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Canete_J/0/1/0/all/0/1\">Jos&#xe9; Ca&#xf1;ete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donoso_S/0/1/0/all/0/1\">Sebasti&#xe1;n Donoso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bravo_Marquez_F/0/1/0/all/0/1\">Felipe Bravo-Marquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvallo_A/0/1/0/all/0/1\">Andr&#xe9;s Carvallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_V/0/1/0/all/0/1\">Vladimir Araujo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assistive Recipe Editing through Critiquing. (arXiv:2205.02454v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02454","description":"<p>There has recently been growing interest in the automatic generation of\ncooking recipes that satisfy some form of dietary restrictions, thanks in part\nto the availability of online recipe data. Prior studies have used pre-trained\nlanguage models, or relied on small paired recipe data (e.g., a recipe paired\nwith a similar one that satisfies a dietary constraint). However, pre-trained\nlanguage models generate inconsistent or incoherent recipes, and paired\ndatasets are not available at scale. We address these deficiencies with\nRecipeCrit, a hierarchical denoising auto-encoder that edits recipes given\ningredient-level critiques. The model is trained for recipe completion to learn\nsemantic relationships within recipes. Our work's main innovation is our\nunsupervised critiquing module that allows users to edit recipes by interacting\nwith the predicted ingredients; the system iteratively rewrites recipes to\nsatisfy users' feedback. Experiments on the Recipe1M recipe dataset show that\nour model can more effectively edit recipes compared to strong\nlanguage-modeling baselines, creating recipes that satisfy user constraints and\nare more correct, serendipitous, coherent, and relevant as measured by human\njudges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Textual Explanations and Critiques in Recommendation Systems. (arXiv:2205.07268v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.07268","description":"<p>Artificial intelligence and machine learning algorithms have become\nubiquitous. Although they offer a wide range of benefits, their adoption in\ndecision-critical fields is limited by their lack of interpretability,\nparticularly with textual data. Moreover, with more data available than ever\nbefore, it has become increasingly important to explain automated predictions.\n</p>\n<p>Generally, users find it difficult to understand the underlying computational\nprocesses and interact with the models, especially when the models fail to\ngenerate the outcomes or explanations, or both, correctly. This problem\nhighlights the growing need for users to better understand the models' inner\nworkings and gain control over their actions. This dissertation focuses on two\nfundamental challenges of addressing this need. The first involves explanation\ngeneration: inferring high-quality explanations from text documents in a\nscalable and data-driven manner. The second challenge consists in making\nexplanations actionable, and we refer to it as critiquing. This dissertation\nexamines two important applications in natural language processing and\nrecommendation tasks.\n</p>\n<p>Overall, we demonstrate that interpretability does not come at the cost of\nreduced performance in two consequential applications. Our framework is\napplicable to other fields as well. This dissertation presents an effective\nmeans of closing the gap between promise and practice in artificial\nintelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning. (arXiv:2206.08657v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.08657","description":"<p>Vision-Language (VL) models with the Two-Tower architecture have dominated\nvisual-language representation learning in recent years. Current VL models\neither use lightweight uni-modal encoders and learn to extract, align and fuse\nboth modalities simultaneously in a deep cross-modal encoder, or feed the\nlast-layer uni-modal representations from the deep pre-trained uni-modal\nencoders into the top cross-modal encoder. Both approaches potentially restrict\nvision-language representation learning and limit model performance. In this\npaper, we propose Bridge-Tower, which introduces multiple bridge layers that\nbuild a connection between the top layers of uni-modal encoders and each layer\nof the cross-modal encoder. This enables effective bottom-up cross-modal\nalignment and fusion between visual and textual representations of different\nsemantic levels of pre-trained uni-modal encoders in the cross-modal encoder.\nPre-trained with only 4M images, Bridge-Tower achieves state-of-the-art\nperformance on various downstream vision-language tasks. In particular, on the\nVQAv2 test-std set, Bridge-Tower achieves an accuracy of 78.73%, outperforming\nthe previous state-of-the-art model METER by 1.09% with the same pre-training\ndata and almost negligible additional parameters and computational costs.\nNotably, when further scaling the model, Bridge-Tower achieves an accuracy of\n81.15%, surpassing models that are pre-trained on orders-of-magnitude larger\ndatasets. Code and checkpoints are available at\n\\url{https://github.com/microsoft/BridgeTower}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenman_S/0/1/0/all/0/1\">Shachar Rosenman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1\">Vasudev Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VAuLT: Augmenting the Vision-and-Language Transformer for Sentiment Classification on Social Media. (arXiv:2208.09021v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2208.09021","description":"<p>We propose the Vision-and-Augmented-Language Transformer (VAuLT). VAuLT is an\nextension of the popular Vision-and-Language Transformer (ViLT), and improves\nperformance on vision-and-language (VL) tasks that involve more complex text\ninputs than image captions while having minimal impact on training and\ninference efficiency. ViLT, importantly, enables efficient training and\ninference in VL tasks, achieved by encoding images using a linear projection of\npatches instead of an object detector. However, it is pretrained on captioning\ndatasets, where the language input is simple, literal, and descriptive,\ntherefore lacking linguistic diversity. So, when working with multimedia data\nin the wild, such as multimodal social media data, there is a notable shift\nfrom captioning language data, as well as diversity of tasks. We indeed find\nevidence that the language capacity of ViLT is lacking. The key insight and\nnovelty of VAuLT is to propagate the output representations of a large language\nmodel (LM) like BERT to the language input of ViLT. We show that joint training\nof the LM and ViLT can yield relative improvements up to 20% over ViLT and\nachieve state-of-the-art or comparable performance on VL tasks involving richer\nlanguage inputs and affective constructs, such as for Target-Oriented Sentiment\nClassification in TWITTER-2015 and TWITTER-2017, and Sentiment Classification\nin MVSA-Single and MVSA-Multiple. Our code is available at\nhttps://github.com/gchochla/VAuLT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chochlakis_G/0/1/0/all/0/1\">Georgios Chochlakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_T/0/1/0/all/0/1\">Tejas Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a> (University of Southern California)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Review of Natural Language Processing in Pharmacology. (arXiv:2208.10228v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.10228","description":"<p>Natural language processing (NLP) is an area of artificial intelligence that\napplies information technologies to process the human language, understand it\nto a certain degree, and use it in various applications. This area has rapidly\ndeveloped in the last few years and now employs modern variants of deep neural\nnetworks to extract relevant patterns from large text corpora. The main\nobjective of this work is to survey the recent use of NLP in the field of\npharmacology. As our work shows, NLP is a highly relevant information\nextraction and processing approach for pharmacology. It has been used\nextensively, from intelligent searches through thousands of medical documents\nto finding traces of adversarial drug interactions in social media. We split\nour coverage into five categories to survey modern NLP methodology, commonly\naddressed tasks, relevant textual data, knowledge bases, and useful programming\nlibraries. We split each of the five categories into appropriate subcategories,\ndescribe their main properties and ideas, and summarize them in a tabular form.\nThe resulting survey presents a comprehensive overview of the area, useful to\npractitioners and interested observers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trajanov_D/0/1/0/all/0/1\">Dimitar Trajanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trajkovski_V/0/1/0/all/0/1\">Vangel Trajkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitrieva_M/0/1/0/all/0/1\">Makedonka Dimitrieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobreva_J/0/1/0/all/0/1\">Jovana Dobreva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jovanovik_M/0/1/0/all/0/1\">Milos Jovanovik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klemen_M/0/1/0/all/0/1\">Matej Klemen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zagar_A/0/1/0/all/0/1\">Ale&#x161; &#x17d;agar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1\">Marko Robnik-&#x160;ikonja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought. (arXiv:2210.01240v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.01240","description":"<p>Large language models (LLMs) have shown remarkable reasoning capabilities\ngiven chain-of-thought prompts (examples with intermediate reasoning steps).\nExisting benchmarks measure reasoning ability indirectly, by evaluating\naccuracy on downstream tasks such as mathematical reasoning. However, it is\nunclear how these models obtain the answers and whether they rely on simple\nheuristics rather than the generated chain-of-thought. To enable systematic\nexploration of the reasoning ability of LLMs, we present a new synthetic\nquestion-answering dataset called PrOntoQA, where each example is generated\nfrom a synthetic world model represented in first-order logic. This allows us\nto parse the generated chain-of-thought into symbolic proofs for formal\nanalysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite\ncapable of making correct individual deduction steps, and so are generally\ncapable of reasoning, even in fictional contexts. However, they have difficulty\nwith proof planning: When multiple valid deduction steps are available, they\nare not able to systematically explore the different options.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saparov_A/0/1/0/all/0/1\">Abulhair Saparov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task. (arXiv:2210.13382v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.13382","description":"<p>Language models show a surprising range of capabilities, but the source of\ntheir apparent competence is unclear. Do these networks just memorize a\ncollection of surface statistics, or do they rely on internal representations\nof the process that generates the sequences they see? We investigate this\nquestion by applying a variant of the GPT model to the task of predicting legal\nmoves in a simple board game, Othello. Although the network has no a priori\nknowledge of the game or its rules, we uncover evidence of an emergent\nnonlinear internal representation of the board state. Interventional\nexperiments indicate this representation can be used to control the output of\nthe network and create \"latent saliency maps\" that can help explain predictions\nin human terms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kenneth Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hopkins_A/0/1/0/all/0/1\">Aspen K. Hopkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viegas_F/0/1/0/all/0/1\">Fernanda Vi&#xe9;gas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenberg_M/0/1/0/all/0/1\">Martin Wattenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parsel: A (De-)compositional Framework for Algorithmic Reasoning with Language Models. (arXiv:2212.10561v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10561","description":"<p>Despite recent success in large language model (LLM) reasoning, LLMs struggle\nwith hierarchical multi-step reasoning tasks like generating complex programs.\nFor these tasks, humans often start with a high-level algorithmic design and\nimplement each part gradually. We introduce Parsel, a framework enabling\nautomatic implementation and validation of complex algorithms with code LLMs,\ntaking hierarchical function descriptions in natural language as input. We show\nthat Parsel can be used across domains requiring hierarchical reasoning,\nincluding program synthesis, robotic planning, and theorem proving. We show\nthat LLMs generating Parsel solve more competition-level problems in the APPS\ndataset, resulting in pass rates that are over 75% higher than prior results\nfrom directly sampling AlphaCode and Codex, while often using a smaller sample\nbudget. We also find that LLM-generated robotic plans using Parsel as an\nintermediate language are more than twice as likely to be considered accurate\nthan directly generated plans. Lastly, we explore how Parsel addresses LLM\nlimitations and discuss how Parsel may be useful for human programmers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zelikman_E/0/1/0/all/0/1\">Eric Zelikman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poesia_G/0/1/0/all/0/1\">Gabriel Poesia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah D. Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haber_N/0/1/0/all/0/1\">Nick Haber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Data Selection for TTS: Using Arabic Broadcast News as a Case Study. (arXiv:2301.09099v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.09099","description":"<p>Several high-resource Text to Speech (TTS) systems currently produce natural,\nwell-established human-like speech. In contrast, low-resource languages,\nincluding Arabic, have very limited TTS systems due to the lack of resources.\nWe propose a fully unsupervised method for building TTS, including automatic\ndata selection and pre-training/fine-tuning strategies for TTS training, using\nbroadcast news as a case study. We show how careful selection of data, yet\nsmaller amounts, can improve the efficiency of TTS system in generating more\nnatural speech than a system trained on a bigger dataset. We adopt to propose\ndifferent approaches for the: 1) data: we applied automatic annotations using\nDNSMOS, automatic vowelization, and automatic speech recognition (ASR) for\nfixing transcriptions' errors; 2) model: we used transfer learning from\nhigh-resource language in TTS model and fine-tuned it with one hour broadcast\nrecording then we used this model to guide a FastSpeech2-based Conformer model\nfor duration. Our objective evaluation shows 3.9% character error rate (CER),\nwhile the groundtruth has 1.3% CER. As for the subjective evaluation, where 1\nis bad and 5 is excellent, our FastSpeech2-based Conformer model achieved a\nmean opinion score (MOS) of 4.4 for intelligibility and 4.2 for naturalness,\nwhere many annotators recognized the voice of the broadcaster, which proves the\neffectiveness of our proposed unsupervised method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baali_M/0/1/0/all/0/1\">Massa Baali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_T/0/1/0/all/0/1\">Tomoki Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiti_S/0/1/0/all/0/1\">Soumi Maiti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Hajj_W/0/1/0/all/0/1\">Wassim El-Hajj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViHOS: Hate Speech Spans Detection for Vietnamese. (arXiv:2301.10186v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.10186","description":"<p>The rise in hateful and offensive language directed at other users is one of\nthe adverse side effects of the increased use of social networking platforms.\nThis could make it difficult for human moderators to review tagged comments\nfiltered by classification systems. To help address this issue, we present the\nViHOS (Vietnamese Hate and Offensive Spans) dataset, the first human-annotated\ncorpus containing 26k spans on 11k comments. We also provide definitions of\nhateful and offensive spans in Vietnamese comments as well as detailed\nannotation guidelines. Besides, we conduct experiments with various\nstate-of-the-art models. Specifically, XLM-R$_{Large}$ achieved the best\nF1-scores in Single span detection and All spans detection, while\nPhoBERT$_{Large}$ obtained the highest in Multiple spans detection. Finally,\nour error analysis demonstrates the difficulties in detecting specific types of\nspans in our data for future research.\n</p>\n<p>Disclaimer: This paper contains real comments that could be considered\nprofane, offensive, or abusive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoang_P/0/1/0/all/0/1\">Phu Gia Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_C/0/1/0/all/0/1\">Canh Duc Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Khanh Quoc Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Document-level Relation Extraction as Semantic Segmentation. (arXiv:2106.03618v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2106.03618","description":"<p>Document-level relation extraction aims to extract relations among multiple\nentity pairs from a document. Previously proposed graph-based or\ntransformer-based models utilize the entities independently, regardless of\nglobal information among relational triples. This paper approaches the problem\nby predicting an entity-level relation matrix to capture local and global\ninformation, parallel to the semantic segmentation task in computer vision.\nHerein, we propose a Document U-shaped Network for document-level relation\nextraction. Specifically, we leverage an encoder module to capture the context\ninformation of entities and a U-shaped segmentation module over the image-style\nfeature map to capture global interdependency among triples. Experimental\nresults show that our approach can obtain state-of-the-art performance on three\nbenchmark datasets DocRED, CDR, and GDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ontology-enhanced Prompt-tuning for Few-shot Learning. (arXiv:2201.11332v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2201.11332","description":"<p>Few-shot Learning (FSL) is aimed to make predictions based on a limited\nnumber of samples. Structured data such as knowledge graphs and ontology\nlibraries has been leveraged to benefit the few-shot setting in various tasks.\nHowever, the priors adopted by the existing methods suffer from challenging\nknowledge missing, knowledge noise, and knowledge heterogeneity, which hinder\nthe performance for few-shot learning. In this study, we explore knowledge\ninjection for FSL with pre-trained language models and propose\nontology-enhanced prompt-tuning (OntoPrompt). Specifically, we develop the\nontology transformation based on the external knowledge graph to address the\nknowledge missing issue, which fulfills and converts structure knowledge to\ntext. We further introduce span-sensitive knowledge injection via a visible\nmatrix to select informative knowledge to handle the knowledge noise issue. To\nbridge the gap between knowledge and text, we propose a collective training\nalgorithm to optimize representations jointly. We evaluate our proposed\nOntoPrompt in three tasks, including relation extraction, event extraction, and\nknowledge graph completion, with eight datasets. Experimental results\ndemonstrate that our approach can obtain better few-shot performance than\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-01-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}