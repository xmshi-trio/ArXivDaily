{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-08-23T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering. (arXiv:2308.10959v1 [cs.CL])","link":"http://arxiv.org/abs/2308.10959","description":"<p>In this paper, we propose Docprompt for document question answering tasks\nwith powerful zero-shot and few-shot performance. We proposed a novel weakly\nsupervised data generation method, a novel multl-stage training method and a\nnovel understanding model &amp; generation model ensemble method. Experiment\nresults show that the Docprompt model after continue pretrain significantly\noutperforms the existing strong baseline models on document question answering\ntasks. This method greatly improves the delivery efficiency and model\nperformance of document question answering customer projects, reducing\nannotation costs and labor costs. Our demo can be found at\nhttps://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Sijin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Teng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shikun Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Explainable AI to Analyze Researchers' Aspect-Based Sentiment about ChatGPT. (arXiv:2308.11001v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11001","description":"<p>The groundbreaking invention of ChatGPT has triggered enormous discussion\namong users across all fields and domains. Among celebration around its various\nadvantages, questions have been raised with regards to its correctness and\nethics of its use. Efforts are already underway towards capturing user\nsentiments around it. But it begs the question as to how the research community\nis analyzing ChatGPT with regards to various aspects of its usage. It is this\nsentiment of the researchers that we analyze in our work. Since Aspect-Based\nSentiment Analysis has usually only been applied on a few datasets, it gives\nlimited success and that too only on short text data. We propose a methodology\nthat uses Explainable AI to facilitate such analysis on research data. Our\ntechnique presents valuable insights into extending the state of the art of\nAspect-Based Sentiment Analysis on newer datasets, where such analysis is not\nhampered by the length of the text data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lakhanpal_S/0/1/0/all/0/1\">Shilpa Lakhanpal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ajay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_R/0/1/0/all/0/1\">Rajeev Agrawal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using language models in the implicit automated assessment of mathematical short answer items. (arXiv:2308.11006v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11006","description":"<p>We propose a new way to assess certain short constructed responses to\nmathematics items. Our approach uses a pipeline that identifies the key values\nspecified by the student in their response. This allows us to determine the\ncorrectness of the response, as well as identify any misconceptions. The\ninformation from the value identification pipeline can then be used to provide\nfeedback to the teacher and student. The value identification pipeline consists\nof two fine-tuned language models. The first model determines if a value is\nimplicit in the student response. The second model identifies where in the\nresponse the key value is specified. We consider both a generic model that can\nbe used for any prompt and value, as well as models that are specific to each\nprompt and value. The value identification pipeline is a more accurate and\ninformative way to assess short constructed responses than traditional\nrubric-based scoring. It can be used to provide more targeted feedback to\nstudents, which can help them improve their understanding of mathematics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ormerod_C/0/1/0/all/0/1\">Christopher Ormerod</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Objective Evaluation of Socially-Situated Conversational Robots: Assessing Human-Likeness through Multimodal User Behaviors. (arXiv:2308.11020v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11020","description":"<p>This paper tackles the challenging task of evaluating socially situated\nconversational robots and presents a novel objective evaluation approach that\nrelies on multimodal user behaviors. In this study, our main focus is on\nassessing the human-likeness of the robot as the primary evaluation metric.\nWhile previous research often relied on subjective evaluations from users, our\napproach aims to evaluate the robot's human-likeness based on observable user\nbehaviors indirectly, thus enhancing objectivity and reproducibility. To begin,\nwe created an annotated dataset of human-likeness scores, utilizing user\nbehaviors found in an attentive listening dialogue corpus. We then conducted an\nanalysis to determine the correlation between multimodal user behaviors and\nhuman-likeness scores, demonstrating the feasibility of our proposed\nbehavior-based evaluation method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inoue_K/0/1/0/all/0/1\">Koji Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lala_D/0/1/0/all/0/1\">Divesh Lala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochi_K/0/1/0/all/0/1\">Keiko Ochi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_T/0/1/0/all/0/1\">Tatsuya Kawahara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skantze_G/0/1/0/all/0/1\">Gabriel Skantze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models. (arXiv:2308.11103v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11103","description":"<p>Anonymity of both natural and legal persons in court rulings is a critical\naspect of privacy protection in the European Union and Switzerland. With the\nadvent of LLMs, concerns about large-scale re-identification of anonymized\npersons are growing. In accordance with the Federal Supreme Court of\nSwitzerland, we explore the potential of LLMs to re-identify individuals in\ncourt rulings by constructing a proof-of-concept using actual legal data from\nthe Swiss federal supreme court. Following the initial experiment, we\nconstructed an anonymized Wikipedia dataset as a more rigorous testing ground\nto further investigate the findings. With the introduction and application of\nthe new task of re-identifying people in texts, we also introduce new metrics\nto measure performance. We systematically analyze the factors that influence\nsuccessful re-identifications, identifying model size, input length, and\ninstruction tuning among the most critical determinants. Despite high\nre-identification rates on Wikipedia, even the best LLMs struggled with court\ndecisions. The complexity is attributed to the lack of test datasets, the\nnecessity for substantial training resources, and data sparsity in the\ninformation used for re-identification. In conclusion, this study demonstrates\nthat re-identification using LLMs may not be feasible for now, but as the\nproof-of-concept on Wikipedia showed, it might become possible in the future.\nWe hope that our system can help enhance the confidence in the security of\nanonymized decisions, thus leading to the courts being more confident to\npublish decisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nyffenegger_A/0/1/0/all/0/1\">Alex Nyffenegger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sturmer_M/0/1/0/all/0/1\">Matthias St&#xfc;rmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niklaus_J/0/1/0/all/0/1\">Joel Niklaus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLP-based detection of systematic anomalies among the narratives of consumer complaints. (arXiv:2308.11138v1 [stat.ME])","link":"http://arxiv.org/abs/2308.11138","description":"<p>We develop an NLP-based procedure for detecting systematic nonmeritorious\nconsumer complaints, simply called systematic anomalies, among complaint\nnarratives. While classification algorithms are used to detect pronounced\nanomalies, in the case of smaller and frequent systematic anomalies, the\nalgorithms may falter due to a variety of reasons, including technical ones as\nwell as natural limitations of human analysts. Therefore, as the next step\nafter classification, we convert the complaint narratives into quantitative\ndata, which are then analyzed using an algorithm for detecting systematic\nanomalies. We illustrate the entire procedure using complaint narratives from\nthe Consumer Complaint Database of the Consumer Financial Protection Bureau.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Gao_P/0/1/0/all/0/1\">Peiheng Gao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sun_N/0/1/0/all/0/1\">Ning Sun</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1\">Xuefeng Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_C/0/1/0/all/0/1\">Chen Yang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zitikis_R/0/1/0/all/0/1\">Ri&#x10d;ardas Zitikis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning (Practical Experience Report). (arXiv:2308.11148v1 [cs.SE])","link":"http://arxiv.org/abs/2308.11148","description":"<p>The automation of code review activities, a long-standing pursuit in software\nengineering, has been primarily addressed by numerous domain-specific\npre-trained models. Despite their success, these models frequently demand\nextensive resources for pre-training from scratch. In contrast, Large Language\nModels (LLMs) provide an intriguing alternative, given their remarkable\ncapabilities when supplemented with domain-specific knowledge. However, their\npotential for automating code review tasks remains largely unexplored.\n</p>\n<p>In response to this research gap, we present LLaMA-Reviewer, an innovative\nframework that leverages the capabilities of LLaMA, a popular LLM, in the realm\nof code review. Mindful of resource constraints, this framework employs\nparameter-efficient fine-tuning (PEFT) methods, delivering high performance\nwhile using less than 1% of trainable parameters.\n</p>\n<p>An extensive evaluation of LLaMA-Reviewer is conducted on two diverse,\npublicly available datasets. Notably, even with the smallest LLaMA base model\nconsisting of 6.7B parameters and a limited number of tuning epochs,\nLLaMA-Reviewer equals the performance of existing code-review-focused models.\n</p>\n<p>The ablation experiments provide insights into the influence of various\nfine-tuning process components, including input representation, instruction\ntuning, and different PEFT methods. To foster continuous progress in this\nfield, the code and all PEFT-weight plugins have been made open-source.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Junyi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaojia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Li Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_C/0/1/0/all/0/1\">Chun Zuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViCo: Engaging Video Comment Generation with Human Preference Rewards. (arXiv:2308.11171v1 [cs.CV])","link":"http://arxiv.org/abs/2308.11171","description":"<p>Engaging video comments play an important role in video social media, as they\nare the carrier of feelings, thoughts, or humor of the audience. Preliminary\nworks have made initial exploration for video comment generation by adopting\ncaption-style encoder-decoder models. However, comment generation presents some\nunique challenges distinct from caption generation, which makes these methods\nsomewhat less effective at generating engaging comments. In contrast to the\nobjective and descriptive nature of captions, comments tend to be inherently\nsubjective, making it hard to quantify and evaluate the engagement of comments.\nFurthermore, the scarcity of truly engaging comments brings difficulty to\ncollecting enough high-quality training examples. In this paper, we propose\nViCo with three novel designs to tackle the above challenges for generating\nengaging Video Comments. Firstly, to quantify the engagement of comments, we\nutilize the number of \"likes\" each comment receives as a proxy of human\npreference after an appropriate debiasing procedure. Secondly, to automatically\nevaluate the engagement of comments, we train a reward model to align its\njudgment to the above proxy. Our user studies indicate that this reward model\neffectively aligns with human judgments. Lastly, to alleviate the scarcity of\nhigh-quality comments, an initial generator is trained on readily available but\nnoisy data to generate comments. Then the reward model is employed to offer\nfeedback on the generated comments, thus optimizing the initial generator. To\nfacilitate the research of video commenting, we collect a large video\ncomment-dataset (ViCo-20k) with rich metadata from a popular video website.\nExperiments on ViCo-20k show that the comments generated by our ViCo model\nexhibit the best performance in terms of both quantitative and qualitative\nresults, particularly when engagement is considered.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuchong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Ruihua Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries. (arXiv:2308.11189v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11189","description":"<p>Error prediction in large language models often relies on domain-specific\ninformation. In this paper, we present measures for quantification of error in\nthe response of a large language model based on the diversity of responses to a\ngiven prompt - hence independent of the underlying application. We describe how\nthree such measures - based on entropy, Gini impurity, and centroid distance -\ncan be employed. We perform a suite of experiments on multiple datasets and\ntemperature settings to demonstrate that these measures strongly correlate with\nthe probability of failure. Additionally, we present empirical results\ndemonstrating how these measures can be applied to few-shot prompting,\nchain-of-thought reasoning, and error detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ngu_N/0/1/0/all/0/1\">Noel Ngu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nathaniel Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakarian_P/0/1/0/all/0/1\">Paulo Shakarian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis. (arXiv:2308.11224v1 [cs.AI])","link":"http://arxiv.org/abs/2308.11224","description":"<p>Large Language Models (LLMs) have garnered considerable interest within both\nacademic and industrial. Yet, the application of LLMs to graph data remains\nunder-explored. In this study, we evaluate the capabilities of four LLMs in\naddressing several analytical problems with graph data. We employ four distinct\nevaluation metrics: Comprehension, Correctness, Fidelity, and Rectification.\nOur results show that: 1) LLMs effectively comprehend graph data in natural\nlanguage and reason with graph topology. 2) GPT models can generate logical and\ncoherent results, outperforming alternatives in correctness. 3) All examined\nLLMs face challenges in structural reasoning, with techniques like zero-shot\nchain-of-thought and few-shot prompting showing diminished efficacy. 4) GPT\nmodels often produce erroneous answers in multi-answer tasks, raising concerns\nin fidelity. 5) GPT models exhibit elevated confidence in their outputs,\npotentially hindering their rectification capacities. Notably, GPT-4 has\ndemonstrated the capacity to rectify responses from GPT-3.5-turbo and its own\nprevious iterations. The code is available at:\nhttps://github.com/Ayame1006/LLMtoGraph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HopPG: Self-Iterative Program Generation for Multi-Hop Question Answering over Heterogeneous Knowledge. (arXiv:2308.11257v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11257","description":"<p>The semantic parsing-based method is an important research branch for\nknowledge-based question answering. It usually generates executable programs\nlean upon the question and then conduct them to reason answers over a knowledge\nbase. Benefit from this inherent mechanism, it has advantages in the\nperformance and the interpretability. However,traditional semantic parsing\nmethods usually generate a complete program before executing it, which\nstruggles with multi-hop question answering over heterogeneous knowledge.\nFirstly,a complete multi-hop program relies on multiple heterogeneous\nsupporting facts, and it is difficult for models to receive these facts\nsimultaneously. Secondly,these methods ignore the interaction information\nbetween the previous-hop execution result and the current-hop program\ngeneration. To alleviate these challenges, we propose a self-iterative\nframework for multi-hop program generation (HopPG) over heterogeneous\nknowledge, which leverages the previous-hop execution results to retrieve\nsupporting facts and generate subsequent programs iteratively. We evaluate our\nmodel on MMQA-T^2. The experimental results show that HopPG outperforms\nexisting semantic-parsing-based baselines, especially on the multi-hop\nquestions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yongwei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chaoqun Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning. (arXiv:2308.11276v1 [cs.SD])","link":"http://arxiv.org/abs/2308.11276","description":"<p>Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity\nof large-scale publicly available music datasets with natural language\ncaptions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA),\ncapable of answering music-related questions and generating captions for music\nfiles. Our model utilizes audio representations from a pretrained MERT model to\nextract music features. However, obtaining a suitable dataset for training the\nMU-LLaMA model remains challenging, as existing publicly accessible audio\nquestion answering datasets lack the necessary depth for open-ended music\nquestion answering. To fill this gap, we present a methodology for generating\nquestion-answer pairs from existing audio captioning datasets and introduce the\nMusicQA Dataset designed for answering open-ended music-related questions. The\nexperiments demonstrate that the proposed MU-LLaMA model, trained on our\ndesigned MusicQA dataset, achieves outstanding performance in both music\nquestion answering and music caption generation across various metrics,\noutperforming current state-of-the-art (SOTA) models in both fields and\noffering a promising advancement in the T2M-Gen research field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shansong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1\">Atin Sakkeer Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chenshuo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEAP: Efficient and Automated Test Method for NLP Software. (arXiv:2308.11284v1 [cs.SE])","link":"http://arxiv.org/abs/2308.11284","description":"<p>The widespread adoption of DNNs in NLP software has highlighted the need for\nrobustness. Researchers proposed various automatic testing techniques for\nadversarial test cases. However, existing methods suffer from two limitations:\nweak error-discovering capabilities, with success rates ranging from 0% to\n24.6% for BERT-based NLP software, and time inefficiency, taking 177.8s to\n205.28s per test case, making them challenging for time-constrained scenarios.\nTo address these issues, this paper proposes LEAP, an automated test method\nthat uses LEvy flight-based Adaptive Particle swarm optimization integrated\nwith textual features to generate adversarial test cases. Specifically, we\nadopt Levy flight for population initialization to increase the diversity of\ngenerated test cases. We also design an inertial weight adaptive update\noperator to improve the efficiency of LEAP's global optimization of\nhigh-dimensional text examples and a mutation operator based on the greedy\nstrategy to reduce the search time. We conducted a series of experiments to\nvalidate LEAP's ability to test NLP software and found that the average success\nrate of LEAP in generating adversarial test cases is 79.1%, which is 6.1%\nhigher than the next best approach (PSOattack). While ensuring high success\nrates, LEAP significantly reduces time overhead by up to 147.6s compared to\nother heuristic-based methods. Additionally, the experimental results\ndemonstrate that LEAP can generate more transferable test cases and\nsignificantly enhance the robustness of DNN-based systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1\">Mingxuan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hai Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shunhui Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengcheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M3PS: End-to-End Multi-Grained Multi-Modal Attribute-Aware Product Summarization in E-commerce. (arXiv:2308.11351v1 [cs.MM])","link":"http://arxiv.org/abs/2308.11351","description":"<p>Given the long textual product information and the product image, Multi-Modal\nProduct Summarization (MMPS) aims to attract customers' interest and increase\ntheir desire to purchase by highlighting product characteristics with a short\ntextual summary. Existing MMPS methods have achieved promising performance.\nNevertheless, there still exist several problems: 1) lack end-to-end product\nsummarization, 2) lack multi-grained multi-modal modeling, and 3) lack\nmulti-modal attribute modeling. To address these issues, we propose an\nend-to-end multi-grained multi-modal attribute-aware product summarization\nmethod (M3PS) for generating high-quality product summaries in e-commerce. M3PS\njointly models product attributes and generates product summaries. Meanwhile,\nwe design several multi-grained multi-modal tasks to better guide the\nmulti-modal learning of M3PS. Furthermore, we model product attributes based on\nboth text and image modalities so that multi-modal product characteristics can\nbe manifested in the generated summaries. Extensive experiments on a real\nlarge-scale Chinese e-commence dataset demonstrate that our model outperforms\nstate-of-the-art product summarization methods w.r.t. several summarization\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Ze Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jiayi Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yiyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convoifilter: A case study of doing cocktail party speech recognition. (arXiv:2308.11380v1 [cs.SD])","link":"http://arxiv.org/abs/2308.11380","description":"<p>This paper presents an end-to-end model designed to improve automatic speech\nrecognition (ASR) for a particular speaker in a crowded, noisy environment. The\nmodel utilizes a single-channel speech enhancement module that isolates the\nspeaker's voice from background noise, along with an ASR module. Through this\napproach, the model is able to decrease the word error rate (WER) of ASR from\n80% to 26.4%. Typically, these two components are adjusted independently due to\nvariations in data requirements. However, speech enhancement can create\nanomalies that decrease ASR efficiency. By implementing a joint fine-tuning\nstrategy, the model can reduce the WER from 26.4% in separate tuning to 14.5%\nin joint tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thai-Binh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alexander Waibel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Relational Triples Based on Graph Recursive Neural Network via Dynamic Feedback Forest Algorithm. (arXiv:2308.11411v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11411","description":"<p>Extracting relational triples (subject, predicate, object) from text enables\nthe transformation of unstructured text data into structured knowledge. The\nnamed entity recognition (NER) and the relation extraction (RE) are two\nfoundational subtasks in this knowledge generation pipeline. The integration of\nsubtasks poses a considerable challenge due to their disparate nature. This\npaper presents a novel approach that converts the triple extraction task into a\ngraph labeling problem, capitalizing on the structural information of\ndependency parsing and graph recursive neural networks (GRNNs). To integrate\nsubtasks, this paper proposes a dynamic feedback forest algorithm that connects\nthe representations of subtasks by inference operations during model training.\nExperimental results demonstrate the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongyin Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Large Language Model based Autonomous Agents. (arXiv:2308.11432v1 [cs.AI])","link":"http://arxiv.org/abs/2308.11432","description":"<p>Autonomous agents have long been a prominent research topic in the academic\ncommunity. Previous research in this field often focuses on training agents\nwith limited knowledge within isolated environments, which diverges\nsignificantly from the human learning processes, and thus makes the agents hard\nto achieve human-like decisions. Recently, through the acquisition of vast\namounts of web knowledge, large language models (LLMs) have demonstrated\nremarkable potential in achieving human-level intelligence. This has sparked an\nupsurge in studies investigating autonomous agents based on LLMs. To harness\nthe full potential of LLMs, researchers have devised diverse agent\narchitectures tailored to different applications. In this paper, we present a\ncomprehensive survey of these studies, delivering a systematic review of the\nfield of autonomous agents from a holistic perspective. More specifically, our\nfocus lies in the construction of LLM-based agents, for which we propose a\nunified framework that encompasses a majority of the previous work.\nAdditionally, we provide a summary of the various applications of LLM-based AI\nagents in the domains of social science, natural science, and engineering.\nLastly, we discuss the commonly employed evaluation strategies for LLM-based AI\nagents. Based on the previous studies, we also present several challenges and\nfuture directions in this field. To keep track of this field and continuously\nupdate our survey, we maintain a repository for the related references at\nhttps://github.com/Paitesanshi/LLM-Agent-Survey.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chen Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xueyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zeyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingsen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiakai Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhewei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aspect-oriented Opinion Alignment Network for Aspect-Based Sentiment Classification. (arXiv:2308.11447v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11447","description":"<p>Aspect-based sentiment classification is a crucial problem in fine-grained\nsentiment analysis, which aims to predict the sentiment polarity of the given\naspect according to its context. Previous works have made remarkable progress\nin leveraging attention mechanism to extract opinion words for different\naspects. However, a persistent challenge is the effective management of\nsemantic mismatches, which stem from attention mechanisms that fall short in\nadequately aligning opinions words with their corresponding aspect in\nmulti-aspect sentences. To address this issue, we propose a novel\nAspect-oriented Opinion Alignment Network (AOAN) to capture the contextual\nassociation between opinion words and the corresponding aspect. Specifically,\nwe first introduce a neighboring span enhanced module which highlights various\ncompositions of neighboring words and given aspects. In addition, we design a\nmulti-perspective attention mechanism that align relevant opinion information\nwith respect to the given aspect. Extensive experiments on three benchmark\ndatasets demonstrate that our model achieves state-of-the-art results. The\nsource code is available at https://github.com/AONE-NLP/ABSA-AOAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xueyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1\">Rui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1\">Yanglei Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1\">Da Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaojun Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models. (arXiv:2308.11462v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11462","description":"<p>The advent of large language models (LLMs) and their adoption by the legal\ncommunity has given rise to the question: what types of legal reasoning can\nLLMs perform? To enable greater study of this question, we present LegalBench:\na collaboratively constructed legal reasoning benchmark consisting of 162 tasks\ncovering six different types of legal reasoning. LegalBench was built through\nan interdisciplinary process, in which we collected tasks designed and\nhand-crafted by legal professionals. Because these subject matter experts took\na leading role in construction, tasks either measure legal reasoning\ncapabilities that are practically useful, or measure reasoning skills that\nlawyers find interesting. To enable cross-disciplinary conversations about LLMs\nin the law, we additionally show how popular legal frameworks for describing\nlegal reasoning -- which distinguish between its many forms -- correspond to\nLegalBench tasks, thus giving lawyers and LLM developers a common vocabulary.\nThis paper describes LegalBench, presents an empirical evaluation of 20\nopen-source and commercial LLMs, and illustrates the types of research\nexplorations LegalBench enables.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guha_N/0/1/0/all/0/1\">Neel Guha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyarko_J/0/1/0/all/0/1\">Julian Nyarko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1\">Daniel E. Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chilton_A/0/1/0/all/0/1\">Adam Chilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayana_A/0/1/0/all/0/1\">Aditya Narayana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chohlas_Wood_A/0/1/0/all/0/1\">Alex Chohlas-Wood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_A/0/1/0/all/0/1\">Austin Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waldon_B/0/1/0/all/0/1\">Brandon Waldon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rockmore_D/0/1/0/all/0/1\">Daniel N. Rockmore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zambrano_D/0/1/0/all/0/1\">Diego Zambrano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talisman_D/0/1/0/all/0/1\">Dmitry Talisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoque_E/0/1/0/all/0/1\">Enam Hoque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surani_F/0/1/0/all/0/1\">Faiz Surani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fagan_F/0/1/0/all/0/1\">Frank Fagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarfaty_G/0/1/0/all/0/1\">Galit Sarfaty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dickinson_G/0/1/0/all/0/1\">Gregory M. Dickinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porat_H/0/1/0/all/0/1\">Haggai Porat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegland_J/0/1/0/all/0/1\">Jason Hegland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jessica Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nudell_J/0/1/0/all/0/1\">Joe Nudell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niklaus_J/0/1/0/all/0/1\">Joel Niklaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nay_J/0/1/0/all/0/1\">John Nay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jonathan H. Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tobia_K/0/1/0/all/0/1\">Kevin Tobia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagan_M/0/1/0/all/0/1\">Margaret Hagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Megan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livermore_M/0/1/0/all/0/1\">Michael Livermore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasumov_Rahe_N/0/1/0/all/0/1\">Nikon Rasumov-Rahe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holzenberger_N/0/1/0/all/0/1\">Nils Holzenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolt_N/0/1/0/all/0/1\">Noam Kolt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_P/0/1/0/all/0/1\">Peter Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehaag_S/0/1/0/all/0/1\">Sean Rehaag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1\">Sharad Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_S/0/1/0/all/0/1\">Spencer Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_S/0/1/0/all/0/1\">Sunny Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zur_T/0/1/0/all/0/1\">Tom Zur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_V/0/1/0/all/0/1\">Varun Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zehua Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence-Level Multimodal and Language-Agnostic Representations. (arXiv:2308.11466v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11466","description":"<p>We introduce SONAR, a new multilingual and multimodal fixed-size sentence\nembedding space. Our single text encoder, covering 200 languages, substantially\noutperforms existing sentence embeddings such as LASER3 and LabSE on the xsim\nand xsim++ multilingual similarity search tasks. Speech segments can be\nembedded in the same SONAR embedding space using language-specific speech\nencoders trained in a teacher-student setting on speech transcription data. Our\nencoders outperform existing speech encoders on similarity search tasks. We\nalso provide a text decoder for 200 languages, which allows us to perform\ntext-to-text and speech-to-text machine translation, including for zero-shot\nlanguage and modality combinations. Our text-to-text results are competitive\ncompared to the state-of-the-art NLLB~1B model, despite the fixed-size\nbottleneck representation. Our zero-shot speech-to-text translation results\ncompare favorably with strong supervised baselines such as Whisper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duquenne_P/0/1/0/all/0/1\">Paul-Ambroise Duquenne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwenk_H/0/1/0/all/0/1\">Holger Schwenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions. (arXiv:2308.11483v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11483","description":"<p>Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious NLP tasks. However, previous works have shown these models are\nsensitive towards prompt wording, and few-shot demonstrations and their order,\nposing challenges to fair assessment of these models. As these models become\nmore powerful, it becomes imperative to understand and address these\nlimitations. In this paper, we focus on LLMs robustness on the task of\nmultiple-choice questions -- commonly adopted task to study reasoning and\nfact-retrieving capability of LLMs. Investigating the sensitivity of LLMs\ntowards the order of options in multiple-choice questions, we demonstrate a\nconsiderable performance gap of approximately 13% to 75% in LLMs on different\nbenchmarks, when answer options are reordered, even when using demonstrations\nin a few-shot setting. Through a detailed analysis, we conjecture that this\nsensitivity arises when LLMs are uncertain about the prediction between the\ntop-2/3 choices, and specific options placements may favor certain prediction\nbetween those top choices depending on the question caused by positional bias.\nWe also identify patterns in top-2 choices that amplify or mitigate the model's\nbias toward option placement. We found that for amplifying bias, the optimal\nstrategy involves positioning the top two choices as the first and last\noptions. Conversely, to mitigate bias, we recommend placing these choices among\nthe adjacent options. To validate our conjecture, we conduct various\nexperiments and adopt two approaches to calibrate LLMs' predictions, leading to\nup to 8 percentage points improvement across different models and benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pezeshkpour_P/0/1/0/all/0/1\">Pouya Pezeshkpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hruschka_E/0/1/0/all/0/1\">Estevam Hruschka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Authorship Representation Learning Capture Stylistic Features?. (arXiv:2308.11490v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11490","description":"<p>Automatically disentangling an author's style from the content of their\nwriting is a longstanding and possibly insurmountable problem in computational\nlinguistics. At the same time, the availability of large text corpora furnished\nwith author labels has recently enabled learning authorship representations in\na purely data-driven manner for authorship attribution, a task that ostensibly\ndepends to a greater extent on encoding writing style than encoding content.\nHowever, success on this surrogate task does not ensure that such\nrepresentations capture writing style since authorship could also be correlated\nwith other latent variables, such as topic. In an effort to better understand\nthe nature of the information these representations convey, and specifically to\nvalidate the hypothesis that they chiefly encode writing style, we\nsystematically probe these representations through a series of targeted\nexperiments. The results of these experiments suggest that representations\nlearned for the surrogate authorship prediction task are indeed sensitive to\nwriting style. As a consequence, authorship representations may be expected to\nbe robust to certain kinds of data shift, such as topic drift over time.\nAdditionally, our findings may open the door to downstream applications that\nrequire stylistic representations, such as style transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Andrew Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggazzotti_C/0/1/0/all/0/1\">Cristina Aggazzotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotula_R/0/1/0/all/0/1\">Rebecca Kotula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_R/0/1/0/all/0/1\">Rafael Rivera Soto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bishop_M/0/1/0/all/0/1\">Marcus Bishop</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andrews_N/0/1/0/all/0/1\">Nicholas Andrews</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Prototype Adapter for Vision-Language Models. (arXiv:2308.11507v1 [cs.CV])","link":"http://arxiv.org/abs/2308.11507","description":"<p>Recently, large-scale pre-trained vision-language models (e.g. CLIP and\nALIGN) have demonstrated remarkable effectiveness in acquiring transferable\nvisual representations. To leverage the valuable knowledge encoded within these\nmodels for downstream tasks, several fine-tuning approaches, including prompt\ntuning methods and adapter-based methods, have been developed to adapt\nvision-language models effectively with supervision. However, these methods\nrely on the availability of annotated samples, which can be labor-intensive and\ntime-consuming to acquire, thus limiting scalability. To address this issue, in\nthis work, we design an unsupervised fine-tuning approach for vision-language\nmodels called Unsupervised Prototype Adapter (UP-Adapter). Specifically, for\nthe unannotated target datasets, we leverage the text-image aligning capability\nof CLIP to automatically select the most confident samples for each class.\nUtilizing these selected samples, we generate class prototypes, which serve as\nthe initialization for the learnable prototype model. After fine-tuning, the\nprototype model prediction is combined with the original CLIP's prediction by a\nresidual connection to perform downstream recognition tasks. Our extensive\nexperimental results on image recognition and domain generalization show that\nthe proposed unsupervised method outperforms 8-shot CoOp, 8-shot Tip-Adapter,\nand also the state-of-the-art UPL method by large margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xueting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhihai He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing Multi-Class Text Classification: A Diverse Stacking Ensemble Framework Utilizing Transformers. (arXiv:2308.11519v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11519","description":"<p>Customer reviews play a crucial role in assessing customer satisfaction,\ngathering feedback, and driving improvements for businesses. Analyzing these\nreviews provides valuable insights into customer sentiments, including\ncompliments, comments, and suggestions. Text classification techniques enable\nbusinesses to categorize customer reviews into distinct categories,\nfacilitating a better understanding of customer feedback. However, challenges\nsuch as overfitting and bias limit the effectiveness of a single classifier in\nensuring optimal prediction. This study proposes a novel approach to address\nthese challenges by introducing a stacking ensemble-based multi-text\nclassification method that leverages transformer models. By combining multiple\nsingle transformers, including BERT, ELECTRA, and DistilBERT, as base-level\nclassifiers, and a meta-level classifier based on RoBERTa, an optimal\npredictive model is generated. The proposed stacking ensemble-based multi-text\nclassification method aims to enhance the accuracy and robustness of customer\nreview analysis. Experimental evaluations conducted on a real-world customer\nreview dataset demonstrate the effectiveness and superiority of the proposed\napproach over traditional single classifier models. The stacking ensemble-based\nmulti-text classification method using transformers proves to be a promising\nsolution for businesses seeking to extract valuable insights from customer\nreviews and make data-driven decisions to enhance customer satisfaction and\ndrive continuous improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_A/0/1/0/all/0/1\">Anusuya Krishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Power of Topic Modeling Techniques in Analyzing Customer Reviews: A Comparative Analysis. (arXiv:2308.11520v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11520","description":"<p>The exponential growth of online social network platforms and applications\nhas led to a staggering volume of user-generated textual content, including\ncomments and reviews. Consequently, users often face difficulties in extracting\nvaluable insights or relevant information from such content. To address this\nchallenge, machine learning and natural language processing algorithms have\nbeen deployed to analyze the vast amount of textual data available online. In\nrecent years, topic modeling techniques have gained significant popularity in\nthis domain. In this study, we comprehensively examine and compare five\nfrequently used topic modeling methods specifically applied to customer\nreviews. The methods under investigation are latent semantic analysis (LSA),\nlatent Dirichlet allocation (LDA), non-negative matrix factorization (NMF),\npachinko allocation model (PAM), Top2Vec, and BERTopic. By practically\ndemonstrating their benefits in detecting important topics, we aim to highlight\ntheir efficacy in real-world scenarios. To evaluate the performance of these\ntopic modeling methods, we carefully select two textual datasets. The\nevaluation is based on standard statistical evaluation metrics such as topic\ncoherence score. Our findings reveal that BERTopic consistently yield more\nmeaningful extracted topics and achieve favorable results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_A/0/1/0/all/0/1\">Anusuya Krishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models. (arXiv:2308.11521v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11521","description":"<p>Large language models (LLMs), such as ChatGPT, have emerged with astonishing\ncapabilities approaching artificial general intelligence. While providing\nconvenience for various societal needs, LLMs have also lowered the cost of\ngenerating harmful content. Consequently, LLM developers have deployed\nsemantic-level defenses to recognize and reject prompts that may lead to\ninappropriate content. Unfortunately, these defenses are not foolproof, and\nsome attackers have crafted \"jailbreak\" prompts that temporarily hypnotize the\nLLM into forgetting content defense rules and answering any improper questions.\nTo date, there is no clear explanation of the principles behind these\nsemantic-level attacks and defenses in both industry and academia.\n</p>\n<p>This paper investigates the LLM jailbreak problem and proposes an automatic\njailbreak method for the first time. We propose the concept of a semantic\nfirewall and provide three technical implementation approaches. Inspired by the\nattack that penetrates traditional firewalls through reverse tunnels, we\nintroduce a \"self-deception\" attack that can bypass the semantic firewall by\ninducing LLM to generate prompts that facilitate jailbreak. We generated a\ntotal of 2,520 attack payloads in six languages (English, Russian, French,\nSpanish, Chinese, and Arabic) across seven virtual scenarios, targeting the\nthree most common types of violations: violence, hate, and pornography. The\nexperiment was conducted on two models, namely the GPT-3.5-Turbo and GPT-4. The\nsuccess rates on the two models were 86.2% and 67%, while the failure rates\nwere 4.7% and 2.2%, respectively. This highlighted the effectiveness of the\nproposed attack method. All experimental code and raw data will be released as\nopen-source to inspire future research. We believe that manipulating AI\nbehavior through carefully crafted prompts will become an important research\ndirection in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Wei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baosheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_Z/0/1/0/all/0/1\">Zhiwen Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1\">Enze Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Representations on Logs for AIOps. (arXiv:2308.11526v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11526","description":"<p>AI for IT Operations (AIOps) is a powerful platform that Site Reliability\nEngineers (SREs) use to automate and streamline operational workflows with\nminimal human intervention. Automated log analysis is a critical task in AIOps\nas it provides key insights for SREs to identify and address ongoing faults.\nTasks such as log format detection, log classification, and log parsing are key\ncomponents of automated log analysis. Most of these tasks require supervised\nlearning; however, there are multiple challenges due to limited labelled log\ndata and the diverse nature of log data. Large Language Models (LLMs) such as\nBERT and GPT3 are trained using self-supervision on a vast amount of unlabeled\ndata. These models provide generalized representations that can be effectively\nused for various downstream tasks with limited labelled data. Motivated by the\nsuccess of LLMs in specific domains like science and biology, this paper\nintroduces a LLM for log data which is trained on public and proprietary log\ndata. The results of our experiments demonstrate that the proposed LLM\noutperforms existing models on multiple downstream tasks. In summary, AIOps\npowered by LLMs offers an efficient and effective solution for automating log\nanalysis tasks and enabling SREs to focus on higher-level tasks. Our proposed\nLLM, trained on public and proprietary log data, offers superior performance on\nmultiple downstream tasks, making it a valuable addition to the AIOps platform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Pranjal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_H/0/1/0/all/0/1\">Harshit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_D/0/1/0/all/0/1\">Debanjana Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhukar_K/0/1/0/all/0/1\">Karan Bhukar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_P/0/1/0/all/0/1\">Pooja Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohapatra_P/0/1/0/all/0/1\">Prateeti Mohapatra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction. (arXiv:2308.11527v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11527","description":"<p>Although deep pre-trained language models have shown promising benefit in a\nlarge set of industrial scenarios, including Click-Through-Rate (CTR)\nprediction, how to integrate pre-trained language models that handle only\ntextual signals into a prediction pipeline with non-textual features is\nchallenging.\n</p>\n<p>Up to now two directions have been explored to integrate multi-modal inputs\nin fine-tuning of pre-trained language models. One consists of fusing the\noutcome of language models and non-textual features through an aggregation\nlayer, resulting into ensemble framework, where the cross-information between\ntextual and non-textual inputs are only learned in the aggregation layer. The\nsecond one consists of splitting non-textual features into fine-grained\nfragments and transforming the fragments to new tokens combined with textual\nones, so that they can be fed directly to transformer layers in language\nmodels. However, this approach increases the complexity of the learning and\ninference because of the numerous additional tokens.\n</p>\n<p>To address these limitations, we propose in this work a novel framework\nBERT4CTR, with the Uni-Attention mechanism that can benefit from the\ninteractions between non-textual and textual features while maintaining low\ntime-costs in training and inference through a dimensionality reduction.\nComprehensive experiments on both public and commercial data demonstrate that\nBERT4CTR can outperform significantly the state-of-the-art frameworks to handle\nmulti-modal inputs and be applicable to CTR prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salamatian_K/0/1/0/all/0/1\">Kav&#xe9; Salamatian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yunqing Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Weiwei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhiang_Q/0/1/0/all/0/1\">Qi Zhiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empowering Refugee Claimants and their Lawyers: Using Machine Learning to Examine Decision-Making in Refugee Law. (arXiv:2308.11531v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11531","description":"<p>Our project aims at helping and supporting stakeholders in refugee status\nadjudications, such as lawyers, judges, governing bodies, and claimants, in\norder to make better decisions through data-driven intelligence and increase\nthe understanding and transparency of the refugee application process for all\ninvolved parties. This PhD project has two primary objectives: (1) to retrieve\npast cases, and (2) to analyze legal decision-making processes on a dataset of\nCanadian cases. In this paper, we present the current state of our work, which\nincludes a completed experiment on part (1) and ongoing efforts related to part\n(2). We believe that NLP-based solutions are well-suited to address these\nchallenges, and we investigate the feasibility of automating all steps\ninvolved. In addition, we introduce a novel benchmark for future NLP research\nin refugee law. Our methodology aims to be inclusive to all end-users and\nstakeholders, with expected benefits including reduced time-to-decision, fairer\nand more transparent outcomes, and improved decision quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barale_C/0/1/0/all/0/1\">Claire Barale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Model as a User Simulator. (arXiv:2308.11534v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11534","description":"<p>The unparalleled performance of closed-sourced ChatGPT has sparked efforts\ntowards its democratization, with notable strides made by leveraging real user\nand ChatGPT conversations, as evidenced by Vicuna. However, while current\nendeavors like Baize and UltraChat aim to auto-generate conversational data due\nto challenges in gathering human participation, they primarily rely on ChatGPT\nto simulate human behaviors based on directives rather than genuine human\nlearning. This results in a limited scope, diminished diversity, and an absence\nof genuine multi-round conversational dynamics. To address the above issues, we\ninnovatively target human questions extracted from genuine human-machine\nconversations as a learning goal and train a user simulator, UserGPT, to\nproduce a high-quality human-centric synthetic conversation dataset, RealChat.\nSubsequently, this dataset trains our assistant model, ReaLM. Experimentally,\nReaLM outpaces baseline models in both Vicuna-Bench and MT-Bench by pairwise\ncomparison when considering equivalent training set sizes, and manual\nevaluation also shows that our model is highly competitive. Impressively, when\nfine-tuned with the latest LLaMA 2 model, ReaLM secured a leading score of 6.33\nin the MT-Bench, outshining the contemporary same-scale models, including the\nLLaMA-2-7B-chat model. Further in-depth analysis demonstrates the scalability\nand transferability of our approach. A preliminary exploration into the\ninterplay between training set data quality and resultant model performance is\nalso undertaken, laying a robust groundwork for future investigations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Chuyi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yaxin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1\">Feng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benyou Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BELB: a Biomedical Entity Linking Benchmark. (arXiv:2308.11537v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11537","description":"<p>Biomedical entity linking (BEL) is the task of grounding entity mentions to a\nknowledge base. It plays a vital role in information extraction pipelines for\nthe life sciences literature. We review recent work in the field and find that,\nas the task is absent from existing benchmarks for biomedical text mining,\ndifferent studies adopt different experimental setups making comparisons based\non published numbers problematic. Furthermore, neural systems are tested\nprimarily on instances linked to the broad coverage knowledge base UMLS,\nleaving their performance to more specialized ones, e.g. genes or variants,\nunderstudied. We therefore developed BELB, a Biomedical Entity Linking\nBenchmark, providing access in a unified format to 11 corpora linked to 7\nknowledge bases and spanning six entity types: gene, disease, chemical,\nspecies, cell line and variant. BELB greatly reduces preprocessing overhead in\ntesting BEL systems on multiple corpora offering a standardized testbed for\nreproducible experiments. Using BELB we perform an extensive evaluation of six\nrule-based entity-specific systems and three recent neural approaches\nleveraging pre-trained language models. Our results reveal a mixed picture\nshowing that neural approaches fail to perform consistently across entity\ntypes, highlighting the need of further studies towards entity-agnostic models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garda_S/0/1/0/all/0/1\">Samuele Garda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_Genzel_L/0/1/0/all/0/1\">Leon Weber-Genzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_R/0/1/0/all/0/1\">Robert Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leser_U/0/1/0/all/0/1\">Ulf Leser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using ChatGPT as a CAT tool in Easy Language translation. (arXiv:2308.11563v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11563","description":"<p>This study sets out to investigate the feasibility of using ChatGPT to\ntranslate citizen-oriented administrative texts into German Easy Language, a\nsimplified, controlled language variety that is adapted to the needs of people\nwith reading impairments. We use ChatGPT to translate selected texts from\nwebsites of German public authorities using two strategies, i.e. linguistic and\nholistic. We analyse the quality of the generated texts based on different\ncriteria, such as correctness, readability, and syntactic complexity. The\nresults indicated that the generated texts are easier than the standard texts,\nbut that they still do not fully meet the established Easy Language standards.\nAdditionally, the content is not always rendered correctly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deilen_S/0/1/0/all/0/1\">Silvana Deilen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrido_S/0/1/0/all/0/1\">Sergio Hern&#xe1;ndez Garrido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapshinova_Koltunski_E/0/1/0/all/0/1\">Ekaterina Lapshinova-Koltunski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maass_C/0/1/0/all/0/1\">Christiane Maa&#xdf;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models. (arXiv:2308.11578v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11578","description":"<p>After the inception of emotion recognition or affective computing, it has\nincreasingly become an active research topic due to its broad applications.\nOver the past couple of decades, emotion recognition models have gradually\nmigrated from statistically shallow models to neural network-based deep models,\nwhich can significantly boost the performance of emotion recognition models and\nconsistently achieve the best results on different benchmarks. Therefore, in\nrecent years, deep models have always been considered the first option for\nemotion recognition. However, the debut of large language models (LLMs), such\nas ChatGPT, has remarkably astonished the world due to their emerged\ncapabilities of zero/few-shot learning, in-context learning, chain-of-thought,\nand others that are never shown in previous deep models. In the present paper,\nwe comprehensively investigate how the LLMs perform in emotion recognition in\nterms of diverse aspects, including in-context learning, few-short learning,\naccuracy, generalisation, and explanation. Moreover, we offer some insights and\npose other potential challenges, hoping to ignite broader discussions about\nenhancing emotion recognition in the new era of advanced and generalised large\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zixing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Liyizhe Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Huan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bjorn W. Schuller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Emotional Support Chatbots in the Era of LLMs. (arXiv:2308.11584v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11584","description":"<p>The integration of emotional support into various conversational scenarios\npresents profound societal benefits, such as social interactions, mental health\ncounseling, and customer service. However, there are unsolved challenges that\nhinder real-world applications in this field, including limited data\navailability and the absence of well-accepted model training paradigms. This\nwork endeavors to navigate these challenges by harnessing the capabilities of\nLarge Language Models (LLMs). We introduce an innovative methodology that\nsynthesizes human insights with the computational prowess of LLMs to curate an\nextensive emotional support dialogue dataset. Our approach is initiated with a\nmeticulously designed set of dialogues spanning diverse scenarios as generative\nseeds. By utilizing the in-context learning potential of ChatGPT, we\nrecursively generate an ExTensible Emotional Support dialogue dataset, named\nExTES. Following this, we deploy advanced tuning techniques on the LLaMA model,\nexamining the impact of diverse training strategies, ultimately yielding an LLM\nmeticulously optimized for emotional support interactions. An exhaustive\nassessment of the resultant model showcases its proficiency in offering\nemotional support, marking a pivotal step in the realm of emotional support\nbots and paving the way for subsequent research and implementations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhonghua Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Lizi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Intersectionality and Dual Form of Gradient Descent for Multimodal Analysis: a Case Study on Hateful Memes. (arXiv:2308.11585v1 [cs.AI])","link":"http://arxiv.org/abs/2308.11585","description":"<p>In the wake of the explosive growth of machine learning (ML) usage,\nparticularly within the context of emerging Large Language Models (LLMs),\ncomprehending the semantic significance rooted in their internal workings is\ncrucial. While causal analyses focus on defining semantics and its\nquantification, the gradient-based approach is central to explainable AI (XAI),\ntackling the interpretation of the black box. By synergizing these approaches,\nthe exploration of how a model's internal mechanisms illuminate its causal\neffect has become integral for evidence-based decision-making. A parallel line\nof research has revealed that intersectionality - the combinatory impact of\nmultiple demographics of an individual - can be structured in the form of an\nAveraged Treatment Effect (ATE). Initially, this study illustrates that the\nhateful memes detection problem can be formulated as an ATE, assisted by the\nprinciples of intersectionality, and that a modality-wise summarization of\ngradient-based attention attribution scores can delineate the distinct\nbehaviors of three Transformerbased models concerning ATE. Subsequently, we\nshow that the latest LLM LLaMA2 has the ability to disentangle the\nintersectional nature of memes detection in an in-context learning setting,\nwith their mechanistic properties elucidated via meta-gradient, a secondary\nform of gradient. In conclusion, this research contributes to the ongoing\ndialogue surrounding XAI and the multifaceted nature of ML models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miyanishi_Y/0/1/0/all/0/1\">Yosuke Miyanishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh Le Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Indonesian Automatic Speech Recognition with XLSR-53. (arXiv:2308.11589v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11589","description":"<p>This study focuses on the development of Indonesian Automatic Speech\nRecognition (ASR) using the XLSR-53 pre-trained model, the XLSR stands for\ncross-lingual speech representations. The use of this XLSR-53 pre-trained model\nis to significantly reduce the amount of training data in non-English languages\nrequired to achieve a competitive Word Error Rate (WER). The total amount of\ndata used in this study is 24 hours, 18 minutes, and 1 second: (1) TITML-IDN 14\nhours and 31 minutes; (2) Magic Data 3 hours and 33 minutes; and (3) Common\nVoice 6 hours, 14 minutes, and 1 second. With a WER of 20%, the model built in\nthis study can compete with similar models using the Common Voice dataset split\ntest. WER can be decreased by around 8% using a language model, resulted in WER\nfrom 20% to 12%. Thus, the results of this study have succeeded in perfecting\nprevious research in contributing to the creation of a better Indonesian ASR\nwith a smaller amount of data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arisaputra_P/0/1/0/all/0/1\">Panji Arisaputra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zahra_A/0/1/0/all/0/1\">Amalia Zahra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding. (arXiv:2308.11592v1 [cs.AI])","link":"http://arxiv.org/abs/2308.11592","description":"<p>In the era of Large Language Models (LLMs), tremendous strides have been made\nin the field of multimodal understanding. However, existing advanced algorithms\nare limited to effectively utilizing the immense representation capabilities\nand rich world knowledge inherent to these large pre-trained models, and the\nbeneficial connections among tasks within the context of text-rich scenarios\nhave not been sufficiently explored. In this work, we introduce UniDoc, a novel\nmultimodal model equipped with text detection and recognition capabilities,\nwhich are deficient in existing approaches. Moreover, UniDoc capitalizes on the\nbeneficial interactions among tasks to enhance the performance of each\nindividual task. To implement UniDoc, we perform unified multimodal instruct\ntuning on the contributed large-scale instruction following datasets.\nQuantitative and qualitative experimental results show that UniDoc sets\nstate-of-the-art scores across multiple challenging benchmarks. To the best of\nour knowledge, this is the first large multimodal model capable of simultaneous\ntext detection, recognition, spotting, and understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Hao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jingqun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jinghui Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Can Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeamlessM4T-Massively Multilingual & Multimodal Machine Translation. (arXiv:2308.11596v1 [cs.CL])","link":"http://arxiv.org/abs/2308.11596","description":"<p>What does it take to create the Babel Fish, a tool that can help individuals\ntranslate speech between any two languages? While recent breakthroughs in\ntext-based models have pushed machine translation coverage beyond 200\nlanguages, unified speech-to-speech translation models have yet to achieve\nsimilar strides. More specifically, conventional speech-to-speech translation\nsystems rely on cascaded systems that perform translation progressively,\nputting high-performing unified systems out of reach. To address these gaps, we\nintroduce SeamlessM4T, a single model that supports speech-to-speech\ntranslation, speech-to-text translation, text-to-speech translation,\ntext-to-text translation, and automatic speech recognition for up to 100\nlanguages. To build this, we used 1 million hours of open speech audio data to\nlearn self-supervised speech representations with w2v-BERT 2.0. Subsequently,\nwe created a multimodal corpus of automatically aligned speech translations.\nFiltered and combined with human-labeled and pseudo-labeled data, we developed\nthe first multilingual system capable of translating from and into English for\nboth speech and text. On FLEURS, SeamlessM4T sets a new standard for\ntranslations into multiple target languages, achieving an improvement of 20%\nBLEU over the previous SOTA in direct speech-to-text translation. Compared to\nstrong cascaded models, SeamlessM4T improves the quality of into-English\ntranslation by 1.3 BLEU points in speech-to-text and by 2.6 ASR-BLEU points in\nspeech-to-speech. Tested for robustness, our system performs better against\nbackground noises and speaker variations in speech-to-text tasks compared to\nthe current SOTA model. Critically, we evaluated SeamlessM4T on gender bias and\nadded toxicity to assess translation safety. Finally, all contributions in this\nwork are open-sourced at this https\nhttps://github.com/facebookresearch/seamless_communication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Communication_S/0/1/0/all/0/1\">Seamless Communication</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrault_L/0/1/0/all/0/1\">Lo&#xef;c Barrault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1\">Yu-An Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meglioli_M/0/1/0/all/0/1\">Mariano Cora Meglioli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dale_D/0/1/0/all/0/1\">David Dale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1\">Ning Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duquenne_P/0/1/0/all/0/1\">Paul-Ambroise Duquenne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsahar_H/0/1/0/all/0/1\">Hady Elsahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heffernan_K/0/1/0/all/0/1\">Kevin Heffernan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1\">John Hoffman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klaiber_C/0/1/0/all/0/1\">Christopher Klaiber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pengwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Licht_D/0/1/0/all/0/1\">Daniel Licht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maillard_J/0/1/0/all/0/1\">Jean Maillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakotoarison_A/0/1/0/all/0/1\">Alice Rakotoarison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadagopan_K/0/1/0/all/0/1\">Kaushik Ram Sadagopan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenzek_G/0/1/0/all/0/1\">Guillaume Wenzek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_E/0/1/0/all/0/1\">Ethan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akula_B/0/1/0/all/0/1\">Bapi Akula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng-Jen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hachem_N/0/1/0/all/0/1\">Naji El Hachem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellis_B/0/1/0/all/0/1\">Brian Ellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_G/0/1/0/all/0/1\">Gabriel Mejia Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haaheim_J/0/1/0/all/0/1\">Justin Haaheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansanti_P/0/1/0/all/0/1\">Prangthip Hansanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howes_R/0/1/0/all/0/1\">Russ Howes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Bernie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_M/0/1/0/all/0/1\">Min-Jae Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Somya Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalbassi_E/0/1/0/all/0/1\">Elahe Kalbassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kallet_A/0/1/0/all/0/1\">Amanda Kallet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulikov_I/0/1/0/all/0/1\">Ilia Kulikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_J/0/1/0/all/0/1\">Janice Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Daniel Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xutai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavlyutov_R/0/1/0/all/0/1\">Ruslan Mavlyutov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peloquin_B/0/1/0/all/0/1\">Benjamin Peloquin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramadan_M/0/1/0/all/0/1\">Mohamed Ramadan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1\">Abinesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Anna Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Kevin Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Tuan Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tufanov_I/0/1/0/all/0/1\">Igor Tufanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogeti_V/0/1/0/all/0/1\">Vish Vogeti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wood_C/0/1/0/all/0/1\">Carleigh Wood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yilin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bokai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andrews_P/0/1/0/all/0/1\">Pierre Andrews</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balioglu_C/0/1/0/all/0/1\">Can Balioglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>, et al. (16 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tryage: Real-time, intelligent Routing of User Prompts to Large Language Model. (arXiv:2308.11601v1 [cs.LG])","link":"http://arxiv.org/abs/2308.11601","description":"<p>The introduction of the transformer architecture and the self-attention\nmechanism has led to an explosive production of language models trained on\nspecific downstream tasks and data domains. With over 200, 000 models in the\nHugging Face ecosystem, users grapple with selecting and optimizing models to\nsuit multifaceted workflows and data domains while addressing computational,\nsecurity, and recency concerns. There is an urgent need for machine learning\nframeworks that can eliminate the burden of model selection and customization\nand unleash the incredible power of the vast emerging model library for end\nusers. Here, we propose a context-aware routing system, Tryage, that leverages\na language model router for optimal selection of expert models from a model\nlibrary based on analysis of individual input prompts. Inspired by the thalamic\nrouter in the brain, Tryage employs a perceptive router to predict down-stream\nmodel performance on prompts and, then, makes a routing decision using an\nobjective function that integrates performance predictions with user goals and\nconstraints that are incorporated through flags (e.g., model size, model\nrecency). Tryage allows users to explore a Pareto front and automatically\ntrade-off between task accuracy and secondary goals including minimization of\nmodel size, recency, security, verbosity, and readability. Across heterogeneous\ndata sets that include code, text, clinical data, and patents, the Tryage\nframework surpasses Gorilla and GPT3.5 turbo in dynamic model selection\nidentifying the optimal model with an accuracy of 50.9% , compared to 23.6% by\nGPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates how\nrouting models can be applied to program and control the behavior of\nmulti-model LLM systems to maximize efficient use of the expanding and evolving\nlanguage model ecosystem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hari_S/0/1/0/all/0/1\">Surya Narayanan Hari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomson_M/0/1/0/all/0/1\">Matt Thomson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StoryBench: A Multifaceted Benchmark for Continuous Story Visualization. (arXiv:2308.11606v1 [cs.CV])","link":"http://arxiv.org/abs/2308.11606","description":"<p>Generating video stories from text prompts is a complex task. In addition to\nhaving high visual quality, videos need to realistically adhere to a sequence\nof text prompts whilst being consistent throughout the frames. Creating a\nbenchmark for video generation requires data annotated over time, which\ncontrasts with the single caption used often in video datasets. To fill this\ngap, we collect comprehensive human annotations on three existing datasets, and\nintroduce StoryBench: a new, challenging multi-task benchmark to reliably\nevaluate forthcoming text-to-video models. Our benchmark includes three video\ngeneration tasks of increasing difficulty: action execution, where the next\naction must be generated starting from a conditioning video; story\ncontinuation, where a sequence of actions must be executed starting from a\nconditioning video; and story generation, where a video must be generated from\nonly text prompts. We evaluate small yet strong text-to-video baselines, and\nshow the benefits of training on story-like data algorithmically generated from\nexisting video captions. Finally, we establish guidelines for human evaluation\nof video stories, and reaffirm the need of better automatic metrics for video\ngeneration. StoryBench aims at encouraging future research efforts in this\nexciting new area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1\">Emanuele Bugliarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moraldo_H/0/1/0/all/0/1\">Hernan Moraldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_R/0/1/0/all/0/1\">Ruben Villegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babaeizadeh_M/0/1/0/all/0/1\">Mohammad Babaeizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffar_M/0/1/0/all/0/1\">Mohammad Taghi Saffar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erhan_D/0/1/0/all/0/1\">Dumitru Erhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kindermans_P/0/1/0/all/0/1\">Pieter-Jan Kindermans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voigtlaender_P/0/1/0/all/0/1\">Paul Voigtlaender</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Social Biases in Grounded Vision and Language Embeddings. (arXiv:2002.08911v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2002.08911","description":"<p>We generalize the notion of social biases from language embeddings to\ngrounded vision and language embeddings. Biases are present in grounded\nembeddings, and indeed seem to be equally or more significant than for\nungrounded embeddings. This is despite the fact that vision and language can\nsuffer from different biases, which one might hope could attenuate the biases\nin both. Multiple ways exist to generalize metrics measuring bias in word\nembeddings to this new setting. We introduce the space of generalizations\n(Grounded-WEAT and Grounded-SEAT) and demonstrate that three generalizations\nanswer different yet important questions about how biases, language, and vision\ninteract. These metrics are used on a new dataset, the first for grounded bias,\ncreated by augmenting extending standard linguistic bias benchmarks with 10,228\nimages from COCO, Conceptual Captions, and Google Images. Dataset construction\nis challenging because vision datasets are themselves very biased. The presence\nof these biases in systems will begin to have real-world consequences as they\nare deployed, making carefully measuring bias and then mitigating it critical\nto building a fair society.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ross_C/0/1/0/all/0/1\">Candace Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_B/0/1/0/all/0/1\">Boris Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbu_A/0/1/0/all/0/1\">Andrei Barbu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Measure-Theoretic Characterization of Tight Language Models. (arXiv:2212.10502v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10502","description":"<p>Language modeling, a central task in natural language processing, involves\nestimating a probability distribution over strings. In most cases, the\nestimated distribution sums to 1 over all finite strings. However, in some\npathological cases, probability mass can ``leak'' onto the set of infinite\nsequences. In order to characterize the notion of leakage more precisely, this\npaper offers a measure-theoretic treatment of language modeling. We prove that\nmany popular language model families are in fact tight, meaning that they will\nnot leak in this sense. We also generalize characterizations of tightness\nproposed in previous works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Li Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennigen_L/0/1/0/all/0/1\">Lucas Torroba Hennigen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1\">Jason Eisner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Truveta Mapper: A Zero-shot Ontology Alignment Framework. (arXiv:2301.09767v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2301.09767","description":"<p>In this paper, a new perspective is suggested for unsupervised Ontology\nMatching (OM) or Ontology Alignment (OA) by treating it as a translation task.\nOntologies are represented as graphs, and the translation is performed from a\nnode in the source ontology graph to a path in the target ontology graph. The\nproposed framework, Truveta Mapper (TM), leverages a multi-task\nsequence-to-sequence transformer model to perform alignment across multiple\nontologies in a zero-shot, unified and end-to-end manner. Multi-tasking enables\nthe model to implicitly learn the relationship between different ontologies via\ntransfer-learning without requiring any explicit cross-ontology manually\nlabeled data. This also enables the formulated framework to outperform existing\nsolutions for both runtime latency and alignment quality. The model is\npre-trained and fine-tuned only on publicly available text corpus and\ninner-ontologies data. The proposed solution outperforms state-of-the-art\napproaches, Edit-Similarity, LogMap, AML, BERTMap, and the recently presented\nnew OM frameworks in Ontology Alignment Evaluation Initiative (OAEI22), offers\nlog-linear complexity, and overall makes the OM task efficient and more\nstraightforward without much post-processing involving mapping extension or\nmapping repair. We are open sourcing our solution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amir_M/0/1/0/all/0/1\">Mariyam Amir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baruah_M/0/1/0/all/0/1\">Murchana Baruah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eslamialishah_M/0/1/0/all/0/1\">Mahsa Eslamialishah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsani_S/0/1/0/all/0/1\">Sina Ehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahramali_A/0/1/0/all/0/1\">Alireza Bahramali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naddaf_Sh_S/0/1/0/all/0/1\">Sadra Naddaf-Sh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarandioon_S/0/1/0/all/0/1\">Saman Zarandioon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrojText: Test-time Invisible Textual Trojan Insertion. (arXiv:2303.02242v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.02242","description":"<p>In Natural Language Processing (NLP), intelligent neuron models can be\nsusceptible to textual Trojan attacks. Such attacks occur when Trojan models\nbehave normally for standard inputs but generate malicious output for inputs\nthat contain a specific trigger. Syntactic-structure triggers, which are\ninvisible, are becoming more popular for Trojan attacks because they are\ndifficult to detect and defend against. However, these types of attacks require\na large corpus of training data to generate poisoned samples with the necessary\nsyntactic structures for Trojan insertion. Obtaining such data can be difficult\nfor attackers, and the process of generating syntactic poisoned triggers and\ninserting Trojans can be time-consuming. This paper proposes a solution called\nTrojText, which aims to determine whether invisible textual Trojan attacks can\nbe performed more efficiently and cost-effectively without training data. The\nproposed approach, called the Representation-Logit Trojan Insertion (RLI)\nalgorithm, uses smaller sampled test data instead of large training data to\nachieve the desired attack. The paper also introduces two additional\ntechniques, namely the accumulated gradient ranking (AGR) and Trojan Weights\nPruning (TWP), to reduce the number of tuned parameters and the attack\noverhead. The TrojText approach was evaluated on three datasets (AG's News,\nSST-2, and OLID) using three NLP models (BERT, XLNet, and DeBERTa). The\nexperiments demonstrated that the TrojText approach achieved a 98.35\\%\nclassification accuracy for test sentences in the target class on the BERT\nmodel for the AG's News dataset. The source code for TrojText is available at\nhttps://github.com/UCF-ML-Research/TrojText.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_Q/0/1/0/all/0/1\">Qian Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yepeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1\">Bo Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models. (arXiv:2304.01852v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01852","description":"<p>This paper presents a comprehensive survey of ChatGPT-related (GPT-3.5 and\nGPT-4) research, state-of-the-art large language models (LLM) from the GPT\nseries, and their prospective applications across diverse domains. Indeed, key\ninnovations such as large-scale pre-training that captures knowledge across the\nentire world wide web, instruction fine-tuning and Reinforcement Learning from\nHuman Feedback (RLHF) have played significant roles in enhancing LLMs'\nadaptability and performance. We performed an in-depth analysis of 194 relevant\npapers on arXiv, encompassing trend analysis, word cloud representation, and\ndistribution analysis across various application domains. The findings reveal a\nsignificant and increasing interest in ChatGPT-related research, predominantly\ncentered on direct natural language processing applications, while also\ndemonstrating considerable potential in areas ranging from education and\nhistory to mathematics, medicine, and physics. This study endeavors to furnish\ninsights into ChatGPT's capabilities, potential implications, ethical concerns,\nand offer direction for future advancements in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tianle Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Siyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiayue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuanyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jiaming Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Antong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mengshen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiang_N/0/1/0/all/0/1\">Ning Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dingang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_B/0/1/0/all/0/1\">Bao Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fairness in Image Search: A Study of Occupational Stereotyping in Image Retrieval and its Debiasing. (arXiv:2305.03881v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2305.03881","description":"<p>Multi-modal search engines have experienced significant growth and widespread\nuse in recent years, making them the second most common internet use. While\nsearch engine systems offer a range of services, the image search field has\nrecently become a focal point in the information retrieval community, as the\nadage goes, \"a picture is worth a thousand words\". Although popular search\nengines like Google excel at image search accuracy and agility, there is an\nongoing debate over whether their search results can be biased in terms of\ngender, language, demographics, socio-cultural aspects, and stereotypes. This\npotential for bias can have a significant impact on individuals' perceptions\nand influence their perspectives.\n</p>\n<p>In this paper, we present our study on bias and fairness in web search, with\na focus on keyword-based image search. We first discuss several kinds of biases\nthat exist in search systems and why it is important to mitigate them. We\nnarrow down our study to assessing and mitigating occupational stereotypes in\nimage search, which is a prevalent fairness issue in image retrieval. For the\nassessment of stereotypes, we take gender as an indicator. We explore various\nopen-source and proprietary APIs for gender identification from images. With\nthese, we examine the extent of gender bias in top-tanked image search results\nobtained for several occupational keywords. To mitigate the bias, we then\npropose a fairness-aware re-ranking algorithm that optimizes (a) relevance of\nthe search result with the keyword and (b) fairness w.r.t genders identified.\nWe experiment on 100 top-ranked images obtained for 10 occupational keywords\nand consider random re-ranking and re-ranking based on relevance as baselines.\nOur experimental results show that the fairness-aware re-ranking algorithm\nproduces rankings with better fairness scores and competitive relevance scores\nthan the baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dash_S/0/1/0/all/0/1\">Swagatika Dash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NollySenti: Leveraging Transfer Learning and Machine Translation for Nigerian Movie Sentiment Classification. (arXiv:2305.10971v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10971","description":"<p>Africa has over 2000 indigenous languages but they are under-represented in\nNLP research due to lack of datasets. In recent years, there have been progress\nin developing labeled corpora for African languages. However, they are often\navailable in a single domain and may not generalize to other domains. In this\npaper, we focus on the task of sentiment classification for cross domain\nadaptation. We create a new dataset, NollySenti - based on the Nollywood movie\nreviews for five languages widely spoken in Nigeria (English, Hausa, Igbo,\nNigerian-Pidgin, and Yoruba. We provide an extensive empirical evaluation using\nclassical machine learning methods and pre-trained language models. Leveraging\ntransfer learning, we compare the performance of cross-domain adaptation from\nTwitter domain, and cross-lingual adaptation from English language. Our\nevaluation shows that transfer from English in the same target domain leads to\nmore than 5% improvement in accuracy compared to transfer from Twitter in the\nsame language. To further mitigate the domain difference, we leverage machine\ntranslation (MT) from English to other Nigerian languages, which leads to a\nfurther improvement of 7% over cross-lingual evaluation. While MT to\nlow-resource languages are often of low quality, through human evaluation, we\nshow that most of the translated sentences preserve the sentiment of the\noriginal English reviews.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shode_I/0/1/0/all/0/1\">Iyanuoluwa Shode</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jing Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_A/0/1/0/all/0/1\">Anna Feldman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blockwise Parallel Transformer for Long Context Large Models. (arXiv:2305.19370v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.19370","description":"<p>Transformers have emerged as the cornerstone of state-of-the-art natural\nlanguage processing models, showcasing exceptional performance across a wide\nrange of AI applications. However, the memory demands posed by the\nself-attention mechanism and the large feedforward network in Transformers\nlimit their ability to handle long sequences, thereby creating challenges for\ntasks involving multiple long sequences or long-term dependencies. We present a\ndistinct approach, Blockwise Parallel Transformer (BPT), that leverages\nblockwise computation of self-attention and feedforward network fusion to\nminimize memory costs. By processing longer input sequences while maintaining\nmemory efficiency, BPT enables training sequences up to 32 times longer than\nvanilla Transformers and 2 to 4 times longer than previous memory-efficient\nmethods. Extensive experiments on language modeling and reinforcement learning\ntasks demonstrate the effectiveness of BPT in reducing memory requirements and\nimproving performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.08158","description":"<p>Deep neural networks often learn unintended bias during training, which might\nhave harmful effects when deployed in real-world settings. This work surveys\n214 papers related to sociodemographic bias in natural language processing\n(NLP). In this study, we aim to provide a more comprehensive understanding of\nthe similarities and differences among approaches to sociodemographic bias in\nNLP. To better understand the distinction between bias and real-world harm, we\nturn to ideas from psychology and behavioral economics to propose a definition\nfor sociodemographic bias. We identify three main categories of NLP bias\nresearch: types of bias, quantifying bias, and debiasing techniques. We\nhighlight the current trends in quantifying bias and debiasing techniques,\noffering insights into their strengths and weaknesses. We conclude that current\napproaches on quantifying bias face reliability issues, that many of the bias\nmetrics do not relate to real-world bias, and that debiasing techniques need to\nfocus more on training methods. Finally, we provide recommendations for future\nwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vipul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkit_P/0/1/0/all/0/1\">Pranav Narayanan Venkit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1\">Shomir Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passonneau_R/0/1/0/all/0/1\">Rebecca J. Passonneau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs. (arXiv:2307.07312v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.07312","description":"<p>In any system that uses structured knowledge graph (KG) data as its\nunderlying knowledge representation, KG-to-text generation is a useful tool for\nturning parts of the graph data into text that can be understood by humans.\nRecent work has shown that models that make use of pretraining on large amounts\nof text data can perform well on the KG-to-text task even with relatively small\nsets of training data on the specific graph-to-text task. In this paper, we\nbuild on this concept by using large language models to perform zero-shot\ngeneration based on nothing but the model's understanding of the triple\nstructure from what it can read. We show that ChatGPT achieves near\nstate-of-the-art performance on some measures of the WebNLG 2020 challenge, but\nfalls behind on others. Additionally, we compare factual, counter-factual and\nfictional statements, and show that there is a significant connection between\nwhat the LLM already knows about the data it is parsing and the quality of the\noutput text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Axelsson_A/0/1/0/all/0/1\">Agnes Axelsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skantze_G/0/1/0/all/0/1\">Gabriel Skantze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Landscape of Natural Language Processing Research. (arXiv:2307.10652v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.10652","description":"<p>As an efficient approach to understand, generate, and process natural\nlanguage texts, research in natural language processing (NLP) has exhibited a\nrapid spread and wide adoption in recent years. Given the increasing research\nwork in this area, several NLP-related approaches have been surveyed in the\nresearch community. However, a comprehensive study that categorizes established\ntopics, identifies trends, and outlines areas for future research remains\nabsent. Contributing to closing this gap, we have systematically classified and\nanalyzed research papers in the ACL Anthology. As a result, we present a\nstructured overview of the research landscape, provide a taxonomy of fields of\nstudy in NLP, analyze recent developments in NLP, summarize our findings, and\nhighlight directions for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schopf_T/0/1/0/all/0/1\">Tim Schopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arabi_K/0/1/0/all/0/1\">Karim Arabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthes_F/0/1/0/all/0/1\">Florian Matthes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LARCH: Large Language Model-based Automatic Readme Creation with Heuristics. (arXiv:2308.03099v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.03099","description":"<p>Writing a readme is a crucial aspect of software development as it plays a\nvital role in managing and reusing program code. Though it is a pain point for\nmany developers, automatically creating one remains a challenge even with the\nrecent advancements in large language models (LLMs), because it requires\ngenerating an abstract description from thousands of lines of code. In this\ndemo paper, we show that LLMs are capable of generating a coherent and\nfactually correct readmes if we can identify a code fragment that is\nrepresentative of the repository. Building upon this finding, we developed\nLARCH (LLM-based Automatic Readme Creation with Heuristics) which leverages\nrepresentative code identification with heuristics and weak supervision.\nThrough human and automated evaluations, we illustrate that LARCH can generate\ncoherent and factually correct readmes in the majority of cases, outperforming\na baseline that does not rely on representative code identification. We have\nmade LARCH open-source and provided a cross-platform Visual Studio Code\ninterface and command-line interface, accessible at\nhttps://github.com/hitachi-nlp/larch. A demo video showcasing LARCH's\ncapabilities is available at https://youtu.be/ZUKkh5ED-O4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koreeda_Y/0/1/0/all/0/1\">Yuta Koreeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morishita_T/0/1/0/all/0/1\">Terufumi Morishita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imaichi_O/0/1/0/all/0/1\">Osamu Imaichi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogawa_Y/0/1/0/all/0/1\">Yasuhiro Sogawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits. (arXiv:2308.03212v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.03212","description":"<p>Transformers have emerged as a widely used neural network model for various\nnatural language processing tasks. Previous research explored their\nrelationship with constant-depth threshold circuits, making two assumptions:\naverage-hard attention and logarithmic precision for internal computations\nrelative to input length. Merrill et al. (2022) prove that average-hard\nattention transformers recognize languages that fall within the complexity\nclass TC0, denoting the set of languages that can be recognized by\nconstant-depth polynomial-size threshold circuits. Likewise, Merrill and\nSabharwal (2023) show that log-precision transformers recognize languages\nwithin the class of uniform TC0. This shows that both transformer models can be\nsimulated by constant-depth threshold circuits, with the latter being more\nrobust due to generating a uniform circuit family. Our paper shows that the\nfirst result can be extended to yield uniform circuits as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Strobl_L/0/1/0/all/0/1\">Lena Strobl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.08742","description":"<p>Model editing techniques modify a minor proportion of knowledge in Large\nLanguage Models (LLMs) at a relatively low cost, which have demonstrated\nnotable success. Existing methods assume Transformer Layer (TL) hidden states\nare values of key-value memories of the Feed-Forward Network (FFN). They\nusually optimize the TL hidden states to memorize target knowledge and use it\nto update the weights of the FFN in LLMs. However, the information flow of TL\nhidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN,\nand residual connections. Existing methods neglect the fact that the TL hidden\nstates contains information not specifically required for FFN. Consequently,\nthe performance of model editing decreases. To achieve more precise model\nediting, we analyze hidden states of MHSA and FFN, finding that MHSA encodes\ncertain general knowledge extraction patterns. This implies that MHSA weights\ndo not require updating when new knowledge is introduced. Based on above\nfindings, we introduce PMET, which simultaneously optimizes Transformer\nComponent (TC, namely MHSA and FFN) hidden states, while only using the\noptimized TC hidden states of FFN to precisely update FFN weights. Our\nexperiments demonstrate that PMET exhibits state-of-the-art performance on both\nthe COUNTERFACT and zsRE datasets. Our ablation experiments substantiate the\neffectiveness of our enhancements, further reinforcing the finding that the\nMHSA encodes certain general knowledge extraction patterns and indicating its\nstorage of a small amount of factual knowledge. Our code is available at\nhttps://github.com/xpq-tech/PMET.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaopeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shasha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shezheng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jie Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2308.09729","description":"<p>LLMs usually exhibit limitations in their ability to incorporate new\nknowledge, the generation of hallucinations, and the transparency of their\ndecision-making process. In this paper, we explore how to prompt LLMs with\nknowledge graphs (KG), working as a remedy to engage LLMs with up-to-date\nknowledge and elicit the reasoning pathways from LLMs. Specifically, we build a\nprompting pipeline that endows LLMs with the capability of comprehending KG\ninputs and inferring with a combined implicit knowledge and the retrieved\nexternal knowledge. In addition, we investigate eliciting the mind map on which\nLLMs perform the reasoning and generate the answers. It is identified that the\nproduced mind map exhibits the reasoning pathways of LLMs grounded on the\nontology of knowledge, hence bringing the prospects of probing and gauging LLM\ninference in production. The experiments on three question &amp; answering datasets\nalso show that MindMap prompting leads to a striking empirical gain. For\ninstance, prompting a GPT-3.5 with MindMap yields an overwhelming performance\nover GPT-4 consistently. We also demonstrate that with structured facts\nretrieved from KG, MindMap can outperform a series of\nprompting-with-document-retrieval methods, benefiting from more accurate,\nconcise, and comprehensive knowledge from KGs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yilin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jimeng Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taken by Surprise: Contrast effect for Similarity Scores. (arXiv:2308.09765v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.09765","description":"<p>Accurately evaluating the similarity of object vector embeddings is of\ncritical importance for natural language processing, information retrieval and\nclassification tasks. Popular similarity scores (e.g cosine similarity) are\nbased on pairs of embedding vectors and disregard the distribution of the\nensemble from which objects are drawn. Human perception of object similarity\nsignificantly depends on the context in which the objects appear. In this work\nwe propose the $\\textit{surprise score}$, an ensemble-normalized similarity\nmetric that encapsulates the contrast effect of human perception and\nsignificantly improves the classification performance on zero- and few-shot\ndocument classification tasks. This score quantifies the surprise to find a\ngiven similarity between two elements relative to the pairwise ensemble\nsimilarities. We evaluate this metric on zero/few shot classification and\nclustering tasks and typically find 10-15 % better performance compared to raw\ncosine similarity. Our code is available at\nhttps://github.com/MeetElise/surprise-similarity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bachlechner_T/0/1/0/all/0/1\">Thomas C. Bachlechner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martone_M/0/1/0/all/0/1\">Mario Martone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schillo_M/0/1/0/all/0/1\">Marjorie Schillo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WMFormer++: Nested Transformer for Visible Watermark Removal via Implict Joint Learning. (arXiv:2308.10195v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2308.10195","description":"<p>Watermarking serves as a widely adopted approach to safeguard media\ncopyright. In parallel, the research focus has extended to watermark removal\ntechniques, offering an adversarial means to enhance watermark robustness and\nfoster advancements in the watermarking field. Existing watermark removal\nmethods mainly rely on UNet with task-specific decoder branches--one for\nwatermark localization and the other for background image restoration. However,\nwatermark localization and background restoration are not isolated tasks;\nprecise watermark localization inherently implies regions necessitating\nrestoration, and the background restoration process contributes to more\naccurate watermark localization. To holistically integrate information from\nboth branches, we introduce an implicit joint learning paradigm. This empowers\nthe network to autonomously navigate the flow of information between implicit\nbranches through a gate mechanism. Furthermore, we employ cross-channel\nattention to facilitate local detail restoration and holistic structural\ncomprehension, while harnessing nested structures to integrate multi-scale\ninformation. Extensive experiments are conducted on various challenging\nbenchmarks to validate the effectiveness of our proposed method. The results\ndemonstrate our approach's remarkable superiority, surpassing existing\nstate-of-the-art methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huo_D/0/1/0/all/0/1\">Dongjian Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zehong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hanjing Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chaowei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qingyao Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LibriSQA: Advancing Free-form and Open-ended Spoken Question Answering with a Novel Dataset and Framework. (arXiv:2308.10390v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.10390","description":"<p>While Large Language Models (LLMs) have demonstrated commendable performance\nacross a myriad of domains and tasks, existing LLMs still exhibit a palpable\ndeficit in handling multimodal functionalities, especially for the Spoken\nQuestion Answering (SQA) task which necessitates precise alignment and deep\ninteraction between speech and text features. To address the SQA challenge on\nLLMs, we initially curated the free-form and open-ended LibriSQA dataset from\nLibrispeech, comprising Part I with natural conversational formats and Part II\nencompassing multiple-choice questions followed by answers and analytical\nsegments. Both parts collectively include 107k SQA pairs that cover various\ntopics. Given the evident paucity of existing speech-text LLMs, we propose a\nlightweight, end-to-end framework to execute the SQA task on the LibriSQA,\nwitnessing significant results. By reforming ASR into the SQA format, we\nfurther substantiate our framework's capability in handling ASR tasks. Our\nempirical findings bolster the LLMs' aptitude for aligning and comprehending\nmultimodal information, paving the way for the development of universal\nmultimodal LLMs. The dataset and demo can be found at\nhttps://github.com/ZihanZhaoSJTU/LibriSQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zihan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yiyang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Heyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Effective Method using Phrase Mechanism in Neural Machine Translation. (arXiv:2308.10482v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.10482","description":"<p>Machine Translation is one of the essential tasks in Natural Language\nProcessing (NLP), which has massive applications in real life as well as\ncontributing to other tasks in the NLP research community. Recently,\nTransformer -based methods have attracted numerous researchers in this domain\nand achieved state-of-the-art results in most of the pair languages. In this\npaper, we report an effective method using a phrase mechanism,\nPhraseTransformer, to improve the strong baseline model Transformer in\nconstructing a Neural Machine Translation (NMT) system for parallel corpora\nVietnamese-Chinese. Our experiments on the MT dataset of the VLSP 2022\ncompetition achieved the BLEU score of 35.3 on Vietnamese to Chinese and 33.2\nBLEU scores on Chinese to Vietnamese data. Our code is available at\nhttps://github.com/phuongnm94/PhraseTransformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phuong Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Le Minh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models. (arXiv:2308.10755v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.10755","description":"<p>The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the\ndevelopment of large models, leading to the creation of numerous impressive\nlarge language models(LLMs) and multimodal large language models (MLLMs). These\ncutting-edge models owe their remarkable performance to high-quality data.\nHowever, the details of the training data used in leading paradigms are often\nkept confidential. This lack of transparency, coupled with the scarcity of\nopen-source data, impedes further developments within the community. As a\nresponse, this paper presents \"Wan Juan\", a large-scale multimodal dataset\ncomposed of both Chinese and English data, collected from a wide range of web\nsources. The dataset incorporates text, image-text, and video modalities, with\na total volume exceeding 2TB. It was utilized in the training of InternLM, a\nmodel that demonstrated significant advantages in multi-dimensional evaluations\nwhen compared to models of a similar scale. All data can be accessed at\nhttps://opendatalab.org.cn/WanJuan1.0.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Conghui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhenjiang Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jiantao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-08-22T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}