{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-01-30T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Knowing About Knowing: An Illusion of Human Competence Can Hinder Appropriate Reliance on AI Systems. (arXiv:2301.11333v1 [cs.HC])","link":"http://arxiv.org/abs/2301.11333","description":"<p>The dazzling promises of AI systems to augment humans in various tasks hinge\non whether humans can appropriately rely on them. Recent research has shown\nthat appropriate reliance is the key to achieving complementary team\nperformance in AI-assisted decision making. This paper addresses an\nunder-explored problem of whether the Dunning-Kruger Effect (DKE) among people\ncan hinder their appropriate reliance on AI systems. DKE is a metacognitive\nbias due to which less-competent individuals overestimate their own skill and\nperformance. Through an empirical study (N = 249), we explored the impact of\nDKE on human reliance on an AI system, and whether such effects can be\nmitigated using a tutorial intervention that reveals the fallibility of AI\nadvice, and exploiting logic units-based explanations to improve user\nunderstanding of AI advice. We found that participants who overestimate their\nperformance tend to exhibit under-reliance on AI systems, which hinders optimal\nteam performance. Logic units-based explanations did not help users in either\nimproving the calibration of their competence or facilitating appropriate\nreliance. While the tutorial intervention was highly effective in helping users\ncalibrate their self-assessment and facilitating appropriate reliance among\nparticipants with overestimated self-assessment, we found that it can\npotentially hurt the appropriate reliance of participants with underestimated\nself-assessment. Our work has broad implications on the design of methods to\ntackle user cognitive biases while facilitating appropriate reliance on AI\nsystems. Our findings advance the current understanding of the role of\nself-assessment in shaping trust and reliance in human-AI decision making. This\nlays out promising future directions for relevant HCI research in this\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1\">Gaole He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuiper_L/0/1/0/all/0/1\">Lucie Kuiper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadiraju_U/0/1/0/all/0/1\">Ujwal Gadiraju</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using novel data and ensemble models to improve automated labeling of Sustainable Development Goals. (arXiv:2301.11353v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11353","description":"<p>A number of labeling systems based on text have been proposed to help monitor\nwork on the United Nations (UN) Sustainable Development Goals (SDGs). Here, we\npresent a systematic comparison of systems using a variety of text sources and\nshow that systems differ considerably in their specificity (i.e., true-positive\nrate) and sensitivity (i.e., true-negative rate), have systematic biases (e.g.,\nare more sensitive to specific SDGs relative to others), and are susceptible to\nthe type and amount of text analyzed. We then show that an ensemble model that\npools labeling systems alleviates some of these limitations, exceeding the\nlabeling performance of all currently available systems. We conclude that\nresearchers and policymakers should care about the choice of labeling system\nand that ensemble methods should be favored when drawing conclusions about the\nabsolute and relative prevalence of work on the SDGs based on automated\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wulff_D/0/1/0/all/0/1\">Dirk U. Wulff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meier_D/0/1/0/all/0/1\">Dominik S. Meier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mata_R/0/1/0/all/0/1\">Rui Mata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Event Transformer for Image-guided Story Ending Generation. (arXiv:2301.11357v1 [cs.CV])","link":"http://arxiv.org/abs/2301.11357","description":"<p>Image-guided story ending generation (IgSEG) is to generate a story ending\nbased on given story plots and ending image. Existing methods focus on\ncross-modal feature fusion but overlook reasoning and mining implicit\ninformation from story plots and ending image. To tackle this drawback, we\npropose a multimodal event transformer, an event-based reasoning framework for\nIgSEG. Specifically, we construct visual and semantic event graphs from story\nplots and ending image, and leverage event-based reasoning to reason and mine\nimplicit information in a single modality. Next, we connect visual and semantic\nevent graphs and utilize cross-modal fusion to integrate different-modality\nfeatures. In addition, we propose a multimodal injector to adaptive pass\nessential information to decoder. Besides, we present an incoherence detection\nto enhance the understanding context of a story plot and the robustness of\ngraph modeling for our model. Experimental results show that our method\nachieves state-of-the-art performance for the image-guided story ending\ngeneration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yucheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Cross-modal Alignment for Text-Guided Image Inpainting. (arXiv:2301.11362v1 [cs.CV])","link":"http://arxiv.org/abs/2301.11362","description":"<p>Text-guided image inpainting (TGII) aims to restore missing regions based on\na given text in a damaged image. Existing methods are based on a strong vision\nencoder and a cross-modal fusion model to integrate cross-modal features.\nHowever, these methods allocate most of the computation to visual encoding,\nwhile light computation on modeling modality interactions. Moreover, they take\ncross-modal fusion for depth features, which ignores a fine-grained alignment\nbetween text and image. Recently, vision-language pre-trained models (VLPM),\nencapsulating rich cross-modal alignment knowledge, have advanced in most\nmultimodal tasks. In this work, we propose a novel model for TGII by improving\ncross-modal alignment (CMA). CMA model consists of a VLPM as a vision-language\nencoder, an image generator and global-local discriminators. To explore\ncross-modal alignment knowledge for image restoration, we introduce cross-modal\nalignment distillation and in-sample distribution distillation. In addition, we\nemploy adversarial training to enhance the model to fill the missing region in\ncomplicated structures effectively. Experiments are conducted on two popular\nvision-language datasets. Results show that our model achieves state-of-the-art\nperformance compared with other strong competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yucheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style-Aware Contrastive Learning for Multi-Style Image Captioning. (arXiv:2301.11367v1 [cs.CV])","link":"http://arxiv.org/abs/2301.11367","description":"<p>Existing multi-style image captioning methods show promising results in\ngenerating a caption with accurate visual content and desired linguistic style.\nHowever, existing methods overlook the relationship between linguistic style\nand visual content. To overcome this drawback, we propose style-aware\ncontrastive learning for multi-style image captioning. First, we present a\nstyle-aware visual encoder with contrastive learning to mine potential visual\ncontent relevant to style. Moreover, we propose a style-aware triplet contrast\nobjective to distinguish whether the image, style and caption matched. To\nprovide positive and negative samples for contrastive learning, we present\nthree retrieval schemes: object-based retrieval, RoI-based retrieval and\ntriplet-based retrieval, and design a dynamic trade-off function to calculate\nretrieval scores. Experimental results demonstrate that our approach achieves\nstate-of-the-art performance. In addition, we conduct an extensive analysis to\nverify the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yucheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task formulation for Extracting Social Determinants of Health from Clinical Narratives. (arXiv:2301.11386v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11386","description":"<p>Objective: The 2022 n2c2 NLP Challenge posed identification of social\ndeterminants of health (SDOH) in clinical narratives. We present three systems\nthat we developed for the Challenge and discuss the distinctive task\nformulation used in each of the three systems. Materials and Methods: The first\nsystem identifies target pieces of information independently using machine\nlearning classifiers. The second system uses a large language model (LLM) to\nextract complete structured outputs per document. The third system extracts\ncandidate phrases using machine learning and identifies target relations with\nhand-crafted rules. Results: The three systems achieved F1 scores of 0.884,\n0.831, and 0.663 in the Subtask A of the Challenge, which are ranked third,\nseventh, and eighth among the 15 participating teams. The review of the\nextraction results from our systems reveals characteristics of each approach\nand those of the SODH extraction task. Discussion: Phrases and relations\nannotated in the task is unique and diverse, not conforming to the conventional\nevent extraction task. These annotations are difficult to model with limited\ntraining data. The system that extracts information independently, ignoring the\nannotated relations, achieves the highest F1 score. Meanwhile, LLM with its\nversatile capability achieves the high F1 score, while respecting the annotated\nrelations. The rule-based system tackling relation extraction obtains the low\nF1 score, while it is the most explainable approach. Conclusion: The F1 scores\nof the three systems vary in this challenge setting, but each approach has\nadvantages and disadvantages in a practical application. The selection of the\napproach depends not only on the F1 score but also on the requirements in the\napplication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Torii_M/0/1/0/all/0/1\">Manabu Torii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_I/0/1/0/all/0/1\">Ian M. Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doan_S/0/1/0/all/0/1\">Son Doan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Paul Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Elly W. Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisook_D/0/1/0/all/0/1\">Daniel S. Zisook</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Pump&Dump Stock Market Manipulation from Online Forums. (arXiv:2301.11403v1 [cs.SI])","link":"http://arxiv.org/abs/2301.11403","description":"<p>The intersection of social media, low-cost trading platforms, and naive\ninvestors has created an ideal situation for information-based market\nmanipulations, especially pump&amp;dumps. Manipulators accumulate small-cap stocks,\ndisseminate false information on social media to inflate their price, and sell\nat the peak. We collect a dataset of stocks whose price and volume profiles\nhave the characteristic shape of a pump&amp;dump, and social media posts for those\nsame stocks that match the timing of the initial price rises. From these we\nbuild predictive models for pump&amp;dump events based on the language used in the\nsocial media posts.\n</p>\n<p>There are multiple difficulties: not every post will cause the intended\nmarket reaction, some pump&amp;dump events may be triggered by posts in other\nforums, and there may be accidental confluences of post timing and market\nmovements. Nevertheless, our best model achieves a prediction accuracy of 85%\nand an F1-score of 62%. Such a tool can provide early warning to investors and\nregulators that a pump&amp;dump may be underway.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nam_D/0/1/0/all/0/1\">D. Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skillicorn_D/0/1/0/all/0/1\">D.B. Skillicorn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Arabic: Software for Perso-Arabic Script Manipulation. (arXiv:2301.11406v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11406","description":"<p>This paper presents an open-source software library that provides a set of\nfinite-state transducer (FST) components and corresponding utilities for\nmanipulating the writing systems of languages that use the Perso-Arabic script.\nThe operations include various levels of script normalization, including visual\ninvariance-preserving operations that subsume and go beyond the standard\nUnicode normalization forms, as well as transformations that modify the visual\nappearance of characters in accordance with the regional orthographies for\neleven contemporary languages from diverse language families. The library also\nprovides simple FST-based romanization and transliteration. We additionally\nattempt to formalize the typology of Perso-Arabic characters by providing\none-to-many mappings from Unicode code points to the languages that use them.\nWhile our work focuses on the Arabic script diaspora rather than Arabic itself,\nthis approach could be adopted for any language that uses the Arabic script,\nthus providing a unified framework for treating a script family used by close\nto a billion people.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gutkin_A/0/1/0/all/0/1\">Alexander Gutkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johny_C/0/1/0/all/0/1\">Cibu Johny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doctor_R/0/1/0/all/0/1\">Raiomond Doctor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roark_B/0/1/0/all/0/1\">Brian Roark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sproat_R/0/1/0/all/0/1\">Richard Sproat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural-Symbolic Inference for Robust Autoregressive Graph Parsing via Compositional Uncertainty Quantification. (arXiv:2301.11459v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11459","description":"<p>Pre-trained seq2seq models excel at graph semantic parsing with rich\nannotated data, but generalize worse to out-of-distribution (OOD) and long-tail\nexamples. In comparison, symbolic parsers under-perform on population-level\nmetrics, but exhibit unique strength in OOD and tail generalization. In this\nwork, we study compositionality-aware approach to neural-symbolic inference\ninformed by model confidence, performing fine-grained neural-symbolic reasoning\nat subgraph level (i.e., nodes and edges) and precisely targeting subgraph\ncomponents with high uncertainty in the neural parser. As a result, the method\ncombines the distinct strength of the neural and symbolic approaches in\ncapturing different aspects of the graph prediction, leading to well-rounded\ngeneralization performance both across domains and in the tail. We empirically\ninvestigate the approach in the English Resource Grammar (ERG) parsing problem\non a diverse suite of standard in-domain and seven OOD corpora. Our approach\nleads to 35.26% and 35.60% error reduction in aggregated Smatch score over\nneural and symbolic approaches respectively, and 14% absolute accuracy gain in\nkey tail linguistic categories over the neural model, outperforming prior\nstate-of-art methods that do not account for compositionality or uncertainty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jeremiah Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech. (arXiv:2301.11462v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11462","description":"<p>When acquiring syntax, children consistently choose hierarchical rules over\ncompeting non-hierarchical possibilities. Is this preference due to a learning\nbias for hierarchical structure, or due to more general biases that interact\nwith hierarchical cues in children's linguistic input? We explore these\npossibilities by training LSTMs and Transformers - two types of neural networks\nwithout a hierarchical bias - on data similar in quantity and content to\nchildren's linguistic input: text from the CHILDES corpus. We then evaluate\nwhat these models have learned about English yes/no questions, a phenomenon for\nwhich hierarchical structure is crucial. We find that, though they perform well\nat capturing the surface statistics of child-directed speech (as measured by\nperplexity), both model types generalize in a way more consistent with an\nincorrect linear rule than the correct hierarchical rule. These results suggest\nthat human-like generalization from text alone requires stronger biases than\nthe general sequence-processing biases of standard neural network\narchitectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yedetore_A/0/1/0/all/0/1\">Aditya Yedetore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_R/0/1/0/all/0/1\">Robert Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCoy_R/0/1/0/all/0/1\">R. Thomas McCoy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Synthetic Data for Conversational Music Recommendation Using Random Walks and Language Models. (arXiv:2301.11489v1 [cs.IR])","link":"http://arxiv.org/abs/2301.11489","description":"<p>Conversational recommendation systems (CRSs) enable users to use natural\nlanguage feedback to control their recommendations, overcoming many of the\nchallenges of traditional recommendation systems. However, the practical\nadoption of CRSs remains limited due to a lack of rich and diverse\nconversational training data that pairs user utterances with recommendations.\nTo address this problem, we introduce a new method to generate synthetic\ntraining data by transforming curated item collections, such as playlists or\nmovie watch lists, into item-seeking conversations. First, we use a biased\nrandom walk to generate a sequence of slates, or sets of item recommendations;\nthen, we use a language model to generate corresponding user utterances. We\ndemonstrate our approach by generating a conversational music recommendation\ndataset with over one million conversations, which were found to be consistent\nwith relevant recommendations by a crowdsourced evaluation. Using the synthetic\ndata to train a CRS, we significantly outperform standard retrieval baselines\nin offline and online evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leszczynski_M/0/1/0/all/0/1\">Megan Leszczynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganti_R/0/1/0/all/0/1\">Ravi Ganti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balog_K/0/1/0/all/0/1\">Krisztian Balog</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radlinski_F/0/1/0/all/0/1\">Filip Radlinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_F/0/1/0/all/0/1\">Fernando Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaganty_A/0/1/0/all/0/1\">Arun Tejasvi Chaganty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Candidate Soups: Fusing Candidate Results Improves Translation Quality for Non-Autoregressive Translation. (arXiv:2301.11503v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11503","description":"<p>Non-autoregressive translation (NAT) model achieves a much faster inference\nspeed than the autoregressive translation (AT) model because it can\nsimultaneously predict all tokens during inference. However, its translation\nquality suffers from degradation compared to AT. And existing NAT methods only\nfocus on improving the NAT model's performance but do not fully utilize it. In\nthis paper, we propose a simple but effective method called \"Candidate Soups,\"\nwhich can obtain high-quality translations while maintaining the inference\nspeed of NAT models. Unlike previous approaches that pick the individual result\nand discard the remainders, Candidate Soups (CDS) can fully use the valuable\ninformation in the different candidate translations through model uncertainty.\nExtensive experiments on two benchmarks (WMT'14 EN-DE and WMT'16 EN-RO)\ndemonstrate the effectiveness and generality of our proposed method, which can\nsignificantly improve the translation quality of various base models. More\nnotably, our best variant outperforms the AT model on three translation tasks\nwith 7.6 times speedup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huanran Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengfei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoling Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Parametric Video-Grounded Text Generation. (arXiv:2301.11507v1 [cs.CV])","link":"http://arxiv.org/abs/2301.11507","description":"<p>Efficient video-language modeling should consider the computational cost\nbecause of a large, sometimes intractable, number of video frames. Parametric\napproaches such as the attention mechanism may not be ideal since its\ncomputational cost quadratically increases as the video length increases.\nRather, previous studies have relied on offline feature extraction or frame\nsampling to represent the video efficiently, focusing on cross-modal modeling\nin short video clips. In this paper, we propose a semi-parametric\nvideo-grounded text generation model, SeViT, a novel perspective on scalable\nvideo-language modeling toward long untrimmed videos. Treating a video as an\nexternal data store, SeViT includes a non-parametric frame retriever to select\na few query-relevant frames from the data store for a given query and a\nparametric generator to effectively aggregate the frames with the query via\nlate fusion methods. Experimental results demonstrate our method has a\nsignificant advantage in longer videos and causal video understanding.\nMoreover, our model achieves the new state of the art on four video-language\ndatasets, iVQA (+4.8), Next-QA (+6.9), and Activitynet-QA (+4.8) in accuracy,\nand MSRVTT-Caption (+3.6) in CIDEr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungdong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jin-Hwa Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Theme-driven Keyphrase Extraction from Social Media on Opioid Recovery. (arXiv:2301.11508v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11508","description":"<p>An emerging trend on social media platforms is their use as safe spaces for\npeer support. Particularly in healthcare, where many medical conditions contain\nharsh stigmas, social media has become a stigma-free way to engage in dialogues\nregarding symptoms, treatments, and personal experiences. Many existing works\nhave employed NLP algorithms to facilitate quantitative analysis of health\ntrends. Notably absent from existing works are keyphrase extraction (KE) models\nfor social health posts-a task crucial to discovering emerging public health\ntrends. This paper presents a novel, theme-driven KE dataset, SuboxoPhrase, and\na qualitative annotation scheme with an overarching goal of extracting targeted\nclinically-relevant keyphrases. To the best of our knowledge, this is the first\nstudy to design a KE schema for social media healthcare texts. To demonstrate\nthe value of this approach, this study analyzes Reddit posts regarding\nmedications for opioid use disorder, a paramount health concern worldwide.\nAdditionally, we benchmark ten off-the-shelf KE models on our new dataset,\ndemonstrating the unique extraction challenges in modeling user-generated\nhealth texts. The proposed theme-driven KE approach lays the foundation of\nfuture work on efficient, large-scale analysis of social health texts, allowing\nresearchers to surface useful public health trends, patterns, and knowledge\ngaps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Romano_W/0/1/0/all/0/1\">William Romano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharif_O/0/1/0/all/0/1\">Omar Sharif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basak_M/0/1/0/all/0/1\">Madhusudan Basak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatto_J/0/1/0/all/0/1\">Joseph Gatto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preum_S/0/1/0/all/0/1\">Sarah Preum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding. (arXiv:2301.11564v1 [cs.RO])","link":"http://arxiv.org/abs/2301.11564","description":"<p>Robotic grasping is a fundamental ability for a robot to interact with the\nenvironment. Current methods focus on how to obtain a stable and reliable\ngrasping pose in object wise, while little work has been studied on part\n(shape)-wise grasping which is related to fine-grained grasping and robotic\naffordance. Parts can be seen as atomic elements to compose an object, which\ncontains rich semantic knowledge and a strong correlation with affordance.\nHowever, lacking a large part-wise 3D robotic dataset limits the development of\npart representation learning and downstream application. In this paper, we\npropose a new large Language-guided SHape grAsPing datasEt (named Lang-SHAPE)\nto learn 3D part-wise affordance and grasping ability. We design a novel\ntwo-stage fine-grained robotic grasping network (named PIONEER), including a\nnovel 3D part language grounding model, and a part-aware grasp pose detection\nmodel. To evaluate the effectiveness, we perform multi-level difficulty part\nlanguage grounding grasping experiments and deploy our proposed model on a real\nrobot. Results show our method achieves satisfactory performance and efficiency\nin reference identification, affordance inference, and 3D part-aware grasping.\nOur dataset and code are available on our project website\nhttps://sites.google.com/view/lang-shape\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yaoxian Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Penglei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Down the Rabbit Hole: Detecting Online Extremism, Radicalisation, and Politicised Hate Speech. (arXiv:2301.11579v1 [cs.SI])","link":"http://arxiv.org/abs/2301.11579","description":"<p>Social media is a modern person's digital voice to project and engage with\nnew ideas and mobilise communities $\\unicode{x2013}$ a power shared with\nextremists. Given the societal risks of unvetted content-moderating algorithms\nfor Extremism, Radicalisation, and Hate speech (ERH) detection, responsible\nsoftware engineering must understand the who, what, when, where, and why such\nmodels are necessary to protect user safety and free expression. Hence, we\npropose and examine the unique research field of ERH context mining to unify\ndisjoint studies. Specifically, we evaluate the start-to-finish design process\nfrom socio-technical definition-building and dataset collection strategies to\ntechnical algorithm design and performance. Our 2015-2021 51-study Systematic\nLiterature Review (SLR) provides the first cross-examination of textual,\nnetwork, and visual approaches to detecting extremist affiliation, hateful\ncontent, and radicalisation towards groups and movements. We identify\nconsensus-driven ERH definitions and propose solutions to existing ideological\nand geographic biases, particularly due to the lack of research in\nOceania/Australasia. Our hybridised investigation on Natural Language\nProcessing, Community Detection, and visual-text models demonstrates the\ndominating performance of textual transformer-based algorithms. We conclude\nwith vital recommendations for ERH context mining researchers and propose an\nuptake roadmap with guidelines for researchers, industries, and governments to\nenable a safer cyberspace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Govers_J/0/1/0/all/0/1\">Jarod Govers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_P/0/1/0/all/0/1\">Philip Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dant_A/0/1/0/all/0/1\">Aaron Dant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patros_P/0/1/0/all/0/1\">Panos Patros</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ThoughtSource: A central hub for large language model reasoning data. (arXiv:2301.11596v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11596","description":"<p>Large language models (LLMs) such as GPT-3 and ChatGPT have recently\ndemonstrated impressive results across a wide range of tasks. LLMs are still\nlimited, however, in that they frequently fail at complex reasoning, their\nreasoning processes are opaque, they are prone to 'hallucinate' facts, and\nthere are concerns about their underlying biases. Letting models verbalize\nreasoning steps as natural language, a technique known as chain-of-thought\nprompting, has recently been proposed as a way to address some of these issues.\nHere we present the first release of ThoughtSource, a meta-dataset and software\nlibrary for chain-of-thought (CoT) reasoning. The goal of ThoughtSource is to\nimprove future artificial intelligence systems by facilitating qualitative\nunderstanding of CoTs, enabling empirical evaluations, and providing training\ndata. This first release of ThoughtSource integrates six scientific/medical,\nthree general-domain and five math word question answering datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ott_S/0/1/0/all/0/1\">Simon Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hebenstreit_K/0/1/0/all/0/1\">Konstantin Hebenstreit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lievin_V/0/1/0/all/0/1\">Valentin Li&#xe9;vin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hother_C/0/1/0/all/0/1\">Christoffer Egeberg Hother</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Milad Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayrhauser_M/0/1/0/all/0/1\">Maximilian Mayrhauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Praas_R/0/1/0/all/0/1\">Robert Praas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1\">Ole Winther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1\">Matthias Samwald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-View Joint Learning Framework for Embedding Clinical Codes and Text Using Graph Neural Networks. (arXiv:2301.11608v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11608","description":"<p>Learning to represent free text is a core task in many clinical machine\nlearning (ML) applications, as clinical text contains observations and plans\nnot otherwise available for inference. State-of-the-art methods use large\nlanguage models developed with immense computational resources and training\ndata; however, applying these models is challenging because of the highly\nvarying syntax and vocabulary in clinical free text. Structured information\nsuch as International Classification of Disease (ICD) codes often succinctly\nabstracts the most important facts of a clinical encounter and yields good\nperformance, but is often not as available as clinical text in real-world\nscenarios. We propose a \\textbf{multi-view learning framework} that jointly\nlearns from codes and text to combine the availability and forward-looking\nnature of text and better performance of ICD codes. The learned text embeddings\ncan be used as inputs to predictive algorithms independent of the ICD codes\nduring inference. Our approach uses a Graph Neural Network (GNN) to process ICD\ncodes, and Bi-LSTM to process text. We apply Deep Canonical Correlation\nAnalysis (DCCA) to enforce the two views to learn a similar representation of\neach patient. In experiments using planned surgical procedure text, our model\noutperforms BERT models fine-tuned to clinical data, and in experiments using\ndiverse text in MIMIC-III, our model is competitive to a fine-tuned BERT at a\ntiny fraction of its computational effort.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lecheng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_C/0/1/0/all/0/1\">Christopher King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_B/0/1/0/all/0/1\">Bradley Fritz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event Causality Extraction with Event Argument Correlations. (arXiv:2301.11621v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11621","description":"<p>Event Causality Identification (ECI), which aims to detect whether a\ncausality relation exists between two given textual events, is an important\ntask for event causality understanding. However, the ECI task ignores crucial\nevent structure and cause-effect causality component information, making it\nstruggle for downstream applications. In this paper, we explore a novel task,\nnamely Event Causality Extraction (ECE), aiming to extract the cause-effect\nevent causality pairs with their structured event information from plain texts.\nThe ECE task is more challenging since each event can contain multiple event\narguments, posing fine-grained correlations between events to decide the\ncauseeffect event pair. Hence, we propose a method with a dual grid tagging\nscheme to capture the intra- and inter-event argument correlations for ECE.\nFurther, we devise a event type-enhanced model architecture to realize the dual\ngrid tagging scheme. Experiments demonstrate the effectiveness of our method,\nand extensive analyses point out several future directions for ECE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shiyao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_J/0/1/0/all/0/1\">Jiawei Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1\">Xin Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">QuanGang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tingwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jinqiao Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient Transfer Learning Methods. (arXiv:2301.11660v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11660","description":"<p>As the size of the pre-trained language model (PLM) continues to increase,\nnumerous parameter-efficient transfer learning methods have been proposed\nrecently to compensate for the tremendous cost of fine-tuning. Despite the\nimpressive results achieved by large pre-trained language models (PLMs) and\nvarious parameter-efficient transfer learning (PETL) methods on sundry\nbenchmarks, it remains unclear if they can handle inputs that have been\ndistributionally shifted effectively. In this study, we systematically explore\nhow the ability to detect out-of-distribution (OOD) changes as the size of the\nPLM grows or the transfer methods are altered. Specifically, we evaluated\nvarious PETL techniques, including fine-tuning, Adapter, LoRA, and\nprefix-tuning, on three different intention classification tasks, each\nutilizing various language models with different scales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyunsoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Choonghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junyeop Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyuhng Joon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-goo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Personalized Review Summarization by Modeling Historical Reviews from Customer and Product Separately. (arXiv:2301.11682v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11682","description":"<p>Review summarization is a non-trivial task that aims to summarize the main\nidea of the product review in the E-commerce website. Different from the\ndocument summary which only needs to focus on the main facts described in the\ndocument, review summarization should not only summarize the main aspects\nmentioned in the review but also reflect the personal style of the review\nauthor. Although existing review summarization methods have incorporated the\nhistorical reviews of both customer and product, they usually simply\nconcatenate and indiscriminately model this two heterogeneous information into\na long sequence. Moreover, the rating information can also provide a high-level\nabstraction of customer preference, it has not been used by the majority of\nmethods. In this paper, we propose the Heterogeneous Historical Review aware\nReview Summarization Model (HHRRS) which separately models the two types of\nhistorical reviews with the rating information by a graph reasoning module with\na contrastive loss. We employ a multi-task framework that conducts the review\nsentiment classification and summarization jointly. Extensive experiments on\nfour benchmark datasets demonstrate the superiority of HHRRS on both tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuchi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can We Use Probing to Better Understand Fine-tuning and Knowledge Distillation of the BERT NLU?. (arXiv:2301.11688v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11688","description":"<p>In this article, we use probing to investigate phenomena that occur during\nfine-tuning and knowledge distillation of a BERT-based natural language\nunderstanding (NLU) model. Our ultimate purpose was to use probing to better\nunderstand practical production problems and consequently to build better NLU\nmodels. We designed experiments to see how fine-tuning changes the linguistic\ncapabilities of BERT, what the optimal size of the fine-tuning dataset is, and\nwhat amount of information is contained in a distilled NLU based on a tiny\nTransformer. The results of the experiments show that the probing paradigm in\nits current form is not well suited to answer such questions. Structural, Edge\nand Conditional probes do not take into account how easy it is to decode probed\ninformation. Consequently, we conclude that quantification of information\ndecodability is critical for many practical applications of the probing\nparadigm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoscilowicz_J/0/1/0/all/0/1\">Jakub Ho&#x15b;ci&#x142;owicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sowanski_M/0/1/0/all/0/1\">Marcin Sowa&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czubowski_P/0/1/0/all/0/1\">Piotr Czubowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janicki_A/0/1/0/all/0/1\">Artur Janicki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLCNN: Sentence-Level Convolutional Neural Network for Text Classification. (arXiv:2301.11696v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11696","description":"<p>Text classification is a fundamental task in natural language processing\n(NLP). Several recent studies show the success of deep learning on text\nprocessing. Convolutional neural network (CNN), as a popular deep learning\nmodel, has shown remarkable success in the task of text classification. In this\npaper, new baseline models have been studied for text classification using CNN.\nIn these models, documents are fed to the network as a three-dimensional tensor\nrepresentation to provide sentence-level analysis. Applying such a method\nenables the models to take advantage of the positional information of the\nsentences in the text. Besides, analysing adjacent sentences allows extracting\nadditional features. The proposed models have been compared with the\nstate-of-the-art models using several datasets. The results have shown that the\nproposed models have better performance, particularly in the longer documents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jarrahi_A/0/1/0/all/0/1\">Ali Jarrahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mousa_R/0/1/0/all/0/1\">Ramin Mousa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safari_L/0/1/0/all/0/1\">Leila Safari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Network Model for Sign Language Comprehension. (arXiv:2301.11709v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11709","description":"<p>In this study, the authors propose a computational cognitive model for sign\nlanguage (SL) perception and comprehension with detailed algorithmic\ndescriptions based on cognitive functionalities in human language processing.\nThe semantic network model (SNM) that represents semantic relations between\nconcepts, it is used as a form of knowledge representation. The proposed model\nis applied in the comprehension of sign language for classifier predicates. The\nspreading activation search method is initiated by labeling a set of source\nnodes (e.g. concepts in the semantic network) with weights or \"activation\" and\nthen iteratively propagating or \"spreading\" that activation out to other nodes\nlinked to the source nodes. The results demonstrate that the proposed search\nmethod improves the performance of sign language comprehension in the SNM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xinchen Kang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Yao_D/0/1/0/all/0/1\">Dengfeng Yao</a> (1,2), <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Minghu Jiang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yunlong Huang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fanshu Li</a> (1) ((1) Beijing Key Lab of Information Service Engineering, Beijing Union University, Beijing, China. (2) Lab of Computational Linguistics, Tsinghua University, Beijing, China.)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-training for Speech Translation: CTC Meets Optimal Transport. (arXiv:2301.11716v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11716","description":"<p>The gap between speech and text modalities is a major challenge in\nspeech-to-text translation (ST). Different methods have been proposed for\nreducing this gap, but most of them require architectural changes in ST\ntraining. In this work, we propose to mitigate this issue at the pre-training\nstage, requiring no change in the ST model. First, we show that the\nconnectionist temporal classification (CTC) loss can reduce the modality gap by\ndesign. We provide a quantitative comparison with the more common cross-entropy\nloss, showing that pre-training with CTC consistently achieves better final ST\naccuracy. Nevertheless, CTC is only a partial solution and thus, in our second\ncontribution, we propose a novel pre-training method combining CTC and optimal\ntransport to further reduce this gap. Our method pre-trains a Siamese-like\nmodel composed of two encoders, one for acoustic inputs and the other for\ntextual inputs, such that they produce representations that are close to each\nother in the Wasserstein space. Extensive experiments on the standard CoVoST-2\nand MuST-C datasets show that our pre-training method applied to the vanilla\nencoder-decoder Transformer achieves state-of-the-art performance under the\nno-external-data setting, and performs on par with recent strong multi-task\nlearning systems trained with external data. Finally, our method can also be\napplied on top of these multi-task systems, leading to further improvements for\nthese models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_P/0/1/0/all/0/1\">Phuong-Hang Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lecouteux_B/0/1/0/all/0/1\">Benjamin Lecouteux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwab_D/0/1/0/all/0/1\">Didier Schwab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Knowledge into Document Summarization: an Application of Prefix-Tuning on GPT-2. (arXiv:2301.11719v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11719","description":"<p>Despite the great development of document summarization techniques nowadays,\nfactual inconsistencies between the generated summaries and the original text\nstill occur from time to time. This paper proposes a prefix-tuning-based\napproach that uses a set of trainable continuous prefix prompt together with\ndiscrete prompts to aid model generation, which makes a significant impact on\nboth CNN/Daily Mail and XSum summaries generated using GPT-2. The improvements\non fact preservation in the generated summaries indicates the effectiveness of\nadopting this prefix-tuning-based method in knowledge-enhanced document\nsummarization, and also shows a great potential on other natural language\nprocessing tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Emma Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-task Multi-stage Transitional Training Framework for Neural Chat Translation. (arXiv:2301.11749v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11749","description":"<p>Neural chat translation (NCT) aims to translate a cross-lingual chat between\nspeakers of different languages. Existing context-aware NMT models cannot\nachieve satisfactory performances due to the following inherent problems: 1)\nlimited resources of annotated bilingual dialogues; 2) the neglect of modelling\nconversational properties; 3) training discrepancy between different stages. To\naddress these issues, in this paper, we propose a multi-task multi-stage\ntransitional (MMT) training framework, where an NCT model is trained using the\nbilingual chat translation dataset and additional monolingual dialogues. We\nelaborately design two auxiliary tasks, namely utterance discrimination and\nspeaker discrimination, to introduce the modelling of dialogue coherence and\nspeaker characteristic into the NCT model. The training process consists of\nthree stages: 1) sentence-level pre-training on large-scale parallel corpus; 2)\nintermediate training with auxiliary tasks using additional monolingual\ndialogues; 3) context-aware fine-tuning with gradual transition. Particularly,\nthe second stage serves as an intermediate phase that alleviates the training\ndiscrepancy between the pre-training and fine-tuning stages. Moreover, to make\nthe stage transition smoother, we train the NCT model using a gradual\ntransition strategy, i.e., gradually transiting from using monolingual to\nbilingual dialogues. Extensive experiments on two language pairs demonstrate\nthe effectiveness and superiority of our proposed training framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chulun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongji Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mo\\^usai: Text-to-Music Generation with Long-Context Latent Diffusion. (arXiv:2301.11757v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11757","description":"<p>The recent surge in popularity of diffusion models for image generation has\nbrought new attention to the potential of these models in other areas of media\nsynthesis. One area that has yet to be fully explored is the application of\ndiffusion models to music generation. Music generation requires to handle\nmultiple aspects, including the temporal dimension, long-term structure,\nmultiple layers of overlapping sounds, and nuances that only trained listeners\ncan detect. In our work, we investigate the potential of diffusion models for\ntext-conditional music generation. We develop a cascading latent diffusion\napproach that can generate multiple minutes of high-quality stereo music at\n48kHz from textual descriptions. For each model, we make an effort to maintain\nreasonable inference speed, targeting real-time on a single consumer GPU. In\naddition to trained models, we provide a collection of open-source libraries\nwith the hope of facilitating future work in the field.\n</p>\n<p>We open-source the following: - Music samples for this paper:\nhttps://bit.ly/anonymous-mousai - All music samples for all models:\nhttps://bit.ly/audio-diffusion - Codes:\nhttps://github.com/archinetai/audio-diffusion-pytorch\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_F/0/1/0/all/0/1\">Flavio Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Attention with Hierarchies for Multi-hop Question Answering. (arXiv:2301.11792v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11792","description":"<p>Multi-hop QA (Question Answering) is the task of finding the answer to a\nquestion across multiple documents. In recent years, a number of Deep\nLearning-based approaches have been proposed to tackle this complex task, as\nwell as a few standard benchmarks to assess models Multi-hop QA capabilities.\nIn this paper, we focus on the well-established HotpotQA benchmark dataset,\nwhich requires models to perform answer span extraction as well as support\nsentence prediction. We present two extensions to the SOTA Graph Neural Network\n(GNN) based model for HotpotQA, Hierarchical Graph Network (HGN): (i) we\ncomplete the original hierarchical structure by introducing new edges between\nthe query and context sentence nodes; (ii) in the graph propagation step, we\npropose a novel extension to Hierarchical Graph Attention Network GATH (Graph\nATtention with Hierarchies) that makes use of the graph hierarchy to update the\nnode representations in a sequential fashion. Experiments on HotpotQA\ndemonstrate the efficiency of the proposed modifications and support our\nassumptions about the effects of model related variables.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yunjie He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorinski_P/0/1/0/all/0/1\">Philip John Gorinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staliunaite_I/0/1/0/all/0/1\">Ieva Staliunaite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Call for Papers -- The BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus. (arXiv:2301.11796v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11796","description":"<p>We present the call for papers for the BabyLM Challenge: Sample-efficient\npretraining on a developmentally plausible corpus. This shared task is intended\nfor participants with an interest in small scale language modeling, human\nlanguage acquisition, low-resource NLP, and cognitive modeling. In partnership\nwith CoNLL and CMCL, we provide a platform for approaches to pretraining with a\nlimited-size corpus sourced from data inspired by the input to children. The\ntask has three tracks, two of which restrict the training data to pre-released\ndatasets of 10M and 100M words and are dedicated to explorations of approaches\nsuch as architectural variations, self-supervised objectives, or curriculum\nlearning. The final track only restricts the amount of text used, allowing\ninnovation in the choice of the data, its domain, and even its modality (i.e.,\ndata from sources other than text is welcome). We will release a shared\nevaluation pipeline which scores models on a variety of benchmarks and tasks,\nincluding targeted syntactic evaluations and natural language understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Warstadt_A/0/1/0/all/0/1\">Alex Warstadt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_A/0/1/0/all/0/1\">Aaron Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilcox_E/0/1/0/all/0/1\">Ethan Wilcox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_C/0/1/0/all/0/1\">Chengxu Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reading and Reasoning over Chart Images for Evidence-based Automated Fact-Checking. (arXiv:2301.11843v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11843","description":"<p>Evidence data for automated fact-checking (AFC) can be in multiple modalities\nsuch as text, tables, images, audio, or video. While there is increasing\ninterest in using images for AFC, previous works mostly focus on detecting\nmanipulated or fake images. We propose a novel task, chart-based fact-checking,\nand introduce ChartBERT as the first model for AFC against chart evidence.\nChartBERT leverages textual, structural and visual information of charts to\ndetermine the veracity of textual claims. For evaluation, we create ChartFC, a\nnew dataset of 15, 886 charts. We systematically evaluate 75 different\nvision-language (VL) baselines and show that ChartBERT outperforms VL models,\nachieving 63.8% accuracy. Our results suggest that the task is complex yet\nfeasible, with many challenges ahead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Mubashara Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cocarascu_O/0/1/0/all/0/1\">Oana Cocarascu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simperl_E/0/1/0/all/0/1\">Elena Simperl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning the Effects of Physical Actions in a Multi-modal Environment. (arXiv:2301.11845v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11845","description":"<p>Large Language Models (LLMs) handle physical commonsense information\ninadequately. As a result of being trained in a disembodied setting, LLMs often\nfail to predict an action's outcome in a given environment. However, predicting\nthe effects of an action before it is executed is crucial in planning, where\ncoherent sequences of actions are often needed to achieve a goal. Therefore, we\nintroduce the multi-modal task of predicting the outcomes of actions solely\nfrom realistic sensory inputs (images and text). Next, we extend an LLM to\nmodel latent representations of objects to better predict action outcomes in an\nenvironment. We show that multi-modal models can capture physical commonsense\nwhen augmented with visual information. Finally, we evaluate our model's\nperformance on novel actions and objects and find that combining modalities\nhelp models to generalize and learn physical commonsense reasoning better.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dagan_G/0/1/0/all/0/1\">Gautier Dagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lascarides_A/0/1/0/all/0/1\">Alex Lascarides</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Pretrained Language Models for Long Clinical Text. (arXiv:2301.11847v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11847","description":"<p>Objective: Clinical knowledge enriched transformer models (e.g.,\nClinicalBERT) have state-of-the-art results on clinical NLP (natural language\nprocessing) tasks. One of the core limitations of these transformer models is\nthe substantial memory consumption due to their full self-attention mechanism,\nwhich leads to the performance degradation in long clinical texts. To overcome\nthis, we propose to leverage long-sequence transformer models (e.g., Longformer\nand BigBird), which extend the maximum input sequence length from 512 to 4096,\nto enhance the ability to model long-term dependencies in long clinical texts.\n</p>\n<p>Materials and Methods: Inspired by the success of long sequence transformer\nmodels and the fact that clinical notes are mostly long, we introduce two\ndomain enriched language models, Clinical-Longformer and Clinical-BigBird,\nwhich are pre-trained on a large-scale clinical corpus. We evaluate both\nlanguage models using 10 baseline tasks including named entity recognition,\nquestion answering, natural language inference, and document classification\ntasks.\n</p>\n<p>Results: The results demonstrate that Clinical-Longformer and\nClinical-BigBird consistently and significantly outperform ClinicalBERT and\nother short-sequence transformers in all 10 downstream tasks and achieve new\nstate-of-the-art results.\n</p>\n<p>Discussion: Our pre-trained language models provide the bedrock for clinical\nNLP using long texts. We have made our source code available at\nhttps://github.com/luoyuanlab/Clinical-Longformer, and the pre-trained models\navailable for public download at:\nhttps://huggingface.co/yikuan8/Clinical-Longformer.\n</p>\n<p>Conclusion: This study demonstrates that clinical knowledge enriched\nlong-sequence transformers are able to learn long-term dependencies in long\nclinical text. Our methods can also inspire the development of other\ndomain-enriched long-sequence transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yikuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wehbe_R/0/1/0/all/0/1\">Ramsey M. Wehbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_F/0/1/0/all/0/1\">Faraz S. Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yuan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factual or Biased? Predicting Sentence-Level Factuality and Bias of News. (arXiv:2301.11850v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11850","description":"<p>We present a study on sentence-level factuality and bias of news articles\nacross domains. While prior work in NLP has mainly focused on predicting the\nfactuality of article-level news reporting and political-ideological bias of\nnews media, we investigated the effects of framing bias in factual reporting\nacross domains so as to predict factuality and bias at the sentence level,\nwhich may explain more accurately the overall reliability of the entire\ndocument. First, we manually produced a large sentence-level annotated dataset,\ntitled FactNews, composed of 6,191 sentences from 100 news stories by three\ndifferent outlets, resulting in 300 news articles. Further, we studied how\nbiased and factual spans surface in news articles from different media outlets\nand different domains. Lastly, a baseline model for factual sentence prediction\nwas presented by fine-tuning BERT. We also provide a detailed analysis of data\ndemonstrating the reliability of the annotation and models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vargas_F/0/1/0/all/0/1\">Francielle Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goes_F/0/1/0/all/0/1\">Fabiana G&#xf3;es</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pardo_T/0/1/0/all/0/1\">Thiago A. S. Pardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benevenuto_F/0/1/0/all/0/1\">Fabr&#xed;cio Benevenuto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Case-Based Reasoning with Language Models for Classification of Logical Fallacies. (arXiv:2301.11879v1 [cs.AI])","link":"http://arxiv.org/abs/2301.11879","description":"<p>The ease and the speed of spreading misinformation and propaganda on the Web\nmotivate the need to develop trustworthy technology for detecting fallacies in\nnatural language arguments. However, state-of-the-art language modeling methods\nexhibit a lack of robustness on tasks like logical fallacy classification that\nrequire complex reasoning. In this paper, we propose a Case-Based Reasoning\nmethod that classifies new cases of logical fallacy by language-modeling-driven\nretrieval and adaptation of historical cases. We design four complementary\nstrategies to enrich the input representation for our model, based on external\ninformation about goals, explanations, counterarguments, and argument\nstructure. Our experiments in in-domain and out-of-domain settings indicate\nthat Case-Based Reasoning improves the accuracy and generalizability of\nlanguage models. Our ablation studies confirm that the representations of\nsimilar cases have a strong impact on the model performance, that models\nperform well with fewer retrieved cases, and that the size of the case database\nhas a negligible effect on the performance. Finally, we dive deeper into the\nrelationship between the properties of the retrieved cases and the model\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sourati_Z/0/1/0/all/0/1\">Zhivar Sourati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandlin_H/0/1/0/all/0/1\">H&#xf4;ng-&#xc2;n Sandlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mermoud_A/0/1/0/all/0/1\">Alain Mermoud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning. (arXiv:2301.11916v1 [cs.CL])","link":"http://arxiv.org/abs/2301.11916","description":"<p>In recent years, pre-trained large language models have demonstrated\nremarkable efficiency in achieving an inference-time few-shot learning\ncapability known as in-context learning. However, existing literature has\nhighlighted the sensitivity of this capability to the selection of few-shot\ndemonstrations. The underlying mechanisms by which this capability arises from\nregular language model pretraining objectives remain poorly understood. In this\nstudy, we aim to examine the in-context learning phenomenon through a Bayesian\nlens, viewing large language models as topic models that implicitly infer\ntask-related information from demonstrations. On this premise, we propose an\nalgorithm for selecting optimal demonstrations from a set of annotated data and\ndemonstrate a significant 12.5% improvement relative to the random selection\nbaseline, averaged over eight GPT2 and GPT3 models on eight different\nreal-world text classification datasets. Our empirical findings support our\nhypothesis that large language models implicitly infer a latent concept\nvariable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jointly Identifying and Fixing Inconsistent Readings from Information Extraction Systems. (arXiv:1808.04816v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1808.04816","description":"<p>KGCleaner is a framework to identify and correct errors in data produced and\ndelivered by an information extraction system. These tasks have been\nunderstudied and KGCleaner is the first to address both. We introduce a\nmulti-task model that jointly learns to predict if an extracted relation is\ncredible and repair it if not. We evaluate our approach and other models as\ninstance of our framework on two collections: a Wikidata corpus of nearly 700K\nfacts and 5M fact-relevant sentences and a collection of 30K facts from the\n2015 TAC Knowledge Base Population task. For credibility classification,\nparameter efficient simple shallow neural network can achieve an absolute\nperformance gain of 30 $F_1$ points on Wikidata and comparable performance on\nTAC. For the repair task, significant performance (at more than twice) gain can\nbe obtained depending on the nature of the dataset and the models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Padia_A/0/1/0/all/0/1\">Ankur Padia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraro_F/0/1/0/all/0/1\">Francis Ferraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finin_T/0/1/0/all/0/1\">Tim Finin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intrinsically Motivated Compositional Language Emergence. (arXiv:2012.05011v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.05011","description":"<p>Recently, there has been a great deal of research in emergent communication\non artificial agents interacting in simulated environments. Recent studies have\nrevealed that, in general, emergent languages do not follow the\ncompositionality patterns of natural language. To deal with this, existing\nworks have proposed a limited channel capacity as an important constraint for\nlearning highly compositional languages. In this paper, we show that this is\nnot a sufficient condition and propose an intrinsic reward framework for\nimproving compositionality in emergent communication. We use a reinforcement\nlearning setting with two agents -- a \\textit{task-aware} Speaker and a\n\\textit{state-aware} Listener that are required to communicate to perform a set\nof tasks. Through our experiments on three different referential game setups,\nincluding a novel environment gComm, we show intrinsic rewards improve\ncompositionality scores by $\\approx \\mathbf{1.5-2}$ times that of existing\nframeworks that use limited channel capacity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hazra_R/0/1/0/all/0/1\">Rishi Hazra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixit_S/0/1/0/all/0/1\">Sonu Dixit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_S/0/1/0/all/0/1\">Sayambhu Sen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Latent-State GPT for Semi-Supervised Task-Oriented Dialog Systems. (arXiv:2109.04314v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04314","description":"<p>Recently, two approaches, fine-tuning large pre-trained language models and\nvariational training, have attracted significant interests, separately, for\nsemi-supervised end-to-end task-oriented dialog (TOD) systems. In this paper,\nwe propose Variational Latent-State GPT model (VLS-GPT), which is the first to\ncombine the strengths of the two approaches. Among many options of models, we\npropose the generative model and the inference model for variational learning\nof the end-to-end TOD system, both as auto-regressive language models based on\nGPT-2, which can be further trained over a mix of labeled and unlabeled dialog\ndata in a semi-supervised manner. Variational training of VLS-GPT is both\nstatistically and computationally more challenging than previous variational\nlearning works for sequential latent variable models, which use turn-level\nfirst-order Markovian. The inference model in VLS-GPT is non-Markovian due to\nthe use of the Transformer architecture. In this work, we establish Recursive\nMonte Carlo Approximation (RMCA) to the variational objective with\nnon-Markovian inference model and prove its unbiasedness. Further, we develop\nthe computational strategy of sampling-then-forward-computation to realize\nRMCA, which successfully overcomes the memory explosion issue of using GPT in\nvariational learning and speeds up training. Semi-supervised TOD experiments\nare conducted on two benchmark multi-domain datasets of different languages -\nMultiWOZ2.1 and CrossWOZ. VLS-GPT is shown to significantly outperform both\nsupervised-only and semi-supervised self-training baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yucheng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhenru Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zhijian Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Junlan Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastKASSIM: A Fast Tree Kernel-Based Syntactic Similarity Metric. (arXiv:2203.08299v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08299","description":"<p>Syntax is a fundamental component of language, yet few metrics have been\nemployed to capture syntactic similarity or coherence at the utterance- and\ndocument-level. The existing standard document-level syntactic similarity\nmetric is computationally expensive and performs inconsistently when faced with\nsyntactically dissimilar documents. To address these challenges, we present\nFastKASSIM, a metric for utterance- and document-level syntactic similarity\nwhich pairs and averages the most similar constituency parse trees between a\npair of documents based on tree kernels. FastKASSIM is more robust to syntactic\ndissimilarities and runs up to to 5.32 times faster than its predecessor over\ndocuments in the r/ChangeMyView corpus. FastKASSIM's improvements allow us to\nexamine hypotheses in two settings with large documents. We find that\nsyntactically similar arguments on r/ChangeMyView tend to be more persuasive,\nand that syntax is predictive of authorship attribution in the Australian High\nCourt Judgment corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Maximillian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Caitlyn Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Low-Resource NLP. (arXiv:2206.10265v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.10265","description":"<p>This paper focuses on the data augmentation for low-resource NLP tasks where\nthe training set is limited. The existing solutions either leverage\ntask-independent heuristic rules (e.g., Synonym Replacement) or fine-tune\ngeneral-purpose pre-trained language models (e.g., GPT2) using the limited\ntraining instances to produce new synthetic data. Consequently, they have\ntrivial task-specific knowledge and are limited to yielding low-quality\nsynthetic data. To combat this issue, we propose Knowledge Mixture Data\nAugmentation Model (KnowDA) which is an Seq2Seq language model pre-trained on a\nmixture of diverse NLP tasks under a novel framework of Knowledge Mixture\nTraining (KoMT). The goal of KoMT is to condense diverse NLP task-specific\nknowledge into the single KnowDA model (i.e., all-in-one) such that KnowDA\ncould utilize these knowledge to quickly grasp the inherent synthesis law of\nthe target task through limited training instances. Specifically, KoMT\nreformulates input examples from various heterogeneous NLP tasks into a unified\ntext-to-text format, and employs denoising training objectives in different\ngranularity to learn to reconstruct partial or complete samples. To the best of\nour knowledge, we are the first attempt to apply 100+ NLP multi-task training\nfor data augmentation. Extensive experiments show that i) the synthetic data\nproduced by KnowDA successfully improves performance of the strong pre-trained\nlanguage models (i.e., Bert, ALBert and Deberta) by a large margin on the\nlow-resource NLP benchmark FewGLUE, CoNLL'03 and WikiAnn; ii) KnowDA\nsuccessfully transfers the task knowledge to NLP tasks whose types are seen and\nunseen in KoMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiayi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search. (arXiv:2207.09068v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.09068","description":"<p>While contextualized word embeddings have been a de-facto standard, learning\ncontextualized phrase embeddings is less explored and being hindered by the\nlack of a human-annotated benchmark that tests machine understanding of phrase\nsemantics given a context sentence or paragraph (instead of phrases alone). To\nfill this gap, we propose PiC -- a dataset of ~28K of noun phrases accompanied\nby their contextual Wikipedia pages and a suite of three tasks for training and\nevaluating phrase embeddings. Training on PiC improves ranking models' accuracy\nand remarkably pushes span-selection (SS) models (i.e., predicting the start\nand end index of the target phrase) near-human accuracy, which is 95% Exact\nMatch (EM) on semantic search given a query phrase and a passage.\nInterestingly, we find evidence that such impressive performance is because the\nSS models learn to better capture the common meaning of a phrase regardless of\nits actual context. SotA models perform poorly in distinguishing two senses of\nthe same phrase in two contexts (~60% EM) and in estimating the similarity\nbetween two different phrases in the same context (~70% EM).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1\">Thang M. Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Patterns in Data with Language Models via Interpretable Autoprompting. (arXiv:2210.01848v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.01848","description":"<p>Large language models (LLMs) have displayed an impressive ability to harness\nnatural language to perform complex tasks. In this work, we explore whether we\ncan leverage this learned ability to find and explain patterns in data.\nSpecifically, given a pre-trained LLM and data examples, we introduce\ninterpretable autoprompting (iPrompt), an algorithm that generates a\nnatural-language string explaining the data. iPrompt iteratively alternates\nbetween generating explanations with an LLM and reranking them based on their\nperformance when used as a prompt. Experiments on a wide range of datasets,\nfrom synthetic mathematics to natural-language understanding, show that iPrompt\ncan yield meaningful insights by accurately finding groundtruth dataset\ndescriptions. Moreover, the prompts produced by iPrompt are simultaneously\nhuman-interpretable and highly effective for generalization: on real-world\nsentiment classification datasets, iPrompt produces prompts that match or even\nimprove upon human-written prompts for GPT-3. Finally, experiments with an fMRI\ndataset show the potential for iPrompt to aid in scientific discovery. All code\nfor using the methods and data here is made available on Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1\">Chandan Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_J/0/1/0/all/0/1\">John X. Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aneja_J/0/1/0/all/0/1\">Jyoti Aneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PAL: Program-aided Language Models. (arXiv:2211.10435v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.10435","description":"<p>Large language models (LLMs) have recently demonstrated an impressive ability\nto perform arithmetic and symbolic reasoning tasks, when provided with a few\nexamples at test time (\"few-shot prompting\"). Much of this success can be\nattributed to prompting methods such as \"chain-of-thought'', which employ LLMs\nfor both understanding the problem description by decomposing it into steps, as\nwell as solving each step of the problem. While LLMs seem to be adept at this\nsort of step-by-step decomposition, LLMs often make logical and arithmetic\nmistakes in the solution part, even when the problem is decomposed correctly.\nIn this paper, we present Program-Aided Language models (PAL): a novel approach\nthat uses the LLM to read natural language problems and generate programs as\nthe intermediate reasoning steps, but offloads the solution step to a runtime\nsuch as a Python interpreter. With PAL, decomposing the natural language\nproblem into runnable steps remains the only learning task for the LLM, while\nsolving is delegated to the interpreter. We demonstrate this synergy between a\nneural LLM and a symbolic interpreter across 13 mathematical, symbolic, and\nalgorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all\nthese natural language reasoning tasks, generating code using an LLM and\nreasoning using a Python interpreter leads to more accurate results than much\nlarger models. For example, PAL using Codex achieves state-of-the-art few-shot\naccuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B\nwhich uses chain-of-thought by absolute 15% top-1. Our code and data are\npublicly available at <a href=\"http://reasonwithpal.com/\">this http URL</a> .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Luyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1\">Uri Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callan_J/0/1/0/all/0/1\">Jamie Callan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal and Explainable Internet Meme Classification. (arXiv:2212.05612v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2212.05612","description":"<p>Warning: this paper contains content that may be offensive or upsetting. In\nthe current context where online platforms have been effectively weaponized in\na variety of geo-political events and social issues, Internet memes make fair\ncontent moderation at scale even more difficult. Existing work on meme\nclassification and tracking has focused on black-box methods that do not\nexplicitly consider the semantics of the memes or the context of their\ncreation. In this paper, we pursue a modular and explainable architecture for\nInternet meme understanding. We design and implement multimodal classification\nmethods that perform example- and prototype-based reasoning over training\ncases, while leveraging both textual and visual SOTA models to represent the\nindividual cases. We study the relevance of our modular and explainable models\nin detecting harmful memes on two existing tasks: Hate Speech Detection and\nMisogyny Classification. We compare the performance between example- and\nprototype-based methods, and between text, vision, and multimodal models,\nacross different categories of harmfulness (e.g., stereotype and\nobjectification). We devise a user-friendly interface that facilitates the\ncomparative analysis of examples retrieved by all of our models for any given\nmeme, informing the community about the strengths and limitations of these\nexplainable methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1\">Abhinav Kumar Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandlin_H/0/1/0/all/0/1\">H&#xf4;ng-&#xc2;n Sandlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mermoud_A/0/1/0/all/0/1\">Alain Mermoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sourati_Z/0/1/0/all/0/1\">Zhivar Sourati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luceri_L/0/1/0/all/0/1\">Luca Luceri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tommasini_R/0/1/0/all/0/1\">Riccardo Tommasini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study of Slang Representation Methods. (arXiv:2212.05613v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.05613","description":"<p>Warning: this paper contains content that may be offensive or upsetting.\nConsidering the large amount of content created online by the minute,\nslang-aware automatic tools are critically needed to promote social good, and\nassist policymakers and moderators in restricting the spread of offensive\nlanguage, abuse, and hate speech. Despite the success of large language models\nand the spontaneous emergence of slang dictionaries, it is unclear how far\ntheir combination goes in terms of slang understanding for downstream social\ngood tasks. In this paper, we provide a framework to study different\ncombinations of representation learning models and knowledge resources for a\nvariety of downstream tasks that rely on slang understanding. Our experiments\nshow the superiority of models that have been pre-trained on social media data,\nwhile the impact of dictionaries is positive only for static word embeddings.\nOur error analysis identifies core challenges for slang representation\nlearning, including out-of-vocabulary words, polysemy, variance, and annotation\ndisagreements, which can be traced to characteristics of slang as a quickly\nevolving and highly subjective language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolla_A/0/1/0/all/0/1\">Aravinda Kolla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandlin_H/0/1/0/all/0/1\">H&#xf4;ng-&#xc2;n Sandlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mermoud_A/0/1/0/all/0/1\">Alain Mermoud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Watermark for Large Language Models. (arXiv:2301.10226v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2301.10226","description":"<p>Potential harms of large language models can be mitigated by watermarking\nmodel output, i.e., embedding signals into generated text that are invisible to\nhumans but algorithmically detectable from a short span of tokens. We propose a\nwatermarking framework for proprietary language models. The watermark can be\nembedded with negligible impact on text quality, and can be detected using an\nefficient open-source algorithm without access to the language model API or\nparameters. The watermark works by selecting a randomized set of \"green\" tokens\nbefore a word is generated, and then softly promoting use of green tokens\nduring sampling. We propose a statistical test for detecting the watermark with\ninterpretable p-values, and derive an information-theoretic framework for\nanalyzing the sensitivity of the watermark. We test the watermark using a\nmulti-billion parameter model from the Open Pretrained Transformer (OPT)\nfamily, and discuss robustness and security.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirchenbauer_J/0/1/0/all/0/1\">John Kirchenbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1\">Jonas Geiping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_J/0/1/0/all/0/1\">Jonathan Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miers_I/0/1/0/all/0/1\">Ian Miers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Commonsense Knowledge Salience Evaluation with a Benchmark Dataset in E-commerce. (arXiv:2205.10843v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2205.10843","description":"<p>In e-commerce, the salience of commonsense knowledge (CSK) is beneficial for\nwidespread applications such as product search and recommendation. For example,\nwhen users search for ``running'' in e-commerce, they would like to find\nproducts highly related to running, such as ``running shoes'' rather than\n``shoes''. Nevertheless, many existing CSK collections rank statements solely\nby confidence scores, and there is no information about which ones are salient\nfrom a human perspective. In this work, we define the task of supervised\nsalience evaluation, where given a CSK triple, the model is required to learn\nwhether the triple is salient or not. In addition to formulating the new task,\nwe also release a new Benchmark dataset of Salience Evaluation in E-commerce\n(BSEE) and hope to promote related research on commonsense knowledge salience\nevaluation. We conduct experiments in the dataset with several representative\nbaseline models. The experimental results show that salience evaluation is a\nchallenging task where models perform poorly on our evaluation set. We further\npropose a simple but effective approach, PMI-tuning, which shows promise for\nsolving this novel problem. Code is available in\n\\url{https://github.com/OpenBGBenchmark/OpenBG-CSK.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yincen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zelin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zezhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-01-29T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}