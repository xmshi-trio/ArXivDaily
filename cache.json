{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-07-03T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Empowering NLG: Offline Reinforcement Learning for Informal Summarization in Online Domains. (arXiv:2306.17174v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17174","description":"<p>Our research introduces an innovative Natural Language Generation (NLG)\napproach that aims to optimize user experience and alleviate the workload of\nhuman customer support agents. Our primary objective is to generate informal\nsummaries for online articles and posts using an offline reinforcement learning\ntechnique. In our study, we compare our proposed method with existing\napproaches to text generation and provide a comprehensive overview of our\narchitectural design, which incorporates crawling, reinforcement learning, and\ntext generation modules. By presenting this original approach, our paper makes\na valuable contribution to the field of NLG by offering a fresh perspective on\ngenerating natural language summaries for online content. Through the\nimplementation of Empowering NLG, we are able to generate higher-quality\nreplies in the online domain. The experimental results demonstrate a\nsignificant improvement in the average \"like\" score, increasing from 0.09954378\nto 0.5000152. This advancement has the potential to enhance the efficiency and\neffectiveness of customer support services and elevate the overall user\nexperience when consuming online content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tai_Z/0/1/0/all/0/1\">Zhi-Xuan Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Po-Chuan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RECAP-KG: Mining Knowledge Graphs from Raw GP Notes for Remote COVID-19 Assessment in Primary Care. (arXiv:2306.17175v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17175","description":"<p>Clinical decision-making is a fundamental stage in delivering appropriate\ncare to patients. In recent years several decision-making systems designed to\naid the clinician in this process have been developed. However, technical\nsolutions currently in use are based on simple regression models and are only\nable to take into account simple pre-defined multiple-choice features, such as\npatient age, pre-existing conditions, smoker status, etc. One particular source\nof patient data, that available decision-making systems are incapable of\nprocessing is the collection of patient consultation GP notes. These contain\ncrucial signs and symptoms - the information used by clinicians in order to\nmake a final decision and direct the patient to the appropriate care.\nExtracting information from GP notes is a technically challenging problem, as\nthey tend to include abbreviations, typos, and incomplete sentences.\n</p>\n<p>This paper addresses this open challenge. We present a framework that\nperforms knowledge graph construction from raw GP medical notes written during\nor after patient consultations. By relying on support phrases mined from the\nSNOMED ontology, as well as predefined supported facts from values used in the\nRECAP (REmote COVID-19 Assessment in Primary Care) patient risk prediction\ntool, our graph generative framework is able to extract structured knowledge\ngraphs from the highly unstructured and inconsistent format that consultation\nnotes are written in. Our knowledge graphs include information about existing\npatient symptoms, their duration, and their severity.\n</p>\n<p>We apply our framework to consultation notes of COVID-19 patients in the UK\nCOVID-19 Clinical Assesment Servcie (CCAS) patient dataset. We provide a\nquantitative evaluation of the performance of our framework, demonstrating that\nour approach has better accuracy than traditional NLP methods when answering\nquestions about patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mekhtieva_R/0/1/0/all/0/1\">Rakhilya Lee Mekhtieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forbes_B/0/1/0/all/0/1\">Brandon Forbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alrajeh_D/0/1/0/all/0/1\">Dalal Alrajeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delaney_B/0/1/0/all/0/1\">Brendan Delaney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russo_A/0/1/0/all/0/1\">Alessandra Russo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"News Verifiers Showdown: A Comparative Performance Evaluation of ChatGPT 3.5, ChatGPT 4.0, Bing AI, and Bard in News Fact-Checking. (arXiv:2306.17176v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17176","description":"<p>This study aimed to evaluate the proficiency of prominent Large Language\nModels (LLMs), namely OpenAI's ChatGPT 3.5 and 4.0, Google's Bard(LaMDA), and\nMicrosoft's Bing AI in discerning the truthfulness of news items using black\nbox testing. A total of 100 fact-checked news items, all sourced from\nindependent fact-checking agencies, were presented to each of these LLMs under\ncontrolled conditions. Their responses were classified into one of three\ncategories: True, False, and Partially True/False. The effectiveness of the\nLLMs was gauged based on the accuracy of their classifications against the\nverified facts provided by the independent agencies. The results showed a\nmoderate proficiency across all models, with an average score of 65.25 out of\n100. Among the models, OpenAI's GPT-4.0 stood out with a score of 71,\nsuggesting an edge in newer LLMs' abilities to differentiate fact from\ndeception. However, when juxtaposed against the performance of human\nfact-checkers, the AI models, despite showing promise, lag in comprehending the\nsubtleties and contexts inherent in news information. The findings highlight\nthe potential of AI in the domain of fact-checking while underscoring the\ncontinued importance of human cognitive skills and the necessity for persistent\nadvancements in AI capabilities. Finally, the experimental data produced from\nthe simulation of this work is openly available on Kaggle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Caramancion_K/0/1/0/all/0/1\">Kevin Matthe Caramancion</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging ChatGPT As Text Annotation Tool For Sentiment Analysis. (arXiv:2306.17177v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17177","description":"<p>Sentiment analysis is a well-known natural language processing task that\ninvolves identifying the emotional tone or polarity of a given piece of text.\nWith the growth of social media and other online platforms, sentiment analysis\nhas become increasingly crucial for businesses and organizations seeking to\nmonitor and comprehend customer feedback as well as opinions. Supervised\nlearning algorithms have been popularly employed for this task, but they\nrequire human-annotated text to create the classifier. To overcome this\nchallenge, lexicon-based tools have been used. A drawback of lexicon-based\nalgorithms is their reliance on pre-defined sentiment lexicons, which may not\ncapture the full range of sentiments in natural language. ChatGPT is a new\nproduct of OpenAI and has emerged as the most popular AI product. It can answer\nquestions on various topics and tasks. This study explores the use of ChatGPT\nas a tool for data labeling for different sentiment analysis tasks. It is\nevaluated on two distinct sentiment analysis datasets with varying purposes.\nThe results demonstrate that ChatGPT outperforms other lexicon-based\nunsupervised methods with significant improvements in overall accuracy.\nSpecifically, compared to the best-performing lexical-based algorithms, ChatGPT\nachieves a remarkable increase in accuracy of 20% for the tweets dataset and\napproximately 25% for the Amazon reviews dataset. These findings highlight the\nexceptional performance of ChatGPT in sentiment analysis tasks, surpassing\nexisting lexicon-based approaches by a significant margin. The evidence\nsuggests it can be used for annotation on different sentiment analysis events\nand taskss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belal_M/0/1/0/all/0/1\">Mohammad Belal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_J/0/1/0/all/0/1\">James She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_S/0/1/0/all/0/1\">Simon Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Replace and Report: NLP Assisted Radiology Report Generation. (arXiv:2306.17180v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17180","description":"<p>Clinical practice frequently uses medical imaging for diagnosis and\ntreatment. A significant challenge for automatic radiology report generation is\nthat the radiology reports are long narratives consisting of multiple sentences\nfor both abnormal and normal findings. Therefore, applying conventional image\ncaptioning approaches to generate the whole report proves to be insufficient,\nas these are designed to briefly describe images with short sentences. We\npropose a template-based approach to generate radiology reports from\nradiographs. Our approach involves the following: i) using a multilabel image\nclassifier, produce the tags for the input radiograph; ii) using a\ntransformer-based model, generate pathological descriptions (a description of\nabnormal findings seen on radiographs) from the tags generated in step (i);\niii) using a BERT-based multi-label text classifier, find the spans in the\nnormal report template to replace with the generated pathological descriptions;\nand iv) using a rule-based system, replace the identified span with the\ngenerated pathological description. We performed experiments with the two most\npopular radiology report datasets, IU Chest X-ray and MIMIC-CXR and\ndemonstrated that the BLEU-1, ROUGE-L, METEOR, and CIDEr scores are better than\nthe State-of-the-Art models by 25%, 36%, 44% and 48% respectively, on the IU\nX-RAY dataset. To the best of our knowledge, this is the first attempt to\ngenerate chest X-ray radiology reports by first creating small sentences for\nabnormal findings and then replacing them in the normal report template.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kale_K/0/1/0/all/0/1\">Kaveri Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_p/0/1/0/all/0/1\">pushpak Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jadhav_K/0/1/0/all/0/1\">Kshitij Jadhav</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis. (arXiv:2306.17181v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17181","description":"<p>Generative Adversarial Networks (GAN) is a model for data synthesis, which\ncreates plausible data through the competition of generator and discriminator.\nAlthough GAN application to image synthesis is extensively studied, it has\ninherent limitations to natural language generation. Because natural language\nis composed of discrete tokens, a generator has difficulty updating its\ngradient through backpropagation; therefore, most text-GAN studies generate\nsentences starting with a random token based on a reward system. Thus, the\ngenerators of previous studies are pre-trained in an autoregressive way before\nadversarial training, causing data memorization that synthesized sentences\nreproduce the training data. In this paper, we synthesize sentences using a\nframework similar to the original GAN. More specifically, we propose Text\nEmbedding Space Generative Adversarial Networks (TESGAN) which generate\ncontinuous text embedding spaces instead of discrete tokens to solve the\ngradient backpropagation problem. Furthermore, TESGAN conducts unsupervised\nlearning which does not directly refer to the text of the training data to\novercome the data memorization issue. By adopting this novel method, TESGAN can\nsynthesize new sentences, showing the potential of unsupervised learning for\ntext synthesis. We expect to see extended research combining Large Language\nModels with a new perspective of viewing text as an continuous space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jun-Min Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_T/0/1/0/all/0/1\">Tae-Bin Ha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why can neural language models solve next-word prediction? A mathematical perspective. (arXiv:2306.17184v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17184","description":"<p>Recently, deep learning has revolutionized the field of natural language\nprocessing, with neural language models proving to be very effective for\nnext-word prediction. However, a rigorous theoretical explanation for their\nsuccess in the context of formal language theory has not yet been developed, as\nit is unclear why neural language models can learn the combinatorial rules that\ngovern the next-word prediction task. In this paper, we study a class of formal\nlanguages that can be used to model real-world examples of English sentences.\nWe construct neural language models can solve the next-word prediction task in\nthis context with zero error. Our proof highlights the different roles of the\nembedding layer and the fully connected component within the neural language\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nandakumar_V/0/1/0/all/0/1\">Vinoth Nandakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_P/0/1/0/all/0/1\">Peng Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Exploitability of Instruction Tuning. (arXiv:2306.17194v1 [cs.CR])","link":"http://arxiv.org/abs/2306.17194","description":"<p>Instruction tuning is an effective technique to align large language models\n(LLMs) with human intents. In this work, we investigate how an adversary can\nexploit instruction tuning by injecting specific instruction-following examples\ninto the training data that intentionally changes the model's behavior. For\nexample, an adversary can achieve content injection by injecting training\nexamples that mention target content and eliciting such behavior from\ndownstream models. To achieve this goal, we propose \\textit{AutoPoison}, an\nautomated data poisoning pipeline. It naturally and coherently incorporates\nversatile attack goals into poisoned data with the help of an oracle LLM. We\nshowcase two example attacks: content injection and over-refusal attacks, each\naiming to induce a specific exploitable behavior. We quantify and benchmark the\nstrength and the stealthiness of our data poisoning scheme. Our results show\nthat AutoPoison allows an adversary to change a model's behavior by poisoning\nonly a small fraction of data while maintaining a high level of stealthiness in\nthe poisoned examples. We hope our work sheds light on how data quality affects\nthe behavior of instruction-tuned models and raises awareness of the importance\nof data quality for responsible deployments of LLMs. Code is available at\n\\url{https://github.com/azshue/AutoPoison}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shu_M/0/1/0/all/0/1\">Manli Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiongxiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1\">Jonas Geiping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Multilingual Expressive Speech Representation for Prosody Prediction without Parallel Data. (arXiv:2306.17199v1 [eess.AS])","link":"http://arxiv.org/abs/2306.17199","description":"<p>We propose a method for speech-to-speech emotionpreserving translation that\noperates at the level of discrete speech units. Our approach relies on the use\nof multilingual emotion embedding that can capture affective information in a\nlanguage-independent manner. We show that this embedding can be used to predict\nthe pitch and duration of speech units in a target language, allowing us to\nresynthesize the source speech signal with the same emotional content. We\nevaluate our approach to English and French speech signals and show that it\noutperforms a baseline method that does not use emotional information,\nincluding when the emotion embedding is extracted from a different language.\nEven if this preliminary study does not address directly the machine\ntranslation issue, our results demonstrate the effectiveness of our approach\nfor cross-lingual emotion preservation in the context of speech resynthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Duret_J/0/1/0/all/0/1\">Jarod Duret</a> (LIA), <a href=\"http://arxiv.org/find/eess/1/au:+Parcollet_T/0/1/0/all/0/1\">Titouan Parcollet</a> (CAM), <a href=\"http://arxiv.org/find/eess/1/au:+Esteve_Y/0/1/0/all/0/1\">Yannick Est&#xe8;ve</a> (LIA)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Personalized Cold-Start Recommendation with Prompts. (arXiv:2306.17256v1 [cs.IR])","link":"http://arxiv.org/abs/2306.17256","description":"<p>Recommender systems play a crucial role in helping users discover information\nthat aligns with their interests based on their past behaviors. However,\ndeveloping personalized recommendation systems becomes challenging when\nhistorical records of user-item interactions are unavailable, leading to what\nis known as the system cold-start recommendation problem. This issue is\nparticularly prominent in start-up businesses or platforms with insufficient\nuser engagement history. Previous studies focus on user or item cold-start\nscenarios, where systems could make recommendations for new users or items but\nare still trained with historical user-item interactions in the same domain,\nwhich cannot solve our problem. To bridge the gap, our research introduces an\ninnovative and effective approach, capitalizing on the capabilities of\npre-trained language models. We transform the recommendation process into\nsentiment analysis of natural languages containing information of user profiles\nand item attributes, where the sentiment polarity is predicted with prompt\nlearning. By harnessing the extensive knowledge housed within language models,\nthe prediction can be made without historical user-item interaction records. A\nbenchmark is also introduced to evaluate the proposed method under the\ncold-start setting, and the results demonstrate the effectiveness of our\nmethod. To the best of our knowledge, this is the first study to tackle the\nsystem cold-start recommendation problem. The benchmark and implementation of\nthe method are available at https://github.com/JacksonWuxs/PromptRec.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xuansheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huachi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wenlin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ninghao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prediction of COVID-19 Patients' Emergency Room Revisit using Multi-Source Transfer Learning. (arXiv:2306.17257v1 [cs.LG])","link":"http://arxiv.org/abs/2306.17257","description":"<p>The coronavirus disease 2019 (COVID-19) has led to a global pandemic of\nsignificant severity. In addition to its high level of contagiousness, COVID-19\ncan have a heterogeneous clinical course, ranging from asymptomatic carriers to\nsevere and potentially life-threatening health complications. Many patients\nhave to revisit the emergency room (ER) within a short time after discharge,\nwhich significantly increases the workload for medical staff. Early\nidentification of such patients is crucial for helping physicians focus on\ntreating life-threatening cases. In this study, we obtained Electronic Health\nRecords (EHRs) of 3,210 encounters from 13 affiliated ERs within the University\nof Pittsburgh Medical Center between March 2020 and January 2021. We leveraged\na Natural Language Processing technique, ScispaCy, to extract clinical concepts\nand used the 1001 most frequent concepts to develop 7-day revisit models for\nCOVID-19 patients in ERs. The research data we collected from 13 ERs may have\ndistributional differences that could affect the model development. To address\nthis issue, we employed a classic deep transfer learning method called the\nDomain Adversarial Neural Network (DANN) and evaluated different modeling\nstrategies, including the Multi-DANN algorithm, the Single-DANN algorithm, and\nthree baseline methods. Results showed that the Multi-DANN models outperformed\nthe Single-DANN models and baseline models in predicting revisits of COVID-19\npatients to the ER within 7 days after discharge. Notably, the Multi-DANN\nstrategy effectively addressed the heterogeneity among multiple source domains\nand improved the adaptation of source data to the target domain. Moreover, the\nhigh performance of Multi-DANN models indicates that EHRs are informative for\ndeveloping a prediction model to identify COVID-19 patients who are very likely\nto revisit an ER within 7 days after discharge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yuelyu Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuhe Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_R/0/1/0/all/0/1\">Runxue Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Disheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yiming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Ye Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Open-Domain Topic Classification. (arXiv:2306.17290v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17290","description":"<p>We introduce an open-domain topic classification system that accepts\nuser-defined taxonomy in real time. Users will be able to classify a text\nsnippet with respect to any candidate labels they want, and get instant\nresponse from our web interface. To obtain such flexibility, we build the\nbackend model in a zero-shot way. By training on a new dataset constructed from\nWikipedia, our label-aware text classifier can effectively utilize implicit\nknowledge in the pretrained language model to handle labels it has never seen\nbefore. We evaluate our model across four datasets from various domains with\ndifferent label sets. Experiments show that the model significantly improves\nover existing zero-shot baselines in open-domain scenarios, and performs\ncompetitively with weakly-supervised models trained on in-domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Hantian Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinrui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yuqian Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Citations as Queries: Source Attribution Using Language Models as Rerankers. (arXiv:2306.17322v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17322","description":"<p>This paper explores new methods for locating the sources used to write a\ntext, by fine-tuning a variety of language models to rerank candidate sources.\nAfter retrieving candidates sources using a baseline BM25 retrieval model, a\nvariety of reranking methods are tested to see how effective they are at the\ntask of source attribution. We conduct experiments on two datasets, English\nWikipedia and medieval Arabic historical writing, and employ a variety of\nretrieval and generation based reranking models. In particular, we seek to\nunderstand how the degree of supervision required affects the performance of\nvarious reranking models. We find that semisupervised methods can be nearly as\neffective as fully supervised methods while avoiding potentially costly\nspan-level annotation of the target and source documents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muther_R/0/1/0/all/0/1\">Ryan Muther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_D/0/1/0/all/0/1\">David Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SummQA at MEDIQA-Chat 2023:In-Context Learning with GPT-4 for Medical Summarization. (arXiv:2306.17384v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17384","description":"<p>Medical dialogue summarization is challenging due to the unstructured nature\nof medical conversations, the use of medical terminology in gold summaries, and\nthe need to identify key information across multiple symptom sets. We present a\nnovel system for the Dialogue2Note Medical Summarization tasks in the MEDIQA\n2023 Shared Task. Our approach for section-wise summarization (Task A) is a\ntwo-stage process of selecting semantically similar dialogues and using the\ntop-k similar dialogues as in-context examples for GPT-4. For full-note\nsummarization (Task B), we use a similar solution with k=1. We achieved 3rd\nplace in Task A (2nd among all teams), 4th place in Task B Division Wise\nSummarization (2nd among all teams), 15th place in Task A Section Header\nClassification (9th among all teams), and 8th place among all teams in Task B.\nOur results highlight the effectiveness of few-shot prompting for this task,\nthough we also identify several weaknesses of prompting-based approaches. We\ncompare GPT-4 performance with several finetuned baselines. We find that GPT-4\nsummaries are more abstractive and shorter. We make our code publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mathur_Y/0/1/0/all/0/1\">Yash Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangreji_S/0/1/0/all/0/1\">Sanketh Rangreji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_R/0/1/0/all/0/1\">Raghav Kapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palavalli_M/0/1/0/all/0/1\">Medha Palavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertsch_A/0/1/0/all/0/1\">Amanda Bertsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gormley_M/0/1/0/all/0/1\">Matthew R. Gormley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Japanese Lexical Complexity for Non-Native Readers: A New Dataset. (arXiv:2306.17399v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17399","description":"<p>Lexical complexity prediction (LCP) is the task of predicting the complexity\nof words in a text on a continuous scale. It plays a vital role in simplifying\nor annotating complex words to assist readers. To study lexical complexity in\nJapanese, we construct the first Japanese LCP dataset. Our dataset provides\nseparate complexity scores for Chinese/Korean annotators and others to address\nthe readers' L1-specific needs. In the baseline experiment, we demonstrate the\neffectiveness of a BERT-based system for Japanese LCP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ide_Y/0/1/0/all/0/1\">Yusuke Ide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mita_M/0/1/0/all/0/1\">Masato Mita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nohejl_A/0/1/0/all/0/1\">Adam Nohejl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouchi_H/0/1/0/all/0/1\">Hiroki Ouchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1\">Taro Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LMBot: Distilling Graph Knowledge into Language Model for Graph-less Deployment in Twitter Bot Detection. (arXiv:2306.17408v1 [cs.AI])","link":"http://arxiv.org/abs/2306.17408","description":"<p>As malicious actors employ increasingly advanced and widespread bots to\ndisseminate misinformation and manipulate public opinion, the detection of\nTwitter bots has become a crucial task. Though graph-based Twitter bot\ndetection methods achieve state-of-the-art performance, we find that their\ninference depends on the neighbor users multi-hop away from the targets, and\nfetching neighbors is time-consuming and may introduce bias. At the same time,\nwe find that after finetuning on Twitter bot detection, pretrained language\nmodels achieve competitive performance and do not require a graph structure\nduring deployment. Inspired by this finding, we propose a novel bot detection\nframework LMBot that distills the knowledge of graph neural networks (GNNs)\ninto language models (LMs) for graph-less deployment in Twitter bot detection\nto combat the challenge of data dependency. Moreover, LMBot is compatible with\ngraph-based and graph-less datasets. Specifically, we first represent each user\nas a textual sequence and feed them into the LM for domain adaptation. For\ngraph-based datasets, the output of LMs provides input features for the GNN,\nenabling it to optimize for bot detection and distill knowledge back to the LM\nin an iterative, mutually enhancing process. Armed with the LM, we can perform\ngraph-less inference, which resolves the graph data dependency and sampling\nbias issues. For datasets without graph structure, we simply replace the GNN\nwith an MLP, which has also shown strong performance. Our experiments\ndemonstrate that LMBot achieves state-of-the-art performance on four Twitter\nbot detection benchmarks. Extensive studies also show that LMBot is more\nrobust, versatile, and efficient compared to graph-based Twitter bot detection\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zijian Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhaoxuan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1\">Zhenyu Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zifeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qinghua Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Minnan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Provable Robust Watermarking for AI-Generated Text. (arXiv:2306.17439v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17439","description":"<p>As AI-generated text increasingly resembles human-written content, the\nability to detect machine-generated text becomes crucial. To address this\nchallenge, we present GPTWatermark, a robust and high-quality solution designed\nto ascertain whether a piece of text originates from a specific model. Our\napproach extends existing watermarking strategies and employs a fixed group\ndesign to enhance robustness against editing and paraphrasing attacks. We show\nthat our watermarked language model enjoys strong provable guarantees on\ngeneration quality, correctness in detection, and security against evasion\nattacks. Experimental results on various large language models (LLMs) and\ndiverse datasets demonstrate that our method achieves superior detection\naccuracy and comparable generation quality in perplexity, thus promoting the\nresponsible use of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xuandong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananth_P/0/1/0/all/0/1\">Prabhanjan Ananth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Multi-task Learning Framework for Chinese Text Error Correction. (arXiv:2306.17447v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17447","description":"<p>Chinese Text Error Correction (CTEC) aims to detect and correct errors in the\ninput text, which benefits human's daily life and various downstream tasks.\nRecent approaches mainly employ Pre-trained Language Models (PLMs) to resolve\nCTEC task and achieve tremendous success. However, previous approaches suffer\nfrom issues of over-correction and under-correction, and the former is\nespecially conspicuous in the precision-critical CTEC task. To mitigate the\nissue of overcorrection, we propose a novel model-agnostic progressive\nmultitask learning framework for CTEC, named ProTEC, which guides a CTEC model\nto learn the task from easy to difficult. We divide CTEC task into three\nsub-tasks from easy to difficult: Error Detection, Error Type Identification,\nand Correction Result Generation. During the training process, ProTEC guides\nthe model to learn text error correction progressively by incorporating these\nsub-tasks into a multi-task training objective. During the inference process,\nthe model completes these sub-tasks in turn to generate the correction results.\nExtensive experiments and detailed analyses fully demonstrate the effectiveness\nand efficiency of our proposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shirong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haojing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shulin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harnessing LLMs in Curricular Design: Using GPT-4 to Support Authoring of Learning Objectives. (arXiv:2306.17459v1 [cs.AI])","link":"http://arxiv.org/abs/2306.17459","description":"<p>We evaluated the capability of a generative pre-trained transformer (GPT-4)\nto automatically generate high-quality learning objectives (LOs) in the context\nof a practically oriented university course on Artificial Intelligence.\nDiscussions of opportunities (e.g., content generation, explanation) and risks\n(e.g., cheating) of this emerging technology in education have intensified, but\nto date there has not been a study of the models' capabilities in supporting\nthe course design and authoring of LOs. LOs articulate the knowledge and skills\nlearners are intended to acquire by engaging with a course. To be effective,\nLOs must focus on what students are intended to achieve, focus on specific\ncognitive processes, and be measurable. Thus, authoring high-quality LOs is a\nchallenging and time consuming (i.e., expensive) effort. We evaluated 127 LOs\nthat were automatically generated based on a carefully crafted prompt (detailed\nguidelines on high-quality LOs authoring) submitted to GPT-4 for conceptual\nmodules and projects of an AI Practitioner course. We analyzed the generated\nLOs if they follow certain best practices such as beginning with action verbs\nfrom Bloom's taxonomy in regards to the level of sophistication intended. Our\nanalysis showed that the generated LOs are sensible, properly expressed (e.g.,\nstarting with an action verb), and that they largely operate at the appropriate\nlevel of Bloom's taxonomy, respecting the different nature of the conceptual\nmodules (lower levels) and projects (higher levels). Our results can be\nleveraged by instructors and curricular designers wishing to take advantage of\nthe state-of-the-art generative models to support their curricular and course\ndesign efforts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_P/0/1/0/all/0/1\">Pragnya Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doyle_A/0/1/0/all/0/1\">Aidan Doyle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Arav Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogart_C/0/1/0/all/0/1\">Christopher Bogart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savelka_J/0/1/0/all/0/1\">Jaromir Savelka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakr_M/0/1/0/all/0/1\">Majd Sakr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Base Completion for Long-Tail Entities. (arXiv:2306.17472v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17472","description":"<p>Despite their impressive scale, knowledge bases (KBs), such as Wikidata,\nstill contain significant gaps. Language models (LMs) have been proposed as a\nsource for filling these gaps. However, prior works have focused on prominent\nentities with rich coverage by LMs, neglecting the crucial case of long-tail\nentities. In this paper, we present a novel method for LM-based-KB completion\nthat is specifically geared for facts about long-tail entities. The method\nleverages two different LMs in two stages: for candidate retrieval and for\ncandidate verification and disambiguation. To evaluate our method and various\nbaselines, we introduce a novel dataset, called MALT, rooted in Wikidata. Our\nmethod outperforms all baselines in F1, with major gains especially in recall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lihu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1\">Simon Razniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preference Ranking Optimization for Human Alignment. (arXiv:2306.17492v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17492","description":"<p>Large language models (LLMs) often contain misleading content, emphasizing\nthe need to align them with human values to ensure secur AI systems.\nReinforcement learning from human feedback (RLHF) has been employed to achieve\nthis alignment by combining a reward model, typically based on Bradley-Terry\npaired comparison, with an RL algorithm such as Proximal Policy Optimization\n(PPO) to optimize LLM responses. However, RLHF exhibits complexity,\ninstability, and sensitivity to hyperparameters. In this paper, we propose\nPreference Ranking Optimization (PRO) as an alternative to PPO for directly\naligning LLMs with the Bradley-Terry comparison. PRO extends the pairwise\nBradley-Terry comparison to accommodate preference rankings of any length. By\niteratively contrasting the likelihood of generating responses, PRO instructs\nthe LLM to prioritize the best response while progressively ranking the\nremaining responses. In this manner, PRO effectively transforms human alignment\ninto aligning the probability ranking of $n$ responses generated by LLM with\nthe preference ranking of humans towards these responses. Experiments have\nshown that PRO outperforms existing alignment algorithms, achieving comparable\nresults to ChatGPT and human responses through automatic-based, reward-based,\nGPT-4, and human evaluations. Furthermore, we demonstrate that longer, more\ndiverse, and higher-quality preference ranking sequences can consistently\nenhance the performance of human alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_F/0/1/0/all/0/1\">Feifan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bowen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Houfeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-FinRE: In-context Learning for Financial Relation Extraction using Large Language Models. (arXiv:2306.17519v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17519","description":"<p>Relation extraction (RE) is a crucial task in natural language processing\n(NLP) that aims to identify and classify relationships between entities\nmentioned in text. In the financial domain, relation extraction plays a vital\nrole in extracting valuable information from financial documents, such as news\narticles, earnings reports, and company filings. This paper describes our\nsolution to relation extraction on one such dataset REFinD. The dataset was\nreleased along with shared task as a part of the Fourth Workshop on Knowledge\nDiscovery from Unstructured Data in Financial Services, co-located with SIGIR\n2023. In this paper, we employed OpenAI models under the framework of\nin-context learning (ICL). We utilized two retrieval strategies to find top K\nrelevant in-context learning demonstrations / examples from training data for a\ngiven test example. The first retrieval mechanism, we employed, is a\nlearning-free dense retriever and the other system is a learning-based\nretriever. We were able to achieve 4th rank on the leaderboard. Our best\nF1-score is 0.718.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajpoot_P/0/1/0/all/0/1\">Pawan Kumar Rajpoot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_A/0/1/0/all/0/1\">Ankur Parikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards the extraction of robust sign embeddings for low resource sign language recognition. (arXiv:2306.17558v1 [cs.CV])","link":"http://arxiv.org/abs/2306.17558","description":"<p>Isolated Sign Language Recognition (SLR) has mostly been applied on\nrelatively large datasets containing signs executed slowly and clearly by a\nlimited group of signers. In real-world scenarios, however, we are met with\nchallenging visual conditions, coarticulated signing, small datasets, and the\nneed for signer independent models. To tackle this difficult problem, we\nrequire a robust feature extractor to process the sign language videos. One\ncould expect human pose estimators to be ideal candidates. However, due to a\ndomain mismatch with their training sets and challenging poses in sign\nlanguage, they lack robustness on sign language data and image based models\noften still outperform keypoint based models. Furthermore, whereas the common\npractice of transfer learning with image based models yields even higher\naccuracy, keypoint based models are typically trained from scratch on every SLR\ndataset. These factors limit their usefulness for SLR. From the existing\nliterature, it is also not clear which, if any, pose estimator performs best\nfor SLR. We compare the three most popular pose estimators for SLR: OpenPose,\nMMPose and MediaPipe. We show that through keypoint normalization, missing\nkeypoint imputation, and learning a pose embedding, we can obtain significantly\nbetter results and enable transfer learning. We show that keypoint-based\nembeddings contain cross-lingual features: they can transfer between sign\nlanguages and achieve competitive performance even when fine-tuning only the\nclassifier layer of an SLR model on a target sign language. We furthermore\nachieve better performance using fine-tuned transferred embeddings than models\ntrained only on the target sign language. The application of these embeddings\ncould prove particularly useful for low resource sign languages in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coster_M/0/1/0/all/0/1\">Mathieu De Coster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rushe_E/0/1/0/all/0/1\">Ellen Rushe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holmes_R/0/1/0/all/0/1\">Ruth Holmes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ventresque_A/0/1/0/all/0/1\">Anthony Ventresque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dambre_J/0/1/0/all/0/1\">Joni Dambre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting. (arXiv:2306.17563v1 [cs.IR])","link":"http://arxiv.org/abs/2306.17563","description":"<p>Ranking documents using Large Language Models (LLMs) by directly feeding the\nquery and candidate documents into the prompt is an interesting and practical\nproblem. However, there has been limited success so far, as researchers have\nfound it difficult to outperform fine-tuned baseline rankers on benchmark\ndatasets. We analyze pointwise and listwise ranking prompts used by existing\nmethods and argue that off-the-shelf LLMs do not fully understand these ranking\nformulations, possibly due to the nature of how LLMs are trained. In this\npaper, we propose to significantly reduce the burden on LLMs by using a new\ntechnique called Pairwise Ranking Prompting (PRP). Our results are the first in\nthe literature to achieve state-of-the-art ranking performance on standard\nbenchmarks using moderate-sized open-sourced LLMs. On TREC-DL2020, PRP based on\nthe Flan-UL2 model with 20B parameters outperforms the previous best approach\nin the literature, which is based on the blackbox commercial GPT-4 that has 50x\n(estimated) model size, by over 5% at NDCG@1. On TREC-DL2019, PRP is only\ninferior to the GPT-4 solution on the NDCG@5 and NDCG@10 metrics, while\noutperforming other existing solutions, such as InstructGPT which has 175B\nparameters, by over 10% for nearly all ranking metrics. Furthermore, we propose\nseveral variants of PRP to improve efficiency and show that it is possible to\nachieve competitive results even with linear complexity. We also discuss other\nbenefits of PRP, such as supporting both generation and scoring LLM APIs, as\nwell as being insensitive to input ordering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagerman_R/0/1/0/all/0/1\">Rolf Jagerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1\">Kai Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_H/0/1/0/all/0/1\">Honglei Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junru Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiaming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jialu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuanhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1\">Michael Bendersky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Cost-aware Study of Depression Language on Social Media using Topic and Affect Contextualization. (arXiv:2306.17564v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17564","description":"<p>Depression is a growing issue in society's mental health that affects all\nareas of life and can even lead to suicide. Fortunately, prevention programs\ncan be effective in its treatment. In this context, this work proposes an\nautomatic system for detecting depression on social media based on machine\nlearning and natural language processing methods. This paper presents the\nfollowing contributions: (i) an ensemble learning system that combines several\ntypes of text representations for depression detection, including recent\nadvances in the field; (ii) a contextualization schema through topic and\naffective information; (iii) an analysis of models' energy consumption,\nestablishing a trade-off between classification performance and overall\ncomputational costs. To assess the proposed models' effectiveness, a thorough\nevaluation is performed in two datasets that model depressive text. Experiments\nindicate that the proposed contextualization strategies can improve the\nclassification and that approaches that use Transformers can improve the\noverall F-score by 2% while augmenting the energy cost a hundred times.\nFinally, this work paves the way for future energy-wise systems by considering\nboth the performance classification and the energy consumption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laguna_A/0/1/0/all/0/1\">Andrea Laguna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araque_O/0/1/0/all/0/1\">Oscar Araque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting Holistic Review in University Admission using Natural Language Processing for Essays and Recommendation Letters. (arXiv:2306.17575v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17575","description":"<p>University admission at many highly selective institutions uses a holistic\nreview process, where all aspects of the application, including protected\nattributes (e.g., race, gender), grades, essays, and recommendation letters are\nconsidered, to compose an excellent and diverse class. In this study, we\nempirically evaluate how influential protected attributes are for predicting\nadmission decisions using a machine learning (ML) model, and in how far textual\ninformation (e.g., personal essay, teacher recommendation) may substitute for\nthe loss of protected attributes in the model. Using data from 14,915\napplicants to an undergraduate admission office at a selective U.S. institution\nin the 2022-2023 cycle, we find that the exclusion of protected attributes from\nthe ML model leads to substantially reduced admission-prediction performance.\nThe inclusion of textual information via both a TF-IDF representation and a\nLatent Dirichlet allocation (LDA) model partially restores model performance,\nbut does not appear to provide a full substitute for admitting a similarly\ndiverse class. In particular, while the text helps with gender diversity, the\nproportion of URM applicants is severely impacted by the exclusion of protected\nattributes, and the inclusion of new attributes generated from the textual\ninformation does not recover this performance loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinsook Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thymes_B/0/1/0/all/0/1\">Bradon Thymes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joyce Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joachims_T/0/1/0/all/0/1\">Thorsten Joachims</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kizilcec_R/0/1/0/all/0/1\">Rene F. Kizilcec</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT for Robotics: Design Principles and Model Abilities. (arXiv:2306.17582v1 [cs.AI])","link":"http://arxiv.org/abs/2306.17582","description":"<p>This paper presents an experimental study regarding the use of OpenAI's\nChatGPT for robotics applications. We outline a strategy that combines design\nprinciples for prompt engineering and the creation of a high-level function\nlibrary which allows ChatGPT to adapt to different robotics tasks, simulators,\nand form factors. We focus our evaluations on the effectiveness of different\nprompt engineering techniques and dialog strategies towards the execution of\nvarious types of robotics tasks. We explore ChatGPT's ability to use free-form\ndialog, parse XML tags, and to synthesize code, in addition to the use of\ntask-specific prompting functions and closed-loop reasoning through dialogues.\nOur study encompasses a range of tasks within the robotics domain, from basic\nlogical, geometrical, and mathematical reasoning all the way to complex domains\nsuch as aerial navigation, manipulation, and embodied agents. We show that\nChatGPT can be effective at solving several of such tasks, while allowing users\nto interact with it primarily via natural language instructions. In addition to\nthese studies, we introduce an open-sourced research tool called PromptCraft,\nwhich contains a platform where researchers can collaboratively upload and vote\non examples of good prompting schemes for robotics applications, as well as a\nsample robotics simulator with ChatGPT integration, making it easier for users\nto get started with using ChatGPT for robotics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vemprala_S/0/1/0/all/0/1\">Sai Vemprala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonatti_R/0/1/0/all/0/1\">Rogerio Bonatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucker_A/0/1/0/all/0/1\">Arthur Bucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1\">Ashish Kapoor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Representation Learning for NL2SQL Generation Based on Coupling and Decoupling. (arXiv:2306.17646v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17646","description":"<p>The NL2SQL task involves parsing natural language statements into SQL\nqueries. While most state-of-the-art methods treat NL2SQL as a slot-filling\ntask and use feature representation learning techniques, they overlook explicit\ncorrelation features between the SELECT and WHERE clauses and implicit\ncorrelation features between sub-tasks within a single clause. To address this\nissue, we propose the Clause Feature Correlation Decoupling and Coupling\n(CFCDC) model, which uses a feature representation decoupling method to\nseparate the SELECT and WHERE clauses at the parameter level. Next, we\nintroduce a multi-task learning architecture to decouple implicit correlation\nfeature representation between different SQL tasks in a specific clause.\nMoreover, we present an improved feature representation coupling module to\nintegrate the decoupled tasks in the SELECT and WHERE clauses and predict the\nfinal SQL query. Our proposed CFCDC model demonstrates excellent performance on\nthe WikiSQL dataset, with significant improvements in logic precision and\nexecution accuracy. The source code for the model will be publicly available on\nGitHub\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1\">Chenduo Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chuanbao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Deyu Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biomedical Language Models are Robust to Sub-optimal Tokenization. (arXiv:2306.17649v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17649","description":"<p>As opposed to general English, many concepts in biomedical terminology have\nbeen designed in recent history by biomedical professionals with the goal of\nbeing precise and concise. This is often achieved by concatenating meaningful\nbiomedical morphemes to create new semantic units. Nevertheless, most modern\nbiomedical language models (LMs) are pre-trained using standard domain-specific\ntokenizers derived from large scale biomedical corpus statistics without\nexplicitly leveraging the agglutinating nature of biomedical language. In this\nwork, we first find that standard open-domain and biomedical tokenizers are\nlargely unable to segment biomedical terms into meaningful components.\nTherefore, we hypothesize that using a tokenizer which segments biomedical\nterminology more accurately would enable biomedical LMs to improve their\nperformance on downstream biomedical NLP tasks, especially ones which involve\nbiomedical terms directly such as named entity recognition (NER) and entity\nlinking. Surprisingly, we find that pre-training a biomedical LM using a more\naccurate biomedical tokenizer does not improve the entity representation\nquality of a language model as measured by several intrinsic and extrinsic\nmeasures such as masked language modeling prediction (MLM) accuracy as well as\nNER and entity linking performance. These quantitative findings, along with a\ncase study which explores entity representation quality more directly, suggest\nthat the biomedical pre-training process is quite robust to instances of\nsub-optimal tokenization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_B/0/1/0/all/0/1\">Bernal Jim&#xe9;nez Guti&#xe9;rrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"X-RiSAWOZ: High-Quality End-to-End Multilingual Dialogue Datasets and Few-shot Agents. (arXiv:2306.17674v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17674","description":"<p>Task-oriented dialogue research has mainly focused on a few popular languages\nlike English and Chinese, due to the high dataset creation cost for a new\nlanguage. To reduce the cost, we apply manual editing to automatically\ntranslated data. We create a new multilingual benchmark, X-RiSAWOZ, by\ntranslating the Chinese RiSAWOZ to 4 languages: English, French, Hindi, Korean;\nand a code-mixed English-Hindi language. X-RiSAWOZ has more than 18,000\nhuman-verified dialogue utterances for each language, and unlike most\nmultilingual prior work, is an end-to-end dataset for building\nfully-functioning agents.\n</p>\n<p>The many difficulties we encountered in creating X-RiSAWOZ led us to develop\na toolset to accelerate the post-editing of a new language dataset after\ntranslation. This toolset improves machine translation with a hybrid entity\nalignment technique that combines neural with dictionary-based methods, along\nwith many automated and semi-automated validation checks.\n</p>\n<p>We establish strong baselines for X-RiSAWOZ by training dialogue agents in\nthe zero- and few-shot settings where limited gold data is available in the\ntarget language. Our results suggest that our translation and post-editing\nmethodology and toolset can be used to create new high-quality multilingual\ndialogue agents cost-effectively. Our dataset, code, and toolkit are released\nopen-source.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moradshahi_M/0/1/0/all/0/1\">Mehrad Moradshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tianhao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bali_K/0/1/0/all/0/1\">Kalika Bali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalendar_G/0/1/0/all/0/1\">Ga&#xeb;l de Chalendar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_A/0/1/0/all/0/1\">Anmol Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungkyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kodali_P/0/1/0/all/0/1\">Prashant Kodali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1\">Ponnurangam Kumaraguru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semmar_N/0/1/0/all/0/1\">Nasredine Semmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semnani_S/0/1/0/all/0/1\">Sina J. Semnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Jiwon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seshadri_V/0/1/0/all/0/1\">Vivek Seshadri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1\">Manish Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Michael Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadavalli_A/0/1/0/all/0/1\">Aditya Yadavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chaobin You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1\">Monica S. Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Task and Dataset on Detecting Attacks on Human Rights Defenders. (arXiv:2306.17695v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17695","description":"<p>The ability to conduct retrospective analyses of attacks on human rights\ndefenders over time and by location is important for humanitarian organizations\nto better understand historical or ongoing human rights violations and thus\nbetter manage the global impact of such events. We hypothesize that NLP can\nsupport such efforts by quickly processing large collections of news articles\nto detect and summarize the characteristics of attacks on human rights\ndefenders. To that end, we propose a new dataset for detecting Attacks on Human\nRights Defenders (HRDsAttack) consisting of crowdsourced annotations on 500\nonline news articles. The annotations include fine-grained information about\nthe type and location of the attacks, as well as information about the\nvictim(s). We demonstrate the usefulness of the dataset by using it to train\nand evaluate baseline models on several sub-tasks to predict the annotated\ncharacteristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ran_S/0/1/0/all/0/1\">Shihao Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Di Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tetreault_J/0/1/0/all/0/1\">Joel Tetreault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahill_A/0/1/0/all/0/1\">Aoife Cahill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaimes_A/0/1/0/all/0/1\">Alejandro Jaimes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Neural-on-Neural Approaches to Speaker Gender Protection. (arXiv:2306.17700v1 [eess.AS])","link":"http://arxiv.org/abs/2306.17700","description":"<p>Recent research has proposed approaches that modify speech to defend against\ngender inference attacks. The goal of these protection algorithms is to control\nthe availability of information about a speaker's gender, a privacy-sensitive\nattribute. Currently, the common practice for developing and testing gender\nprotection algorithms is \"neural-on-neural\", i.e., perturbations are generated\nand tested with a neural network. In this paper, we propose to go beyond this\npractice to strengthen the study of gender protection. First, we demonstrate\nthe importance of testing gender inference attacks that are based on speech\nfeatures historically developed by speech scientists, alongside the\nconventionally used neural classifiers. Next, we argue that researchers should\nuse speech features to gain insight into how protective modifications change\nthe speech signal. Finally, we point out that gender-protection algorithms\nshould be compared with novel \"vocal adversaries\", human-executed voice\nadaptations, in order to improve interpretability and enable before-the-mic\nprotection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bemmel_L/0/1/0/all/0/1\">Loes van Bemmel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zhuoran Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vaessen_N/0/1/0/all/0/1\">Nik Vaessen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Larson_M/0/1/0/all/0/1\">Martha Larson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved NL2SQL based on Multi-layer Expert Network. (arXiv:2306.17727v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17727","description":"<p>The Natural Language to SQL (NL2SQL) technique is used to convert natural\nlanguage queries into executable SQL statements. Typically, slot-filling is\nemployed as a classification method for multi-task cases to achieve this goal.\nHowever, slot-filling can result in inaccurate SQL statement generation due to\nnegative migration issues arising from different classification tasks. To\novercome this limitation, this study introduces a new approach called\nMulti-Layer Expert Generate SQL (MLEG-SQL), which utilizes a dedicated\nmulti-task hierarchical network. The lower layer of the network extracts\nsemantic features of natural language statements, while the upper layer builds\na specialized expert system for handling specific classification tasks. This\nhierarchical approach mitigates performance degradation resulting from\ndifferent task conflicts. The proposed method was evaluated on the WiKSQL\ndataset and was found to be effective in generating accurate SQL statements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1\">Chenduo Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Token-Event-Role Structure-based Multi-Channel Document-Level Event Extraction. (arXiv:2306.17733v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17733","description":"<p>Document-level event extraction is a long-standing challenging information\nretrieval problem involving a sequence of sub-tasks: entity extraction, event\ntype judgment, and event type-specific multi-event extraction. However,\naddressing the problem as multiple learning tasks leads to increased model\ncomplexity. Also, existing methods insufficiently utilize the correlation of\nentities crossing different events, resulting in limited event extraction\nperformance. This paper introduces a novel framework for document-level event\nextraction, incorporating a new data structure called token-event-role and a\nmulti-channel argument role prediction module. The proposed data structure\nenables our model to uncover the primary role of tokens in multiple events,\nfacilitating a more comprehensive understanding of event relationships. By\nleveraging the multi-channel prediction module, we transform entity and\nmulti-event extraction into a single task of predicting token-event pairs,\nthereby reducing the overall parameter size and enhancing model efficiency. The\nresults demonstrate that our approach outperforms the state-of-the-art method\nby 9.5 percentage points in terms of the F1 score, highlighting its superior\nperformance in event extraction. Furthermore, an ablation study confirms the\nsignificant value of the proposed data structure in improving event extraction\ntasks, further validating its importance in enhancing the overall performance\nof the framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Q/0/1/0/all/0/1\">Qizhi Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1\">Changxuan Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1\">Keli Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dexi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiping Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Should you marginalize over possible tokenizations?. (arXiv:2306.17757v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17757","description":"<p>Autoregressive language models (LMs) map token sequences to probabilities.\nThe usual practice for computing the probability of any character string (e.g.\nEnglish sentences) is to first transform it into a sequence of tokens that is\nscored by the model. However, there are exponentially many token sequences that\nrepresent any given string. To truly compute the probability of a string one\nshould marginalize over all tokenizations, which is typically intractable.\nHere, we analyze whether the practice of ignoring the marginalization is\njustified. To this end, we devise an importance-sampling-based algorithm that\nallows us to compute estimates of the marginal probabilities and compare them\nto the default procedure in a range of state-of-the-art models and datasets.\nOur results show that the gap in log-likelihood is no larger than 0.5% in most\ncases, but that it becomes more pronounced for data with long complex words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1\">Nadezhda Chirkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruszewski_G/0/1/0/all/0/1\">Germ&#xe1;n Kruszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozen_J/0/1/0/all/0/1\">Jos Rozen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dymetman_M/0/1/0/all/0/1\">Marc Dymetman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Improving the Performance of Pre-Trained Speech Models for Low-Resource Languages Through Lateral Inhibition. (arXiv:2306.17792v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17792","description":"<p>With the rise of bidirectional encoder representations from Transformer\nmodels in natural language processing, the speech community has adopted some of\ntheir development methodologies. Therefore, the Wav2Vec models were introduced\nto reduce the data required to obtain state-of-the-art results. This work\nleverages this knowledge and improves the performance of the pre-trained speech\nmodels by simply replacing the fine-tuning dense layer with a lateral\ninhibition layer inspired by the biological process. Our experiments on\nRomanian, a low-resource language, show an average improvement of 12.5% word\nerror rate (WER) using the lateral inhibition layer. In addition, we obtain\nstate-of-the-art results on both the Romanian Speech Corpus and the Robin\nTechnical Acquisition Corpus with 1.78% WER and 29.64% WER, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avram_A/0/1/0/all/0/1\">Andrei-Marius Avram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smadu_R/0/1/0/all/0/1\">R&#x103;zvan-Alexandru Sm&#x103;du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pais_V/0/1/0/all/0/1\">Vasile P&#x103;i&#x15f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cercel_D/0/1/0/all/0/1\">Dumitru-Clementin Cercel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ion_R/0/1/0/all/0/1\">Radu Ion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tufis_D/0/1/0/all/0/1\">Dan Tufi&#x15f;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stay on topic with Classifier-Free Guidance. (arXiv:2306.17806v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17806","description":"<p>Classifier-Free Guidance (CFG) has recently emerged in text-to-image\ngeneration as a lightweight technique to encourage prompt-adherence in\ngenerations. In this work, we demonstrate that CFG can be used broadly as an\ninference-time technique in pure language modeling. We show that CFG (1)\nimproves the performance of Pythia, GPT-2 and LLaMA-family models across an\narray of tasks: Q\\&amp;A, reasoning, code generation, and machine translation,\nachieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements\nequivalent to a model with twice the parameter-count; (3) can stack alongside\nother inference-time methods like Chain-of-Thought and Self-Consistency,\nyielding further improvements in difficult tasks; (4) can be used to increase\nthe faithfulness and coherence of assistants in challenging form-driven and\ncontent-driven prompts: in a human evaluation we show a 75\\% preference for\nGPT4All using CFG over baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_G/0/1/0/all/0/1\">Guillaume Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Honglu Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spangher_A/0/1/0/all/0/1\">Alexander Spangher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levi_E/0/1/0/all/0/1\">Elad Levi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammanamanchi_P/0/1/0/all/0/1\">Pawan Sasanka Ammanamanchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Massive Scale Semantic Similarity Dataset of Historical English. (arXiv:2306.17810v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17810","description":"<p>A diversity of tasks use language models trained on semantic similarity data.\nWhile there are a variety of datasets that capture semantic similarity, they\nare either constructed from modern web data or are relatively small datasets\ncreated in the past decade by human annotators. This study utilizes a novel\nsource, newly digitized articles from off-copyright, local U.S. newspapers, to\nassemble a massive-scale semantic similarity dataset spanning 70 years from\n1920 to 1989 and containing nearly 400M positive semantic similarity pairs.\nHistorically, around half of articles in U.S. local newspapers came from\nnewswires like the Associated Press. While local papers reproduced articles\nfrom the newswire, they wrote their own headlines, which form abstractive\nsummaries of the associated articles. We associate articles and their headlines\nby exploiting document layouts and language understanding. We then use deep\nneural methods to detect which articles are from the same underlying source, in\nthe presence of substantial noise and abridgement. The headlines of reproduced\narticles form positive semantic similarity pairs. The resulting publicly\navailable HEADLINES dataset is significantly larger than most existing semantic\nsimilarity datasets and covers a much longer span of time. It will facilitate\nthe application of contrastively trained semantic similarity models to a\nvariety of tasks, including the study of semantic change across space and time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silcock_E/0/1/0/all/0/1\">Emily Silcock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dell_M/0/1/0/all/0/1\">Melissa Dell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Reasoning: Semantics-Symbol Deconstruction For Large Language Models. (arXiv:2306.17820v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17820","description":"<p>Symbolization methods in large language models (LLMs) have been shown\neffective to improve LLMs' reasoning ability. However, most of these approaches\nhinge on mapping natural languages to formal languages (e.g., Python, SQL) that\nare more syntactically complete and free of ambiguity. Although effective, they\ndepart from the natural language itself and deviate from the habits of human\nthinking, and instead cater more to the execution mindset of computers. In\ncontrast, we hope to simplify natural language by starting from the concept of\nsymbols in linguistics itself, so that LLMs can learn the common formulation\nand general solution of reasoning problems wrapped in different natural\nsemantics. From this consideration, we propose \\textbf{Meta-Reasoning}, which\nallows LLMs to automatically accomplish semantic-symbol deconstruction, i.e.,\nsemantic resolution, to maximally reduce different questions of certain\nreasoning tasks to similar natural language representation, thus gaining the\nability to learn by analogy and facilitating data-efficient in-context\nlearning. Our experiments show that the Meta-Reasoning paradigm saliently\nenhances LLMs' reasoning performance with fewer demonstrations. They can learn\nnot only reasoning chains but also general solutions to certain types of tasks.\nIn particular, for symbolic reasoning tasks, such as 7-step Tracking Shuffled\nObjects, GPT-3 (text-davinci-002) achieves over 99% accuracy with only one\nMeta-Reasoning demonstration, outperforming all current LLMs with the standard\nchain-of-thought prompting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Statler: State-Maintaining Language Models for Embodied Reasoning. (arXiv:2306.17840v1 [cs.RO])","link":"http://arxiv.org/abs/2306.17840","description":"<p>Large language models (LLMs) provide a promising tool that enable robots to\nperform complex robot reasoning tasks. However, the limited context window of\ncontemporary LLMs makes reasoning over long time horizons difficult. Embodied\ntasks such as those that one might expect a household robot to perform\ntypically require that the planner consider information acquired a long time\nago (e.g., properties of the many objects that the robot previously encountered\nin the environment). Attempts to capture the world state using an LLM's\nimplicit internal representation is complicated by the paucity of task- and\nenvironment-relevant information available in a robot's action history, while\nmethods that rely on the ability to convey information via the prompt to the\nLLM are subject to its limited context window. In this paper, we propose\nStatler, a framework that endows LLMs with an explicit representation of the\nworld state as a form of ``memory'' that is maintained over time. Integral to\nStatler is its use of two instances of general LLMs -- a world-model reader and\na world-model writer -- that interface with and maintain the world state. By\nproviding access to this world state ``memory'', Statler improves the ability\nof existing LLMs to reason over longer time horizons without the constraint of\ncontext length. We evaluate the effectiveness of our approach on three\nsimulated table-top manipulation domains and a real robot domain, and show that\nit improves the state-of-the-art in LLM-based robot reasoning. Project website:\nhttps://statler-lm.github.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoneda_T/0/1/0/all/0/1\">Takuma Yoneda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jiading Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huanyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tianchong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shengjie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picker_B/0/1/0/all/0/1\">Ben Picker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yunis_D/0/1/0/all/0/1\">David Yunis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1\">Hongyuan Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walter_M/0/1/0/all/0/1\">Matthew R. Walter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs. (arXiv:2306.17842v1 [cs.CV])","link":"http://arxiv.org/abs/2306.17842","description":"<p>In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling\nfrozen LLMs to perform both understanding and generation tasks involving\nnon-linguistic modalities such as images or videos. SPAE converts between raw\npixels and interpretable lexical tokens (or words) extracted from the LLM's\nvocabulary. The resulting tokens capture both the semantic meaning and the\nfine-grained details needed for visual reconstruction, effectively translating\nthe visual content into a language comprehensible to the LLM, and empowering it\nto perform a wide array of multimodal tasks. Our approach is validated through\nin-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set\nof image understanding and generation tasks. Our method marks the first\nsuccessful attempt to enable a frozen LLM to generate image content while\nsurpassing state-of-the-art performance in image understanding tasks, under the\nsame setting, by over 25%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lijun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiruo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vivek Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macherey_W/0/1/0/all/0/1\">Wolfgang Macherey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1\">David A. Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Essa_I/0/1/0/all/0/1\">Irfan Essa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1\">Kevin Murphy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1\">Alexander G. Hauptmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lu Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting. (arXiv:2110.05367v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.05367","description":"<p>Existing studies addressing gender bias of pre-trained language models,\nusually build a small gender-neutral data set and conduct a second phase\npre-training on the model with such data. However, given the limited size and\nconcentrated focus of the gender-neutral data, catastrophic forgetting would\noccur during second-phase pre-training. Forgetting information in the original\ntraining data may damage the model's downstream performance by a large margin.\nIn this work, we empirically show that catastrophic forgetting occurs in such\nmethods by evaluating them with general NLP tasks in GLUE. Then, we propose a\nnew method, GEnder Equality Prompt (GEEP), to improve gender fairness of\npre-trained models with less forgetting. GEEP freezes the pre-trained model and\nlearns gender-related prompts with gender-neutral data. Empirical results show\nthat GEEP not only achieves SOTA performances on gender fairness tasks, but\nalso forgets less and performs better on GLUE by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fatemi_Z/0/1/0/all/0/1\">Zahra Fatemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1\">Chen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversational Question Answering on Heterogeneous Sources. (arXiv:2204.11677v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2204.11677","description":"<p>Conversational question answering (ConvQA) tackles sequential information\nneeds where contexts in follow-up questions are left implicit. Current ConvQA\nsystems operate over homogeneous sources of information: either a knowledge\nbase (KB), or a text corpus, or a collection of tables. This paper addresses\nthe novel issue of jointly tapping into all of these together, this way\nboosting answer coverage and confidence. We present CONVINSE, an end-to-end\npipeline for ConvQA over heterogeneous sources, operating in three stages: i)\nlearning an explicit structured representation of an incoming question and its\nconversational context, ii) harnessing this frame-like representation to\nuniformly capture relevant evidences from KB, text, and tables, and iii)\nrunning a fusion-in-decoder model to generate the answer. We construct and\nrelease the first benchmark, ConvMix, for ConvQA over heterogeneous sources,\ncomprising 3000 real-user conversations with 16000 questions, along with entity\nannotations, completed question utterances, and question paraphrases.\nExperiments demonstrate the viability and advantages of our method, compared to\nstate-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Christmann_P/0/1/0/all/0/1\">Philipp Christmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers. (arXiv:2301.13741v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2301.13741","description":"<p>Real-world data contains a vast amount of multimodal information, among which\nvision and language are the two most representative modalities. Moreover,\nincreasingly heavier models, \\textit{e}.\\textit{g}., Transformers, have\nattracted the attention of researchers to model compression. However, how to\ncompress multimodal models, especially vison-language Transformers, is still\nunder-explored. This paper proposes the \\textbf{U}nified and\n\\textbf{P}r\\textbf{o}gressive \\textbf{P}runing (\\textbf{\\emph{UPop}}) as a\nuniversal vison-language Transformer compression framework, which incorporates\n1) unifiedly searching multimodal subnets in a continuous optimization space\nfrom the original model, which enables automatic assignment of pruning ratios\namong compressible modalities and structures; 2) progressively searching and\nretraining the subnet, which maintains convergence between the search and\nretrain to attain higher compression ratios. Experiments on various tasks,\ndatasets, and model architectures demonstrate the effectiveness and versatility\nof the proposed UPop framework. The code is available at\nhttps://github.com/sdc17/UPop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Dachuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chaofan Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Ying Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhendong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions. (arXiv:2305.15083v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.15083","description":"<p>Large-scale Pretrained Language Models (LLMs), such as ChatGPT and GPT4, have\nshown strong abilities in multilingual translations, without being explicitly\ntrained on parallel corpora. It is interesting how the LLMs obtain their\nability to carry out translation instructions for different languages. In this\npaper, we present a detailed analysis by finetuning a multilingual pretrained\nlanguage model, XGLM-7B, to perform multilingual translation following given\ninstructions. Firstly, we show that multilingual LLMs have stronger translation\nabilities than previously demonstrated. For a certain language, the performance\ndepends on its similarity to English and the amount of data used in the\npretraining phase. Secondly, we find that LLMs' ability to carry out\ntranslation instructions relies on the understanding of translation\ninstructions and the alignment among different languages. With multilingual\nfinetuning, LLMs could learn to perform the translation task well even for\nthose language pairs unseen during the instruction tuning phase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiahuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shanbo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faithfulness Tests for Natural Language Explanations. (arXiv:2305.18029v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18029","description":"<p>Explanations of neural models aim to reveal a model's decision-making process\nfor its predictions. However, recent work shows that current methods giving\nexplanations such as saliency maps or counterfactuals can be misleading, as\nthey are prone to present reasons that are unfaithful to the model's inner\nworkings. This work explores the challenging question of evaluating the\nfaithfulness of natural language explanations (NLEs). To this end, we present\ntwo tests. First, we propose a counterfactual input editor for inserting\nreasons that lead to counterfactual predictions but are not reflected by the\nNLEs. Second, we reconstruct inputs from the reasons stated in the generated\nNLEs and check how often they lead to the same predictions. Our tests can\nevaluate emerging NLE models, proving a fundamental tool in the development of\nfaithful NLEs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atanasova_P/0/1/0/all/0/1\">Pepa Atanasova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lioma_C/0/1/0/all/0/1\">Christina Lioma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonsen_J/0/1/0/all/0/1\">Jakob Grue Simonsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Layout and Task Aware Instruction Prompt for Zero-shot Document Image Question Answering. (arXiv:2306.00526v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.00526","description":"<p>The pre-training-fine-tuning paradigm based on layout-aware multimodal\npre-trained models has achieved significant progress on document image question\nanswering. However, domain pre-training and task fine-tuning for additional\nvisual, layout, and task modules prevent them from directly utilizing\noff-the-shelf instruction-tuning language foundation models, which have\nrecently shown promising potential in zero-shot learning. Contrary to aligning\nlanguage models to the domain of document image question answering, we align\ndocument image question answering to off-the-shell instruction-tuning language\nfoundation models to utilize their zero-shot capability. Specifically, we\npropose layout and task aware instruction prompt called LATIN-Prompt, which\nconsists of layout-aware document content and task-aware descriptions. The\nformer recovers the layout information among text segments from OCR tools by\nappropriate spaces and line breaks. The latter ensures that the model generates\nanswers that meet the requirements, especially format requirements, through a\ndetailed description of task. Experimental results on three benchmarks show\nthat LATIN-Prompt can improve the zero-shot performance of instruction-tuning\nlanguage foundation models on document image question answering and help them\nachieve comparable levels to SOTAs based on the pre-training-fine-tuning\nparadigm. Quantitative analysis and qualitative analysis demonstrate the\neffectiveness of LATIN-Prompt. We provide the code in supplementary and will\nrelease the code to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenjin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1\">Yixin Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identity Construction in a Misogynist Incels Forum. (arXiv:2306.15745v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.15745","description":"<p>Online communities of involuntary celibates (incels) are a prominent source\nof misogynist hate speech. In this paper, we use quantitative text and network\nanalysis approaches to examine how identity groups are discussed on\n&lt;incels.is&gt;, the largest black-pilled incels forum. We find that this community\nproduces a wide range of novel identity terms and, while terms for women are\nmost common, mentions of other minoritized identities are increasing. An\nanalysis of the associations made with identity groups suggests an essentialist\nideology where physical appearance, as well as gender and racial hierarchies,\ndetermine human value. We discuss implications for research into automated\nmisogynist hate speech detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoder_M/0/1/0/all/0/1\">Michael Miller Yoder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perry_C/0/1/0/all/0/1\">Chloe Perry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1\">David West Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carley_K/0/1/0/all/0/1\">Kathleen M. Carley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruden_M/0/1/0/all/0/1\">Meredith Pruden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors. (arXiv:2306.17156v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2306.17156","description":"<p>Generative AI and large language models hold great promise in enhancing\ncomputing education by powering next-generation educational technologies for\nintroductory programming. Recent works have studied these models for different\nscenarios relevant to programming education; however, these works are limited\nfor several reasons, as they typically consider already outdated models or only\nspecific scenario(s). Consequently, there is a lack of a systematic study that\nbenchmarks state-of-the-art models for a comprehensive set of programming\neducation scenarios. In our work, we systematically evaluate two models,\nChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human\ntutors for a variety of scenarios. We evaluate using five introductory Python\nprogramming problems and real-world buggy programs from an online platform, and\nassess performance using expert-based annotations. Our results show that GPT-4\ndrastically outperforms ChatGPT (based on GPT-3.5) and comes close to human\ntutors' performance for several scenarios. These results also highlight\nsettings where GPT-4 still struggles, providing exciting future directions on\ndeveloping techniques to improve the performance of these models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phung_T/0/1/0/all/0/1\">Tung Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padurean_V/0/1/0/all/0/1\">Victor-Alexandru P&#x103;durean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambronero_J/0/1/0/all/0/1\">Jos&#xe9; Cambronero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulwani_S/0/1/0/all/0/1\">Sumit Gulwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohn_T/0/1/0/all/0/1\">Tobias Kohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_R/0/1/0/all/0/1\">Rupak Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1\">Adish Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_G/0/1/0/all/0/1\">Gustavo Soares</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-07-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/"}}]}]}