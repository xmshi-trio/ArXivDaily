{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-01-02T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Using Large Language Models to Generate Engaging Captions for Data Visualizations. (arXiv:2212.14047v1 [cs.CL])","link":"http://arxiv.org/abs/2212.14047","description":"<p>Creating compelling captions for data visualizations has been a longstanding\nchallenge. Visualization researchers are typically untrained in journalistic\nreporting and hence the captions that are placed below data visualizations tend\nto be not overly engaging and rather just stick to basic observations about the\ndata. In this work we explore the opportunities offered by the newly emerging\ncrop of large language models (LLM) which use sophisticated deep learning\ntechnology to produce human-like prose. We ask, can these powerful software\ndevices be purposed to produce engaging captions for generic data\nvisualizations like a scatterplot. It turns out that the key challenge lies in\ndesigning the most effective prompt for the LLM, a task called prompt\nengineering. We report on first experiments using the popular LLM GPT-3 and\ndeliver some promising results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liew_A/0/1/0/all/0/1\">Ashley Liew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_K/0/1/0/all/0/1\">Klaus Mueller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hungry Hungry Hippos: Towards Language Modeling with State Space Models. (arXiv:2212.14052v1 [cs.LG])","link":"http://arxiv.org/abs/2212.14052","description":"<p>State space models (SSMs) have demonstrated state-of-the-art sequence\nmodeling performance in some modalities, but underperform attention in language\nmodeling. Moreover, despite scaling nearly linearly in sequence length instead\nof quadratically, SSMs are still slower than Transformers due to poor hardware\nutilization. In this paper, we make progress on understanding the expressivity\ngap between SSMs and attention in language modeling, and on reducing the\nhardware barrier between SSMs and attention. First, we use synthetic language\nmodeling tasks to understand the gap between SSMs and attention. We find that\nexisting SSMs struggle with two capabilities: recalling earlier tokens in the\nsequence and comparing tokens across the sequence. To understand the impact on\nlanguage modeling, we propose a new SSM layer, H3, that is explicitly designed\nfor these abilities. H3 matches attention on the synthetic languages and comes\nwithin 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid\n125M-parameter H3-attention model that retains two attention layers\nsurprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to\nimprove the efficiency of training SSMs on modern hardware, we propose\nFlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on\nsequences up to 8K, and introduces a novel state passing algorithm that\nexploits the recurrent properties of SSMs to scale to longer sequences.\nFlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows\nhybrid language models to generate text 1.6$\\times$ faster than Transformers.\nUsing FlashConv, we scale hybrid H3-attention language models up to 1.3B\nparameters on the Pile and find promising initial results, achieving lower\nperplexity than Transformers and outperforming Transformers in zero- and\nfew-shot learning on a majority of tasks in the SuperGLUE benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dao_T/0/1/0/all/0/1\">Tri Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1\">Daniel Y. Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saab_K/0/1/0/all/0/1\">Khaled K. Saab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_A/0/1/0/all/0/1\">Armin W. Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudra_A/0/1/0/all/0/1\">Atri Rudra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Choosing the Number of Topics in LDA Models -- A Monte Carlo Comparison of Selection Criteria. (arXiv:2212.14074v1 [cs.CL])","link":"http://arxiv.org/abs/2212.14074","description":"<p>Selecting the number of topics in LDA models is considered to be a difficult\ntask, for which alternative approaches have been proposed. The performance of\nthe recently developed singular Bayesian information criterion (sBIC) is\nevaluated and compared to the performance of alternative model selection\ncriteria. The sBIC is a generalization of the standard BIC that can be\nimplemented to singular statistical models. The comparison is based on Monte\nCarlo simulations and carried out for several alternative settings, varying\nwith respect to the number of topics, the number of documents and the size of\ndocuments in the corpora. Performance is measured using different criteria\nwhich take into account the correct number of topics, but also whether the\nrelevant topics from the DGPs are identified. Practical recommendations for LDA\nmodel selection in applications are derived.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bystrov_V/0/1/0/all/0/1\">Victor Bystrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naboka_V/0/1/0/all/0/1\">Viktoriia Naboka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staszewska_Bystrova_A/0/1/0/all/0/1\">Anna Staszewska-Bystrova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winker_P/0/1/0/all/0/1\">Peter Winker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging World Knowledge in Implicit Hate Speech Detection. (arXiv:2212.14100v1 [cs.CL])","link":"http://arxiv.org/abs/2212.14100","description":"<p>While much attention has been paid to identifying explicit hate speech,\nimplicit hateful expressions that are disguised in coded or indirect language\nare pervasive and remain a major challenge for existing hate speech detection\nsystems. This paper presents the first attempt to apply Entity Linking (EL)\ntechniques to both explicit and implicit hate speech detection, where we show\nthat such real world knowledge about entity mentions in a text does help models\nbetter detect hate speech, and the benefit of adding it into the model is more\npronounced when explicit entity triggers (e.g., rally, KKK) are present. We\nalso discuss cases where real world knowledge does not add value to hate speech\ndetection, which provides more insights into understanding and modeling the\nsubtleties of hate speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jessica Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Customizing Knowledge Graph Embedding to Improve Clinical Study Recommendation. (arXiv:2212.14102v1 [cs.LG])","link":"http://arxiv.org/abs/2212.14102","description":"<p>Inferring knowledge from clinical trials using knowledge graph embedding is\nan emerging area. However, customizing graph embeddings for different use cases\nremains a significant challenge. We propose custom2vec, an algorithmic\nframework to customize graph embeddings by incorporating user preferences in\ntraining the embeddings. It captures user preferences by adding custom nodes\nand links derived from manually vetted results of a separate information\nretrieval method. We propose a joint learning objective to preserve the\noriginal network structure while incorporating the user's custom annotations.\nWe hypothesize that the custom training improves user-expected predictions, for\nexample, in link prediction tasks. We demonstrate the effectiveness of\ncustom2vec for clinical trials related to non-small cell lung cancer (NSCLC)\nwith two customization scenarios: recommending immuno-oncology trials\nevaluating PD-1 inhibitors and exploring similar trials that compare new\ntherapies with a standard of care. The results show that custom2vec training\nachieves better performance than the conventional training methods. Our\napproach is a novel way to customize knowledge graph embeddings and enable more\naccurate recommendations and predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalil_I/0/1/0/all/0/1\">Iya Khalil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devarakonda_M/0/1/0/all/0/1\">Murthy Devarakonda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards automating Codenames spymasters with deep reinforcement learning. (arXiv:2212.14104v1 [cs.CL])","link":"http://arxiv.org/abs/2212.14104","description":"<p>Although most reinforcement learning research has centered on competitive\ngames, little work has been done on applying it to co-operative multiplayer\ngames or text-based games. Codenames is a board game that involves both\nasymmetric co-operation and natural language processing, which makes it an\nexcellent candidate for advancing RL research. To my knowledge, this work is\nthe first to formulate Codenames as a Markov Decision Process and apply some\nwell-known reinforcement learning algorithms such as SAC, PPO, and A2C to the\nenvironment. Although none of the above algorithms converge for the Codenames\nenvironment, neither do they converge for a simplified environment called\nClickPixel, except when the board size is small.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siu_S/0/1/0/all/0/1\">Sherman Siu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving a sequence-to-sequence nlp model using a reinforcement learning policy algorithm. (arXiv:2212.14117v1 [cs.CL])","link":"http://arxiv.org/abs/2212.14117","description":"<p>Nowadays, the current neural network models of dialogue generation(chatbots)\nshow great promise for generating answers for chatty agents. But they are\nshort-sighted in that they predict utterances one at a time while disregarding\ntheir impact on future outcomes. Modelling a dialogue's future direction is\ncritical for generating coherent, interesting dialogues, a need that has led\ntraditional NLP dialogue models that rely on reinforcement learning. In this\narticle, we explain how to combine these objectives by using deep reinforcement\nlearning to predict future rewards in chatbot dialogue. The model simulates\nconversations between two virtual agents, with policy gradient methods used to\nreward sequences that exhibit three useful conversational characteristics: the\nflow of informality, coherence, and simplicity of response (related to\nforward-looking function). We assess our model based on its diversity, length,\nand complexity with regard to humans. In dialogue simulation, evaluations\ndemonstrated that the proposed model generates more interactive responses and\nencourages a more sustained successful conversation. This work commemorates a\npreliminary step toward developing a neural conversational model based on the\nlong-term success of dialogues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ismail_J/0/1/0/all/0/1\">Jabri Ismail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1\">Aboulbichr Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aziza_E/0/1/0/all/0/1\">El ouaazizi Aziza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Macro-block dropout for improved regularization in training end-to-end speech recognition models. (arXiv:2212.14149v1 [cs.LG])","link":"http://arxiv.org/abs/2212.14149","description":"<p>This paper proposes a new regularization algorithm referred to as macro-block\ndropout. The overfitting issue has been a difficult problem in training large\nneural network models. The dropout technique has proven to be simple yet very\neffective for regularization by preventing complex co-adaptations during\ntraining. In our work, we define a macro-block that contains a large number of\nunits from the input to a Recurrent Neural Network (RNN). Rather than applying\ndropout to each unit, we apply random dropout to each macro-block. This\nalgorithm has the effect of applying different drop out rates for each layer\neven if we keep a constant average dropout rate, which has better\nregularization effects. In our experiments using Recurrent Neural\nNetwork-Transducer (RNN-T), this algorithm shows relatively 4.30 % and 6.13 %\nWord Error Rates (WERs) improvement over the conventional dropout on\nLibriSpeech test-clean and test-other. With an Attention-based Encoder-Decoder\n(AED) model, this algorithm shows relatively 4.36 % and 5.85 % WERs improvement\nover the conventional dropout on the same test sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chanwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Indurti_S/0/1/0/all/0/1\">Sathish Indurti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinhwan Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_W/0/1/0/all/0/1\">Wonyong Sung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Maximizing Use-Case Specificity through Precision Model Tuning. (arXiv:2212.14206v1 [cs.CL])","link":"http://arxiv.org/abs/2212.14206","description":"<p>Language models have become increasingly popular in recent years for tasks\nlike information retrieval. As use-cases become oriented toward specific\ndomains, fine-tuning becomes default for standard performance. To fine-tune\nthese models for specific tasks and datasets, it is necessary to carefully tune\nthe model's hyperparameters and training techniques. In this paper, we present\nan in-depth analysis of the performance of four transformer-based language\nmodels on the task of biomedical information retrieval. The models we consider\nare DeepMind's RETRO (7B parameters), GPT-J (6B parameters), GPT-3 (175B\nparameters), and BLOOM (176B parameters). We compare their performance on the\nbasis of relevance, accuracy, and interpretability, using a large corpus of\n480000 research papers on protein structure/function prediction as our dataset.\nOur findings suggest that smaller models, with &lt;10B parameters and fine-tuned\non domain-specific datasets, tend to outperform larger language models on\nhighly specific questions in terms of accuracy, relevancy, and interpretability\nby a significant margin (+50% on average). However, larger models do provide\ngenerally better results on broader prompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Awasthi_P/0/1/0/all/0/1\">Pranjali Awasthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recio_Mitter_D/0/1/0/all/0/1\">David Recio-Mitter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugi_Y/0/1/0/all/0/1\">Yosuke Kyle Sugi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence Generation with Label Augmentation for Relation Extraction. (arXiv:2212.14266v1 [cs.CL])","link":"http://arxiv.org/abs/2212.14266","description":"<p>Sequence generation demonstrates promising performance in recent information\nextraction efforts, by incorporating large-scale pre-trained Seq2Seq models.\nThis paper investigates the merits of employing sequence generation in relation\nextraction, finding that with relation names or synonyms as generation targets,\ntheir textual semantics and the correlation (in terms of word sequence pattern)\namong them affect model performance. We then propose Relation Extraction with\nLabel Augmentation (RELA), a Seq2Seq model with automatic label augmentation\nfor RE. By saying label augmentation, we mean prod semantically synonyms for\neach relation name as the generation target. Besides, we present an in-depth\nanalysis of the Seq2Seq model's behavior when dealing with RE. Experimental\nresults show that RELA achieves competitive results compared with previous\nmethods on four RE datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dingyao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinglei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shikun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reviewing Labels: Label Graph Network with Top-k Prediction Set for Relation Extraction. (arXiv:2212.14270v1 [cs.CL])","link":"http://arxiv.org/abs/2212.14270","description":"<p>The typical way for relation extraction is fine-tuning large pre-trained\nlanguage models on task-specific datasets, then selecting the label with the\nhighest probability of the output distribution as the final prediction.\nHowever, the usage of the Top-k prediction set for a given sample is commonly\noverlooked. In this paper, we first reveal that the Top-k prediction set of a\ngiven sample contains useful information for predicting the correct label. To\neffectively utilizes the Top-k prediction set, we propose Label Graph Network\nwith Top-k Prediction Set, termed as KLG. Specifically, for a given sample, we\nbuild a label graph to review candidate labels in the Top-k prediction set and\nlearn the connections between them. We also design a dynamic $k$-selection\nmechanism to learn more powerful and discriminative relation representation.\nOur experiments show that KLG achieves the best performances on three relation\nextraction datasets. Moreover, we observe that KLG is more effective in dealing\nwith long-tailed classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinglei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shikun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Error syntax aware augmentation of feedback comment generation dataset. (arXiv:2212.14293v1 [cs.CL])","link":"http://arxiv.org/abs/2212.14293","description":"<p>This paper presents a solution to the GenChal 2022 shared task dedicated to\nfeedback comment generation for writing learning. In terms of this task given a\ntext with an error and a span of the error, a system generates an explanatory\nnote that helps the writer (language learner) to improve their writing skills.\nOur solution is based on fine-tuning the T5 model on the initial dataset\naugmented according to syntactical dependencies of the words located within\nindicated error span. The solution of our team \"nigula\" obtained second place\naccording to manual evaluation by the organizers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Babakov_N/0/1/0/all/0/1\">Nikolay Babakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lysyuk_M/0/1/0/all/0/1\">Maria Lysyuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shvets_A/0/1/0/all/0/1\">Alexander Shvets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazakova_L/0/1/0/all/0/1\">Lilya Kazakova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panchenko_A/0/1/0/all/0/1\">Alexander Panchenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT Takes the Bar Exam. (arXiv:2212.14402v1 [cs.CL])","link":"http://arxiv.org/abs/2212.14402","description":"<p>Nearly all jurisdictions in the United States require a professional license\nexam, commonly referred to as \"the Bar Exam,\" as a precondition for law\npractice. To even sit for the exam, most jurisdictions require that an\napplicant completes at least seven years of post-secondary education, including\nthree years at an accredited law school. In addition, most test-takers also\nundergo weeks to months of further, exam-specific preparation. Despite this\nsignificant investment of time and capital, approximately one in five\ntest-takers still score under the rate required to pass the exam on their first\ntry. In the face of a complex task that requires such depth of knowledge, what,\nthen, should we expect of the state of the art in \"AI?\" In this research, we\ndocument our experimental evaluation of the performance of OpenAI's\n`text-davinci-003` model, often-referred to as GPT-3.5, on the multistate\nmultiple choice (MBE) section of the exam. While we find no benefit in\nfine-tuning over GPT-3.5's zero-shot performance at the scale of our training\ndata, we do find that hyperparameter optimization and prompt engineering\npositively impacted GPT-3.5's zero-shot performance. For best prompt and\nparameters, GPT-3.5 achieves a headline correct rate of 50.3% on a complete\nNCBE MBE practice exam, significantly in excess of the 25% baseline guessing\nrate, and performs at a passing rate for both Evidence and Torts. GPT-3.5's\nranking of responses is also highly-correlated with correctness; its top two\nand top three choices are correct 71% and 88% of the time, respectively,\nindicating very strong non-entailment performance. While our ability to\ninterpret these results is limited by nascent scientific understanding of LLMs\nand the proprietary nature of GPT, we believe that these results strongly\nsuggest that an LLM will pass the MBE component of the Bar Exam in the near\nfuture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bommarito_M/0/1/0/all/0/1\">Michael Bommarito II</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_D/0/1/0/all/0/1\">Daniel Martin Katz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Multimodal Data Augmentation in Feature Space. (arXiv:2212.14453v1 [cs.LG])","link":"http://arxiv.org/abs/2212.14453","description":"<p>The ability to jointly learn from multiple modalities, such as text, audio,\nand visual data, is a defining feature of intelligent systems. While there have\nbeen promising advances in designing neural networks to harness multimodal\ndata, the enormous success of data augmentation currently remains limited to\nsingle-modality tasks like image classification. Indeed, it is particularly\ndifficult to augment each modality while preserving the overall semantic\nstructure of the data; for example, a caption may no longer be a good\ndescription of an image after standard augmentations have been applied, such as\ntranslation. Moreover, it is challenging to specify reasonable transformations\nthat are not tailored to a particular modality. In this paper, we introduce\nLeMDA, Learning Multimodal Data Augmentation, an easy-to-use method that\nautomatically learns to jointly augment multimodal data in feature space, with\nno constraints on the identities of the modalities or the relationship between\nmodalities. We show that LeMDA can (1) profoundly improve the performance of\nmultimodal deep learning architectures, (2) apply to combinations of modalities\nthat have not been previously considered, and (3) achieve state-of-the-art\nresults on a wide range of applications comprised of image, text, and tabular\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zichang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhiqiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xingjian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Anshumali Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1\">Andrew Gordon Wilson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEAformer: Multi-modal Entity Alignment Transformer for Meta Modality Hybrid. (arXiv:2212.14454v1 [cs.AI])","link":"http://arxiv.org/abs/2212.14454","description":"<p>As an important variant of entity alignment (EA), multi-modal entity\nalignment (MMEA) aims to discover identical entities across different knowledge\ngraphs (KGs) with multiple modalities like images. However, current MMEA\nalgorithms all adopt KG-level modality fusion strategies but ignore modality\ndifferences among individual entities, hurting the robustness to potential\nnoise involved in modalities (e.g., unidentifiable images and relations). In\nthis paper we present MEAformer, a multi-modal entity alignment transformer\napproach for meta modality hybrid, to dynamically predict the mutual\ncorrelation coefficients among modalities for instance-level feature fusion. A\nmodal-aware hard entity replay strategy is also proposed for addressing vague\nentity details. Extensive experimental results show that our model not only\nachieves SOTA performance on multiple training scenarios including supervised,\nunsupervised, iterative, and low resource, but also has limited parameters,\noptimistic speed, and good interpretability. Our code will be available soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Lingbing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1\">Yuxia Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Wenting Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Examining Political Rhetoric with Epistemic Stance Detection. (arXiv:2212.14486v1 [cs.CL])","link":"http://arxiv.org/abs/2212.14486","description":"<p>Participants in political discourse employ rhetorical strategies -- such as\nhedging, attributions, or denials -- to display varying degrees of belief\ncommitments to claims proposed by themselves or others. Traditionally,\npolitical scientists have studied these epistemic phenomena through\nlabor-intensive manual content analysis. We propose to help automate such work\nthrough epistemic stance prediction, drawn from research in computational\nsemantics, to distinguish at the clausal level what is asserted, denied, or\nonly ambivalently suggested by the author or other mentioned entities (belief\nholders). We first develop a simple RoBERTa-based model for multi-source stance\npredictions that outperforms more complex state-of-the-art modeling. Then we\ndemonstrate its novel application to political science by conducting a\nlarge-scale analysis of the Mass Market Manifestos corpus of U.S. political\nopinion books, where we characterize trends in cited belief holders --\nrespected allies and opposed bogeymen -- across U.S. political ideologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ankita Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blodgett_S/0/1/0/all/0/1\">Su Lin Blodgett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gross_J/0/1/0/all/0/1\">Justin H Gross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_B/0/1/0/all/0/1\">Brendan O&#x27;Connor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal deep learning system for depression and anxiety detection. (arXiv:2212.14490v1 [cs.SD])","link":"http://arxiv.org/abs/2212.14490","description":"<p>Traditional screening practices for anxiety and depression pose an impediment\nto monitoring and treating these conditions effectively. However, recent\nadvances in NLP and speech modelling allow textual, acoustic, and hand-crafted\nlanguage-based features to jointly form the basis of future mental health\nscreening and condition detection. Speech is a rich and readily available\nsource of insight into an individual's cognitive state and by leveraging\ndifferent aspects of speech, we can develop new digital biomarkers for\ndepression and anxiety. To this end, we propose a multi-modal system for the\nscreening of depression and anxiety from self-administered speech tasks. The\nproposed model integrates deep-learned features from audio and text, as well as\nhand-crafted features that are informed by clinically-validated domain\nknowledge. We find that augmenting hand-crafted features with deep-learned\nfeatures improves our overall classification F1 score comparing to a baseline\nof hand-crafted features alone from 0.58 to 0.63 for depression and from 0.54\nto 0.57 for anxiety. The findings of our work suggest that speech-based\nbiomarkers for depression and anxiety hold significant promise in the future of\ndigital health.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diep_B/0/1/0/all/0/1\">Brian Diep</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanojevic_M/0/1/0/all/0/1\">Marija Stanojevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novikova_J/0/1/0/all/0/1\">Jekaterina Novikova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ResGrad: Residual Denoising Diffusion Probabilistic Models for Text to Speech. (arXiv:2212.14518v1 [eess.AS])","link":"http://arxiv.org/abs/2212.14518","description":"<p>Denoising Diffusion Probabilistic Models (DDPMs) are emerging in\ntext-to-speech (TTS) synthesis because of their strong capability of generating\nhigh-fidelity samples. However, their iterative refinement process in\nhigh-dimensional data space results in slow inference speed, which restricts\ntheir application in real-time systems. Previous works have explored speeding\nup by minimizing the number of inference steps but at the cost of sample\nquality. In this work, to improve the inference speed for DDPM-based TTS model\nwhile achieving high sample quality, we propose ResGrad, a lightweight\ndiffusion model which learns to refine the output spectrogram of an existing\nTTS model (e.g., FastSpeech 2) by predicting the residual between the model\noutput and the corresponding ground-truth speech. ResGrad has several\nadvantages: 1) Compare with other acceleration methods for DDPM which need to\nsynthesize speech from scratch, ResGrad reduces the complexity of task by\nchanging the generation target from ground-truth mel-spectrogram to the\nresidual, resulting into a more lightweight model and thus a smaller real-time\nfactor. 2) ResGrad is employed in the inference process of the existing TTS\nmodel in a plug-and-play way, without re-training this model. We verify ResGrad\non the single-speaker dataset LJSpeech and two more challenging datasets with\nmultiple speakers (LibriTTS) and high sampling rate (VCTK). Experimental\nresults show that in comparison with other speed-up methods of DDPMs: 1)\nResGrad achieves better sample quality with the same inference speed measured\nby real-time factor; 2) with similar speech quality, ResGrad synthesizes speech\nfaster than baseline methods by more than 10 times. Audio samples are available\nat https://resgrad1.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zehua Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yihan Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Haohe Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_Y/0/1/0/all/0/1\">Yang Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1\">Ke Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mandic_D/0/1/0/all/0/1\">Danilo Mandic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training. (arXiv:2212.14546v1 [cs.CV])","link":"http://arxiv.org/abs/2212.14546","description":"<p>Video-language pre-training has advanced the performance of various\ndownstream video-language tasks. However, most previous methods directly\ninherit or adapt typical image-language pre-training paradigms to\nvideo-language pre-training, thus not fully exploiting the unique\ncharacteristic of video, i.e., temporal. In this paper, we propose a\nHierarchical Temporal-Aware video-language pre-training framework, HiTeA, with\ntwo novel pre-training tasks for modeling cross-modal alignment between moments\nand texts as well as the temporal relations of video-text pairs. Specifically,\nwe propose a cross-modal moment exploration task to explore moments in videos,\nwhich results in detailed video moment representation. Besides, the inherent\ntemporal relations are captured by aligning video-text pairs as a whole in\ndifferent time resolutions with multi-modal temporal relation exploration task.\nFurthermore, we introduce the shuffling test to evaluate the temporal reliance\nof datasets and video-language pre-training models. We achieve state-of-the-art\nresults on 15 well-established video-language understanding and generation\ntasks, especially on temporal-oriented datasets (e.g., SSv2-Template and\nSSv2-Label) with 8.6% and 11.1% improvement respectively. HiTeA also\ndemonstrates strong generalization ability when directly transferred to\ndownstream tasks in a zero-shot manner. Models and demo will be available on\nModelScope.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinghao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guohai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1\">Qi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How would Stance Detection Techniques Evolve after the Launch of ChatGPT?. (arXiv:2212.14548v1 [cs.CL])","link":"http://arxiv.org/abs/2212.14548","description":"<p>Stance detection refers to the task of extracting the standpoint (Favor,\nAgainst or Neither) towards a target in given texts. Such research gains\nincreasing attention with the proliferation of social media contents. The\nconventional framework of handling stance detection is converting it into text\nclassification tasks. Deep learning models have already replaced rule-based\nmodels and traditional machine learning models in solving such problems.\nCurrent deep neural networks are facing two main challenges which are\ninsufficient labeled data and information in social media posts and the\nunexplainable nature of deep learning models. A new pre-trained language model\nchatGPT was launched on Nov 30, 2022. For the stance detection tasks, our\nexperiments show that ChatGPT can achieve SOTA or similar performance for\ncommonly used datasets including SemEval-2016 and P-Stance. At the same time,\nChatGPT can provide explanation for its own prediction, which is beyond the\ncapability of any existing model. The explanations for the cases it cannot\nprovide classification results are especially useful. ChatGPT has the potential\nto be the best AI model for stance detection tasks in NLP, or at least change\nthe research paradigm of this field. ChatGPT also opens up the possibility of\nbuilding explanatory AI for stance detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">Daijun Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liwen Jing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAUVE Scores for Generative Models: Theory and Practice. (arXiv:2212.14578v1 [cs.LG])","link":"http://arxiv.org/abs/2212.14578","description":"<p>Generative AI has matured to a point where large-scale models can generate\ntext that seems indistinguishable from human-written text and remarkably\nphotorealistic images. Automatically measuring how close the distribution of\ngenerated data is to the target real data distribution is a key step in\ndiagnosing existing models and developing better models. We present MAUVE, a\nfamily of comparison measures between pairs of distributions such as those\nencountered in the generative modeling of text or images. These scores are\nstatistical summaries of divergence frontiers capturing two types of errors in\ngenerative modeling. We explore four approaches to statistically estimate these\nscores: vector quantization, non-parametric estimation, classifier-based\nestimation, and parametric Gaussian approximations. We provide statistical\nbounds for the vector quantization approach. Empirically, we find that the\nproposed scores paired with a range of $f$-divergences and statistical\nestimation methods can quantify the gaps between the distributions of\nhuman-written text and those of modern neural language models by correlating\nwith human judgments and identifying known properties of the generated texts.\nWe conclude the paper by demonstrating its applications to other AI domains and\ndiscussing practical recommendations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pillutla_K/0/1/0/all/0/1\">Krishna Pillutla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thickstun_J/0/1/0/all/0/1\">John Thickstun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Sewoong Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harchaoui_Z/0/1/0/all/0/1\">Zaid Harchaoui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distant Reading of the German Coalition Deal: Recognizing Policy Positions with BERT-based Text Classification. (arXiv:2212.14648v1 [cs.CL])","link":"http://arxiv.org/abs/2212.14648","description":"<p>Automated text analysis has become a widely used tool in political science.\nIn this research, we use a BERT model trained on German party manifestos to\nidentify the individual parties' contribution to the coalition agreement of\n2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zylla_M/0/1/0/all/0/1\">Michael Zylla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haider_T/0/1/0/all/0/1\">Thomas Haider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear programming word problems formulation using EnsembleCRF NER labeler and T5 text generator with data augmentations. (arXiv:2212.14657v1 [cs.CL])","link":"http://arxiv.org/abs/2212.14657","description":"<p>We propose an ensemble approach to predict the labels in linear programming\nword problems. The entity identification and the meaning representation are two\ntypes of tasks to be solved in the NL4Opt competition. We propose the\nensembleCRF method to identify the named entities for the first task. We found\nthat single models didn't improve for the given task in our analysis. A set of\nprediction models predict the entities. The generated results are combined to\nform a consensus result in the ensembleCRF method. We present an ensemble text\ngenerator to produce the representation sentences for the second task. We\nthought of dividing the problem into multiple small tasks due to the overflow\nin the output. A single model generates different representations based on the\nprompt. All the generated text is combined to form an ensemble and produce a\nmathematical meaning of a linear programming problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">JiangLong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+N_M/0/1/0/all/0/1\">Mamatha N</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vignesh_S/0/1/0/all/0/1\">Shiv Vignesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1\">Deepak Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uppal_A/0/1/0/all/0/1\">Akshay Uppal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Twitter's Agenda-Setting Role: A Study of Twitter Strategy for Political Diversion. (arXiv:2212.14672v1 [cs.CY])","link":"http://arxiv.org/abs/2212.14672","description":"<p>This study verified the effectiveness of Donald Trump's Twitter campaign in\nguiding agen-da-setting and deflecting political risk and examined Trump's\nTwitter communication strategy and explores the communication effects of his\ntweet content during Covid-19 pandemic. We collected all tweets posted by Trump\non the Twitter platform from January 1, 2020 to December 31, 2020.We used\nOrdinary Least Squares (OLS) regression analysis with a fixed effects model to\nanalyze the existence of the Twitter strategy. The correlation between the\nnumber of con-firmed daily Covid-19 diagnoses and the number of particular\nthematic tweets was investigated using time series analysis. Empirical analysis\nrevealed Twitter's strategy is used to divert public attention from negative\nCovid-19 reports during the epidemic, and it posts a powerful political\ncommunication effect on Twitter. However, findings suggest that Trump did not\nuse false claims to divert political risk and shape public opinion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiaoyu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yunjie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Manli Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Gold Standard and Benchmark for Comics Text Detection and Recognition. (arXiv:2212.14674v1 [cs.CL])","link":"http://arxiv.org/abs/2212.14674","description":"<p>This study focuses on improving the optical character recognition (OCR) data\nfor panels in the COMICS dataset, the largest dataset containing text and\nimages from comic books. To do this, we developed a pipeline for OCR processing\nand labeling of comic books and created the first text detection and\nrecognition datasets for western comics, called \"COMICS Text+: Detection\" and\n\"COMICS Text+: Recognition\". We evaluated the performance of state-of-the-art\ntext detection and recognition models on these datasets and found significant\nimprovement in word accuracy and normalized edit distance compared to the text\nin COMICS. We also created a new dataset called \"COMICS Text+\", which contains\nthe extracted text from the textboxes in the COMICS dataset. Using the improved\ntext data of COMICS Text+ in the comics processing model from resulted in\nstate-of-the-art performance on cloze-style tasks without changing the model\narchitecture. The COMICS Text+ dataset can be a valuable resource for\nresearchers working on tasks including text detection, recognition, and\nhigh-level processing of comics, such as narrative understanding, character\nrelations, and story generation. All the data and inference instructions can be\naccessed in https://github.com/gsoykan/comics_text_plus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soykan_G/0/1/0/all/0/1\">G&#xfc;rkan Soykan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuret_D/0/1/0/all/0/1\">Deniz Yuret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sezgin_T/0/1/0/all/0/1\">Tevfik Metin Sezgin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Countering Malicious Content Moderation Evasion in Online Social Networks: Simulation and Detection of Word Camouflage. (arXiv:2212.14727v1 [cs.CL])","link":"http://arxiv.org/abs/2212.14727","description":"<p>Content moderation is the process of screening and monitoring user-generated\ncontent online. It plays a crucial role in stopping content resulting from\nunacceptable behaviors such as hate speech, harassment, violence against\nspecific groups, terrorism, racism, xenophobia, homophobia, or misogyny, to\nmention some few, in Online Social Platforms. These platforms make use of a\nplethora of tools to detect and manage malicious information; however,\nmalicious actors also improve their skills, developing strategies to surpass\nthese barriers and continuing to spread misleading information. Twisting and\ncamouflaging keywords are among the most used techniques to evade platform\ncontent moderation systems. In response to this recent ongoing issue, this\npaper presents an innovative approach to address this linguistic trend in\nsocial networks through the simulation of different content evasion techniques\nand a multilingual Transformer model for content evasion detection. In this\nway, we share with the rest of the scientific community a multilingual public\ntool, named \"pyleetspeak\" to generate/simulate in a customizable way the\nphenomenon of content evasion through automatic word camouflage and a\nmultilingual Named-Entity Recognition (NER) Transformer-based model tuned for\nits recognition and detection. The multilingual NER model is evaluated in\ndifferent textual scenarios, detecting different types and mixtures of\ncamouflage techniques, achieving an overall weighted F1 score of 0.8795. This\narticle contributes significantly to countering malicious information by\ndeveloping multilingual tools to simulate and detect new methods of evasion of\ncontent on social networks, making the fight against information disorders more\neffective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huertas_Garcia_A/0/1/0/all/0/1\">&#xc1;lvaro Huertas-Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1\">Alejandro Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tato_J/0/1/0/all/0/1\">Javier Huertas Tato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_D/0/1/0/all/0/1\">David Camacho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Black-box language model explanation by context length probing. (arXiv:2212.14815v1 [cs.CL])","link":"http://arxiv.org/abs/2212.14815","description":"<p>The increasingly widespread adoption of large language models has highlighted\nthe need for improving their explainability. We present context length probing,\na novel explanation technique for causal language models, based on tracking the\npredictions of a model as a function of the length of available context, and\nallowing to assign differential importance scores to different contexts. The\ntechnique is model-agnostic and does not rely on access to model internals\nbeyond computing token-level probabilities. We apply context length probing to\nlarge pre-trained language models and offer some initial analyses and insights,\nincluding the potential for studying long-range dependencies. The source code\nand a demo of the method are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cifka_O/0/1/0/all/0/1\">Ond&#x159;ej C&#xed;fka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liutkus_A/0/1/0/all/0/1\">Antoine Liutkus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports. (arXiv:2212.14882v1 [cs.CL])","link":"http://arxiv.org/abs/2212.14882","description":"<p>The release of ChatGPT, a language model capable of generating text that\nappears human-like and authentic, has gained significant attention beyond the\nresearch community. We expect that the convincing performance of ChatGPT\nincentivizes users to apply it to a variety of downstream tasks, including\nprompting the model to simplify their own medical reports. To investigate this\nphenomenon, we conducted an exploratory case study. In a questionnaire, we\nasked 15 radiologists to assess the quality of radiology reports simplified by\nChatGPT. Most radiologists agreed that the simplified reports were factually\ncorrect, complete, and not potentially harmful to the patient. Nevertheless,\ninstances of incorrect statements, missed key medical findings, and potentially\nharmful passages were reported. While further studies are needed, the initial\ninsights of this study indicate a great potential in using large language\nmodels like ChatGPT to improve patient-centered care in radiology and other\nmedical domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeblick_K/0/1/0/all/0/1\">Katharina Jeblick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schachtner_B/0/1/0/all/0/1\">Balthasar Schachtner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dexl_J/0/1/0/all/0/1\">Jakob Dexl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittermeier_A/0/1/0/all/0/1\">Andreas Mittermeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuber_A/0/1/0/all/0/1\">Anna Theresa St&#xfc;ber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topalis_J/0/1/0/all/0/1\">Johanna Topalis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_T/0/1/0/all/0/1\">Tobias Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wesp_P/0/1/0/all/0/1\">Philipp Wesp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabel_B/0/1/0/all/0/1\">Bastian Sabel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricke_J/0/1/0/all/0/1\">Jens Ricke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingrisch_M/0/1/0/all/0/1\">Michael Ingrisch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Discussion on Building Practical NLP Leaderboards: The Case of Machine Translation. (arXiv:2106.06292v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.06292","description":"<p>Recent advances in AI and ML applications have benefited from rapid progress\nin NLP research. Leaderboards have emerged as a popular mechanism to track and\naccelerate progress in NLP through competitive model development. While this\nhas increased interest and participation, the over-reliance on single, and\naccuracy-based metrics have shifted focus from other important metrics that\nmight be equally pertinent to consider in real-world contexts. In this paper,\nwe offer a preliminary discussion of the risks associated with focusing\nexclusively on accuracy metrics and draw on recent discussions to highlight\nprescriptive suggestions on how to develop more practical and effective\nleaderboards that can better reflect the real-world utility of models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santy_S/0/1/0/all/0/1\">Sebastin Santy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_P/0/1/0/all/0/1\">Prasanta Bhattacharya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks. (arXiv:2201.05729v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.05729","description":"<p>Contrastive language-image pretraining (CLIP) links vision and language\nmodalities into a unified embedding space, yielding the tremendous potential\nfor vision-language (VL) tasks. While early concurrent works have begun to\nstudy this potential on a subset of tasks, important questions remain: 1) What\nis the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in\nlow-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches\nwithout impacting inference or pretraining complexity? In this work, we seek to\nanswer these questions through two key contributions. First, we introduce an\nevaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual\nEntailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of\ndata availability constraints and conditions of domain shift. Second, we\npropose an approach, named CLIP Targeted Distillation (CLIP-TD), to\nintelligently distill knowledge from CLIP into existing architectures using a\ndynamically weighted objective applied to adaptively selected tokens per\ninstance. Experiments demonstrate that our proposed CLIP-TD leads to\nexceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to\n71.3%) conditions of VCR, while simultaneously improving performance under\nstandard fully-supervised conditions (up to 2%), achieving state-of-art\nperformance on VCR compared to other single models that are pretrained with\nimage-text data only. On SNLI-VE, CLIP-TD produces significant gains in\nlow-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On\nVQA, CLIP-TD provides improvement in low-shot (up to 9%), and in\nfully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works\nutilizing CLIP for finetuning, as well as baseline naive distillation\napproaches. Code will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhecan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Codella_N/0/1/0/all/0/1\">Noel Codella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1\">Haoxuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation. (arXiv:2202.07959v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.07959","description":"<p>We introduce EdgeFormer -- a parameter-efficient Transformer for on-device\nseq2seq generation under the strict computation and memory constraints.\nCompared with the previous parameter-efficient Transformers, EdgeFormer applies\ntwo novel principles for cost-effective parameterization, allowing it to\nperform better given the same parameter budget; moreover, EdgeFormer is further\nenhanced by layer adaptation innovation that is proposed for improving the\nnetwork with shared layers.\n</p>\n<p>Extensive experiments show EdgeFormer can effectively outperform previous\nparameter-efficient Transformer baselines and achieve competitive results under\nboth the computation and memory constraints. Given the promising results, we\nrelease EdgeLM -- the pretrained version of EdgeFormer, which is the first\npublicly available pretrained on-device seq2seq model that can be easily\nfine-tuned for seq2seq tasks with strong results, facilitating on-device\nseq2seq generation in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si-Qing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translating Hanja Historical Documents to Contemporary Korean and English. (arXiv:2205.10019v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10019","description":"<p>The Annals of Joseon Dynasty (AJD) contain the daily records of the Kings of\nJoseon, the 500-year kingdom preceding the modern nation of Korea. The Annals\nwere originally written in an archaic Korean writing system, `Hanja', and were\ntranslated into Korean from 1968 to 1993. The resulting translation was however\ntoo literal and contained many archaic Korean words; thus, a new expert\ntranslation effort began in 2012. Since then, the records of only one king have\nbeen completed in a decade. In parallel, expert translators are working on\nEnglish translation, also at a slow pace and produced only one king's records\nin English so far. Thus, we propose H2KE, a neural machine translation model,\nthat translates historical documents in Hanja to more easily understandable\nKorean and to English. Built on top of multilingual neural machine translation,\nH2KE learns to translate a historical document written in Hanja, from both a\nfull dataset of outdated Korean translation and a small dataset of more\nrecently translated contemporary Korean and English. We compare our method\nagainst two baselines: a recent model that simultaneously learns to restore and\ntranslate Hanja historical document and a Transformer based model trained only\non newly translated corpora. The experiments reveal that our method\nsignificantly outperforms the baselines in terms of BLEU scores for both\ncontemporary Korean and English translations. We further conduct extensive\nhuman evaluation which shows that our translation is preferred over the\noriginal expert translations by both experts and non-expert Korean speakers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Son_J/0/1/0/all/0/1\">Juhee Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jiho Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_H/0/1/0/all/0/1\">Haneul Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bak_J/0/1/0/all/0/1\">JinYeong Bak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Robustness of Dialogue History Representation in Conversational Question Answering: A Comprehensive Study and a New Prompt-based Method. (arXiv:2206.14796v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.14796","description":"<p>Most works on modeling the conversation history in Conversational Question\nAnswering (CQA) report a single main result on a common CQA benchmark. While\nexisting models show impressive results on CQA leaderboards, it remains unclear\nwhether they are robust to shifts in setting (sometimes to more realistic\nones), training data size (e.g. from large to small sets) and domain. In this\nwork, we design and conduct the first large-scale robustness study of history\nmodeling approaches for CQA. We find that high benchmark scores do not\nnecessarily translate to strong robustness, and that various methods can\nperform extremely differently under different settings. Equipped with the\ninsights from our study, we design a novel prompt-based history modeling\napproach, and demonstrate its strong robustness across various settings. Our\napproach is inspired by existing methods that highlight historic answers in the\npassage. However, instead of highlighting by modifying the passage token\nembeddings, we add textual prompts directly in the passage text. Our approach\nis simple, easy-to-plug into practically any model, and highly effective, thus\nwe recommend it as a starting point for future model developers. We also hope\nthat our study and insights will raise awareness to the importance of\nrobustness-focused evaluation, in addition to obtaining high leaderboard\nscores, leading to better CQA systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gekhman_Z/0/1/0/all/0/1\">Zorik Gekhman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oved_N/0/1/0/all/0/1\">Nadav Oved</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_O/0/1/0/all/0/1\">Orgad Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szpektor_I/0/1/0/all/0/1\">Idan Szpektor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Synthesis with Mixed Emotions. (arXiv:2208.05890v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.05890","description":"<p>Emotional speech synthesis aims to synthesize human voices with various\nemotional effects. The current studies are mostly focused on imitating an\naveraged style belonging to a specific emotion type. In this paper, we seek to\ngenerate speech with a mixture of emotions at run-time. We propose a novel\nformulation that measures the relative difference between the speech samples of\ndifferent emotions. We then incorporate our formulation into a\nsequence-to-sequence emotional text-to-speech framework. During the training,\nthe framework does not only explicitly characterize emotion styles, but also\nexplores the ordinal nature of emotions by quantifying the differences with\nother emotions. At run-time, we control the model to produce the desired\nemotion mixture by manually defining an emotion attribute vector. The objective\nand subjective evaluations have validated the effectiveness of the proposed\nframework. To our best knowledge, this research is the first study on\nmodelling, synthesizing, and evaluating mixed emotions in speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sisman_B/0/1/0/all/0/1\">Berrak Sisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_R/0/1/0/all/0/1\">Rajib Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">B. W. Schuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Linguistically Informed Multi-Objective Pre-Training for Natural Language Inference. (arXiv:2212.07428v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.07428","description":"<p>We introduce a linguistically enhanced combination of pre-training methods\nfor transformers. The pre-training objectives include POS-tagging, synset\nprediction based on semantic knowledge graphs, and parent prediction based on\ndependency parse trees. Our approach achieves competitive results on the\nNatural Language Inference task, compared to the state of the art. Specifically\nfor smaller models, the method results in a significant performance boost,\nemphasizing the fact that intelligent pre-training can make up for fewer\nparameters and help building more efficient models. Combining POS-tagging and\nsynset prediction yields the overall best results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pielka_M/0/1/0/all/0/1\">Maren Pielka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_S/0/1/0/all/0/1\">Svetlana Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pucknat_L/0/1/0/all/0/1\">Lisa Pucknat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifa_R/0/1/0/all/0/1\">Rafet Sifa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Needle in a Haystack: An Analysis of Finding Qualified Workers on MTurk for Summarization. (arXiv:2212.10397v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10397","description":"<p>The acquisition of high-quality human annotations through crowdsourcing\nplatforms like Amazon Mechanical Turk (MTurk) is more challenging than\nexpected. The annotation quality might be affected by various aspects like\nannotation instructions, Human Intelligence Task (HIT) design, and wages paid\nto annotators, etc. To avoid potentially low-quality annotations which could\nmislead the evaluation of automatic summarization system outputs, we\ninvestigate the recruitment of high-quality MTurk workers via a three-step\nqualification pipeline. We show that we can successfully filter out bad workers\nbefore they carry out the evaluations and obtain high-quality annotations while\noptimizing the use of resources. This paper can serve as basis for the\nrecruitment of qualified annotators in other challenging annotation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lining Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mille_S/0/1/0/all/0/1\">Simon Mille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yufang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deutsch_D/0/1/0/all/0/1\">Daniel Deutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_E/0/1/0/all/0/1\">Elizabeth Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clinciu_M/0/1/0/all/0/1\">Miruna Clinciu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahamood_S/0/1/0/all/0/1\">Saad Mahamood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1\">Khyathi Chandu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-01-01T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}