{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-07-11T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models. (arXiv:2307.03762v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03762","description":"<p>In this perspective paper, we first comprehensively review existing\nevaluations of Large Language Models (LLMs) using both standardized tests and\nability-oriented benchmarks. We pinpoint several problems with current\nevaluation methods that tend to overstate the capabilities of LLMs. We then\narticulate what artificial general intelligence should encompass beyond the\ncapabilities of LLMs. We propose four characteristics of generally intelligent\nagents: 1) they can perform unlimited tasks; 2) they can generate new tasks\nwithin a context; 3) they operate based on a value system that underpins task\ngeneration; and 4) they have a world model reflecting reality, which shapes\ntheir interaction with the world. Building on this viewpoint, we highlight the\nmissing pieces in artificial general intelligence, that is, the unity of\nknowing and acting. We argue that active engagement with objects in the real\nworld delivers more robust signals for forming conceptual representations.\nAdditionally, knowledge acquisition isn't solely reliant on passive input but\nrequires repeated trials and errors. We conclude by outlining promising future\nresearch directions in the field of artificial general intelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuxi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"For Women, Life, Freedom: A Participatory AI-Based Social Web Analysis of a Watershed Moment in Iran's Gender Struggles. (arXiv:2307.03764v1 [cs.CY])","link":"http://arxiv.org/abs/2307.03764","description":"<p>In this paper, we present a computational analysis of the Persian language\nTwitter discourse with the aim to estimate the shift in stance toward gender\nequality following the death of Mahsa Amini in police custody. We present an\nensemble active learning pipeline to train a stance classifier. Our novelty\nlies in the involvement of Iranian women in an active role as annotators in\nbuilding this AI system. Our annotators not only provide labels, but they also\nsuggest valuable keywords for more meaningful corpus creation as well as\nprovide short example documents for a guided sampling step. Our analyses\nindicate that Mahsa Amini's death triggered polarized Persian language\ndiscourse where both fractions of negative and positive tweets toward gender\nequality increased. The increase in positive tweets was slightly greater than\nthe increase in negative tweets. We also observe that with respect to account\ncreation time, between the state-aligned Twitter accounts and pro-protest\nTwitter accounts, pro-protest accounts are more similar to baseline Persian\nTwitter activity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khorramrouz_A/0/1/0/all/0/1\">Adel Khorramrouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Sujan Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+KhudaBukhsh_A/0/1/0/all/0/1\">Ashiqur R. KhudaBukhsh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linguistic representations for fewer-shot relation extraction across domains. (arXiv:2307.03823v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03823","description":"<p>Recent work has demonstrated the positive impact of incorporating linguistic\nrepresentations as additional context and scaffolding on the in-domain\nperformance of several NLP tasks. We extend this work by exploring the impact\nof linguistic representations on cross-domain performance in a few-shot\ntransfer setting. An important question is whether linguistic representations\nenhance generalizability by providing features that function as cross-domain\npivots. We focus on the task of relation extraction on three datasets of\nprocedural text in two domains, cooking and materials science. Our approach\naugments a popular transformer-based architecture by alternately incorporating\nsyntactic and semantic graphs constructed by freely available off-the-shelf\ntools. We examine their utility for enhancing generalization, and investigate\nwhether earlier findings, e.g. that semantic representations can be more\nhelpful than syntactic ones, extend to relation extraction in multiple domains.\nWe find that while the inclusion of these graphs results in significantly\nhigher performance in few-shot transfer, both types of graph exhibit roughly\nequivalent utility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gururaja_S/0/1/0/all/0/1\">Sireesh Gururaja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutt_R/0/1/0/all/0/1\">Ritam Dutt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_T/0/1/0/all/0/1\">Tinglong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rose_C/0/1/0/all/0/1\">Carolyn Rose</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RADAR: Robust AI-Text Detection via Adversarial Learning. (arXiv:2307.03838v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03838","description":"<p>Recent advances in large language models (LLMs) and the intensifying\npopularity of ChatGPT-like applications have blurred the boundary of\nhigh-quality text generation between humans and machines. However, in addition\nto the anticipated revolutionary changes to our technology and society, the\ndifficulty of distinguishing LLM-generated texts (AI-text) from human-generated\ntexts poses new challenges of misuse and fairness, such as fake content\ngeneration, plagiarism, and false accusation of innocent writers. While\nexisting works show that current AI-text detectors are not robust to LLM-based\nparaphrasing, this paper aims to bridge this gap by proposing a new framework\ncalled RADAR, which jointly trains a Robust AI-text Detector via Adversarial\nleaRning. RADAR is based on adversarial training of a paraphraser and a\ndetector. The paraphraser's goal is to generate realistic contents to evade\nAI-text detection. RADAR uses the feedback from the detector to update the\nparaphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly\n2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets,\nexperimental results show that RADAR significantly outperforms existing AI-text\ndetection methods, especially when paraphrasing is in place. We also identify\nthe strong transferability of RADAR from instruction-tuned LLMs to other LLMs,\nand evaluate the improved capability of RADAR via GPT-3.5.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaomeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_T/0/1/0/all/0/1\">Tsung-Yi Ho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDACE: MIMIC Documents Annotated with Code Evidence. (arXiv:2307.03859v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03859","description":"<p>We introduce a dataset for evidence/rationale extraction on an extreme\nmulti-label classification task over long medical documents. One such task is\nComputer-Assisted Coding (CAC) which has improved significantly in recent\nyears, thanks to advances in machine learning technologies. Yet simply\npredicting a set of final codes for a patient encounter is insufficient as CAC\nsystems are required to provide supporting textual evidence to justify the\nbilling codes. A model able to produce accurate and reliable supporting\nevidence for each code would be a tremendous benefit. However, a human\nannotated code evidence corpus is extremely difficult to create because it\nrequires specialized knowledge. In this paper, we introduce MDACE, the first\npublicly available code evidence dataset, which is built on a subset of the\nMIMIC-III clinical records. The dataset -- annotated by professional medical\ncoders -- consists of 302 Inpatient charts with 3,934 evidence spans and 52\nProfee charts with 5,563 evidence spans. We implemented several evidence\nextraction methods based on the EffectiveCAN model (Liu et al., 2021) to\nestablish baseline performance on this dataset. MDACE can be used to evaluate\ncode evidence extraction methods for CAC systems, as well as the accuracy and\ninterpretability of deep learning models for multi-label classification. We\nbelieve that the release of MDACE will greatly improve the understanding and\napplication of deep learning technologies for medical coding and document\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hua Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafari_R/0/1/0/all/0/1\">Rana Jafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_A/0/1/0/all/0/1\">April Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klopfer_R/0/1/0/all/0/1\">Russell Klopfer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_E/0/1/0/all/0/1\">Edmond Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Striner_B/0/1/0/all/0/1\">Benjamin Striner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gormley_M/0/1/0/all/0/1\">Matthew R. Gormley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models for Supply Chain Optimization. (arXiv:2307.03875v1 [cs.AI])","link":"http://arxiv.org/abs/2307.03875","description":"<p>Supply chain operations traditionally involve a variety of complex decision\nmaking problems. Over the last few decades, supply chains greatly benefited\nfrom advances in computation, which allowed the transition from manual\nprocessing to automation and cost-effective optimization. Nonetheless, business\noperators still need to spend substantial efforts in \\emph{explaining} and\ninterpreting the optimization outcomes to stakeholders. Motivated by the recent\nadvances in Large Language Models (LLMs), we study how this disruptive\ntechnology can help bridge the gap between supply chain automation and human\ncomprehension and trust thereof. We design \\name{} -- a framework that accepts\nas input queries in plain text, and outputs insights about the underlying\noptimization outcomes. Our framework does not forgo the state-of-the-art\ncombinatorial optimization technology, but rather leverages it to\nquantitatively answer what-if scenarios (e.g., how would the cost change if we\nused supplier B instead of supplier A for a given demand?). Importantly, our\ndesign does not require sending proprietary data over to LLMs, which can be a\nprivacy concern in some circumstances. We demonstrate the effectiveness of our\nframework on a real server placement scenario within Microsoft's cloud supply\nchain. Along the way, we develop a general evaluation benchmark, which can be\nused to evaluate the accuracy of the LLM output in other scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Beibin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mellou_K/0/1/0/all/0/1\">Konstantina Mellou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathuri_J/0/1/0/all/0/1\">Jeevan Pathuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menache_I/0/1/0/all/0/1\">Ishai Menache</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embedding Mental Health Discourse for Community Recommendation. (arXiv:2307.03892v1 [cs.IR])","link":"http://arxiv.org/abs/2307.03892","description":"<p>Our paper investigates the use of discourse embedding techniques to develop a\ncommunity recommendation system that focuses on mental health support groups on\nsocial media. Social media platforms provide a means for users to anonymously\nconnect with communities that cater to their specific interests. However, with\nthe vast number of online communities available, users may face difficulties in\nidentifying relevant groups to address their mental health concerns. To address\nthis challenge, we explore the integration of discourse information from\nvarious subreddit communities using embedding techniques to develop an\neffective recommendation system. Our approach involves the use of content-based\nand collaborative filtering techniques to enhance the performance of the\nrecommendation system. Our findings indicate that the proposed approach\noutperforms the use of each technique separately and provides interpretability\nin the recommendation process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dang_H/0/1/0/all/0/1\">Hy Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Bang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziems_N/0/1/0/all/0/1\">Noah Ziems</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Answering Ambiguous Questions via Iterative Prompting. (arXiv:2307.03897v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03897","description":"<p>In open-domain question answering, due to the ambiguity of questions,\nmultiple plausible answers may exist. To provide feasible answers to an\nambiguous question, one approach is to directly predict all valid answers, but\nthis can struggle with balancing relevance and diversity. An alternative is to\ngather candidate answers and aggregate them, but this method can be\ncomputationally costly and may neglect dependencies among answers. In this\npaper, we present AmbigPrompt to address the imperfections of existing\napproaches to answering ambiguous questions. Specifically, we integrate an\nanswering model with a prompting model in an iterative manner. The prompting\nmodel adaptively tracks the reading process and progressively triggers the\nanswering model to compose distinct and relevant answers. Additionally, we\ndevelop a task-specific post-pretraining approach for both the answering model\nand the prompting model, which greatly improves the performance of our\nframework. Empirical studies on two commonly-used open benchmarks show that\nAmbigPrompt achieves state-of-the-art or competitive results while using less\nmemory and having a lower inference latency than competing approaches.\nAdditionally, AmbigPrompt also performs well in low-resource settings. The code\nare available at: https://github.com/sunnweiwei/AmbigPrompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hengyi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongshen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ScriptWorld: Text Based Environment For Learning Procedural Knowledge. (arXiv:2307.03906v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03906","description":"<p>Text-based games provide a framework for developing natural language\nunderstanding and commonsense knowledge about the world in reinforcement\nlearning based agents. Existing text-based environments often rely on fictional\nsituations and characters to create a gaming framework and are far from\nreal-world scenarios. In this paper, we introduce ScriptWorld: a text-based\nenvironment for teaching agents about real-world daily chores and hence\nimparting commonsense knowledge. To the best of our knowledge, it is the first\ninteractive text-based gaming framework that consists of daily real-world human\nactivities designed using scripts dataset. We provide gaming environments for\n10 daily activities and perform a detailed analysis of the proposed\nenvironment. We develop RL-based baseline models/agents to play the games in\nScriptworld. To understand the role of language models in such environments, we\nleverage features obtained from pre-trained language models in the RL agents.\nOur experiments show that prior knowledge obtained from a pre-trained language\nmodel helps to solve real-world text-based gaming environments. We release the\nenvironment via Github: https://github.com/Exploration-Lab/ScriptWorld\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Abhinav Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1\">Areeb Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_U/0/1/0/all/0/1\">Umang Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1\">Ashutosh Modi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On decoder-only architecture for speech-to-text and large language model integration. (arXiv:2307.03917v1 [eess.AS])","link":"http://arxiv.org/abs/2307.03917","description":"<p>Large language models (LLMs) have achieved remarkable success in the field of\nnatural language processing, enabling better human-computer interaction using\nnatural language. However, the seamless integration of speech signals into LLMs\nhas not been explored well. The \"decoder-only\" architecture has also not been\nwell studied for speech processing tasks. In this research, we introduce\nSpeech-LLaMA, a novel approach that effectively incorporates acoustic\ninformation into text-based large language models. Our method leverages\nConnectionist Temporal Classification and a simple audio encoder to map the\ncompressed acoustic features to the continuous semantic space of the LLM. In\naddition, we further probe the decoder-only architecture for speech-to-text\ntasks by training a smaller scale randomly initialized speech-LLaMA model from\nspeech-text paired data alone. We conduct experiments on multilingual\nspeech-to-text translation tasks and demonstrate a significant improvement over\nstrong baselines, highlighting the potential advantages of decoder-only models\nfor speech-to-text conversion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaur_Y/0/1/0/all/0/1\">Yashesh Gaur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1\">Yimeng Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1\">Tianrui Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_B/0/1/0/all/0/1\">Bo Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_L/0/1/0/all/0/1\">Linquan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions. (arXiv:2307.03941v1 [cs.CY])","link":"http://arxiv.org/abs/2307.03941","description":"<p>The Right to be Forgotten (RTBF) was first established as the result of the\nruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz\\'alez, and\nwas later included as the Right to Erasure under the General Data Protection\nRegulation (GDPR) of European Union to allow individuals the right to request\npersonal data be deleted by organizations. Specifically for search engines,\nindividuals can send requests to organizations to exclude their information\nfrom the query results. With the recent development of Large Language Models\n(LLMs) and their use in chatbots, LLM-enabled software systems have become\npopular. But they are not excluded from the RTBF. Compared with the indexing\napproach used by search engines, LLMs store, and process information in a\ncompletely different way. This poses new challenges for compliance with the\nRTBF. In this paper, we explore these challenges and provide our insights on\nhow to implement technical solutions for the RTBF, including the use of machine\nunlearning, model editing, and prompting engineering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dawen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finckenberg_Broman_P/0/1/0/all/0/1\">Pamela Finckenberg-Broman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1\">Thong Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shidong Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1\">Zhenchang Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staples_M/0/1/0/all/0/1\">Mark Staples</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiwei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT a Good Personality Recognizer? A Preliminary Study. (arXiv:2307.03952v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03952","description":"<p>In recent years, personality has been regarded as a valuable personal factor\nbeing incorporated into numerous tasks such as sentiment analysis and product\nrecommendation. This has led to widespread attention to text-based personality\nrecognition task, which aims to identify an individual's personality based on\ngiven text. Considering that ChatGPT has recently exhibited remarkable\nabilities on various natural language processing tasks, we provide a\npreliminary evaluation of ChatGPT on text-based personality recognition task\nfor generating effective personality data. Concretely, we employ a variety of\nprompting strategies to explore ChatGPT's ability in recognizing personality\nfrom given text, especially the level-oriented prompting strategy we designed\nfor guiding ChatGPT in analyzing given text at a specified level. We compare\nthe performance of ChatGPT on two representative real-world datasets with\ntraditional neural network, fine-tuned RoBERTa, and corresponding\nstate-of-the-art task-specific model. The experimental results show that\nChatGPT with zero-shot chain-of-thought prompting exhibits impressive\npersonality recognition ability. Triggered by zero-shot chain-of-thought\nprompting, ChatGPT outperforms fine-tuned RoBERTa on the two datasets and is\ncapable to provide natural language explanations through text-based logical\nreasoning. Furthermore, relative to zero-shot chain-of-thought prompting,\nzero-shot level-oriented chain-of-thought prompting enhances the personality\nprediction ability of ChatGPT and reduces the performance gap between ChatGPT\nand corresponding state-of-the-art task-specific model. Besides, we also\nconduct experiments to observe the fairness of ChatGPT when identifying\npersonality and discover that ChatGPT shows unfairness to some sensitive\ndemographic attributes such as gender and age.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yu Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Liang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Capability of Large-scale Language Models on Chinese Grammatical Error Correction Task. (arXiv:2307.03972v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03972","description":"<p>Large-scale language models (LLMs) has shown remarkable capability in various\nof Natural Language Processing (NLP) tasks and attracted lots of attention\nrecently. However, some studies indicated that large language models fail to\nachieve promising result beyond the state-of-the-art models in English\ngrammatical error correction (GEC) tasks. In this report, we aim to explore the\nhow large language models perform on Chinese grammatical error correction tasks\nand provide guidance for future work. We conduct experiments with 3 different\nLLMs of different model scale on 4 Chinese GEC dataset. Our experimental\nresults indicate that the performances of LLMs on automatic evaluation metrics\nfalls short of the previous sota models because of the problem of\nover-correction. Furthermore, we also discover notable variations in the\nperformance of LLMs when evaluated on different data distributions. Our\nfindings demonstrates that further investigation is required for the\napplication of LLMs on Chinese GEC task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_F/0/1/0/all/0/1\">Fanyi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation. (arXiv:2307.03987v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03987","description":"<p>Recently developed large language models have achieved remarkable success in\ngenerating fluent and coherent text. However, these models often tend to\n'hallucinate' which critically hampers their reliability. In this work, we\naddress this crucial problem and propose an approach that actively detects and\nmitigates hallucinations during the generation process. Specifically, we first\nidentify the candidates of potential hallucination leveraging the model's logit\noutput values, check their correctness through a validation procedure, mitigate\nthe detected hallucinations, and then continue with the generation process.\nThrough extensive experiments with the 'article generation task', we first\ndemonstrate the individual efficacy of our detection and mitigation techniques.\nSpecifically, the detection technique achieves a recall of 88% and the\nmitigation technique successfully mitigates 57.6% of the correctly detected\nhallucinations. Importantly, our mitigation technique does not introduce new\nhallucinations even in the case of incorrectly detected hallucinations, i.e.,\nfalse positives. Then, we show that the proposed active detection and\nmitigation approach successfully reduces the hallucinations of the GPT-3 model\nfrom 47.5% to 14.5% on average. In summary, our work contributes to improving\nthe reliability and trustworthiness of large language models, a crucial step en\nroute to enabling their widespread adoption in real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wenlin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianshu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Interactive Dictation. (arXiv:2307.04008v1 [cs.CL])","link":"http://arxiv.org/abs/2307.04008","description":"<p>Voice dictation is an increasingly important text input modality. Existing\nsystems that allow both dictation and editing-by-voice restrict their command\nlanguage to flat templates invoked by trigger words. In this work, we study the\nfeasibility of allowing users to interrupt their dictation with spoken editing\ncommands in open-ended natural language. We introduce a new task and dataset,\nTERTiUS, to experiment with such systems. To support this flexibility in\nreal-time, a system must incrementally segment and classify spans of speech as\neither dictation or command, and interpret the spans that are commands. We\nexperiment with using large pre-trained language models to predict the edited\ntext, or alternatively, to predict a small text-editing program. Experiments\nshow a natural trade-off between model accuracy and latency: a smaller model\nachieves 30% end-state accuracy with 1.3 seconds of latency, while a larger\nmodel achieves 55% end-state accuracy with 7 seconds of latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Belinda Z. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1\">Jason Eisner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauls_A/0/1/0/all/0/1\">Adam Pauls</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomson_S/0/1/0/all/0/1\">Sam Thomson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Cross-Lingual Summarization: A Corpus-based Study and A New Benchmark with Improved Annotation. (arXiv:2307.04018v1 [cs.CL])","link":"http://arxiv.org/abs/2307.04018","description":"<p>Most existing cross-lingual summarization (CLS) work constructs CLS corpora\nby simply and directly translating pre-annotated summaries from one language to\nanother, which can contain errors from both summarization and translation\nprocesses. To address this issue, we propose ConvSumX, a cross-lingual\nconversation summarization benchmark, through a new annotation schema that\nexplicitly considers source input context. ConvSumX consists of 2 sub-tasks\nunder different real-world scenarios, with each covering 3 language directions.\nWe conduct thorough analysis on ConvSumX and 3 widely-used manually annotated\nCLS corpora and empirically find that ConvSumX is more faithful towards input\ntext. Additionally, based on the same intuition, we propose a 2-Step method,\nwhich takes both conversation and summary as input to simulate human annotation\nprocess. Experimental results show that 2-Step method surpasses strong\nbaselines on ConvSumX under both automatic and human evaluation. Analysis shows\nthat both source input text and summary are crucial for modeling cross-lingual\nsummaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huajian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yijie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xuefeng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yueguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jianhao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yafu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Judy Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Michael Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How is Fatherhood Framed Online in Singapore?. (arXiv:2307.04053v1 [cs.CL])","link":"http://arxiv.org/abs/2307.04053","description":"<p>The proliferation of discussion about fatherhood in Singapore attests to its\nsignificance, indicating the need for an exploration of how fatherhood is\nframed, aiding policy-making around fatherhood in Singapore. Sound and holistic\npolicy around fatherhood in Singapore may reduce stigma and apprehension around\nbeing a parent, critical to improving the nations flagging birth rate. We\nanalyzed 15,705 articles and 56,221 posts to study how fatherhood is framed in\nSingapore across a range of online platforms (news outlets, parenting forums,\nTwitter). We used NLP techniques to understand these differences. While\nfatherhood was framed in a range of ways on the Singaporean online environment,\nit did not seem that fathers were framed as central to the Singaporean family\nunit. A strength of our work is how the different techniques we have applied\nvalidate each other.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Van_T/0/1/0/all/0/1\">Tran Hien Van</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1\">Abhay Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddique_M/0/1/0/all/0/1\">Muhammad Siddique</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_L/0/1/0/all/0/1\">Lam Yin Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_N/0/1/0/all/0/1\">Nimay Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jonathan Y Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCrickerd_K/0/1/0/all/0/1\">Keri McCrickerd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandoc_E/0/1/0/all/0/1\">Edson C Tandoc Jr.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_G/0/1/0/all/0/1\">Gerard Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1\">Navin Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bidirectional Attention as a Mixture of Continuous Word Experts. (arXiv:2307.04057v1 [cs.CL])","link":"http://arxiv.org/abs/2307.04057","description":"<p>Bidirectional attention $\\unicode{x2013}$ composed of self-attention with\npositional encodings and the masked language model (MLM) objective\n$\\unicode{x2013}$ has emerged as a key component of modern large language\nmodels (LLMs). Despite its empirical success, few studies have examined its\nstatistical underpinnings: What statistical model is bidirectional attention\nimplicitly fitting? What sets it apart from its non-attention predecessors? We\nexplore these questions in this paper. The key observation is that fitting a\nsingle-layer single-head bidirectional attention, upon reparameterization, is\nequivalent to fitting a continuous bag of words (CBOW) model with\nmixture-of-experts (MoE) weights. Further, bidirectional attention with\nmultiple heads and multiple layers is equivalent to stacked MoEs and a mixture\nof MoEs, respectively. This statistical viewpoint reveals the distinct use of\nMoE in bidirectional attention, which aligns with its practical effectiveness\nin handling heterogeneous data. It also suggests an immediate extension to\ncategorical tabular data, if we view each word location in a sentence as a\ntabular feature. Across empirical studies, we find that this extension\noutperforms existing tabular extensions of transformers in out-of-distribution\n(OOD) generalization. Finally, this statistical perspective of bidirectional\nattention enables us to theoretically characterize when linear word analogies\nare present in its word embeddings. These analyses show that bidirectional\nattention can require much stronger assumptions to exhibit linear word\nanalogies than its non-attention predecessors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wibisono_K/0/1/0/all/0/1\">Kevin Christian Wibisono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yixin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs. (arXiv:2307.04090v1 [cs.CL])","link":"http://arxiv.org/abs/2307.04090","description":"<p>Recent work within the Argument Mining community has shown the applicability\nof Natural Language Processing systems for solving problems found within\ncompetitive debate. One of the most important tasks within competitive debate\nis for debaters to create high quality debate cases. We show that effective\ndebate cases can be constructed using constrained shortest path traversals on\nArgumentative Semantic Knowledge Graphs. We study this potential in the context\nof a type of American Competitive Debate, called Policy Debate, which already\nhas a large scale dataset targeting it called DebateSum. We significantly\nimprove upon DebateSum by introducing 53180 new examples, as well as further\nuseful metadata for every example, to the dataset. We leverage the txtai\nsemantic search and knowledge graph toolchain to produce and contribute 9\nsemantic knowledge graphs built on this dataset. We create a unique method for\nevaluating which knowledge graphs are better in the context of producing policy\ndebate cases. A demo which automatically generates debate cases, along with all\nother code and the Knowledge Graphs, are open-sourced and made available to the\npublic here: https://github.com/Hellisotherpeople/DebateKG\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roush_A/0/1/0/all/0/1\">Allen Roush</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing. (arXiv:2307.04096v1 [cs.CL])","link":"http://arxiv.org/abs/2307.04096","description":"<p>Cross-lingual semantic parsing transfers parsing capability from a\nhigh-resource language (e.g., English) to low-resource languages with scarce\ntraining data. Previous work has primarily considered silver-standard data\naugmentation or zero-shot methods, however, exploiting few-shot gold data is\ncomparatively unexplored. We propose a new approach to cross-lingual semantic\nparsing by explicitly minimizing cross-lingual divergence between probabilistic\nlatent variables using Optimal Transport. We demonstrate how this direct\nguidance improves parsing from natural languages using fewer examples and less\ntraining. We evaluate our method on two datasets, MTOP and MultiATIS++SQL,\nestablishing state-of-the-art results under a few-shot cross-lingual regime.\nAblation studies further reveal that our method improves performance even\nwithout parallel input translations. In addition, we show that our model better\ncaptures cross-lingual structure in the latent space to improve semantic\nrepresentation similarity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sherborne_T/0/1/0/all/0/1\">Tom Sherborne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosking_T/0/1/0/all/0/1\">Tom Hosking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FILM: How can Few-Shot Image Classification Benefit from Pre-Trained Language Models?. (arXiv:2307.04114v1 [cs.LG])","link":"http://arxiv.org/abs/2307.04114","description":"<p>Few-shot learning aims to train models that can be generalized to novel\nclasses with only a few samples. Recently, a line of works are proposed to\nenhance few-shot learning with accessible semantic information from class\nnames. However, these works focus on improving existing modules such as visual\nprototypes and feature extractors of the standard few-shot learning framework.\nThis limits the full potential use of semantic information. In this paper, we\npropose a novel few-shot learning framework that uses pre-trained language\nmodels based on contrastive learning. To address the challenge of alignment\nbetween visual features and textual embeddings obtained from text-based\npre-trained language model, we carefully design the textual branch of our\nframework and introduce a metric module to generalize the cosine similarity.\nFor better transferability, we let the metric module adapt to different\nfew-shot tasks and adopt MAML to train the model via bi-level optimization.\nMoreover, we conduct extensive experiments on multiple benchmarks to\ndemonstrate the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zihao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_Y/0/1/0/all/0/1\">Yunkai Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_D/0/1/0/all/0/1\">Dong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huishuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Weiran Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards cross-language prosody transfer for dialog. (arXiv:2307.04123v1 [cs.CL])","link":"http://arxiv.org/abs/2307.04123","description":"<p>Speech-to-speech translation systems today do not adequately support use for\ndialog purposes. In particular, nuances of speaker intent and stance can be\nlost due to improper prosody transfer. We present an exploration of what needs\nto be done to overcome this. First, we developed a data collection protocol in\nwhich bilingual speakers re-enact utterances from an earlier conversation in\ntheir other language, and used this to collect an English-Spanish corpus, so\nfar comprising 1871 matched utterance pairs. Second, we developed a simple\nprosodic dissimilarity metric based on Euclidean distance over a broad set of\nprosodic features. We then used these to investigate cross-language prosodic\ndifferences, measure the likely utility of three simple baseline models, and\nidentify phenomena which will require more powerful modeling. Our findings\nshould inform future research on cross-language prosody and the design of\nspeech-to-speech translation systems capable of effective prosody transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avila_J/0/1/0/all/0/1\">Jonathan E. Avila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ward_N/0/1/0/all/0/1\">Nigel G. Ward</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dream Content Discovery from Reddit with an Unsupervised Mixed-Method Approach. (arXiv:2307.04167v1 [cs.CY])","link":"http://arxiv.org/abs/2307.04167","description":"<p>Dreaming is a fundamental but not fully understood part of human experience\nthat can shed light on our thought patterns. Traditional dream analysis\npractices, while popular and aided by over 130 unique scales and rating\nsystems, have limitations. Mostly based on retrospective surveys or lab\nstudies, they struggle to be applied on a large scale or to show the importance\nand connections between different dream themes. To overcome these issues, we\ndeveloped a new, data-driven mixed-method approach for identifying topics in\nfree-form dream reports through natural language processing. We tested this\nmethod on 44,213 dream reports from Reddit's r/Dreams subreddit, where we found\n217 topics, grouped into 22 larger themes: the most extensive collection of\ndream topics to date. We validated our topics by comparing it to the\nwidely-used Hall and van de Castle scale. Going beyond traditional scales, our\nmethod can find unique patterns in different dream types (like nightmares or\nrecurring dreams), understand topic importance and connections, and observe\nchanges in collective dream experiences over time and around major events, like\nthe COVID-19 pandemic and the recent Russo-Ukrainian war. We envision that the\napplications of our method will provide valuable insights into the intricate\nnature of dreaming.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Anubhab Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scepanovic_S/0/1/0/all/0/1\">Sanja &#x160;&#x107;epanovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aiello_L/0/1/0/all/0/1\">Luca Maria Aiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallett_R/0/1/0/all/0/1\">Remington Mallett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrett_D/0/1/0/all/0/1\">Deirdre Barrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quercia_D/0/1/0/all/0/1\">Daniele Quercia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Generative Large Language Models Perform ASR Error Correction?. (arXiv:2307.04172v1 [cs.CL])","link":"http://arxiv.org/abs/2307.04172","description":"<p>ASR error correction continues to serve as an important part of\npost-processing for speech recognition systems. Traditionally, these models are\ntrained with supervised training using the decoding results of the underlying\nASR system and the reference text. This approach is computationally intensive\nand the model needs to be re-trained when switching the underlying ASR model.\nRecent years have seen the development of large language models and their\nability to perform natural language processing tasks in a zero-shot manner. In\nthis paper, we take ChatGPT as an example to examine its ability to perform ASR\nerror correction in the zero-shot or 1-shot settings. We use the ASR N-best\nlist as model input and propose unconstrained error correction and N-best\nconstrained error correction methods. Results on a Conformer-Transducer model\nand the pre-trained Whisper model show that we can largely improve the ASR\nsystem performance with error correction using the powerful ChatGPT model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_M/0/1/0/all/0/1\">Mengjie Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manakul_P/0/1/0/all/0/1\">Potsawee Manakul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark Gales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knill_K/0/1/0/all/0/1\">Kate Knill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering. (arXiv:2307.04192v1 [cs.CV])","link":"http://arxiv.org/abs/2307.04192","description":"<p>Video question--answering is a fundamental task in the field of video\nunderstanding. Although current vision--language models (VLMs) equipped with\nVideo Transformers have enabled temporal modeling and yielded superior results,\nthey are at the cost of huge computational power and thus too expensive to\ndeploy in real-time application scenarios. An economical workaround only\nsamples a small portion of frames to represent the main content of that video\nand tune an image--text model on these sampled frames. Recent video\nunderstanding models usually randomly sample a set of frames or clips,\nregardless of internal correlations between their visual contents, nor their\nrelevance to the problem. We argue that such kinds of aimless sampling may omit\nthe key frames from which the correct answer can be deduced, and the situation\ngets worse when the sampling sparsity increases, which always happens as the\nvideo lengths increase. To mitigate this issue, we propose two frame sampling\nstrategies, namely the most domain frames (MDF) and most implied frames (MIF),\nto maximally preserve those frames that are most likely vital to the given\nquestions. MDF passively minimizes the risk of key frame omission in a\nbootstrap manner, while MIS actively searches key frames customized for each\nvideo--question pair with the assistance of auxiliary models. The experimental\nresults on three public datasets from three advanced VLMs (CLIP, GIT and\nAll-in-one) demonstrate that our proposed strategies can boost the performance\nfor image--text pretrained models. The source codes pertaining to the method\nproposed in this paper are publicly available at\nhttps://github.com/declare-lab/sas-vqa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Min-Yen Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey. (arXiv:2307.04251v1 [cs.CL])","link":"http://arxiv.org/abs/2307.04251","description":"<p>ChatGPT is a large language model (LLM) created by OpenAI that has been\ncarefully trained on a large amount of data. It has revolutionized the field of\nnatural language processing (NLP) and has pushed the boundaries of LLM\ncapabilities. ChatGPT has played a pivotal role in enabling widespread public\ninteraction with generative artificial intelligence (GAI) on a large scale. It\nhas also sparked research interest in developing similar technologies and\ninvestigating their applications and implications. In this paper, our primary\ngoal is to provide a concise survey on the current lines of research on ChatGPT\nand its evolution. We considered both the glass box and black box views of\nChatGPT, encompassing the components and foundational elements of the\ntechnology, as well as its applications, impacts, and implications. The glass\nbox approach focuses on understanding the inner workings of the technology, and\nthe black box approach embraces it as a complex system, and thus examines its\ninputs, outputs, and effects. This paves the way for a comprehensive\nexploration of the technology and provides a road map for further research and\nexperimentation. We also lay out essential foundational literature on LLMs and\nGAI in general and their connection with ChatGPT. This overview sheds light on\nexisting and missing research lines in the emerging field of LLMs, benefiting\nboth public users and developers. Furthermore, the paper delves into the broad\nspectrum of applications and significant concerns in fields such as education,\nresearch, healthcare, finance, etc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohamadi_S/0/1/0/all/0/1\">Salman Mohamadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mujtaba_G/0/1/0/all/0/1\">Ghulam Mujtaba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doretto_G/0/1/0/all/0/1\">Gianfranco Doretto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adjeroh_D/0/1/0/all/0/1\">Donald A. Adjeroh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing the efficacy of large language models in generating accurate teacher responses. (arXiv:2307.04274v1 [cs.CL])","link":"http://arxiv.org/abs/2307.04274","description":"<p>(Tack et al., 2023) organized the shared task hosted by the 18th Workshop on\nInnovative Use of NLP for Building Educational Applications on generation of\nteacher language in educational dialogues. Following the structure of the\nshared task, in this study, we attempt to assess the generative abilities of\nlarge language models in providing informative and helpful insights to\nstudents, thereby simulating the role of a knowledgeable teacher. To this end,\nwe present an extensive evaluation of several benchmarking generative models,\nincluding GPT-4 (few-shot, in-context learning), fine-tuned GPT-2, and\nfine-tuned DialoGPT. Additionally, to optimize for pedagogical quality, we\nfine-tuned the Flan-T5 model using reinforcement learning. Our experimental\nfindings on the Teacher-Student Chatroom Corpus subset indicate the efficacy of\nGPT-4 over other fine-tuned models, measured using BERTScore and DialogRPT.\n</p>\n<p>We hypothesize that several dataset characteristics, including sampling,\nrepresentativeness, and dialog completeness, pose significant challenges to\nfine-tuning, thus contributing to the poor generalizability of the fine-tuned\nmodels. Finally, we note the need for these generative models to be evaluated\nwith a metric that relies not only on dialog coherence and matched language\nmodeling distribution but also on the model's ability to showcase pedagogical\nskills.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hicke_Y/0/1/0/all/0/1\">Yann Hicke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masand_A/0/1/0/all/0/1\">Abhishek Masand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wentao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangavarapu_T/0/1/0/all/0/1\">Tushaar Gangavarapu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Essay Scoring in Argumentative Writing: DeBERTeachingAssistant. (arXiv:2307.04276v1 [cs.CL])","link":"http://arxiv.org/abs/2307.04276","description":"<p>Automated Essay scoring has been explored as a research and industry problem\nfor over 50 years. It has drawn a lot of attention from the NLP community\nbecause of its clear educational value as a research area that can engender the\ncreation of valuable time-saving tools for educators around the world. Yet,\nthese tools are generally focused on detecting good grammar, spelling mistakes,\nand organization quality but tend to fail at incorporating persuasiveness\nfeatures in their final assessment. The responsibility to give actionable\nfeedback to the student to improve the strength of their arguments is left\nsolely on the teacher's shoulders. In this work, we present a transformer-based\narchitecture capable of achieving above-human accuracy in annotating\nargumentative writing discourse elements for their persuasiveness quality and\nwe expand on planned future work investigating the explainability of our model\nso that actionable feedback can be offered to the student and thus potentially\nenable a partnership between the teacher's advice and the machine's advice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hicke_Y/0/1/0/all/0/1\">Yann Hicke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_T/0/1/0/all/0/1\">Tonghua Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_K/0/1/0/all/0/1\">Karan Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Choong Hee Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HistRED: A Historical Document-Level Relation Extraction Dataset. (arXiv:2307.04285v1 [cs.CL])","link":"http://arxiv.org/abs/2307.04285","description":"<p>Despite the extensive applications of relation extraction (RE) tasks in\nvarious domains, little has been explored in the historical context, which\ncontains promising data across hundreds and thousands of years. To promote the\nhistorical RE research, we present HistRED constructed from Yeonhaengnok.\nYeonhaengnok is a collection of records originally written in Hanja, the\nclassical Chinese writing, which has later been translated into Korean. HistRED\nprovides bilingual annotations such that RE can be performed on Korean and\nHanja texts. In addition, HistRED supports various self-contained subtexts with\ndifferent lengths, from a sentence level to a document level, supporting\ndiverse context settings for researchers to evaluate the robustness of their RE\nmodels. To demonstrate the usefulness of our dataset, we propose a bilingual RE\nmodel that leverages both Korean and Hanja contexts to predict relations\nbetween entities. Our model outperforms monolingual baselines on HistRED,\nshowing that employing multiple language contexts supplements the RE\npredictions. The dataset is publicly available at:\nhttps://huggingface.co/datasets/Soyoung/HistRED under CC BY-NC-ND 4.0 license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Soyoung Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1\">Minseok Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1\">Youngwoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Generate Equitable Text in Dialogue from Biased Training Data. (arXiv:2307.04303v1 [cs.CL])","link":"http://arxiv.org/abs/2307.04303","description":"<p>The ingrained principles of fairness in a dialogue system's decision-making\nprocess and generated responses are crucial for user engagement, satisfaction,\nand task achievement. Absence of equitable and inclusive principles can hinder\nthe formation of common ground, which in turn negatively impacts the overall\nperformance of the system. For example, misusing pronouns in a user interaction\nmay cause ambiguity about the intended subject. Yet, there is no comprehensive\nstudy of equitable text generation in dialogue. Aptly, in this work, we use\ntheories of computational learning to study this problem. We provide formal\ndefinitions of equity in text generation, and further, prove formal connections\nbetween learning human-likeness and learning equity: algorithms for improving\nequity ultimately reduce to algorithms for improving human-likeness (on\naugmented data). With this insight, we also formulate reasonable conditions\nunder which text generation algorithms can learn to generate equitable text\nwithout any modifications to the biased training data on which they learn. To\nexemplify our theory in practice, we look at a group of algorithms for the\nGuessWhat?! visual dialogue game and, using this example, test our theory\nempirically. Our theory accurately predicts relative-performance of multiple\nalgorithms in generating equitable text as measured by both human and automated\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sicilia_A/0/1/0/all/0/1\">Anthony Sicilia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forecasting financial markets with semantic network analysis in the COVID-19 crisis. (arXiv:2009.04975v4 [q-fin.GN] UPDATED)","link":"http://arxiv.org/abs/2009.04975","description":"<p>This paper uses a new textual data index for predicting stock market data.\nThe index is applied to a large set of news to evaluate the importance of one\nor more general economic-related keywords appearing in the text. The index\nassesses the importance of the economic-related keywords, based on their\nfrequency of use and semantic network position. We apply it to the Italian\npress and construct indices to predict Italian stock and bond market returns\nand volatilities in a recent sample period, including the COVID-19 crisis. The\nevidence shows that the index captures the different phases of financial time\nseries well. Moreover, results indicate strong evidence of predictability for\nbond market data, both returns and volatilities, short and long maturities, and\nstock market volatility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Colladon_A/0/1/0/all/0/1\">A. Fronzetti Colladon</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Grassi_S/0/1/0/all/0/1\">S. Grassi</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Ravazzolo_F/0/1/0/all/0/1\">F. Ravazzolo</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Violante_F/0/1/0/all/0/1\">F. Violante</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KenSwQuAD -- A Question Answering Dataset for Swahili Low Resource Language. (arXiv:2205.02364v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02364","description":"<p>The need for Question Answering datasets in low resource languages is the\nmotivation of this research, leading to the development of Kencorpus Swahili\nQuestion Answering Dataset, KenSwQuAD. This dataset is annotated from raw story\ntexts of Swahili low resource language, which is a predominantly spoken in\nEastern African and in other parts of the world. Question Answering (QA)\ndatasets are important for machine comprehension of natural language for tasks\nsuch as internet search and dialog systems. Machine learning systems need\ntraining data such as the gold standard Question Answering set developed in\nthis research. The research engaged annotators to formulate QA pairs from\nSwahili texts collected by the Kencorpus project, a Kenyan languages corpus.\nThe project annotated 1,445 texts from the total 2,585 texts with at least 5 QA\npairs each, resulting into a final dataset of 7,526 QA pairs. A quality\nassurance set of 12.5% of the annotated texts confirmed that the QA pairs were\nall correctly annotated. A proof of concept on applying the set to the QA task\nconfirmed that the dataset can be usable for such tasks. KenSwQuAD has also\ncontributed to resourcing of the Swahili language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wanjawa_B/0/1/0/all/0/1\">Barack W. Wanjawa</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wanzare_L/0/1/0/all/0/1\">Lilian D.A. Wanzare</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Indede_F/0/1/0/all/0/1\">Florence Indede</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+McOnyango_O/0/1/0/all/0/1\">Owen McOnyango</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Muchemi_L/0/1/0/all/0/1\">Lawrence Muchemi</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ombui_E/0/1/0/all/0/1\">Edward Ombui</a> (3) ((1) University of Nairobi Kenya, (2) Maseno University Kenya (3) Africa Nazarene University Kenya)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Coreference Resolution in Multiparty Dialogue. (arXiv:2208.01307v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.01307","description":"<p>Existing multiparty dialogue datasets for entity coreference resolution are\nnascent, and many challenges are still unaddressed. We create a large-scale\ndataset, Multilingual Multiparty Coref (MMC), for this task based on TV\ntranscripts. Due to the availability of gold-quality subtitles in multiple\nlanguages, we propose reusing the annotations to create silver coreference\nresolution data in other languages (Chinese and Farsi) via annotation\nprojection. On the gold (English) data, off-the-shelf models perform relatively\npoorly on MMC, suggesting that MMC has broader coverage of multiparty\ncoreference than prior datasets. On the silver data, we find success both using\nit for data augmentation and training from scratch, which effectively simulates\nthe zero-shot cross-lingual setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Boyuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1\">Patrick Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yarmohammadi_M/0/1/0/all/0/1\">Mahsa Yarmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies. (arXiv:2208.10264v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.10264","description":"<p>We introduce a new type of test, called a Turing Experiment (TE), for\nevaluating to what extent a given language model, such as GPT models, can\nsimulate different aspects of human behavior. A TE can also reveal consistent\ndistortions in a language model's simulation of a specific human behavior.\nUnlike the Turing Test, which involves simulating a single arbitrary\nindividual, a TE requires simulating a representative sample of participants in\nhuman subject research. We carry out TEs that attempt to replicate\nwell-established findings from prior studies. We design a methodology for\nsimulating TEs and illustrate its use to compare how well different language\nmodels are able to reproduce classic economic, psycholinguistic, and social\npsychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock\nExperiment, and Wisdom of Crowds. In the first three TEs, the existing findings\nwere replicated using recent models, while the last TE reveals a\n\"hyper-accuracy distortion\" present in some language models (including ChatGPT\nand GPT-4), which could affect downstream applications in education and the\narts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aher_G/0/1/0/all/0/1\">Gati Aher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arriaga_R/0/1/0/all/0/1\">Rosa I. Arriaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalai_A/0/1/0/all/0/1\">Adam Tauman Kalai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimedia Generative Script Learning for Task Planning. (arXiv:2208.12306v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.12306","description":"<p>Goal-oriented generative script learning aims to generate subsequent steps to\nreach a particular goal, which is an essential task to assist robots or humans\nin performing stereotypical activities. An important aspect of this process is\nthe ability to capture historical states visually, which provides detailed\ninformation that is not covered by text and will guide subsequent steps.\nTherefore, we propose a new task, Multimedia Generative Script Learning, to\ngenerate subsequent steps by tracking historical states in both text and vision\nmodalities, as well as presenting the first benchmark containing 5,652 tasks\nand 79,089 multimedia steps. This task is challenging in three aspects: the\nmultimedia challenge of capturing the visual states in images, the induction\nchallenge of performing unseen tasks, and the diversity challenge of covering\ndifferent information in individual steps. We propose to encode visual state\nchanges through a selective multimedia encoder to address the multimedia\nchallenge, transfer knowledge from previously observed tasks using a\nretrieval-augmented decoder to overcome the induction challenge, and further\npresent distinct information at each step by optimizing a diversity-oriented\ncontrastive learning objective. We define metrics to evaluate both generation\nand inductive quality. Experiment results demonstrate that our approach\nsignificantly outperforms strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingyun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Hou Pong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hockenmaier_J/0/1/0/all/0/1\">Julia Hockenmaier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhary_G/0/1/0/all/0/1\">Girish Chowdhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mars: Modeling Context & State Representations with Contrastive Learning for End-to-End Task-Oriented Dialog. (arXiv:2210.08917v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08917","description":"<p>Traditional end-to-end task-oriented dialog systems first convert dialog\ncontext into belief state and action state before generating the system\nresponse. The system response performance is significantly affected by the\nquality of the belief state and action state. We first explore what dialog\ncontext representation is beneficial to improving the quality of the belief\nstate and action state, which further enhances the generated response quality.\nTo tackle our exploration, we propose Mars, an end-to-end task-oriented dialog\nsystem with two contrastive learning strategies to model the relationship\nbetween dialog context and belief/action state representations. Empirical\nresults show dialog context representations, which are more different from\nsemantic state representations, are more conducive to multi-turn task-oriented\ndialog. Moreover, our proposed Mars achieves state-of-the-art performance on\nthe MultiWOZ 2.0, CamRest676, and CrossWOZ.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haipeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control. (arXiv:2211.05750v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05750","description":"<p>Pretrained language models have demonstrated extraordinary capabilities in\nlanguage generation. However, real-world tasks often require controlling the\ndistribution of generated text in order to mitigate bias, promote fairness, and\nachieve personalization. Existing techniques for controlling the distribution\nof generated text only work with quantified distributions, which require\npre-defined categories, proportions of the distribution, or an existing corpus\nfollowing the desired distributions. However, many important distributions,\nsuch as personal preferences, are unquantified. In this work, we tackle the\nproblem of generating text following arbitrary distributions (quantified and\nunquantified) by proposing Nano, a few-shot human-in-the-loop training\nalgorithm that continuously learns from human feedback. Nano achieves\nstate-of-the-art results on single topic/attribute as well as quantified\ndistribution control compared to previous works. We also show that Nano is able\nto learn unquantified distributions, achieves personalization, and captures\ndifferences between different individuals' personal preferences with high\nsample efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions. (arXiv:2211.12316v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2211.12316","description":"<p>Despite the widespread success of Transformers on NLP tasks, recent works\nhave found that they struggle to model several formal languages when compared\nto recurrent models. This raises the question of why Transformers perform well\nin practice and whether they have any properties that enable them to generalize\nbetter than recurrent models. In this work, we conduct an extensive empirical\nstudy on Boolean functions to demonstrate the following: (i) Random\nTransformers are relatively more biased towards functions of low sensitivity.\n(ii) When trained on Boolean functions, both Transformers and LSTMs prioritize\nlearning functions of low sensitivity, with Transformers ultimately converging\nto functions of lower sensitivity. (iii) On sparse Boolean functions which have\nlow sensitivity, we find that Transformers generalize near perfectly even in\nthe presence of noisy labels whereas LSTMs overfit and achieve poor\ngeneralization accuracy. Overall, our results provide strong quantifiable\nevidence that suggests differences in the inductive biases of Transformers and\nrecurrent models which may help explain Transformer's effective generalization\nperformance despite relatively limited expressiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattamishra_S/0/1/0/all/0/1\">Satwik Bhattamishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Arkil Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanade_V/0/1/0/all/0/1\">Varun Kanade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blunsom_P/0/1/0/all/0/1\">Phil Blunsom</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LENS: A Learnable Evaluation Metric for Text Simplification. (arXiv:2212.09739v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09739","description":"<p>Training learnable metrics using modern language models has recently emerged\nas a promising method for the automatic evaluation of machine translation.\nHowever, existing human evaluation datasets for text simplification have\nlimited annotations that are based on unitary or outdated models, making them\nunsuitable for this approach. To address these issues, we introduce the\nSimpEval corpus that contains: SimpEval_past, comprising 12K human ratings on\n2.4K simplifications of 24 past systems, and SimpEval_2022, a challenging\nsimplification benchmark consisting of over 1K human ratings of 360\nsimplifications including GPT-3.5 generated text. Training on SimpEval, we\npresent LENS, a Learnable Evaluation Metric for Text Simplification. Extensive\nempirical results show that LENS correlates much better with human judgment\nthan existing metrics, paving the way for future progress in the evaluation of\ntext simplification. We also introduce Rank and Rate, a human evaluation\nframework that rates simplifications from several models in a list-wise manner\nusing an interactive interface, which ensures both consistency and accuracy in\nthe evaluation process and is used to create the SimpEval datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maddela_M/0/1/0/all/0/1\">Mounica Maddela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">Yao Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heineman_D/0/1/0/all/0/1\">David Heineman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CausalDialogue: Modeling Utterance-level Causality in Conversations. (arXiv:2212.10515v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10515","description":"<p>Despite their widespread adoption, neural conversation models have yet to\nexhibit natural chat capabilities with humans. In this research, we examine\nuser utterances as causes and generated responses as effects, recognizing that\nchanges in a cause should produce a different effect. To further explore this\nconcept, we have compiled and expanded upon a new dataset called CausalDialogue\nthrough crowd-sourcing. This dataset includes multiple cause-effect pairs\nwithin a directed acyclic graph (DAG) structure. Our analysis reveals that\ntraditional loss functions struggle to effectively incorporate the DAG\nstructure, leading us to propose a causality-enhanced method called Exponential\nMaximum Average Treatment Effect (ExMATE) to enhance the impact of causality at\nthe utterance level in training neural conversation models. To evaluate the\nneeds of considering causality in dialogue generation, we built a comprehensive\nbenchmark on CausalDialogue dataset using different models, inference, and\ntraining methods. Through experiments, we find that a causality-inspired loss\nlike ExMATE can improve the diversity and agility of conventional loss function\nand there is still room for improvement to reach human-level quality on this\nnew dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tuan_Y/0/1/0/all/0/1\">Yi-Lin Tuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1\">Alon Albalak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenda Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pryor_C/0/1/0/all/0/1\">Connor Pryor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Getoor_L/0/1/0/all/0/1\">Lise Getoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Talk the Walk: Synthetic Data Generation for Conversational Music Recommendation. (arXiv:2301.11489v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2301.11489","description":"<p>Recommendation systems are ubiquitous yet often difficult for users to\ncontrol and adjust when recommendation quality is poor. This has motivated the\ndevelopment of conversational recommendation systems (CRSs), with control over\nrecommendations provided through natural language feedback. However, building\nconversational recommendation systems requires conversational training data\ninvolving user utterances paired with items that cover a diverse range of\npreferences. Such data has proved challenging to collect scalably using\nconventional methods like crowdsourcing. We address it in the context of\nitem-set recommendation, noting the increasing attention to this task motivated\nby use cases like music, news and recipe recommendation. We present a new\ntechnique, TalkTheWalk, that synthesizes realistic high-quality conversational\ndata by leveraging domain expertise encoded in widely available curated item\ncollections, showing how these can be transformed into corresponding item set\ncuration conversations. Specifically, TalkTheWalk generates a sequence of\nhypothetical yet plausible item sets returned by a system, then uses a language\nmodel to produce corresponding user utterances. Applying TalkTheWalk to music\nrecommendation, we generate over one million diverse playlist curation\nconversations. A human evaluation shows that the conversations contain\nconsistent utterances with relevant item sets, nearly matching the quality of\nsmall human-collected conversational data for this task. At the same time, when\nthe synthetic corpus is used to train a CRS, it improves Hits@100 by 10.5\npoints on a benchmark dataset over standard baselines and is preferred over the\ntop-performing baseline in an online evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leszczynski_M/0/1/0/all/0/1\">Megan Leszczynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganti_R/0/1/0/all/0/1\">Ravi Ganti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balog_K/0/1/0/all/0/1\">Krisztian Balog</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radlinski_F/0/1/0/all/0/1\">Filip Radlinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_F/0/1/0/all/0/1\">Fernando Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaganty_A/0/1/0/all/0/1\">Arun Tejasvi Chaganty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building High-accuracy Multilingual ASR with Gated Language Experts and Curriculum Training. (arXiv:2303.00786v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.00786","description":"<p>We propose gated language experts and curriculum training to enhance\nmultilingual transformer transducer models without requiring language\nidentification (LID) input from users during inference. Our method incorporates\na gating mechanism and LID loss, enabling transformer experts to learn\nlanguage-specific information. By combining gated transformer experts with\nshared transformer layers, we construct multilingual transformer blocks and\nutilize linear experts to effectively regularize the joint network. The\ncurriculum training scheme leverages LID to guide the gated experts in\nimproving their respective language performance. Experimental results on a\nbilingual task involving English and Spanish demonstrate significant\nimprovements, with average relative word error reductions of 12.5% and 7.3%\ncompared to the baseline bilingual and monolingual models, respectively.\nNotably, our method achieves performance comparable to the upper-bound model\ntrained and inferred with oracle LID. Extending our approach to trilingual,\nquadrilingual, and pentalingual models reveals similar advantages to those\nobserved in the bilingual models, highlighting its ease of extension to\nmultiple languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_E/0/1/0/all/0/1\">Eric Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuxuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yimeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jian Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1\">Edward Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Student's t-Distribution: On Measuring the Inter-Rater Reliability When the Observations are Scarce. (arXiv:2303.04526v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.04526","description":"<p>In natural language processing (NLP) we always rely on human judgement as the\ngolden quality evaluation method. However, there has been an ongoing debate on\nhow to better evaluate inter-rater reliability (IRR) levels for certain\nevaluation tasks, such as translation quality evaluation (TQE), especially when\nthe data samples (observations) are very scarce. In this work, we first\nintroduce the study on how to estimate the confidence interval for the\nmeasurement value when only one data (evaluation) point is available. Then,\nthis leads to our example with two human-generated observational scores, for\nwhich, we introduce ``Student's \\textit{t}-Distribution'' method and explain\nhow to use it to measure the IRR score using only these two data points, as\nwell as the confidence intervals (CIs) of the quality evaluation. We give\nquantitative analysis on how the evaluation confidence can be greatly improved\nby introducing more observations, even if only one extra observation. We\nencourage researchers to report their IRR scores in all possible means, e.g.\nusing Student's \\textit{t}-Distribution method whenever possible; thus making\nthe NLP evaluation more meaningful, transparent, and trustworthy. This\n\\textit{t}-Distribution method can be also used outside of NLP fields to\nmeasure IRR level for trustworthy evaluation of experimental investigations,\nwhenever the observational data is scarce.\n</p>\n<p>Keywords: Inter-Rater Reliability (IRR); Scarce Observations; Confidence\nIntervals (CIs); Natural Language Processing (NLP); Translation Quality\nEvaluation (TQE); Student's \\textit{t}-Distribution\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gladkoff_S/0/1/0/all/0/1\">Serge Gladkoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1\">Goran Nenadic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Creativity of Large Language Models. (arXiv:2304.00008v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2304.00008","description":"<p>Large Language Models (LLMs) are revolutionizing several areas of Artificial\nIntelligence. One of the most remarkable applications is creative writing,\ne.g., poetry or storytelling: the generated outputs are often of astonishing\nquality. However, a natural question arises: can LLMs be really considered\ncreative? In this article we firstly analyze the development of LLMs under the\nlens of creativity theories, investigating the key open questions and\nchallenges. In particular, we focus our discussion around the dimensions of\nvalue, novelty and surprise as proposed by Margaret Boden in her work. Then, we\nconsider different classic perspectives, namely product, process, press and\nperson. We discuss a set of ``easy'' and ``hard'' problems in machine\ncreativity, presenting them in relation to LLMs. Finally, we examine the\nsocietal impact of these technologies with a particular focus on the creative\nindustries, analyzing the opportunities offered by them, the challenges arising\nby them and the potential associated risks, from both legal and ethical points\nof view.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Franceschelli_G/0/1/0/all/0/1\">Giorgio Franceschelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musolesi_M/0/1/0/all/0/1\">Mirco Musolesi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers. (arXiv:2304.00215v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.00215","description":"<p>Relation prediction on knowledge graphs (KGs) is a key research topic.\nDominant embedding-based methods mainly focus on the transductive setting and\nlack the inductive ability to generalize to new entities for inference.\nExisting methods for inductive reasoning mostly mine the connections between\nentities, i.e., relational paths, without considering the nature of head and\ntail entities contained in the relational context. This paper proposes a novel\nmethod that captures both connections between entities and the intrinsic nature\nof entities, by simultaneously aggregating RElational Paths and cOntext with a\nunified hieRarchical Transformer framework, namely REPORT. REPORT relies solely\non relation semantics and can naturally generalize to the fully-inductive\nsetting, where KGs for training and inference have no common entities. In the\nexperiments, REPORT performs consistently better than all baselines on almost\nall the eight version subsets of two fully-inductive datasets. Moreover. REPORT\nis interpretable by providing each element's contribution to the prediction\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhendong Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v4 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2305.06569","description":"<p>Recommendation foundation model utilizes large language models (LLM) for\nrecommendation by converting recommendation tasks into natural language tasks.\nIt enables generative recommendation which directly generates the item(s) to\nrecommend rather than calculating a ranking score for each and every candidate\nitem in traditional recommendation models, simplifying the recommendation\npipeline from multi-stage filtering to single-stage filtering. To avoid\ngenerating excessively long text when deciding which item(s) to recommend,\ncreating LLM-compatible item IDs is essential for recommendation foundation\nmodels. In this study, we systematically examine the item indexing problem for\nrecommendation foundation models, using P5 as the representative backbone model\nand replicating its results with various indexing methods. To emphasize the\nimportance of item indexing, we first discuss the issues of several trivial\nitem indexing methods, such as independent indexing, title indexing, and random\nindexing. We then propose four simple yet effective solutions, including\nsequential indexing, collaborative indexing, semantic (content-based) indexing,\nand hybrid indexing. Our reproducibility study of P5 highlights the significant\ninfluence of item indexing methods on the model performance, and our results on\nreal-world datasets validate the effectiveness of our proposed solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wenyue Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Giant Language Brains Just Aren't Enough! Domain Pizzazz with Knowledge Sparkle Dust. (arXiv:2305.07230v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.07230","description":"<p>Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing, with GPT models at the forefront. While their remarkable\nperformance spans a range of tasks, adapting LLMs for real-world business\nscenarios still poses challenges warranting further investigation. This paper\npresents an empirical analysis aimed at bridging the gap in adapting LLMs to\npractical use cases. To do that, we select the question answering (QA) task of\ninsurance as a case study due to its challenge of reasoning. Based on the task\nwe design a new model relied on LLMs which are empowered by additional\nknowledge extracted from insurance policy rulebooks and DBpedia. The additional\nknowledge helps LLMs to understand new concepts of insurance for domain\nadaptation. Preliminary results on two QA datasets show that knowledge\nenhancement significantly improves the reasoning ability of GPT-3.5 (55.80% and\n57.83% in terms of accuracy). The analysis also indicates that existing public\nknowledge bases, e.g., DBPedia is beneficial for knowledge enhancement. Our\nfindings reveal that the inherent complexity of business scenarios often\nnecessitates the incorporation of domain-specific knowledge and external\nresources for effective problem-solving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh-Tien Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duy-Hung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabahi_S/0/1/0/all/0/1\">Shahab Sabahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jeff Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hotta_H/0/1/0/all/0/1\">Hajime Hotta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Textless Spoken Language Understanding with Discrete Units as Intermediate Target. (arXiv:2305.18096v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18096","description":"<p>Spoken Language Understanding (SLU) is a task that aims to extract semantic\ninformation from spoken utterances. Previous research has made progress in\nend-to-end SLU by using paired speech-text data, such as pre-trained Automatic\nSpeech Recognition (ASR) models or paired text as intermediate targets.\nHowever, acquiring paired transcripts is expensive and impractical for\nunwritten languages. On the other hand, Textless SLU extracts semantic\ninformation from speech without utilizing paired transcripts. However, the\nabsence of intermediate targets and training guidance for textless SLU often\nresults in suboptimal performance. In this work, inspired by the\ncontent-disentangled discrete units from self-supervised speech models, we\nproposed to use discrete units as intermediate guidance to improve textless SLU\nperformance. Our method surpasses the baseline method on five SLU benchmark\ncorpora. Additionally, we find that unit guidance facilitates few-shot learning\nand enhances the model's ability to handle noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guan-Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guan-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntax and Semantics Meet in the \"Middle\": Probing the Syntax-Semantics Interface of LMs Through Agentivity. (arXiv:2305.18185v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18185","description":"<p>Recent advances in large language models have prompted researchers to examine\ntheir abilities across a variety of linguistic tasks, but little has been done\nto investigate how models handle the interactions in meaning across words and\nlarger syntactic forms -- i.e. phenomena at the intersection of syntax and\nsemantics. We present the semantic notion of agentivity as a case study for\nprobing such interactions. We created a novel evaluation dataset by utilitizing\nthe unique linguistic properties of a subset of optionally transitive English\nverbs. This dataset was used to prompt varying sizes of three model classes to\nsee if they are sensitive to agentivity at the lexical level, and if they can\nappropriately employ these word-level priors given a specific syntactic\ncontext. Overall, GPT-3 text-davinci-003 performs extremely well across all\nexperiments, outperforming all other models tested by far. In fact, the results\nare even better correlated with human judgements than both syntactic and\nsemantic corpus statistics. This suggests that LMs may potentially serve as\nmore useful tools for linguistic annotation, theory testing, and discovery than\nselect corpora for certain tasks. Code is available at\nhttps://github.com/lindiatjuatja/lm_sem\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tjuatja_L/0/1/0/all/0/1\">Lindia Tjuatja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1\">Emmy Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levin_L/0/1/0/all/0/1\">Lori Levin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conformal Prediction with Large Language Models for Multi-Choice Question Answering. (arXiv:2305.18404v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18404","description":"<p>As large language models continue to be widely developed, robust uncertainty\nquantification techniques will become crucial for their safe deployment in\nhigh-stakes scenarios. In this work, we explore how conformal prediction can be\nused to provide uncertainty quantification in language models for the specific\ntask of multiple-choice question-answering. We find that the uncertainty\nestimates from conformal prediction are tightly correlated with prediction\naccuracy. This observation can be useful for downstream applications such as\nselective classification and filtering out low-quality predictions. We also\ninvestigate the exchangeability assumption required by conformal prediction to\nout-of-subject questions, which may be a more realistic scenario for many\npractical applications. Our work contributes towards more trustworthy and\nreliable usage of large language models in safety-critical situations, where\nrobust guarantees of error rate are required.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_B/0/1/0/all/0/1\">Bhawesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Charlie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1\">Gauri Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palepu_A/0/1/0/all/0/1\">Anil Palepu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellamy_D/0/1/0/all/0/1\">David Bellamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raskar_R/0/1/0/all/0/1\">Ramesh Raskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beam_A/0/1/0/all/0/1\">Andrew Beam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models, Natural Language Processing, Domain Specialization. (arXiv:2305.18703v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18703","description":"<p>Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing (NLP), providing a highly useful, task-agnostic foundation\nfor a wide range of applications. However, directly applying LLMs to solve\nsophisticated problems in specific domains meets many hurdles, caused by the\nheterogeneity of domain data, the sophistication of domain knowledge, the\nuniqueness of domain objectives, and the diversity of the constraints (e.g.,\nvarious social norms, cultural conformity, religious beliefs, and ethical\nstandards in the domain applications). Domain specification techniques are key\nto make large language models disruptive in many applications. Specifically, to\nsolve these hurdles, there has been a notable increase in research and\npractices conducted in recent years on the domain specialization of LLMs. This\nemerging field of study, with its substantial potential for impact,\nnecessitates a comprehensive and systematic review to better summarize and\nguide ongoing work in this area. In this article, we present a comprehensive\nsurvey on domain specification techniques for large language models, an\nemerging direction critical for large language model applications. First, we\npropose a systematic taxonomy that categorizes the LLM domain-specialization\ntechniques based on the accessibility to LLMs and summarizes the framework for\nall the subcategories as well as their relations and differences to each other.\nSecond, we present an extensive taxonomy of critical application domains that\ncan benefit dramatically from specialized LLMs, discussing their practical\nsignificance and open challenges. Last, we offer our insights into the current\nresearch status and future trends in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1\">Chen Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xujiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiaying Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chengyuan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Can Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_T/0/1/0/all/0/1\">Tanmoy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianjiao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panalkar_A/0/1/0/all/0/1\">Amit Panalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengzhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1\">Chris White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantization-Aware and Tensor-Compressed Training of Transformers for Natural Language Understanding. (arXiv:2306.01076v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.01076","description":"<p>Fine-tuned transformer models have shown superior performances in many\nnatural language tasks. However, the large model size prohibits deploying\nhigh-performance transformer models on resource-constrained devices. This paper\nproposes a quantization-aware tensor-compressed training approach to reduce the\nmodel size, arithmetic operations, and ultimately runtime latency of\ntransformer-based models. We compress the embedding and linear layers of\ntransformers into small low-rank tensor cores, which significantly reduces\nmodel parameters. A quantization-aware training with learnable scale factors is\nused to further obtain low-precision representations of the tensor-compressed\nmodels. The developed approach can be used for both end-to-end training and\ndistillation-based training. To improve the convergence, a layer-by-layer\ndistillation is applied to distill a quantized and tensor-compressed student\nmodel from a pre-trained transformer. The performance is demonstrated in two\nnatural language understanding tasks, showing up to $63\\times$ compression\nratio, little accuracy loss and remarkable inference and training speedup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1\">Samridhi Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunzmann_S/0/1/0/all/0/1\">Siegfried Kunzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Agnostic Structured Pruning of Speech Representation Models. (arXiv:2306.01385v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2306.01385","description":"<p>Self-supervised pre-trained models such as Wav2vec2, Hubert, and WavLM have\nbeen shown to significantly improve many speech tasks. However, their large\nmemory and strong computational requirements hinder their industrial\napplicability. Structured pruning is a hardware-friendly model compression\ntechnique but usually results in a larger loss of accuracy. In this paper, we\npropose a fine-grained attention head pruning method to compensate for the\nperformance degradation. In addition, we also introduce the straight through\nestimator into the L0 regularization to further accelerate the pruned model.\nExperiments on the SUPERB benchmark show that our model can achieve comparable\nperformance to the dense model in multiple tasks and outperforms the Wav2vec\n2.0 base model on average, with 72% fewer parameters and 2 times faster\ninference speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Siyuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Qiang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suo_H/0/1/0/all/0/1\">Hongbin Suo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_Y/0/1/0/all/0/1\">Yulong Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations. (arXiv:2306.01505v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.01505","description":"<p>Extracting generalized and robust representations is a major challenge in\nemotion recognition in conversations (ERC). To address this, we propose a\nsupervised adversarial contrastive learning (SACL) framework for learning\nclass-spread structured representations in a supervised manner. SACL applies\ncontrast-aware adversarial training to generate worst-case samples and uses\njoint class-spread contrastive learning to extract structured representations.\nIt can effectively utilize label-level feature consistency and retain\nfine-grained intra-class features. To avoid the negative impact of adversarial\nperturbations on context-dependent data, we design a contextual adversarial\ntraining (CAT) strategy to learn more diverse features from context and enhance\nthe model's context robustness. Under the framework with CAT, we develop a\nsequence-based SACL-LSTM to learn label-consistent and context-robust features\nfor ERC. Experiments on three datasets show that SACL-LSTM achieves\nstate-of-the-art performance on ERC. Extended experiments prove the\neffectiveness of SACL and CAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dou Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yinan Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Lingwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songlin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation. (arXiv:2306.01657v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.01657","description":"<p>Empathy is a crucial factor in open-domain conversations, which naturally\nshows one's caring and understanding to others. Though several methods have\nbeen proposed to generate empathetic responses, existing works often lead to\nmonotonous empathy that refers to generic and safe expressions. In this paper,\nwe propose to use explicit control to guide the empathy expression and design a\nframework DiffusEmp based on conditional diffusion language model to unify the\nutilization of dialogue context and attribute-oriented control signals.\nSpecifically, communication mechanism, intent, and semantic frame are imported\nas multi-grained signals that control the empathy realization from coarse to\nfine levels. We then design a specific masking strategy to reflect the\nrelationship between multi-grained signals and response tokens, and integrate\nit into the diffusion model to influence the generative process. Experimental\nresults on a benchmark dataset EmpatheticDialogue show that our framework\noutperforms competitive baselines in terms of controllability, informativeness,\nand diversity without the loss of context-relatedness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bi_G/0/1/0/all/0/1\">Guanqun Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Meng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuqiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Cross-Linguistic Pressure for Uniform Information Density in Word Order. (arXiv:2306.03734v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.03734","description":"<p>While natural languages differ widely in both canonical word order and word\norder flexibility, their word orders still follow shared cross-linguistic\nstatistical patterns, often attributed to functional pressures. In the effort\nto identify these pressures, prior work has compared real and counterfactual\nword orders. Yet one functional pressure has been overlooked in such\ninvestigations: the uniform information density (UID) hypothesis, which holds\nthat information should be spread evenly throughout an utterance. Here, we ask\nwhether a pressure for UID may have influenced word order patterns\ncross-linguistically. To this end, we use computational models to test whether\nreal orders lead to greater information uniformity than counterfactual orders.\nIn our empirical study of 10 typologically diverse languages, we find that: (i)\namong SVO languages, real word orders consistently have greater uniformity than\nreverse word orders, and (ii) only linguistically implausible counterfactual\norders consistently exceed the uniformity of real orders. These findings are\ncompatible with a pressure for information uniformity in the development and\nusage of natural languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clark_T/0/1/0/all/0/1\">Thomas Hikaru Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahn_M/0/1/0/all/0/1\">Michael Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Futrell_R/0/1/0/all/0/1\">Richard Futrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition. (arXiv:2306.07848v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.07848","description":"<p>Contrastive learning based pretraining methods have recently exhibited\nimpressive success in diverse fields. In this paper, we propose GEmo-CLAP, a\nkind of efficient gender-attribute-enhanced contrastive language-audio\npretraining (CLAP) model for speech emotion recognition. To be specific, we\nfirst build an effective emotion CLAP model Emo-CLAP for emotion recognition,\nutilizing various self-supervised learning based pre-trained models. Then,\nconsidering the importance of the gender attribute in speech emotion modeling,\ntwo GEmo-CLAP approaches are further proposed to integrate the emotion and\ngender information of speech signals, forming more reasonable objectives.\nExtensive experiments on the IEMOCAP corpus demonstrate that our proposed two\nGEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with\ndifferent pre-trained models, while also achieving superior recognition\nperformance compared with other state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yanni Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuguang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jixun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_W/0/1/0/all/0/1\">Wen Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Heng Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can ChatGPT pass the Vietnamese National High School Graduation Examination?. (arXiv:2306.09170v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.09170","description":"<p>This research article highlights the potential of AI-powered chatbots in\neducation and presents the results of using ChatGPT, a large language model, to\ncomplete the Vietnamese National High School Graduation Examination (VNHSGE).\nThe study dataset included 30 essays in the literature test case and 1,700\nmultiple-choice questions designed for other subjects. The results showed that\nChatGPT was able to pass the examination with an average score of 6-7,\ndemonstrating the technology's potential to revolutionize the educational\nlandscape. The analysis of ChatGPT performance revealed its proficiency in a\nrange of subjects, including mathematics, English, physics, chemistry, biology,\nhistory, geography, civic education, and literature, which suggests its\npotential to provide effective support for learners. However, further research\nis needed to assess ChatGPT performance on more complex exam questions and its\npotential to support learners in different contexts. As technology continues to\nevolve and improve, we can expect to see the use of AI tools like ChatGPT\nbecome increasingly common in educational settings, ultimately enhancing the\neducational experience for both students and educators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dao_X/0/1/0/all/0/1\">Xuan-Quy Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngoc-Bich Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_X/0/1/0/all/0/1\">Xuan-Dung Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_B/0/1/0/all/0/1\">Bac-Bien Ngo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Modular Activation for Efficient Sequence Modeling. (arXiv:2306.11197v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2306.11197","description":"<p>Linear State Space Models (SSMs) have demonstrated strong performance in a\nvariety of sequence modeling tasks due to their efficient encoding of the\nrecurrent structure. However, in more comprehensive tasks like language\nmodeling and machine translation, self-attention-based models still outperform\nSSMs. Hybrid models employing both SSM and self-attention generally show\npromising performance, but current approaches apply attention modules\nstatically and uniformly to all elements in the input sequences, leading to\nsub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse\nModular Activation (SMA), a general mechanism enabling neural networks to\nsparsely and dynamically activate sub-modules for sequence elements in a\ndifferentiable manner. Through allowing each element to skip non-activated\nsub-modules, SMA reduces computation and memory consumption at both training\nand inference stages of sequence modeling. As a specific instantiation of SMA,\nwe design a novel neural architecture, SeqBoat, which employs SMA to sparsely\nactivate a Gated Attention Unit (GAU) based on the state representations\nlearned from an SSM. By constraining the GAU to only conduct local attention on\nthe activated inputs, SeqBoat can achieve linear inference complexity with\ntheoretically infinite attention span, and provide substantially better\nquality-efficiency trade-off than the chunking-based models. With experiments\non a wide range of tasks, including language modeling, speech classification\nand long-range arena, SeqBoat brings new state-of-the-art results among hybrid\nmodels with linear complexity and reveals the amount of attention needed for\neach task through the learned sparse activation patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Liliang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Testing of Detection Tools for AI-Generated Text. (arXiv:2306.15666v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.15666","description":"<p>Recent advances in generative pre-trained transformer large language models\nhave emphasised the potential risks of unfair use of artificial intelligence\n(AI) generated content in an academic environment and intensified efforts in\nsearching for solutions to detect such content. The paper examines the general\nfunctionality of detection tools for artificial intelligence generated text and\nevaluates them based on accuracy and error type analysis. Specifically, the\nstudy seeks to answer research questions about whether existing detection tools\ncan reliably differentiate between human-written text and ChatGPT-generated\ntext, and whether machine translation and content obfuscation techniques affect\nthe detection of AI-generated text. The research covers 12 publicly available\ntools and two commercial systems (Turnitin and PlagiarismCheck) that are widely\nused in the academic setting. The researchers conclude that the available\ndetection tools are neither accurate nor reliable and have a main bias towards\nclassifying the output as human-written rather than detecting AI-generated\ntext. Furthermore, content obfuscation techniques significantly worsen the\nperformance of tools. The study makes several significant contributions. First,\nit summarises up-to-date similar scientific and non-scientific efforts in the\nfield. Second, it presents the result of one of the most comprehensive tests\nconducted so far, based on a rigorous research methodology, an original\ndocument set, and a broad coverage of tools. Third, it discusses the\nimplications and drawbacks of using detection tools for AI-generated text in\nacademic settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weber_Wulff_D/0/1/0/all/0/1\">Debora Weber-Wulff</a> (University of Applied Sciences HTW Berlin, Germany), <a href=\"http://arxiv.org/find/cs/1/au:+Anohina_Naumeca_A/0/1/0/all/0/1\">Alla Anohina-Naumeca</a> (Riga Technical University, Latvia), <a href=\"http://arxiv.org/find/cs/1/au:+Bjelobaba_S/0/1/0/all/0/1\">Sonja Bjelobaba</a> (Uppsala University, Sweden), <a href=\"http://arxiv.org/find/cs/1/au:+Foltynek_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Folt&#xfd;nek</a> (Masaryk University, Czechia), <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_Dib_J/0/1/0/all/0/1\">Jean Guerrero-Dib</a> (Universidad de Monterrey, Mexico), <a href=\"http://arxiv.org/find/cs/1/au:+Popoola_O/0/1/0/all/0/1\">Olumide Popoola</a> (Queen Mary University of London, UK), <a href=\"http://arxiv.org/find/cs/1/au:+Sigut_P/0/1/0/all/0/1\">Petr &#x160;igut</a> (Masaryk University, Czechia), <a href=\"http://arxiv.org/find/cs/1/au:+Waddington_L/0/1/0/all/0/1\">Lorna Waddington</a> (University of Leeds, UK)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis. (arXiv:2306.17181v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.17181","description":"<p>Generative Adversarial Networks (GAN) is a model for data synthesis, which\ncreates plausible data through the competition of generator and discriminator.\nAlthough GAN application to image synthesis is extensively studied, it has\ninherent limitations to natural language generation. Because natural language\nis composed of discrete tokens, a generator has difficulty updating its\ngradient through backpropagation; therefore, most text-GAN studies generate\nsentences starting with a random token based on a reward system. Thus, the\ngenerators of previous studies are pre-trained in an autoregressive way before\nadversarial training, causing data memorization that synthesized sentences\nreproduce the training data. In this paper, we synthesize sentences using a\nframework similar to the original GAN. More specifically, we propose Text\nEmbedding Space Generative Adversarial Networks (TESGAN) which generate\ncontinuous text embedding spaces instead of discrete tokens to solve the\ngradient backpropagation problem. Furthermore, TESGAN conducts unsupervised\nlearning which does not directly refer to the text of the training data to\novercome the data memorization issue. By adopting this novel method, TESGAN can\nsynthesize new sentences, showing the potential of unsupervised learning for\ntext synthesis. We expect to see extended research combining Large Language\nModels with a new perspective of viewing text as an continuous space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jun-Min Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_T/0/1/0/all/0/1\">Tae-Bin Ha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biomedical Language Models are Robust to Sub-optimal Tokenization. (arXiv:2306.17649v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.17649","description":"<p>As opposed to general English, many concepts in biomedical terminology have\nbeen designed in recent history by biomedical professionals with the goal of\nbeing precise and concise. This is often achieved by concatenating meaningful\nbiomedical morphemes to create new semantic units. Nevertheless, most modern\nbiomedical language models (LMs) are pre-trained using standard domain-specific\ntokenizers derived from large scale biomedical corpus statistics without\nexplicitly leveraging the agglutinating nature of biomedical language. In this\nwork, we first find that standard open-domain and biomedical tokenizers are\nlargely unable to segment biomedical terms into meaningful components.\nTherefore, we hypothesize that using a tokenizer which segments biomedical\nterminology more accurately would enable biomedical LMs to improve their\nperformance on downstream biomedical NLP tasks, especially ones which involve\nbiomedical terms directly such as named entity recognition (NER) and entity\nlinking. Surprisingly, we find that pre-training a biomedical LM using a more\naccurate biomedical tokenizer does not improve the entity representation\nquality of a language model as measured by several intrinsic and extrinsic\nmeasures such as masked language modeling prediction (MLM) accuracy as well as\nNER and entity linking performance. These quantitative findings, along with a\ncase study which explores entity representation quality more directly, suggest\nthat the biomedical pre-training process is quite robust to instances of\nsub-optimal tokenization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_B/0/1/0/all/0/1\">Bernal Jim&#xe9;nez Guti&#xe9;rrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Language Models are not Multicultural: A Case Study in Emotion. (arXiv:2307.01370v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.01370","description":"<p>Emotions are experienced and expressed differently across the world. In order\nto use Large Language Models (LMs) for multilingual tasks that require\nemotional sensitivity, LMs must reflect this cultural variation in emotion. In\nthis study, we investigate whether the widely-used multilingual LMs in 2023\nreflect differences in emotional expressions across cultures and languages. We\nfind that embeddings obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric,\nand generative LMs (e.g., ChatGPT) reflect Western norms, even when responding\nto prompts in other languages. Our results show that multilingual LMs do not\nsuccessfully learn the culturally appropriate nuances of emotion and we\nhighlight possible research directions towards correcting this.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Havaldar_S/0/1/0/all/0/1\">Shreya Havaldar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rai_S/0/1/0/all/0/1\">Sunny Rai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_B/0/1/0/all/0/1\">Bhumika Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Langchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guntuku_S/0/1/0/all/0/1\">Sharath Chandra Guntuku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1\">Lyle Ungar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ValiTex -- a unified validation framework for computational text-based measures of social science constructs. (arXiv:2307.02863v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.02863","description":"<p>Guidance on how to validate computational text-based measures of social\nscience constructs is fragmented. Whereas scholars are generally acknowledging\nthe importance of validating their text-based measures, they often lack common\nterminology and a unified framework to do so. This paper introduces a new\nvalidation framework called ValiTex, designed to assist scholars to measure\nsocial science constructs based on textual data. The framework draws on a\nlong-established tradition within psychometrics while extending the framework\nfor the purpose of computational text analysis. ValiTex consists of two\ncomponents, a conceptual model, and a dynamic checklist. Whereas the conceptual\nmodel provides a general structure along distinct phases on how to approach\nvalidation, the dynamic checklist defines specific validation steps and\nprovides guidance on which steps might be considered recommendable (i.e.,\nproviding relevant and necessary validation evidence) or optional (i.e., useful\nfor providing additional supporting validation evidence. The utility of the\nframework is demonstrated by applying it to a use case of detecting sexism from\nsocial media data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Birkenmaier_L/0/1/0/all/0/1\">Lukas Birkenmaier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lechner_C/0/1/0/all/0/1\">Clemens Lechner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_C/0/1/0/all/0/1\">Claudia Wagner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BLEURT Has Universal Translations: An Analysis of Automatic Metrics by Minimum Risk Training. (arXiv:2307.03131v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.03131","description":"<p>Automatic metrics play a crucial role in machine translation. Despite the\nwidespread use of n-gram-based metrics, there has been a recent surge in the\ndevelopment of pre-trained model-based metrics that focus on measuring sentence\nsemantics. However, these neural metrics, while achieving higher correlations\nwith human evaluations, are often considered to be black boxes with potential\nbiases that are difficult to detect. In this study, we systematically analyze\nand compare various mainstream and cutting-edge automatic metrics from the\nperspective of their guidance for training machine translation systems. Through\nMinimum Risk Training (MRT), we find that certain metrics exhibit robustness\ndefects, such as the presence of universal adversarial translations in BLEURT\nand BARTScore. In-depth analysis suggests two main causes of these robustness\ndeficits: distribution biases in the training datasets, and the tendency of the\nmetric paradigm. By incorporating token-level constraints, we enhance the\nrobustness of evaluation metrics, which in turn leads to an improvement in the\nperformance of machine translation systems. Codes are available at\n\\url{https://github.com/powerpuffpomelo/fairseq_mrt}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yiming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chengqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Testing the Predictions of Surprisal Theory in 11 Languages. (arXiv:2307.03667v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.03667","description":"<p>A fundamental result in psycholinguistics is that less predictable words take\na longer time to process. One theoretical explanation for this finding is\nSurprisal Theory (Hale, 2001; Levy, 2008), which quantifies a word's\npredictability as its surprisal, i.e. its negative log-probability given a\ncontext. While evidence supporting the predictions of Surprisal Theory have\nbeen replicated widely, most have focused on a very narrow slice of data:\nnative English speakers reading English texts. Indeed, no comprehensive\nmultilingual analysis exists. We address this gap in the current literature by\ninvestigating the relationship between surprisal and reading times in eleven\ndifferent languages, distributed across five language families. Deriving\nestimates from language models trained on monolingual and multilingual corpora,\nwe test three predictions associated with surprisal theory: (i) whether\nsurprisal is predictive of reading times; (ii) whether expected surprisal, i.e.\ncontextual entropy, is predictive of reading times; (iii) and whether the\nlinking function between surprisal and reading times is linear. We find that\nall three predictions are borne out crosslinguistically. By focusing on a more\ndiverse set of languages, we argue that these results offer the most robust\nlink to-date between information theory and incremental language processing\nacross languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilcox_E/0/1/0/all/0/1\">Ethan Gotlieb Wilcox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger P. Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GenRec: Large Language Model for Generative Recommendation. (arXiv:2307.00457v2 [cs.IR] CROSS LISTED)","link":"http://arxiv.org/abs/2307.00457","description":"<p>In recent years, large language models (LLM) have emerged as powerful tools\nfor diverse natural language processing tasks. However, their potential for\nrecommender systems under the generative recommendation paradigm remains\nrelatively unexplored. This paper presents an innovative approach to\nrecommendation systems using large language models (LLMs) based on text data.\nIn this paper, we present a novel LLM for generative recommendation (GenRec)\nthat utilized the expressive power of LLM to directly generate the target item\nto recommend, rather than calculating ranking score for each candidate item one\nby one as in traditional discriminative recommendation. GenRec uses LLM's\nunderstanding ability to interpret context, learn user preferences, and\ngenerate relevant recommendation. Our proposed approach leverages the vast\nknowledge encoded in large language models to accomplish recommendation tasks.\nWe first we formulate specialized prompts to enhance the ability of LLM to\ncomprehend recommendation tasks. Subsequently, we use these prompts to\nfine-tune the LLaMA backbone LLM on a dataset of user-item interactions,\nrepresented by textual data, to capture user preferences and item\ncharacteristics. Our research underscores the potential of LLM-based generative\nrecommendation in revolutionizing the domain of recommendation systems and\noffers a foundational framework for future explorations in this field. We\nconduct extensive experiments on benchmark datasets, and the experiments shows\nthat our GenRec has significant better results on large dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jianchao Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zelong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wenyue Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Juntao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"vONTSS: vMF based semi-supervised neural topic modeling with optimal transport. (arXiv:2307.01226v1 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2307.01226","description":"<p>Recently, Neural Topic Models (NTM), inspired by variational autoencoders,\nhave attracted a lot of research interest; however, these methods have limited\napplications in the real world due to the challenge of incorporating human\nknowledge. This work presents a semi-supervised neural topic modeling method,\nvONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and\noptimal transport. When a few keywords per topic are provided, vONTSS in the\nsemi-supervised setting generates potential topics and optimizes topic-keyword\nquality and topic classification. Experiments show that vONTSS outperforms\nexisting semi-supervised topic modeling methods in classification accuracy and\ndiversity. vONTSS also supports unsupervised topic modeling. Quantitative and\nqualitative experiments show that vONTSS in the unsupervised setting\noutperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered\nand coherent topics on benchmark datasets. It is also much faster than the\nstate-of-the-art weakly supervised text classification method while achieving\nsimilar classification performance. We further prove the equivalence of optimal\ntransport loss and cross-entropy loss at the global minimum.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weijie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengamedu_S/0/1/0/all/0/1\">Srinivasan H. Sengamedu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iannacci_F/0/1/0/all/0/1\">Francis Iannacci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinjin Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizing Backpropagation for Gradient-Based Interpretability. (arXiv:2307.03056v1 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2307.03056","description":"<p>Many popular feature-attribution methods for interpreting deep neural\nnetworks rely on computing the gradients of a model's output with respect to\nits inputs. While these methods can indicate which input features may be\nimportant for the model's prediction, they reveal little about the inner\nworkings of the model itself. In this paper, we observe that the gradient\ncomputation of a model is a special case of a more general formulation using\nsemirings. This observation allows us to generalize the backpropagation\nalgorithm to efficiently compute other interpretable statistics about the\ngradient graph of a neural network, such as the highest-weighted path and\nentropy. We implement this generalized algorithm, evaluate it on synthetic\ndatasets to better understand the statistics it computes, and apply it to study\nBERT's behavior on the subject-verb number agreement task (SVA). With this\nmethod, we (a) validate that the amount of gradient flow through a component of\na model reflects its importance to a prediction and (b) for SVA, identify which\npathways of the self-attention mechanism are most important.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_K/0/1/0/all/0/1\">Kevin Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennigen_L/0/1/0/all/0/1\">Lucas Torroba Hennigen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoehr_N/0/1/0/all/0/1\">Niklas Stoehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warstadt_A/0/1/0/all/0/1\">Alexander Warstadt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-07-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}