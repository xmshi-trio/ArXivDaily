{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-08-02T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"An Overview Of Temporal Commonsense Reasoning and Acquisition. (arXiv:2308.00002v1 [cs.AI])","link":"http://arxiv.org/abs/2308.00002","description":"<p>Temporal commonsense reasoning refers to the ability to understand the\ntypical temporal context of phrases, actions, and events, and use it to reason\nover problems requiring such knowledge. This trait is essential in temporal\nnatural language processing tasks, with possible applications such as timeline\nsummarization, temporal question answering, and temporal natural language\ninference. Recent research on the performance of large language models suggests\nthat, although they are adept at generating syntactically correct sentences and\nsolving classification tasks, they often take shortcuts in their reasoning and\nfall prey to simple linguistic traps. This article provides an overview of\nresearch in the domain of temporal commonsense reasoning, particularly focusing\non enhancing language model performance through a variety of augmentations and\ntheir evaluation across a growing number of datasets. However, these augmented\nmodels still struggle to approach human performance on reasoning tasks over\ntemporal common sense properties, such as the typical occurrence times,\norderings, or durations of events. We further emphasize the need for careful\ninterpretation of research to guard against overpromising evaluation results in\nlight of the shallow reasoning present in transformers. This can be achieved by\nappropriately preparing datasets and suitable evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wenzel_G/0/1/0/all/0/1\">Georg Wenzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A new mapping of technological interdependence. (arXiv:2308.00014v1 [econ.EM])","link":"http://arxiv.org/abs/2308.00014","description":"<p>Which technological linkages affect the sector's ability to innovate? How do\nthese effects transmit through the technology space? This paper answers these\ntwo key questions using novel methods of text mining and network analysis. We\nexamine technological interdependence across sectors over a period of half a\ncentury (from 1976 to 2021) by analyzing the text of 6.5 million patents\ngranted by the United States Patent and Trademark Office (USPTO), and applying\nnetwork analysis to uncover the full spectrum of linkages existing across\ntechnology areas. We demonstrate that patent text contains a wealth of\ninformation often not captured by traditional innovation metrics, such as\npatent citations. By using network analysis, we document that indirect linkages\nare as important as direct connections and that the former would remain mostly\nhidden using more traditional measures of indirect linkages, such as the\nLeontief inverse matrix. Finally, based on an impulse-response analysis, we\nillustrate how technological shocks transmit through the technology\n(network-based) space, affecting the innovation capacity of the sectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/econ/1/au:+Colladon_A/0/1/0/all/0/1\">A. Fronzetti Colladon</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Guardabascio_B/0/1/0/all/0/1\">B. Guardabascio</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Venturini_F/0/1/0/all/0/1\">F. Venturini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment. (arXiv:2308.00016v1 [q-fin.CP])","link":"http://arxiv.org/abs/2308.00016","description":"<p>One of the most important tasks in quantitative investment research is mining\nnew alphas (effective trading signals or factors). Traditional alpha mining\nmethods, either hand-crafted factor synthesizing or algorithmic factor mining\n(e.g., search with genetic programming), have inherent limitations, especially\nin implementing the ideas of quants. In this work, we propose a new alpha\nmining paradigm by introducing human-AI interaction, and a novel prompt\nengineering algorithmic framework to implement this paradigm by leveraging the\npower of large language models. Moreover, we develop Alpha-GPT, a new\ninteractive alpha mining system framework that provides a heuristic way to\n``understand'' the ideas of quant researchers and outputs creative, insightful,\nand effective alphas. We demonstrate the effectiveness and advantage of\nAlpha-GPT via a number of alpha mining experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Wang_S/0/1/0/all/0/1\">Saizhuo Wang</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Yuan_H/0/1/0/all/0/1\">Hang Yuan</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zhou_L/0/1/0/all/0/1\">Leon Zhou</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Ni_L/0/1/0/all/0/1\">Lionel M. Ni</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Shum_H/0/1/0/all/0/1\">Heung-Yeung Shum</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Guo_J/0/1/0/all/0/1\">Jian Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models. (arXiv:2308.00065v1 [q-fin.RM])","link":"http://arxiv.org/abs/2308.00065","description":"<p>Financial risk prediction plays a crucial role in the financial sector.\nMachine learning methods have been widely applied for automatically detecting\npotential risks and thus saving the cost of labor. However, the development in\nthis field is lagging behind in recent years by the following two facts: 1) the\nalgorithms used are somewhat outdated, especially in the context of the fast\nadvance of generative AI and large language models (LLMs); 2) the lack of a\nunified and open-sourced financial benchmark has impeded the related research\nfor years. To tackle these issues, we propose FinPT and FinBench: the former is\na novel approach for financial risk prediction that conduct Profile Tuning on\nlarge pretrained foundation models, and the latter is a set of high-quality\ndatasets on financial risks such as default, fraud, and churn. In FinPT, we\nfill the financial tabular data into the pre-defined instruction template,\nobtain natural-language customer profiles by prompting LLMs, and fine-tune\nlarge foundation models with the profile text to make predictions. We\ndemonstrate the effectiveness of the proposed FinPT by experimenting with a\nrange of representative strong baselines on FinBench. The analytical studies\nfurther deepen the understanding of LLMs for financial risk prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Yin_Y/0/1/0/all/0/1\">Yuwei Yin</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Yang_Y/0/1/0/all/0/1\">Yazheng Yang</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Stereotype Identification through Reasoning. (arXiv:2308.00071v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00071","description":"<p>Given that language models are trained on vast datasets that may contain\ninherent biases, there is a potential danger of inadvertently perpetuating\nsystemic discrimination. Consequently, it becomes essential to examine and\naddress biases in language models, integrating fairness into their development\nto ensure these models are equitable and free from bias. In this work, we\ndemonstrate the importance of reasoning in zero-shot stereotype identification\nbased on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from\n13B to 33B, we show that the performance gain from reasoning significantly\nexceeds the gain from scaling up. Our findings suggest that reasoning could be\na key factor that enables LLMs to trescend the scaling law on out-of-domain\ntasks such as stereotype identification. Additionally, through a qualitative\nanalysis of select reasoning traces, we highlight how reasoning enhances not\njust accuracy but also the interpretability of the decision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jacob-Junqi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dige_O/0/1/0/all/0/1\">Omkar Dige</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emerson_D/0/1/0/all/0/1\">David Emerson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khattak_F/0/1/0/all/0/1\">Faiza Khan Khattak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How User Language Affects Conflict Fatality Estimates in ChatGPT. (arXiv:2308.00072v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00072","description":"<p>OpenAI's ChatGPT language model has gained popularity as a powerful tool for\ncomplex problem-solving and information retrieval. However, concerns arise\nabout the reproduction of biases present in the language-specific training\ndata. In this study, we address this issue in the context of the\nIsraeli-Palestinian and Turkish-Kurdish conflicts. Using GPT-3.5, we employed\nan automated query procedure to inquire about casualties in specific\nairstrikes, in both Hebrew and Arabic for the former conflict and Turkish and\nKurdish for the latter. Our analysis reveals that GPT-3.5 provides 27$\\pm$11\npercent lower fatality estimates when queried in the language of the attacker\nthan in the language of the targeted group. Evasive answers denying the\nexistence of such attacks further increase the discrepancy, creating a novel\nbias mechanism not present in regular search engines. This language bias has\nthe potential to amplify existing media biases and contribute to information\nbubbles, ultimately reinforcing conflicts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kazenwadel_D/0/1/0/all/0/1\">Daniel Kazenwadel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinert_C/0/1/0/all/0/1\">Christoph V. Steinert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trustworthiness of Children Stories Generated by Large Language Models. (arXiv:2308.00073v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00073","description":"<p>Large Language Models (LLMs) have shown a tremendous capacity for generating\nliterary text. However, their effectiveness in generating children's stories\nhas yet to be thoroughly examined. In this study, we evaluate the\ntrustworthiness of children's stories generated by LLMs using various measures,\nand we compare and contrast our results with both old and new children's\nstories to better assess their significance. Our findings suggest that LLMs\nstill struggle to generate children's stories at the level of quality and\nnuance found in actual stories\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhandari_P/0/1/0/all/0/1\">Prabin Bhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brennan_H/0/1/0/all/0/1\">Hannah Marie Brennan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Semantically Enriched Embeddings for Knowledge Graph Completion. (arXiv:2308.00081v1 [cs.AI])","link":"http://arxiv.org/abs/2308.00081","description":"<p>Embedding based Knowledge Graph (KG) Completion has gained much attention\nover the past few years. Most of the current algorithms consider a KG as a\nmultidirectional labeled graph and lack the ability to capture the semantics\nunderlying the schematic information. In a separate development, a vast amount\nof information has been captured within the Large Language Models (LLMs) which\nhas revolutionized the field of Artificial Intelligence. KGs could benefit from\nthese LLMs and vice versa. This vision paper discusses the existing algorithms\nfor KG completion based on the variations for generating KG embeddings. It\nstarts with discussing various KG completion algorithms such as transductive\nand inductive link prediction and entity type prediction algorithms. It then\nmoves on to the algorithms utilizing type information within the KGs, LLMs, and\nfinally to algorithms capturing the semantics represented in different\ndescription logic axioms. We conclude the paper with a critical reflection on\nthe current state of work in the community and give recommendations for future\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mehwish Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harmelen_F/0/1/0/all/0/1\">Frank van Harmelen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acosta_M/0/1/0/all/0/1\">Maribel Acosta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation. (arXiv:2308.00085v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00085","description":"<p>Recent approaches to empathetic response generation try to incorporate\ncommonsense knowledge or reasoning about the causes of emotions to better\nunderstand the user's experiences and feelings. However, these approaches\nmainly focus on understanding the causalities of context from the user's\nperspective, ignoring the system's perspective. In this paper, we propose a\ncommonsense-based causality explanation approach for diverse empathetic\nresponse generation that considers both the user's perspective (user's desires\nand reactions) and the system's perspective (system's intentions and\nreactions). We enhance ChatGPT's ability to reason for the system's perspective\nby integrating in-context learning with commonsense knowledge. Then, we\nintegrate the commonsense-based causality explanation with both ChatGPT and a\nT5-based model. Experimental evaluations demonstrate that our method\noutperforms other comparable methods on both automatic and human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yahui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inoue_K/0/1/0/all/0/1\">Koji Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Chenhui Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_T/0/1/0/all/0/1\">Tatsuya Kawahara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Validation of a Zero-Shot Learning Natural Language Processing Tool for Data Abstraction from Unstructured Healthcare Data. (arXiv:2308.00107v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00107","description":"<p>Objectives: To describe the development and validation of a zero-shot\nlearning natural language processing (NLP) tool for abstracting data from\nunstructured text contained within PDF documents, such as those found within\nelectronic health records. Materials and Methods: A data abstraction tool based\non the GPT-3.5 model from OpenAI was developed and compared to three physician\nhuman abstractors in terms of time to task completion and accuracy for\nabstracting data on 14 unique variables from a set of 199 de-identified radical\nprostatectomy pathology reports. The reports were processed by the software\ntool in vectorized and scanned formats to establish the impact of optical\ncharacter recognition on data abstraction. The tool was assessed for\nsuperiority for data abstraction speed and non-inferiority for accuracy.\nResults: The human abstractors required a mean of 101s per report for data\nabstraction, with times varying from 15 to 284 s. In comparison, the software\ntool required a mean of 12.8 s to process the vectorized reports and a mean of\n15.8 to process the scanned reports (P &lt; 0.001). The overall accuracies of the\nthree human abstractors were 94.7%, 97.8%, and 96.4% for the combined set of\n2786 datapoints. The software tool had an overall accuracy of 94.2% for the\nvectorized reports, proving to be non-inferior to the human abstractors at a\nmargin of -10% ($\\alpha$=0.025). The tool had a slightly lower accuracy of\n88.7% using the scanned reports, proving to be non-inferiority to 2 out of 3\nhuman abstractors. Conclusion: The developed zero-shot learning NLP tool\naffords researchers comparable levels of accuracy to that of human abstractors,\nwith significant time savings benefits. Because of the lack of need for\ntask-specific model training, the developed tool is highly generalizable and\ncan be used for a wide variety of data abstraction tasks, even outside the\nfield of medicine.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaufmann_B/0/1/0/all/0/1\">Basil Kaufmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busby_D/0/1/0/all/0/1\">Dallin Busby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_C/0/1/0/all/0/1\">Chandan Krushna Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tillu_N/0/1/0/all/0/1\">Neeraja Tillu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_M/0/1/0/all/0/1\">Mani Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1\">Ashutosh K. Tewari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorin_M/0/1/0/all/0/1\">Michael A. Gorin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DPBERT: Efficient Inference for BERT based on Dynamic Planning. (arXiv:2308.00108v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00108","description":"<p>Large-scale pre-trained language models such as BERT have contributed\nsignificantly to the development of NLP. However, those models require large\ncomputational resources, making it difficult to be applied to mobile devices\nwhere computing power is limited. In this paper we aim to address the weakness\nof existing input-adaptive inference methods which fail to take full advantage\nof the structure of BERT. We propose Dynamic Planning in BERT, a novel\nfine-tuning strategy that can accelerate the inference process of BERT through\nselecting a subsequence of transformer layers list of backbone as a\ncomputational path for an input sample. To do this, our approach adds a\nplanning module to the original BERT model to determine whether a layer is\nincluded or bypassed during inference. Experimental results on the GLUE\nbenchmark exhibit that our method reduces latency to 75\\% while maintaining\n98\\% accuracy, yielding a better accuracy-speed trade-off compared to\nstate-of-the-art input-adaptive methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weixin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_H/0/1/0/all/0/1\">Hankz Hankui Zhuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Sentence is Worth a Thousand Pictures: Can Large Language Models Understand Human Language?. (arXiv:2308.00109v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00109","description":"<p>Artificial Intelligence applications show great potential for\nlanguage-related tasks that rely on next-word prediction. The current\ngeneration of large language models have been linked to claims about human-like\nlinguistic performance and their applications are hailed both as a key step\ntowards Artificial General Intelligence and as major advance in understanding\nthe cognitive, and even neural basis of human language. We analyze the\ncontribution of large language models as theoretically informative\nrepresentations of a target system vs. atheoretical powerful mechanistic tools,\nand we identify the key abilities that are still missing from the current state\nof development and exploitation of these models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marcus_G/0/1/0/all/0/1\">Gary Marcus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leivada_E/0/1/0/all/0/1\">Evelina Leivada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_E/0/1/0/all/0/1\">Elliot Murphy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Three Bricks to Consolidate Watermarks for Large Language Models. (arXiv:2308.00113v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00113","description":"<p>The task of discerning between generated and natural texts is increasingly\nchallenging. In this context, watermarking emerges as a promising technique for\nascribing generated text to a specific model. It alters the sampling generation\nprocess so as to leave an invisible trace in the generated output, facilitating\nlater detection. This research consolidates watermarks for large language\nmodels based on three theoretical and empirical considerations. First, we\nintroduce new statistical tests that offer robust theoretical guarantees which\nremain valid even at low false-positive rates (less than 10$^{\\text{-6}}$).\nSecond, we compare the effectiveness of watermarks using classical benchmarks\nin the field of natural language processing, gaining insights into their\nreal-world applicability. Third, we develop advanced detection schemes for\nscenarios where access to the LLM is available, as well as multi-bit\nwatermarking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_P/0/1/0/all/0/1\">Pierre Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaffin_A/0/1/0/all/0/1\">Antoine Chaffin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tit_K/0/1/0/all/0/1\">Karim Tit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chappelier_V/0/1/0/all/0/1\">Vivien Chappelier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furon_T/0/1/0/all/0/1\">Teddy Furon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Modular Ontology for MODS -- Metadata Object Description Schema. (arXiv:2308.00116v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00116","description":"<p>The Metadata Object Description Schema (MODS) was developed to describe\nbibliographic concepts and metadata and is maintained by the Library of\nCongress. Its authoritative version is given as an XML schema based on an XML\nmindset which means that it has significant limitations for use in a knowledge\ngraphs context. We have therefore developed the Modular MODS Ontology (MMODS-O)\nwhich incorporates all elements and attributes of the MODS XML schema. In\ndesigning the ontology, we adopt the recent Modular Ontology Design Methodology\n(MOMo) with the intention to strike a balance between modularity and quality\nontology design on the one hand, and conservative backward compatibility with\nMODS on the other.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rayan_R/0/1/0/all/0/1\">Rushrukh Rayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimizu_C/0/1/0/all/0/1\">Cogan Shimizu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sieverding_H/0/1/0/all/0/1\">Heidi Sieverding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hitzler_P/0/1/0/all/0/1\">Pascal Hitzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Getting pwn'd by AI: Penetration Testing with Large Language Models. (arXiv:2308.00121v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00121","description":"<p>The field of software security testing, more specifically penetration\ntesting, is an activity that requires high levels of expertise and involves\nmany manual testing and analysis steps. This paper explores the potential usage\nof large-language models, such as GPT3.5, to augment penetration testers with\nAI sparring partners. We explore the feasibility of supplementing penetration\ntesters with AI models for two distinct use cases: high-level task planning for\nsecurity testing assignments and low-level vulnerability hunting within a\nvulnerable virtual machine. For the latter, we implemented a closed-feedback\nloop between LLM-generated low-level actions with a vulnerable virtual machine\n(connected through SSH) and allowed the LLM to analyze the machine state for\nvulnerabilities and suggest concrete attack vectors which were automatically\nexecuted within the virtual machine. We discuss promising initial results,\ndetail avenues for improvement, and close deliberating on the ethics of\nproviding AI-based sparring partners.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Happe_A/0/1/0/all/0/1\">Andreas Happe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cito_J/0/1/0/all/0/1\">J&#xfc;rgen Cito</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech representation learning: Learning bidirectional encoders with single-view, multi-view, and multi-task methods. (arXiv:2308.00129v1 [eess.AS])","link":"http://arxiv.org/abs/2308.00129","description":"<p>This thesis focuses on representation learning for sequence data over time or\nspace, aiming to improve downstream sequence prediction tasks by using the\nlearned representations. Supervised learning has been the most dominant\napproach for training deep neural networks for learning good sequential\nrepresentations. However, one limiting factor to scale supervised learning is\nthe lack of enough annotated data. Motivated by this challenge, it is natural\nto explore representation learning methods that can utilize large amounts of\nunlabeled and weakly labeled data, as well as an additional data modality. I\ndescribe my broad study of representation learning for speech data. Unlike most\nother works that focus on a single learning setting, this thesis studies\nmultiple settings: supervised learning with auxiliary losses, unsupervised\nlearning, semi-supervised learning, and multi-view learning. Besides different\nlearning problems, I also explore multiple approaches for representation\nlearning. Though I focus on speech data, the methods described in this thesis\ncan also be applied to other domains. Overall, the field of representation\nlearning is developing rapidly. State-of-the-art results on speech related\ntasks are typically based on Transformers pre-trained with large-scale\nself-supervised learning, which aims to learn generic representations that can\nbenefit multiple downstream tasks. Since 2020, large-scale pre-training has\nbeen the de facto choice to achieve good performance. This delayed thesis does\nnot attempt to summarize and compare with the latest results on speech\nrepresentation learning; instead, it presents a unique study on speech\nrepresentation learning before the Transformer era, that covers multiple\nlearning settings. Some of the findings in this thesis can still be useful\ntoday.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tang_Q/0/1/0/all/0/1\">Qingming Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Adverse Drug Event Normalization on Social Media: General-Purpose Model Initialization and Biomedical Semantic Text Similarity Benefit Zero-Shot Linking in Informal Contexts. (arXiv:2308.00157v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00157","description":"<p>Biomedical entity linking, also known as biomedical concept normalization,\nhas recently witnessed the rise to prominence of zero-shot contrastive models.\nHowever, the pre-training material used for these models has, until now,\nlargely consisted of specialist biomedical content such as MIMIC-III clinical\nnotes (Johnson et al., 2016) and PubMed papers (Sayers et al., 2021; Gao et\nal., 2020). While the resulting in-domain models have shown promising results\nfor many biomedical tasks, adverse drug event normalization on social media\ntexts has so far remained challenging for them (Portelli et al., 2022). In this\npaper, we propose a new approach for adverse drug event normalization on social\nmedia relying on general-purpose model initialization via BioLORD (Remy et al.,\n2022) and a semantic-text-similarity fine-tuning named STS. Our experimental\nresults on several social media datasets demonstrate the effectiveness of our\nproposed approach, by achieving state-of-the-art performance. Based on its\nstrong performance across all the tested datasets, we believe this work could\nemerge as a turning point for the task of adverse drug event normalization on\nsocial media and has the potential to serve as a benchmark for future research\nin the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Remy_F/0/1/0/all/0/1\">Fran&#xe7;ois Remy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaboro_S/0/1/0/all/0/1\">Simone Scaboro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portelli_B/0/1/0/all/0/1\">Beatrice Portelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?. (arXiv:2308.00158v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00158","description":"<p>Translation Quality Estimation (TQE) is an important step before deploying\nthe output translation into usage. TQE is also critical in assessing machine\ntranslation (MT) and human translation (HT) quality without seeing the\nreference translations. In this work, we examine if the state-of-the-art large\nlanguage models (LLMs) can be fine-tuned for the TQE task and their capability.\nWe take ChatGPT as one example and approach TQE as a binary classification\ntask. Using English-Italian and English-German training corpus, our\nexperimental results show that fine-tuned ChatGPT via its API can achieve a\nrelatively high score on predicting translation quality, i.e. if the\ntranslation needs to be edited, but there is definitely space to improve the\naccuracy. English-Italiano bilingual Abstract is available in the paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gladkoff_S/0/1/0/all/0/1\">Serge Gladkoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erofeev_G/0/1/0/all/0/1\">Gleb Erofeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1\">Goran Nenadic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarially Robust Neural Legal Judgement Systems. (arXiv:2308.00165v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00165","description":"<p>Legal judgment prediction is the task of predicting the outcome of court\ncases on a given text description of facts of cases. These tasks apply Natural\nLanguage Processing (NLP) techniques to predict legal judgment results based on\nfacts. Recently, large-scale public datasets and NLP models have increased\nresearch in areas related to legal judgment prediction systems. For such\nsystems to be practically helpful, they should be robust from adversarial\nattacks. Previous works mainly focus on making a neural legal judgement system;\nhowever, significantly less or no attention has been given to creating a robust\nLegal Judgement Prediction(LJP) system. We implemented adversarial attacks on\nearly existing LJP systems and found that none of them could handle attacks. In\nthis work, we proposed an approach for making robust LJP systems. Extensive\nexperiments on three legal datasets show significant improvements in our\napproach over the state-of-the-art LJP system in handling adversarial attacks.\nTo the best of our knowledge, we are the first to increase the robustness of\nearly-existing LJP systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raj_R/0/1/0/all/0/1\">Rohit Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devi_V/0/1/0/all/0/1\">V Susheela Devi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Models as a Complex Systems Science: How can we make sense of large language model behavior?. (arXiv:2308.00189v1 [cs.LG])","link":"http://arxiv.org/abs/2308.00189","description":"<p>Coaxing out desired behavior from pretrained models, while avoiding\nundesirable ones, has redefined NLP and is reshaping how we interact with\ncomputers. What was once a scientific engineering discipline-in which building\nblocks are stacked one on top of the other-is arguably already a complex\nsystems science, in which emergent behaviors are sought out to support\npreviously unimagined use cases.\n</p>\n<p>Despite the ever increasing number of benchmarks that measure task\nperformance, we lack explanations of what behaviors language models exhibit\nthat allow them to complete these tasks in the first place. We argue for a\nsystematic effort to decompose language model behavior into categories that\nexplain cross-task performance, to guide mechanistic explanations and help\nfuture-proof analytic research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advancing Beyond Identification: Multi-bit Watermark for Language Models. (arXiv:2308.00221v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00221","description":"<p>This study aims to proactively tackle misuse of large language models beyond\nidentification of machine-generated text. While existing methods focus on\ndetection, some malicious misuses demand tracing the adversary user for\ncounteracting them. To address this, we propose \"Multi-bit Watermark through\nColor-listing\" (COLOR), embedding traceable multi-bit information during\nlanguage model generation. Leveraging the benefits of zero-bit watermarking\n(Kirchenbauer et al., 2023a), COLOR enables extraction without model access,\non-the-fly embedding, and maintains text quality, while allowing zero-bit\ndetection all at the same time. Preliminary experiments demonstrates successful\nembedding of 32-bit messages with 91.9% accuracy in moderate-length texts\n($\\sim$500 tokens). This work advances strategies to counter language model\nmisuse effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">KiYoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_W/0/1/0/all/0/1\">Wonhyuk Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Effective Ancient Chinese Translation: Dataset, Model, and Evaluation. (arXiv:2308.00240v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00240","description":"<p>Interpreting ancient Chinese has been the key to comprehending vast Chinese\nliterature, tradition, and civilization. In this paper, we propose Erya for\nancient Chinese translation. From a dataset perspective, we collect, clean, and\nclassify ancient Chinese materials from various sources, forming the most\nextensive ancient Chinese resource to date. From a model perspective, we devise\nErya training method oriented towards ancient Chinese. We design two\njointly-working tasks: disyllabic aligned substitution (DAS) and dual masked\nlanguage model (DMLM). From an evaluation perspective, we build a benchmark to\njudge ancient Chinese translation quality in different scenarios and evaluate\nthe ancient Chinese translation capacities of various existing models. Our\nmodel exhibits remarkable zero-shot performance across five domains, with over\n+12.0 BLEU against GPT-3.5 models and better human evaluation results than\nERNIE Bot. Subsequent fine-tuning further shows the superior transfer\ncapability of Erya model with +6.2 BLEU gain. We release all the\nabove-mentioned resources at https://github.com/RUCAIBox/Erya.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Geyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiarong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1\">Fengyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jiaxin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modality Multi-Loss Fusion Network. (arXiv:2308.00264v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00264","description":"<p>In this work we investigate the optimal selection and fusion of features\nacross multiple modalities and combine these in a neural network to improve\nemotion detection. We compare different fusion methods and examine the impact\nof multi-loss training within the multi-modality fusion network, identifying\nuseful findings relating to subnet performance. Our best model achieves\nstate-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and\nCH-SIMS), and outperforms the other methods in most metrics. We have found that\ntraining on multimodal features improves single modality testing and designing\nfusion methods based on dataset annotation schema enhances model performance.\nThese results suggest a roadmap towards an optimized feature selection and\nfusion approach for enhancing emotion detection in neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zehui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Ziwei Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_J/0/1/0/all/0/1\">Jaywon Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirschberg_J/0/1/0/all/0/1\">Julia Hirschberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making the V in Text-VQA Matter. (arXiv:2308.00295v1 [cs.CV])","link":"http://arxiv.org/abs/2308.00295","description":"<p>Text-based VQA aims at answering questions by reading the text present in the\nimages. It requires a large amount of scene-text relationship understanding\ncompared to the VQA task. Recent studies have shown that the question-answer\npairs in the dataset are more focused on the text present in the image but less\nimportance is given to visual features and some questions do not require\nunderstanding the image. The models trained on this dataset predict biased\nanswers due to the lack of understanding of visual context. For example, in\nquestions like \"What is written on the signboard?\", the answer predicted by the\nmodel is always \"STOP\" which makes the model to ignore the image. To address\nthese issues, we propose a method to learn visual features (making V matter in\nTextVQA) along with the OCR features and question features using VQA dataset as\nexternal knowledge for Text-based VQA. Specifically, we combine the TextVQA\ndataset and VQA dataset and train the model on this combined dataset. Such a\nsimple, yet effective approach increases the understanding and correlation\nbetween the image features and text present in the image, which helps in the\nbetter answering of questions. We further test the model on different datasets\nand compare their qualitative and quantitative results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hegde_S/0/1/0/all/0/1\">Shamanthak Hegde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahagirdar_S/0/1/0/all/0/1\">Soumya Jahagirdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangisetty_S/0/1/0/all/0/1\">Shankar Gangisetty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models. (arXiv:2308.00304v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00304","description":"<p>We consider the problem of eliciting compositional generalization\ncapabilities in large language models (LLMs) with a novel type of prompting\nstrategy. Compositional generalization empowers the LLMs to solve problems that\nare harder than the ones they have seen (i.e., easy-to-hard generalization),\nwhich is a critical reasoning capability of human-like intelligence. However,\neven the current state-of-the-art LLMs still struggle with this form of\nreasoning. To bridge this gap, we propose skills-in-context (SKiC) prompting,\nwhich instructs LLMs how to compose basic skills to resolve more complex\nproblems. We find that it is crucial to demonstrate both the skills and the\ncompositional examples within the same prompting context. With as few as two\nexamplars, our SKiC prompting initiates strong synergies between skills and\ntheir composition capabilities. Notably, it empowers LLMs to solve unseen\nproblems that require innovative skill compositions, achieving near-perfect\ngeneralization on a broad range of challenging compositionality tasks.\nIntriguingly, SKiC prompting unlocks the latent potential of LLMs, enabling\nthem to leverage pre-existing internal skills acquired during earlier\npretraining and alignment stages, even when these skills are not explicitly\npresented in the prompting context. This results in the capability of LLMs to\nsolve unseen complex problems by activating and composing these internal\ncompetencies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiaoman Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaiqiang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianshu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LimeAttack: Local Explainable Method for Textual Hard-Label Adversarial Attack. (arXiv:2308.00319v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00319","description":"<p>Natural language processing models are vulnerable to adversarial examples.\nPrevious textual adversarial attacks adopt gradients or confidence scores to\ncalculate word importance ranking and generate adversarial examples. However,\nthis information is unavailable in the real world. Therefore, we focus on a\nmore realistic and challenging setting, named hard-label attack, in which the\nattacker can only query the model and obtain a discrete prediction label.\nExisting hard-label attack algorithms tend to initialize adversarial examples\nby random substitution and then utilize complex heuristic algorithms to\noptimize the adversarial perturbation. These methods require a lot of model\nqueries and the attack success rate is restricted by adversary initialization.\nIn this paper, we propose a novel hard-label attack algorithm named LimeAttack,\nwhich leverages a local explainable method to approximate word importance\nranking, and then adopts beam search to find the optimal solution. Extensive\nexperiments show that LimeAttack achieves the better attacking performance\ncompared with existing hard-label attack under the same query budget. In\naddition, we evaluate the effectiveness of LimeAttack on large language models,\nand results indicate that adversarial examples remain a significant threat to\nlarge language models. The adversarial examples crafted by LimeAttack are\nhighly transferable and effectively improve model robustness in adversarial\ntraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhaoqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_W/0/1/0/all/0/1\">Weiwei Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuren Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fountain -- an intelligent contextual assistant combining knowledge representation and language models for manufacturing risk identification. (arXiv:2308.00364v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00364","description":"<p>Deviations from the approved design or processes during mass production can\nlead to unforeseen risks. However, these changes are sometimes necessary due to\nchanges in the product design characteristics or an adaptation in the\nmanufacturing process. A major challenge is to identify these risks early in\nthe workflow so that failures leading to warranty claims can be avoided. We\ndeveloped Fountain as a contextual assistant integrated in the deviation\nmanagement workflow that helps in identifying the risks based on the\ndescription of the existing design and process criteria and the proposed\ndeviation. In the manufacturing context, it is important that the assistant\nprovides recommendations that are explainable and consistent. We achieve this\nthrough a combination of the following two components 1) language models\nfinetuned for domain specific semantic similarity and, 2) knowledge\nrepresentation in the form of a property graph derived from the bill of\nmaterials, Failure Modes and Effect Analysis (FMEA) and prior failures reported\nby customers. Here, we present the nuances of selecting and adapting pretrained\nlanguage models for an engineering domain, continuous model updates based on\nuser interaction with the contextual assistant and creating the causal chain\nfor explainable recommendations based on the knowledge representation.\nAdditionally, we demonstrate that the model adaptation is feasible using\nmoderate computational infrastructure already available to most engineering\nteams in manufacturing organizations and inference can be performed on standard\nCPU only instances for integration with existing applications making these\nmethods easily deployable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Saurabh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuchs_D/0/1/0/all/0/1\">Daniel Fuchs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spindler_K/0/1/0/all/0/1\">Klaus Spindler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tackling Hallucinations in Neural Chart Summarization. (arXiv:2308.00399v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00399","description":"<p>Hallucinations in text generation occur when the system produces text that is\nnot grounded in the input. In this work, we tackle the problem of\nhallucinations in neural chart summarization. Our analysis shows that the\ntarget side of chart summarization training datasets often contains additional\ninformation, leading to hallucinations. We propose a natural language inference\n(NLI) based method to preprocess the training data and show through human\nevaluation that our method significantly reduces hallucinations. We also found\nthat shortening long-distance dependencies in the input sequence and adding\nchart-related information like title and legends improves the overall\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1\">Saad Obaid ul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skrjanec_I/0/1/0/all/0/1\">Iza &#x160;krjanec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1\">Ond&#x159;ej Du&#x161;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demberg_V/0/1/0/all/0/1\">Vera Demberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZRIGF: An Innovative Multimodal Framework for Zero-Resource Image-Grounded Dialogue Generation. (arXiv:2308.00400v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00400","description":"<p>Image-grounded dialogue systems benefit greatly from integrating visual\ninformation, resulting in high-quality response generation. However, current\nmodels struggle to effectively utilize such information in zero-resource\nscenarios, mainly due to the disparity between image and text modalities. To\novercome this challenge, we propose an innovative multimodal framework, called\nZRIGF, which assimilates image-grounded information for dialogue generation in\nzero-resource situations. ZRIGF implements a two-stage learning strategy,\ncomprising contrastive pre-training and generative pre-training. Contrastive\npre-training includes a text-image matching module that maps images and texts\ninto a unified encoded vector space, along with a text-assisted masked image\nmodeling module that preserves pre-training visual features and fosters further\nmultimodal feature alignment. Generative pre-training employs a multimodal\nfusion module and an information transfer module to produce insightful\nresponses based on harmonized multimodal representations. Comprehensive\nexperiments conducted on both text-based and image-grounded dialogue datasets\ndemonstrate ZRIGF's efficacy in generating contextually pertinent and\ninformative responses. Furthermore, we adopt a fully zero-resource scenario in\nthe image-grounded dialogue dataset to demonstrate our framework's robust\ngeneralization capabilities in novel domains. The code is available at\nhttps://github.com/zhangbo-nlp/ZRIGF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongfei Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discourse-Aware Text Simplification: From Complex Sentences to Linked Propositions. (arXiv:2308.00425v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00425","description":"<p>Sentences that present a complex syntax act as a major stumbling block for\ndownstream Natural Language Processing applications whose predictive quality\ndeteriorates with sentence length and complexity. The task of Text\nSimplification (TS) may remedy this situation. It aims to modify sentences in\norder to make them easier to process, using a set of rewriting operations, such\nas reordering, deletion, or splitting. State-of-the-art syntactic TS approaches\nsuffer from two major drawbacks: first, they follow a very conservative\napproach in that they tend to retain the input rather than transforming it, and\nsecond, they ignore the cohesive nature of texts, where context spread across\nclauses or sentences is needed to infer the true meaning of a statement. To\naddress these problems, we present a discourse-aware TS approach that splits\nand rephrases complex English sentences within the semantic context in which\nthey occur. Based on a linguistically grounded transformation stage that uses\nclausal and phrasal disembedding mechanisms, complex sentences are transformed\ninto shorter utterances with a simple canonical structure that can be easily\nanalyzed by downstream applications. With sentence splitting, we thus address a\nTS task that has hardly been explored so far. Moreover, we introduce the notion\nof minimality in this context, as we aim to decompose source sentences into a\nset of self-contained minimal semantic units. To avoid breaking down the input\ninto a disjointed sequence of statements that is difficult to interpret because\nimportant contextual information is missing, we incorporate the semantic\ncontext between the split propositions in the form of hierarchical structures\nand semantic relationships. In that way, we generate a semantic hierarchy of\nminimal propositions that leads to a novel representation of complex assertions\nthat puts a semantic layer on top of the simplified sentences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niklaus_C/0/1/0/all/0/1\">Christina Niklaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cetto_M/0/1/0/all/0/1\">Matthias Cetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Handschuh_S/0/1/0/all/0/1\">Siegfried Handschuh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. (arXiv:2308.00436v1 [cs.AI])","link":"http://arxiv.org/abs/2308.00436","description":"<p>The recent progress in large language models (LLMs), especially the invention\nof chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning\nproblems. However, even the strongest LLMs are still struggling with more\ncomplicated problems that require non-linear thinking and multi-step reasoning.\nIn this work, we explore whether LLMs have the ability to recognize their own\nerrors, without resorting to external resources. In particular, we investigate\nwhether they can be used to identify individual errors within a step-by-step\nreasoning. To this end, we propose a zero-shot verification scheme to recognize\nsuch errors. We then use this verification scheme to improve question-answering\nperformance, by using it to perform weighted voting on different generated\nanswers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and\nfind that it successfully recognizes errors and, in turn, increases final\npredictive performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miao_N/0/1/0/all/0/1\">Ning Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1\">Tom Rainforth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structural Embeddings of Tools for Large Language Models. (arXiv:2308.00447v1 [cs.AI])","link":"http://arxiv.org/abs/2308.00447","description":"<p>It is evident that the current state of Large Language Models (LLMs)\nnecessitates the incorporation of external tools. The lack of straightforward\nalgebraic and logical reasoning is well documented and prompted researchers to\ndevelop frameworks which allow LLMs to operate via external tools. The\nontological nature of tool utilization for a specific task can be well\nformulated with a Directed Acyclic Graph (DAG). The central aim of the paper is\nto highlight the importance of graph based approaches to LLM-tool interaction\nin near future. We propose an exemplary framework to guide the orchestration of\nexponentially increasing numbers of external tools with LLMs,where objectives\nand functionalities of tools are graph encoded hierarchically. Assuming that\ntextual segments of a Chain-of-Thought (CoT) can be imagined as a tool as\ndefined here, the graph based framework can pave new avenues in that particular\ndirection as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Unlu_E/0/1/0/all/0/1\">Eren Unlu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval Augmented Generation and Representative Vector Summarization for large unstructured textual data in Medical Education. (arXiv:2308.00479v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00479","description":"<p>Large Language Models are increasingly being used for various tasks including\ncontent generation and as chatbots. Despite their impressive performances in\ngeneral tasks, LLMs need to be aligned when applying for domain specific tasks\nto mitigate the problems of hallucination and producing harmful answers.\nRetrieval Augmented Generation (RAG) allows to easily attach and manipulate a\nnon-parametric knowledgebases to LLMs. Applications of RAG in the field of\nmedical education are discussed in this paper. A combined extractive and\nabstractive summarization method for large unstructured textual data using\nrepresentative vectors is proposed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manathunga_S/0/1/0/all/0/1\">S. S. Manathunga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Illangasekara_Y/0/1/0/all/0/1\">Y. A. Illangasekara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unimodal Intermediate Training for Multimodal Meme Sentiment Classification. (arXiv:2308.00528v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00528","description":"<p>Internet Memes remain a challenging form of user-generated content for\nautomated sentiment classification. The availability of labelled memes is a\nbarrier to developing sentiment classifiers of multimodal memes. To address the\nshortage of labelled memes, we propose to supplement the training of a\nmultimodal meme classifier with unimodal (image-only and text-only) data. In\nthis work, we present a novel variant of supervised intermediate training that\nuses relatively abundant sentiment-labelled unimodal data. Our results show a\nstatistically significant performance improvement from the incorporation of\nunimodal text data. Furthermore, we show that the training set of labelled\nmemes can be reduced by 40% without reducing the performance of the downstream\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hazman_M/0/1/0/all/0/1\">Muzhaffar Hazman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeever_S/0/1/0/all/0/1\">Susan McKeever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffith_J/0/1/0/all/0/1\">Josephine Griffith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JIANG: Chinese Open Foundation Language Model. (arXiv:2308.00624v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00624","description":"<p>With the advancements in large language model technology, it has showcased\ncapabilities that come close to those of human beings across various tasks.\nThis achievement has garnered significant interest from companies and\nscientific research institutions, leading to substantial investments in the\nresearch and development of these models. While numerous large models have\nemerged during this period, the majority of them have been trained primarily on\nEnglish data. Although they exhibit decent performance in other languages, such\nas Chinese, their potential remains limited due to factors like vocabulary\ndesign and training corpus. Consequently, their ability to fully express their\ncapabilities in Chinese falls short. To address this issue, we introduce the\nmodel named JIANG (Chinese pinyin of ginger) specifically designed for the\nChinese language. We have gathered a substantial amount of Chinese corpus to\ntrain the model and have also optimized its structure. The extensive\nexperimental results demonstrate the excellent performance of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_Q/0/1/0/all/0/1\">Qinhua Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1\">Wenchao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yujia Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Wenxin Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zewen Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Hui Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models. (arXiv:2308.00675v1 [cs.CL])","link":"http://arxiv.org/abs/2308.00675","description":"<p>Today, large language models (LLMs) are taught to use new tools by providing\na few demonstrations of the tool's usage. Unfortunately, demonstrations are\nhard to acquire, and can result in undesirable biased usage if the wrong\ndemonstration is chosen. Even in the rare scenario that demonstrations are\nreadily available, there is no principled selection protocol to determine how\nmany and which ones to provide. As tasks grow more complex, the selection\nsearch grows combinatorially and invariably becomes intractable. Our work\nprovides an alternative to demonstrations: tool documentation. We advocate the\nuse of tool documentation, descriptions for the individual tool usage, over\ndemonstrations. We substantiate our claim through three main empirical findings\non 6 tasks across both vision and language modalities. First, on existing\nbenchmarks, zero-shot prompts with only tool documentation are sufficient for\neliciting proper tool usage, achieving performance on par with few-shot\nprompts. Second, on a newly collected realistic tool-use dataset with hundreds\nof available tool APIs, we show that tool documentation is significantly more\nvaluable than demonstrations, with zero-shot documentation significantly\noutperforming few-shot without documentation. Third, we highlight the benefits\nof tool documentations by tackling image generation and video tracking using\njust-released unseen state-of-the-art models as tools. Finally, we highlight\nthe possibility of using tool documentation to automatically enable new\napplications: by using nothing more than the documentation of GroundingDino,\nStable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of the\njust-released Grounded-SAM and Track Anything models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cheng-Yu Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si-An Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1\">Yasuhisa Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratner_A/0/1/0/all/0/1\">Alexander Ratner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code. (arXiv:2308.00683v1 [cs.LG])","link":"http://arxiv.org/abs/2308.00683","description":"<p>Recent works have widely adopted large language model pretraining for source\ncode, suggested source code-specific pretraining objectives and investigated\nthe applicability of various Transformer-based language model architectures for\nsource code. This work investigates another important aspect of such models,\nnamely the effect of different subtokenization options, and aims at identifying\nmost effective and length-efficient subtokenizations, taking into account code\nspecifics. We propose subtokenziation that reduces average length by 17%\nwithout downstream performance drop, and show that a carefully chosen\nsubtokenization may improve quality by 0.5-2%, possibly with some length\nincrease.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1\">Nadezhda Chirkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troshin_S/0/1/0/all/0/1\">Sergey Troshin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Compositionality with Formal Languages. (arXiv:2208.08195v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.08195","description":"<p>Recombining known primitive concepts into larger novel combinations is a\nquintessentially human cognitive capability. Whether large neural models in NLP\ncan acquire this ability while learning from data is an open question. In this\npaper, we investigate this problem from the perspective of formal languages. We\nuse deterministic finite-state transducers to make an unbounded number of\ndatasets with controllable properties governing compositionality. By randomly\nsampling over many transducers, we explore which of their properties contribute\nto learnability of a compositional relation by a neural network. We find that\nthe models either learn the relations completely or not at all. The key is\ntransition coverage, setting a soft learnability limit at 400 examples per\ntransition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valvoda_J/0/1/0/all/0/1\">Josef Valvoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1\">Naomi Saphra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawski_J/0/1/0/all/0/1\">Jonathan Rawski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Multimodal Sentiment Analysis based on Multimodal Probabilistic Fusion Prompts. (arXiv:2211.06607v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.06607","description":"<p>Multimodal sentiment analysis has gained significant attention due to the\nproliferation of multimodal content on social media. However, existing studies\nin this area rely heavily on large-scale supervised data, which is\ntime-consuming and labor-intensive to collect. Thus, there is a need to address\nthe challenge of few-shot multimodal sentiment analysis. To tackle this\nproblem, we propose a novel method called Multimodal Probabilistic Fusion\nPrompts (MultiPoint) that leverages diverse cues from different modalities for\nmultimodal sentiment detection in the few-shot scenario. Specifically, we start\nby introducing a Consistently Distributed Sampling approach called CDS, which\nensures that the few-shot dataset has the same category distribution as the\nfull dataset. Unlike previous approaches primarily using prompts based on the\ntext modality, we design unified multimodal prompts to reduce discrepancies\nbetween different modalities and dynamically incorporate multimodal\ndemonstrations into the context of each multimodal instance. To enhance the\nmodel's robustness, we introduce a probabilistic fusion method to fuse output\npredictions from multiple diverse prompts for each input. Our extensive\nexperiments on six datasets demonstrate the effectiveness of our approach.\nFirst, our method outperforms strong baselines in the multimodal few-shot\nsetting. Furthermore, under the same amount of data (1% of the full dataset),\nour CDS-based experimental results significantly outperform those based on\npreviously sampled datasets constructed from the same number of instances of\neach class.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaocui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1\">Pengfei Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reliable Measures of Spread in High Dimensional Latent Spaces. (arXiv:2212.08172v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2212.08172","description":"<p>Understanding geometric properties of natural language processing models'\nlatent spaces allows the manipulation of these properties for improved\nperformance on downstream tasks. One such property is the amount of data spread\nin a model's latent space, or how fully the available latent space is being\nused. In this work, we define data spread and demonstrate that the commonly\nused measures of data spread, Average Cosine Similarity and a partition\nfunction min/max ratio I(V), do not provide reliable metrics to compare the use\nof latent space across models. We propose and examine eight alternative\nmeasures of data spread, all but one of which improve over these current\nmetrics when applied to seven synthetic data distributions. Of our proposed\nmeasures, we recommend one principal component-based measure and one\nentropy-based measure that provide reliable, relative measures of spread and\ncan be used to compare models of different sizes and dimensionalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marbut_A/0/1/0/all/0/1\">Anna C. Marbut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKinney_Bock_K/0/1/0/all/0/1\">Katy McKinney-Bock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wheeler_T/0/1/0/all/0/1\">Travis J. Wheeler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallel Context Windows for Large Language Models. (arXiv:2212.10947v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10947","description":"<p>When applied to processing long text, Large Language Models (LLMs) are\nlimited by their context window. Existing efforts to address this limitation\ninvolve training specialized architectures, and cannot be easily applied to\noff-the-shelf LLMs. We present Parallel Context Windows (PCW), a method that\nalleviates the context window restriction for any off-the-shelf LLM without\nfurther training. The key to the approach is to carve a long context into\nchunks (``windows''), restrict the attention mechanism to apply only within\neach window, and re-use the positional embeddings across the windows. Our main\nresults test the PCW approach on in-context learning with models that range in\nsize between 750 million and 178 billion parameters, and show substantial\nimprovements for tasks with diverse input and output spaces. We show additional\nbenefits in other settings where long context windows may be beneficial:\nmulti-hop questions and retrieval-augmented question answering with multiple\nretrieved documents. Our results highlight Parallel Context Windows as a\npromising method for applying off-the-shelf LLMs in a range of settings that\nrequire long text sequences. We make our code publicly available at\nhttps://github.com/ai21labs/parallel-context-windows.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ratner_N/0/1/0/all/0/1\">Nir Ratner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1\">Yoav Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ram_O/0/1/0/all/0/1\">Ori Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magar_I/0/1/0/all/0/1\">Inbal Magar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpas_E/0/1/0/all/0/1\">Ehud Karpas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leyton_Brown_K/0/1/0/all/0/1\">Kevin Leyton-Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoham_Y/0/1/0/all/0/1\">Yoav Shoham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-Context Retrieval-Augmented Language Models. (arXiv:2302.00083v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.00083","description":"<p>Retrieval-Augmented Language Modeling (RALM) methods, which condition a\nlanguage model (LM) on relevant documents from a grounding corpus during\ngeneration, were shown to significantly improve language modeling performance.\nIn addition, they can mitigate the problem of factually inaccurate text\ngeneration and provide natural source attribution mechanism. Existing RALM\napproaches focus on modifying the LM architecture in order to facilitate the\nincorporation of external information, significantly complicating deployment.\nThis paper considers a simple alternative, which we dub In-Context RALM:\nleaving the LM architecture unchanged and prepending grounding documents to the\ninput, without any further training of the LM. We show that In-Context RALM\nthat builds on off-the-shelf general purpose retrievers provides surprisingly\nlarge LM gains across model sizes and diverse corpora. We also demonstrate that\nthe document retrieval and ranking mechanism can be specialized to the RALM\nsetting to further boost performance. We conclude that In-Context RALM has\nconsiderable potential to increase the prevalence of LM grounding, particularly\nin settings where a pretrained LM must be used without modification or even via\nAPI access.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ram_O/0/1/0/all/0/1\">Ori Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1\">Yoav Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalmedigos_I/0/1/0/all/0/1\">Itay Dalmedigos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhlgay_D/0/1/0/all/0/1\">Dor Muhlgay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leyton_Brown_K/0/1/0/all/0/1\">Kevin Leyton-Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoham_Y/0/1/0/all/0/1\">Yoav Shoham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HL Dataset: Visually-grounded Description of Scenes, Actions and Rationales. (arXiv:2302.12189v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12189","description":"<p>Current captioning datasets focus on object-centric captions, describing the\nvisible objects in the image, e.g. \"people eating food in a park\". Although\nthese datasets are useful to evaluate the ability of Vision &amp; Language models\nto recognize and describe visual content, they do not support controlled\nexperiments involving model testing or fine-tuning, with more high-level\ncaptions, which humans find easy and natural to produce. For example, people\noften describe images based on the type of scene they depict ('people at a\nholiday resort') and the actions they perform ('people having a picnic'). Such\ndescriptions draw on personal experience and commonsense assumptions. We\npresent the High-Level Dataset a dataset extending 14997 images from the COCO\ndataset, aligned with a new set of 134,973 human-annotated (high-level)\ncaptions collected along three axes: scenes, actions, and rationales. We\nfurther extend this dataset with confidence scores collected from an\nindependent set of readers, as well as a set of narrative captions generated\nsynthetically, by combining each of the three axes. We describe this dataset\nand analyse it extensively. We also present baseline results for the High-Level\nCaptioning task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cafagna_M/0/1/0/all/0/1\">Michele Cafagna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deemter_K/0/1/0/all/0/1\">Kees van Deemter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depression Detection Using Digital Traces on Social Media: A Knowledge-aware Deep Learning Approach. (arXiv:2303.05389v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.05389","description":"<p>Depression is a common disease worldwide. It is difficult to diagnose and\ncontinues to be underdiagnosed. Because depressed patients constantly share\ntheir symptoms, major life events, and treatments on social media, researchers\nare turning to user-generated digital traces on social media for depression\ndetection. Such methods have distinct advantages in combating depression\nbecause they can facilitate innovative approaches to fight depression and\nalleviate its social and economic burden. However, most existing studies lack\neffective means to incorporate established medical domain knowledge in\ndepression detection or suffer from feature extraction difficulties that impede\ngreater performance. Following the design science research paradigm, we propose\na Deep Knowledge-aware Depression Detection (DKDD) framework to accurately\ndetect social media users at risk of depression and explain the critical\nfactors that contribute to such detection. Extensive empirical studies with\nreal-world data demonstrate that, by incorporating domain knowledge, our method\noutperforms existing state-of-the-art methods. Our work has significant\nimplications for IS research in knowledge-aware machine learning, digital\ntraces utilization, and NLP research in IS. Practically, by providing early\ndetection and explaining the critical factors, DKDD can supplement clinical\ndepression screening and enable large-scale evaluations of a population's\nmental health status.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiaheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chat with the Environment: Interactive Multimodal Perception Using Large Language Models. (arXiv:2303.08268v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2303.08268","description":"<p>Programming robot behavior in a complex world faces challenges on multiple\nlevels, from dextrous low-level skills to high-level planning and reasoning.\nRecent pre-trained Large Language Models (LLMs) have shown remarkable reasoning\nability in few-shot robotic planning. However, it remains challenging to ground\nLLMs in multimodal sensory input and continuous action output, while enabling a\nrobot to interact with its environment and acquire novel information as its\npolicies unfold. We develop a robot interaction scenario with a partially\nobservable state, which necessitates a robot to decide on a range of epistemic\nactions in order to sample sensory information among multiple modalities,\nbefore being able to execute the task correctly. An interactive perception\nframework is therefore proposed with an LLM as its backbone, whose ability is\nexploited to instruct epistemic actions and to reason over the resulting\nmultimodal sensations (vision, sound, haptics, proprioception), as well as to\nplan an entire task execution based on the interactively acquired information.\nOur study demonstrates that LLMs can provide high-level planning and reasoning\nskills and control interactive robot behavior in a multimodal environment,\nwhile multimodal modules with the context of the environmental state help\nground the LLMs and extend their processing ability. The project website can be\nfound at\n\\href{https://matcha-model.github.io}{\\textcolor{blue}{https://matcha-model.github.io/}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xufeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengdi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hafez_M/0/1/0/all/0/1\">Muhammad Burhan Hafez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection. (arXiv:2303.09901v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.09901","description":"<p>This paper presents the winning system for the zero-shot Spanish framing\ndetection task, which also achieves competitive places in eight additional\nlanguages. The challenge of the framing detection task lies in identifying a\nset of 14 frames when only a few or zero samples are available, i.e., a\nmultilingual multi-label few- or zero-shot setting. Our developed solution\nemploys a pre-training procedure based on multilingual Transformers using a\nlabel-aware contrastive loss function. In addition to describing the system, we\nperform an embedding space analysis and ablation study to demonstrate how our\npre-training procedure supports framing detection to advance computational\nframing analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reiter_Haas_M/0/1/0/all/0/1\">Markus Reiter-Haas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ertl_A/0/1/0/all/0/1\">Alexander Ertl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Innerebner_K/0/1/0/all/0/1\">Kevin Innerebner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lex_E/0/1/0/all/0/1\">Elisabeth Lex</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fundamental Limitations of Alignment in Large Language Models. (arXiv:2304.11082v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.11082","description":"<p>An important aspect in developing language models that interact with humans\nis aligning their behavior to be useful and unharmful for their human users.\nThis is usually achieved by tuning the model in a way that enhances desired\nbehaviors and inhibits undesired ones, a process referred to as alignment. In\nthis paper, we propose a theoretical approach called Behavior Expectation\nBounds (BEB) which allows us to formally investigate several inherent\ncharacteristics and limitations of alignment in large language models.\nImportantly, we prove that for any behavior that has a finite probability of\nbeing exhibited by the model, there exist prompts that can trigger the model\ninto outputting this behavior, with probability that increases with the length\nof the prompt. This implies that any alignment process that attenuates\nundesired behavior but does not remove it altogether, is not safe against\nadversarial prompting attacks. Furthermore, our framework hints at the\nmechanism by which leading alignment approaches such as reinforcement learning\nfrom human feedback increase the LLM's proneness to being prompted into the\nundesired behaviors. Moreover, we include the notion of personas in our BEB\nframework, and find that behaviors which are generally very unlikely to be\nexhibited by the model can be brought to the front by prompting the model to\nbehave as specific persona. This theoretical result is being experimentally\ndemonstrated in large scale by the so called contemporary \"chatGPT jailbreaks\",\nwhere adversarial users trick the LLM into breaking its alignment guardrails by\ntriggering it into acting as a malicious persona. Our results expose\nfundamental limitations in alignment of LLMs and bring to the forefront the\nneed to devise reliable mechanisms for ensuring AI safety.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolf_Y/0/1/0/all/0/1\">Yotam Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wies_N/0/1/0/all/0/1\">Noam Wies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avnery_O/0/1/0/all/0/1\">Oshri Avnery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1\">Yoav Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Current State of Summarization. (arXiv:2305.04853v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.04853","description":"<p>With the explosive growth of textual information, summarization systems have\nbecome increasingly important. This work aims to concisely indicate the current\nstate of the art in abstractive text summarization. As part of this, we outline\nthe current paradigm shifts towards pre-trained encoder-decoder models and\nlarge autoregressive language models. Additionally, we delve further into the\nchallenges of evaluating summarization systems and the potential of\ninstruction-tuned models for zero-shot summarization. Finally, we provide a\nbrief overview of how summarization systems are currently being integrated into\ncommercial applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Retkowski_F/0/1/0/all/0/1\">Fabian Retkowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Multimodal Knowledge Graph Construction. (arXiv:2305.08698v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08698","description":"<p>Multimodal Knowledge Graph Construction (MKGC) involves creating structured\nrepresentations of entities and relations using multiple modalities, such as\ntext and images. However, existing MKGC models face challenges in handling the\naddition of new entities and relations in dynamic real-world scenarios. The\ncurrent continual setting for knowledge graph construction mainly focuses on\nentity and relation extraction from text data, overlooking other multimodal\nsources. Therefore, there arises the need to explore the challenge of continual\nMKGC to address the phenomenon of catastrophic forgetting and ensure the\nretention of past knowledge extracted from different forms of data. This\nresearch focuses on investigating this complex topic by developing lifelong\nMKGC benchmark datasets. Based on the empirical findings that several typical\nMKGC models, when trained on multimedia data, might unexpectedly underperform\ncompared to those solely utilizing textual resources in a continual setting, we\npropose a Lifelong MultiModal Consistent Transformer Framework (LMC) for\ncontinual MKGC, which plays the strengths of the consistent multimodal\noptimization in continual learning and leads to a better stability-plasticity\ntrade-off. Our experiments demonstrate the superior performance of our method\nover prevailing continual learning techniques or multimodal approaches in\ndynamic scenarios. Code and datasets can be found at\nhttps://github.com/zjunlp/ContinueMKGC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jintian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongtong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence-Level Knowledge Distillation for Class-Incremental End-to-End Spoken Language Understanding. (arXiv:2305.13899v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2305.13899","description":"<p>The ability to learn new concepts sequentially is a major weakness for modern\nneural networks, which hinders their use in non-stationary environments. Their\npropensity to fit the current data distribution to the detriment of the past\nacquired knowledge leads to the catastrophic forgetting issue. In this work we\ntackle the problem of Spoken Language Understanding applied to a continual\nlearning setting. We first define a class-incremental scenario for the SLURP\ndataset. Then, we propose three knowledge distillation (KD) approaches to\nmitigate forgetting for a sequence-to-sequence transformer model: the first KD\nmethod is applied to the encoder output (audio-KD), and the other two work on\nthe decoder output, either directly on the token-level (tok-KD) or on the\nsequence-level (seq-KD) distributions. We show that the seq-KD substantially\nimproves all the performance metrics, and its combination with the audio-KD\nfurther decreases the average WER and enhances the entity prediction metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cappellazzo_U/0/1/0/all/0/1\">Umberto Cappellazzo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Falavigna_D/0/1/0/all/0/1\">Daniele Falavigna</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brutti_A/0/1/0/all/0/1\">Alessio Brutti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.14387","description":"<p>Large language models (LLMs) such as ChatGPT have seen widespread adoption\ndue to their ability to follow user instructions well. Developing these LLMs\ninvolves a complex yet poorly understood workflow requiring training with human\nfeedback. Replicating and understanding this instruction-following process\nfaces three major challenges: the high cost of data collection, the lack of\ntrustworthy evaluation, and the absence of reference method implementations. We\naddress these challenges with AlpacaFarm, a simulator that enables research and\ndevelopment for learning from feedback at a low cost. First, we design LLM\nprompts to simulate human feedback that are 45x cheaper than crowdworkers and\ndisplay high agreement with humans. Second, we propose an automatic evaluation\nand validate it against human instructions obtained on real-world interactions.\nThird, we contribute reference implementations for several methods (PPO,\nbest-of-n, expert iteration, and more) that learn from pairwise feedback.\nFinally, as an end-to-end validation of AlpacaFarm, we train and evaluate\neleven models on 10k pairs of real human feedback and show that rankings of\nmodels trained in AlpacaFarm match rankings of models trained on human data. As\na demonstration of the research possible in AlpacaFarm, we find that methods\nthat use a reward model can substantially improve over supervised fine-tuning\nand that our reference PPO implementation leads to a +10% improvement in\nwin-rate against Davinci003. We release all components of AlpacaFarm at\nhttps://github.com/tatsu-lab/alpaca_farm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1\">Yann Dubois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuechen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taori_R/0/1/0/all/0/1\">Rohan Taori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulrajani_I/0/1/0/all/0/1\">Ishaan Gulrajani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ba_J/0/1/0/all/0/1\">Jimmy Ba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guestrin_C/0/1/0/all/0/1\">Carlos Guestrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori B. Hashimoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diable: Efficient Dialogue State Tracking as Operations on Tables. (arXiv:2305.17020v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17020","description":"<p>Sequence-to-sequence state-of-the-art systems for dialogue state tracking\n(DST) use the full dialogue history as input, represent the current state as a\nlist with all the slots, and generate the entire state from scratch at each\ndialogue turn. This approach is inefficient, especially when the number of\nslots is large and the conversation is long. We propose Diable, a new task\nformalisation that simplifies the design and implementation of efficient DST\nsystems and allows one to easily plug and play large language models. We\nrepresent the dialogue state as a table and formalise DST as a table\nmanipulation task. At each turn, the system updates the previous state by\ngenerating table operations based on the dialogue context. Extensive\nexperimentation on the MultiWoz datasets demonstrates that Diable (i)\noutperforms strong efficient DST baselines, (ii) is 2.4x more time efficient\nthan current state-of-the-art methods while retaining competitive Joint Goal\nAccuracy, and (iii) is robust to noisy data annotations due to the table\noperations approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lesci_P/0/1/0/all/0/1\">Pietro Lesci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujinuma_Y/0/1/0/all/0/1\">Yoshinari Fujinuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardalov_M/0/1/0/all/0/1\">Momchil Hardalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_C/0/1/0/all/0/1\">Chao Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marquez_L/0/1/0/all/0/1\">Lluis Marquez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models. (arXiv:2305.17446v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17446","description":"<p>Pre-trained language models (PLMs) are known to be overly parameterized and\nhave significant redundancy, indicating a small degree of freedom of the PLMs.\nMotivated by the observation, in this paper, we study the problem of\nre-parameterizing and fine-tuning PLMs from a new perspective: Discovery of\nintrinsic task-specific subspace. Specifically, by exploiting the dynamics of\nthe fine-tuning process for a given task, the parameter optimization trajectory\nis learned to uncover its intrinsic task-specific subspace. A key finding is\nthat PLMs can be effectively fine-tuned in the subspace with a small number of\nfree parameters. Beyond, we observe some outlier dimensions emerging during\nfine-tuning in the subspace. Disabling these dimensions degrades the model\nperformance significantly. This suggests that these dimensions are crucial to\ninduce task-specific knowledge to downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Junming Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18624","description":"<p>Contrastive learning has become a popular solution for few-shot Name Entity\nRecognization (NER). The conventional configuration strives to reduce the\ndistance between tokens with the same labels and increase the distance between\ntokens with different labels. The effect of this setup may, however, in the\nmedical domain, there are a lot of entities annotated as OUTSIDE (O), and they\nare undesirably pushed apart to other entities that are not labeled as OUTSIDE\n(O) by the current contrastive learning method end up with a noisy prototype\nfor the semantic representation of the label, though there are many OUTSIDE (O)\nlabeled entities are relevant to the labeled entities. To address this\nchallenge, we propose a novel method named Weighted Prototypical Contrastive\nLearning for Medical Few Shot Named Entity Recognization (W-PROCER). Our\napproach primarily revolves around constructing the prototype-based contractive\nloss and weighting network. These components play a crucial role in assisting\nthe model in differentiating the negative samples from OUTSIDE (O) tokens and\nenhancing the discrimination ability of contrastive learning. Experimental\nresults show that our proposed W-PROCER framework significantly outperforms the\nstrong baselines on the three medical benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingchen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_J/0/1/0/all/0/1\">Jeremy Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huixue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1\">Huaiyuan Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MyCrunchGPT: A chatGPT assisted framework for scientific machine learning. (arXiv:2306.15551v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2306.15551","description":"<p>Scientific Machine Learning (SciML) has advanced recently across many\ndifferent areas in computational science and engineering. The objective is to\nintegrate data and physics seamlessly without the need of employing elaborate\nand computationally taxing data assimilation schemes. However, preprocessing,\nproblem formulation, code generation, postprocessing and analysis are still\ntime consuming and may prevent SciML from wide applicability in industrial\napplications and in digital twin frameworks. Here, we integrate the various\nstages of SciML under the umbrella of ChatGPT, to formulate MyCrunchGPT, which\nplays the role of a conductor orchestrating the entire workflow of SciML based\non simple prompts by the user. Specifically, we present two examples that\ndemonstrate the potential use of MyCrunchGPT in optimizing airfoils in\naerodynamics, and in obtaining flow fields in various geometries in interactive\nmode, with emphasis on the validation stage. To demonstrate the flow of the\nMyCrunchGPT, and create an infrastructure that can facilitate a broader vision,\nwe built a webapp based guided user interface, that includes options for a\ncomprehensive summary report. The overall objective is to extend MyCrunchGPT to\nhandle diverse problems in computational mechanics, design, optimization and\ncontrols, and general scientific computing tasks involved in SciML, hence using\nit as a research assistant tool but also as an educational tool. While here the\nexamples focus in fluid mechanics, future versions will target solid mechanics\nand materials science, geophysics, systems biology and bioinformatics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gleyzer_L/0/1/0/all/0/1\">Leonard Gleyzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahana_A/0/1/0/all/0/1\">Adar Kahana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_K/0/1/0/all/0/1\">Khemraj Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1\">George Em Karniadakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors. (arXiv:2306.17156v3 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2306.17156","description":"<p>Generative AI and large language models hold great promise in enhancing\ncomputing education by powering next-generation educational technologies for\nintroductory programming. Recent works have studied these models for different\nscenarios relevant to programming education; however, these works are limited\nfor several reasons, as they typically consider already outdated models or only\nspecific scenario(s). Consequently, there is a lack of a systematic study that\nbenchmarks state-of-the-art models for a comprehensive set of programming\neducation scenarios. In our work, we systematically evaluate two models,\nChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human\ntutors for a variety of scenarios. We evaluate using five introductory Python\nprogramming problems and real-world buggy programs from an online platform, and\nassess performance using expert-based annotations. Our results show that GPT-4\ndrastically outperforms ChatGPT (based on GPT-3.5) and comes close to human\ntutors' performance for several scenarios. These results also highlight\nsettings where GPT-4 still struggles, providing exciting future directions on\ndeveloping techniques to improve the performance of these models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phung_T/0/1/0/all/0/1\">Tung Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padurean_V/0/1/0/all/0/1\">Victor-Alexandru P&#x103;durean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambronero_J/0/1/0/all/0/1\">Jos&#xe9; Cambronero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulwani_S/0/1/0/all/0/1\">Sumit Gulwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohn_T/0/1/0/all/0/1\">Tobias Kohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_R/0/1/0/all/0/1\">Rupak Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1\">Adish Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_G/0/1/0/all/0/1\">Gustavo Soares</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks. (arXiv:2307.02477v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.02477","description":"<p>The impressive performance of recent language models across a wide range of\ntasks suggests that they possess a degree of abstract reasoning skills. Are\nthese skills general and transferable, or specialized to specific tasks seen\nduring pretraining? To disentangle these effects, we propose an evaluation\nframework based on \"counterfactual\" task variants that deviate from the default\nassumptions underlying standard tasks. Across a suite of 11 tasks, we observe\nnontrivial performance on the counterfactual variants, but nevertheless find\nthat performance substantially and consistently degrades compared to the\ndefault conditions. This suggests that while current LMs may possess abstract\ntask-solving skills to a degree, they often also rely on narrow,\nnon-transferable procedures for task-solving. These results motivate a more\ncareful interpretation of language model performance that teases apart these\naspects of behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhaofeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Linlu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1\">Alexis Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1\">Ekin Aky&#xfc;rek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bailin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Najoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering. (arXiv:2307.04192v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2307.04192","description":"<p>Video question--answering is a fundamental task in the field of video\nunderstanding. Although current vision--language models (VLMs) equipped with\nVideo Transformers have enabled temporal modeling and yielded superior results,\nthey are at the cost of huge computational power and thus too expensive to\ndeploy in real-time application scenarios. An economical workaround only\nsamples a small portion of frames to represent the main content of that video\nand tune an image--text model on these sampled frames. Recent video\nunderstanding models usually randomly sample a set of frames or clips,\nregardless of internal correlations between their visual contents, nor their\nrelevance to the problem. We argue that such kinds of aimless sampling may omit\nthe key frames from which the correct answer can be deduced, and the situation\ngets worse when the sampling sparsity increases, which always happens as the\nvideo lengths increase. To mitigate this issue, we propose two frame sampling\nstrategies, namely the most domain frames (MDF) and most implied frames (MIF),\nto maximally preserve those frames that are most likely vital to the given\nquestions. MDF passively minimizes the risk of key frame omission in a\nbootstrap manner, while MIS actively searches key frames customized for each\nvideo--question pair with the assistance of auxiliary models. The experimental\nresults on three public datasets from three advanced VLMs (CLIP, GIT and\nAll-in-one) demonstrate that our proposed strategies can boost the performance\nfor image--text pretrained models. The source codes pertaining to the method\nproposed in this paper are publicly available at\nhttps://github.com/declare-lab/sas-vqa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Min-Yen Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How is ChatGPT's behavior changing over time?. (arXiv:2307.09009v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.09009","description":"<p>GPT-3.5 and GPT-4 are the two most widely used large language model (LLM)\nservices. However, when and how these models are updated over time is opaque.\nHere, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on\nseveral diverse tasks: 1) math problems, 2) sensitive/dangerous questions, 3)\nopinion surveys, 4) multi-hop knowledge-intensive questions, 5) generating\ncode, 6) US Medical License tests, and 7) visual reasoning. We find that the\nperformance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time.\nFor example, GPT-4 (March 2023) was reasonable at identifying prime vs.\ncomposite numbers (84% accuracy) but GPT-4 (June 2023) was poor on these same\nquestions (51% accuracy). This is partly explained by a drop in GPT-4's amenity\nto follow chain-of-thought prompting. Interestingly, GPT-3.5 was much better in\nJune than in March in this task. GPT-4 became less willing to answer sensitive\nquestions and opinion survey questions in June than in March. GPT-4 performed\nbetter at multi-hop questions in June than in March, while GPT-3.5's\nperformance dropped on this task. Both GPT-4 and GPT-3.5 had more formatting\nmistakes in code generation in June than in March. Overall, our findings show\nthat the behavior of the \"same\" LLM service can change substantially in a\nrelatively short amount of time, highlighting the need for continuous\nmonitoring of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lingjiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models. (arXiv:2307.11224v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.11224","description":"<p>Jina Embeddings constitutes a set of high-performance sentence embedding\nmodels adept at translating various textual inputs into numerical\nrepresentations, thereby capturing the semantic essence of the text. The models\nexcel in applications such as dense retrieval and semantic textual similarity.\nThis paper details the development of Jina Embeddings, starting with the\ncreation of high-quality pairwise and triplet datasets. It underlines the\ncrucial role of data cleaning in dataset preparation, gives in-depth insights\ninto the model training process, and concludes with a comprehensive performance\nevaluation using the Massive Textual Embedding Benchmark (MTEB). To increase\nthe model's awareness of negations, we constructed a novel training and\nevaluation dataset of negated and non-negated statements, which we make\npublicly available to the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gunther_M/0/1/0/all/0/1\">Michael G&#xfc;nther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milliken_L/0/1/0/all/0/1\">Louis Milliken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geuter_J/0/1/0/all/0/1\">Jonathan Geuter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mastrapas_G/0/1/0/all/0/1\">Georgios Mastrapas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Han Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. (arXiv:2307.11760v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.11760","description":"<p>Large language models (LLMs) have achieved significant performance in many\nfields such as reasoning, language understanding, and math problem-solving, and\nare regarded as a crucial step to artificial general intelligence (AGI).\nHowever, the sensitivity of LLMs to prompts remains a major bottleneck for\ntheir daily adoption. In this paper, we take inspiration from psychology and\npropose EmotionPrompt to explore emotional intelligence to enhance the\nperformance of LLMs. EmotionPrompt operates on a remarkably straightforward\nprinciple: the incorporation of emotional stimulus into prompts. Experimental\nresults demonstrate that our EmotionPrompt, using the same single prompt\ntemplates, significantly outperforms original zero-shot prompt and\nZero-shot-CoT on 8 tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and\nT5. Further, EmotionPrompt was observed to improve both truthfulness and\ninformativeness. We believe that EmotionPrompt heralds a novel avenue for\nexploring interdisciplinary knowledge for humans-LLMs interaction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kaijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yixuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1\">Wenxin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1\">Jianxun Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CliniDigest: A Case Study in Large Language Model Based Large-Scale Summarization of Clinical Trial Descriptions. (arXiv:2307.14522v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.14522","description":"<p>A clinical trial is a study that evaluates new biomedical interventions. To\ndesign new trials, researchers draw inspiration from those current and\ncompleted. In 2022, there were on average more than 100 clinical trials\nsubmitted to ClinicalTrials.gov every day, with each trial having a mean of\napproximately 1500 words [1]. This makes it nearly impossible to keep up to\ndate. To mitigate this issue, we have created a batch clinical trial summarizer\ncalled CliniDigest using GPT-3.5. CliniDigest is, to our knowledge, the first\ntool able to provide real-time, truthful, and comprehensive summaries of\nclinical trials. CliniDigest can reduce up to 85 clinical trial descriptions\n(approximately 10,500 words) into a concise 200-word summary with references\nand limited hallucinations. We have tested CliniDigest on its ability to\nsummarize 457 trials divided across 27 medical subdomains. For each field,\nCliniDigest generates summaries of $\\mu=153,\\ \\sigma=69 $ words, each of which\nutilizes $\\mu=54\\%,\\ \\sigma=30\\% $ of the sources. A more comprehensive\nevaluation is planned and outlined in this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+White_R/0/1/0/all/0/1\">Renee D. White</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1\">Tristan Peng</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Sripitak_P/0/1/0/all/0/1\">Pann Sripitak</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Johansen_A/0/1/0/all/0/1\">Alexander Rosenberg Johansen</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Snyder_M/0/1/0/all/0/1\">Michael Snyder</a> (1) ((1) Stanford University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gzip versus bag-of-words for text classification with KNN. (arXiv:2307.15002v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.15002","description":"<p>The effectiveness of compression distance in KNN-based text classification\n('gzip') has recently garnered lots of attention. In this note we show that\nsimpler means can also be effective, and compression may not be needed. Indeed,\na 'bag-of-words' matching can achieve similar or better results, and is more\nefficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Opitz_J/0/1/0/all/0/1\">Juri Opitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Learning Behaviour of In-context Learning: A Comparison with Supervised Learning. (arXiv:2307.15411v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.15411","description":"<p>Large language models (LLMs) have shown remarkable capacity for in-context\nlearning (ICL), where learning a new task from just a few training examples is\ndone without being explicitly pre-trained. However, despite the success of\nLLMs, there has been little understanding of how ICL learns the knowledge from\nthe given prompts. In this paper, to make progress toward understanding the\nlearning behaviour of ICL, we train the same LLMs with the same demonstration\nexamples via ICL and supervised learning (SL), respectively, and investigate\ntheir performance under label perturbations (i.e., noisy labels and label\nimbalance) on a range of classification tasks. First, via extensive\nexperiments, we find that gold labels have significant impacts on the\ndownstream in-context performance, especially for large language models;\nhowever, imbalanced labels matter little to ICL across all model sizes. Second,\nwhen comparing with SL, we show empirically that ICL is less sensitive to label\nperturbations than SL, and ICL gradually attains comparable performance to SL\nas the model size increases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xindi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1\">Frank Rudzicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mercer_R/0/1/0/all/0/1\">Robert E. Mercer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Dive into the Language of International Relations: NLP-based Analysis of UNESCO's Summary Records. (arXiv:2307.16573v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.16573","description":"<p>Cultural heritage is an arena of international relations that interests all\nstates worldwide. The inscription process on the UNESCO World Heritage List and\nthe UNESCO Representative List of the Intangible Cultural Heritage of Humanity\noften leads to tensions and conflicts among states. This research addresses\nthese challenges by developing automatic tools that provide valuable insights\ninto the decision-making processes regarding inscriptions to the two lists\nmentioned above. We propose innovative topic modelling and tension detection\nmethods based on UNESCO's summary records. Our analysis achieved a commendable\naccuracy rate of 72% in identifying tensions. Furthermore, we have developed an\napplication tailored for diplomats, lawyers, political scientists, and\ninternational relations researchers that facilitates the efficient search of\nparagraphs from selected documents and statements from specific speakers about\nchosen topics. This application is a valuable resource for enhancing the\nunderstanding of complex decision-making dynamics within international heritage\ninscription procedures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wojciechowska_J/0/1/0/all/0/1\">Joanna Wojciechowska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sypniewski_M/0/1/0/all/0/1\">Mateusz Sypniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smigielska_M/0/1/0/all/0/1\">Maria &#x15a;migielska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaminski_I/0/1/0/all/0/1\">Igor Kami&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wisnios_E/0/1/0/all/0/1\">Emilia Wi&#x15b;nios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schreiber_H/0/1/0/all/0/1\">Hanna Schreiber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pielinski_B/0/1/0/all/0/1\">Bartosz Pieli&#x144;ski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-08-01T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}