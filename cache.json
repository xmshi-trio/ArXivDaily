{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-08-25T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Vision Transformer Adapters for Generalizable Multitask Learning. (arXiv:2308.12372v1 [cs.CV])","link":"http://arxiv.org/abs/2308.12372","description":"<p>We introduce the first multitasking vision transformer adapters that learn\ngeneralizable task affinities which can be applied to novel tasks and domains.\nIntegrated into an off-the-shelf vision transformer backbone, our adapters can\nsimultaneously solve multiple dense vision tasks in a parameter-efficient\nmanner, unlike existing multitasking transformers that are parametrically\nexpensive. In contrast to concurrent methods, we do not require retraining or\nfine-tuning whenever a new task or domain is added. We introduce a task-adapted\nattention mechanism within our adapter framework that combines gradient-based\ntask similarities with attention-based ones. The learned task affinities\ngeneralize to the following settings: zero-shot task transfer, unsupervised\ndomain adaptation, and generalization without fine-tuning to novel domains. We\ndemonstrate that our approach outperforms not only the existing convolutional\nneural network-based multitasking methods but also the vision transformer-based\nones. Our project page is at \\url{https://ivrl.github.io/VTAGML}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_D/0/1/0/all/0/1\">Deblina Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susstrunk_S/0/1/0/all/0/1\">Sabine S&#xfc;sstrunk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inferring gender from name: a large scale performance evaluation study. (arXiv:2308.12381v1 [cs.CL])","link":"http://arxiv.org/abs/2308.12381","description":"<p>A person's gender is a crucial piece of information when performing research\nacross a wide range of scientific disciplines, such as medicine, sociology,\npolitical science, and economics, to name a few. However, in increasing\ninstances, especially given the proliferation of big data, gender information\nis not readily available. In such cases researchers need to infer gender from\nreadily available information, primarily from persons' names. While inferring\ngender from name may raise some ethical questions, the lack of viable\nalternatives means that researchers have to resort to such approaches when the\ngoal justifies the means - in the majority of such studies the goal is to\nexamine patterns and determinants of gender disparities. The necessity of\nname-to-gender inference has generated an ever-growing domain of algorithmic\napproaches and software products. These approaches have been used throughout\nthe world in academia, industry, governmental and non-governmental\norganizations. Nevertheless, the existing approaches have yet to be\nsystematically evaluated and compared, making it challenging to determine the\noptimal approach for future research. In this work, we conducted a large scale\nperformance evaluation of existing approaches for name-to-gender inference.\nAnalysis are performed using a variety of large annotated datasets of names. We\nfurther propose two new hybrid approaches that achieve better performance than\nany single existing approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krstovski_K/0/1/0/all/0/1\">Kriste Krstovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Ye Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"With a Little Help from your own Past: Prototypical Memory Networks for Image Captioning. (arXiv:2308.12383v1 [cs.CV])","link":"http://arxiv.org/abs/2308.12383","description":"<p>Image captioning, like many tasks involving vision and language, currently\nrelies on Transformer-based architectures for extracting the semantics in an\nimage and translating it into linguistically coherent descriptions. Although\nsuccessful, the attention operator only considers a weighted summation of\nprojections of the current input sample, therefore ignoring the relevant\nsemantic information which can come from the joint observation of other\nsamples. In this paper, we devise a network which can perform attention over\nactivations obtained while processing other training samples, through a\nprototypical memory model. Our memory models the distribution of past keys and\nvalues through the definition of prototype vectors which are both\ndiscriminative and compact. Experimentally, we assess the performance of the\nproposed model on the COCO dataset, in comparison with carefully designed\nbaselines and state-of-the-art approaches, and by investigating the role of\neach of the proposed components. We demonstrate that our proposal can increase\nthe performance of an encoder-decoder Transformer by 3.7 CIDEr points both when\ntraining in cross-entropy only and when fine-tuning with self-critical sequence\ntraining. Source code and trained models are available at:\nhttps://github.com/aimagelab/PMA-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barraco_M/0/1/0/all/0/1\">Manuele Barraco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarto_S/0/1/0/all/0/1\">Sara Sarto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward American Sign Language Processing in the Real World: Data, Tasks, and Methods. (arXiv:2308.12419v1 [cs.CV])","link":"http://arxiv.org/abs/2308.12419","description":"<p>Sign language, which conveys meaning through gestures, is the chief means of\ncommunication among deaf people. Recognizing sign language in natural settings\npresents significant challenges due to factors such as lighting, background\nclutter, and variations in signer characteristics. In this thesis, I study\nautomatic sign language processing in the wild, using signing videos collected\nfrom the Internet. This thesis contributes new datasets, tasks, and methods.\nMost chapters of this thesis address tasks related to fingerspelling, an\nimportant component of sign language and yet has not been studied widely by\nprior work. I present three new large-scale ASL datasets in the wild:\nChicagoFSWild, ChicagoFSWild+, and OpenASL. Using ChicagoFSWild and\nChicagoFSWild+, I address fingerspelling recognition, which consists of\ntranscribing fingerspelling sequences into text. I propose an end-to-end\napproach based on iterative attention that allows recognition from a raw video\nwithout explicit hand detection. I further show that using a Conformer-based\nnetwork jointly modeling handshape and mouthing can bring performance close to\nthat of humans. Next, I propose two tasks for building real-world\nfingerspelling-based applications: fingerspelling detection and search. For\nfingerspelling detection, I introduce a suite of evaluation metrics and a new\ndetection model via multi-task training. To address the problem of searching\nfor fingerspelled keywords in raw sign language videos, we propose a novel\nmethod that jointly localizes and matches fingerspelling segments to text.\nFinally, I will describe a benchmark for large-vocabulary open-domain sign\nlanguage translation based on OpenASL. To address the challenges of sign\nlanguage translation in realistic settings, we propose a set of techniques\nincluding sign search as a pretext task for pre-training and fusion of mouthing\nand handshape features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bowen Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature. (arXiv:2308.12420v1 [cs.IR])","link":"http://arxiv.org/abs/2308.12420","description":"<p>Distributed Ledger Technologies (DLTs) have rapidly evolved, necessitating\ncomprehensive insights into their diverse components. However, a systematic\nliterature review that emphasizes the Environmental, Sustainability, and\nGovernance (ESG) components of DLT remains lacking. To bridge this gap, we\nselected 107 seed papers to build a citation network of 63,083 references and\nrefined it to a corpus of 24,539 publications for analysis. Then, we labeled\nthe named entities in 46 papers according to twelve top-level categories\nderived from an established technology taxonomy and enhanced the taxonomy by\npinpointing DLT's ESG elements. Leveraging transformer-based language models,\nwe fine-tuned a pre-trained language model for a Named Entity Recognition (NER)\ntask using our labeled dataset. We used our fine-tuned language model to\ndistill the corpus to 505 key papers, facilitating a literature review via\nnamed entities and temporal graph analysis on DLT evolution in the context of\nESG. Our contributions are a methodology to conduct a machine learning-driven\nsystematic literature review in the DLT field, placing a special emphasis on\nESG aspects. Furthermore, we present a first-of-its-kind NER dataset, composed\nof 54,808 named entities, designed for DLT and ESG-related explorations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_W/0/1/0/all/0/1\">Walter Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tylinski_K/0/1/0/all/0/1\">Kamil Tylinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moore_A/0/1/0/all/0/1\">Alastair Moore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roche_N/0/1/0/all/0/1\">Niall Roche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vadgama_N/0/1/0/all/0/1\">Nikhil Vadgama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treiblmaier_H/0/1/0/all/0/1\">Horst Treiblmaier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shangguan_J/0/1/0/all/0/1\">Jiangbo Shangguan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tasca_P/0/1/0/all/0/1\">Paolo Tasca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiahua Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are ChatGPT and GPT-4 Good Poker Players? -- A Pre-Flop Analysis. (arXiv:2308.12466v1 [cs.CL])","link":"http://arxiv.org/abs/2308.12466","description":"<p>Since the introduction of ChatGPT and GPT-4, these models have been tested\nacross a large number of tasks. Their adeptness across domains is evident, but\ntheir aptitude in playing games and specifically their aptitude in the realm of\npoker has remained unexplored. Poker is a game that requires decision making\nunder uncertainty and incomplete information. In this paper, we put ChatGPT and\nGPT-4 through the poker test and evaluate their poker skills. Our findings\nreveal that while both models display an advanced understanding of poker,\nencompassing concepts like the valuation of starting hands, playing positions\nand other intricacies of game theory optimal (GTO) poker, both ChatGPT and\nGPT-4 are NOT game theory optimal poker players.\n</p>\n<p>Through a series of experiments, we first discover the characteristics of\noptimal prompts and model parameters for playing poker with these models. Our\nobservations then unveil the distinct playing personas of the two models. We\nfirst conclude that GPT-4 is a more advanced poker player than ChatGPT. This\nexploration then sheds light on the divergent poker tactics of the two models:\nChatGPT's conservativeness juxtaposed against GPT-4's aggression. In poker\nvernacular, when tasked to play GTO poker, ChatGPT plays like a Nit, which\nmeans that it has a propensity to only engage with premium hands and folds a\nmajority of hands. When subjected to the same directive, GPT-4 plays like a\nmaniac, showcasing a loose and aggressive style of play. Both strategies,\nalthough relatively advanced, are not game theory optimal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akshat Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"American Stories: A Large-Scale Structured Text Dataset of Historical U.S. Newspapers. (arXiv:2308.12477v1 [cs.CL])","link":"http://arxiv.org/abs/2308.12477","description":"<p>Existing full text datasets of U.S. public domain newspapers do not recognize\nthe often complex layouts of newspaper scans, and as a result the digitized\ncontent scrambles texts from articles, headlines, captions, advertisements, and\nother layout regions. OCR quality can also be low. This study develops a novel,\ndeep learning pipeline for extracting full article texts from newspaper images\nand applies it to the nearly 20 million scans in Library of Congress's public\ndomain Chronicling America collection. The pipeline includes layout detection,\nlegibility classification, custom OCR, and association of article texts\nspanning multiple bounding boxes. To achieve high scalability, it is built with\nefficient architectures designed for mobile phones. The resulting American\nStories dataset provides high quality data that could be used for pre-training\na large language model to achieve better understanding of historical English\nand historical world knowledge. The dataset could also be added to the external\ndatabase of a retrieval-augmented language model to make historical information\n- ranging from interpretations of political events to minutiae about the lives\nof people's ancestors - more widely accessible. Furthermore, structured article\ntexts facilitate using transformer-based methods for popular social science\napplications like topic classification, detection of reproduced content, and\nnews story clustering. Finally, American Stories provides a massive silver\nquality dataset for innovating multimodal layout analysis models and other\nmultimodal applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dell_M/0/1/0/all/0/1\">Melissa Dell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlson_J/0/1/0/all/0/1\">Jacob Carlson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bryan_T/0/1/0/all/0/1\">Tom Bryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silcock_E/0/1/0/all/0/1\">Emily Silcock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Abhishek Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zejiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAmico_Wong_L/0/1/0/all/0/1\">Luca D&#x27;Amico-Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Querubin_P/0/1/0/all/0/1\">Pablo Querubin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heldring_L/0/1/0/all/0/1\">Leander Heldring</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPTEval: A Survey on Assessments of ChatGPT and GPT-4. (arXiv:2308.12488v1 [cs.AI])","link":"http://arxiv.org/abs/2308.12488","description":"<p>The emergence of ChatGPT has generated much speculation in the press about\nits potential to disrupt social and economic systems. Its astonishing language\nability has aroused strong curiosity among scholars about its performance in\ndifferent domains. There have been many studies evaluating the ability of\nChatGPT and GPT-4 in different tasks and disciplines. However, a comprehensive\nreview summarizing the collective assessment findings is lacking. The objective\nof this survey is to thoroughly analyze prior assessments of ChatGPT and GPT-4,\nfocusing on its language and reasoning abilities, scientific knowledge, and\nethical considerations. Furthermore, an examination of the existing evaluation\nmethods is conducted, offering several recommendations for future research in\nevaluating large language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_R/0/1/0/all/0/1\">Rui Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xulang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1\">Frank Guerin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiPA: a multi-task speech pronunciation assessment system for a closed and open response scenario. (arXiv:2308.12490v1 [cs.CL])","link":"http://arxiv.org/abs/2308.12490","description":"<p>The design of automatic speech pronunciation assessment can be categorized\ninto closed and open response scenarios, each with strengths and limitations. A\nsystem with the ability to function in both scenarios can cater to diverse\nlearning needs and provide a more precise and holistic assessment of\npronunciation skills. In this study, we propose a Multi-task Pronunciation\nAssessment model called MultiPA. MultiPA provides an alternative to Kaldi-based\nsystems in that it has simpler format requirements and better compatibility\nwith other neural network models. Compared with previous open response systems,\nMultiPA provides a wider range of evaluations, encompassing assessments at both\nthe sentence and word-level. Our experimental results show that MultiPA\nachieves comparable performance when working in closed response scenarios and\nmaintains more robust performance when directly used for open responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Wen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirschberg_J/0/1/0/all/0/1\">Julia Hirschberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Model as Autonomous Decision Maker. (arXiv:2308.12519v1 [cs.CL])","link":"http://arxiv.org/abs/2308.12519","description":"<p>While large language models (LLMs) exhibit impressive language understanding\nand in-context learning abilities, their decision-making ability still heavily\nrelies on the guidance of task-specific expert knowledge when solving\nreal-world tasks. To unleash the potential of LLMs as autonomous decision\nmakers, this paper presents an approach JuDec to endow LLMs with the\nself-judgment ability, enabling LLMs to achieve autonomous judgment and\nexploration for decision making. Specifically, in JuDec, Elo-based\nSelf-Judgment Mechanism is designed to assign Elo scores to decision steps to\njudge their values and utilities via pairwise comparisons between two solutions\nand then guide the decision-searching process toward the optimal solution\naccordingly. Experimental results on the ToolBench dataset demonstrate JuDec's\nsuperiority over baselines, achieving over 10% improvement in Pass Rate on\ndiverse tasks. It offers higher-quality solutions and reduces costs (ChatGPT\nAPI calls), highlighting its effectiveness and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yining Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1\">Xin Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CARE: Co-Attention Network for Joint Entity and Relation Extraction. (arXiv:2308.12531v1 [cs.CL])","link":"http://arxiv.org/abs/2308.12531","description":"<p>Joint entity and relation extraction is the fundamental task of information\nextraction, consisting of two subtasks: named entity recognition and relation\nextraction. Most existing joint extraction methods suffer from issues of\nfeature confusion or inadequate interaction between two subtasks. In this work,\nwe propose a Co-Attention network for joint entity and Relation Extraction\n(CARE). Our approach involves learning separate representations for each\nsubtask, aiming to avoid feature overlap. At the core of our approach is the\nco-attention module that captures two-way interaction between two subtasks,\nallowing the model to leverage entity information for relation prediction and\nvice versa, thus promoting mutual enhancement. Extensive experiments on three\njoint entity-relation extraction benchmark datasets (NYT, WebNLG and SciERC)\nshow that our proposed model achieves superior performance, surpassing existing\nbaseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_W/0/1/0/all/0/1\">Wenjun Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yamei Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias. (arXiv:2308.12539v1 [cs.CL])","link":"http://arxiv.org/abs/2308.12539","description":"<p>As language models (LMs) become increasingly powerful, it is important to\nquantify and compare them for sociodemographic bias with potential for harm.\nPrior bias measurement datasets are sensitive to perturbations in their\nmanually designed templates, therefore unreliable. To achieve reliability, we\nintroduce the Comprehensive Assessment of Language Model bias (CALM), a\nbenchmark dataset to quantify bias in LMs across three tasks. We integrate 16\nexisting datasets across different domains, such as Wikipedia and news\narticles, to filter 224 templates from which we construct a dataset of 78,400\nexamples. We compare the diversity of CALM with prior datasets on metrics such\nas average semantic similarity, and variation in template length, and test the\nsensitivity to small perturbations. We show that our dataset is more diverse\nand reliable than previous datasets, thus better capture the breadth of\nlinguistic variation required to reliably evaluate model bias. We evaluate 20\nlarge language models including six prominent families of LMs such as Llama-2.\nIn two LM series, OPT and Bloom, we found that larger parameter models are more\nbiased than lower parameter models. We found the T0 series of models to be the\nleast biased. Furthermore, we noticed a tradeoff between gender and racial bias\nwith increasing model size in some model series. The code is available at\nhttps://github.com/vipulgupta1011/CALM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vipul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkit_P/0/1/0/all/0/1\">Pranav Narayanan Venkit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurencon_H/0/1/0/all/0/1\">Hugo Lauren&#xe7;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1\">Shomir Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passonneau_R/0/1/0/all/0/1\">Rebecca J. Passonneau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Small and Fast BERT for Chinese Medical Punctuation Restoration. (arXiv:2308.12568v1 [cs.CL])","link":"http://arxiv.org/abs/2308.12568","description":"<p>In clinical dictation, utterances after automatic speech recognition (ASR)\nwithout explicit punctuation marks may lead to the misunderstanding of dictated\nreports. To give a precise and understandable clinical report with ASR,\nautomatic punctuation restoration is required. Considering a practical\nscenario, we propose a fast and light pre-trained model for Chinese medical\npunctuation restoration based on 'pretraining and fine-tuning' paradigm. In\nthis work, we distill pre-trained models by incorporating supervised\ncontrastive learning and a novel auxiliary pre-training task (Punctuation Mark\nPrediction) to make it well-suited for punctuation restoration. Our experiments\non various distilled models reveal that our model can achieve 95% performance\nwhile 10% model size relative to state-of-the-art Chinese RoBERTa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_T/0/1/0/all/0/1\">Tongtao Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1\">Chen Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhipeng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shilei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models. (arXiv:2308.12578v1 [cs.CL])","link":"http://arxiv.org/abs/2308.12578","description":"<p>Recent researches indicate that Pre-trained Large Language Models (LLMs)\npossess cognitive constructs similar to those observed in humans, prompting\nresearchers to investigate the cognitive aspects of LLMs. This paper focuses on\nexplicit and implicit social bias, a distinctive two-level cognitive construct\nin psychology. It posits that individuals' explicit social bias, which is their\nconscious expression of bias in the statements, may differ from their implicit\nsocial bias, which represents their unconscious bias. We propose a two-stage\napproach and discover a parallel phenomenon in LLMs known as \"re-judge\ninconsistency\" in social bias. In the initial stage, the LLM is tasked with\nautomatically completing statements, potentially incorporating implicit social\nbias. However, in the subsequent stage, the same LLM re-judges the biased\nstatement generated by itself but contradicts it. We propose that this re-judge\ninconsistency can be similar to the inconsistency between human's unaware\nimplicit social bias and their aware explicit social bias. Experimental\ninvestigations on ChatGPT and GPT-4 concerning common gender biases examined in\npsychology corroborate the highly stable nature of the re-judge inconsistency.\nThis finding may suggest that diverse cognitive constructs emerge as LLMs'\ncapabilities strengthen. Consequently, leveraging psychological theories can\nprovide enhanced insights into the underlying mechanisms governing the\nexpressions of explicit and implicit constructs in LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yachao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruifang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yuexian Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptMRG: Diagnosis-Driven Prompts for Medical Report Generation. (arXiv:2308.12604v1 [cs.CV])","link":"http://arxiv.org/abs/2308.12604","description":"<p>Automatic medical report generation (MRG) is of great research value as it\nhas the potential to relieve radiologists from the heavy burden of report\nwriting. Despite recent advancements, accurate MRG remains challenging due to\nthe need for precise clinical understanding and the identification of clinical\nfindings. Moreover, the imbalanced distribution of diseases makes the challenge\neven more pronounced, as rare diseases are underrepresented in training data,\nmaking their diagnostic performance unreliable. To address these challenges, we\npropose diagnosis-driven prompts for medical report generation (PromptMRG), a\nnovel framework that aims to improve the diagnostic accuracy of MRG with the\nguidance of diagnosis-aware prompts. Specifically, PromptMRG is based on\nencoder-decoder architecture with an extra disease classification branch. When\ngenerating reports, the diagnostic results from the classification branch are\nconverted into token prompts to explicitly guide the generation process. To\nfurther improve the diagnostic accuracy, we design cross-modal feature\nenhancement, which retrieves similar reports from the database to assist the\ndiagnosis of a query image by leveraging the knowledge from a pre-trained CLIP.\nMoreover, the disease imbalanced issue is addressed by applying an adaptive\nlogit-adjusted loss to the classification branch based on the individual\nlearning status of each disease, which overcomes the barrier of text decoder's\ninability to manipulate disease distributions. Experiments on two MRG\nbenchmarks show the effectiveness of the proposed method, where it obtains\nstate-of-the-art clinical efficacy performance on both datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Haibo Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_H/0/1/0/all/0/1\">Haoxuan Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advancing Hungarian Text Processing with HuSpaCy: Efficient and Accurate NLP Pipelines. (arXiv:2308.12635v1 [cs.CL])","link":"http://arxiv.org/abs/2308.12635","description":"<p>This paper presents a set of industrial-grade text processing models for\nHungarian that achieve near state-of-the-art performance while balancing\nresource efficiency and accuracy. Models have been implemented in the spaCy\nframework, extending the HuSpaCy toolkit with several improvements to its\narchitecture. Compared to existing NLP tools for Hungarian, all of our\npipelines feature all basic text processing steps including tokenization,\nsentence-boundary detection, part-of-speech tagging, morphological feature\ntagging, lemmatization, dependency parsing and named entity recognition with\nhigh accuracy and throughput. We thoroughly evaluated the proposed\nenhancements, compared the pipelines with state-of-the-art tools and\ndemonstrated the competitive performance of the new models in all text\npreprocessing steps. All experiments are reproducible and the pipelines are\nfreely available under a permissive license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orosz_G/0/1/0/all/0/1\">Gy&#xf6;rgy Orosz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szabo_G/0/1/0/all/0/1\">Gerg&#x151; Szab&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berkecz_P/0/1/0/all/0/1\">P&#xe9;ter Berkecz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szanto_Z/0/1/0/all/0/1\">Zsolt Sz&#xe1;nt&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farkas_R/0/1/0/all/0/1\">Rich&#xe1;rd Farkas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistic Method of Measuring Linguistic Productivity. (arXiv:2308.12643v1 [cs.CL])","link":"http://arxiv.org/abs/2308.12643","description":"<p>In this paper I propose a new way of measuring linguistic productivity that\nobjectively assesses the ability of an affix to be used to coin new complex\nwords and, unlike other popular measures, is not directly dependent upon token\nfrequency. Specifically, I suggest that linguistic productivity may be viewed\nas the probability of an affix to combine with a random base. The advantages of\nthis approach include the following. First, token frequency does not dominate\nthe productivity measure but naturally influences the sampling of bases.\nSecond, we are not just counting attested word types with an affix but rather\nsimulating the construction of these types and then checking whether they are\nattested in the corpus. Third, a corpus-based approach and randomised design\nassure that true neologisms and words coined long ago have equal chances to be\nselected. The proposed algorithm is evaluated both on English and Russian data.\nThe obtained results provide some valuable insights into the relation of\nlinguistic productivity to the number of types and tokens. It looks like\nburgeoning linguistic productivity manifests itself in an increasing number of\ntypes. However, this process unfolds in two stages: first comes the increase in\nhigh-frequency items, and only then follows the increase in low-frequency\nitems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Monakhov_S/0/1/0/all/0/1\">Sergei Monakhov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Chatter to Matter: Addressing Critical Steps of Emotion Recognition Learning in Task-oriented Dialogue. (arXiv:2308.12648v1 [cs.CL])","link":"http://arxiv.org/abs/2308.12648","description":"<p>Emotion recognition in conversations (ERC) is a crucial task for building\nhuman-like conversational agents. While substantial efforts have been devoted\nto ERC for chit-chat dialogues, the task-oriented counterpart is largely left\nunattended. Directly applying chit-chat ERC models to task-oriented dialogues\n(ToDs) results in suboptimal performance as these models overlook key features\nsuch as the correlation between emotions and task completion in ToDs. In this\npaper, we propose a framework that turns a chit-chat ERC model into a\ntask-oriented one, addressing three critical aspects: data, features and\nobjective. First, we devise two ways of augmenting rare emotions to improve ERC\nperformance. Second, we use dialogue states as auxiliary features to\nincorporate key information from the goal of the user. Lastly, we leverage a\nmulti-aspect emotion definition in ToDs to devise a multi-task learning\nobjective and a novel emotion-distance weighted loss function. Our framework\nyields significant improvements for a range of chit-chat ERC models on EmoWOZ,\na large-scale dataset for user emotion in ToDs. We further investigate the\ngeneralisability of the best resulting model to predict user satisfaction in\ndifferent ToD datasets. A comparison with supervised baselines shows a strong\nzero-shot capability, highlighting the potential usage of our framework in\nwider scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lubis_N/0/1/0/all/0/1\">Nurul Lubis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruppik_B/0/1/0/all/0/1\">Benjamin Ruppik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geishauser_C/0/1/0/all/0/1\">Christian Geishauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heck_M/0/1/0/all/0/1\">Michael Heck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hsien-chin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekerk_C/0/1/0/all/0/1\">Carel van Niekerk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vukovic_R/0/1/0/all/0/1\">Renato Vukovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasic_M/0/1/0/all/0/1\">Milica Ga&#x161;i&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Translation Faithfulness of Large Language Models via Augmenting Instructions. (arXiv:2308.12674v1 [cs.CL])","link":"http://arxiv.org/abs/2308.12674","description":"<p>Large Language Models (LLMs) present strong general capabilities, and a\ncurrent compelling challenge is stimulating their specialized capabilities,\nsuch as machine translation, through low-cost instruction tuning. The standard\ninstruction-following data is sequentially organized as the concatenation of an\ninstruction, an input, and a response. As the attention mechanism of LLMs has\nlimitations on local focus, LLMs tend to focus more on the words or sentences\nnearby at each position. This leads to a high risk of instruction forgetting\nduring decoding. To alleviate the above issues, We propose SWIE\n(Segment-Weighted Instruction Embedding) and an instruction-following dataset\nOVERMISS. SWIE improves the model instruction understanding by adding a global\ninstruction representation on the following input and response representations.\nOVERMISS improves model faithfulness by comparing over-translation and\nmiss-translation results with the correct translation. We apply our methods to\ntwo main-stream open-source LLMs, BLOOM and LLaMA. The experimental results\ndemonstrate significant improvements in translation performance with SWIE based\non BLOOMZ-3b, particularly in zero-shot and long text translations due to\nreduced instruction forgetting risk. Additionally, OVERMISS outperforms the\nbaseline in translation performance (e.g. an increase in BLEU scores from 0.69\nto 3.12 and an average improvement of 0.48 percentage comet scores for\nLLaMA-7b) with further enhancements seen in models combining OVERMISS and SWIE\n(e.g. the BLUE scores increase up to 0.56 from English to German across three\ndifferent backbones), and both exhibit improvements in the faithfulness metric\nbased on word alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yijie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yijin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harnessing the Power of David against Goliath: Exploring Instruction Data Generation without Using Closed-Source Models. (arXiv:2308.12711v1 [cs.CL])","link":"http://arxiv.org/abs/2308.12711","description":"<p>Instruction tuning is instrumental in enabling Large Language Models~(LLMs)\nto follow user instructions to complete various open-domain tasks. The success\nof instruction tuning depends on the availability of high-quality instruction\ndata. Owing to the exorbitant cost and substandard quality of human annotation,\nrecent works have been deeply engaged in the exploration of the utilization of\npowerful closed-source models to generate instruction data automatically.\nHowever, these methods carry potential risks arising from the usage\nrequirements of powerful closed-source models, which strictly forbid the\nutilization of their outputs to develop machine learning models. To deal with\nthis problem, in this work, we explore alternative approaches to generate\nhigh-quality instruction data that do not rely on closed-source models. Our\nexploration includes an investigation of various existing instruction\ngeneration methods, culminating in the integration of the most efficient\nvariant with two novel strategies to enhance the quality further. Evaluation\nresults from two benchmarks and the GPT-4 model demonstrate the effectiveness\nof our generated instruction data, which can outperform Alpaca, a method\nreliant on closed-source models. We hope that more progress can be achieved in\ngenerating high-quality instruction data without using closed-source models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jinxiong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qishen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhongyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guannan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion. (arXiv:2308.12734v1 [cs.SD])","link":"http://arxiv.org/abs/2308.12734","description":"<p>There are growing implications surrounding generative AI in the speech domain\nthat enable voice cloning and real-time voice conversion from one individual to\nanother. This technology poses a significant ethical threat and could lead to\nbreaches of privacy and misrepresentation, thus there is an urgent need for\nreal-time detection of AI-generated speech for DeepFake Voice Conversion. To\naddress the above emerging issues, the DEEP-VOICE dataset is generated in this\nstudy, comprised of real human speech from eight well-known figures and their\nspeech converted to one another using Retrieval-based Voice Conversion.\nPresenting as a binary classification problem of whether the speech is real or\nAI-generated, statistical analysis of temporal audio features through t-testing\nreveals that there are significantly different distributions. Hyperparameter\noptimisation is implemented for machine learning models to identify the source\nof speech. Following the training of 208 individual machine learning models\nover 10-fold cross validation, it is found that the Extreme Gradient Boosting\nmodel can achieve an average classification accuracy of 99.3% and can classify\nspeech in real-time, at around 0.004 milliseconds given one second of speech.\nAll data generated for this study is released publicly for future research on\nAI speech detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bird_J/0/1/0/all/0/1\">Jordan J. Bird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotfi_A/0/1/0/all/0/1\">Ahmad Lotfi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WavMark: Watermarking for Audio Generation. (arXiv:2308.12770v1 [cs.SD])","link":"http://arxiv.org/abs/2308.12770","description":"<p>Recent breakthroughs in zero-shot voice synthesis have enabled imitating a\nspeaker's voice using just a few seconds of recording while maintaining a high\nlevel of realism. Alongside its potential benefits, this powerful technology\nintroduces notable risks, including voice fraud and speaker impersonation.\nUnlike the conventional approach of solely relying on passive methods for\ndetecting synthetic data, watermarking presents a proactive and robust defence\nmechanism against these looming risks. This paper introduces an innovative\naudio watermarking framework that encodes up to 32 bits of watermark within a\nmere 1-second audio snippet. The watermark is imperceptible to human senses and\nexhibits strong resilience against various attacks. It can serve as an\neffective identifier for synthesized voices and holds potential for broader\napplications in audio copyright protection. Moreover, this framework boasts\nhigh flexibility, allowing for the combination of multiple watermark segments\nto achieve heightened robustness and expanded capacity. Utilizing 10 to\n20-second audio as the host, our approach demonstrates an average Bit Error\nRate (BER) of 0.48\\% across ten common attacks, a remarkable reduction of over\n2800\\% in BER compared to the state-of-the-art watermarking tool. See\nhttps://aka.ms/wavmark for demos of our work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xiaoyong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities. (arXiv:2308.12833v1 [cs.CL])","link":"http://arxiv.org/abs/2308.12833","description":"<p>Spurred by the recent rapid increase in the development and distribution of\nlarge language models (LLMs) across industry and academia, much recent work has\ndrawn attention to safety- and security-related threats and vulnerabilities of\nLLMs, including in the context of potentially criminal activities.\nSpecifically, it has been shown that LLMs can be misused for fraud,\nimpersonation, and the generation of malware; while other authors have\nconsidered the more general problem of AI alignment. It is important that\ndevelopers and practitioners alike are aware of security-related problems with\nsuch models. In this paper, we provide an overview of existing - predominantly\nscientific - efforts on identifying and mitigating threats and vulnerabilities\narising from LLMs. We present a taxonomy describing the relationship between\nthreats caused by the generative capabilities of LLMs, prevention measures\nintended to address such threats, and vulnerabilities arising from imperfect\nprevention measures. With our work, we hope to raise awareness of the\nlimitations of LLMs in light of such security concerns, among both experienced\ndevelopers and novel users of such technologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mozes_M/0/1/0/all/0/1\">Maximilian Mozes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuanli He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinberg_B/0/1/0/all/0/1\">Bennett Kleinberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffin_L/0/1/0/all/0/1\">Lewis D. Griffin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Similarity from Image Contents using Statistical and Semantic Analysis Techniques. (arXiv:2308.12842v1 [cs.CL])","link":"http://arxiv.org/abs/2308.12842","description":"<p>Plagiarism detection is one of the most researched areas among the Natural\nLanguage Processing(NLP) community. A good plagiarism detection covers all the\nNLP methods including semantics, named entities, paraphrases etc. and produces\ndetailed plagiarism reports. Detection of Cross Lingual Plagiarism requires\ndeep knowledge of various advanced methods and algorithms to perform effective\ntext similarity checking. Nowadays the plagiarists are also advancing\nthemselves from hiding the identity from being catch in such offense. The\nplagiarists are bypassed from being detected with techniques like paraphrasing,\nsynonym replacement, mismatching citations, translating one language to\nanother. Image Content Plagiarism Detection (ICPD) has gained importance,\nutilizing advanced image content processing to identify instances of plagiarism\nto ensure the integrity of image content. The issue of plagiarism extends\nbeyond textual content, as images such as figures, graphs, and tables also have\nthe potential to be plagiarized. However, image content plagiarism detection\nremains an unaddressed challenge. Therefore, there is a critical need to\ndevelop methods and systems for detecting plagiarism in image content. In this\npaper, the system has been implemented to detect plagiarism form contents of\nImages such as Figures, Graphs, Tables etc. Along with statistical algorithms\nsuch as Jaccard and Cosine, introducing semantic algorithms such as LSA, BERT,\nWordNet outperformed in detecting efficient and accurate plagiarism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_S/0/1/0/all/0/1\">Sagar Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Govilkar_S/0/1/0/all/0/1\">Sharvari Govilkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amin_D/0/1/0/all/0/1\">Dhiraj Amin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DS4DH at #SMM4H 2023: Zero-Shot Adverse Drug Events Normalization using Sentence Transformers and Reciprocal-Rank Fusion. (arXiv:2308.12877v1 [cs.CL])","link":"http://arxiv.org/abs/2308.12877","description":"<p>This paper outlines the performance evaluation of a system for adverse drug\nevent normalization, developed by the Data Science for Digital Health group for\nthe Social Media Mining for Health Applications 2023 shared task 5. Shared task\n5 targeted the normalization of adverse drug event mentions in Twitter to\nstandard concepts from the Medical Dictionary for Regulatory Activities\nterminology. Our system hinges on a two-stage approach: BERT fine-tuning for\nentity recognition, followed by zero-shot normalization using sentence\ntransformers and reciprocal-rank fusion. The approach yielded a precision of\n44.9%, recall of 40.5%, and an F1-score of 42.6%. It outperformed the median\nperformance in shared task 5 by 10% and demonstrated the highest performance\namong all participants. These results substantiate the effectiveness of our\napproach and its potential application for adverse drug event normalization in\nthe realm of social media text mining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_A/0/1/0/all/0/1\">Anthony Yazdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouhizadeh_H/0/1/0/all/0/1\">Hossein Rouhizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_D/0/1/0/all/0/1\">David Vicente Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teodoro_D/0/1/0/all/0/1\">Douglas Teodoro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inducing Causal Structure for Abstractive Text Summarization. (arXiv:2308.12888v1 [cs.CL])","link":"http://arxiv.org/abs/2308.12888","description":"<p>The mainstream of data-driven abstractive summarization models tends to\nexplore the correlations rather than the causal relationships. Among such\ncorrelations, there can be spurious ones which suffer from the language prior\nlearned from the training corpus and therefore undermine the overall\neffectiveness of the learned model. To tackle this issue, we introduce a\nStructural Causal Model (SCM) to induce the underlying causal structure of the\nsummarization data. We assume several latent causal factors and non-causal\nfactors, representing the content and style of the document and summary.\nTheoretically, we prove that the latent factors in our SCM can be identified by\nfitting the observed training data under certain conditions. On the basis of\nthis, we propose a Causality Inspired Sequence-to-Sequence model (CI-Seq2Seq)\nto learn the causal representations that can mimic the causal factors, guiding\nus to pursue causal information for summary generation. The key idea is to\nreformulate the Variational Auto-encoder (VAE) to fit the joint distribution of\nthe document and summary variables from the training corpus. Experimental\nresults on two widely used text summarization datasets demonstrate the\nadvantages of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Vote: Prompting for Rare Disease Identification. (arXiv:2308.12890v1 [cs.CL])","link":"http://arxiv.org/abs/2308.12890","description":"<p>The emergence of generative Large Language Models (LLMs) emphasizes the need\nfor accurate and efficient prompting approaches. LLMs are often applied in\nFew-Shot Learning (FSL) contexts, where tasks are executed with minimal\ntraining data. FSL has become popular in many Artificial Intelligence (AI)\nsubdomains, including AI for health. Rare diseases, affecting a small fraction\nof the population, inherently require FSL techniques due to limited data\navailability, though manual data collection and annotation is costly and\ntime-consuming. In this paper, we propose Models-Vote Prompting (MVP), a\nflexible prompting approach for improving the performance of LLM queries in FSL\nsettings. MVP works by prompting numerous LLMs to perform the same tasks and\nthen conducting a majority vote on the resulting outputs. This method achieves\nimproved results to any one model in the ensemble on one-shot rare disease\nidentification and classification tasks. We also release a novel rare disease\ndataset for FSL, available to those who agreed to the MIMIC-IV Data Use\nAgreement (DUA). Furthermore, in using MVP, each model is prompted multiple\ntimes, substantially increasing the time needed for manual annotation, and to\naddress this, we assess the feasibility of using JSON for automating generative\nLLM evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oniani_D/0/1/0/all/0/1\">David Oniani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilsman_J/0/1/0/all/0/1\">Jordan Hilsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Fengyi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1\">Shiven Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanshan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Document Page Classification: Design, Datasets, and Challenges. (arXiv:2308.12896v1 [cs.CV])","link":"http://arxiv.org/abs/2308.12896","description":"<p>This paper highlights the need to bring document classification benchmarking\ncloser to real-world applications, both in the nature of data tested ($X$:\nmulti-channel, multi-paged, multi-industry; $Y$: class distributions and label\nset variety) and in classification tasks considered ($f$: multi-page document,\npage stream, and document bundle classification, ...). We identify the lack of\npublic multi-page document classification datasets, formalize different\nclassification tasks arising in application scenarios, and motivate the value\nof targeting efficient multi-page document representations. An experimental\nstudy on proposed multi-page document classification datasets demonstrates that\ncurrent benchmarks have become irrelevant and need to be updated to evaluate\ncomplete documents, as they naturally occur in practice. This reality check\nalso calls for more mature evaluation methodologies, covering calibration\nevaluation, inference complexity (time-memory), and a range of realistic\ndistribution shifts (e.g., born-digital vs. scanning noise, shifting page\norder). Our study ends on a hopeful note by recommending concrete avenues for\nfuture improvements.}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Landeghem_J/0/1/0/all/0/1\">Jordy Van Landeghem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1\">Sanket Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blaschko_M/0/1/0/all/0/1\">Matthew B. Blaschko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?. (arXiv:2308.12898v1 [cs.MM])","link":"http://arxiv.org/abs/2308.12898","description":"<p>The multimedia community has shown a significant interest in perceiving and\nrepresenting the physical world with multimodal pretrained neural network\nmodels, and among them, the visual-language pertaining (VLP) is, currently, the\nmost captivating topic. However, there have been few endeavors dedicated to the\nexploration of 1) whether essential linguistic knowledge (e.g., semantics and\nsyntax) can be extracted during VLP, and 2) how such linguistic knowledge\nimpact or enhance the multimodal alignment. In response, here we aim to\nelucidate the impact of comprehensive linguistic knowledge, including semantic\nexpression and syntactic structure, on multimodal alignment. Specifically, we\ndesign and release the SNARE, the first large-scale multimodal alignment\nprobing benchmark, to detect the vital linguistic components, e.g., lexical,\nsemantic, and syntax knowledge, containing four tasks: Semantic structure,\nNegation logic, Attribute ownership, and Relationship composition. Based on our\nproposed probing benchmarks, our holistic analyses of five advanced VLP models\nillustrate that the VLP model: i) shows insensitivity towards complex syntax\nstructures and relies on content words for sentence comprehension; ii)\ndemonstrates limited comprehension of combinations between sentences and\nnegations; iii) faces challenges in determining the presence of actions or\nspatial relationships within visual information and struggles with verifying\nthe correctness of triple combinations. We make our benchmark and code\navailable at \\url{https://github.com/WangFei-2019/SNARE/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jun Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diagnosing Infeasible Optimization Problems Using Large Language Models. (arXiv:2308.12923v1 [cs.HC])","link":"http://arxiv.org/abs/2308.12923","description":"<p>Decision-making problems can be represented as mathematical optimization\nmodels, finding wide applications in fields such as economics, engineering and\nmanufacturing, transportation, and health care. Optimization models are\nmathematical abstractions of the problem of making the best decision while\nsatisfying a set of requirements or constraints. One of the primary barriers to\ndeploying these models in practice is the challenge of helping practitioners\nunderstand and interpret such models, particularly when they are infeasible,\nmeaning no decision satisfies all the constraints. Existing methods for\ndiagnosing infeasible optimization models often rely on expert systems,\nnecessitating significant background knowledge in optimization. In this paper,\nwe introduce OptiChat, a first-of-its-kind natural language-based system\nequipped with a chatbot GUI for engaging in interactive conversations about\ninfeasible optimization models. OptiChat can provide natural language\ndescriptions of the optimization model itself, identify potential sources of\ninfeasibility, and offer suggestions to make the model feasible. The\nimplementation of OptiChat is built on GPT-4, which interfaces with an\noptimization solver to identify the minimal subset of constraints that render\nthe entire optimization problem infeasible, also known as the Irreducible\nInfeasible Subset (IIS). We utilize few-shot learning, expert chain-of-thought,\nkey-retrieve, and sentiment prompts to enhance OptiChat's reliability. Our\nexperiments demonstrate that OptiChat assists both expert and non-expert users\nin improving their understanding of the optimization models, enabling them to\nquickly identify the sources of infeasibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constante_Flores_G/0/1/0/all/0/1\">Gonzalo E. Constante-Flores</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Can Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code Llama: Open Foundation Models for Code. (arXiv:2308.12950v1 [cs.CL])","link":"http://arxiv.org/abs/2308.12950","description":"<p>We release Code Llama, a family of large language models for code based on\nLlama 2 providing state-of-the-art performance among open models, infilling\ncapabilities, support for large input contexts, and zero-shot instruction\nfollowing ability for programming tasks. We provide multiple flavors to cover a\nwide range of applications: foundation models (Code Llama), Python\nspecializations (Code Llama - Python), and instruction-following models (Code\nLlama - Instruct) with 7B, 13B and 34B parameters each. All models are trained\non sequences of 16k tokens and show improvements on inputs with up to 100k\ntokens. 7B and 13B Code Llama and Code Llama - Instruct variants support\ninfilling based on surrounding content. Code Llama reaches state-of-the-art\nperformance among open models on several code benchmarks, with scores of up to\n53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python\n7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform\nevery other publicly available model on MultiPL-E. We release Code Llama under\na permissive license that allows for both research and commercial use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roziere_B/0/1/0/all/0/1\">Baptiste Rozi&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehring_J/0/1/0/all/0/1\">Jonas Gehring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gloeckle_F/0/1/0/all/0/1\">Fabian Gloeckle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sootla_S/0/1/0/all/0/1\">Sten Sootla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gat_I/0/1/0/all/0/1\">Itai Gat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiaoqing Ellen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rapin_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;my Rapin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozhevnikov_A/0/1/0/all/0/1\">Artyom Kozhevnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evtimov_I/0/1/0/all/0/1\">Ivan Evtimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitton_J/0/1/0/all/0/1\">Joanna Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_M/0/1/0/all/0/1\">Manish Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_C/0/1/0/all/0/1\">Cristian Canton Ferrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grattafiori_A/0/1/0/all/0/1\">Aaron Grattafiori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Wenhan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Defossez_A/0/1/0/all/0/1\">Alexandre D&#xe9;fossez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azhar_F/0/1/0/all/0/1\">Faisal Azhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Touvron_H/0/1/0/all/0/1\">Hugo Touvron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1\">Louis Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usunier_N/0/1/0/all/0/1\">Nicolas Usunier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities. (arXiv:2308.12966v1 [cs.CV])","link":"http://arxiv.org/abs/2308.12966","description":"<p>We introduce the Qwen-VL series, a set of large-scale vision-language models\ndesigned to perceive and understand both text and images. Comprising Qwen-VL\nand Qwen-VL-Chat, these models exhibit remarkable performance in tasks like\nimage captioning, question answering, visual localization, and flexible\ninteraction. The evaluation covers a wide range of tasks including zero-shot\ncaptioning, visual or document visual question answering, and grounding. We\ndemonstrate the Qwen-VL outperforms existing Large Vision Language Models\n(LVLMs). We present their architecture, training, capabilities, and\nperformance, highlighting their contributions to advancing multimodal\nartificial intelligence. Code, demo and models are available at\nhttps://github.com/QwenLM/Qwen-VL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jinze Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Shuai Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shusheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Sinan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models. (arXiv:2201.05337v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05337","description":"<p>Controllable Text Generation (CTG) is emerging area in the field of natural\nlanguage generation (NLG). It is regarded as crucial for the development of\nadvanced text generation technologies that better meet the specific constraints\nin practical applications. In recent years, methods using large-scale\npre-trained language models (PLMs), in particular the widely used\ntransformer-based PLMs, have become a new paradigm of NLG, allowing generation\nof more diverse and fluent text. However, due to the limited level of\ninterpretability of deep neural networks, the controllability of these methods\nneed to be guaranteed. To this end, controllable text generation using\ntransformer-based PLMs has become a rapidly growing yet challenging new\nresearch hotspot. A diverse range of approaches have emerged in the recent 3-4\nyears, targeting different CTG tasks that require different types of controlled\nconstraints. In this paper, we present a systematic critical review on the\ncommon tasks, main approaches, and evaluation methods in this area. Finally, we\ndiscuss the challenges that the field is facing, and put forward various\npromising future directions. To the best of our knowledge, this is the first\nsurvey paper to summarize the state-of-the-art CTG techniques from the\nperspective of Transformer-based PLMs. We hope it can help researchers and\npractitioners in the related fields to quickly track the academic and\ntechnological frontier, providing them with a landscape of the area and a\nroadmap for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Haolin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaoyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Speech Recognition for Speech Assessment of Persian Preschool Children. (arXiv:2203.12886v10 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12886","description":"<p>Preschool evaluation is crucial because it gives teachers and parents\ninfluential knowledge about children's growth and development. The COVID-19\npandemic has highlighted the necessity of online assessment for preschool\nchildren. One of the areas that should be tested is their ability to speak.\nEmploying an Automatic Speech Recognition (ASR) system would not help since\nthey are pre-trained on voices that differ from children's in terms of\nfrequency and amplitude. Because most of these are pre-trained with data in a\nspecific range of amplitude, their objectives do not make them ready for voices\nin different amplitudes. To overcome this issue, we added a new objective to\nthe masking objective of the Wav2Vec 2.0 model called Random Frequency Pitch\n(RFP). In addition, we used our newly introduced dataset to fine-tune our model\nfor Meaningless Words (MW) and Rapid Automatic Naming (RAN) tests. Using\nmasking in concatenation with RFP outperforms the masking objective of Wav2Vec\n2.0 by reaching a Word Error Rate (WER) of 1.35. Our new approach reaches a WER\nof 6.45 on the Persian section of the CommonVoice dataset. Furthermore, our\nnovel methodology produces positive outcomes in zero- and few-shot scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abaskohi_A/0/1/0/all/0/1\">Amirhossein Abaskohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortazavi_F/0/1/0/all/0/1\">Fatemeh Mortazavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1\">Hadi Moradi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion. (arXiv:2210.08471v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08471","description":"<p>Transformer-based pre-trained models like BERT have achieved great progress\non Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also\nshown general benefits in multiple NLP tasks. However, how to efficiently\nintegrate dependency prior structure into pre-trained models to better model\ncomplex semantic matching relations is still unsettled. In this paper, we\npropose the \\textbf{D}ependency-Enhanced \\textbf{A}daptive \\textbf{F}usion\n\\textbf{A}ttention (\\textbf{DAFA}), which explicitly introduces dependency\nstructure into pre-trained models and adaptively fuses it with semantic\ninformation. Specifically, \\textbf{\\emph{(i)}} DAFA first proposes a\nstructure-sensitive paradigm to construct a dependency matrix for calibrating\nattention weights. It adopts an adaptive fusion module to integrate the\nobtained dependency information and the original semantic signals. Moreover,\nDAFA reconstructs the attention calculation flow and provides better\ninterpretability. By applying it on BERT, our method achieves state-of-the-art\nor competitive performance on 10 public datasets, demonstrating the benefits of\nadaptively fusing dependency structure in semantic matching task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jian Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Di Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rumei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1\">Minlong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yongxin Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure-CLIP: Towards Scene Graph Knowledge to Enhance Multi-modal Structured Representations. (arXiv:2305.06152v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06152","description":"<p>Large-scale vision-language pre-training has achieved significant performance\nin multi-modal understanding and generation tasks. However, existing methods\noften perform poorly on image-text matching tasks that require structured\nrepresentations, i.e., representations of objects, attributes, and relations.\nPrevious models cannot make a distinction between ``An astronaut rides a horse\"\nand ``A horse rides an astronaut\". This is because they fail to fully leverage\nstructured knowledge when learning representations in multi-modal scenarios. In\nthis paper, we present an end-to-end framework Structure-CLIP, which integrates\nScene Graph Knowledge (SGK) to enhance multi-modal structured representations.\nFirstly, we use scene graphs to guide the construction of semantic negative\nexamples, which results in an increased emphasis on learning structured\nrepresentations. Moreover, a Knowledge-Enhance Encoder (KEE) is proposed to\nleverage SGK as input to further enhance structured representations. To verify\nthe effectiveness of the proposed framework, we pre-train our model with the\naforementioned approaches and conduct experiments on downstream tasks.\nExperimental results demonstrate that Structure-CLIP achieves state-of-the-art\n(SOTA) performance on VG-Attribution and VG-Relation datasets, with 12.5% and\n4.1% ahead of the multi-modal SOTA model respectively. Meanwhile, the results\non MSCOCO indicate that Structure-CLIP significantly enhances the structured\nrepresentations while maintaining the ability of general representations. Our\ncode will be available soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiji Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongsheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weijie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tangjie Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers. (arXiv:2306.04504v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.04504","description":"<p>ChatGPT is a large language model developed by OpenAI. Despite its impressive\nperformance across various tasks, no prior work has investigated its capability\nin the biomedical domain yet. To this end, this paper aims to evaluate the\nperformance of ChatGPT on various benchmark biomedical tasks, such as relation\nextraction, document classification, question answering, and summarization. To\nthe best of our knowledge, this is the first work that conducts an extensive\nevaluation of ChatGPT in the biomedical domain. Interestingly, we find based on\nour evaluation that in biomedical datasets that have smaller training sets,\nzero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative\ntransformer models, such as BioGPT and BioBART. This suggests that ChatGPT's\npre-training on large text corpora makes it quite specialized even in the\nbiomedical domain. Our findings demonstrate that ChatGPT has the potential to\nbe a valuable tool for various tasks in the biomedical domain that lack large\nannotated data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jahan_I/0/1/0/all/0/1\">Israt Jahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskar_M/0/1/0/all/0/1\">Md Tahmid Rahman Laskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Chun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jimmy Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts. (arXiv:2306.04528v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.04528","description":"<p>The increasing reliance on Large Language Models (LLMs) across academia and\nindustry necessitates a comprehensive understanding of their robustness to\nprompts. In response to this vital need, we introduce PromptBench, a robustness\nbenchmark designed to measure LLMs' resilience to adversarial prompts. This\nstudy uses a plethora of adversarial textual attacks targeting prompts across\nmultiple levels: character, word, sentence, and semantic. These prompts are\nthen employed in diverse tasks, such as sentiment analysis, natural language\ninference, reading comprehension, machine translation, and math\nproblem-solving. Our study generates 4,032 adversarial prompts, meticulously\nevaluated over 8 tasks and 13 datasets, with 567,084 test samples in total. Our\nfindings demonstrate that contemporary LLMs are vulnerable to adversarial\nprompts. Furthermore, we present comprehensive analysis to understand the\nmystery behind prompt robustness and its transferability. We then offer\ninsightful robustness analysis and pragmatic recommendations for prompt\ncomposition, beneficial to both researchers and everyday users. We make our\ncode, prompts, and methodologies to generate adversarial prompts publicly\naccessible, thereby enabling and encouraging collaborative exploration in this\npivotal field: https://github.com/microsoft/promptbench.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kaijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiaheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zichen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Massive Scale Semantic Similarity Dataset of Historical English. (arXiv:2306.17810v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.17810","description":"<p>A diversity of tasks use language models trained on semantic similarity data.\nWhile there are a variety of datasets that capture semantic similarity, they\nare either constructed from modern web data or are relatively small datasets\ncreated in the past decade by human annotators. This study utilizes a novel\nsource, newly digitized articles from off-copyright, local U.S. newspapers, to\nassemble a massive-scale semantic similarity dataset spanning 70 years from\n1920 to 1989 and containing nearly 400M positive semantic similarity pairs.\nHistorically, around half of articles in U.S. local newspapers came from\nnewswires like the Associated Press. While local papers reproduced articles\nfrom the newswire, they wrote their own headlines, which form abstractive\nsummaries of the associated articles. We associate articles and their headlines\nby exploiting document layouts and language understanding. We then use deep\nneural methods to detect which articles are from the same underlying source, in\nthe presence of substantial noise and abridgement. The headlines of reproduced\narticles form positive semantic similarity pairs. The resulting publicly\navailable HEADLINES dataset is significantly larger than most existing semantic\nsimilarity datasets and covers a much longer span of time. It will facilitate\nthe application of contrastively trained semantic similarity models to a\nvariety of tasks, including the study of semantic change across space and time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silcock_E/0/1/0/all/0/1\">Emily Silcock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dell_M/0/1/0/all/0/1\">Melissa Dell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language is All a Graph Needs. (arXiv:2308.07134v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.07134","description":"<p>The emergence of large-scale pre-trained language models, such as ChatGPT,\nhas revolutionized various research fields in artificial intelligence.\nTransformers-based large language models (LLMs) have gradually replaced CNNs\nand RNNs to unify fields of computer vision and natural language processing.\nCompared with the data that exists relatively independently such as images,\nvideos or texts, graph is a type of data that contains rich structural and\nrelational information. Meanwhile, natural language, as one of the most\nexpressive mediums, excels in describing complex structures. However, existing\nwork on incorporating graph learning problems into the generative language\nmodeling framework remains very limited. As the importance of large language\nmodels continues to grow, it becomes essential to explore whether LLMs can also\nreplace GNNs as the foundation model for graphs. In this paper, we propose\nInstructGLM (Instruction-finetuned Graph Language Model), systematically design\nhighly scalable prompts based on natural language instructions, and use natural\nlanguage to describe the geometric structure and node features of the graph for\ninstruction tuning an LLM to perform learning and inference on graphs in a\ngenerative manner. Our method exceeds all competitive GNN baselines on\nogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of\nour method and sheds light on generative large language models as the\nfoundation model for graph machine learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Ruosong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Caiqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Runhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue. (arXiv:2308.08043v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.08043","description":"<p>Large Language Models (LLMs), such as ChatGPT, are becoming increasingly\nsophisticated, demonstrating capabilities that closely resemble those of\nhumans. These AI models are playing an essential role in assisting humans with\na wide array of tasks in daily life. A significant application of AI is its use\nas a chat agent, responding to human inquiries across various domains. Current\nLLMs have shown proficiency in answering general questions. However, basic\nquestion-answering dialogue often falls short in complex diagnostic scenarios,\nsuch as legal or medical consultations. These scenarios typically necessitate\nTask-Oriented Dialogue (TOD), wherein an AI chat agent needs to proactively\npose questions and guide users towards specific task completion. Previous\nfine-tuning models have underperformed in TOD, and current LLMs do not\ninherently possess this capability. In this paper, we introduce DiagGPT\n(Dialogue in Diagnosis GPT), an innovative method that extends LLMs to TOD\nscenarios. Our experiments reveal that DiagGPT exhibits outstanding performance\nin conducting TOD with users, demonstrating its potential for practical\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Lang Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeamlessM4T-Massively Multilingual & Multimodal Machine Translation. (arXiv:2308.11596v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.11596","description":"<p>What does it take to create the Babel Fish, a tool that can help individuals\ntranslate speech between any two languages? While recent breakthroughs in\ntext-based models have pushed machine translation coverage beyond 200\nlanguages, unified speech-to-speech translation models have yet to achieve\nsimilar strides. More specifically, conventional speech-to-speech translation\nsystems rely on cascaded systems that perform translation progressively,\nputting high-performing unified systems out of reach. To address these gaps, we\nintroduce SeamlessM4T, a single model that supports speech-to-speech\ntranslation, speech-to-text translation, text-to-speech translation,\ntext-to-text translation, and automatic speech recognition for up to 100\nlanguages. To build this, we used 1 million hours of open speech audio data to\nlearn self-supervised speech representations with w2v-BERT 2.0. Subsequently,\nwe created a multimodal corpus of automatically aligned speech translations.\nFiltered and combined with human-labeled and pseudo-labeled data, we developed\nthe first multilingual system capable of translating from and into English for\nboth speech and text. On FLEURS, SeamlessM4T sets a new standard for\ntranslations into multiple target languages, achieving an improvement of 20%\nBLEU over the previous SOTA in direct speech-to-text translation. Compared to\nstrong cascaded models, SeamlessM4T improves the quality of into-English\ntranslation by 1.3 BLEU points in speech-to-text and by 2.6 ASR-BLEU points in\nspeech-to-speech. Tested for robustness, our system performs better against\nbackground noises and speaker variations in speech-to-text tasks compared to\nthe current SOTA model. Critically, we evaluated SeamlessM4T on gender bias and\nadded toxicity to assess translation safety. Finally, all contributions in this\nwork are open-sourced and accessible at\nhttps://github.com/facebookresearch/seamless_communication\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Communication_S/0/1/0/all/0/1\">Seamless Communication</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrault_L/0/1/0/all/0/1\">Lo&#xef;c Barrault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1\">Yu-An Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meglioli_M/0/1/0/all/0/1\">Mariano Cora Meglioli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dale_D/0/1/0/all/0/1\">David Dale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1\">Ning Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duquenne_P/0/1/0/all/0/1\">Paul-Ambroise Duquenne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsahar_H/0/1/0/all/0/1\">Hady Elsahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heffernan_K/0/1/0/all/0/1\">Kevin Heffernan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1\">John Hoffman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klaiber_C/0/1/0/all/0/1\">Christopher Klaiber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pengwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Licht_D/0/1/0/all/0/1\">Daniel Licht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maillard_J/0/1/0/all/0/1\">Jean Maillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakotoarison_A/0/1/0/all/0/1\">Alice Rakotoarison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadagopan_K/0/1/0/all/0/1\">Kaushik Ram Sadagopan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenzek_G/0/1/0/all/0/1\">Guillaume Wenzek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_E/0/1/0/all/0/1\">Ethan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akula_B/0/1/0/all/0/1\">Bapi Akula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng-Jen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hachem_N/0/1/0/all/0/1\">Naji El Hachem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellis_B/0/1/0/all/0/1\">Brian Ellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_G/0/1/0/all/0/1\">Gabriel Mejia Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haaheim_J/0/1/0/all/0/1\">Justin Haaheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansanti_P/0/1/0/all/0/1\">Prangthip Hansanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howes_R/0/1/0/all/0/1\">Russ Howes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Bernie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_M/0/1/0/all/0/1\">Min-Jae Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Somya Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalbassi_E/0/1/0/all/0/1\">Elahe Kalbassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kallet_A/0/1/0/all/0/1\">Amanda Kallet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulikov_I/0/1/0/all/0/1\">Ilia Kulikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_J/0/1/0/all/0/1\">Janice Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Daniel Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xutai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavlyutov_R/0/1/0/all/0/1\">Ruslan Mavlyutov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peloquin_B/0/1/0/all/0/1\">Benjamin Peloquin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramadan_M/0/1/0/all/0/1\">Mohamed Ramadan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1\">Abinesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Anna Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Kevin Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Tuan Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tufanov_I/0/1/0/all/0/1\">Igor Tufanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogeti_V/0/1/0/all/0/1\">Vish Vogeti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wood_C/0/1/0/all/0/1\">Carleigh Wood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yilin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bokai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andrews_P/0/1/0/all/0/1\">Pierre Andrews</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balioglu_C/0/1/0/all/0/1\">Can Balioglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>, et al. (16 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models. (arXiv:2308.11764v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.11764","description":"<p>Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP). Although convenient for research and practical applications, open-source\nLLMs with fewer parameters often suffer from severe hallucinations compared to\ntheir larger counterparts. This paper focuses on measuring and reducing\nhallucinations in BLOOM 7B, a representative of such weaker open-source LLMs\nthat are publicly available for research and commercial applications. We\nintroduce HaloCheck, a lightweight BlackBox knowledge-free framework designed\nto quantify the severity of hallucinations in LLMs. Additionally, we explore\ntechniques like knowledge injection and teacher-student approaches to alleviate\nhallucinations in low-parameter LLMs. Our experiments effectively demonstrate\nthe reduction of hallucinations in challenging domains for these LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elaraby_M/0/1/0/all/0/1\">Mohamed Elaraby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Mengyin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunn_J/0/1/0/all/0/1\">Jacob Dunn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xueying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shizhu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-08-24T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/"}}]}]}