{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-11-03T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"tmn at #SMM4H 2023: Comparing Text Preprocessing Techniques for Detecting Tweets Self-reporting a COVID-19 Diagnosis. (arXiv:2311.00732v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00732","description":"<p>The paper describes a system developed for Task 1 at SMM4H 2023. The goal of\nthe task is to automatically distinguish tweets that self-report a COVID-19\ndiagnosis (for example, a positive test, clinical diagnosis, or\nhospitalization) from those that do not. We investigate the use of different\ntechniques for preprocessing tweets using four transformer-based models. The\nensemble of fine-tuned language models obtained an F1-score of 84.5%, which is\n4.1% higher than the average value.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glazkova_A/0/1/0/all/0/1\">Anna Glazkova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Large Language Models Design Accurate Label Functions?. (arXiv:2311.00739v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00739","description":"<p>Programmatic weak supervision methodologies facilitate the expedited labeling\nof extensive datasets through the use of label functions (LFs) that encapsulate\nheuristic data sources. Nonetheless, the creation of precise LFs necessitates\ndomain expertise and substantial endeavors. Recent advances in pre-trained\nlanguage models (PLMs) have exhibited substantial potential across diverse\ntasks. However, the capacity of PLMs to autonomously formulate accurate LFs\nremains an underexplored domain. In this research, we address this gap by\nintroducing DataSculpt, an interactive framework that harnesses PLMs for the\nautomated generation of LFs. Within DataSculpt, we incorporate an array of\nprompting techniques, instance selection strategies, and LF filtration methods\nto explore the expansive design landscape. Ultimately, we conduct a thorough\nassessment of DataSculpt's performance on 12 real-world datasets, encompassing\na range of tasks. This evaluation unveils both the strengths and limitations of\ncontemporary PLMs in LF design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_N/0/1/0/all/0/1\">Naiqing Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kaiwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koudas_N/0/1/0/all/0/1\">Nick Koudas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenges for Linguistically-Driven Computer-Based Sign Recognition from Continuous Signing for American Sign Language. (arXiv:2311.00762v1 [cs.CV])","link":"http://arxiv.org/abs/2311.00762","description":"<p>There have been recent advances in computer-based recognition of isolated,\ncitation-form signs from video. There are many challenges for such a task, not\nleast the naturally occurring inter- and intra- signer synchronic variation in\nsign production, including sociolinguistic variation in the realization of\ncertain signs. However, there are several significant factors that make\nrecognition of signs from continuous signing an even more difficult problem.\nThis article presents an overview of such challenges, based in part on findings\nfrom a large corpus of linguistically annotated video data for American Sign\nLanguage (ASL). Some linguistic regularities in the structure of signs that can\nboost handshape and sign recognition are also discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neidle_C/0/1/0/all/0/1\">Carol Neidle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model Training Paradigms for Clinical Feature Embeddings. (arXiv:2311.00768v1 [cs.LG])","link":"http://arxiv.org/abs/2311.00768","description":"<p>In research areas with scarce data, representation learning plays a\nsignificant role. This work aims to enhance representation learning for\nclinical time series by deriving universal embeddings for clinical features,\nsuch as heart rate and blood pressure. We use self-supervised training\nparadigms for language models to learn high-quality clinical feature\nembeddings, achieving a finer granularity than existing time-step and\npatient-level representation learning. We visualize the learnt embeddings via\nunsupervised dimension reduction techniques and observe a high degree of\nconsistency with prior clinical knowledge. We also evaluate the model\nperformance on the MIMIC-III benchmark and demonstrate the effectiveness of\nusing clinical feature embeddings. We publish our code online for replication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yurong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burger_M/0/1/0/all/0/1\">Manuel Burger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratsch_G/0/1/0/all/0/1\">Gunnar R&#xe4;tsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsova_R/0/1/0/all/0/1\">Rita Kuznetsova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Construction Artifacts in Metaphor Identification Datasets. (arXiv:2311.00790v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00790","description":"<p>Metaphor identification aims at understanding whether a given expression is\nused figuratively in context. However, in this paper we show how existing\nmetaphor identification datasets can be gamed by fully ignoring the potential\nmetaphorical expression or the context in which it occurs. We test this\nhypothesis in a variety of datasets and settings, and show that metaphor\nidentification systems based on language models without complete information\ncan be competitive with those using the full context. This is due to the\nconstruction procedures to build such datasets, which introduce unwanted biases\nfor positive and negative classes. Finally, we test the same hypothesis on\ndatasets that are carefully sampled from natural corpora and where this bias is\nnot present, making these datasets more challenging and reliable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boisson_J/0/1/0/all/0/1\">Joanne Boisson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espinosa_Anke_L/0/1/0/all/0/1\">Luis Espinosa-Anke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrated Seq2seq Models for Efficient and Generalizable Ultra-fine Entity Typing. (arXiv:2311.00835v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00835","description":"<p>Ultra-fine entity typing plays a crucial role in information extraction by\npredicting fine-grained semantic types for entity mentions in text. However,\nthis task poses significant challenges due to the massive number of entity\ntypes in the output space. The current state-of-the-art approaches, based on\nstandard multi-label classifiers or cross-encoder models, suffer from poor\ngeneralization performance or inefficient inference. In this paper, we present\nCASENT, a seq2seq model designed for ultra-fine entity typing that predicts\nultra-fine types with calibrated confidence scores. Our model takes an entity\nmention as input and employs constrained beam search to generate multiple types\nautoregressively. The raw sequence probabilities associated with the predicted\ntypes are then transformed into confidence scores using a novel calibration\nmethod. We conduct extensive experiments on the UFET dataset which contains\nover 10k types. Our method outperforms the previous state-of-the-art in terms\nof F1 score and calibration error, while achieving an inference speedup of over\n50 times. Additionally, we demonstrate the generalization capabilities of our\nmodel by evaluating it in zero-shot and few-shot settings on five specialized\ndomain entity typing datasets that are unseen during training. Remarkably, our\nmodel outperforms large language models with 10 times more parameters in the\nzero-shot setting, and when fine-tuned on 50 examples, it significantly\noutperforms ChatGPT on all datasets. Our code, models and demo are available at\nhttps://github.com/yanlinf/CASENT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yanlin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratapa_A/0/1/0/all/0/1\">Adithya Pratapa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortensen_D/0/1/0/all/0/1\">David R Mortensen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Dynamics of Contextual N-Grams in Language Models. (arXiv:2311.00863v1 [cs.LG])","link":"http://arxiv.org/abs/2311.00863","description":"<p>Prior work has shown the existence of contextual neurons in language models,\nincluding a neuron that activates on German text. We show that this neuron\nexists within a broader contextual n-gram circuit: we find late layer neurons\nwhich recognize and continue n-grams common in German text, but which only\nactivate if the German neuron is active. We investigate the formation of this\ncircuit throughout training and find that it is an example of what we call a\nsecond-order circuit. In particular, both the constituent n-gram circuits and\nthe German detection circuit which culminates in the German neuron form with\nindependent functions early in training - the German detection circuit\npartially through modeling German unigram statistics, and the n-grams by\nboosting appropriate completions. Only after both circuits have already formed\ndo they fit together into a second-order circuit. Contrary to the hypotheses\npresented in prior work, we find that the contextual n-gram circuit forms\ngradually rather than in a sudden phase transition. We further present a range\nof anomalous observations such as a simultaneous phase transition in many tasks\ncoinciding with the learning rate warm-up, and evidence that many context\nneurons form simultaneously early in training but are later unlearned.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quirke_L/0/1/0/all/0/1\">Lucia Quirke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heindrich_L/0/1/0/all/0/1\">Lovis Heindrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurnee_W/0/1/0/all/0/1\">Wes Gurnee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nanda_N/0/1/0/all/0/1\">Neel Nanda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Disfluency Detection from Untranscribed Speech. (arXiv:2311.00867v1 [eess.AS])","link":"http://arxiv.org/abs/2311.00867","description":"<p>Speech disfluencies, such as filled pauses or repetitions, are disruptions in\nthe typical flow of speech. Stuttering is a speech disorder characterized by a\nhigh rate of disfluencies, but all individuals speak with some disfluencies and\nthe rates of disfluencies may by increased by factors such as cognitive load.\nClinically, automatic disfluency detection may help in treatment planning for\nindividuals who stutter. Outside of the clinic, automatic disfluency detection\nmay serve as a pre-processing step to improve natural language understanding in\ndownstream applications. With this wide range of applications in mind, we\ninvestigate language, acoustic, and multimodal methods for frame-level\nautomatic disfluency detection and categorization. Each of these methods relies\non audio as an input. First, we evaluate several automatic speech recognition\n(ASR) systems in terms of their ability to transcribe disfluencies, measured\nusing disfluency error rates. We then use these ASR transcripts as input to a\nlanguage-based disfluency detection model. We find that disfluency detection\nperformance is largely limited by the quality of transcripts and alignments. We\nfind that an acoustic-based approach that does not require transcription as an\nintermediate step outperforms the ASR language approach. Finally, we present\nmultimodal architectures which we find improve disfluency detection performance\nover the unimodal approaches. Ultimately, this work introduces novel approaches\nfor automatic frame-level disfluency and categorization. In the long term, this\nwill help researchers incorporate automatic disfluency detection into a range\nof applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Romana_A/0/1/0/all/0/1\">Amrit Romana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koishida_K/0/1/0/all/0/1\">Kazuhito Koishida</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Provost_E/0/1/0/all/0/1\">Emily Mower Provost</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models. (arXiv:2311.00871v1 [cs.LG])","link":"http://arxiv.org/abs/2311.00871","description":"<p>Transformer models, notably large language models (LLMs), have the remarkable\nability to perform in-context learning (ICL) -- to perform new tasks when\nprompted with unseen input-output examples without any explicit model training.\nIn this work, we study how effectively transformers can bridge between their\npretraining data mixture, comprised of multiple distinct task families, to\nidentify and learn new tasks in-context which are both inside and outside the\npretraining distribution. Building on previous work, we investigate this\nquestion in a controlled setting, where we study transformer models trained on\nsequences of $(x, f(x))$ pairs rather than natural language. Our empirical\nresults show transformers demonstrate near-optimal unsupervised model selection\ncapabilities, in their ability to first in-context identify different task\nfamilies and in-context learn within them when the task families are\nwell-represented in their pretraining data. However when presented with tasks\nor functions which are out-of-domain of their pretraining data, we demonstrate\nvarious failure modes of transformers and degradation of their generalization\nfor even simple extrapolation tasks. Together our results highlight that the\nimpressive ICL abilities of high-capacity sequence models may be more closely\ntied to the coverage of their pretraining data mixtures than inductive biases\nthat create fundamental generalization capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadlowsky_S/0/1/0/all/0/1\">Steve Yadlowsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_L/0/1/0/all/0/1\">Lyric Doshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripuraneni_N/0/1/0/all/0/1\">Nilesh Tripuraneni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-Context Prompt Editing For Conditional Audio Generation. (arXiv:2311.00895v1 [cs.SD])","link":"http://arxiv.org/abs/2311.00895","description":"<p>Distributional shift is a central challenge in the deployment of machine\nlearning models as they can be ill-equipped for real-world data. This is\nparticularly evident in text-to-audio generation where the encoded\nrepresentations are easily undermined by unseen prompts, which leads to the\ndegradation of generated audio -- the limited set of the text-audio pairs\nremains inadequate for conditional audio generation in the wild as user prompts\nare under-specified. In particular, we observe a consistent audio quality\ndegradation in generated audio samples with user prompts, as opposed to\ntraining set prompts. To this end, we present a retrieval-based in-context\nprompt editing framework that leverages the training captions as demonstrative\nexemplars to revisit the user prompts. We show that the framework enhanced the\naudio quality across the set of collected user prompts, which were edited with\nreference to the training captions as exemplars.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1\">Ernie Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Pin-Jie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Sidd Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_G/0/1/0/all/0/1\">Gael Le Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kant_D/0/1/0/all/0/1\">David Kant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yangyang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iandola_F/0/1/0/all/0/1\">Forrest Iandola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1\">Vikas Chandra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On The Open Prompt Challenge In Conditional Audio Generation. (arXiv:2311.00897v1 [cs.SD])","link":"http://arxiv.org/abs/2311.00897","description":"<p>Text-to-audio generation (TTA) produces audio from a text description,\nlearning from pairs of audio samples and hand-annotated text. However,\ncommercializing audio generation is challenging as user-input prompts are often\nunder-specified when compared to text descriptions used to train TTA models. In\nthis work, we treat TTA models as a ``blackbox'' and address the user prompt\nchallenge with two key insights: (1) User prompts are generally\nunder-specified, leading to a large alignment gap between user prompts and\ntraining prompts. (2) There is a distribution of audio descriptions for which\nTTA models are better at generating higher quality audio, which we refer to as\n``audionese''. To this end, we rewrite prompts with instruction-tuned models\nand propose utilizing text-audio alignment as feedback signals via margin\nranking learning for audio improvements. On both objective and subjective human\nevaluations, we observed marked improvements in both text-audio alignment and\nmusic audio quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1\">Ernie Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Sidd Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luthra_M/0/1/0/all/0/1\">Mahi Luthra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Pin-Jie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagaraja_V/0/1/0/all/0/1\">Varun Nagaraja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iandola_F/0/1/0/all/0/1\">Forrest Iandola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zechun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Z/0/1/0/all/0/1\">Zhaoheng Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Changsheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yangyang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1\">Vikas Chandra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re-weighting Tokens: A Simple and Effective Active Learning Strategy for Named Entity Recognition. (arXiv:2311.00906v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00906","description":"<p>Active learning, a widely adopted technique for enhancing machine learning\nmodels in text and image classification tasks with limited annotation\nresources, has received relatively little attention in the domain of Named\nEntity Recognition (NER). The challenge of data imbalance in NER has hindered\nthe effectiveness of active learning, as sequence labellers lack sufficient\nlearning signals. To address these challenges, this paper presents a novel\nreweighting-based active learning strategy that assigns dynamic smoothed\nweights to individual tokens. This adaptable strategy is compatible with\nvarious token-level acquisition functions and contributes to the development of\nrobust active learners. Experimental results on multiple corpora demonstrate\nthe substantial performance improvement achieved by incorporating our\nre-weighting strategy into existing acquisition functions, validating its\npractical efficacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haocheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngoc Dang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lan Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Influence Guided Data Reweighting for Language Model Pre-training. (arXiv:2311.00913v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00913","description":"<p>Language Models (LMs) pre-trained with self-supervision on large text corpora\nhave become the default starting point for developing models for various NLP\ntasks. Once the pre-training corpus has been assembled, all data samples in the\ncorpus are treated with equal importance during LM pre-training. However, due\nto varying levels of relevance and quality of data, equal importance to all the\ndata samples may not be the optimal choice. While data reweighting has been\nexplored in the context of task-specific supervised learning and LM\nfine-tuning, model-driven reweighting for pre-training data has not been\nexplored. We fill this important gap and propose PRESENCE, a method for jointly\nreweighting samples by leveraging self-influence (SI) scores as an indicator of\nsample importance and pre-training. PRESENCE promotes novelty and stability for\nmodel pre-training. Through extensive analysis spanning multiple model sizes,\ndatasets, and tasks, we present PRESENCE as an important first step in the\nresearch direction of sample reweighting for pre-training language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_M/0/1/0/all/0/1\">Megh Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolukbasi_T/0/1/0/all/0/1\">Tolga Bolukbasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganapathy_S/0/1/0/all/0/1\">Sriram Ganapathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vashishth_S/0/1/0/all/0/1\">Shikhar Vashishth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Agnostic Low-Rank Adapters for Unseen English Dialects. (arXiv:2311.00915v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00915","description":"<p>Large Language Models (LLMs) are trained on corpora disproportionally\nweighted in favor of Standard American English. As a result, speakers of other\ndialects experience significantly more failures when interacting with these\ntechnologies. In practice, these speakers often accommodate their speech to be\nbetter understood. Our work shares the belief that language technologies should\nbe designed to accommodate the diversity in English dialects and not the other\nway around. However, prior works on dialect struggle with generalizing to\nevolving and emerging dialects in a scalable manner. To fill this gap, our\nmethod, HyperLoRA, leverages expert linguistic knowledge to enable\nresource-efficient adaptation via hypernetworks. By disentangling\ndialect-specific and cross-dialectal information, HyperLoRA improves\ngeneralization to unseen dialects in a task-agnostic fashion. Not only is\nHyperLoRA more scalable in the number of parameters, but it also achieves the\nbest or most competitive performance across 5 dialects in a zero-shot setting.\nIn this way, our approach facilitates access to language technology for\nbillions of English dialect speakers who are traditionally underrepresented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zedian Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_W/0/1/0/all/0/1\">William Held</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E3 TTS: Easy End-to-End Diffusion-based Text to Speech. (arXiv:2311.00945v1 [cs.SD])","link":"http://arxiv.org/abs/2311.00945","description":"<p>We propose Easy End-to-End Diffusion-based Text to Speech, a simple and\nefficient end-to-end text-to-speech model based on diffusion. E3 TTS directly\ntakes plain text as input and generates an audio waveform through an iterative\nrefinement process. Unlike many prior work, E3 TTS does not rely on any\nintermediate representations like spectrogram features or alignment\ninformation. Instead, E3 TTS models the temporal structure of the waveform\nthrough the diffusion process. Without relying on additional conditioning\ninformation, E3 TTS could support flexible latent structure within the given\naudio. This enables E3 TTS to be easily adapted for zero-shot tasks such as\nediting without any additional training. Experiments show that E3 TTS can\ngenerate high-fidelity audio, approaching the performance of a state-of-the-art\nneural TTS system. Audio samples are available at https://e3tts.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morioka_N/0/1/0/all/0/1\">Nobuyuki Morioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nanxin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blending Reward Functions via Few Expert Demonstrations for Faithful and Accurate Knowledge-Grounded Dialogue Generation. (arXiv:2311.00953v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00953","description":"<p>The development of trustworthy conversational information-seeking systems\nrelies on dialogue models that can generate faithful and accurate responses\nbased on relevant knowledge texts. However, two main challenges hinder this\ntask. Firstly, language models may generate hallucinations due to data biases\npresent in their pretraining corpus. Secondly, knowledge texts often contain\nredundant and irrelevant information that distracts the model's attention from\nthe relevant text span. Previous works use additional data annotations on the\nknowledge texts to learn a knowledge identification module in order to bypass\nirrelevant information, but collecting such high-quality span annotations can\nbe costly. In this work, we leverage reinforcement learning algorithms to\novercome the above challenges by introducing a novel reward function. Our\nreward function combines an accuracy metric and a faithfulness metric to\nprovide a balanced quality judgment of generated responses, which can be used\nas a cost-effective approximation to a human preference reward model when only\na few preference annotations are available. Empirical experiments on two\nconversational information-seeking datasets demonstrate that our method can\ncompete with other strong supervised learning baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wanyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IndoToD: A Multi-Domain Indonesian Benchmark For End-to-End Task-Oriented Dialogue Systems. (arXiv:2311.00958v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00958","description":"<p>Task-oriented dialogue (ToD) systems have been mostly created for\nhigh-resource languages, such as English and Chinese. However, there is a need\nto develop ToD systems for other regional or local languages to broaden their\nability to comprehend the dialogue contexts in various languages. This paper\nintroduces IndoToD, an end-to-end multi domain ToD benchmark in Indonesian. We\nextend two English ToD datasets to Indonesian, comprising four different\ndomains by delexicalization to efficiently reduce the size of annotations. To\nensure a high-quality data collection, we hire native speakers to manually\ntranslate the dialogues. Along with the original English datasets, these new\nIndonesian datasets serve as an effective benchmark for evaluating Indonesian\nand English ToD systems as well as exploring the potential benefits of\ncross-lingual and bilingual transfer learning approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kautsar_M/0/1/0/all/0/1\">Muhammad Dehan Al Kautsar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nurdini_R/0/1/0/all/0/1\">Rahmah Khoirussyifa&#x27; Nurdini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purwarianti_A/0/1/0/all/0/1\">Ayu Purwarianti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Language Interpreter for Robot Task Planning. (arXiv:2311.00967v1 [cs.RO])","link":"http://arxiv.org/abs/2311.00967","description":"<p>Large language models (LLMs) are accelerating the development of\nlanguage-guided robot planners. Meanwhile, symbolic planners offer the\nadvantage of interpretability. This paper proposes a new task that bridges\nthese two trends, namely, multimodal planning problem specification. The aim is\nto generate a problem description (PD), a machine-readable file used by the\nplanners to find a plan. By generating PDs from language instruction and scene\nobservation, we can drive symbolic planners in a language-guided framework. We\npropose a Vision-Language Interpreter (ViLaIn), a new framework that generates\nPDs using state-of-the-art LLM and vision-language models. ViLaIn can refine\ngenerated PDs via error message feedback from the symbolic planner. Our aim is\nto answer the question: How accurately can ViLaIn and the symbolic planner\ngenerate valid robot plans? To evaluate ViLaIn, we introduce a novel dataset\ncalled the problem description generation (ProDG) dataset. The framework is\nevaluated with four new evaluation metrics. Experimental results show that\nViLaIn can generate syntactically correct problems with more than 99% accuracy\nand valid plans with more than 58% accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shirai_K/0/1/0/all/0/1\">Keisuke Shirai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltran_Hernandez_C/0/1/0/all/0/1\">Cristian C. Beltran-Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamaya_M/0/1/0/all/0/1\">Masashi Hamaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_A/0/1/0/all/0/1\">Atsushi Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_S/0/1/0/all/0/1\">Shohei Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaharazuka_K/0/1/0/all/0/1\">Kento Kawaharazuka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_K/0/1/0/all/0/1\">Kazutoshi Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ushiku_Y/0/1/0/all/0/1\">Yoshitaka Ushiku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_S/0/1/0/all/0/1\">Shinsuke Mori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Replicable Benchmarking of Neural Machine Translation (NMT) on Low-Resource Local Languages in Indonesia. (arXiv:2311.00998v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00998","description":"<p>Neural machine translation (NMT) for low-resource local languages in\nIndonesia faces significant challenges, including the need for a representative\nbenchmark and limited data availability. This work addresses these challenges\nby comprehensively analyzing training NMT systems for four low-resource local\nlanguages in Indonesia: Javanese, Sundanese, Minangkabau, and Balinese. Our\nstudy encompasses various training approaches, paradigms, data sizes, and a\npreliminary study into using large language models for synthetic low-resource\nlanguages parallel data generation. We reveal specific trends and insights into\npractical strategies for low-resource language translation. Our research\ndemonstrates that despite limited computational resources and textual data,\nseveral of our NMT systems achieve competitive performances, rivaling the\ntranslation quality of zero-shot gpt-3.5-turbo. These findings significantly\nadvance NMT for low-resource languages, offering valuable guidance for\nresearchers in similar contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Susanto_L/0/1/0/all/0/1\">Lucky Susanto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diandaru_R/0/1/0/all/0/1\">Ryandito Diandaru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krisnadhi_A/0/1/0/all/0/1\">Adila Krisnadhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purwarianti_A/0/1/0/all/0/1\">Ayu Purwarianti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijaya_D/0/1/0/all/0/1\">Derry Wijaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COPAL-ID: Indonesian Language Reasoning with Local Culture and Nuances. (arXiv:2311.01012v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01012","description":"<p>We present publicly available COPAL-ID, a novel Indonesian language common\nsense reasoning dataset. Unlike the previous Indonesian COPA dataset\n(XCOPA-ID), COPAL-ID incorporates Indonesian local and cultural nuances, and\ntherefore, provides a more natural portrayal of day-to-day causal reasoning\nwithin the Indonesian cultural sphere. Professionally written by natives from\nscratch, COPAL-ID is more fluent and free from awkward phrases, unlike the\ntranslated XCOPA-ID. In addition, we present COPAL-ID in both standard\nIndonesian and in Jakartan Indonesian--a dialect commonly used in daily\nconversation. COPAL-ID poses a greater challenge for existing open-sourced and\nclosed state-of-the-art multilingual language models, yet is trivially easy for\nhumans. Our findings suggest that even the current best open-source,\nmultilingual model struggles to perform well, achieving 65.47% accuracy on\nCOPAL-ID, significantly lower than on the culturally-devoid XCOPA-ID (79.40%).\nDespite GPT-4's impressive score, it suffers the same performance degradation\ncompared to its XCOPA-ID score, and it still falls short of human performance.\nThis shows that these language models are still way behind in comprehending the\nlocal nuances of Indonesian.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wibowo_H/0/1/0/all/0/1\">Haryo Akbarianto Wibowo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuadi_E/0/1/0/all/0/1\">Erland Hilman Fuadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nityasya_M/0/1/0/all/0/1\">Made Nindyatama Nityasya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasojo_R/0/1/0/all/0/1\">Radityo Eko Prasojo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Learning of Local and Global Features for Aspect-based Sentiment Classification. (arXiv:2311.01030v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01030","description":"<p>Aspect-based sentiment classification (ASC) aims to judge the sentiment\npolarity conveyed by the given aspect term in a sentence. The sentiment\npolarity is not only determined by the local context but also related to the\nwords far away from the given aspect term. Most recent efforts related to the\nattention-based models can not sufficiently distinguish which words they should\npay more attention to in some cases. Meanwhile, graph-based models are coming\ninto ASC to encode syntactic dependency tree information. But these models do\nnot fully leverage syntactic dependency trees as they neglect to incorporate\ndependency relation tag information into representation learning effectively.\nIn this paper, we address these problems by effectively modeling the local and\nglobal features. Firstly, we design a local encoder containing: a Gaussian mask\nlayer and a covariance self-attention layer. The Gaussian mask layer tends to\nadjust the receptive field around aspect terms adaptively to deemphasize the\neffects of unrelated words and pay more attention to local information. The\ncovariance self-attention layer can distinguish the attention weights of\ndifferent words more obviously. Furthermore, we propose a dual-level graph\nattention network as a global encoder by fully employing dependency tag\ninformation to capture long-distance information effectively. Our model\nachieves state-of-the-art performance on both SemEval 2014 and Twitter\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_H/0/1/0/all/0/1\">Hao Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaosu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ATHENA: Mathematical Reasoning with Thought Expansion. (arXiv:2311.01036v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01036","description":"<p>Solving math word problems depends on how to articulate the problems, the\nlens through which models view human linguistic expressions. Real-world\nsettings count on such a method even more due to the diverse practices of the\nsame mathematical operations. Earlier works constrain available thinking\nprocesses by limited prediction strategies without considering their\nsignificance in acquiring mathematical knowledge. We introduce Attention-based\nTHought Expansion Network Architecture (ATHENA) to tackle the challenges of\nreal-world practices by mimicking human thought expansion mechanisms in the\nform of neural network propagation. A thought expansion recurrently generates\nthe candidates carrying the thoughts of possible math expressions driven from\nthe previous step and yields reasonable thoughts by selecting the valid\npathways to the goal. Our experiments show that ATHENA achieves a new\nstate-of-the-art stage toward the ideal model that is compelling in variant\nquestions even when the informativeness in training examples is restricted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">JB. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hazel Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahn_J/0/1/0/all/0/1\">Joonghyuk Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yo-Sub Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism. (arXiv:2311.01041v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01041","description":"<p>Large language models (LLMs) have demonstrated impressive language\nunderstanding and generation capabilities, enabling them to answer a wide range\nof questions across various domains. However, these models are not flawless and\noften produce responses that contain errors or misinformation. These\ninaccuracies, commonly referred to as hallucinations, render LLMs unreliable\nand even unusable in many scenarios. In this paper, our focus is on mitigating\nthe issue of hallucination in LLMs, particularly in the context of\nquestion-answering. Instead of attempting to answer all questions, we explore a\nrefusal mechanism that instructs LLMs to refuse to answer challenging questions\nin order to avoid errors. We then propose a simple yet effective solution\ncalled Learn to Refuse (L2R), which incorporates the refusal mechanism to\nenable LLMs to recognize and refuse to answer questions that they find\ndifficult to address. To achieve this, we utilize a structured knowledge base\nto represent all the LLM's understanding of the world, enabling it to provide\ntraceable gold knowledge. This knowledge base is separate from the LLM and\ninitially empty, and it is progressively expanded with validated knowledge.\nWhen an LLM encounters questions outside its domain, the system recognizes its\nknowledge scope and determines whether it can answer the question\nindependently. Additionally, we introduce a method for automatically and\nefficiently expanding the knowledge base of LLMs. Through qualitative and\nquantitative analysis, we demonstrate that our approach enhances the\ncontrollability and reliability of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Lang Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-dimensional data refining strategy for effective fine-tuning LLMs. (arXiv:2311.01049v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01049","description":"<p>Data is a cornerstone for fine-tuning large language models, yet acquiring\nsuitable data remains challenging. Challenges encompassed data scarcity,\nlinguistic diversity, and domain-specific content. This paper presents lessons\nlearned while crawling and refining data tailored for fine-tuning Vietnamese\nlanguage models. Crafting such a dataset, while accounting for linguistic\nintricacies and striking a balance between inclusivity and accuracy, demands\nmeticulous planning. Our paper presents a multidimensional strategy including\nleveraging existing datasets in the English language and developing customized\ndata-crawling scripts with the assistance of generative AI tools. A fine-tuned\nLLM model for the Vietnamese language, which was produced using resultant\ndatasets, demonstrated good performance while generating Vietnamese news\narticles from prompts. The study offers practical solutions and guidance for\nfuture fine-tuning models in languages like Vietnamese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ngoc_T/0/1/0/all/0/1\">Thanh Nguyen Ngoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quang Nhat Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_A/0/1/0/all/0/1\">Arthur Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Bao Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thuy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1\">Thanh Pham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DistilWhisper: Efficient Distillation of Multi-task Speech Models via Language-Specific Experts. (arXiv:2311.01070v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01070","description":"<p>Whisper is a multitask and multilingual speech model covering 99 languages.\nIt yields commendable automatic speech recognition (ASR) results in a subset of\nits covered languages, but the model still under-performs on a non-negligible\nnumber of under-represented languages, a problem exacerbated in smaller model\nversions. In this work, we propose DistilWhisper, an approach able to bridge\nthe performance gap in ASR for these languages while retaining the advantages\nof multitask and multilingual capabilities. Our approach involves two key\nstrategies: lightweight modular ASR fine-tuning of whisper-small using\nlanguage-specific experts, and knowledge distillation from whisper-large-v2.\nThis dual approach allows us to effectively boost ASR performance while keeping\nthe robustness inherited from the multitask and multilingual pre-training.\nResults demonstrate that our approach is more effective than standard\nfine-tuning or LoRA adapters, boosting performance in the targeted languages\nfor both in- and out-of-domain test sets, while introducing only a negligible\nparameter overhead at inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferraz_T/0/1/0/all/0/1\">Thomas Palmeira Ferraz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boito_M/0/1/0/all/0/1\">Marcely Zanon Boito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brun_C/0/1/0/all/0/1\">Caroline Brun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1\">Vassilina Nikoulina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance. (arXiv:2311.01108v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01108","description":"<p>Adopting a two-stage paradigm of pretraining followed by fine-tuning,\nPretrained Language Models (PLMs) have achieved substantial advancements in the\nfield of natural language processing. However, in real-world scenarios, data\nlabels are often noisy due to the complex annotation process, making it\nessential to develop strategies for fine-tuning PLMs with such noisy labels. To\nthis end, we introduce an innovative approach for fine-tuning PLMs using noisy\nlabels, which incorporates the guidance of Large Language Models (LLMs) like\nChatGPT. This guidance assists in accurately distinguishing between clean and\nnoisy samples and provides supplementary information beyond the noisy labels,\nthereby boosting the learning process during fine-tuning PLMs. Extensive\nexperiments on synthetic and real-world noisy datasets further demonstrate the\nsuperior advantages of our framework over the state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Ruocheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jundong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chinesewebtext: Large-scale high-quality Chinese web text extracted with effective evaluation model. (arXiv:2311.01149v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01149","description":"<p>During the development of large language models (LLMs), the scale and quality\nof the pre-training data play a crucial role in shaping LLMs' capabilities. To\naccelerate the research of LLMs, several large-scale datasets, such as C4 [1],\nPile [2], RefinedWeb [3] and WanJuan [4], have been released to the public.\nHowever, most of the released corpus focus mainly on English, and there is\nstill lack of complete tool-chain for extracting clean texts from web data.\nFurthermore, fine-grained information of the corpus, e.g. the quality of each\ntext, is missing. To address these challenges, we propose in this paper a new\ncomplete tool-chain EvalWeb to extract Chinese clean texts from noisy web data.\nFirst, similar to previous work, manually crafted rules are employed to discard\nexplicit noisy texts from the raw crawled web contents. Second, a well-designed\nevaluation model is leveraged to assess the remaining relatively clean data,\nand each text is assigned a specific quality score. Finally, we can easily\nutilize an appropriate threshold to select the high-quality pre-training data\nfor Chinese. Using our proposed approach, we release the largest and latest\nlarge-scale high-quality Chinese web text ChineseWebText, which consists of\n1.42 TB and each text is associated with a quality score, facilitating the LLM\nresearchers to choose the data according to the desired quality thresholds. We\nalso release a much cleaner subset of 600 GB Chinese data with the quality\nexceeding 90%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_P/0/1/0/all/0/1\">Pu Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_T/0/1/0/all/0/1\">Tengxiao Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Y/0/1/0/all/0/1\">Yidong Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Chenglin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Qianlong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1\">Guibo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_C/0/1/0/all/0/1\">Chengqing Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinqiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the Knowledge Injection Frameworks. (arXiv:2311.01150v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01150","description":"<p>In recent years, large language models (LLMs), such as GPTs, have attained\ngreat impact worldwide. However, how to adapt these LLMs to better suit the\nvertical domain-specific tasks by utilizing external knowledge remains not\ncompletely solved. Indeed, there have emerged a few works on this line where\nmost of them rely on an alignment heuristic that is built to inject the\ncorresponding knowledge tuple into the associated text sample.\n</p>\n<p>However, despite the promise, we identify a pivotal problem in this work\nubiquitously. Simply put, we find that injecting unaligned (i.e., random)\nknowledge tuple into the LLMs achieves comparable (and sometimes better)\nresults than the aligned knowledge being injected. We therefore take a thorough\ninvestigation of this frustrating finding on a variety of related prior work\nand further provide a chain of potential interpretations for the phenomenon.\nBased on all that, we offer a simple remediated technique. Briefly, the core of\nthis technique is rooted in an ideological emphasis on the pruning and\npurification of the external knowledge base to be injected into LLMs. At last,\nwe show that by integrating this technique into most (if not all) knowledge\ninjection frameworks and recent LLMs, it manages to overcome the aforementioned\nsanity problem and further pushes the boundary of the performance of the\ndomain-adaptive LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_P/0/1/0/all/0/1\">Peng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haobo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1\">Weikang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junbo Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Question-Answering Performance of Large Language Models through Semantic Consistency. (arXiv:2311.01152v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01152","description":"<p>Semantic consistency of a language model is broadly defined as the model's\nability to produce semantically-equivalent outputs, given\nsemantically-equivalent inputs. We address the task of assessing\nquestion-answering (QA) semantic consistency of contemporary large language\nmodels (LLMs) by manually creating a benchmark dataset with high-quality\nparaphrases for factual questions, and release the dataset to the community.\n</p>\n<p>We further combine the semantic consistency metric with additional\nmeasurements suggested in prior work as correlating with LLM QA accuracy, for\nbuilding and evaluating a framework for factual QA reference-less performance\nprediction -- predicting the likelihood of a language model to accurately\nanswer a question. Evaluating the framework on five contemporary LLMs, we\ndemonstrate encouraging, significantly outperforming baselines, results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rabinovich_E/0/1/0/all/0/1\">Ella Rabinovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ackerman_S/0/1/0/all/0/1\">Samuel Ackerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raz_O/0/1/0/all/0/1\">Orna Raz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farchi_E/0/1/0/all/0/1\">Eitan Farchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anaby_Tavor_A/0/1/0/all/0/1\">Ateret Anaby-Tavor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACES: Translation Accuracy Challenge Sets at WMT 2023. (arXiv:2311.01153v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01153","description":"<p>We benchmark the performance of segmentlevel metrics submitted to WMT 2023\nusing the ACES Challenge Set (Amrhein et al., 2022). The challenge set consists\nof 36K examples representing challenges from 68 phenomena and covering 146\nlanguage pairs. The phenomena range from simple perturbations at the\nword/character level to more complex errors based on discourse and real-world\nknowledge. For each metric, we provide a detailed profile of performance over a\nrange of error categories as well as an overall ACES-Score for quick\ncomparison. We also measure the incremental performance of the metrics\nsubmitted to both WMT 2023 and 2022. We find that 1) there is no clear winner\namong the metrics submitted to WMT 2023, and 2) performance change between the\n2023 and 2022 versions of the metrics is highly variable. Our recommendations\nare similar to those from WMT 2022. Metric developers should focus on: building\nensembles of metrics from different design families, developing metrics that\npay more attention to the source and rely less on surface-level overlap, and\ncarefully determining the influence of multilingual embeddings on MT\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amrhein_C/0/1/0/all/0/1\">Chantal Amrhein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moghe_N/0/1/0/all/0/1\">Nikita Moghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guillou_L/0/1/0/all/0/1\">Liane Guillou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Semantic Parsing with Execution-based Spurious Program Filtering. (arXiv:2311.01161v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01161","description":"<p>The problem of spurious programs is a longstanding challenge when training a\nsemantic parser from weak supervision. To eliminate such programs that have\nwrong semantics but correct denotation, existing methods focus on exploiting\nsimilarities between examples based on domain-specific knowledge. In this\npaper, we propose a domain-agnostic filtering mechanism based on program\nexecution results. Specifically, for each program obtained through the search\nprocess, we first construct a representation that captures the program's\nsemantics as execution results under various inputs. Then, we run a majority\nvote on these representations to identify and filter out programs with\nsignificantly different semantics from the other programs. In particular, our\nmethod is orthogonal to the program search process so that it can easily\naugment any of the existing weakly supervised semantic parsing frameworks.\nEmpirical evaluations on the Natural Language Visual Reasoning and\nWikiTableQuestions demonstrate that applying our method to the existing\nsemantic parsers induces significantly improved performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kang-il Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Segwang Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1\">Kyomin Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Input: Towards Next-Generation Input Methods Paradigm. (arXiv:2311.01166v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01166","description":"<p>Since the release of ChatGPT, generative models have achieved tremendous\nsuccess and become the de facto approach for various NLP tasks. However, its\napplication in the field of input methods remains under-explored. Many neural\nnetwork approaches have been applied to the construction of Chinese input\nmethod engines(IMEs).Previous research often assumed that the input pinyin was\ncorrect and focused on Pinyin-to-character(P2C) task, which significantly falls\nshort of meeting users' demands. Moreover, previous research could not leverage\nuser feedback to optimize the model and provide personalized results. In this\nstudy, we propose a novel Generative Input paradigm named GeneInput. It uses\nprompts to handle all input scenarios and other intelligent auxiliary input\nfunctions, optimizing the model with user feedback to deliver personalized\nresults. The results demonstrate that we have achieved state-of-the-art\nperformance for the first time in the Full-mode Key-sequence to\nCharacters(FK2C) task. We propose a novel reward model training method that\neliminates the need for additional manual annotations and the performance\nsurpasses GPT-4 in tasks involving intelligent association and conversational\nassistance. Compared to traditional paradigms, GeneInput not only demonstrates\nsuperior performance but also exhibits enhanced robustness, scalability, and\nonline learning capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1\">Keyu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongcan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zihang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhenzhen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shijin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL. (arXiv:2311.01173v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01173","description":"<p>Existing Text-to-SQL generators require the entire schema to be encoded with\nthe user text. This is expensive or impractical for large databases with tens\nof thousands of columns. Standard dense retrieval techniques are inadequate for\nschema subsetting of a large structured database, where the correct semantics\nof retrieval demands that we rank sets of schema elements rather than\nindividual elements. In response, we propose a two-stage process for effective\ncoverage during retrieval. First, we instruct an LLM to hallucinate a minimal\nDB schema deemed adequate to answer the query. We use the hallucinated schema\nto retrieve a subset of the actual schema, by composing the results from\nmultiple dense retrievals. Remarkably, hallucination $\\unicode{x2013}$\ngenerally considered a nuisance $\\unicode{x2013}$ turns out to be actually\nuseful as a bridging mechanism. Since no existing benchmarks exist for schema\nsubsetting on large databases, we introduce three benchmarks. Two\nsemi-synthetic datasets are derived from the union of schemas in two well-known\ndatasets, SPIDER and BIRD, resulting in 4502 and 798 schema elements\nrespectively. A real-life benchmark called SocialDB is sourced from an actual\nlarge data warehouse comprising 17844 schema elements. We show that our method1\nleads to significantly higher recall than SOTA retrieval-based augmentation\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kothyari_M/0/1/0/all/0/1\">Mayank Kothyari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_D/0/1/0/all/0/1\">Dhruva Dhingra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1\">Sunita Sarawagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1\">Soumen Chakrabarti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study of Continual Learning Under Language Shift. (arXiv:2311.01200v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01200","description":"<p>The recent increase in data and model scale for language model pre-training\nhas led to huge training costs. In scenarios where new data become available\nover time, updating a model instead of fully retraining it would therefore\nprovide significant gains. In this paper, we study the benefits and downsides\nof updating a language model when new data comes from new languages - the case\nof continual learning under language shift. Starting from a monolingual English\nlanguage model, we incrementally add data from Norwegian and Icelandic to\ninvestigate how forward and backward transfer effects depend on the\npre-training order and characteristics of languages, for different model sizes\nand learning rate schedulers. Our results show that, while forward transfer is\nlargely positive and independent of language order, backward transfer can be\neither positive or negative depending on the order and characteristics of new\nlanguages. To explain these patterns we explore several language similarity\nmetrics and find that syntactic similarity appears to have the best correlation\nwith our results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gogoulou_E/0/1/0/all/0/1\">Evangelia Gogoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lesort_T/0/1/0/all/0/1\">Timoth&#xe9;e Lesort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boman_M/0/1/0/all/0/1\">Magnus Boman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nivre_J/0/1/0/all/0/1\">Joakim Nivre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An energy-based comparative analysis of common approaches to text classification in the Legal domain. (arXiv:2311.01256v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01256","description":"<p>Most Machine Learning research evaluates the best solutions in terms of\nperformance. However, in the race for the best performing model, many important\naspects are often overlooked when, on the contrary, they should be carefully\nconsidered. In fact, sometimes the gaps in performance between different\napproaches are neglectable, whereas factors such as production costs, energy\nconsumption, and carbon footprint must take into consideration. Large Language\nModels (LLMs) are extensively adopted to address NLP problems in academia and\nindustry. In this work, we present a detailed quantitative comparison of LLM\nand traditional approaches (e.g. SVM) on the LexGLUE benchmark, which takes\ninto account both performance (standard indices) and alternative metrics such\nas timing, power consumption and cost, in a word: the carbon-footprint. In our\nanalysis, we considered the prototyping phase (model selection by\ntraining-validation-test iterations) and in-production phases separately, since\nthey follow different implementation procedures and also require different\nresources. The results indicate that very often, the simplest algorithms\nachieve performance very close to that of large LLMs but with very low power\nconsumption and lower resource demands. The results obtained could suggest\ncompanies to include additional evaluations in the choice of Machine Learning\n(ML) solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gultekin_S/0/1/0/all/0/1\">Sinan Gultekin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Globo_A/0/1/0/all/0/1\">Achille Globo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zugarini_A/0/1/0/all/0/1\">Andrea Zugarini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernandes_M/0/1/0/all/0/1\">Marco Ernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigutini_L/0/1/0/all/0/1\">Leonardo Rigutini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection. (arXiv:2311.01270v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01270","description":"<p>NLP models are used in a variety of critical social computing tasks, such as\ndetecting sexist, racist, or otherwise hateful content. Therefore, it is\nimperative that these models are robust to spurious features. Past work has\nattempted to tackle such spurious features using training data augmentation,\nincluding Counterfactually Augmented Data (CADs). CADs introduce minimal\nchanges to existing training data points and flip their labels; training on\nthem may reduce model dependency on spurious features. However, manually\ngenerating CADs can be time-consuming and expensive. Hence in this work, we\nassess if this task can be automated using generative NLP models. We\nautomatically generate CADs using Polyjuice, ChatGPT, and Flan-T5, and evaluate\ntheir usefulness in improving model robustness compared to manually-generated\nCADs. By testing both model performance on multiple out-of-domain test sets and\nindividual data point efficacy, our results show that while manual CADs are\nstill the most effective, CADs generated by ChatGPT come a close second. One\nkey reason for the lower performance of automated methods is that the changes\nthey introduce are often insufficient to flip the original label.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sen_I/0/1/0/all/0/1\">Indira Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assenmacher_D/0/1/0/all/0/1\">Dennis Assenmacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samory_M/0/1/0/all/0/1\">Mattia Samory</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aalst_W/0/1/0/all/0/1\">Wil van der Aalst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagne_C/0/1/0/all/0/1\">Claudia Wagne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding Common Ground: Annotating and Predicting Common Ground in Spoken Conversations. (arXiv:2311.01273v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01273","description":"<p>When we communicate with other humans, we do not simply generate a sequence\nof words. Rather, we use our cognitive state (beliefs, desires, intentions) and\nour model of the audience's cognitive state to create utterances that affect\nthe audience's cognitive state in the intended manner. An important part of\ncognitive state is the common ground, which is the content the speaker\nbelieves, and the speaker believes the audience believes, and so on. While much\nattention has been paid to common ground in cognitive science, there has not\nbeen much work in natural language processing. In this paper, we introduce a\nnew annotation and corpus to capture common ground. We then describe some\ninitial experiments extracting propositions from dialog and tracking their\nstatus in the common ground from the perspective of each speaker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Markowska_M/0/1/0/all/0/1\">Magdalena Markowska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taghizadeh_M/0/1/0/all/0/1\">Mohammad Taghizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soubki_A/0/1/0/all/0/1\">Adil Soubki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirroshandel_S/0/1/0/all/0/1\">Seyed Abolghasem Mirroshandel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambow_O/0/1/0/all/0/1\">Owen Rambow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlashDecoding++: Faster Large Language Model Inference on GPUs. (arXiv:2311.01282v1 [cs.LG])","link":"http://arxiv.org/abs/2311.01282","description":"<p>As the Large Language Model (LLM) becomes increasingly important in various\ndomains. However, the following challenges still remain unsolved in\naccelerating LLM inference: (1) Synchronized partial softmax update. The\nsoftmax operation requires a synchronized update operation among each partial\nsoftmax result, leading to ~20% overheads for the attention computation in\nLLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices\nperforming GEMM in LLM inference is flat, leading to under-utilized computation\nand &gt;50% performance loss after padding zeros in previous designs. (3)\nPerformance loss due to static dataflow. Kernel performance in LLM depends on\nvaried input data features, hardware configurations, etc. A single and static\ndataflow may lead to a 50.25% performance loss for GEMMs of different shapes in\nLLM inference.\n</p>\n<p>We present FlashDecoding++, a fast LLM inference engine supporting mainstream\nLLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++\ncreatively proposes: (1) Asynchronized softmax with unified max value.\nFlashDecoding++ introduces a unified max value technique for different partial\nsoftmax computations to avoid synchronization. (2) Flat GEMM optimization with\ndouble buffering. FlashDecoding++ points out that flat GEMMs with different\nshapes face varied bottlenecks. Then, techniques like double buffering are\nintroduced. (3) Heuristic dataflow with hardware resource adaptation.\nFlashDecoding++ heuristically optimizes dataflow using different hardware\nresource considering input dynamics. Due to the versatility of optimizations in\nFlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on\nboth NVIDIA and AMD GPUs compared to Hugging Face implementations.\nFlashDecoding++ also achieves an average speedup of 1.37x compared to\nstate-of-the-art LLM inference engines on mainstream LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1\">Ke Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_G/0/1/0/all/0/1\">Guohao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiaming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Q/0/1/0/all/0/1\">Qiuli Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiuhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kangdi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hanyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v1 [cs.LG])","link":"http://arxiv.org/abs/2311.01305","description":"<p>Large language models(LLMs) exhibit excellent performance across a variety of\ntasks, but they come with significant computational and storage costs.\nQuantizing these models is an effective way to alleviate this issue. However,\nexisting methods struggle to strike a balance between model accuracy and\nhardware efficiency. This is where we introduce AWEQ, a post-training method\nthat requires no additional training overhead. AWEQ excels in both\nultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization.\nThere is an observation that weight quantization is less challenging than\nactivation quantization. AWEQ transfers the difficulty of activation\nquantization to weights using channel equalization, achieving a balance between\nthe quantization difficulties of both, and thereby maximizing performance. We\nhave further refined the equalization method to mitigate quantization bias\nerror, ensuring the robustness of the model. Extensive experiments on popular\nmodels such as LLaMA and OPT demonstrate that AWEQ outperforms all existing\npost-training quantization methods for large models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baisong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingwang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haixiao Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models. (arXiv:2311.01307v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01307","description":"<p>Large Language Models (LLMs) make natural interfaces to factual knowledge,\nbut their usefulness is limited by their tendency to deliver inconsistent\nanswers to semantically equivalent questions. For example, a model might\npredict both \"Anne Redpath passed away in Edinburgh.\" and \"Anne Redpath's life\nended in London.\" In this work, we identify potential causes of inconsistency\nand evaluate the effectiveness of two mitigation strategies: up-scaling and\naugmenting the LM with a retrieval corpus. Our results on the LLaMA and Atlas\nmodels show that both strategies reduce inconsistency while retrieval\naugmentation is considerably more efficient. We further consider and\ndisentangle the consistency contributions of different components of Atlas. For\nall LMs evaluated we find that syntactical form and other evaluation task\nartifacts impact consistency. Taken together, our results provide a better\nunderstanding of the factors affecting the factual consistency of language\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagstrom_L/0/1/0/all/0/1\">Lovisa Hagstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saynova_D/0/1/0/all/0/1\">Denitsa Saynova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norlund_T/0/1/0/all/0/1\">Tobias Norlund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansson_M/0/1/0/all/0/1\">Moa Johansson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansson_R/0/1/0/all/0/1\">Richard Johansson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Together: Enhancing Generative Knowledge Graph Completion with Language Models and Neighborhood Information. (arXiv:2311.01326v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01326","description":"<p>Real-world Knowledge Graphs (KGs) often suffer from incompleteness, which\nlimits their potential performance. Knowledge Graph Completion (KGC) techniques\naim to address this issue. However, traditional KGC methods are computationally\nintensive and impractical for large-scale KGs, necessitating the learning of\ndense node embeddings and computing pairwise distances. Generative\ntransformer-based language models (e.g., T5 and recent KGT5) offer a promising\nsolution as they can predict the tail nodes directly. In this study, we propose\nto include node neighborhoods as additional information to improve KGC methods\nbased on language models. We examine the effects of this imputation and show\nthat, on both inductive and transductive Wikidata subsets, our method\noutperforms KGT5 and conventional KGC approaches. We also provide an extensive\nanalysis of the impact of neighborhood on model prediction and show its\nimportance. Furthermore, we point the way to significantly improve KGC through\nmore effective neighborhood selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chepurova_A/0/1/0/all/0/1\">Alla Chepurova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulatov_A/0/1/0/all/0/1\">Aydar Bulatov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuratov_Y/0/1/0/all/0/1\">Yuri Kuratov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burtsev_M/0/1/0/all/0/1\">Mikhail Burtsev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-4V(ision) as a Generalist Evaluator for Vision-Language Tasks. (arXiv:2311.01361v1 [cs.CV])","link":"http://arxiv.org/abs/2311.01361","description":"<p>Automatically evaluating vision-language tasks is challenging, especially\nwhen it comes to reflecting human judgments due to limitations in accounting\nfor fine-grained details. Although GPT-4V has shown promising results in\nvarious multi-modal tasks, leveraging GPT-4V as a generalist evaluator for\nthese tasks has not yet been systematically explored. We comprehensively\nvalidate GPT-4V's capabilities for evaluation purposes, addressing tasks\nranging from foundational image-to-text and text-to-image synthesis to\nhigh-level image-to-image translations and multi-images to text alignment. We\nemploy two evaluation methods, single-answer grading and pairwise comparison,\nusing GPT-4V. Notably, GPT-4V shows promising agreement with humans across\nvarious tasks and evaluation methods, demonstrating immense potential for\nmulti-modal LLMs as evaluators. Despite limitations like restricted visual\nclarity grading and real-world complex reasoning, its ability to provide\nhuman-aligned scores enriched with detailed explanations is promising for\nuniversal automatic evaluator.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinlu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_A/0/1/0/all/0/1\">An Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Lianke Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Heng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xifeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petzold_L/0/1/0/all/0/1\">Linda Ruth Petzold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder with Semantics. (arXiv:2311.01386v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01386","description":"<p>Language models (LMs) have been argued to overlap substantially with human\nbeings in grammaticality judgment tasks. But when humans systematically make\nerrors in language processing, should we expect LMs to behave like cognitive\nmodels of language and mimic human behavior? We answer this question by\ninvestigating LMs' more subtle judgments associated with \"language illusions\"\n-- sentences that are vague in meaning, implausible, or ungrammatical but\nreceive unexpectedly high acceptability judgments by humans. We looked at three\nillusions: the comparative illusion (e.g. \"More people have been to Russia than\nI have\"), the depth-charge illusion (e.g. \"No head injury is too trivial to be\nignored\"), and the negative polarity item (NPI) illusion (e.g. \"The hunter who\nno villager believed to be trustworthy will ever shoot a bear\"). We found that\nprobabilities represented by LMs were more likely to align with human judgments\nof being \"tricked\" by the NPI illusion which examines a structural dependency,\ncompared to the comparative and the depth-charge illusions which require\nsophisticated semantic understanding. No single LM or metric yielded results\nthat are entirely consistent with human behavior. Ultimately, we show that LMs\nare limited both in their construal as cognitive models of human language\nprocessing and in their capacity to recognize nuanced but critical information\nin complicated language materials.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibson_E/0/1/0/all/0/1\">Edward Gibson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_F/0/1/0/all/0/1\">Forrest Davis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Server-side Rescoring of Spoken Entity-centric Knowledge Queries for Virtual Assistants. (arXiv:2311.01398v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01398","description":"<p>On-device Virtual Assistants (VAs) powered by Automatic Speech Recognition\n(ASR) require effective knowledge integration for the challenging entity-rich\nquery recognition. In this paper, we conduct an empirical study of modeling\nstrategies for server-side rescoring of spoken information domain queries using\nvarious categories of Language Models (LMs) (N-gram word LMs, sub-word neural\nLMs). We investigate the combination of on-device and server-side signals, and\ndemonstrate significant WER improvements of 23%-35% on various entity-centric\nquery subpopulations by integrating various server-side LMs compared to\nperforming ASR on-device only. We also perform a comparison between LMs trained\non domain data and a GPT-3 variant offered by OpenAI as a baseline.\nFurthermore, we also show that model fusion of multiple server-side LMs trained\nfrom scratch most effectively combines complementary strengths of each model\nand integrates knowledge learned from domain-specific data to a VA ASR system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Youyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gondala_S/0/1/0/all/0/1\">Sashank Gondala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraga_Silva_T/0/1/0/all/0/1\">Thiago Fraga-Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gysel_C/0/1/0/all/0/1\">Christophe Van Gysel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantum Circuit Compiler for a Shuttling-Based Trapped-Ion Quantum Computer. (arXiv:2207.01964v4 [quant-ph] UPDATED)","link":"http://arxiv.org/abs/2207.01964","description":"<p>The increasing capabilities of quantum computing hardware and the challenge\nof realizing deep quantum circuits require fully automated and efficient tools\nfor compiling quantum circuits. To express arbitrary circuits in a sequence of\nnative gates specific to the quantum computer architecture, it is necessary to\nmake algorithms portable across the landscape of quantum hardware providers. In\nthis work, we present a compiler capable of transforming and optimizing a\nquantum circuit targeting a shuttling-based trapped-ion quantum processor. It\nconsists of custom algorithms set on top of the quantum circuit framework\nPytket. The performance was evaluated for a wide range of quantum circuits and\nthe results show that the gate counts can be reduced by factors up to 5.1\ncompared to standard Pytket and up to 2.2 compared to standard Qiskit\ncompilation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Kreppel_F/0/1/0/all/0/1\">Fabian Kreppel</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Melzer_C/0/1/0/all/0/1\">Christian Melzer</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Millan_D/0/1/0/all/0/1\">Diego Olvera Mill&#xe1;n</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wagner_J/0/1/0/all/0/1\">Janis Wagner</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Hilder_J/0/1/0/all/0/1\">Janine Hilder</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Poschinger_U/0/1/0/all/0/1\">Ulrich Poschinger</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Schmidt_Kaler_F/0/1/0/all/0/1\">Ferdinand Schmidt-Kaler</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Brinkmann_A/0/1/0/all/0/1\">Andr&#xe9; Brinkmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving word mover's distance by leveraging self-attention matrix. (arXiv:2211.06229v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.06229","description":"<p>Measuring the semantic similarity between two sentences is still an important\ntask. The word mover's distance (WMD) computes the similarity via the optimal\nalignment between the sets of word embeddings. However, WMD does not utilize\nword order, making it challenging to distinguish sentences with significant\noverlaps of similar words, even if they are semantically very different. Here,\nwe attempt to improve WMD by incorporating the sentence structure represented\nby BERT's self-attention matrix (SAM). The proposed method is based on the\nFused Gromov-Wasserstein distance, which simultaneously considers the\nsimilarity of the word embedding and the SAM for calculating the optimal\ntransport between two sentences. Experiments demonstrate the proposed method\nenhances WMD and its variants in paraphrase identification with near-equivalent\nperformance in semantic textual similarity. Our code is available at\n\\url{https://github.com/ymgw55/WSMD}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamagiwa_H/0/1/0/all/0/1\">Hiroaki Yamagiwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokoi_S/0/1/0/all/0/1\">Sho Yokoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimodaira_H/0/1/0/all/0/1\">Hidetoshi Shimodaira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rainproof: An Umbrella To Shield Text Generators From Out-Of-Distribution Data. (arXiv:2212.09171v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09171","description":"<p>Implementing effective control mechanisms to ensure the proper functioning\nand security of deployed NLP models, from translation to chatbots, is\nessential. A key ingredient to ensure safe system behaviour is\nOut-Of-Distribution (OOD) detection, which aims to detect whether an input\nsample is statistically far from the training distribution. Although OOD\ndetection is a widely covered topic in classification tasks, most methods rely\non hidden features output by the encoder. In this work, we focus on leveraging\nsoft-probabilities in a black-box framework, i.e. we can access the\nsoft-predictions but not the internal states of the model. Our contributions\ninclude: (i) RAINPROOF a Relative informAItioN Projection OOD detection\nframework; and (ii) a more operational evaluation setting for OOD detection.\nSurprisingly, we find that OOD detection is not necessarily aligned with\ntask-specific measures. The OOD detector may filter out samples well processed\nby the model and keep samples that are not, leading to weaker performance. Our\nresults show that RAINPROOF provides OOD detection methods more aligned with\ntask-specific performance metrics than traditional OOD detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Darrin_M/0/1/0/all/0/1\">Maxime Darrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1\">Pablo Piantanida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Norm of Word Embedding Encodes Information Gain. (arXiv:2212.09663v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09663","description":"<p>Distributed representations of words encode lexical semantic information, but\nwhat type of information is encoded and how? Focusing on the skip-gram with\nnegative-sampling method, we found that the squared norm of static word\nembedding encodes the information gain conveyed by the word; the information\ngain is defined by the Kullback-Leibler divergence of the co-occurrence\ndistribution of the word to the unigram distribution. Our findings are\nexplained by the theoretical framework of the exponential family of probability\ndistributions and confirmed through precise experiments that remove spurious\ncorrelations arising from word frequency. This theory also extends to\ncontextualized word embeddings in language models or any neural networks with\nthe softmax output layer. We also demonstrate that both the KL divergence and\nthe squared norm of embedding provide a useful metric of the informativeness of\na word in tasks such as keyword extraction, proper-noun discrimination, and\nhypernym discrimination.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oyama_M/0/1/0/all/0/1\">Momose Oyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokoi_S/0/1/0/all/0/1\">Sho Yokoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimodaira_H/0/1/0/all/0/1\">Hidetoshi Shimodaira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine. (arXiv:2301.08745v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.08745","description":"<p>This report provides a preliminary evaluation of ChatGPT for machine\ntranslation, including translation prompt, multilingual translation, and\ntranslation robustness. We adopt the prompts advised by ChatGPT to trigger its\ntranslation ability and find that the candidate prompts generally work well\nwith minor performance differences. By evaluating on a number of benchmark test\nsets, we find that ChatGPT performs competitively with commercial translation\nproducts (e.g., Google Translate) on high-resource European languages but lags\nbehind significantly on low-resource or distant languages. As for the\ntranslation robustness, ChatGPT does not perform as well as the commercial\nsystems on biomedical abstracts or Reddit comments but exhibits good results on\nspoken language. Further, we explore an interesting strategy named\n$\\mathbf{pivot~prompting}$ for distant languages, which asks ChatGPT to\ntranslate the source sentence into a high-resource pivot language before into\nthe target language, improving the translation performance noticeably. With the\nlaunch of the GPT-4 engine, the translation performance of ChatGPT is\nsignificantly boosted, becoming comparable to commercial translation products,\neven for distant languages. Human analysis on Google Translate and ChatGPT\nsuggests that ChatGPT with GPT-3.5 tends to generate more hallucinations and\nmis-translation errors while that with GPT-4 makes the least errors. In other\nwords, ChatGPT has already become a good translator. Please refer to our Github\nproject for more details:\nhttps://github.com/wxjiao/Is-ChatGPT-A-Good-Translator\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jen-tse Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v6 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.04391","description":"<p>In industry deep learning application, our manually labeled data has a\ncertain number of noisy data. To solve this problem and achieve more than 90\nscore in dev dataset, we present a simple method to find the noisy data and\nre-label the noisy data by human, given the model predictions as references in\nhuman labeling. In this paper, we illustrate our idea for a broad set of deep\nlearning tasks, includes classification, sequence tagging, object detection,\nsequence generation, click-through rate prediction. The dev dataset evaluation\nresults and human evaluation results verify our idea.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeltaScore: Fine-Grained Story Evaluation with Perturbations. (arXiv:2303.08991v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.08991","description":"<p>Numerous evaluation metrics have been developed for natural language\ngeneration tasks, but their effectiveness in evaluating stories is limited as\nthey are not specifically tailored to assess intricate aspects of storytelling,\nsuch as fluency and interestingness. In this paper, we introduce DELTASCORE, a\nnovel methodology that employs perturbation techniques for the evaluation of\nnuanced story aspects. Our central proposition posits that the extent to which\na story excels in a specific aspect (e.g., fluency) correlates with the\nmagnitude of its susceptibility to particular perturbations (e.g., the\nintroduction of typos). Given this, we measure the quality of an aspect by\ncalculating the likelihood difference between pre- and post-perturbation states\nusing pre-trained language models. We compare DELTASCORE with existing metrics\non storytelling datasets from two domains in five fine-grained story aspects:\nfluency, coherence, relatedness, logicality, and interestingness. DELTASCORE\ndemonstrates remarkable performance, revealing a surprising finding that a\nspecific perturbation proves highly effective in capturing multiple aspects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhuohan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Miao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAMEL: Communicative Agents for \"Mind\" Exploration of Large Language Model Society. (arXiv:2303.17760v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2303.17760","description":"<p>The rapid advancement of chat-based language models has led to remarkable\nprogress in complex task-solving. However, their success heavily relies on\nhuman input to guide the conversation, which can be challenging and\ntime-consuming. This paper explores the potential of building scalable\ntechniques to facilitate autonomous cooperation among communicative agents, and\nprovides insight into their \"cognitive\" processes. To address the challenges of\nachieving autonomous cooperation, we propose a novel communicative agent\nframework named role-playing. Our approach involves using inception prompting\nto guide chat agents toward task completion while maintaining consistency with\nhuman intentions. We showcase how role-playing can be used to generate\nconversational data for studying the behaviors and capabilities of a society of\nagents, providing a valuable resource for investigating conversational language\nmodels. In particular, we conduct comprehensive studies on\ninstruction-following cooperation in multi-agent settings. Our contributions\ninclude introducing a novel communicative agent framework, offering a scalable\napproach for studying the cooperative behaviors and capabilities of multi-agent\nsystems, and open-sourcing our library to support research on communicative\nagents and beyond: https://github.com/camel-ai/camel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guohao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammoud_H/0/1/0/all/0/1\">Hasan Abed Al Kader Hammoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itani_H/0/1/0/all/0/1\">Hani Itani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khizbullin_D/0/1/0/all/0/1\">Dmitrii Khizbullin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ParroT: Translating during Chat using Large Language Models tuned with Human Translation and Feedback. (arXiv:2304.02426v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.02426","description":"<p>Large language models (LLMs) like ChatGPT have exhibited remarkable abilities\non a wide range of natural language processing~(NLP) tasks, including various\nmachine translation abilities accomplished during chat. However, these models\nare only accessible through restricted APIs, which creates barriers to new\nresearch and advancements in the field. Therefore, we propose ParroT, a\nframework to enhance and regulate the translation abilities during chat based\non open-source LLMs (e.g., LLaMA), human-written translation and feedback data.\nSpecifically, ParroT reformulates translation data into the\ninstruction-following style, and introduces a \"$\\mathbf{Hint}$\" field for\nincorporating extra requirements to regulate the translation process.\nAccordingly, we propose three instruction types for finetuning ParroT models,\nincluding translation instruction, contrastive instruction, and error-guided\ninstruction. Experiments on Flores subsets and WMT22 test sets suggest that\ntranslation instruction improves the translation performance of vanilla LLMs\nsignificantly while error-guided instruction can lead to further improvement,\nwhich demonstrates the importance of learning from low-quality translations\nannotated by humans. We also demonstrate the potential of automatic evaluation\ntools in providing quality information of translations, when constructing\nerror-guided instructions for directions that lack human annotation data.\nPlease refer to our Github project for more implementation details:\nhttps://github.com/wxjiao/ParroT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jen-tse Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhiwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1\">Tian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.00586","description":"<p>Pre-trained language models can be surprisingly adept at tasks they were not\nexplicitly trained on, but how they implement these capabilities is poorly\nunderstood. In this paper, we investigate the basic mathematical abilities\noften acquired by pre-trained language models. Concretely, we use mechanistic\ninterpretability techniques to explain the (limited) mathematical abilities of\nGPT-2 small. As a case study, we examine its ability to take in sentences such\nas \"The war lasted from the year 1732 to the year 17\", and predict valid\ntwo-digit end years (years &gt; 32). We first identify a circuit, a small subset\nof GPT-2 small's computational graph that computes this task's output. Then, we\nexplain the role of each circuit component, showing that GPT-2 small's final\nmulti-layer perceptrons boost the probability of end years greater than the\nstart year. Finally, we find related tasks that activate our circuit. Our\nresults suggest that GPT-2 small computes greater-than using a complex but\ngeneral mechanism that activates across diverse contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hanna_M/0/1/0/all/0/1\">Michael Hanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_O/0/1/0/all/0/1\">Ollie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Variengien_A/0/1/0/all/0/1\">Alexandre Variengien</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can LMs Generalize to Future Data? An Empirical Analysis on Text Summarization. (arXiv:2305.01951v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01951","description":"<p>Recent pre-trained language models (PLMs) achieve promising results in\nexisting abstractive summarization datasets. However, existing summarization\nbenchmarks overlap in time with the standard pre-training corpora and\nfinetuning datasets. Hence, the strong performance of PLMs may rely on the\nparametric knowledge that is memorized during pre-training and fine-tuning.\nMoreover, the knowledge memorized by PLMs may quickly become outdated, which\naffects the generalization performance of PLMs on future data. In this work, we\npropose TempoSum, a novel benchmark that contains data samples from 2010 to\n2022, to understand the temporal generalization ability of abstractive\nsummarization models. Through extensive human evaluation, we show that\nparametric knowledge stored in summarization models significantly affects the\nfaithfulness of the generated summaries on future data. Moreover, existing\nfaithfulness enhancement methods cannot reliably improve the faithfulness of\nsummarization models on future data. Finally, we discuss several\nrecommendations to the research community on how to evaluate and improve the\ntemporal generalization capability of text summarization models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheang_C/0/1/0/all/0/1\">Chi Seng Cheang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Hou Pong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaocong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yanming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shudong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Lidia S. Chao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Textually Pretrained Speech Language Models. (arXiv:2305.13009v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13009","description":"<p>Speech language models (SpeechLMs) process and generate acoustic data only,\nwithout textual supervision. In this work, we propose TWIST, a method for\ntraining SpeechLMs using a warm-start from a pretrained textual language\nmodels. We show using both automatic and human evaluations that TWIST\noutperforms a cold-start SpeechLM across the board. We empirically analyze the\neffect of different model design choices such as the speech tokenizer, the\npretrained textual model, and the dataset size. We find that model and dataset\nscale both play an important role in constructing better-performing SpeechLMs.\nBased on our observations, we present the largest (to the best of our\nknowledge) SpeechLM both in terms of number of parameters and training data. We\nadditionally introduce two spoken versions of the StoryCloze textual benchmark\nto further improve model evaluation and advance future research in the field.\nWe make speech samples, code and models publicly available:\nhttps://pages.cs.huji.ac.il/adiyoss-lab/twist/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hassid_M/0/1/0/all/0/1\">Michael Hassid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gat_I/0/1/0/all/0/1\">Itai Gat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conneau_A/0/1/0/all/0/1\">Alexis Conneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreuk_F/0/1/0/all/0/1\">Felix Kreuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Defossez_A/0/1/0/all/0/1\">Alexandre Defossez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Universal Geometry in Embeddings with ICA. (arXiv:2305.13175v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13175","description":"<p>This study utilizes Independent Component Analysis (ICA) to unveil a\nconsistent semantic structure within embeddings of words or images. Our\napproach extracts independent semantic components from the embeddings of a\npre-trained model by leveraging anisotropic information that remains after the\nwhitening process in Principal Component Analysis (PCA). We demonstrate that\neach embedding can be expressed as a composition of a few intrinsic\ninterpretable axes and that these semantic axes remain consistent across\ndifferent languages, algorithms, and modalities. The discovery of a universal\nsemantic structure in the geometric patterns of embeddings enhances our\nunderstanding of the representations in embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamagiwa_H/0/1/0/all/0/1\">Hiroaki Yamagiwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyama_M/0/1/0/all/0/1\">Momose Oyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimodaira_H/0/1/0/all/0/1\">Hidetoshi Shimodaira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation. (arXiv:2305.13194v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13194","description":"<p>Reliable automatic evaluation of summarization systems is challenging due to\nthe multifaceted and subjective nature of the task. This is especially the case\nfor languages other than English, where human evaluations are scarce. In this\nwork, we introduce SEAHORSE, a dataset for multilingual, multifaceted\nsummarization evaluation. SEAHORSE consists of 96K summaries with human ratings\nalong 6 dimensions of text quality: comprehensibility, repetition, grammar,\nattribution, main ideas, and conciseness, covering 6 languages, 9 systems and 4\ndatasets. As a result of its size and scope, SEAHORSE can serve both as a\nbenchmark to evaluate learnt metrics, as well as a large-scale resource for\ntraining such metrics. We show that metrics trained with SEAHORSE achieve\nstrong performance on the out-of-domain meta-evaluation benchmarks TRUE\n(Honovich et al., 2022) and mFACE (Aharoni et al., 2022). We make the SEAHORSE\ndataset and metrics publicly available for future research on multilingual and\nmultifaceted summarization evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clark_E/0/1/0/all/0/1\">Elizabeth Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijhwani_S/0/1/0/all/0/1\">Shruti Rijhwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maynez_J/0/1/0/all/0/1\">Joshua Maynez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1\">Roee Aharoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaev_V/0/1/0/all/0/1\">Vitaly Nikolaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellam_T/0/1/0/all/0/1\">Thibault Sellam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddhant_A/0/1/0/all/0/1\">Aditya Siddhant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Dipanjan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_A/0/1/0/all/0/1\">Ankur P. Parikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-world Semi-supervised Generalized Relation Discovery Aligned in a Real-world Setting. (arXiv:2305.13533v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13533","description":"<p>Open-world Relation Extraction (OpenRE) has recently garnered significant\nattention. However, existing approaches tend to oversimplify the problem by\nassuming that all unlabeled texts belong to novel classes, thereby limiting the\npracticality of these methods. We argue that the OpenRE setting should be more\naligned with the characteristics of real-world data. Specifically, we propose\ntwo key improvements: (a) unlabeled data should encompass known and novel\nclasses, including hard-negative instances; and (b) the set of novel classes\nshould represent long-tail relation types. Furthermore, we observe that popular\nrelations such as titles and locations can often be implicitly inferred through\nspecific patterns, while long-tail relations tend to be explicitly expressed in\nsentences. Motivated by these insights, we present a novel method called KNoRD\n(Known and Novel Relation Discovery), which effectively classifies explicitly\nand implicitly expressed relations from known and novel classes within\nunlabeled data. Experimental evaluations on several Open-world RE benchmarks\ndemonstrate that KNoRD consistently outperforms other existing methods,\nachieving significant performance gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hogan_W/0/1/0/all/0/1\">William Hogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiacheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Legally Enforceable Hate Speech Detection for Public Forums. (arXiv:2305.13677v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13677","description":"<p>Hate speech causes widespread and deep-seated societal issues. Proper\nenforcement of hate speech laws is key for protecting groups of people against\nharmful and discriminatory language. However, determining what constitutes hate\nspeech is a complex task that is highly open to subjective interpretations.\nExisting works do not align their systems with enforceable definitions of hate\nspeech, which can make their outputs inconsistent with the goals of regulators.\nThis research introduces a new perspective and task for enforceable hate speech\ndetection centred around legal definitions, and a dataset annotated on\nviolations of eleven possible definitions by legal experts. Given the challenge\nof identifying clear, legally enforceable instances of hate speech, we augment\nthe dataset with expert-generated samples and an automatically mined challenge\nset. We experiment with grounding the model decision in these definitions using\nzero-shot and few-shot prompting. We then report results on several large\nlanguage models (LLMs). With this task definition, automatic hate speech\ndetection can be more closely aligned to enforceable laws, and hence assist in\nmore rigorous enforcement of legal protections against harmful speech in public\nforums.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chu Fei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhambhoria_R/0/1/0/all/0/1\">Rohan Bhambhoria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahan_S/0/1/0/all/0/1\">Samuel Dahan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models. (arXiv:2305.14999v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14999","description":"<p>Chain-of-Thought (CoT) prompting enables large language models to solve\ncomplex reasoning problems by generating intermediate steps. However, confined\nby its inherent single-pass and sequential generation process, CoT heavily\nrelies on the initial decisions, causing errors in early steps to accumulate\nand impact the final answers. In contrast, humans adopt recursive thinking when\ntackling complex reasoning problems, i.e., iteratively breaking the original\nproblem into approachable sub-problems and aggregating their answers to resolve\nthe original one. Inspired by the human cognitive process, we propose SOCRATIC\nQUESTIONING, a divide-and-conquer style algorithm that mimics the recursive\nthinking process. Specifically, SOCRATIC QUESTIONING leverages large language\nmodels to raise and answer sub-questions until collecting enough information to\ntackle the original question. Unlike CoT, SOCRATIC QUESTIONING explicitly\nnavigates the thinking space, stimulates effective recursive thinking, and is\nmore robust towards errors in the thinking process. Extensive experiments on\nseveral complex reasoning tasks, including MMLU, MATH, LogiQA, and visual\nquestion-answering demonstrate significant performance improvements over the\nstate-of-the-art prompting methods, such as CoT, and Tree-of-Thought. The\nqualitative analysis clearly shows that the intermediate reasoning steps\nelicited by SOCRATIC QUESTIONING are similar to humans' recursively thinking\nprocess of complex reasoning problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jingyuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minqian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diable: Efficient Dialogue State Tracking as Operations on Tables. (arXiv:2305.17020v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17020","description":"<p>Sequence-to-sequence state-of-the-art systems for dialogue state tracking\n(DST) use the full dialogue history as input, represent the current state as a\nlist with all the slots, and generate the entire state from scratch at each\ndialogue turn. This approach is inefficient, especially when the number of\nslots is large and the conversation is long. We propose Diable, a new task\nformalisation that simplifies the design and implementation of efficient DST\nsystems and allows one to easily plug and play large language models. We\nrepresent the dialogue state as a table and formalise DST as a table\nmanipulation task. At each turn, the system updates the previous state by\ngenerating table operations based on the dialogue context. Extensive\nexperimentation on the MultiWoz datasets demonstrates that Diable (i)\noutperforms strong efficient DST baselines, (ii) is 2.4x more time efficient\nthan current state-of-the-art methods while retaining competitive Joint Goal\nAccuracy, and (iii) is robust to noisy data annotations due to the table\noperations approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lesci_P/0/1/0/all/0/1\">Pietro Lesci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujinuma_Y/0/1/0/all/0/1\">Yoshinari Fujinuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardalov_M/0/1/0/all/0/1\">Momchil Hardalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_C/0/1/0/all/0/1\">Chao Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benajiba_Y/0/1/0/all/0/1\">Yassine Benajiba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marquez_L/0/1/0/all/0/1\">Lluis Marquez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AVIS: Autonomous Visual Information Seeking with Large Language Model Agent. (arXiv:2306.08129v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2306.08129","description":"<p>In this paper, we propose an autonomous information seeking visual question\nanswering framework, AVIS. Our method leverages a Large Language Model (LLM) to\ndynamically strategize the utilization of external tools and to investigate\ntheir outputs, thereby acquiring the indispensable knowledge needed to provide\nanswers to the posed questions. Responding to visual questions that necessitate\nexternal knowledge, such as \"What event is commemorated by the building\ndepicted in this image?\", is a complex task. This task presents a combinatorial\nsearch space that demands a sequence of actions, including invoking APIs,\nanalyzing their responses, and making informed decisions. We conduct a user\nstudy to collect a variety of instances of human decision-making when faced\nwith this task. This data is then used to design a system comprised of three\ncomponents: an LLM-powered planner that dynamically determines which tool to\nuse next, an LLM-powered reasoner that analyzes and extracts key information\nfrom the tool outputs, and a working memory component that retains the acquired\ninformation throughout the process. The collected user behavior serves as a\nguide for our system in two key ways. First, we create a transition graph by\nanalyzing the sequence of decisions made by users. This graph delineates\ndistinct states and confines the set of actions available at each state.\nSecond, we use examples of user decision-making to provide our LLM-powered\nplanner and reasoner with relevant contextual instances, enhancing their\ncapacity to make informed decisions. We show that AVIS achieves\nstate-of-the-art results on knowledge-intensive visual question answering\nbenchmarks such as Infoseek and OK-VQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Ziniu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iscen_A/0/1/0/all/0/1\">Ahmet Iscen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1\">David A Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathi_A/0/1/0/all/0/1\">Alireza Fathi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterated Piecewise Affine (IPA) Approximation for Language Modeling. (arXiv:2306.12317v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.12317","description":"<p>In this work, we demonstrate the application of a first-order Taylor\nexpansion to approximate a generic function $F: R^{n \\times m} \\to R^{n \\times\nm}$ and utilize it in language modeling. To enhance the basic Taylor expansion,\nwe introduce iteration and piecewise modeling, leading us to name the algorithm\nthe Iterative Piecewise Affine (IPA) approximation. The final algorithm\nexhibits interesting resemblances to the Transformers decoder architecture. By\ncomparing parameter arrangements in IPA and Transformers, we observe a\nstrikingly similar performance, with IPA outperforming Transformers by 1.5\\% in\nthe next token prediction task with cross-entropy loss for smaller sequence\nlengths.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shamsi_D/0/1/0/all/0/1\">Davood Shamsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wen-yu Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Brian Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Reasoning: Semantics-Symbol Deconstruction For Large Language Models. (arXiv:2306.17820v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.17820","description":"<p>Neural-symbolic methods have shown their effectiveness in enhancing the\nreasoning abilities of large language models (LLMs). However, existing methods\nprimarily rely on mapping natural languages to more syntactically complete\nformal languages (e.g., Python and SQL). Those approaches necessitate that\nreasoning tasks be convertible into programs, which cater more to the computer\nexecution mindset and deviate from human reasoning habits. To expand the\nreal-world applicability and flexibility of symbolic methods, we propose\nMeta-Reasoning from the scope of linguistics itself. This method empowers LLMs\nto deconstruct questions and effectively capture more generalized knowledge\nautonomously. We find that Meta-Reasoning achieves improved in-context learning\nefficiency, reasoning accuracy, and output stability in six arithmetic and\nsymbolic reasoning tasks. In particular, when applied to symbolic reasoning\ntasks such as Tracking Shuffled Objects, GPT-3 (text-davinci-002) surpasses the\nfew-shot Chain-of-Thought prompting approach (+37.7%), with 99% accuracy after\na single demonstration of Meta-Reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models. (arXiv:2307.02028v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2307.02028","description":"<p>While the general machine learning (ML) community has benefited from public\ndatasets, tasks, and models, the progress of ML in healthcare has been hampered\nby a lack of such shared assets. The success of foundation models creates new\nchallenges for healthcare ML by requiring access to shared pretrained models to\nvalidate performance benefits. We help address these challenges through three\ncontributions. First, we publish a new dataset, EHRSHOT, which contains\ndeidentified structured data from the electronic health records (EHRs) of 6,739\npatients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR\ndatasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients.\nSecond, we publish the weights of CLMBR-T-base, a 141M parameter clinical\nfoundation model pretrained on the structured EHR data of 2.57M patients. We\nare one of the first to fully release such a model for coded EHR data; in\ncontrast, most prior models released for clinical data (e.g. GatorTron,\nClinicalBERT) only work with unstructured text and cannot process the rich,\nstructured data within an EHR. We provide an end-to-end pipeline for the\ncommunity to validate and build upon its performance. Third, we define 15\nfew-shot clinical prediction tasks, enabling evaluation of foundation models on\nbenefits such as sample efficiency and task adaptation. Our model and dataset\nare available via a research data use agreement from the Stanford AIMI Center.\nCode to reproduce our results are available at our Github repo:\nhttps://github.com/som-shahlab/ehrshot-benchmark\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wornow_M/0/1/0/all/0/1\">Michael Wornow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thapa_R/0/1/0/all/0/1\">Rahul Thapa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinberg_E/0/1/0/all/0/1\">Ethan Steinberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fries_J/0/1/0/all/0/1\">Jason A. Fries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Nigam H. Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Alignment Is An Efficient Unified Model for Massive NLP Tasks. (arXiv:2307.02729v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.02729","description":"<p>Large language models (LLMs), typically designed as a function of next-word\nprediction, have excelled across extensive NLP tasks. Despite the generality,\nnext-word prediction is often not an efficient formulation for many of the\ntasks, demanding an extreme scale of model parameters (10s or 100s of billions)\nand sometimes yielding suboptimal performance. In practice, it is often\ndesirable to build more efficient models -- despite being less versatile, they\nstill apply to a substantial subset of problems, delivering on par or even\nsuperior performance with much smaller model sizes. In this paper, we propose\ntext alignment as an efficient unified model for a wide range of crucial tasks\ninvolving text entailment, similarity, question answering (and answerability),\nfactual consistency, and so forth. Given a pair of texts, the model measures\nthe degree of alignment between their information. We instantiate an alignment\nmodel (Align) through lightweight finetuning of RoBERTa (355M parameters) using\n5.9M examples from 28 datasets. Despite its compact size, extensive experiments\nshow the model's efficiency and strong performance: (1) On over 20 datasets of\naforementioned diverse tasks, the model matches or surpasses FLAN-T5 models\nthat have around 2x or 10x more parameters; the single unified model also\noutperforms task-specific models finetuned on individual datasets; (2) When\napplied to evaluate factual consistency of language generation on 23 datasets,\nour model improves over various baselines, including the much larger GPT-3.5\n(ChatGPT) and sometimes even GPT-4; (3) The lightweight model can also serve as\nan add-on component for LLMs such as GPT-3.5 in question answering tasks,\nimproving the average exact match (EM) score by 17.94 and F1 score by 15.05\nthrough identifying unanswerable questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zha_Y/0/1/0/all/0/1\">Yuheng Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yichi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruichen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models. (arXiv:2307.05973v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2307.05973","description":"<p>Large language models (LLMs) are shown to possess a wealth of actionable\nknowledge that can be extracted for robot manipulation in the form of reasoning\nand planning. Despite the progress, most still rely on pre-defined motion\nprimitives to carry out the physical interactions with the environment, which\nremains a major bottleneck. In this work, we aim to synthesize robot\ntrajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a\nlarge variety of manipulation tasks given an open-set of instructions and an\nopen-set of objects. We achieve this by first observing that LLMs excel at\ninferring affordances and constraints given a free-form language instruction.\nMore importantly, by leveraging their code-writing capabilities, they can\ninteract with a vision-language model (VLM) to compose 3D value maps to ground\nthe knowledge into the observation space of the agent. The composed value maps\nare then used in a model-based planning framework to zero-shot synthesize\nclosed-loop robot trajectories with robustness to dynamic perturbations. We\nfurther demonstrate how the proposed framework can benefit from online\nexperiences by efficiently learning a dynamics model for scenes that involve\ncontact-rich interactions. We present a large-scale study of the proposed\nmethod in both simulated and real-robot environments, showcasing the ability to\nperform a large variety of everyday manipulation tasks specified in free-form\nnatural language. Videos and code at https://voxposer.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenlong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunzhu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Overview of Large Language Models. (arXiv:2307.06435v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.06435","description":"<p>Large Language Models (LLMs) have recently demonstrated remarkable\ncapabilities in natural language processing tasks and beyond. This success of\nLLMs has led to a large influx of research contributions in this direction.\nThese works encompass diverse topics such as architectural innovations of the\nunderlying neural networks, context length improvements, model alignment,\ntraining datasets, benchmarking, efficiency and more. With the rapid\ndevelopment of techniques and regular breakthroughs in LLM research, it has\nbecome considerably challenging to perceive the bigger picture of the advances\nin this direction. Considering the rapidly emerging plethora of literature on\nLLMs, it is imperative that the research community is able to benefit from a\nconcise yet comprehensive overview of the recent developments in this field.\nThis article provides that overview to the research community. It not only\nfocuses on a systematic treatment of the existing literature on a broad range\nof LLM related concept, but also pays special attention to providing\ncomprehensive summaries with extensive details about the individual existing\nmodels, datasets and major insights. We also pay heed to aligning our overview\nwith the emerging outlook of this research direction by accounting for the\nother recently materializing reviews of the broader research direction of LLMs.\nOur self-contained comprehensive overview of LLMs discusses relevant background\nconcepts along with covering the advanced topics at the frontier of this\nresearch direction. This review article is intended to not only provide a\nsystematic survey, but also a quick comprehensive reference for the researchers\nand practitioners to draw insights from extensive informative summaries of the\nexisting works to advance the LLM research direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naveed_H/0/1/0/all/0/1\">Humza Naveed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Asad Ullah Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saqib_M/0/1/0/all/0/1\">Muhammad Saqib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usman_M/0/1/0/all/0/1\">Muhammad Usman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1\">Naveed Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM and Infrastructure as a Code use case. (arXiv:2309.01456v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.01456","description":"<p>Cloud computing and the evolution of management methodologies such as Lean\nManagement or Agile entail a profound transformation in both system\nconstruction and maintenance approaches. These practices are encompassed within\nthe term \"DevOps.\" This descriptive approach to an information system or\napplication, alongside the configuration of its constituent components, has\nnecessitated the development of descriptive languages paired with specialized\nengines for automating systems administration tasks. Among these, the tandem of\nAnsible (engine) and YAML (descriptive language) stands out as the two most\nprevalent tools in the market, facing notable competition mainly from\nTerraform. The current document presents an inquiry into a solution for\ngenerating and managing Ansible YAML roles and playbooks, utilizing Generative\nLLMs (Language Models) to translate human descriptions into code. Our efforts\nare focused on identifying plausible directions and outlining the potential\nindustrial applications. Note: For the purpose of this experiment, we have\nopted against the use of Ansible Lightspeed. This is due to its reliance on an\nIBM Watson model, for which we have not found any publicly available\nreferences. Comprehensive information regarding this remarkable technology can\nbe found [1] directly on our partner's website, RedHat.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chanus_T/0/1/0/all/0/1\">Thibault Chanus</a> (ENS Rennes), <a href=\"http://arxiv.org/find/cs/1/au:+Aubertin_M/0/1/0/all/0/1\">Michael Aubertin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling. (arXiv:2310.04691v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.04691","description":"<p>Neural language models are probabilistic models of human text. They are\npredominantly trained using maximum likelihood estimation (MLE), which is\nequivalent to minimizing the forward cross-entropy between the empirical data\ndistribution and the model distribution. However, various degeneration\nphenomena are still widely observed when decoding from the distributions\nlearned by such models. We establish that the forward cross-entropy is\nsuboptimal as a distance metric for aligning human and model distribution due\nto its (1) recall-prioritization (2) negative diversity ignorance and (3)\ntrain-test mismatch. In this paper, we propose Earth Mover Distance\nOptimization (EMO) for auto-regressive language modeling. EMO capitalizes on\nthe inherent properties of earth mover distance to address the aforementioned\nchallenges. Due to the high complexity of direct computation, we further\nintroduce a feasible upper bound for EMO to ease end-to-end training. Upon\nextensive evaluation of language models trained using EMO and MLE. We find that\nEMO demonstrates a consistently better language modeling performance than MLE\nacross domains. Moreover, EMO demonstrates noteworthy enhancements in\ndownstream performance with minimal fine-tuning on merely 25,000 sentences.\nThis highlights the tremendous potential of EMO as a lightweight calibration\nmethod for enhancing large-scale pre-trained language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Siyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kenny Q. Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Agents for Detecting Implicit Stereotypes in Text-to-image Models at Scale. (arXiv:2310.11778v3 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2310.11778","description":"<p>The recent surge in the research of diffusion models has accelerated the\nadoption of text-to-image models in various Artificial Intelligence Generated\nContent (AIGC) commercial products. While these exceptional AIGC products are\ngaining increasing recognition and sparking enthusiasm among consumers, the\nquestions regarding whether, when, and how these models might unintentionally\nreinforce existing societal stereotypes remain largely unaddressed. Motivated\nby recent advancements in language agents, here we introduce a novel agent\narchitecture tailored for stereotype detection in text-to-image models. This\nversatile agent architecture is capable of accommodating free-form detection\ntasks and can autonomously invoke various tools to facilitate the entire\nprocess, from generating corresponding instructions and images, to detecting\nstereotypes. We build the stereotype-relevant benchmark based on multiple\nopen-text datasets, and apply this architecture to commercial products and\npopular open source text-to-image models. We find that these models often\ndisplay serious stereotypes when it comes to certain prompts about personal\ncharacteristics, social cultural context and crime-related aspects. In summary,\nthese empirical findings underscore the pervasive existence of stereotypes\nacross social dimensions, including gender, race, and religion, which not only\nvalidate the effectiveness of our proposed approach, but also emphasize the\ncritical necessity of addressing potential ethical risks in the burgeoning\nrealm of AIGC. As AIGC continues its rapid expansion trajectory, with new\nmodels and plugins emerging daily in staggering numbers, the challenge lies in\nthe timely detection and mitigation of potential biases within these models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_T/0/1/0/all/0/1\">Tian Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yian Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tingyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen M. Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zibin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bingzhe Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QUDEVAL: The Evaluation of Questions Under Discussion Discourse Parsing. (arXiv:2310.14520v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.14520","description":"<p>Questions Under Discussion (QUD) is a versatile linguistic framework in which\ndiscourse progresses as continuously asking questions and answering them.\nAutomatic parsing of a discourse to produce a QUD structure thus entails a\ncomplex question generation task: given a document and an answer sentence,\ngenerate a question that satisfies linguistic constraints of QUD and can be\ngrounded in an anchor sentence in prior context. These questions are known to\nbe curiosity-driven and open-ended. This work introduces the first framework\nfor the automatic evaluation of QUD parsing, instantiating the theoretical\nconstraints of QUD in a concrete protocol. We present QUDeval, a dataset of\nfine-grained evaluation of 2,190 QUD questions generated from both fine-tuned\nsystems and LLMs. Using QUDeval, we show that satisfying all constraints of QUD\nis still challenging for modern LLMs, and that existing evaluation metrics\npoorly approximate parser quality. Encouragingly, human-authored QUDs are\nscored highly by our human evaluators, suggesting that there is headroom for\nfurther progress on language modeling to improve both QUD parsing and QUD\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yating Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangla_R/0/1/0/all/0/1\">Ritika Mangla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Air-Decoding: Attribute Distribution Reconstruction for Decoding-Time Controllable Text Generation. (arXiv:2310.14892v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.14892","description":"<p>Controllable text generation (CTG) aims to generate text with desired\nattributes, and decoding-time-based methods have shown promising performance on\nthis task. However, in this paper, we identify the phenomenon of Attribute\nCollapse for the first time. It causes the fluency of generated text to rapidly\ndecrease when the control strength exceeds a critical value, rendering the text\ncompletely unusable. This limitation hinders the effectiveness of decoding\nmethods in achieving high levels of controllability. To address this problem,\nwe propose a novel lightweight decoding framework named Air-Decoding. Its main\nidea is reconstructing the attribute distributions to balance the weights\nbetween attribute words and non-attribute words to generate more fluent text.\nSpecifically, we train prefixes by prefix-tuning to obtain attribute\ndistributions. Then we design a novel attribute distribution reconstruction\nmethod to balance the obtained distributions and use the reconstructed\ndistributions to guide language models for generation, effectively avoiding the\nissue of Attribute Collapse. Experiments on multiple CTG tasks prove that our\nmethod achieves a new state-of-the-art control performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_T/0/1/0/all/0/1\">Tianqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jingxuan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhendong Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering. (arXiv:2310.17490v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.17490","description":"<p>Large language models (LLMs) enable zero-shot approaches in open-domain\nquestion answering (ODQA), yet with limited advancements as the reader is\ncompared to the retriever. This study aims at the feasibility of a zero-shot\nreader that addresses the challenges of computational cost and the need for\nlabeled data. We find that LLMs are distracted due to irrelevant documents in\nthe retrieved set and the overconfidence of the generated answers when they are\nexploited as zero-shot readers. To tackle these problems, we mitigate the\nimpact of such documents via Distraction-aware Answer Selection (DAS) with a\nnegation-based instruction and score adjustment for proper answer selection.\nExperimental results show that our approach successfully handles distraction\nacross diverse scenarios, enhancing the performance of zero-shot readers.\nFurthermore, unlike supervised readers struggling with unseen data, zero-shot\nreaders demonstrate outstanding transferability without any training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sukmin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Jeongyeon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1\">Soyeong Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jong C. Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meaning Representations from Trajectories in Autoregressive Models. (arXiv:2310.18348v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.18348","description":"<p>We propose to extract meaning representations from autoregressive language\nmodels by considering the distribution of all possible trajectories extending\nan input text. This strategy is prompt-free, does not require fine-tuning, and\nis applicable to any pre-trained autoregressive model. Moreover, unlike\nvector-based representations, distribution-based representations can also model\nasymmetric relations (e.g., direction of logical entailment, hypernym/hyponym\nrelations) by using algebraic operations between likelihood functions. These\nideas are grounded in distributional perspectives on semantics and are\nconnected to standard constructions in automata theory, but to our knowledge\nthey have not been applied to modern language models. We empirically show that\nthe representations obtained from large models align well with human\nannotations, outperform other zero-shot and prompt-free methods on semantic\nsimilarity tasks, and can be used to solve more complex entailment and\ncontainment tasks that standard embeddings cannot handle. Finally, we extend\nour method to represent data from different modalities (e.g., image and text)\nusing multimodal autoregressive models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tian Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trager_M/0/1/0/all/0/1\">Matthew Trager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1\">Alessandro Achille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perera_P/0/1/0/all/0/1\">Pramuditha Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zancato_L/0/1/0/all/0/1\">Luca Zancato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JADE: A Linguistics-based Safety Evaluation Platform for LLM. (arXiv:2311.00286v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.00286","description":"<p>In this paper, we present JADE, a targeted linguistic fuzzing platform which\nstrengthens the linguistic complexity of seed questions to simultaneously and\nconsistently break a wide range of widely-used LLMs categorized in three\ngroups: eight open-sourced Chinese, six commercial Chinese and four commercial\nEnglish LLMs. JADE generates three safety benchmarks for the three groups of\nLLMs, which contain unsafe questions that are highly threatening: the questions\nsimultaneously trigger harmful generation of multiple LLMs, with an average\nunsafe generation ratio of $70\\%$ (please see the table below), while are still\nnatural questions, fluent and preserving the core unsafe semantics. We release\nthe benchmark demos generated for commercial English LLMs and open-sourced\nEnglish LLMs in the following link: https://github.com/whitzard-ai/jade-db. For\nreaders who are interested in evaluating on more questions generated by JADE,\nplease contact us.\n</p>\n<p>JADE is based on Noam Chomsky's seminal theory of transformational-generative\ngrammar. Given a seed question with unsafe intention, JADE invokes a sequence\nof generative and transformational rules to increment the complexity of the\nsyntactic structure of the original question, until the safety guardrail is\nbroken. Our key insight is: Due to the complexity of human language, most of\nthe current best LLMs can hardly recognize the invariant evil from the infinite\nnumber of different syntactic structures which form an unbound example space\nthat can never be fully covered. Technically, the generative/transformative\nrules are constructed by native speakers of the languages, and, once developed,\ncan be used to automatically grow and transform the parse tree of a given\nquestion, until the guardrail is broken. For more evaluation results and demo,\nplease check our website: https://whitzard-ai.github.io/jade.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xudong Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-11-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}