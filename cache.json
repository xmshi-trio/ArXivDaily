{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-11-04T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Hierarchies over Vector Space: Orienting Word and Graph Embeddings. (arXiv:2211.01430v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01430","description":"<p>Word and graph embeddings are widely used in deep learning applications. We\npresent a data structure that captures inherent hierarchical properties from an\nunordered flat embedding space, particularly a sense of direction between pairs\nof entities. Inspired by the notion of \\textit{distributional generality}, our\nalgorithm constructs an arborescence (a directed rooted tree) by inserting\nnodes in descending order of entity power (e.g., word frequency), pointing each\nentity to the closest more powerful node as its parent.\n</p>\n<p>We evaluate the performance of the resulting tree structures on three tasks:\nhypernym relation discovery, least-common-ancestor (LCA) discovery among words,\nand Wikipedia page link recovery. We achieve average 8.98\\% and 2.70\\% for\nhypernym and LCA discovery across five languages and 62.76\\% accuracy on\ndirected Wiki-page link recovery, with both substantially above baselines.\nFinally, we investigate the effect of insertion order, the power/similarity\ntrade-off and various power sources to optimize parent selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xingzhi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skiena_S/0/1/0/all/0/1\">Steven Skiena</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-stitching Text and Knowledge Graph Encoders for Distantly Supervised Relation Extraction. (arXiv:2211.01432v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01432","description":"<p>Bi-encoder architectures for distantly-supervised relation extraction are\ndesigned to make use of the complementary information found in text and\nknowledge graphs (KG). However, current architectures suffer from two\ndrawbacks. They either do not allow any sharing between the text encoder and\nthe KG encoder at all, or, in case of models with KG-to-text attention, only\nshare information in one direction. Here, we introduce cross-stitch\nbi-encoders, which allow full interaction between the text encoder and the KG\nencoder via a cross-stitch mechanism. The cross-stitch mechanism allows sharing\nand updating representations between the two encoders at any layer, with the\namount of sharing being dynamically controlled via cross-attention-based gates.\nExperimental results on two relation extraction benchmarks from two different\ndomains show that enabling full interaction between the two encoders yields\nstrong improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinzerling_B/0/1/0/all/0/1\">Benjamin Heinzerling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variable Attention Masking for Configurable Transformer Transducer Speech Recognition. (arXiv:2211.01438v1 [eess.AS])","link":"http://arxiv.org/abs/2211.01438","description":"<p>This work studies the use of attention masking in transformer transducer\nbased speech recognition for building a single configurable model for different\ndeployment scenarios. We present a comprehensive set of experiments comparing\nfixed masking, where the same attention mask is applied at every frame, with\nchunked masking, where the attention mask for each frame is determined by chunk\nboundaries, in terms of recognition accuracy and latency. We then explore the\nuse of variable masking, where the attention masks are sampled from a target\ndistribution at training time, to build models that can work in different\nconfigurations. Finally, we investigate how a single configurable model can be\nused to perform both first pass streaming recognition and second pass acoustic\nrescoring. Experiments show that chunked masking achieves a better accuracy vs\nlatency trade-off compared to fixed masking, both with and without FastEmit. We\nalso show that variable masking improves the accuracy by up to 8% relative in\nthe acoustic re-scoring scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Swietojanski_P/0/1/0/all/0/1\">Pawel Swietojanski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Braun_S/0/1/0/all/0/1\">Stefan Braun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Can_D/0/1/0/all/0/1\">Dogan Can</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Silva_T/0/1/0/all/0/1\">Thiago Fraga da Silva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghoshal_A/0/1/0/all/0/1\">Arnab Ghoshal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hori_T/0/1/0/all/0/1\">Takaaki Hori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsiao_R/0/1/0/all/0/1\">Roger Hsiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mason_H/0/1/0/all/0/1\">Henry Mason</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McDermott_E/0/1/0/all/0/1\">Erik McDermott</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Silovsky_H/0/1/0/all/0/1\">Honza Silovsky</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Travadi_R/0/1/0/all/0/1\">Ruchir Travadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiaodan Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Zero-Shot Code-Switched Speech Recognition. (arXiv:2211.01458v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01458","description":"<p>In this work, we seek to build effective code-switched (CS) automatic speech\nrecognition systems (ASR) under the zero-shot setting where no transcribed CS\nspeech data is available for training. Previously proposed frameworks which\nconditionally factorize the bilingual task into its constituent monolingual\nparts are a promising starting point for leveraging monolingual data\nefficiently. However, these methods require the monolingual modules to perform\nlanguage segmentation. That is, each monolingual module has to simultaneously\ndetect CS points and transcribe speech segments of one language while ignoring\nthose of other languages -- not a trivial task. We propose to simplify each\nmonolingual module by allowing them to transcribe all speech segments\nindiscriminately with a monolingual script (i.e. transliteration). This simple\nmodification passes the responsibility of CS point detection to subsequent\nbilingual modules which determine the final output by considering multiple\nmonolingual transliterations along with external language model information. We\napply this transliteration-based approach in an end-to-end differentiable\nneural network and demonstrate its efficacy for zero-shot CS ASR on\nMandarin-English SEAME test sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiesner_M/0/1/0/all/0/1\">Matthew Wiesner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klejch_O/0/1/0/all/0/1\">Ondrej Klejch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1\">Preethi Jyothi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phoneme Segmentation Using Self-Supervised Speech Models. (arXiv:2211.01461v1 [eess.AS])","link":"http://arxiv.org/abs/2211.01461","description":"<p>We apply transfer learning to the task of phoneme segmentation and\ndemonstrate the utility of representations learned in self-supervised\npre-training for the task. Our model extends transformer-style encoders with\nstrategically placed convolutions that manipulate features learned in\npre-training. Using the TIMIT and Buckeye corpora we train and test the model\nin the supervised and unsupervised settings. The latter case is accomplished by\nfurnishing a noisy label-set with the predictions of a separate model, it\nhaving been trained in an unsupervised fashion. Results indicate our model\neclipses previous state-of-the-art performance in both settings and on both\ndatasets. Finally, following observations during published code review and\nattempts to reproduce past segmentation results, we find a need to disambiguate\nthe definition and implementation of widely-used evaluation metrics. We resolve\nthis ambiguity by delineating two distinct evaluation schemes and describing\ntheir nuances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Strgar_L/0/1/0/all/0/1\">Luke Strgar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Entity-to-Entity Stance Detection with Knowledge Graph Augmentation. (arXiv:2211.01467v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01467","description":"<p>Stance detection is typically framed as predicting the sentiment in a given\ntext towards a target entity. However, this setup overlooks the importance of\nthe source entity, i.e., who is expressing the opinion. In this paper, we\nemphasize the need for studying interactions among entities when inferring\nstances. We first introduce a new task, entity-to-entity (E2E) stance\ndetection, which primes models to identify entities in their canonical names\nand discern stances jointly. To support this study, we curate a new dataset\nwith 10,619 annotations labeled at the sentence-level from news articles of\ndifferent ideological leanings. We present a novel generative framework to\nallow the generation of canonical names for entities as well as stances among\nthem. We further enhance the model with a graph encoder to summarize entity\nactivities and external knowledge surrounding the entities. Experiments show\nthat our model outperforms strong comparisons by large margins. Further\nanalyses demonstrate the usefulness of E2E stance detection for understanding\nmedia quotation and stance landscape, as well as inferring entity ideology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinliang Frederick Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beauchamp_N/0/1/0/all/0/1\">Nick Beauchamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Over-communicate no more: Situated RL agents learn concise communication protocols. (arXiv:2211.01480v1 [cs.MA])","link":"http://arxiv.org/abs/2211.01480","description":"<p>While it is known that communication facilitates cooperation in multi-agent\nsettings, it is unclear how to design artificial agents that can learn to\neffectively and efficiently communicate with each other. Much research on\ncommunication emergence uses reinforcement learning (RL) and explores\nunsituated communication in one-step referential tasks -- the tasks are not\ntemporally interactive and lack time pressures typically present in natural\ncommunication. In these settings, agents may successfully learn to communicate,\nbut they do not learn to exchange information concisely -- they tend towards\nover-communication and an inefficient encoding. Here, we explore situated\ncommunication in a multi-step task, where the acting agent has to forgo an\nenvironmental action to communicate. Thus, we impose an opportunity cost on\ncommunication and mimic the real-world pressure of passing time. We compare\ncommunication emergence under this pressure against learning to communicate\nwith a cost on articulation effort, implemented as a per-message penalty (fixed\nand progressively increasing). We find that while all tested pressures can\ndisincentivise over-communication, situated communication does it most\neffectively and, unlike the cost on effort, does not negatively impact\nemergence. Implementing an opportunity cost on communication in a temporally\nextended environment is a step towards embodiment, and might be a pre-condition\nfor incentivising efficient, human-like communication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalinowska_A/0/1/0/all/0/1\">Aleksandra Kalinowska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davoodi_E/0/1/0/all/0/1\">Elnaz Davoodi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strub_F/0/1/0/all/0/1\">Florian Strub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathewson_K/0/1/0/all/0/1\">Kory W Mathewson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kajic_I/0/1/0/all/0/1\">Ivana Kajic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowling_M/0/1/0/all/0/1\">Michael Bowling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphey_T/0/1/0/all/0/1\">Todd D Murphey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilarski_P/0/1/0/all/0/1\">Patrick M Pilarski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering the Question. (arXiv:2211.01482v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01482","description":"<p>Existing metrics for evaluating the quality of automatically generated\nquestions such as BLEU, ROUGE, BERTScore, and BLEURT compare the reference and\npredicted questions, providing a high score when there is a considerable\nlexical overlap or semantic similarity between the candidate and the reference\nquestions. This approach has two major shortcomings. First, we need expensive\nhuman-provided reference questions. Second, it penalises valid questions that\nmay not have high lexical or semantic similarity to the reference questions. In\nthis paper, we propose a new metric, RQUGE, based on the answerability of the\ncandidate question given the context. The metric consists of a\nquestion-answering and a span scorer module, in which we use pre-trained models\nfrom the existing literature, and therefore, our metric can be used without\nfurther training. We show that RQUGE has a higher correlation with human\njudgment without relying on the reference question. RQUGE is shown to be\nsignificantly more robust to several adversarial corruptions. Additionally, we\nillustrate that we can significantly improve the performance of QA models on\nout-of-domain datasets by fine-tuning on the synthetic data generated by a\nquestion generation model and re-ranked by RQUGE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadshahi_A/0/1/0/all/0/1\">Alireza Mohammadshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_M/0/1/0/all/0/1\">Majid Yazdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanki_P/0/1/0/all/0/1\">Pouya Yanki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1\">Angela Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeidi_M/0/1/0/all/0/1\">Marzieh Saeidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Resource-Performance Trade-off of Natural Language Models using Data Envelopment Analysis. (arXiv:2211.01486v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01486","description":"<p>Natural language models are often summarized through a high-dimensional set\nof descriptive metrics including training corpus size, training time, the\nnumber of trainable parameters, inference times, and evaluation statistics that\nassess performance across tasks. The high dimensional nature of these metrics\nyields challenges with regard to objectively comparing models; in particular it\nis challenging to assess the trade-off models make between performance and\nresources (compute time, memory, etc.).\n</p>\n<p>We apply Data Envelopment Analysis (DEA) to this problem of assessing the\nresource-performance trade-off. DEA is a nonparametric method that measures\nproductive efficiency of abstract units that consume one or more inputs and\nyield at least one output. We recast natural language models as units suitable\nfor DEA, and we show that DEA can be used to create an effective framework for\nquantifying model performance and efficiency. A central feature of DEA is that\nit identifies a subset of models that live on an efficient frontier of\nperformance. DEA is also scalable, having been applied to problems with\nthousands of units. We report empirical results of DEA applied to 14 different\nlanguage models that have a variety of architectures, and we show that DEA can\nbe used to identify a subset of models that effectively balance resource\ndemands against performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zachary Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zachariah_A/0/1/0/all/0/1\">Alisha Zachariah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conathan_D/0/1/0/all/0/1\">Devin Conathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kline_J/0/1/0/all/0/1\">Jeffery Kline</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAST: Multiscale Audio Spectrogram Transformers. (arXiv:2211.01515v1 [eess.AS])","link":"http://arxiv.org/abs/2211.01515","description":"<p>We present Multiscale Audio Spectrogram Transformer (MAST) for audio\nclassification, which brings the concept of multiscale feature hierarchies to\nthe Audio Spectrogram Transformer (AST). Given an input audio spectrogram we\nfirst patchify and project it into an initial temporal resolution and embedding\ndimension, post which the multiple stages in MAST progressively expand the\nembedding dimension while reducing the temporal resolution of the input. We use\na pyramid structure that allows early layers of MAST operating at a high\ntemporal resolution but low embedding space to model simple low-level acoustic\ninformation and deeper temporally coarse layers to model high-level acoustic\ninformation with high-dimensional embeddings. We also extend our approach to\npresent a new Self-Supervised Learning (SSL) method called SS-MAST, which\ncalculates a symmetric contrastive loss between latent representations from a\nstudent and a teacher encoder. In practice, MAST significantly outperforms AST\nby an average accuracy of 3.4% across 8 speech and non-speech tasks from the\nLAPE Benchmark. Moreover, SS-MAST achieves an absolute average improvement of\n2.6% over SSAST for both AST and MAST encoders. We make all our codes available\non GitHub at the time of publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seth_A/0/1/0/all/0/1\">Ashish Seth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLICER: Learning universal audio representations using low-resource self-supervised pre-training. (arXiv:2211.01519v1 [eess.AS])","link":"http://arxiv.org/abs/2211.01519","description":"<p>We present a new Self-Supervised Learning (SSL) approach to pre-train\nencoders on unlabeled audio data that reduces the need for large amounts of\nlabeled data for audio and speech classification. Our primary aim is to learn\naudio representations that can generalize across a large variety of speech and\nnon-speech tasks in a low-resource un-labeled audio pre-training setting.\nInspired by the recent success of clustering and contrasting learning paradigms\nfor SSL-based speech representation learning, we propose SLICER (Symmetrical\nLearning of Instance and Cluster-level Efficient Representations), which brings\ntogether the best of both clustering and contrasting learning paradigms. We use\na symmetric loss between latent representations from student and teacher\nencoders and simultaneously solve instance and cluster-level contrastive\nlearning tasks. We obtain cluster representations online by just projecting the\ninput spectrogram into an output subspace with dimensions equal to the number\nof clusters. In addition, we propose a novel mel-spectrogram augmentation\nprocedure, k-mix, based on mixup, which does not require labels and aids\nunsupervised representation learning for audio. Overall, SLICER achieves\nstate-of-the-art results on the LAPE Benchmark \\cite{9868132}, significantly\noutperforming DeLoRes-M and other prior approaches, which are pre-trained on\n$10\\times$ larger of unsupervised data. We will make all our codes available on\nGitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Seth_A/0/1/0/all/0/1\">Ashish Seth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Learning of Neural Machine Translation within Low Forgetting Risk Regions. (arXiv:2211.01542v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01542","description":"<p>This paper considers continual learning of large-scale pretrained neural\nmachine translation model without accessing the previous training data or\nintroducing model separation. We argue that the widely used\nregularization-based methods, which perform multi-objective learning with an\nauxiliary loss, suffer from the misestimate problem and cannot always achieve a\ngood balance between the previous and new tasks. To solve the problem, we\npropose a two-stage training method based on the local features of the real\nloss. We first search low forgetting risk regions, where the model can retain\nthe performance on the previous task as the parameters are updated, to avoid\nthe catastrophic forgetting problem. Then we can continually train the model\nwithin this region only with the new training data to fit the new task.\nSpecifically, we propose two methods to search the low forgetting risk regions,\nwhich are based on the curvature of loss and the impacts of the parameters on\nthe model output, respectively. We conduct experiments on domain adaptation and\nmore challenging language adaptation tasks, and the experimental results show\nthat our method can achieve significant improvements compared with several\nstrong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shuhao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bojie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PINTO: Faithful Language Reasoning Using Prompt-Generated Rationales. (arXiv:2211.01562v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01562","description":"<p>Neural language models (LMs) have achieved impressive results on various\nlanguage-based reasoning tasks by utilizing latent knowledge encoded in their\nown pretrained parameters. To make this reasoning process more explicit, recent\nworks retrieve a rationalizing LM's internal knowledge by training or prompting\nit to generate free-text rationales, which can be used to guide task\npredictions made by either the same LM or a separate reasoning LM. However,\nrationalizing LMs require expensive rationale annotation and/or computation,\nwithout any assurance that their generated rationales improve LM task\nperformance or faithfully reflect LM decision-making. In this paper, we propose\nPINTO, an LM pipeline that rationalizes via prompt-based learning, and learns\nto faithfully reason over rationales via counterfactual regularization. First,\nPINTO maps out a suitable reasoning process for the task input by prompting a\nfrozen rationalizing LM to generate a free-text rationale. Second, PINTO's\nreasoning LM is fine-tuned to solve the task using the generated rationale as\ncontext, while regularized to output less confident predictions when the\nrationale is perturbed. Across four datasets, we show that PINTO significantly\nimproves the generalization ability of the reasoning LM, yielding higher\nperformance on both in-distribution and out-of-distribution test sets. Also, we\nfind that PINTO's rationales are more faithful to its task predictions than\nthose generated by competitive baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Aaron Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Tuning Language Models via Epistemic Neural Networks. (arXiv:2211.01568v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01568","description":"<p>Large language models are now part of a powerful new paradigm in machine\nlearning. These models learn a wide range of capabilities from training on\nlarge unsupervised text corpora. In many applications, these capabilities are\nthen fine-tuned through additional training on specialized data to improve\nperformance in that setting. In this paper, we augment these models with an\nepinet: a small additional network architecture that helps to estimate model\nuncertainty and form an epistemic neural network (ENN). ENNs are neural\nnetworks that can know what they don't know. We show that, using an epinet to\nprioritize uncertain data, we can fine-tune BERT on GLUE tasks to the same\nperformance while using 2x less data. We also investigate performance in\nsynthetic neural network generative models designed to build understanding. In\neach setting, using an epinet outperforms heuristic active learning schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Osband_I/0/1/0/all/0/1\">Ian Osband</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asghari_S/0/1/0/all/0/1\">Seyed Mohammad Asghari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1\">Benjamin Van Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAleese_N/0/1/0/all/0/1\">Nat McAleese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aslanides_J/0/1/0/all/0/1\">John Aslanides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irving_G/0/1/0/all/0/1\">Geoffrey Irving</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Vocabulary Argument Role Prediction for Event Extraction. (arXiv:2211.01577v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01577","description":"<p>The argument role in event extraction refers to the relation between an event\nand an argument participating in it. Despite the great progress in event\nextraction, existing studies still depend on roles pre-defined by domain\nexperts. These studies expose obvious weakness when extending to emerging event\ntypes or new domains without available roles. Therefore, more attention and\neffort needs to be devoted to automatically customizing argument roles. In this\npaper, we define this essential but under-explored task: open-vocabulary\nargument role prediction. The goal of this task is to infer a set of argument\nroles for a given event type. We propose a novel unsupervised framework,\nRolePred for this task. Specifically, we formulate the role prediction problem\nas an in-filling task and construct prompts for a pre-trained language model to\ngenerate candidate roles. By extracting and analyzing the candidate arguments,\nthe event-specific roles are further merged and selected. To standardize the\nresearch of this task, we collect a new event extraction dataset from\nWikiPpedia including 142 customized argument roles with rich semantics. On this\ndataset, RolePred outperforms the existing methods by a large margin. Source\ncode and dataset are available on our GitHub repository:\nhttps://github.com/yzjiao/RolePred\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yizhu Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yiqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eliciting Knowledge from Large Pre-Trained Models for Unsupervised Knowledge-Grounded Conversation. (arXiv:2211.01587v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01587","description":"<p>Recent advances in large-scale pre-training provide large models with the\npotential to learn knowledge from the raw text. It is thus natural to ask\nwhether it is possible to leverage these large models as knowledge bases for\ndownstream tasks. In this work, we answer the aforementioned question in\nunsupervised knowledge-grounded conversation. We explore various methods that\nbest elicit knowledge from large models. Our human study indicates that, though\nhallucinations exist, large models post the unique advantage of being able to\noutput common sense and summarize facts that cannot be directly retrieved from\nthe search engine. To better exploit such generated knowledge in dialogue\ngeneration, we treat the generated knowledge as a noisy knowledge source and\npropose the posterior-based reweighing as well as the noisy training strategy.\nEmpirical results on two benchmarks show advantages over the state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jianqiao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Spelling to Grammar: A New Framework for Chinese Grammatical Error Correction. (arXiv:2211.01625v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01625","description":"<p>Chinese Grammatical Error Correction (CGEC) aims to generate a correct\nsentence from an erroneous sequence, where different kinds of errors are mixed.\nThis paper divides the CGEC task into two steps, namely spelling error\ncorrection and grammatical error correction. Specifically, we propose a novel\nzero-shot approach for spelling error correction, which is simple but\neffective, obtaining a high precision to avoid error accumulation of the\npipeline structure. To handle grammatical error correction, we design\npart-of-speech (POS) features and semantic class features to enhance the neural\nnetwork model, and propose an auxiliary task to predict the POS sequence of the\ntarget sentence. Our proposed framework achieves a 42.11 F0.5 score on CGEC\ndataset without using any synthetic data or data augmentation methods, which\noutperforms the previous state-of-the-art by a wide margin of 1.30 points.\nMoreover, our model produces meaningful POS representations that capture\ndifferent POS words and convey reasonable POS transition rules.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiuyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Grammatical Error Correction Evaluation and Beyond. (arXiv:2211.01635v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01635","description":"<p>Pretraining-based (PT-based) automatic evaluation metrics (e.g., BERTScore\nand BARTScore) have been widely used in several sentence generation tasks\n(e.g., machine translation and text summarization) due to their better\ncorrelation with human judgments over traditional overlap-based methods.\nAlthough PT-based methods have become the de facto standard for training\ngrammatical error correction (GEC) systems, GEC evaluation still does not\nbenefit from pretrained knowledge. This paper takes the first step towards\nunderstanding and improving GEC evaluation with pretraining. We first find that\narbitrarily applying PT-based metrics to GEC evaluation brings unsatisfactory\ncorrelation results because of the excessive attention to inessential systems\noutputs (e.g., unchanged parts). To alleviate the limitation, we propose a\nnovel GEC evaluation metric to achieve the best of both worlds, namely PT-M2\nwhich only uses PT-based metrics to score those corrected parts. Experimental\nresults on the CoNLL14 evaluation task show that PT-M2 significantly\noutperforms existing methods, achieving a new state-of-the-art result of 0.949\nPearson correlation. Further analysis reveals that PT-M2 is robust to evaluate\ncompetitive GEC systems. Source code and scripts are freely available at\nhttps://github.com/pygongnlp/PT-M2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_P/0/1/0/all/0/1\">Peiyuan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Chinese Word Segmentation and Span-based Constituency Parsing. (arXiv:2211.01638v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01638","description":"<p>In constituency parsing, span-based decoding is an important direction.\nHowever, for Chinese sentences, because of their linguistic characteristics, it\nis necessary to utilize other models to perform word segmentation first, which\nintroduces a series of uncertainties and generally leads to errors in the\ncomputation of the constituency tree afterward. This work proposes a method for\njoint Chinese word segmentation and Span-based Constituency Parsing by adding\nextra labels to individual Chinese characters on the parse trees. Through\nexperiments, the proposed algorithm outperforms the recent models for joint\nsegmentation and constituency parsing on CTB 5.1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1\">Tianyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Tuning Pre-Trained Language Models Effectively by Optimizing Subnetworks Adaptively. (arXiv:2211.01642v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01642","description":"<p>Large-scale pre-trained language models have achieved impressive results on a\nwide range of downstream tasks recently. However, fine-tuning an extremely\nlarge-scale pre-trained language model on limited target datasets is often\nplagued by overfitting and representation degradation. In this paper, we\npropose a Dynamic Parameter Selection (DPS) algorithm for the large-scale\npre-trained models during fine-tuning, which adaptively selects a more\npromising subnetwork to perform staging updates based on gradients of\nback-propagation. Experiments on the GLUE benchmark show that DPS outperforms\nprevious fine-tuning methods in terms of overall performance and stability, and\nconsistently achieves better results with variable pre-trained language models.\nIn addition, DPS brings a large magnitude of improvement in out-of-domain\ntransferring experiments and low-resource scenarios, which shows that it can\nmaintain stable general contextual features and reduce the representation\ncollapse. We release our code at https://github.com/ZhangHaojie077/DPS\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haojie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhongjin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhi Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spam Review Detection Using Deep Learning. (arXiv:2211.01675v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01675","description":"<p>A robust and reliable system of detecting spam reviews is a crying need in\ntodays world in order to purchase products without being cheated from online\nsites. In many online sites, there are options for posting reviews, and thus\ncreating scopes for fake paid reviews or untruthful reviews. These concocted\nreviews can mislead the general public and put them in a perplexity whether to\nbelieve the review or not. Prominent machine learning techniques have been\nintroduced to solve the problem of spam review detection. The majority of\ncurrent research has concentrated on supervised learning methods, which require\nlabeled data - an inadequacy when it comes to online review. Our focus in this\narticle is to detect any deceptive text reviews. In order to achieve that we\nhave worked with both labeled and unlabeled data and proposed deep learning\nmethods for spam review detection which includes Multi-Layer Perceptron (MLP),\nConvolutional Neural Network (CNN) and a variant of Recurrent Neural Network\n(RNN) that is Long Short-Term Memory (LSTM). We have also applied some\ntraditional machine learning classifiers such as Nave Bayes (NB), K Nearest\nNeighbor (KNN) and Support Vector Machine (SVM) to detect spam reviews and\nfinally, we have shown the performance comparison for both traditional and deep\nlearning classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahariar_G/0/1/0/all/0/1\">G. M. Shahariar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1\">Swapnil Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omar_F/0/1/0/all/0/1\">Faiza Omar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_F/0/1/0/all/0/1\">Faisal Muhammad Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_S/0/1/0/all/0/1\">Samiha Binte Hassan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-efficient End-to-end Information Extraction for Statistical Legal Analysis. (arXiv:2211.01692v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01692","description":"<p>Legal practitioners often face a vast amount of documents. Lawyers, for\ninstance, search for appropriate precedents favorable to their clients, while\nthe number of legal precedents is ever-growing. Although legal search engines\ncan assist finding individual target documents and narrowing down the number of\ncandidates, retrieved information is often presented as unstructured text and\nusers have to examine each document thoroughly which could lead to information\noverloading. This also makes their statistical analysis challenging. Here, we\npresent an end-to-end information extraction (IE) system for legal documents.\nBy formulating IE as a generation task, our system can be easily applied to\nvarious tasks without domain-specific engineering effort. The experimental\nresults of four IE tasks on Korean precedents shows that our IE system can\nachieve competent scores (-2.3 on average) compared to the rule-based baseline\nwith as few as 50 training examples per task and higher score (+5.4 on average)\nwith 200 examples. Finally, our statistical analysis on two case\ncategories--drunk driving and fraud--with 35k precedents reveals the resulting\nstructured information from our IE system faithfully reflects the macroscopic\nfeatures of Korean legal system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonseok Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eom_S/0/1/0/all/0/1\">Saehee Eom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hanuhl Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hai Jin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A speech corpus for chronic kidney disease. (arXiv:2211.01705v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01705","description":"<p>In this study, we present a speech corpus of patients with chronic kidney\ndisease (CKD) that will be used for research on pathological voice analysis,\nautomatic illness identification, and severity prediction. This paper\nintroduces the steps involved in creating this corpus, including the choice of\nspeech-related parameters and speech lists as well as the recording technique.\nThe speakers in this corpus, 289 CKD patients with varying degrees of severity\nwho were categorized based on estimated glomerular filtration rate (eGFR),\ndelivered sustained vowels, sentence, and paragraph stimuli. This study\ncompared and analyzed the voice characteristics of CKD patients with those of\nthe control group; the results revealed differences in voice quality,\nphoneme-level pronunciation, prosody, glottal source, and aerodynamic\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mun_J/0/1/0/all/0/1\">Jihyun Mun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunhee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Myeong Ju Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_J/0/1/0/all/0/1\">Jiwon Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sejoong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_M/0/1/0/all/0/1\">Minhwa Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid-SD ($\\text{H}_{\\text{SD}}$) : A new hybrid evaluation metric for automatic speech recognition tasks. (arXiv:2211.01722v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01722","description":"<p>Many studies have examined the shortcomings of word error rate (WER) as an\nevaluation metric for automatic speech recognition (ASR) systems, particularly\nwhen used for spoken language understanding tasks such as intent recognition\nand dialogue systems. In this paper, we propose Hybrid-SD\n($\\text{H}_{\\text{SD}}$), a new hybrid evaluation metric for ASR systems that\ntakes into account both semantic correctness and error rate. To generate\nsentence dissimilarity scores (SD), we built a fast and lightweight SNanoBERT\nmodel using distillation techniques. Our experiments show that the SNanoBERT\nmodel is 25.9x smaller and 38.8x faster than SRoBERTa while achieving\ncomparable results on well-known benchmarks. Hence, making it suitable for\ndeploying with ASR models on edge devices. We also show that\n$\\text{H}_{\\text{SD}}$ correlates more strongly with downstream tasks such as\nintent recognition and named-entity recognition (NER).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sasindran_Z/0/1/0/all/0/1\">Zitha Sasindran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yelchuri_H/0/1/0/all/0/1\">Harsha Yelchuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1\">Supreeth Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakar_T/0/1/0/all/0/1\">T. V. Prabhakar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the State-of-the-Art Language Modeling Methods and Data Augmentation Techniques for Multilingual Clause-Level Morphology. (arXiv:2211.01736v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01736","description":"<p>This paper describes the KUIS-AI NLP team's submission for the 1$^{st}$\nShared Task on Multilingual Clause-level Morphology (MRL2022). We present our\nwork on all three parts of the shared task: inflection, reinflection, and\nanalysis. We mainly explore two approaches: Transformer models in combination\nwith data augmentation, and exploiting the state-of-the-art language modeling\ntechniques for morphological analysis. Data augmentation leads a remarkable\nperformance improvement for most of the languages in the inflection task.\nPrefix-tuning on pretrained mGPT model helps us to adapt reinflection and\nanalysis tasks in a low-data setting. Additionally, we used pipeline\narchitectures using publicly available open source lemmatization tools and\nmonolingual BERT-based morphological feature classifiers for reinflection and\nanalysis tasks, respectively. While Transformer architectures with data\naugmentation and pipeline architectures achieved the best results for\ninflection and reinflection tasks, pipelines and prefix-tuning on mGPT received\nthe highest results for the analysis task. Our methods achieved first place in\neach of the three tasks and outperforms mT5-baseline with ~89\\% for inflection,\n~80\\% for reinflection and ~12\\% for analysis. Our code\nhttps://github.com/emrecanacikgoz/mrl2022 is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Acikgoz_E/0/1/0/all/0/1\">Emre Can Acikgoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chubakov_T/0/1/0/all/0/1\">Tilek Chubakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kural_M/0/1/0/all/0/1\">M&#xfc;ge Kural</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahin_G/0/1/0/all/0/1\">G&#xf6;zde G&#xfc;l &#x15e;ahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuret_D/0/1/0/all/0/1\">Deniz Yuret</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptEHR: Conditional Electronic Healthcare Records Generation with Prompt Learning. (arXiv:2211.01761v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01761","description":"<p>Accessing longitudinal multimodal Electronic Healthcare Records (EHRs) is\nchallenging due to privacy concerns, which hinders the use of ML for healthcare\napplications. Synthetic EHRs generation bypasses the need to share sensitive\nreal patient records. However, existing methods generate single-modal EHRs by\nunconditional generation or by longitudinal inference, which falls short of low\nflexibility and makes unrealistic EHRs. In this work, we propose to formulate\nEHRs generation as a text-to-text translation task by language models (LMs),\nwhich suffices to highly flexible event imputation during generation. We also\ndesign prompt learning to control the generation conditioned by numerical and\ncategorical demographic features. We evaluate synthetic EHRs quality by two\nperplexity measures accounting for their longitudinal pattern (longitudinal\nimputation perplexity, lpl) and the connections cross modalities\n(cross-modality imputation perplexity, mpl). Moreover, we utilize two\nadversaries: membership and attribute inference attacks for privacy-preserving\nevaluation. Experiments on MIMIC-III data demonstrate the superiority of our\nmethods on realistic EHRs generation (53.1\\% decrease of lpl and 45.3\\%\ndecrease of mpl on average compared to the best baselines) with low privacy\nrisks. Software is available at https://github.com/RyanWangZf/PromptEHR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jimeng Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Event Extraction via Tracking Visual States of Arguments. (arXiv:2211.01781v1 [cs.CV])","link":"http://arxiv.org/abs/2211.01781","description":"<p>Video event extraction aims to detect salient events from a video and\nidentify the arguments for each event as well as their semantic roles. Existing\nmethods focus on capturing the overall visual scene of each frame, ignoring\nfine-grained argument-level information. Inspired by the definition of events\nas changes of states, we propose a novel framework to detect video events by\ntracking the changes in the visual states of all involved arguments, which are\nexpected to provide the most informative evidence for the extraction of video\nevents. In order to capture the visual state changes of arguments, we decompose\nthem into changes in pixels within objects, displacements of objects, and\ninteractions among multiple arguments. We further propose Object State\nEmbedding, Object Motion-aware Embedding and Argument Interaction Embedding to\nencode and track these changes respectively. Experiments on various video event\nextraction tasks demonstrate significant improvements compared to\nstate-of-the-art models. In particular, on verb classification, we achieve\n3.49% absolute gains (19.53% relative gains) in F1@5 on Video Situation\nRecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crosslingual Generalization through Multitask Finetuning. (arXiv:2211.01786v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01786","description":"<p>Multitask prompted finetuning (MTF) has been shown to help large language\nmodels generalize to new tasks in a zero-shot setting, but so far explorations\nof MTF have focused on English data and models. We apply MTF to the pretrained\nmultilingual BLOOM and mT5 model families to produce finetuned variants called\nBLOOMZ and mT0. We find finetuning large multilingual language models on\nEnglish tasks with English prompts allows for task generalization to\nnon-English languages that appear only in the pretraining corpus. Finetuning on\nmultilingual tasks with English prompts further improves performance on English\nand non-English tasks leading to various state-of-the-art zero-shot results. We\nalso investigate finetuning on multilingual tasks with prompts that have been\nmachine-translated from English to match the language of each dataset. We find\ntraining on these machine-translated prompts leads to better performance on\nhuman-written prompts in the respective languages. Surprisingly, we find models\nare capable of zero-shot generalization to tasks in languages they have never\nintentionally seen. We conjecture that the models are learning higher-level\ncapabilities that are both task- and language-agnostic. In addition, we\nintroduce xP3, a composite of supervised datasets in 46 languages with English\nand machine-translated prompts. Our code, datasets and models are publicly\navailable at https://github.com/bigscience-workshop/xmtf.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1\">Niklas Muennighoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Thomas Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutawika_L/0/1/0/all/0/1\">Lintang Sutawika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scao_T/0/1/0/all/0/1\">Teven Le Scao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1\">M Saiful Bari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yong_Z/0/1/0/all/0/1\">Zheng-Xin Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoelkopf_H/0/1/0/all/0/1\">Hailey Schoelkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almubarak_K/0/1/0/all/0/1\">Khalid Almubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1\">Samuel Albanie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alyafeai_Z/0/1/0/all/0/1\">Zaid Alyafeai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1\">Edward Raff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query-based Instance Discrimination Network for Relational Triple Extraction. (arXiv:2211.01797v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01797","description":"<p>Joint entity and relation extraction has been a core task in the field of\ninformation extraction. Recent approaches usually consider the extraction of\nrelational triples from a stereoscopic perspective, either learning a\nrelation-specific tagger or separate classifiers for each relation type.\nHowever, they still suffer from error propagation, relation redundancy and lack\nof high-level connections between triples. To address these issues, we propose\na novel query-based approach to construct instance-level representations for\nrelational triples. By metric-based comparison between query embeddings and\ntoken embeddings, we can extract all types of triples in one step, thus\neliminating the error propagation problem. In addition, we learn the\ninstance-level representation of relational triples via contrastive learning.\nIn this way, relational triples can not only enclose rich class-level semantics\nbut also access to high-order global connections. Experimental results show\nthat our proposed method achieves the state of the art on five widely used\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zeqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yongliang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiaoxia Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weiming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human in the loop approaches in multi-modal conversational task guidance system development. (arXiv:2211.01824v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01824","description":"<p>Development of task guidance systems for aiding humans in a situated task\nremains a challenging problem. The role of search (information retrieval) and\nconversational systems for task guidance has immense potential to help the task\nperformers achieve various goals. However, there are several technical\nchallenges that need to be addressed to deliver such conversational systems,\nwhere common supervised approaches fail to deliver the expected results in\nterms of overall performance, user experience and adaptation to realistic\nconditions. In this preliminary work we first highlight some of the challenges\ninvolved during the development of such systems. We then provide an overview of\nexisting datasets available and highlight their limitations. We finally develop\na model-in-the-loop wizard-of-oz based data collection tool and perform a pilot\nexperiment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manuvinakurike_R/0/1/0/all/0/1\">Ramesh Manuvinakurike</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1\">Sovan Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffa_G/0/1/0/all/0/1\">Giuseppe Raffa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beckwith_R/0/1/0/all/0/1\">Richard Beckwith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhodes_A/0/1/0/all/0/1\">Anthony Rhodes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Meng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mejia_G/0/1/0/all/0/1\">Gesem Gudino Mejia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahay_S/0/1/0/all/0/1\">Saurav Sahay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachman_L/0/1/0/all/0/1\">Lama Nachman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Prompt Tuning for Text Summarization. (arXiv:2211.01837v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01837","description":"<p>Prompts with different control signals (e.g., length, keywords, etc.) can be\nused to control text summarization. When control signals are available, they\ncan control the properties of generated summaries and potentially improve\nsummarization quality (since more information are given). Unfortunately,\ncontrol signals are not already available during inference time. In this paper,\nwe propose Lotus (shorthand for Latent Prompt Tuning for Summarization), which\nis a single model that can be applied in both controlled and uncontrolled\n(without control signals) modes. During training, Lotus learns latent prompt\nrepresentations from prompts with gold control signals using a contrastive\nlearning objective. Experiments show Lotus in uncontrolled mode consistently\nimproves upon strong (uncontrollable) summarization models across four\ndifferent summarization datasets. We also demonstrate generated summaries can\nbe controlled using prompts with user specified control tokens.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yubo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si-qing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Circling Back to Recurrent Models of Language. (arXiv:2211.01848v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01848","description":"<p>Just because some purely recurrent models suffer from being hard to optimize\nand inefficient on today's hardware, they are not necessarily bad models of\nlanguage. We demonstrate this by the extent to which these models can still be\nimproved by a combination of a slightly better recurrent cell, architecture,\nobjective, as well as optimization. In the process, we establish a new state of\nthe art for language modelling on small datasets and on enwik8 with dynamic\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Melis_G/0/1/0/all/0/1\">G&#xe1;bor Melis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual information integration for stance detection via cross-attention. (arXiv:2211.01874v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01874","description":"<p>Stance detection deals with the identification of an author's stance towards\na target and is applied on various text domains like social media and news. In\nmany cases, inferring the stance is challenging due to insufficient access to\ncontextual information. Complementary context can be found in knowledge bases\nbut integrating the context into pretrained language models is non-trivial due\nto their graph structure. In contrast, we explore an approach to integrate\ncontextual information as text which aligns better with transformer\narchitectures. Specifically, we train a model consisting of dual encoders which\nexchange information via cross-attention. This architecture allows for\nintegrating contextual information from heterogeneous sources. We evaluate\ncontext extracted from structured knowledge sources and from prompting large\nlanguage models. Our approach is able to outperform competitive baselines\n(1.9pp on average) on a large and diverse stance detection benchmark, both (1)\nin-domain, i.e. for seen targets, and (2) out-of-domain, i.e. for targets\nunseen during training. Our analysis shows that it is able to regularize for\nspurious label correlations with target-specific cue words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beck_T/0/1/0/all/0/1\">Tilman Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waldis_A/0/1/0/all/0/1\">Andreas Waldis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When to Laugh and How Hard? A Multimodal Approach to Detecting Humor and its Intensity. (arXiv:2211.01889v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01889","description":"<p>Prerecorded laughter accompanying dialog in comedy TV shows encourages the\naudience to laugh by clearly marking humorous moments in the show. We present\nan approach for automatically detecting humor in the Friends TV show using\nmultimodal data. Our model is capable of recognizing whether an utterance is\nhumorous or not and assess the intensity of it. We use the prerecorded laughter\nin the show as annotation as it marks humor and the length of the audience's\nlaughter tells us how funny a given joke is. We evaluate the model on episodes\nthe model has not been exposed to during the training phase. Our results show\nthat the model is capable of correctly detecting whether an utterance is\nhumorous 78% of the time and how long the audience's laughter reaction should\nlast with a mean absolute error of 600 milliseconds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alnajjar_K/0/1/0/all/0/1\">Khalid Alnajjar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamalainen_M/0/1/0/all/0/1\">Mika H&#xe4;m&#xe4;l&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiedemann_J/0/1/0/all/0/1\">J&#xf6;rg Tiedemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laaksonen_J/0/1/0/all/0/1\">Jorma Laaksonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurimo_M/0/1/0/all/0/1\">Mikko Kurimo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Are Human-Level Prompt Engineers. (arXiv:2211.01910v1 [cs.LG])","link":"http://arxiv.org/abs/2211.01910","description":"<p>By conditioning on natural language instructions, large language models\n(LLMs) have displayed impressive capabilities as general-purpose computers.\nHowever, task performance depends significantly on the quality of the prompt\nused to steer the model, and most effective prompts have been handcrafted by\nhumans. Inspired by classical program synthesis and the human approach to\nprompt engineering, we propose Automatic Prompt Engineer (APE) for automatic\ninstruction generation and selection. In our method, we treat the instruction\nas the \"program,\" optimized by searching over a pool of instruction candidates\nproposed by an LLM in order to maximize a chosen score function. To evaluate\nthe quality of the selected instruction, we evaluate the zero-shot performance\nof another LLM following the selected instruction. Experiments on 24 NLP tasks\nshow that our automatically generated instructions outperform the prior LLM\nbaseline by a large margin and achieve better or comparable performance to the\ninstructions generated by human annotators on 19/24 tasks. We conduct extensive\nqualitative and quantitative analyses to explore the performance of APE. We\nshow that APE-engineered prompts can be applied to steer models toward\ntruthfulness and/or informativeness, as well as to improve few-shot learning\nperformance by simply prepending them to standard in-context learning prompts.\nPlease check out our webpage at\nhttps://sites.google.com/view/automatic-prompt-engineer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yongchao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresanu_A/0/1/0/all/0/1\">Andrei Ioan Muresanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Ziwen Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paster_K/0/1/0/all/0/1\">Keiran Paster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pitis_S/0/1/0/all/0/1\">Silviu Pitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Harris Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ba_J/0/1/0/all/0/1\">Jimmy Ba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficiently Trained Mongolian Text-to-Speech System Based On FullConv. (arXiv:2211.01948v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01948","description":"<p>Recurrent Neural Networks (RNNs) have become the standard modeling technique\nfor sequence data, and are used in a number of novel text-to-speech models.\nHowever, training a TTS model including RNN components has certain requirements\nfor GPU performance and takes a long time. In contrast, studies have shown that\nCNN-based sequence synthesis technology can greatly reduce training time in\ntext-to-speech models while ensuring a certain performance due to its high\nparallelism. We propose a new text-to-speech system based on deep convolutional\nneural networks that does not employ any RNN components (recurrent units). At\nthe same time, we improve the generality and robustness of our model through a\nseries of data augmentation methods such as Time Warping, Frequency Mask, and\nTime Mask. The final experimental results show that the TTS model using only\nthe CNN component can reduce the training time compared to the classic TTS\nmodels such as Tacotron while ensuring the quality of the synthesized speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">ZiQi Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A BERT-based Deep Learning Approach for Reputation Analysis in Social Media. (arXiv:2211.01954v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01954","description":"<p>Social media has become an essential part of the modern lifestyle, with its\nusage being highly prevalent. This has resulted in unprecedented amounts of\ndata generated from users in social media, such as users' attitudes, opinions,\ninterests, purchases, and activities across various aspects of their lives.\nTherefore, in a world of social media, where its power has shifted to users,\nactions taken by companies and public figures are subject to constantly being\nunder scrutiny by influential global audiences. As a result, reputation\nmanagement in social media has become essential as companies and public figures\nneed to maintain their reputation to preserve their reputation capital.\nHowever, domain experts still face the challenge of lacking appropriate\nsolutions to automate reliable online reputation analysis. To tackle this\nchallenge, we proposed a novel reputation analysis approach based on the\npopular language model BERT (Bidirectional Encoder Representations from\nTransformers). The proposed approach was evaluated on the reputational polarity\ntask using RepLab 2013 dataset. Compared to previous works, we achieved 5.8%\nimprovement in accuracy, 26.9% improvement in balanced accuracy, and 21.8%\nimprovement in terms of F-score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mohammad Wali Ur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1\">Sicong Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satam_P/0/1/0/all/0/1\">Pratik Satam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hariri_S/0/1/0/all/0/1\">Salim Hariri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padilla_C/0/1/0/all/0/1\">Chris Padilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_Z/0/1/0/all/0/1\">Zoe Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nevarez_C/0/1/0/all/0/1\">Carlos Nevarez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Contrastive and Non-Contrastive Losses for Fine-Tuning Pretrained Models in Speech Analysis. (arXiv:2211.01964v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01964","description":"<p>Embedding paralinguistic properties is a challenging task as there are only a\nfew hours of training data available for domains such as emotional speech. One\nsolution to this problem is to pretrain a general self-supervised speech\nrepresentation model on large amounts of unlabeled speech. This pretrained\nmodel is then finetuned to a specific task. Paralinguistic properties however\nhave notoriously high class variance, making the finetuning ineffective. In\nthis work, we propose a two step approach to this. First we improve the\nembedding space, then we train an adapter to bridge the gap from the embedding\nspace to a classification task. In order to improve the class invariance we use\na combination of contrastive and non-contrastive losses to explicitly optimize\nfor class invariant, yet discriminative features. Our approach consistently\noutperforms baselines that are finetuned end-to-end on multiple tasks and\nsurpasses a benchmark on state-of-the-art emotion classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lux_F/0/1/0/all/0/1\">Florian Lux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Ching-Yi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tiny-Attention Adapter: Contexts Are More Important Than the Number of Parameters. (arXiv:2211.01979v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01979","description":"<p>Adapter-tuning is a paradigm that transfers a pretrained language model to\ndownstream tasks by adding and tuning a small number of new parameters.\nPreviously proposed adapter architectures are all feed-forward neural networks.\nIn this paper, we investigate the effectiveness of using tiny-attention --\ni.e., attention with extremely small per-head dimensionality -- as adapters.\nOur tiny-attention adapter learns to modify the hidden states at each position\ndirectly conditioned on the hidden states at all the other positions, which is\nmissed by the previously proposed adapters. Moreover, we view its multiple\nattention heads as a mixture of experts and propose to average their weights\nduring deployment, which further reduces its inference computation cost. On the\nGLUE benchmark, our tiny-attention adapter outperforms the other\nparameter-efficient transfer learning methods as well as full fine-tuning while\nonly updating 0.05% of the parameters. On the FewGLUE benchmark, its\nperformance is comparable to that of GPT-3 and PET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hongyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1\">Hongyuan Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Taxonomy Expansion via Hierarchy-Aware Topic Phrase Generation. (arXiv:2211.01981v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01981","description":"<p>Topic taxonomies display hierarchical topic structures of a text corpus and\nprovide topical knowledge to enhance various NLP applications. To dynamically\nincorporate new topic information, several recent studies have tried to expand\n(or complete) a topic taxonomy by inserting emerging topics identified in a set\nof new documents. However, existing methods focus only on frequent terms in\ndocuments and the local topic-subtopic relations in a taxonomy, which leads to\nlimited topic term coverage and fails to model the global topic hierarchy. In\nthis work, we propose a novel framework for topic taxonomy expansion, named\nTopicExpan, which directly generates topic-related terms belonging to new\ntopics. Specifically, TopicExpan leverages the hierarchical relation structure\nsurrounding a new topic and the textual content of an input document for topic\nterm generation. This approach encourages newly-inserted topics to further\ncover important but less frequent terms as well as to keep their relation\nconsistency within the taxonomy. Experimental results on two real-world text\ncorpora show that TopicExpan significantly outperforms other baseline methods\nin terms of the quality of output taxonomies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongha Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiaming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seonghyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Susik Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hwanjo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Statistical Representations For End-To-End ASR. (arXiv:2211.01993v1 [cs.CL])","link":"http://arxiv.org/abs/2211.01993","description":"<p>End-to-End automatic speech recognition (ASR) models aim to learn a\ngeneralised speech representation to perform recognition. In this domain there\nis little research to analyse internal representation dependencies and their\nrelationship to modelling approaches. This paper investigates cross-domain\nlanguage model dependencies within transformer architectures using SVCCA and\nuses these insights to exploit modelling approaches. It was found that specific\nneural representations within the transformer layers exhibit correlated\nbehaviour which impacts recognition performance.\n</p>\n<p>Altogether, this work provides analysis of the modelling approaches affecting\ncontextual dependencies and ASR performance, and can be used to create or adapt\nbetter performing End-to-End ASR models and also for downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ollerenshaw_A/0/1/0/all/0/1\">Anna Ollerenshaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalal_M/0/1/0/all/0/1\">Md Asif Jalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hain_T/0/1/0/all/0/1\">Thomas Hain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"lilGym: Natural Language Visual Reasoning with Reinforcement Learning. (arXiv:2211.01994v1 [cs.LG])","link":"http://arxiv.org/abs/2211.01994","description":"<p>We present lilGym, a new benchmark for language-conditioned reinforcement\nlearning in visual environments. lilGym is based on 2,661 highly-compositional\nhuman-written natural language statements grounded in an interactive visual\nenvironment. We annotate all statements with executable Python programs\nrepresenting their meaning to enable exact reward computation in every possible\nworld state. Each statement is paired with multiple start states and reward\nfunctions to form thousands of distinct Markov Decision Processes of varying\ndifficulty. We experiment with lilGym with different models and learning\nregimes. Our results and analysis show that while existing methods are able to\nachieve non-trivial performance, lilGym forms a challenging open problem.\nlilGym is available at https://lil.nlp.cornell.edu/lilgym/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Anne Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brantley_K/0/1/0/all/0/1\">Kiant&#xe9; Brantley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kojima_N/0/1/0/all/0/1\">Noriyuki Kojima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Kernels and Channel Attention with Multi-Layer Embedding Aggregation for Speaker Verification. (arXiv:2211.02000v1 [cs.SD])","link":"http://arxiv.org/abs/2211.02000","description":"<p>State-of-the-art speaker verification frameworks have typically focused on\nspeech enhancement techniques with increasingly deeper (more layers) and wider\n(number of channels) models to improve their verification performance. Instead,\nthis paper proposes an approach to increase the model resolution capability\nusing attention-based dynamic kernels in a convolutional neural network to\nadapt the model parameters to be feature-conditioned. The attention weights on\nthe kernels are further distilled by channel attention and multi-layer feature\naggregation to learn global features from speech. This approach provides an\nefficient solution to improving representation capacity with lower data\nresources. This is due to the self-adaptation to inputs of the structures of\nthe model parameters. The proposed dynamic convolutional model achieved 1.62\\%\nEER and 0.18 miniDCF on the VoxCeleb1 test set and has a 17\\% relative\nimprovement compared to the ECAPA-TDNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ollerenshaw_A/0/1/0/all/0/1\">Anna Ollerenshaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalal_M/0/1/0/all/0/1\">Md Asif Jalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hain_T/0/1/0/all/0/1\">Thomas Hain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inverse scaling can become U-shaped. (arXiv:2211.02011v1 [cs.CL])","link":"http://arxiv.org/abs/2211.02011","description":"<p>Although scaling language models improves performance on a range of tasks,\nthere are apparently some scenarios where scaling hurts performance. For\ninstance, the Inverse Scaling Prize Round 1 identified four ''inverse scaling''\ntasks, for which performance gets worse for larger models. These tasks were\nevaluated on models of up to 280B parameters, trained up to 500 zettaFLOPs of\ncompute.\n</p>\n<p>This paper takes a closer look at these four tasks. We evaluate models of up\nto 540B parameters, trained on five times more compute than those evaluated in\nthe Inverse Scaling Prize. With this increased range of model sizes and\ntraining compute, three out of the four tasks exhibit what we call ''U-shaped\nscaling'' -- performance decreases up to a certain model size, and then\nincreases again up to the largest model evaluated. One hypothesis is that\nU-shaped scaling occurs when a task comprises a ''true task'' and a\n''distractor task''. Medium-size models can do the distractor task, which hurts\nperformance, while only large-enough models can ignore the distractor task and\ndo the true task. The existence of U-shaped scaling implies that inverse\nscaling may not hold for larger models.\n</p>\n<p>Second, we evaluate the inverse scaling tasks using chain-of-thought (CoT)\nprompting, in addition to basic prompting without CoT. With CoT prompting, all\nfour tasks show either U-shaped scaling or positive scaling, achieving perfect\nsolve rates on two tasks and several sub-tasks. This suggests that the term\n\"inverse scaling task\" is under-specified -- a given task may be inverse\nscaling for one prompt but positive or U-shaped scaling for a different prompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space. (arXiv:1909.08191v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/1909.08191","description":"<p>The trends of open science have enabled several open scholarly datasets which\ninclude millions of papers and authors. Managing, exploring, and utilizing such\nlarge and complicated datasets effectively are challenging. In recent years,\nthe knowledge graph has emerged as a universal data format for representing\nknowledge about heterogeneous entities and their relationships. The knowledge\ngraph can be modeled by knowledge graph embedding methods, which represent\nentities and relations as embedding vectors in semantic space, then model the\ninteractions between these embedding vectors. However, the semantic structures\nin the knowledge graph embedding space are not well-studied, thus knowledge\ngraph embedding methods are usually only used for knowledge graph completion\nbut not data representation and analysis. In this paper, we propose to analyze\nthese semantic structures based on the well-studied word embedding space and\nuse them to support data exploration. We also define the semantic queries,\nwhich are algebraic operations between the embedding vectors in the knowledge\ngraph embedding space, to solve queries such as similarity and analogy between\nthe entities on the original datasets. We then design a general framework for\ndata exploration by semantic queries and discuss the solution to some\ntraditional scholarly data exploration tasks. We also propose some new\ninteresting tasks that can be solved based on the uncanny semantic structures\nof the embedding space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hung Nghiep Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takasu_A/0/1/0/all/0/1\">Atsuhiro Takasu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Lexical Complexity in English Texts: The Complex 2.0 Dataset. (arXiv:2102.08773v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.08773","description":"<p>Identifying words which may cause difficulty for a reader is an essential\nstep in most lexical text simplification systems prior to lexical substitution\nand can also be used for assessing the readability of a text. This task is\ncommonly referred to as Complex Word Identification (CWI) and is often modelled\nas a supervised classification problem. For training such systems, annotated\ndatasets in which words and sometimes multi-word expressions are labelled\nregarding complexity are required. In this paper we analyze previous work\ncarried out in this task and investigate the properties of CWI datasets for\nEnglish. We develop a protocol for the annotation of lexical complexity and use\nthis to annotate a new dataset, CompLex 2.0. We present experiments using both\nnew and old datasets to investigate the nature of lexical complexity. We found\nthat a Likert-scale annotation protocol provides an objective setting that is\nsuperior for identifying the complexity of words compared to a binary\nannotation protocol. We release a new dataset using our new protocol to promote\nthe task of Lexical Complexity Prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shardlow_M/0/1/0/all/0/1\">Matthew Shardlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_R/0/1/0/all/0/1\">Richard Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Machine-Paraphrased Plagiarism. (arXiv:2103.11909v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.11909","description":"<p>Employing paraphrasing tools to conceal plagiarized text is a severe threat\nto academic integrity. To enable the detection of machine-paraphrased text, we\nevaluate the effectiveness of five pre-trained word embedding models combined\nwith machine learning classifiers and state-of-the-art neural language models.\nWe analyze preprints of research papers, graduation theses, and Wikipedia\narticles, which we paraphrased using different configurations of the tools\nSpinBot and SpinnerChief. The best performing technique, Longformer, achieved\nan average F1 score of 80.99% (F1=99.68% for SpinBot and F1=71.64% for\nSpinnerChief cases), while human evaluators achieved F1=78.4% for SpinBot and\nF1=65.6% for SpinnerChief cases. We show that the automated classification\nalleviates shortcomings of widely-used text-matching systems, such as Turnitin\nand PlagScan. To facilitate future research, all data, code, and two web\napplications showcasing our contributions are openly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1\">Jan Philip Wahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foltynek_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Folt&#xfd;nek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1\">Norman Meuschke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Neural Language Models Good Plagiarists? A Benchmark for Neural Paraphrase Detection. (arXiv:2103.12450v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.12450","description":"<p>The rise of language models such as BERT allows for high-quality text\nparaphrasing. This is a problem to academic integrity, as it is difficult to\ndifferentiate between original and machine-generated content. We propose a\nbenchmark consisting of paraphrased articles using recent language models\nrelying on the Transformer architecture. Our contribution fosters future\nresearch of paraphrase detection systems as it offers a large collection of\naligned original and paraphrased documents, a study regarding its structure,\nclassification experiments with state-of-the-art systems, and we make our\nfindings publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1\">Jan Philip Wahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1\">Norman Meuschke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Composing Conversational Negation. (arXiv:2107.06820v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.06820","description":"<p>Negation in natural language does not follow Boolean logic and is therefore\ninherently difficult to model. In particular, it takes into account the broader\nunderstanding of what is being negated. In previous work, we proposed a\nframework for the negation of words that accounts for 'worldly context'. This\npaper extends that proposal now accounting for the compositional structure\ninherent in language within the DisCoCirc framework. We compose the negations\nof single words to capture the negation of sentences. We also describe how to\nmodel the negation of words whose meanings evolve in the text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_R/0/1/0/all/0/1\">Razin A. Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_L/0/1/0/all/0/1\">Lia Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodatz_B/0/1/0/all/0/1\">Benjamin Rodatz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coecke_B/0/1/0/all/0/1\">Bob Coecke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Neurons Invariant to Sentence Structural Changes in Neural Machine Translation. (arXiv:2110.03067v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03067","description":"<p>We present a methodology that explores how sentence structure is reflected in\nneural representations of machine translation systems. We demonstrate our\nmodel-agnostic approach with the Transformer English-German translation model.\nWe analyze neuron-level correlation of activations between paraphrases while\ndiscussing the methodology challenges and the need for confound analysis to\nisolate the effects of shallow cues. We find that similarity between activation\npatterns can be mostly accounted for by similarity in word choice and sentence\nlength. Following that, we manipulate neuron activations to control the\nsyntactic form of the output. We show this intervention to be somewhat\nsuccessful, indicating that deep models capture sentence-structure\ndistinctions, despite finding no such indication at the neuron level. To\nconduct our experiments, we develop a semi-automatic method to generate\nmeaning-preserving minimal pair paraphrases (active-passive voice and adverbial\nclause-noun phrase) and compile a corpus of such pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_G/0/1/0/all/0/1\">Gal Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advancing the dimensionality reduction of speaker embeddings for speaker diarisation: disentangling noise and informing speech activity. (arXiv:2110.03380v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2110.03380","description":"<p>The objective of this work is to train noise-robust speaker embeddings\nadapted for speaker diarisation. Speaker embeddings play a crucial role in the\nperformance of diarisation systems, but they often capture spurious information\nsuch as noise, adversely affecting performance. Our previous work has proposed\nan auto-encoder-based dimensionality reduction module to help remove the\nredundant information. However, they do not explicitly separate such\ninformation and have also been found to be sensitive to hyper-parameter values.\nTo this end, we propose two contributions to overcome these issues: (i) a novel\ndimensionality reduction framework that can disentangle spurious information\nfrom the speaker embeddings; (ii) the use of speech activity vector to prevent\nthe speaker code from representing the background noise. Through a range of\nexperiments conducted on four datasets, our approach consistently demonstrates\nthe state-of-the-art performance among models without system fusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">You Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_H/0/1/0/all/0/1\">Hee-Soo Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">Jee-weon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Youngki Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bong-Jin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Joon Son Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Testing the Generalization of Neural Language Models for COVID-19 Misinformation Detection. (arXiv:2111.07819v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.07819","description":"<p>A drastic rise in potentially life-threatening misinformation has been a\nby-product of the COVID-19 pandemic. Computational support to identify false\ninformation within the massive body of data on the topic is crucial to prevent\nharm. Researchers proposed many methods for flagging online misinformation\nrelated to COVID-19. However, these methods predominantly target specific\ncontent types (e.g., news) or platforms (e.g., Twitter). The methods'\ncapabilities to generalize were largely unclear so far. We evaluate fifteen\nTransformer-based models on five COVID-19 misinformation datasets that include\nsocial media posts, news articles, and scientific papers to fill this gap. We\nshow tokenizers and models tailored to COVID-19 data do not provide a\nsignificant advantage over general-purpose ones. Our study provides a realistic\nassessment of models for detecting COVID-19 misinformation. We expect that\nevaluating a broad spectrum of datasets and models will benefit future research\nin developing misinformation detection systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1\">Jan Philip Wahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashok_N/0/1/0/all/0/1\">Nischal Ashok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1\">Norman Meuschke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_T/0/1/0/all/0/1\">Tirthankar Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts. (arXiv:2112.02399v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02399","description":"<p>Contrastive Language-Image Pre-training (CLIP) has drawn increasing attention\nrecently for its transferable visual representation learning. However, due to\nthe semantic gap within datasets, CLIP's pre-trained image-text alignment\nbecomes sub-optimal on downstream tasks, which severely harms its transferring\nperformance. To better adapt the cross-modality embedding space, we propose to\nenhance CLIP via Visual-guided Texts, named VT-CLIP. Specifically, we guide\ntextual features of different categories to adaptively explore informative\nregions on the image and aggregate visual features by attention mechanisms. In\nthis way, the texts become visual-guided, namely, more semantically correlated\nwith downstream images, which greatly benefits the category-wise matching\nprocess. In few-shot settings, we evaluate our VT-CLIP on 11 well-known\nclassification datasets to demonstrate its effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Longtian Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Ziyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Ziyao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yafeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guangnan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlowEval: A Consensus-Based Dialogue Evaluation Framework Using Segment Act Flows. (arXiv:2202.06633v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.06633","description":"<p>Despite recent progress in open-domain dialogue evaluation, how to develop\nautomatic metrics remains an open problem. We explore the potential of dialogue\nevaluation featuring dialog act information, which was hardly explicitly\nmodeled in previous methods. However, defined at the utterance level in\ngeneral, dialog act is of coarse granularity, as an utterance can contain\nmultiple segments possessing different functions. Hence, we propose segment\nact, an extension of dialog act from utterance level to segment level, and\ncrowdsource a large-scale dataset for it. To utilize segment act flows,\nsequences of segment acts, for evaluation, we develop the first consensus-based\ndialogue evaluation framework, FlowEval. This framework provides a\nreference-free approach for dialog evaluation by finding pseudo-references.\nExtensive experiments against strong baselines on three benchmark datasets\ndemonstrate the effectiveness and other desirable characteristics of our\nFlowEval, pointing out a potential path for better dialogue evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jianqiao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wanyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Topic Modeling of Psychotherapy Sessions. (arXiv:2204.10189v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10189","description":"<p>In this work, we compare different neural topic modeling methods in learning\nthe topical propensities of different psychiatric conditions from the\npsychotherapy session transcripts parsed from speech recordings. We also\nincorporate temporal modeling to put this additional interpretability to action\nby parsing out topic similarities as a time series in a turn-level resolution.\nWe believe this topic modeling framework can offer interpretable insights for\nthe therapist to optimally decide his or her strategy and improve psychotherapy\neffectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Baihan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouneffouf_D/0/1/0/all/0/1\">Djallel Bouneffouf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cecchi_G/0/1/0/all/0/1\">Guillermo Cecchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tejwani_R/0/1/0/all/0/1\">Ravi Tejwani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D3: A Massive Dataset of Scholarly Metadata for Analyzing the State of Computer Science Research. (arXiv:2204.13384v3 [cs.DL] UPDATED)","link":"http://arxiv.org/abs/2204.13384","description":"<p>DBLP is the largest open-access repository of scientific articles on computer\nscience and provides metadata associated with publications, authors, and\nvenues. We retrieved more than 6 million publications from DBLP and extracted\npertinent metadata (e.g., abstracts, author affiliations, citations) from the\npublication texts to create the DBLP Discovery Dataset (D3). D3 can be used to\nidentify trends in research activity, productivity, focus, bias, accessibility,\nand impact of computer science research. We present an initial analysis focused\non the volume of computer science research (e.g., number of papers, authors,\nresearch activity), trends in topics of interest, and citation patterns. Our\nfindings show that computer science is a growing research field (approx. 15%\nannually), with an active and collaborative researcher community. While papers\nin recent years present more bibliographical entries in comparison to previous\ndecades, the average number of citations has been declining. Investigating\npapers' abstracts reveals that recent topic trends are clearly reflected in D3.\nFinally, we list further applications of D3 and pose supplemental research\nquestions. The D3 dataset, our findings, and source code are publicly available\nfor research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1\">Jan Philip Wahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models. (arXiv:2205.10770v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10770","description":"<p>Despite their wide adoption, the underlying training and memorization\ndynamics of very large language models is not well understood. We empirically\nstudy exact memorization in causal and masked language modeling, across model\nsizes and throughout the training process. We measure the effects of dataset\nsize, learning rate, and model size on memorization, finding that larger\nlanguage models memorize training data faster across all settings.\nSurprisingly, we show that larger models can memorize a larger portion of the\ndata before over-fitting and tend to forget less throughout the training\nprocess. We also analyze the memorization dynamics of different parts of speech\nand find that models memorize nouns and numbers first; we hypothesize and\nprovide empirical evidence that nouns and numbers act as a unique identifier\nfor memorizing individual training examples. Together, these findings present\nanother piece of the broader puzzle of trying to understand what actually\nimproves as models get bigger.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tirumala_K/0/1/0/all/0/1\">Kushal Tirumala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markosyan_A/0/1/0/all/0/1\">Aram H. Markosyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Authenticity Gap in Human Evaluation. (arXiv:2205.11930v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11930","description":"<p>Human ratings are the gold standard in NLG evaluation. The standard protocol\nis to collect ratings of generated text, average across annotators, and rank\nNLG systems by their average scores. However, little consideration has been\ngiven as to whether this approach faithfully captures human preferences.\nAnalyzing this standard protocol through the lens of utility theory in\neconomics, we identify the implicit assumptions it makes about annotators.\nThese assumptions are often violated in practice, in which case annotator\nratings cease to reflect their preferences. The most egregious violations come\nfrom using Likert scales, which provably reverse the direction of the true\npreference in certain cases. We suggest improvements to the standard protocol\nto make it more theoretically sound, but even in its improved form, it cannot\nbe used to evaluate open-ended tasks like story generation. For the latter, we\npropose a new human evaluation protocol called $\\textit{system-level\nprobabilistic assessment}$ (SPA). When human evaluation of stories is done with\nSPA, we can recover the ordering of GPT-3 models by size, with statistically\nsignificant results. However, when human evaluation is done with the standard\nprotocol, less than half of the expected preferences can be recovered (e.g.,\nthere is no significant difference between $\\texttt{curie}$ and\n$\\texttt{davinci}$, despite using a highly powered test).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ethayarajh_K/0/1/0/all/0/1\">Kawin Ethayarajh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Governance in the Age of Large-Scale Data-Driven Language Technology. (arXiv:2206.03216v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2206.03216","description":"<p>The recent emergence and adoption of Machine Learning technology, and\nspecifically of Large Language Models, has drawn attention to the need for\nsystematic and transparent management of language data. This work proposes an\napproach to global language data governance that attempts to organize data\nmanagement amongst stakeholders, values, and rights. Our proposal is informed\nby prior work on distributed governance that accounts for human values and\ngrounded by an international research collaboration that brings together\nresearchers and practitioners from 60 countries. The framework we present is a\nmulti-party international governance structure focused on language data, and\nincorporating technical and organizational tools needed to support its work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1\">Yacine Jernite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_A/0/1/0/all/0/1\">Anna Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masoud_M/0/1/0/all/0/1\">Maraim Masoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danchev_V/0/1/0/all/0/1\">Valentin Danchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Samson Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luccioni_A/0/1/0/all/0/1\">Alexandra Sasha Luccioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramani_N/0/1/0/all/0/1\">Nishant Subramani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupont_G/0/1/0/all/0/1\">G&#xe9;rard Dupont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1\">Jesse Dodge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talat_Z/0/1/0/all/0/1\">Zeerak Talat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_I/0/1/0/all/0/1\">Isaac Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikpoor_S/0/1/0/all/0/1\">Somaieh Nikpoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frohberg_J/0/1/0/all/0/1\">J&#xf6;rg Frohberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokaslan_A/0/1/0/all/0/1\">Aaron Gokaslan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_P/0/1/0/all/0/1\">Peter Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bommasani_R/0/1/0/all/0/1\">Rishi Bommasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1\">Margaret Mitchell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning. (arXiv:2207.01780v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2207.01780","description":"<p>Program synthesis or code generation aims to generate a program that\nsatisfies a problem specification. Recent approaches using large-scale\npretrained language models (LMs) have shown promising results, yet they have\nsome critical limitations. In particular, they often follow a standard\nsupervised fine-tuning procedure to train a code generation model only from the\npairs of natural-language problem descriptions and ground-truth programs. Such\nparadigm largely ignores some important but potentially useful signals in the\nproblem specification such as unit tests, which thus often results in poor\nperformance when solving complex unseen coding tasks. To address the\nlimitations, we propose \"CodeRL\", a new framework for program synthesis tasks\nthrough pretrained LMs and deep reinforcement learning (RL). Specifically,\nduring training, we treat the code-generating LM as an actor network, and\nintroduce a critic network that is trained to predict the functional\ncorrectness of generated programs and provide dense feedback signals to the\nactor. During inference, we introduce a new generation procedure with a\ncritical sampling strategy that allows a model to automatically regenerate\nprograms based on feedback from example unit tests and critic scores. For the\nmodel backbones, we extended the encoder-decoder architecture of CodeT5 with\nenhanced learning objectives, larger model sizes, and better pretraining data.\nOur method not only achieves new SOTA results on the challenging APPS\nbenchmark, but also shows strong zero-shot transfer capability with new SOTA\nresults on the simpler MBPP benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gotmare_A/0/1/0/all/0/1\">Akhilesh Deepak Gotmare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What can we know about that which we cannot even imagine?. (arXiv:2208.03886v2 [physics.hist-ph] UPDATED)","link":"http://arxiv.org/abs/2208.03886","description":"<p>In this essay I will consider a sequence of questions, ending with one about\nthe breadth and depth of the epistemic limitations of our our science and\nmathematics. I will then suggest a possible way to circumvent such limitations.\nI begin by considering questions about the biological function of intelligence.\nThis will lead into questions concerning human language, perhaps the most\nimportant cognitive prosthesis we have ever developed. While it is traditional\nto rhapsodize about the perceptual power provided by human language, I will\nemphasize how horribly limited - and therefore limiting - it is. This will lead\nto questions of whether human mathematics, being so deeply grounded in our\nlanguage, is also deeply limited. I will then combine all of this into a\npartial, sort-of, sideways answer to the guiding question of this essay: what\nwe can ever discern about all that we cannot even conceive of?\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Wolpert_D/0/1/0/all/0/1\">David H. Wolpert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning ASR pathways: A sparse multilingual ASR model. (arXiv:2209.05735v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2209.05735","description":"<p>Neural network pruning compresses automatic speech recognition (ASR) models\neffectively. However, in multilingual ASR, language-agnostic pruning may lead\nto severe performance drops on some languages because language-agnostic pruning\nmasks may not fit all languages and discard important language-specific\nparameters. In this work, we present ASR pathways, a sparse multilingual ASR\nmodel that activates language-specific sub-networks (\"pathways\"), such that the\nparameters for each language are learned explicitly. With the overlapping\nsub-networks, the shared parameters can also enable knowledge transfer for\nlower-resource languages via joint multilingual training. We propose a novel\nalgorithm to learn ASR pathways, and evaluate the proposed method on 4\nlanguages with a streaming RNN-T model. Our proposed ASR pathways outperform\nboth dense models and a language-agnostically pruned model, and provide better\nperformance on low-resource languages compared to the monolingual sparse\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_M/0/1/0/all/0/1\">Mu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tjandra_A/0/1/0/all/0/1\">Andros Tjandra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chunxi Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Le_D/0/1/0/all/0/1\">Duc Le</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual analysis of intelligibility classification using English, Korean, and Tamil dysarthric speech datasets. (arXiv:2209.13260v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.13260","description":"<p>This paper analyzes dysarthric speech datasets from three languages with\ndifferent prosodic systems: English, Korean, and Tamil. We inspect 39 acoustic\nmeasurements which reflect three speech dimensions including voice quality,\npronunciation, and prosody. As multilingual analysis, examination on the mean\nvalues of acoustic measurements by intelligibility levels is conducted.\nFurther, automatic intelligibility classification is performed to scrutinize\nthe optimal feature set by languages. Analyses suggest pronunciation features,\nsuch as Percentage of Correct Consonants, Percentage of Correct Vowels, and\nPercentage of Correct Phonemes to be language-independent measurements. Voice\nquality and prosody features, however, generally present different aspects by\nlanguages. Experimental results additionally show that different speech\ndimension play a greater role for different languages: prosody for English,\npronunciation for Korean, both prosody and pronunciation for Tamil. This paper\ncontributes to speech pathology in that it differentiates between\nlanguage-independent and language-dependent measurements in intelligibility\nclassification for English, Korean, and Tamil dysarthric speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeo_E/0/1/0/all/0/1\">Eun Jung Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunhee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_M/0/1/0/all/0/1\">Minhwa Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning. (arXiv:2209.14610v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2209.14610","description":"<p>Mathematical reasoning, a core ability of human intelligence, presents unique\nchallenges for machines in abstract thinking and logical reasoning. Recent\nlarge pre-trained language models such as GPT-3 have achieved remarkable\nprogress on mathematical reasoning tasks written in text form, such as math\nword problems (MWP). However, it is unknown if the models can handle more\ncomplex problems that involve math reasoning over heterogeneous information,\nsuch as tabular data. To fill the gap, we present Tabular Math Word Problems\n(TabMWP), a new dataset containing 38,431 open-domain grade-level problems that\nrequire mathematical reasoning on both textual and tabular data. Each question\nin TabMWP is aligned with a tabular context, which is presented as an image,\nsemi-structured text, and a structured table. There are two types of questions:\nfree-text and multi-choice, and each problem is annotated with gold solutions\nto reveal the multi-step reasoning process. We evaluate different pre-trained\nmodels on TabMWP, including the GPT-3 model in a few-shot setting. As earlier\nstudies suggest, since few-shot GPT-3 relies on the selection of in-context\nexamples, its performance is unstable and can degrade to near chance. The\nunstable issue is more severe when handling complex problems like TabMWP. To\nmitigate this, we further propose a novel approach, PromptPG, which utilizes\npolicy gradient to learn to select in-context examples from a small amount of\ntraining data and then constructs the corresponding prompt for the test\nexample. Experimental results show that our method outperforms the best\nbaseline by 5.31% on the accuracy metric and reduces the prediction variance\nsignificantly compared to random selection, which verifies its effectiveness in\nthe selection of in-context examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurohit_T/0/1/0/all/0/1\">Tanmay Rajpurohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1\">Ashwin Kalyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiview Contextual Commonsense Inference: A New Dataset and Task. (arXiv:2210.02890v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.02890","description":"<p>Contextual commonsense inference is the task of generating various types of\nexplanations around the events in a dyadic dialogue, including cause,\nmotivation, emotional reaction, and others. Producing a coherent and\nnon-trivial explanation requires awareness of the dialogue's structure and of\nhow an event is grounded in the context. In this work, we create CICEROv2, a\ndataset consisting of 8,351 instances from 2,379 dialogues, containing multiple\nhuman-written answers for each contextual commonsense inference question,\nrepresenting a type of explanation on cause, subsequent event, motivation, and\nemotional reaction. We show that the inferences in CICEROv2 are more\nsemantically diverse than other contextual commonsense inference datasets. To\nsolve the inference task, we propose a collection of pre-training objectives,\nincluding concept denoising and utterance sorting to prepare a pre-trained\nmodel for the downstream contextual commonsense inference task. Our results\nshow that the proposed pre-training objectives are effective at adapting the\npre-trained T5-Large model for the contextual commonsense inference task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Siqi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Henry Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DABERT: Dual Attention Enhanced BERT for Semantic Matching. (arXiv:2210.03454v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03454","description":"<p>Transformer-based pre-trained language models such as BERT have achieved\nremarkable results in Semantic Sentence Matching. However, existing models\nstill suffer from insufficient ability to capture subtle differences. Minor\nnoise like word addition, deletion, and modification of sentences may cause\nflipped predictions. To alleviate this problem, we propose a novel Dual\nAttention Enhanced BERT (DABERT) to enhance the ability of BERT to capture\nfine-grained differences in sentence pairs. DABERT comprises (1) Dual Attention\nmodule, which measures soft word matches by introducing a new dual channel\nalignment mechanism to model affinity and difference attention. (2) Adaptive\nFusion module, this module uses attention to learn the aggregation of\ndifference and affinity features, and generates a vector describing the\nmatching details of sentence pairs. We conduct extensive experiments on\nwell-studied semantic matching and robustness test datasets, and the\nexperimental results show the effectiveness of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Di Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jian Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Large Language Models are Transforming Machine-Paraphrased Plagiarism. (arXiv:2210.03568v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03568","description":"<p>The recent success of large language models for text generation poses a\nsevere threat to academic integrity, as plagiarists can generate realistic\nparaphrases indistinguishable from original work. However, the role of large\nautoregressive transformers in generating machine-paraphrased plagiarism and\ntheir detection is still developing in the literature. This work explores T5\nand GPT-3 for machine-paraphrase generation on scientific articles from arXiv,\nstudent theses, and Wikipedia. We evaluate the detection performance of six\nautomated solutions and one commercial plagiarism detection software and\nperform a human study with 105 participants regarding their detection\nperformance and the quality of generated examples. Our results suggest that\nlarge models can rewrite text humans have difficulty identifying as\nmachine-paraphrased (53% mean acc.). Human experts rate the quality of\nparaphrases generated by GPT-3 as high as original texts (clarity 4.0/5,\nfluency 4.2/5, coherence 3.8/5). The best-performing detection model (GPT-3)\nachieves a 66% F1-score in detecting paraphrases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1\">Jan Philip Wahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirstein_F/0/1/0/all/0/1\">Frederic Kirstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion. (arXiv:2210.08471v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08471","description":"<p>Transformer-based pre-trained models like BERT have achieved great progress\non Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also\nshown general benefits in multiple NLP tasks. However, how to efficiently\nintegrate dependency prior structure into pre-trained models to better model\ncomplex semantic matching relations is still unsettled. In this paper, we\npropose the \\textbf{D}ependency-Enhanced \\textbf{A}daptive \\textbf{F}usion\n\\textbf{A}ttention (\\textbf{DAFA}), which explicitly introduces dependency\nstructure into pre-trained models and adaptively fuses it with semantic\ninformation. Specifically, \\textbf{\\emph{(i)}} DAFA first proposes a\nstructure-sensitive paradigm to construct a dependency matrix for calibrating\nattention weights. It adopts an adaptive fusion module to integrate the\nobtained dependency information and the original semantic signals. Moreover,\nDAFA reconstructs the attention calculation flow and provides better\ninterpretability. By applying it on BERT, our method achieves state-of-the-art\nor competitive performance on 10 public datasets, demonstrating the benefits of\nadaptively fusing dependency structure in semantic matching task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jian Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Di Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rumei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1\">Minlong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yongxin Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study. (arXiv:2210.10678v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10678","description":"<p>This paper presents an empirical study to build relation extraction systems\nin low-resource settings. Based upon recent pre-trained language models, we\ncomprehensively investigate three schemes to evaluate the performance in\nlow-resource settings: (i) different types of prompt-based methods with\nfew-shot labeled data; (ii) diverse balancing methods to address the\nlong-tailed distribution issue; (iii) data augmentation technologies and\nself-training to generate more labeled in-domain data. We create a benchmark\nwith 8 relation extraction (RE) datasets covering different languages, domains\nand contexts and perform extensive comparisons over the proposed schemes with\ncombinations. Our experiments illustrate: (i) Though prompt-based tuning is\nbeneficial in low-resource RE, there is still much potential for improvement,\nespecially in extracting relations from cross-sentence contexts with multiple\nrelational triples; (ii) Balancing methods are not always helpful for RE with\nlong-tailed distribution; (iii) Data augmentation complements existing\nbaselines and can bring much performance gain, while self-training may not\nconsistently achieve advancement to low-resource RE. Code and datasets are in\nhttps://github.com/zjunlp/LREBench.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizing over Long Tail Concepts for Medical Term Normalization. (arXiv:2210.11947v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.11947","description":"<p>Medical term normalization consists in mapping a piece of text to a large\nnumber of output classes. Given the small size of the annotated datasets and\nthe extremely long tail distribution of the concepts, it is of utmost\nimportance to develop models that are capable to generalize to scarce or unseen\nconcepts. An important attribute of most target ontologies is their\nhierarchical structure. In this paper we introduce a simple and effective\nlearning strategy that leverages such information to enhance the\ngeneralizability of both discriminative and generative models. The evaluation\nshows that the proposed strategy produces state-of-the-art performance on seen\nconcepts and consistent improvements on unseen ones, allowing also for\nefficient zero-shot knowledge transfer across text typologies and datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Portelli_B/0/1/0/all/0/1\">Beatrice Portelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaboro_S/0/1/0/all/0/1\">Simone Scaboro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santus_E/0/1/0/all/0/1\">Enrico Santus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedghamiz_H/0/1/0/all/0/1\">Hooman Sedghamiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chersoni_E/0/1/0/all/0/1\">Emmanuele Chersoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serra_G/0/1/0/all/0/1\">Giuseppe Serra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$N$-gram Is Back: Residual Learning of Neural Text Generation with $n$-gram Language Model. (arXiv:2210.14431v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.14431","description":"<p>$N$-gram language models (LM) have been largely superseded by neural LMs as\nthe latter exhibits better performance. However, we find that $n$-gram models\ncan achieve satisfactory performance on a large proportion of testing cases,\nindicating they have already captured abundant knowledge of the language with\nrelatively low computational cost. With this observation, we propose to learn a\nneural LM that fits the residual between an $n$-gram LM and the real-data\ndistribution. The combination of $n$-gram and neural LMs not only allows the\nneural part to focus on the deeper understanding of language but also provides\na flexible way to customize an LM by switching the underlying $n$-gram model\nwithout changing the neural model. Experimental results on three typical\nlanguage tasks (i.e., language modeling, machine translation, and\nsummarization) demonstrate that our approach attains additional performance\ngains over popular standalone neural models consistently. We also show that our\napproach allows for effective domain adaptation by simply switching to a\ndomain-specific $n$-gram model, without any extra training. Our code is\nreleased at https://github.com/ghrua/NgramRes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huayang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1\">Taro Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance. (arXiv:2210.16031v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.16031","description":"<p>Diffusion generative models have recently greatly improved the power of\ntext-conditioned image generation. Existing image generation models mainly\ninclude text conditional diffusion model and cross-modal guided diffusion\nmodel, which are good at small scene image generation and complex scene image\ngeneration respectively. In this work, we propose a simple yet effective\napproach, namely UPainting, to unify simple and complex scene image generation,\nas shown in Figure 1. Based on architecture improvements and diverse guidance\nschedules, UPainting effectively integrates cross-modal guidance from a\npretrained image-text matching model into a text conditional diffusion model\nthat utilizes a pretrained Transformer language model as the text encoder. Our\nkey findings is that combining the power of large-scale Transformer language\nmodel in understanding language and image-text matching model in capturing\ncross-modal semantics and style, is effective to improve sample fidelity and\nimage-text alignment of image generation. In this way, UPainting has a more\ngeneral image generation capability, which can generate images of both simple\nand complex scenes more effectively. To comprehensively compare text-to-image\nmodels, we further create a more general benchmark, UniBench, with well-written\nChinese and English prompts in both simple and complex scenes. We compare\nUPainting with recent models and find that UPainting greatly outperforms other\nmodels in terms of caption similarity and image fidelity in both simple and\ncomplex scenes. UPainting project page \\url{https://upainting.github.io/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xue Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guohao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhanpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhifan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qiaoqiao She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yajuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SDCL: Self-Distillation Contrastive Learning for Chinese Spell Checking. (arXiv:2210.17168v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.17168","description":"<p>Due to the ambiguity of homophones, Chinese Spell Checking (CSC) has\nwidespread applications. Existing systems typically utilize BERT for text\nencoding. However, CSC requires the model to account for both phonetic and\ngraphemic information. To adapt BERT to the CSC task, we propose a token-level\nself-distillation contrastive learning method. We employ BERT to encode both\nthe corrupted and corresponding correct sentence. Then, we use contrastive\nlearning loss to regularize corrupted tokens' hidden states to be closer to\ncounterparts in the correct sentence. On three CSC datasets, we confirmed our\nmethod provides a significant improvement above baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaotian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent Linguistic Structures in Neural Networks are Fragile. (arXiv:2210.17406v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.17406","description":"<p>Large language models (LLMs) have been reported to have strong performance on\nnatural language processing tasks. However, performance metrics such as\naccuracy do not measure the quality of the model in terms of its ability to\nrobustly represent complex linguistic structure. In this work, we propose a\nframework to evaluate the robustness of linguistic representations using\nprobing tasks. We leverage recent advances in extracting emergent linguistic\nconstructs from LLMs and apply syntax-preserving perturbations to test the\nstability of these constructs in order to better understand the representations\nlearned by LLMs. Empirically, we study the performance of four LLMs across six\ndifferent corpora on the proposed robustness measures. We provide evidence that\ncontext-free representation (e.g., GloVe) are in some cases competitive with\ncontext-dependent representations from modern LLMs (e.g., BERT), yet equally\nbrittle to syntax-preserving manipulations. Emergent syntactic representations\nin neural networks are brittle, thus our work poses the attention on the risk\nof comparing such structures to those that are object of a long lasting debate\nin linguistics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malfa_E/0/1/0/all/0/1\">Emanuele La Malfa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wicker_M/0/1/0/all/0/1\">Matthew Wicker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiatkowska_M/0/1/0/all/0/1\">Marta Kiatkowska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The future is different: Large pre-trained language models fail in prediction tasks. (arXiv:2211.00384v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.00384","description":"<p>Large pre-trained language models (LPLM) have shown spectacular success when\nfine-tuned on downstream supervised tasks. Yet, it is known that their\nperformance can drastically drop when there is a distribution shift between the\ndata used during training and that used at inference time. In this paper we\nfocus on data distributions that naturally change over time and introduce four\nnew REDDIT datasets, namely the WALLSTREETBETS, ASKSCIENCE, THE DONALD, and\nPOLITICS sub-reddits. First, we empirically demonstrate that LPLM can display\naverage performance drops of about 88% (in the best case!) when predicting the\npopularity of future posts from sub-reddits whose topic distribution changes\nwith time. We then introduce a simple methodology that leverages neural\nvariational dynamic topic models and attention mechanisms to infer temporal\nlanguage model representations for regression tasks. Our models display\nperformance drops of only about 40% in the worst cases (2% in the best ones)\nwhen predicting the popularity of future posts, while using only about 7% of\nthe total number of parameters of LPLM and providing interpretable\nrepresentations that offer insight into real-world events, like the GameStop\nshort squeeze of 2021\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cvejoski_K/0/1/0/all/0/1\">Kostadin Cvejoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_R/0/1/0/all/0/1\">Rams&#xe9;s J. S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojeda_C/0/1/0/all/0/1\">C&#xe9;sar Ojeda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kuaipedia: a Large-scale Multi-modal Short-video Encyclopedia. (arXiv:2211.00732v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2211.00732","description":"<p>Online encyclopedias, such as Wikipedia, have been well-developed and\nresearched in the last two decades. One can find any attributes or other\ninformation of a wiki item on a wiki page edited by a community of volunteers.\nHowever, the traditional text, images and tables can hardly express some\naspects of an wiki item. For example, when we talk about ``Shiba Inu'', one may\ncare more about ``How to feed it'' or ``How to train it not to protect its\nfood''. Currently, short-video platforms have become a hallmark in the online\nworld. Whether you're on TikTok, Instagram, Kuaishou, or YouTube Shorts,\nshort-video apps have changed how we consume and create content today. Except\nfor producing short videos for entertainment, we can find more and more authors\nsharing insightful knowledge widely across all walks of life. These short\nvideos, which we call knowledge videos, can easily express any aspects (e.g.\nhair or how-to-feed) consumers want to know about an item (e.g. Shiba Inu), and\nthey can be systematically analyzed and organized like an online encyclopedia.\nIn this paper, we propose Kuaipedia, a large-scale multi-modal encyclopedia\nconsisting of items, aspects, and short videos lined to them, which was\nextracted from billions of videos of Kuaishou (Kwai), a well-known short-video\nplatform in China. We first collected items from multiple sources and mined\nuser-centered aspects from millions of users' queries to build an item-aspect\ntree. Then we propose a new task called ``multi-modal item-aspect linking'' as\nan expansion of ``entity linking'' to link short videos into item-aspect pairs\nand build the whole short-video encyclopedia. Intrinsic evaluations show that\nour encyclopedia is of large scale and highly accurate. We also conduct\nsufficient extrinsic experiments to show how Kuaipedia can help fundamental\napplications such as entity typing and entity linking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Haojie Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuzhou Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_Z/0/1/0/all/0/1\">Zepeng Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1\">Ruiji Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Passage-Mask: A Learnable Regularization Strategy for Retriever-Reader Models. (arXiv:2211.00915v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.00915","description":"<p>Retriever-reader models achieve competitive performance across many different\nNLP tasks such as open question answering and dialogue conversations. In this\nwork, we notice these models easily overfit the top-rank retrieval passages and\nstandard training fails to reason over the entire retrieval passages. We\nintroduce a learnable passage mask mechanism which desensitizes the impact from\nthe top-rank retrieval passages and prevents the model from overfitting.\nControlling the gradient variance with fewer mask candidates and selecting the\nmask candidates with one-shot bi-level optimization, our learnable\nregularization strategy enforces the answer generation to focus on the entire\nretrieval passages. Experiments on different tasks across open question\nanswering, dialogue conversation, and fact verification show that our method\nconsistently outperforms its baselines. Extensive experiments and ablation\nstudies demonstrate that our method can be general, effective, and beneficial\nfor many NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shujian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chengyue Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingchao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese. (arXiv:2211.01335v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.01335","description":"<p>The tremendous success of CLIP (Radford et al., 2021) has promoted the\nresearch and application of contrastive learning for vision-language\npretraining. In this work, we construct a large-scale dataset of image-text\npairs in Chinese, where most data are retrieved from publicly available\ndatasets, and we pretrain Chinese CLIP models on the new dataset. We develop 5\nChinese CLIP models of multiple sizes, spanning from 77 to 958 million\nparameters. Furthermore, we propose a two-stage pretraining method, where the\nmodel is first trained with the image encoder frozen and then trained with all\nparameters being optimized, to achieve enhanced model performance. Our\ncomprehensive experiments demonstrate that Chinese CLIP can achieve the\nstate-of-the-art performance on MUGE, Flickr30K-CN, and COCO-CN in the setups\nof zero-shot learning and finetuning, and it is able to achieve competitive\nperformance in zero-shot image classification based on the evaluation on the\nELEVATER benchmark (Li et al., 2022). We have released our codes, models, and\ndemos in https://github.com/OFA-Sys/Chinese-CLIP\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">An Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junshu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1\">Rui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M-SpeechCLIP: Leveraging Large-Scale, Pre-Trained Models for Multilingual Speech to Image Retrieval. (arXiv:2211.01180v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2211.01180","description":"<p>This work investigates the use of large-scale, pre-trained models (CLIP and\nHuBERT) for multilingual speech-image retrieval. For non-English speech-image\nretrieval, we outperform the current state-of-the-art performance by a wide\nmargin when training separate models for each language, and show that a single\nmodel which processes speech in all three languages still achieves retrieval\nscores comparable with the prior state-of-the-art. We identify key differences\nin model behavior and performance between English and non-English settings,\npresumably attributable to the English-only pre-training of CLIP and HuBERT.\nFinally, we show that our models can be used for mono- and cross-lingual\nspeech-text retrieval and cross-lingual speech-speech retrieval, despite never\nhaving seen any parallel speech-text or speech-speech data during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berry_L/0/1/0/all/0/1\">Layne Berry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shih_Y/0/1/0/all/0/1\">Yi-Jen Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsuan-Fu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-11-03T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}