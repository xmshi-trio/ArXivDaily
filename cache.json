{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-04-04T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"On the Creativity of Large Language Models. (arXiv:2304.00008v1 [cs.AI])","link":"http://arxiv.org/abs/2304.00008","description":"<p>Large Language Models (LLMs) are revolutionizing several areas of Artificial\nIntelligence. One of the most remarkable applications is creative writing,\ne.g., poetry or storytelling: the generated outputs are often of astonishing\nquality. However, a natural question arise: can LLMs really be considered\ncreative? In this article we firstly analyze the development of LLMs under the\nlens of creativity theories, investigating the key open questions and\nchallenges. Then, we identify a set of \"easy\" and \"hard\" problems in machine\ncreativity, discussing them in relation to LLMs. Finally, we analyze the\nsocietal impact of these technologies with a particular focus on the creative\nindustries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Franceschelli_G/0/1/0/all/0/1\">Giorgio Franceschelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musolesi_M/0/1/0/all/0/1\">Mirco Musolesi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis. (arXiv:2304.00020v1 [cs.LG])","link":"http://arxiv.org/abs/2304.00020","description":"<p>The prevalence of memes on social media has created the need to sentiment\nanalyze their underlying meanings for censoring harmful content. Meme censoring\nsystems by machine learning raise the need for a semi-supervised learning\nsolution to take advantage of the large number of unlabeled memes available on\nthe internet and make the annotation process less challenging. Moreover, the\napproach needs to utilize multimodal data as memes' meanings usually come from\nboth images and texts. This research proposes a multimodal semi-supervised\nlearning approach that outperforms other multimodal semi-supervised learning\nand supervised learning state-of-the-art models on two datasets, the Multimedia\nAutomatic Misogyny Identification and Hateful Memes dataset. Building on the\ninsights gained from Contrastive Language-Image Pre-training, which is an\neffective multimodal learning technique, this research introduces SemiMemes, a\nnovel training method that combines auto-encoder and classification task to\nmake use of the resourceful unlabeled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tung_P/0/1/0/all/0/1\">Pham Thai Hoang Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viet_N/0/1/0/all/0/1\">Nguyen Tan Viet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anh_N/0/1/0/all/0/1\">Ngo Tien Anh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_P/0/1/0/all/0/1\">Phan Duy Hung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Demo Alleviate: Demonstrating Artificial Intelligence Enabled Virtual Assistance for Telehealth: The Mental Health Case. (arXiv:2304.00025v1 [cs.CL])","link":"http://arxiv.org/abs/2304.00025","description":"<p>After the pandemic, artificial intelligence (AI) powered support for mental\nhealth care has become increasingly important. The breadth and complexity of\nsignificant challenges required to provide adequate care involve: (a)\nPersonalized patient understanding, (b) Safety-constrained and medically\nvalidated chatbot patient interactions, and (c) Support for continued\nfeedback-based refinements in design using chatbot-patient interactions. We\npropose Alleviate, a chatbot designed to assist patients suffering from mental\nhealth challenges with personalized care and assist clinicians with\nunderstanding their patients better. Alleviate draws from an array of publicly\navailable clinically valid mental-health texts and databases, allowing\nAlleviate to make medically sound and informed decisions. In addition,\nAlleviate's modular design and explainable decision-making lends itself to\nrobust and continued feedback-based refinements to its design. In this paper,\nwe explain the different modules of Alleviate and submit a short video\ndemonstrating Alleviate's capabilities to help patients and clinicians\nunderstand each other better to facilitate optimal care strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_V/0/1/0/all/0/1\">Vedant Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goswami_R/0/1/0/all/0/1\">Raxit Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolbir_N/0/1/0/all/0/1\">Nathan Dolbir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malekar_J/0/1/0/all/0/1\">Jinendra Malekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Symptoms of Delirium from Clinical Narratives Using Natural Language Processing. (arXiv:2304.00111v1 [cs.CL])","link":"http://arxiv.org/abs/2304.00111","description":"<p>Delirium is an acute decline or fluctuation in attention, awareness, or other\ncognitive function that can lead to serious adverse outcomes. Despite the\nsevere outcomes, delirium is frequently unrecognized and uncoded in patients'\nelectronic health records (EHRs) due to its transient and diverse nature.\nNatural language processing (NLP), a key technology that extracts medical\nconcepts from clinical narratives, has shown great potential in studies of\ndelirium outcomes and symptoms. To assist in the diagnosis and phenotyping of\ndelirium, we formed an expert panel to categorize diverse delirium symptoms,\ncomposed annotation guidelines, created a delirium corpus with diverse delirium\nsymptoms, and developed NLP methods to extract delirium symptoms from clinical\nnotes. We compared 5 state-of-the-art transformer models including 2 models\n(BERT and RoBERTa) from the general domain and 3 models (BERT_MIMIC,\nRoBERTa_MIMIC, and GatorTron) from the clinical domain. GatorTron achieved the\nbest strict and lenient F1 scores of 0.8055 and 0.8759, respectively. We\nconducted an error analysis to identify challenges in annotating delirium\nsymptoms and developing NLP systems. To the best of our knowledge, this is the\nfirst large language model-based delirium symptom extraction system. Our study\nlays the foundation for the future development of computable phenotypes and\ndiagnosis methods for delirium.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Aokun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paredes_D/0/1/0/all/0/1\">Daniel Paredes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zehao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_X/0/1/0/all/0/1\">Xiwei Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunson_R/0/1/0/all/0/1\">Roberta Brunson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_J/0/1/0/all/0/1\">Jamie N. Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_K/0/1/0/all/0/1\">Kimberly A. Martinez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucero_R/0/1/0/all/0/1\">Robert J. Lucero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magoc_T/0/1/0/all/0/1\">Tanja Magoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solberg_L/0/1/0/all/0/1\">Laurence M. Solberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snigurska_U/0/1/0/all/0/1\">Urszula A. Snigurska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ser_S/0/1/0/all/0/1\">Sarah E. Ser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prosperi_M/0/1/0/all/0/1\">Mattia Prosperi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bjarnadottir_R/0/1/0/all/0/1\">Ragnhildur I. Bjarnadottir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Sparse Retrieval: Using Sparse Language Models for Inference Efficient Dense Retrieval. (arXiv:2304.00114v1 [cs.IR])","link":"http://arxiv.org/abs/2304.00114","description":"<p>Vector-based retrieval systems have become a common staple for academic and\nindustrial search applications because they provide a simple and scalable way\nof extending the search to leverage contextual representations for documents\nand queries. As these vector-based systems rely on contextual language models,\ntheir usage commonly requires GPUs, which can be expensive and difficult to\nmanage. Given recent advances in introducing sparsity into language models for\nimproved inference efficiency, in this paper, we study how sparse language\nmodels can be used for dense retrieval to improve inference efficiency. Using\nthe popular retrieval library Tevatron and the MSMARCO, NQ, and TriviaQA\ndatasets, we find that sparse language models can be used as direct\nreplacements with little to no drop in accuracy and up to 4.3x improved\ninference speeds\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Campos_D/0/1/0/all/0/1\">Daniel Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Thyroid Nodules Characteristics from Ultrasound Reports Using Transformer-based Natural Language Processing Methods. (arXiv:2304.00115v1 [cs.CL])","link":"http://arxiv.org/abs/2304.00115","description":"<p>The ultrasound characteristics of thyroid nodules guide the evaluation of\nthyroid cancer in patients with thyroid nodules. However, the characteristics\nof thyroid nodules are often documented in clinical narratives such as\nultrasound reports. Previous studies have examined natural language processing\n(NLP) methods in extracting a limited number of characteristics (&lt;9) using\nrule-based NLP systems. In this study, a multidisciplinary team of NLP experts\nand thyroid specialists, identified thyroid nodule characteristics that are\nimportant for clinical care, composed annotation guidelines, developed a\ncorpus, and compared 5 state-of-the-art transformer-based NLP methods,\nincluding BERT, RoBERTa, LongFormer, DeBERTa, and GatorTron, for extraction of\nthyroid nodule characteristics from ultrasound reports. Our GatorTron model, a\ntransformer-based large language model trained using over 90 billion words of\ntext, achieved the best strict and lenient F1-score of 0.8851 and 0.9495 for\nthe extraction of a total number of 16 thyroid nodule characteristics, and\n0.9321 for linking characteristics to nodules, outperforming other clinical\ntransformer models. To the best of our knowledge, this is the first study to\nsystematically categorize and apply transformer-based NLP models to extract a\nlarge number of clinical relevant thyroid nodule characteristics from\nultrasound reports. This study lays ground for assessing the documentation\nquality of thyroid ultrasound reports and examining outcomes of patients with\nthyroid nodules using electronic health records.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pathak_A/0/1/0/all/0/1\">Aman Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zehao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paredes_D/0/1/0/all/0/1\">Daniel Paredes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monsour_E/0/1/0/all/0/1\">Elio Paul Monsour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocha_A/0/1/0/all/0/1\">Andrea Ortiz Rocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brito_J/0/1/0/all/0/1\">Juan P. Brito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ospina_N/0/1/0/all/0/1\">Naykky Singh Ospina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Large Language Models with Climate Resources. (arXiv:2304.00116v1 [cs.CL])","link":"http://arxiv.org/abs/2304.00116","description":"<p>Large language models (LLMs) have significantly transformed the landscape of\nartificial intelligence by demonstrating their ability in generating human-like\ntext across diverse topics. However, despite their impressive capabilities,\nLLMs lack recent information and often employ imprecise language, which can be\ndetrimental in domains where accuracy is crucial, such as climate change. In\nthis study, we make use of recent ideas to harness the potential of LLMs by\nviewing them as agents that access multiple sources, including databases\ncontaining recent and precise information about organizations, institutions,\nand companies. We demonstrate the effectiveness of our method through a\nprototype agent that retrieves emission data from ClimateWatch\n(https://www.climatewatchdata.org/) and leverages general Google search. By\nintegrating these resources with LLMs, our approach overcomes the limitations\nassociated with imprecise language and delivers more reliable and accurate\ninformation in the critical domain of climate change. This work paves the way\nfor future advancements in LLMs and their application in domains where\nprecision is of paramount importance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kraus_M/0/1/0/all/0/1\">Mathias Kraus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bingler_J/0/1/0/all/0/1\">Julia Anna Bingler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leippold_M/0/1/0/all/0/1\">Markus Leippold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schimanski_T/0/1/0/all/0/1\">Tobias Schimanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senni_C/0/1/0/all/0/1\">Chiara Colesanti Senni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stammbach_D/0/1/0/all/0/1\">Dominik Stammbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaghefi_S/0/1/0/all/0/1\">Saeid Ashraf Vaghefi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webersinke_N/0/1/0/all/0/1\">Nicolas Webersinke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoding the End-to-end Writing Trajectory in Scholarly Manuscripts. (arXiv:2304.00121v1 [cs.CL])","link":"http://arxiv.org/abs/2304.00121","description":"<p>Scholarly writing presents a complex space that generally follows a\nmethodical procedure to plan and produce both rationally sound and creative\ncompositions. Recent works involving large language models (LLM) demonstrate\nconsiderable success in text generation and revision tasks; however, LLMs still\nstruggle to provide structural and creative feedback on the document level that\nis crucial to academic writing. In this paper, we introduce a novel taxonomy\nthat categorizes scholarly writing behaviors according to intention, writer\nactions, and the information types of the written data. We also provide\nManuScript, an original dataset annotated with a simplified version of our\ntaxonomy to show writer actions and the intentions behind them. Motivated by\ncognitive writing theory, our taxonomy for scientific papers includes three\nlevels of categorization in order to trace the general writing flow and\nidentify the distinct writer activities embedded within each higher-level\nprocess. ManuScript intends to provide a complete picture of the scholarly\nwriting process by capturing the linearity and non-linearity of writing\ntrajectory, such that writing assistants can provide stronger feedback and\nsuggestions on an end-to-end level. The collected writing trajectories are\nviewed at https://minnesotanlp.github.io/REWARD_demo/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koo_R/0/1/0/all/0/1\">Ryan Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1\">Anna Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Linghe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical Conformer: Optimizing size, speed and flops of Conformer for on-Device and cloud ASR. (arXiv:2304.00171v1 [cs.CL])","link":"http://arxiv.org/abs/2304.00171","description":"<p>Conformer models maintain a large number of internal states, the vast\nmajority of which are associated with self-attention layers. With limited\nmemory bandwidth, reading these from memory at each inference step can slow\ndown inference. In this paper, we design an optimized conformer that is small\nenough to meet on-device restrictions and has fast inference on TPUs. We\nexplore various ideas to improve the execution speed, including replacing lower\nconformer blocks with convolution-only blocks, strategically downsizing the\narchitecture, and utilizing an RNNAttention-Performer. Our optimized conformer\ncan be readily incorporated into a cascaded-encoder setting, allowing a\nsecond-pass decoder to operate on its output and improve the accuracy whenever\nmore resources are available. Altogether, we find that these optimizations can\nreduce latency by a factor of 6.8x, and come at a reasonable trade-off in\nquality. With the cascaded second-pass, we show that the recognition accuracy\nis completely recoverable. Thus, our proposed encoder can double as a strong\nstandalone encoder in on device, and as the first part of a high-performance\nASR pipeline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Botros_R/0/1/0/all/0/1\">Rami Botros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulati_A/0/1/0/all/0/1\">Anmol Gulati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1\">Krzysztof Choromanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1\">Ruoming Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lego-Features: Exporting modular encoder features for streaming and deliberation ASR. (arXiv:2304.00173v1 [cs.CL])","link":"http://arxiv.org/abs/2304.00173","description":"<p>In end-to-end (E2E) speech recognition models, a representational\ntight-coupling inevitably emerges between the encoder and the decoder. We build\nupon recent work that has begun to explore building encoders with modular\nencoded representations, such that encoders and decoders from different models\ncan be stitched together in a zero-shot manner without further fine-tuning.\nWhile previous research only addresses full-context speech models, we explore\nthe problem in a streaming setting as well. Our framework builds on top of\nexisting encoded representations, converting them to modular features, dubbed\nas Lego-Features, without modifying the pre-trained model. The features remain\ninterchangeable when the model is retrained with distinct initializations.\nThough sparse, we show that the Lego-Features are powerful when tested with\nRNN-T or LAS decoders, maintaining high-quality downstream performance. They\nare also rich enough to represent the first-pass prediction during two-pass\ndeliberation. In this scenario, they outperform the N-best hypotheses, since\nthey do not need to be supplemented with acoustic features to deliver the best\nresults. Moreover, generating the Lego-Features does not require beam search or\nauto-regressive computation. Overall, they present a modular, powerful and\ncheap alternative to the standard encoder output, as well as the N-best\nhypotheses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Botros_R/0/1/0/all/0/1\">Rami Botros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schalkwyk_J/0/1/0/all/0/1\">Johan Schalkwyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chelba_C/0/1/0/all/0/1\">Ciprian Chelba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FCC: Fusing Conversation History and Candidate Provenance for Contextual Response Ranking in Dialogue Systems. (arXiv:2304.00180v1 [cs.CL])","link":"http://arxiv.org/abs/2304.00180","description":"<p>Response ranking in dialogues plays a crucial role in retrieval-based\nconversational systems. In a multi-turn dialogue, to capture the gist of a\nconversation, contextual information serves as essential knowledge to achieve\nthis goal. In this paper, we present a flexible neural framework that can\nintegrate contextual information from multiple channels. Specifically for the\ncurrent task, our approach is to provide two information channels in parallel,\nFusing Conversation history and domain knowledge extracted from Candidate\nprovenance (FCC), where candidate responses are curated, as contextual\ninformation to improve the performance of multi-turn dialogue response ranking.\nThe proposed approach can be generalized as a module to incorporate\nmiscellaneous contextual features for other context-oriented tasks. We evaluate\nour model on the MSDialog dataset widely used for evaluating conversational\nresponse ranking tasks. Our experimental results show that our framework\nsignificantly outperforms the previous state-of-the-art models, improving\nRecall@1 by 7% and MAP by 4%. Furthermore, we conduct ablation studies to\nevaluate the contributions of each information channel, and of the framework\ncomponents, to the overall ranking performance, providing additional insights\nand directions for further improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agichtein_E/0/1/0/all/0/1\">Eugene Agichtein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers. (arXiv:2304.00215v1 [cs.CL])","link":"http://arxiv.org/abs/2304.00215","description":"<p>Relation prediction on knowledge graphs (KGs) is a key research topic.\nDominant embedding-based methods mainly focus on the transductive setting and\nlack the inductive ability to generalize to new entities for inference.\nExisting methods for inductive reasoning mostly mine the connections between\nentities, i.e., relational paths, without considering the nature of head and\ntail entities contained in the relational context. This paper proposes a novel\nmethod that captures both connections between entities and the intrinsic nature\nof entities, by simultaneously aggregating RElational Paths and cOntext with a\nunified hieRarchical Transformer framework, namely REPORT. REPORT relies solely\non relation semantics and can naturally generalize to the fully-inductive\nsetting, where KGs for training and inference have no common entities. In the\nexperiments, REPORT performs consistently better than all baselines on almost\nall the eight version subsets of two fully-inductive datasets. Moreover. REPORT\nis interpretable by providing each element's contribution to the prediction\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhendong Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large language models can rate news outlet credibility. (arXiv:2304.00228v1 [cs.CL])","link":"http://arxiv.org/abs/2304.00228","description":"<p>Although large language models (LLMs) have shown exceptional performance in\nvarious natural language processing tasks, they are prone to hallucinations.\nState-of-the-art chatbots, such as the new Bing, attempt to mitigate this issue\nby gathering information directly from the internet to ground their answers. In\nthis setting, the capacity to distinguish trustworthy sources is critical for\nproviding appropriate accuracy contexts to users. Here we assess whether\nChatGPT, a prominent LLM, can evaluate the credibility of news outlets. With\nappropriate instructions, ChatGPT can provide ratings for a diverse set of news\noutlets, including those in non-English languages and satirical sources, along\nwith contextual explanations. Our results show that these ratings correlate\nwith those from human experts (Spearmam's $\\rho=0.54, p&lt;0.001$). These findings\nsuggest that LLMs could be an affordable reference for credibility ratings in\nfact-checking applications. Future LLMs should enhance their alignment with\nhuman expert judgments of source credibility to improve information accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kai-Cheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menczer_F/0/1/0/all/0/1\">Filippo Menczer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Does the Indian Parliament Discuss? An Exploratory Analysis of the Question Hour in the Lok Sabha. (arXiv:2304.00235v1 [cs.CL])","link":"http://arxiv.org/abs/2304.00235","description":"<p>The TCPD-IPD dataset is a collection of questions and answers discussed in\nthe Lower House of the Parliament of India during the Question Hour between\n1999 and 2019. Although it is difficult to analyze such a huge collection\nmanually, modern text analysis tools can provide a powerful means to navigate\nit. In this paper, we perform an exploratory analysis of the dataset. In\nparticular, we present insightful corpus-level statistics and a detailed\nanalysis of three subsets of the dataset. In the latter analysis, the focus is\non understanding the temporal evolution of topics using a dynamic topic model.\nWe observe that the parliamentary conversation indeed mirrors the political and\nsocio-economic tensions of each period.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adhya_S/0/1/0/all/0/1\">Suman Adhya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_D/0/1/0/all/0/1\">Debarshi Kumar Sanyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Crowd Meets Persona: Creating a Large-Scale Open-Domain Persona Dialogue Corpus. (arXiv:2304.00350v1 [cs.CL])","link":"http://arxiv.org/abs/2304.00350","description":"<p>Building a natural language dataset requires caution since word semantics is\nvulnerable to subtle text change or the definition of the annotated concept.\nSuch a tendency can be seen in generative tasks like question-answering and\ndialogue generation and also in tasks that create a categorization-based\ncorpus, like topic classification or sentiment analysis. Open-domain\nconversations involve two or more crowdworkers freely conversing about any\ntopic, and collecting such data is particularly difficult for two reasons: 1)\nthe dataset should be ``crafted\" rather than ``obtained\" due to privacy\nconcerns, and 2) paid creation of such dialogues may differ from how\ncrowdworkers behave in real-world settings. In this study, we tackle these\nissues when creating a large-scale open-domain persona dialogue corpus, where\npersona implies that the conversation is performed by several actors with a\nfixed persona and user-side workers from an unspecified crowd.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_W/0/1/0/all/0/1\">Won Ik Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yoon Kyung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1\">Seoyeon Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jihwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sangah Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Moosung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahn_S/0/1/0/all/0/1\">Sowon Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Nam Soo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Authorship Attribution in the Work of Tirso de Molina. (arXiv:2304.00363v1 [cs.CL])","link":"http://arxiv.org/abs/2304.00363","description":"<p>Automatic Authorship Attribution (AAA) is the result of applying tools and\ntechniques from Digital Humanities to authorship attribution studies. Through a\nquantitative and statistical approach this discipline can draw further\nconclusions about renowned authorship issues which traditional critics have\nbeen dealing with for centuries, opening a new door to style comparison. The\naim of this paper is to prove the potential of these tools and techniques by\ntesting the authorship of five comedies traditionally attributed to Spanish\nplaywright Tirso de Molina (1579-1648): La ninfa del cielo, El burlador de\nSevilla, Tan largo me lo fiais, La mujer por fuerza and El condenado por\ndesconfiado. To accomplish this purpose some experiments concerning clustering\nanalysis by Stylo package from R and four distance measures are carried out on\na corpus built with plays by Tirso, Andres de Claramonte (c. 1560-1626),\nAntonio Mira de Amescua (1577-1644) and Luis Velez de Guevara (1579-1644). The\nresults obtained point to the denial of all the attributions to Tirso except\nfor the case of La mujer por fuerza.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cavadas_M/0/1/0/all/0/1\">Miguel Cavadas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gamallo_P/0/1/0/all/0/1\">Pablo Gamallo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Healthy AI: Large Language Models Need Therapists Too. (arXiv:2304.00416v1 [cs.AI])","link":"http://arxiv.org/abs/2304.00416","description":"<p>Recent advances in large language models (LLMs) have led to the development\nof powerful AI chatbots capable of engaging in natural and human-like\nconversations. However, these chatbots can be potentially harmful, exhibiting\nmanipulative, gaslighting, and narcissistic behaviors. We define Healthy AI to\nbe safe, trustworthy and ethical. To create healthy AI systems, we present the\nSafeguardGPT framework that uses psychotherapy to correct for these harmful\nbehaviors in AI chatbots. The framework involves four types of AI agents: a\nChatbot, a \"User,\" a \"Therapist,\" and a \"Critic.\" We demonstrate the\neffectiveness of SafeguardGPT through a working example of simulating a social\nconversation. Our results show that the framework can improve the quality of\nconversations between AI chatbots and humans. Although there are still several\nchallenges and directions to be addressed in the future, SafeguardGPT provides\na promising approach to improving the alignment between AI chatbots and human\nvalues. By incorporating psychotherapy and reinforcement learning techniques,\nthe framework enables AI chatbots to learn and adapt to human preferences and\nvalues in a safe and ethical way, contributing to the development of a more\nhuman-centric and responsible AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Baihan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouneffouf_D/0/1/0/all/0/1\">Djallel Bouneffouf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cecchi_G/0/1/0/all/0/1\">Guillermo Cecchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_K/0/1/0/all/0/1\">Kush R. Varshney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structural invariants and semantic fingerprints in the \"ego network\" of words. (arXiv:2203.00588v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2203.00588","description":"<p>Well-established cognitive models coming from anthropology have shown that,\ndue to the cognitive constraints that limit our \"bandwidth\" for social\ninteractions, humans organize their social relations according to a regular\nstructure. In this work, we postulate that similar regularities can be found in\nother cognitive processes, such as those involving language production. In\norder to investigate this claim, we analyse a dataset containing tweets of a\nheterogeneous group of Twitter users (regular users and professional writers).\nLeveraging a methodology similar to the one used to uncover the\nwell-established social cognitive constraints, we find regularities at both the\nstructural and semantic level. At the former, we find that a concentric layered\nstructure (which we call ego network of words, in analogy to the ego network of\nsocial relationships) very well captures how individuals organise the words\nthey use. The size of the layers in this structure regularly grows\n(approximately 2-3 times with respect to the previous one) when moving\noutwards, and the two penultimate external layers consistently account for\napproximately 60% and 30% of the used words, irrespective of the number of the\ntotal number of layers of the user. For the semantic analysis, each ring of\neach ego network is described by a semantic profile, which captures the topics\nassociated with the words in the ring. We find that ring #1 has a special role\nin the model. It is semantically the most dissimilar and the most diverse among\nthe rings. We also show that the topics that are important in the innermost\nring also have the characteristic of being predominant in each of the other\nrings, as well as in the entire ego network. In this respect, ring #1 can be\nseen as the semantic fingerprint of the ego network of words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ollivier_K/0/1/0/all/0/1\">Kilian Ollivier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boldrini_C/0/1/0/all/0/1\">Chiara Boldrini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passarella_A/0/1/0/all/0/1\">Andrea Passarella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Marco Conti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding. (arXiv:2203.05711v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05711","description":"<p>Despite recent advances of AI, story understanding remains an open and\nunder-investigated problem. We collect, preprocess, and publicly release a\nvideo-language story dataset, Synopses of Movie Narratives (SYMON), containing\n5,193 video summaries of popular movies and TV series. SYMON captures\nnaturalistic story-telling videos for human audience made by human creators. As\na prototypical and naturalistic story dataset, SYMON features high coverage of\nmultimodal story events, abundant mental-state descriptions, and large semantic\ngaps between the visual and the textual modalities. We establish benchmarks on\nvideo-text retrieval and zero-shot alignment on movie summary videos, which\nshowcase the importance of in-domain data in story understanding. With SYMON,\nwe hope to lay the groundwork for progress in multimodal story understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yidan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_Q/0/1/0/all/0/1\">Qin Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Passage Retrieval with Zero-Shot Question Generation. (arXiv:2204.07496v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07496","description":"<p>We propose a simple and effective re-ranking method for improving passage\nretrieval in open question answering. The re-ranker re-scores retrieved\npassages with a zero-shot question generation model, which uses a pre-trained\nlanguage model to compute the probability of the input question conditioned on\na retrieved passage. This approach can be applied on top of any retrieval\nmethod (e.g. neural or keyword-based), does not require any domain- or\ntask-specific training (and therefore is expected to generalize better to data\ndistribution shifts), and provides rich cross-attention between query and\npassage (i.e. it must explain every token in the question). When evaluated on a\nnumber of open-domain retrieval datasets, our re-ranker improves strong\nunsupervised retrieval models by 6%-18% absolute and strong supervised models\nby up to 12% in terms of top-20 passage retrieval accuracy. We also obtain new\nstate-of-the-art results on full open-domain question answering by simply\nadding the new re-ranker to existing models with no further changes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sachan_D/0/1/0/all/0/1\">Devendra Singh Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1\">Mandar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1\">Joelle Pineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization. (arXiv:2205.12191v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12191","description":"<p>Vision-and-language (V&amp;L) models pretrained on large-scale multimodal data\nhave demonstrated strong performance on various tasks such as image captioning\nand visual question answering (VQA). The quality of such models is commonly\nassessed by measuring their performance on unseen data that typically comes\nfrom the same distribution as the training data. However, when evaluated under\nout-of-distribution (out-of-dataset) settings for VQA, we observe that these\nmodels exhibit poor generalization. We comprehensively evaluate two pretrained\nV&amp;L models under different settings (i.e. classification and open-ended text\ngeneration) by conducting cross-dataset evaluations. We find that these models\ntend to learn to solve the benchmark, rather than learning the high-level\nskills required by the VQA task. We also find that in most cases generative\nmodels are less susceptible to shifts in data distribution compared to\ndiscriminative ones, and that multimodal pretraining is generally helpful for\nOOD generalization. Finally, we revisit assumptions underlying the use of\nautomatic VQA evaluation metrics, and empirically show that their stringent\nnature repeatedly penalizes models for correct responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Aishwarya Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kajic_I/0/1/0/all/0/1\">Ivana Kaji&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1\">Emanuele Bugliarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davoodi_E/0/1/0/all/0/1\">Elnaz Davoodi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gergely_A/0/1/0/all/0/1\">Anita Gergely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blunsom_P/0/1/0/all/0/1\">Phil Blunsom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1\">Aida Nematzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse*BERT: Sparse Models Generalize To New tasks and Domains. (arXiv:2205.12452v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12452","description":"<p>Large Language Models have become the core architecture upon which most\nmodern natural language processing (NLP) systems build. These models can\nconsistently deliver impressive accuracy and robustness across tasks and\ndomains, but their high computational overhead can make inference difficult and\nexpensive. To make using these models less costly, recent work has explored\nleveraging structured and unstructured pruning, quantization, and distillation\nto improve inference speed and decrease size. This paper studies how models\npruned using Gradual Unstructured Magnitude Pruning can transfer between\ndomains and tasks. Our experimentation shows that models that are pruned during\npretraining using general domain masked language models can transfer to novel\ndomains and tasks without extensive hyperparameter exploration or specialized\napproaches. We demonstrate that our general sparse model Sparse*BERT can become\nSparseBioBERT simply by pretraining the compressed architecture on unstructured\nbiomedical text. Moreover, we show that SparseBioBERT can match the quality of\nBioBERT with only 10\\% of the parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Campos_D/0/1/0/all/0/1\">Daniel Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marques_A/0/1/0/all/0/1\">Alexandre Marques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtz_M/0/1/0/all/0/1\">Mark Kurtz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Questions Are All You Need to Train a Dense Passage Retriever. (arXiv:2206.10658v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.10658","description":"<p>We introduce ART, a new corpus-level autoencoding approach for training dense\nretrieval models that does not require any labeled training data. Dense\nretrieval is a central challenge for open-domain tasks, such as Open QA, where\nstate-of-the-art methods typically require large supervised datasets with\ncustom hard-negative mining and denoising of positive examples. ART, in\ncontrast, only requires access to unpaired inputs and outputs (e.g. questions\nand potential answer documents). It uses a new document-retrieval autoencoding\nscheme, where (1) an input question is used to retrieve a set of evidence\ndocuments, and (2) the documents are then used to compute the probability of\nreconstructing the original question. Training for retrieval based on question\nreconstruction enables effective unsupervised learning of both document and\nquestion encoders, which can be later incorporated into complete Open QA\nsystems without any further finetuning. Extensive experiments demonstrate that\nART obtains state-of-the-art results on multiple QA retrieval benchmarks with\nonly generic initialization from a pre-trained language model, removing the\nneed for labeled data and task-specific losses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sachan_D/0/1/0/all/0/1\">Devendra Singh Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1\">Joelle Pineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Manzil Zaheer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DHGE: Dual-View Hyper-Relational Knowledge Graph Embedding for Link Prediction and Entity Typing. (arXiv:2207.08562v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2207.08562","description":"<p>In the field of representation learning on knowledge graphs (KGs), a\nhyper-relational fact consists of a main triple and several auxiliary\nattribute-value descriptions, which is considered more comprehensive and\nspecific than a triple-based fact. However, currently available\nhyper-relational KG embedding methods in a single view are limited in\napplication because they weaken the hierarchical structure that represents the\naffiliation between entities. To overcome this limitation, we propose a\ndual-view hyper-relational KG structure (DH-KG) that contains a\nhyper-relational instance view for entities and a hyper-relational ontology\nview for concepts that are abstracted hierarchically from the entities. This\npaper defines link prediction and entity typing tasks on DH-KG for the first\ntime and constructs two DH-KG datasets, JW44K-6K, extracted from Wikidata, and\nHTDM based on medical data. Furthermore, we propose DHGE, a DH-KG embedding\nmodel based on GRAN encoders, HGNNs, and joint learning. DHGE outperforms\nbaseline models on DH-KG, according to experimental results. Finally, we\nprovide an example of how this technology can be used to treat hypertension.\nOur model and new datasets are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haoran Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+E_H/0/1/0/all/0/1\">Haihong E</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Ling Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Gengxian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Tianyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_K/0/1/0/all/0/1\">Kaiyang Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large scale analysis of gender bias and sexism in song lyrics. (arXiv:2208.02052v3 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2208.02052","description":"<p>We employ Natural Language Processing techniques to analyse 377808 English\nsong lyrics from the \"Two Million Song Database\" corpus, focusing on the\nexpression of sexism across five decades (1960-2010) and the measurement of\ngender biases. Using a sexism classifier, we identify sexist lyrics at a larger\nscale than previous studies using small samples of manually annotated popular\nsongs. Furthermore, we reveal gender biases by measuring associations in word\nembeddings learned on song lyrics. We find sexist content to increase across\ntime, especially from male artists and for popular songs appearing in Billboard\ncharts. Songs are also shown to contain different language biases depending on\nthe gender of the performer, with male solo artist songs containing more and\nstronger biases. This is the first large scale analysis of this type, giving\ninsights into language usage in such an influential part of popular culture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Betti_L/0/1/0/all/0/1\">Lorenzo Betti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abrate_C/0/1/0/all/0/1\">Carlo Abrate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaltenbrunner_A/0/1/0/all/0/1\">Andreas Kaltenbrunner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MENLI: Robust Evaluation Metrics from Natural Language Inference. (arXiv:2208.07316v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.07316","description":"<p>Recently proposed BERT-based evaluation metrics for text generation perform\nwell on standard benchmarks but are vulnerable to adversarial attacks, e.g.,\nrelating to information correctness. We argue that this stems (in part) from\nthe fact that they are models of semantic similarity. In contrast, we develop\nevaluation metrics based on Natural Language Inference (NLI), which we deem a\nmore appropriate modeling. We design a preference-based adversarial attack\nframework and show that our NLI based metrics are much more robust to the\nattacks than the recent BERT-based metrics. On standard benchmarks, our NLI\nbased metrics outperform existing summarization metrics, but perform below SOTA\nMT metrics. However, when combining existing metrics with our NLI metrics, we\nobtain both higher adversarial robustness (15%-30%) and higher quality metrics\nas measured on standard benchmarks (+5% to 30%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining. (arXiv:2210.10341v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10341","description":"<p>Pre-trained language models have attracted increasing attention in the\nbiomedical domain, inspired by their great success in the general natural\nlanguage domain. Among the two main branches of pre-trained language models in\nthe general language domain, i.e., BERT (and its variants) and GPT (and its\nvariants), the first one has been extensively studied in the biomedical domain,\nsuch as BioBERT and PubMedBERT. While they have achieved great success on a\nvariety of discriminative downstream biomedical tasks, the lack of generation\nability constrains their application scope. In this paper, we propose BioGPT, a\ndomain-specific generative Transformer language model pre-trained on large\nscale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and\ndemonstrate that our model outperforms previous models on most tasks.\nEspecially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI\nend-to-end relation extraction tasks respectively, and 78.2% accuracy on\nPubMedQA, creating a new record. Our case study on text generation further\ndemonstrates the advantage of BioGPT on biomedical literature to generate\nfluent descriptions for biomedical terms. Code is available at\nhttps://github.com/microsoft/BioGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Liai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yingce Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs. (arXiv:2210.13312v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13312","description":"<p>Social intelligence and Theory of Mind (ToM), i.e., the ability to reason\nabout the different mental states, intents, and reactions of all people\ninvolved, allow humans to effectively navigate and understand everyday social\ninteractions. As NLP systems are used in increasingly complex social\nsituations, their ability to grasp social dynamics becomes crucial. In this\nwork, we examine the open question of social intelligence and Theory of Mind in\nmodern NLP systems from an empirical and theory-based perspective. We show that\none of today's largest language models (GPT-3; Brown et al., 2020) lacks this\nkind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et\nal., 2019), which measures models' ability to understand intents and reactions\nof participants of social interactions, and ToMi (Le et al., 2019), which\nmeasures whether models can infer mental states and realities of participants\nof situations. Our results show that models struggle substantially at these\nTheory of Mind tasks, with well-below-human accuracies of 55% and 60% on\nSocialIQa and ToMi, respectively. To conclude, we draw on theories from\npragmatics to contextualize this shortcoming of large language models, by\nexamining the limitations stemming from their data, neural architecture, and\ntraining paradigms. Challenging the prevalent narrative that only scale is\nneeded, we posit that person-centric NLP approaches might be more effective\ntowards neural Theory of Mind.\n</p>\n<p>In our updated version, we also analyze newer instruction tuned and RLFH\nmodels for neural ToM. We find that even ChatGPT and GPT-4 do not display\nemergent Theory of Mind; strikingly even GPT-4 performs only 60% accuracy on\nthe ToMi questions related to mental states and realities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeBras_R/0/1/0/all/0/1\">Ronan LeBras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Representations of Mathematical Expressions in a Continuous Vector Space. (arXiv:2211.08142v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08142","description":"<p>Mathematical notation makes up a large portion of STEM literature, yet,\nfinding semantic representations for formulae remains a challenging problem.\nBecause mathematical notation is precise, and its meaning changes significantly\nwith small character shifts, the methods that work for natural text do not\nnecessarily work well for mathematical expressions. In this work, we describe\nan approach for representing mathematical expressions in a continuous vector\nspace. We use the encoder of a sequence-to-sequence architecture, trained on\nvisually different but mathematically equivalent expressions, to generate\nvector representations (or embeddings). We compare this approach with an\nautoencoder and show that the former is better at capturing mathematical\nsemantics. Finally, to expedite future research, we publish a corpus of\nequivalent transcendental and algebraic expression pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gangwar_N/0/1/0/all/0/1\">Neeraj Gangwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kani_N/0/1/0/all/0/1\">Nickvash Kani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model. (arXiv:2211.11152v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.11152","description":"<p>Large-scale Transformer models bring significant improvements for various\ndownstream vision language tasks with a unified architecture. The performance\nimprovements come with increasing model size, resulting in slow inference speed\nand increased cost for severing. While some certain predictions benefit from\nthe full complexity of the large-scale model, not all of inputs need the same\namount of computation to conduct, potentially leading to computation resource\nwaste. To handle this challenge, early exiting is proposed to adaptively\nallocate computational power in term of input complexity to improve inference\nefficiency. The existing early exiting strategies usually adopt output\nconfidence based on intermediate layers as a proxy of input complexity to incur\nthe decision of skipping following layers. However, such strategies cannot\napply to encoder in the widely-used unified architecture with both encoder and\ndecoder due to difficulty of output confidence estimation in the encoder. It is\nsuboptimal in term of saving computation power to ignore the early exiting in\nencoder component. To handle this challenge, we propose a novel early exiting\nstrategy for unified visual language models, which allows dynamically skip the\nlayers in encoder and decoder simultaneously in term of input layer-wise\nsimilarities with multiple times of early exiting, namely \\textbf{MuE}. By\ndecomposing the image and text modalities in the encoder, MuE is flexible and\ncan skip different layers in term of modalities, advancing the inference\nefficiency while minimizing performance drop. Experiments on the SNLI-VE and MS\nCOCO datasets show that the proposed approach MuE can reduce expected inference\ntime by up to 50\\% and 40\\% while maintaining 99\\% and 96\\% performance\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shengkun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1\">Zhenglun Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianchi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Caiwen Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yi Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dongkuan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ham2Pose: Animating Sign Language Notation into Pose Sequences. (arXiv:2211.13613v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.13613","description":"<p>Translating spoken languages into Sign languages is necessary for open\ncommunication between the hearing and hearing-impaired communities. To achieve\nthis goal, we propose the first method for animating a text written in\nHamNoSys, a lexical Sign language notation, into signed pose sequences. As\nHamNoSys is universal by design, our proposed method offers a generic solution\ninvariant to the target Sign language. Our method gradually generates pose\npredictions using transformer encoders that create meaningful representations\nof the text and poses while considering their spatial and temporal information.\nWe use weak supervision for the training process and show that our method\nsucceeds in learning from partial and inaccurate data. Additionally, we offer a\nnew distance measurement that considers missing keypoints, to measure the\ndistance between pose sequences using DTW-MJE. We validate its correctness\nusing AUTSL, a large-scale Sign language dataset, show that it measures the\ndistance between pose sequences more accurately than existing measurements, and\nuse it to assess the quality of our generated pose sequences. Code for the data\npre-processing, the model, and the distance measurement is publicly released\nfor future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shalev_Arkushin_R/0/1/0/all/0/1\">Rotem Shalev-Arkushin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moryossef_A/0/1/0/all/0/1\">Amit Moryossef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_O/0/1/0/all/0/1\">Ohad Fried</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Montague semantics and modifier consistency measurement in neural language models. (arXiv:2212.04310v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.04310","description":"<p>In recent years, distributional language representation models have\ndemonstrated great practical success. At the same time, the need for\ninterpretability has elicited questions on their intrinsic properties and\ncapabilities. Crucially, distributional models are often inconsistent when\ndealing with compositional phenomena in natural language, which has significant\nimplications for their safety and fairness. Despite this, most current research\non compositionality is directed towards improving their performance on\nsimilarity tasks only. This work takes a different approach, and proposes a\nmethodology for measuring compositional behavior in contemporary language\nmodels. Specifically, we focus on adjectival modifier phenomena in\nadjective-noun phrases. We introduce three novel tests of compositional\nbehavior inspired by Montague semantics. Our experimental results indicate that\ncurrent neural language models behave according to the expected linguistic\ntheories to a limited extent only. This raises the question of whether these\nlanguage models are not able to capture the semantic properties we evaluated,\nor whether linguistic theories from Montagovian tradition would not match the\nexpected capabilities of distributional models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_D/0/1/0/all/0/1\">Danilo S. Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manino_E/0/1/0/all/0/1\">Edoardo Manino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozanova_J/0/1/0/all/0/1\">Julia Rozanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordeiro_L/0/1/0/all/0/1\">Lucas Cordeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Highlighting Named Entities in Input for Auto-Formulation of Optimization Problems. (arXiv:2212.13201v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.13201","description":"<p>Operations research deals with modeling and solving real-world problems as\nmathematical optimization problems. While solving mathematical systems is\naccomplished by analytical software, formulating a problem as a set of\nmathematical operations has been typically done manually by domain experts.\nRecent machine learning methods have shown promise in converting textual\nproblem descriptions to corresponding mathematical formulations. This paper\npresents an approach that converts linear programming word problems into\nmathematical formulations. We leverage the named entities in the input and\naugment the input to highlight these entities. Our approach achieves the\nhighest accuracy among all submissions to the NL4Opt Competition, securing\nfirst place in the generation track.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gangwar_N/0/1/0/all/0/1\">Neeraj Gangwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kani_N/0/1/0/all/0/1\">Nickvash Kani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in Radiology. (arXiv:2301.02228v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2301.02228","description":"<p>In this paper, we consider enhancing medical visual-language pre-training\n(VLP) with domain-specific knowledge, by exploiting the paired image-text\nreports from the radiological daily practice. In particular, we make the\nfollowing contributions: First, unlike existing works that directly process the\nraw reports, we adopt a novel triplet extraction module to extract the\nmedical-related information, avoiding unnecessary complexity from language\ngrammar and enhancing the supervision signals; Second, we propose a novel\ntriplet encoding module with entity translation by querying a knowledge base,\nto exploit the rich domain knowledge in medical field, and implicitly build\nrelationships between medical entities in the language embedding space; Third,\nwe propose to use a Transformer-based fusion model for spatially aligning the\nentity description with visual signals at the image patch level, enabling the\nability for medical diagnosis; Fourth, we conduct thorough experiments to\nvalidate the effectiveness of our architecture, and benchmark on numerous\npublic benchmarks, e.g., ChestX-ray14, RSNA Pneumonia, SIIM-ACR Pneumothorax,\nCOVIDx CXR-2, COVID Rural, and EdemaSeverity. In both zero-shot and fine-tuning\nsettings, our model has demonstrated strong performance compared with the\nformer methods on disease classification and grounding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1\">Chaoyi Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoman Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Generation of Paired Antibody Chain Sequences through Encoder-Decoder Language Model. (arXiv:2301.02748v2 [q-bio.BM] UPDATED)","link":"http://arxiv.org/abs/2301.02748","description":"<p>Protein language models (LMs) have been successful in sequence, structural\nand functional predictions. However, currently, protein LMs are limited to\nencoder- or decoder-only architectures for single sequences while many\nbiological contexts involve protein-protein interactions. Here, we introduce\npAbT5, which models antibody chain pairing as forward- and back-translations\nusing a T5-based architecture. We show that pAbT5 accurately reflects chain\npairing through sequence generation. Our protein LM generates variable-length\nsequences and its next-word prediction probability agrees with\nposition-specific scoring matrix from sequence alignment. Like other works in\nprotein LM, pAbT5 performs state-of-the-art unsupervised prediction on\nexperimental measurements. To the best of our knowledge, pAbT5 is the first\ngenerative encoder-decoder protein LM for protein-protein interactions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Chu_S/0/1/0/all/0/1\">Simon K.S. Chu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wei_K/0/1/0/all/0/1\">Kathy Y. Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Truveta Mapper: A Zero-shot Ontology Alignment Framework. (arXiv:2301.09767v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2301.09767","description":"<p>In this paper, a new perspective is suggested for unsupervised Ontology\nMatching (OM) or Ontology Alignment (OA) by treating it as a translation task.\nOntologies are represented as graphs, and the translation is performed from a\nnode in the source ontology graph to a path in the target ontology graph. The\nproposed framework, Truveta Mapper (TM), leverages a multi-task\nsequence-to-sequence transformer model to perform alignment across multiple\nontologies in a zero-shot, unified and end-to-end manner. Multi-tasking enables\nthe model to implicitly learn the relationship between different ontologies via\ntransfer-learning without requiring any explicit cross-ontology manually\nlabeled data. This also enables the formulated framework to outperform existing\nsolutions for both runtime latency and alignment quality. The model is\npre-trained and fine-tuned only on publicly available text corpus and\ninner-ontologies data. The proposed solution outperforms state-of-the-art\napproaches, Edit-Similarity, LogMap, AML, BERTMap, and the recently presented\nnew OM frameworks in Ontology Alignment Evaluation Initiative (OAEI22), offers\nlog-linear complexity in contrast to quadratic in the existing end-to-end\nmethods, and overall makes the OM task efficient and more straightforward\nwithout much post-processing involving mapping extension or mapping repair.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amir_M/0/1/0/all/0/1\">Mariyam Amir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baruah_M/0/1/0/all/0/1\">Murchana Baruah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eslamialishah_M/0/1/0/all/0/1\">Mahsa Eslamialishah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsani_S/0/1/0/all/0/1\">Sina Ehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahramali_A/0/1/0/all/0/1\">Alireza Bahramali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naddaf_Sh_S/0/1/0/all/0/1\">Sadra Naddaf-Sh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarandioon_S/0/1/0/all/0/1\">Saman Zarandioon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AfriSenti: A Twitter Sentiment Analysis Benchmark for African Languages. (arXiv:2302.08956v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.08956","description":"<p>Africa is home to over 2000 languages from over six language families and has\nthe highest linguistic diversity among all continents. This includes 75\nlanguages with at least one million speakers each. Yet, there is little NLP\nresearch conducted on African languages. Crucial in enabling such research is\nthe availability of high-quality annotated datasets. In this paper, we\nintroduce AfriSenti, which consists of 14 sentiment datasets of 110,000+ tweets\nin 14 African languages (Amharic, Algerian Arabic, Hausa, Igbo, Kinyarwanda,\nMoroccan Arabic, Mozambican Portuguese, Nigerian Pidgin, Oromo, Swahili,\nTigrinya, Twi, Xitsonga, and Yor\\`ub\\'a) from four language families annotated\nby native speakers. The data is used in SemEval 2023 Task 12, the first\nAfro-centric SemEval shared task. We describe the data collection methodology,\nannotation process, and related challenges when curating each of the datasets.\nWe conduct experiments with different sentiment classification baselines and\ndiscuss their usefulness. We hope AfriSenti enables new work on\nunder-represented languages. The dataset is available at\nhttps://github.com/afrisenti-semeval/afrisent-semeval-2023 and can also be\nloaded as a huggingface datasets\n(https://huggingface.co/datasets/shmuhammad/AfriSenti).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen Hassan Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdulmumin_I/0/1/0/all/0/1\">Idris Abdulmumin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayele_A/0/1/0/all/0/1\">Abinew Ali Ayele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ousidhoum_N/0/1/0/all/0/1\">Nedjma Ousidhoum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yimam_S/0/1/0/all/0/1\">Seid Muhie Yimam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_I/0/1/0/all/0/1\">Ibrahim Sa&#x27;id Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beloucif_M/0/1/0/all/0/1\">Meriem Beloucif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif Mohammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hourrane_O/0/1/0/all/0/1\">Oumaima Hourrane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brazdil_P/0/1/0/all/0/1\">Pavel Brazdil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_F/0/1/0/all/0/1\">Felermino D&#xe1;rio M&#xe1;rio Ant&#xf3;nio Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_D/0/1/0/all/0/1\">Davis Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osei_S/0/1/0/all/0/1\">Salomey Osei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bello_B/0/1/0/all/0/1\">Bello Shehu Bello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_F/0/1/0/all/0/1\">Falalu Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwadabe_T/0/1/0/all/0/1\">Tajuddeen Gwadabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rutunda_S/0/1/0/all/0/1\">Samuel Rutunda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belay_T/0/1/0/all/0/1\">Tadesse Belay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messelle_W/0/1/0/all/0/1\">Wendimu Baye Messelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balcha_H/0/1/0/all/0/1\">Hailu Beshada Balcha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chala_S/0/1/0/all/0/1\">Sisay Adugna Chala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gebremichael_H/0/1/0/all/0/1\">Hagos Tesfahun Gebremichael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Opoku_B/0/1/0/all/0/1\">Bernard Opoku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arthur_S/0/1/0/all/0/1\">Steven Arthur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Block-wise Bit-Compression of Transformer-based Models. (arXiv:2303.09184v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.09184","description":"<p>With the popularity of the recent Transformer-based models represented by\nBERT, GPT-3 and ChatGPT, there has been state-of-the-art performance in a range\nof natural language processing tasks. However, the massive computations, huge\nmemory footprint, and thus high latency of Transformer-based models is an\ninevitable challenge for the cloud with high real-time requirement. To tackle\nthe issue, we propose BBCT, a method of block-wise bit-compression for\ntransformer without retraining. Our method achieves more fine-grained\ncompression of the whole transformer, including embedding, matrix\nmultiplication, GELU, softmax, layer normalization, and all the intermediate\nresults. As a case, we compress an efficient BERT with the method of BBCT. Our\nbenchmark test results on General Language Understanding Evaluation (GLUE) show\nthat BBCT can achieve less than 1% accuracy drop in most tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Gaochen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEGA: Multilingual Evaluation of Generative AI. (arXiv:2303.12528v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.12528","description":"<p>Generative AI models have impressive performance on many Natural Language\nProcessing tasks such as language understanding, reasoning and language\ngeneration. One of the most important questions that is being asked by the AI\ncommunity today is about the capabilities and limits of these models, and it is\nclear that evaluating generative AI is very challenging. Most studies on\ngenerative Large Language Models (LLMs) are restricted to English and it is\nunclear how capable these models are at understanding and generating other\nlanguages. We present the first comprehensive benchmarking of generative LLMs -\nMEGA, which evaluates models on standard NLP benchmarks, covering 8 diverse\ntasks and 33 typologically diverse languages. We also compare the performance\nof generative LLMs to State of the Art (SOTA) non-autoregressive models on\nthese tasks to determine how well generative models perform compared to the\nprevious generation of LLMs. We present a thorough analysis of the performance\nof models across languages and discuss some of the reasons why generative LLMs\nare currently not optimal for all languages. We create a framework for\nevaluating generative LLMs in the multilingual setting and provide directions\nfor future progress in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1\">Kabir Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hada_R/0/1/0/all/0/1\">Rishav Hada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochieng_M/0/1/0/all/0/1\">Millicent Ochieng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Prachi Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diddee_H/0/1/0/all/0/1\">Harshita Diddee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maina_S/0/1/0/all/0/1\">Samuel Maina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganu_T/0/1/0/all/0/1\">Tanuja Ganu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segal_S/0/1/0/all/0/1\">Sameer Segal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Axmed_M/0/1/0/all/0/1\">Maxamed Axmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bali_K/0/1/0/all/0/1\">Kalika Bali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitaram_S/0/1/0/all/0/1\">Sunayana Sitaram</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. (arXiv:2303.12570v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.12570","description":"<p>The task of repository-level code completion is to continue writing the\nunfinished code based on a broader context of the repository. While for\nautomated code completion tools, it is difficult to utilize the useful\ninformation scattered in different files. We propose RepoCoder, a simple,\ngeneric, and effective framework to address the challenge. It streamlines the\nrepository-level code completion process by incorporating a similarity-based\nretriever and a pre-trained code language model, which allows for the effective\nutilization of repository-level information for code completion and grants the\nability to generate code at various levels of granularity. Furthermore,\nRepoCoder utilizes a novel iterative retrieval-generation paradigm that bridges\nthe gap between retrieval context and the intended completion target. We also\npropose a new benchmark RepoEval, which consists of the latest and high-quality\nreal-world repositories covering line, API invocation, and function body\ncompletion scenarios. We test the performance of RepoCoder by using various\ncombinations of code retrievers and generators. Experimental results indicate\nthat RepoCoder significantly improves the zero-shot code completion baseline by\nover 10% in all settings and consistently outperforms the vanilla\nretrieval-augmented code completion approach. Furthermore, we validate the\neffectiveness of RepoCoder through comprehensive analysis, providing valuable\ninsights for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fengji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zan_D/0/1/0/all/0/1\">Daoguang Zan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge. (arXiv:2303.14070v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.14070","description":"<p>Recent large language models (LLMs) in the general domain, such as ChatGPT,\nhave shown remarkable success in following instructions and producing\nhuman-like responses. However, such language models have not been tailored to\nthe medical domain, resulting in poor answer accuracy and inability to give\nplausible recommendations for medical diagnosis, medications, etc. To address\nthis issue, we collected more than 700 diseases and their corresponding\nsymptoms, required medical tests, and recommended medications, from which we\ngenerated 5K doctor-patient conversations. In addition, we obtained 200K real\npatient-doctor conversations from online Q\\&amp;A medical consultation sites. By\nfine-tuning LLMs using these 205k doctor-patient conversations, the resulting\nmodels emerge with great potential to understand patients' needs, provide\ninformed advice, and offer valuable assistance in a variety of medical-related\nfields. The integration of these advanced language models into healthcare can\nrevolutionize the way healthcare professionals and patients communicate,\nultimately improving the overall efficiency and quality of patient care and\noutcomes. In addition, we made public all the source codes, datasets, and model\nweights to facilitate the further development of dialogue models in the medical\nfield. The training data, codes, and weights of this project are available at:\nThe training data, codes, and weights of this project are available at:\nhttps://github.com/Kent0n-Li/ChatDoctor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zihan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dan_R/0/1/0/all/0/1\">Ruilong Dan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">You Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Information Extraction Study: Take In Mind the Tokenization!. (arXiv:2303.15100v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.15100","description":"<p>Current research on the advantages and trade-offs of using characters,\ninstead of tokenized text, as input for deep learning models, has evolved\nsubstantially. New token-free models remove the traditional tokenization step;\nhowever, their efficiency remains unclear. Moreover, the effect of tokenization\nis relatively unexplored in sequence tagging tasks. To this end, we investigate\nthe impact of tokenization when extracting information from documents and\npresent a comparative study and analysis of subword-based and character-based\nmodels. Specifically, we study Information Extraction (IE) from biomedical\ntexts. The main outcome is twofold: tokenization patterns can introduce\ninductive bias that results in state-of-the-art performance, and the\ncharacter-based models produce promising results; thus, transitioning to\ntoken-free IE models is feasible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Theodoropoulos_C/0/1/0/all/0/1\">Christos Theodoropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. (arXiv:2303.17580v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.17580","description":"<p>Solving complicated AI tasks with different domains and modalities is a key\nstep toward advanced artificial intelligence. While there are abundant AI\nmodels available for different domains and modalities, they cannot handle\ncomplicated AI tasks. Considering large language models (LLMs) have exhibited\nexceptional ability in language understanding, generation, interaction, and\nreasoning, we advocate that LLMs could act as a controller to manage existing\nAI models to solve complicated AI tasks and language could be a generic\ninterface to empower this. Based on this philosophy, we present HuggingGPT, a\nframework that leverages LLMs (e.g., ChatGPT) to connect various AI models in\nmachine learning communities (e.g., Hugging Face) to solve AI tasks.\nSpecifically, we use ChatGPT to conduct task planning when receiving a user\nrequest, select models according to their function descriptions available in\nHugging Face, execute each subtask with the selected AI model, and summarize\nthe response according to the execution results. By leveraging the strong\nlanguage capability of ChatGPT and abundant AI models in Hugging Face,\nHuggingGPT is able to cover numerous sophisticated AI tasks in different\nmodalities and domains and achieve impressive results in language, vision,\nspeech, and other challenging tasks, which paves a new way towards advanced\nartificial intelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yongliang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weiming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aligning a medium-size GPT model in English to a small closed domain in Spanish using reinforcement learning. (arXiv:2303.17649v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.17649","description":"<p>In this paper, we propose a methodology to align a medium-sized GPT model,\noriginally trained in English for an open domain, to a small closed domain in\nSpanish. The application for which the model is finely tuned is the question\nanswering task. To achieve this we also needed to train and implement another\nneural network (which we called the reward model) that could score and\ndetermine whether an answer is appropriate for a given question. This component\nserved to improve the decoding and generation of the answers of the system.\nNumerical metrics such as BLEU and perplexity were used to evaluate the model,\nand human judgment was also used to compare the decoding technique with others.\nFinally, the results favored the proposed method, and it was determined that it\nis feasible to use a reward model to align the generation of responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Navarrete_Parra_O/0/1/0/all/0/1\">Oscar R. Navarrete-Parra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uc_Cetina_V/0/1/0/all/0/1\">Victor Uc-Cetina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_Magana_J/0/1/0/all/0/1\">Jorge Reyes-Magana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Referring Image Segmentation with Global-Local Context Features. (arXiv:2303.17811v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2303.17811","description":"<p>Referring image segmentation (RIS) aims to find a segmentation mask given a\nreferring expression grounded to a region of the input image. Collecting\nlabelled datasets for this task, however, is notoriously costly and\nlabor-intensive. To overcome this issue, we propose a simple yet effective\nzero-shot referring image segmentation method by leveraging the pre-trained\ncross-modal knowledge from CLIP. In order to obtain segmentation masks grounded\nto the input text, we propose a mask-guided visual encoder that captures global\nand local contextual information of an input image. By utilizing instance masks\nobtained from off-the-shelf mask proposal techniques, our method is able to\nsegment fine-detailed Istance-level groundings. We also introduce a\nglobal-local text encoder where the global feature captures complex\nsentence-level semantics of the entire input expression while the local feature\nfocuses on the target noun phrase extracted by a dependency parser. In our\nexperiments, the proposed method outperforms several zero-shot baselines of the\ntask and even the weakly supervised referring expression segmentation method\nwith substantial margins. Our code is available at\nhttps://github.com/Seonghoon-Yu/Zero-shot-RIS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Seonghoon Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_P/0/1/0/all/0/1\">Paul Hongsuck Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_J/0/1/0/all/0/1\">Jeany Son</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can AI Chatbots Pass the Fundamentals of Engineering (FE) and Principles and Practice of Engineering (PE) Structural Exams?. (arXiv:2303.18149v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.18149","description":"<p>The engineering community has recently witnessed the emergence of chatbot\ntechnology with the release of OpenAI ChatGPT-4 and Google Bard. While these\nchatbots have been reported to perform well and even pass various standardized\ntests, including medical and law exams, this forum paper explores whether these\nchatbots can also pass the Fundamentals of Engineering (FE) and Principles and\nPractice of Engineering (PE) exams. A diverse range of civil and environmental\nengineering questions and scenarios are used to evaluate the chatbots'\nperformance, as commonly present in the FE and PE exams. The chatbots'\nresponses were analyzed based on their relevance, accuracy, and clarity and\nthen compared against the recommendations of the National Council of Examiners\nfor Engineering and Surveying (NCEES). Our report shows that ChatGPT-4 and\nBard, respectively scored 70.9% and 39.2% in the FE exam and 46.2% and 41% in\nthe PE exam. It is evident that the current version of ChatGPT-4 could\npotentially pass the FE exam. While future editions are much more likely to\npass both exams, this study also highlights the potential of using chatbots as\nteaching assistants and guiding engineers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naser_M/0/1/0/all/0/1\">M.Z. Naser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_B/0/1/0/all/0/1\">Brandon Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogle_J/0/1/0/all/0/1\">Jennier Ogle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kodur_V/0/1/0/all/0/1\">Venkatesh Kodur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawileh_R/0/1/0/all/0/1\">Rami Hawileh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdalla_J/0/1/0/all/0/1\">Jamal Abdalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thai_H/0/1/0/all/0/1\">Huu-Tai Thai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-04-03T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}