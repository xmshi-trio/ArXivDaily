{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-10-27T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Exploring Document-Level Literary Machine Translation with Parallel Paragraphs from World Literature. (arXiv:2210.14250v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14250","description":"<p>Literary translation is a culturally significant task, but it is bottlenecked\nby the small number of qualified literary translators relative to the many\nuntranslated works published around the world. Machine translation (MT) holds\npotential to complement the work of human translators by improving both\ntraining procedures and their overall efficiency. Literary translation is less\nconstrained than more traditional MT settings since translators must balance\nmeaning equivalence, readability, and critical interpretability in the target\nlanguage. This property, along with the complex discourse-level context present\nin literary texts, also makes literary MT more challenging to computationally\nmodel and evaluate. To explore this task, we collect a dataset (Par3) of\nnon-English language novels in the public domain, each aligned at the paragraph\nlevel to both human and automatic English translations. Using Par3, we discover\nthat expert literary translators prefer reference human translations over\nmachine-translated paragraphs at a rate of 84%, while state-of-the-art\nautomatic MT metrics do not correlate with those preferences. The experts note\nthat MT outputs contain not only mistranslations, but also discourse-disrupting\nerrors and stylistic inconsistencies. To address these problems, we train a\npost-editing model whose output is preferred over normal MT output at a rate of\n69% by experts. We publicly release Par3 at\nhttps://github.com/katherinethai/par3/ to spur future research into literary\nMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thai_K/0/1/0/all/0/1\">Katherine Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpinska_M/0/1/0/all/0/1\">Marzena Karpinska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kalpesh Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_B/0/1/0/all/0/1\">Bill Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inghilleri_M/0/1/0/all/0/1\">Moira Inghilleri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieting_J/0/1/0/all/0/1\">John Wieting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Open Data and Task Augmentation to Automated Behavioral Coding of Psychotherapy Conversations in Low-Resource Scenarios. (arXiv:2210.14254v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14254","description":"<p>In psychotherapy interactions, the quality of a session is assessed by\ncodifying the communicative behaviors of participants during the conversation\nthrough manual observation and annotation. Developing computational approaches\nfor automated behavioral coding can reduce the burden on human coders and\nfacilitate the objective evaluation of the intervention. In the real world,\nhowever, implementing such algorithms is associated with data sparsity\nchallenges since privacy concerns lead to limited available in-domain data. In\nthis paper, we leverage a publicly available conversation-based dataset and\ntransfer knowledge to the low-resource behavioral coding task by performing an\nintermediate language model training via meta-learning. We introduce a task\naugmentation method to produce a large number of \"analogy tasks\" - tasks\nsimilar to the target one - and demonstrate that the proposed framework\npredicts target behaviors more accurately than all the other baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuohao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flemotomos_N/0/1/0/all/0/1\">Nikolaos Flemotomos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imel_Z/0/1/0/all/0/1\">Zac E. Imel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atkins_D/0/1/0/all/0/1\">David C. Atkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revision for Concision: A Constrained Paraphrase Generation Task. (arXiv:2210.14257v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14257","description":"<p>Academic writing should be concise as concise sentences better keep the\nreaders' attention and convey meaning clearly. Writing concisely is\nchallenging, for writers often struggle to revise their drafts. We introduce\nand formulate revising for concision as a natural language processing task at\nthe sentence level. Revising for concision requires algorithms to use only\nnecessary words to rewrite a sentence while preserving its meaning. The revised\nsentence should be evaluated according to its word choice, sentence structure,\nand organization. The revised sentence also needs to fulfil semantic retention\nand syntactic soundness. To aide these efforts, we curate and make available a\nbenchmark parallel dataset that can depict revising for concision. The dataset\ncontains 536 pairs of sentences before and after revising, and all pairs are\ncollected from college writing centres. We also present and evaluate the\napproaches to this problem, which may assist researchers in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_W/0/1/0/all/0/1\">Wenchuan Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1\">Kwan Hui Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Evasion Attacks on Summarization Scoring. (arXiv:2210.14260v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14260","description":"<p>The automatic scoring of summaries is important as it guides the development\nof summarizers. Scoring is also complex, as it involves multiple aspects such\nas fluency, grammar, and even textual entailment with the source text. However,\nsummary scoring has not been considered a machine learning task to study its\naccuracy and robustness. In this study, we place automatic scoring in the\ncontext of regression machine learning tasks and perform evasion attacks to\nexplore its robustness. Attack systems predict a non-summary string from each\ninput, and these non-summary strings achieve competitive scores with good\nsummarizers on the most popular metrics: ROUGE, METEOR, and BERTScore. Attack\nsystems also \"outperform\" state-of-the-art summarization methods on ROUGE-1 and\nROUGE-L, and score the second-highest on METEOR. Furthermore, a BERTScore\nbackdoor is observed: a simple trigger can score higher than any automatic\nsummarization method. The evasion attacks in this work indicate the low\nrobustness of current scoring systems at the system level. We hope that our\nhighlighting of these proposed attacks will facilitate the development of\nsummary scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_W/0/1/0/all/0/1\">Wenchuan Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1\">Kwan Hui Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Similarity between Units of Natural Language: The Transition from Coarse to Fine Estimation. (arXiv:2210.14275v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14275","description":"<p>Capturing the similarities between human language units is crucial for\nexplaining how humans associate different objects, and therefore its\ncomputation has received extensive attention, research, and applications. With\nthe ever-increasing amount of information around us, calculating similarity\nbecomes increasingly complex, especially in many cases, such as legal or\nmedical affairs, measuring similarity requires extra care and precision, as\nsmall acts within a language unit can have significant real-world effects. My\nresearch goal in this thesis is to develop regression models that account for\nsimilarities between language units in a more refined way.\n</p>\n<p>Computation of similarity has come a long way, but approaches to debugging\nthe measures are often based on continually fitting human judgment values. To\nthis end, my goal is to develop an algorithm that precisely catches loopholes\nin a similarity calculation. Furthermore, most methods have vague definitions\nof the similarities they compute and are often difficult to interpret. The\nproposed framework addresses both shortcomings. It constantly improves the\nmodel through catching different loopholes. In addition, every refinement of\nthe model provides a reasonable explanation. The regression model introduced in\nthis thesis is called progressively refined similarity computation, which\ncombines attack testing with adversarial training. The similarity regression\nmodel of this thesis achieves state-of-the-art performance in handling edge\ncases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_W/0/1/0/all/0/1\">Wenchuan Mu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenStance: Real-world Zero-shot Stance Detection. (arXiv:2210.14299v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14299","description":"<p>Prior studies of zero-shot stance detection identify the attitude of texts\ntowards unseen topics occurring in the same document corpus. Such task\nformulation has three limitations: (i) Single domain/dataset. A system is\noptimized on a particular dataset from a single domain; therefore, the\nresulting system cannot work well on other datasets; (ii) the model is\nevaluated on a limited number of unseen topics; (iii) it is assumed that part\nof the topics has rich annotations, which might be impossible in real-world\napplications. These drawbacks will lead to an impractical stance detection\nsystem that fails to generalize to open domains and open-form topics. This work\ndefines OpenStance: open-domain zero-shot stance detection, aiming to handle\nstance detection in an open world with neither domain constraints nor\ntopic-specific annotations. The key challenge of OpenStance lies in the\nopen-domain generalization: learning a system with fully unspecific supervision\nbut capable of generalizing to any dataset. To solve OpenStance, we propose to\ncombine indirect supervision, from textual entailment datasets, and weak\nsupervision, from data generated automatically by pre-trained Language Models.\nOur single system, without any topic-specific supervision, outperforms the\nsupervised method on three popular datasets. To our knowledge, this is the\nfirst work that studies stance detection under the open-domain zero-shot\nsetting. All data and code are publicly released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hanzi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vucetic_S/0/1/0/all/0/1\">Slobodan Vucetic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenpeng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Better Intent Representations for Financial Open Intent Classification. (arXiv:2210.14304v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14304","description":"<p>With the recent surge of NLP technologies in the financial domain, banks and\nother financial entities have adopted virtual agents (VA) to assist customers.\nA challenging problem for VAs in this domain is determining a user's reason or\nintent for contacting the VA, especially when the intent was unseen or open\nduring the VA's training. One method for handling open intents is adaptive\ndecision boundary (ADB) post-processing, which learns tight decision boundaries\nfrom intent representations to separate known and open intents. We propose\nincorporating two methods for supervised pre-training of intent\nrepresentations: prefix-tuning and fine-tuning just the last layer of a large\nlanguage model (LLM). With this proposal, our accuracy is 1.63% - 2.07% higher\nthan the prior state-of-the-art ADB method for open intent classification on\nthe banking77 benchmark amongst others. Notably, we only supplement the\noriginal ADB model with 0.1% additional trainable parameters. Ablation studies\nalso determine that our method yields better results than full fine-tuning the\nentire model. We hypothesize that our findings could stimulate a new optimal\nmethod of downstream tuning that combines parameter efficient tuning modules\nwith fine-tuning a subset of the base model's layers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aitken_W/0/1/0/all/0/1\">Will Aitken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Stephen W. Thomas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Robust Incremental Learning over Many Multilingual Steps. (arXiv:2210.14307v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14307","description":"<p>Recent work in incremental learning has introduced diverse approaches to\ntackle catastrophic forgetting from data augmentation to optimized training\nregimes. However, most of them focus on very few training steps. We propose a\nmethod for robust incremental learning over dozens of fine-tuning steps using\ndata from a variety of languages. We show that a combination of\ndata-augmentation and an optimized training regime allows us to continue\nimproving the model even for as many as fifty training steps. Crucially, our\naugmentation strategy does not require retaining access to previous training\ndata and is suitable in scenarios with privacy constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Praharaj_K/0/1/0/all/0/1\">Karan Praharaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matveeva_I/0/1/0/all/0/1\">Irina Matveeva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Analysis of Syntactic Agreement Neurons in Multilingual Language Models. (arXiv:2210.14328v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14328","description":"<p>Structural probing work has found evidence for latent syntactic information\nin pre-trained language models. However, much of this analysis has focused on\nmonolingual models, and analyses of multilingual models have employed\ncorrelational methods that are confounded by the choice of probing tasks. In\nthis study, we causally probe multilingual language models (XGLM and\nmultilingual BERT) as well as monolingual BERT-based models across various\nlanguages; we do this by performing counterfactual perturbations on neuron\nactivations and observing the effect on models' subject-verb agreement\nprobabilities. We observe where in the model and to what extent syntactic\nagreement is encoded in each language. We find significant neuron overlap\nacross languages in autoregressive multilingual language models, but not masked\nlanguage models. We also find two distinct layer-wise effect patterns and two\ndistinct sets of neurons used for syntactic agreement, depending on whether the\nsubject and verb are separated by other tokens. Finally, we find that\nbehavioral analyses of language models are likely underestimating how sensitive\nmasked language models are to syntactic information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mueller_A/0/1/0/all/0/1\">Aaron Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yu Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe. (arXiv:2210.14348v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14348","description":"<p>Privacy concerns have attracted increasing attention in data-driven products\nand services. Existing legislation forbids arbitrary processing of personal\ndata collected from individuals. Generating synthetic versions of such data\nwith a formal privacy guarantee such as differential privacy (DP) is considered\nto be a solution to address privacy concerns. In this direction, we show a\nsimple, practical, and effective recipe in the text domain: simply fine-tuning\na generative language model with DP allows us to generate useful synthetic text\nwhile mitigating privacy concerns. Through extensive empirical analyses, we\ndemonstrate that our method produces synthetic data that is competitive in\nterms of utility with its non-private counterpart and meanwhile provides strong\nprotection against potential privacy leakages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1\">Huseyin A. Inan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuechen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1\">Girish Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAnallen_J/0/1/0/all/0/1\">Julia McAnallen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levitan_D/0/1/0/all/0/1\">David Levitan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_R/0/1/0/all/0/1\">Robert Sim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering. (arXiv:2210.14353v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14353","description":"<p>We introduce RoMQA, the first benchmark for robust, multi-evidence,\nmulti-answer question answering (QA). RoMQA contains clusters of questions that\nare derived from related constraints mined from the Wikidata knowledge graph.\nRoMQA evaluates robustness of QA models to varying constraints by measuring\nworst-case performance within each question cluster. Compared to prior QA\ndatasets, RoMQA has more human-written questions that require reasoning over\nmore evidence text and have, on average, many more correct answers. In\naddition, human annotators rate RoMQA questions as more natural or likely to be\nasked by people. We evaluate state-of-the-art large language models in\nzero-shot, few-shot, and fine-tuning settings, and find that RoMQA is\nchallenging: zero-shot and few-shot models perform similarly to naive\nbaselines, while supervised retrieval methods perform well below gold evidence\nupper bounds. Moreover, existing models are not robust to variations in\nquestion constraints, but can be made more robust by tuning on clusters of\nrelated questions. Our results show that RoMQA is a challenging benchmark for\nlarge language models, and provides a quantifiable test to build more robust QA\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_V/0/1/0/all/0/1\">Victor Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weijia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Product Safety in E-Commerce with NLP. (arXiv:2210.14363v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14363","description":"<p>Ensuring safety of the products offered to the customers is of paramount\nimportance to any e- commerce platform. Despite stringent quality and safety\nchecking of products listed on these platforms, occasionally customers might\nreceive a product that can pose a safety issue arising out of its use. In this\npaper, we present an innovative mechanism of how a large scale multinational\ne-commerce platform, Zalando, uses Natural Language Processing techniques to\nassist timely investigation of the potentially unsafe products mined directly\nfrom customer written claims in unstructured plain text. We systematically\ndescribe the types of safety issues that concern Zalando customers. We\ndemonstrate how we map this core business problem into a supervised text\nclassification problem with highly imbalanced, noisy, multilingual data in a\nAI-in-the-loop setup with a focus on Key Performance Indicator (KPI) driven\nevaluation. Finally, we present detailed ablation studies to show a\ncomprehensive comparison between different classification techniques. We\nconclude the work with how this NLP model was deployed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Halder_K/0/1/0/all/0/1\">Kishaloy Halder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krapac_J/0/1/0/all/0/1\">Josip Krapac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goryunov_D/0/1/0/all/0/1\">Dmitry Goryunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brew_A/0/1/0/all/0/1\">Anthony Brew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyra_M/0/1/0/all/0/1\">Matti Lyra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dizdari_A/0/1/0/all/0/1\">Alsida Dizdari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillett_W/0/1/0/all/0/1\">William Gillett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renahy_A/0/1/0/all/0/1\">Adrien Renahy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Sinan Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Monitor Model and its Misconceptions: A Clarification. (arXiv:2210.14367v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14367","description":"<p>Horizontal (automatic) and vertical (control) processes have long been\nreported in human translation production (e.g., Konig 1987, Lorscher 1991,\nJaaskelainen 1996, de Groot 1997, Tirkkonen-Condit 2005, Macizo and Bajo 2006).\nThe Monitor Model (Schaeffer and Carl 2013, 2015) integrates horizontal and\nvertical processes, assuming priming mechanisms underlie horizontal/automatic\nprocesses, while vertical/monitoring processes implement consciously accessible\ncontrol mechanisms. Carl (2021a) argues that priming processes in translation\nare part of perception-action loops, interpretable in an embodied/enactivist\nframework. Carl (2022) develops a post-humanist view on translator-technology\ninteraction facilitated by priming mechanisms which enable representationally\nunmediated translator-environment coupling. I substantiate these claims,\narguing that translation priming results in basic, non-representational\ncontent. I update the Monitor Model with additional evidence and address an\naccumulation of misconceptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carl_M/0/1/0/all/0/1\">Michael Carl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bilingual Lexicon Induction for Low-Resource Languages using Graph Matching via Optimal Transport. (arXiv:2210.14378v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14378","description":"<p>Bilingual lexicons form a critical component of various natural language\nprocessing applications, including unsupervised and semisupervised machine\ntranslation and crosslingual information retrieval. We improve bilingual\nlexicon induction performance across 40 language pairs with a graph-matching\nmethod based on optimal transport. The method is especially strong with low\namounts of supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marchisio_K/0/1/0/all/0/1\">Kelly Marchisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saad_Eldin_A/0/1/0/all/0/1\">Ali Saad-Eldin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duh_K/0/1/0/all/0/1\">Kevin Duh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1\">Carey Priebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deploying a Retrieval based Response Model for Task Oriented Dialogues. (arXiv:2210.14379v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14379","description":"<p>Task-oriented dialogue systems in industry settings need to have high\nconversational capability, be easily adaptable to changing situations and\nconform to business constraints. This paper describes a 3-step procedure to\ndevelop a conversational model that satisfies these criteria and can\nefficiently scale to rank a large set of response candidates. First, we provide\na simple algorithm to semi-automatically create a high-coverage template set\nfrom historic conversations without any annotation. Second, we propose a neural\narchitecture that encodes the dialogue context and applicable business\nconstraints as profile features for ranking the next turn. Third, we describe a\ntwo-stage learning strategy with self-supervised training, followed by\nsupervised fine-tuning on limited data collected through a human-in-the-loop\nplatform. Finally, we describe offline experiments and present results of\ndeploying our model with human-in-the-loop to converse with live customers\nonline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poddar_L/0/1/0/all/0/1\">Lahari Poddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szarvas_G/0/1/0/all/0/1\">Gy&#xf6;rgy Szarvas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balazs_J/0/1/0/all/0/1\">Jorge Balazs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danchenko_P/0/1/0/all/0/1\">Pavel Danchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernst_P/0/1/0/all/0/1\">Patrick Ernst</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Sentiment Analysis for Code-Switched Text Data. (arXiv:2210.14380v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14380","description":"<p>Multilingual transformer language models have recently attracted much\nattention from researchers and are used in cross-lingual transfer learning for\nmany NLP tasks such as text classification and named entity recognition.\nHowever, similar methods for transfer learning from monolingual text to\ncode-switched text have not been extensively explored mainly due to the\nfollowing challenges: (1) Code-switched corpus, unlike monolingual corpus,\nconsists of more than one language and existing methods can't be applied\nefficiently, (2) Code-switched corpus is usually made of resource-rich and\nlow-resource languages and upon using multilingual pre-trained language models,\nthe final model might bias towards resource-rich language. In this paper, we\nfocus on code-switched sentiment analysis where we have a labelled\nresource-rich language dataset and unlabelled code-switched data. We propose a\nframework that takes the distinction between resource-rich and low-resource\nlanguage into account. Instead of training on the entire code-switched corpus\nat once, we create buckets based on the fraction of words in the resource-rich\nlanguage and progressively train from resource-rich language dominated samples\nto low-resource language dominated samples. Extensive experiments across\nmultiple language pairs demonstrate that progressive training helps\nlow-resource language dominated samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_S/0/1/0/all/0/1\">Sudhanshu Ranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mekala_D/0/1/0/all/0/1\">Dheeraj Mekala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards standardizing Korean Grammatical Error Correction: Datasets and Annotation. (arXiv:2210.14389v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14389","description":"<p>Research on Korean grammatical error correction (GEC) is limited compared to\nother major languages such as English and Chinese. We attribute this\nproblematic circumstance to the lack of a carefully designed evaluation\nbenchmark for Korean. Thus, in this work, we first collect three datasets from\ndifferent sources (Kor-Lang8, Kor-Native, and Kor-Learner) to cover a wide\nrange of error types and annotate them using our newly proposed tool called\nKorean Automatic Grammatical error Annotation System (KAGAS). KAGAS is a\ncarefully designed edit alignment &amp; classification tool that considers the\nnature of Korean on generating an alignment between a source sentence and a\ntarget sentence, and identifies error types on each aligned edit. We also\npresent baseline models fine-tuned over our datasets. We show that the model\ntrained with our datasets significantly outperforms the public statistical GEC\nsystem (Hanspell) on a wider range of error types, demonstrating the diversity\nand usefulness of the datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Soyoung Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungjoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyuwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Junhee Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kihyo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyu Tae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IMU2CLIP: Multimodal Contrastive Learning for IMU Motion Sensors from Egocentric Videos and Text. (arXiv:2210.14395v1 [cs.CV])","link":"http://arxiv.org/abs/2210.14395","description":"<p>We present IMU2CLIP, a novel pre-training approach to align Inertial\nMeasurement Unit (IMU) motion sensor recordings with video and text, by\nprojecting them into the joint representation space of Contrastive\nLanguage-Image Pre-training (CLIP). The proposed approach allows IMU2CLIP to\ntranslate human motions (as measured by IMU sensors) into their corresponding\ntextual descriptions and videos -- while preserving the transitivity across\nthese modalities.\n</p>\n<p>We explore several new IMU-based applications that IMU2CLIP enables, such as\nmotion-based media retrieval and natural language reasoning tasks with motion\ndata. In addition, we show that IMU2CLIP can significantly improve the\ndownstream performance when fine-tuned for each application (e.g. activity\nrecognition), demonstrating the universal usage of IMU2CLIP as a new\npre-trained resource. Our code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Seungwhan Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhaojiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dirafzoon_A/0/1/0/all/0/1\">Alireza Dirafzoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saraf_A/0/1/0/all/0/1\">Aparajita Saraf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bearman_A/0/1/0/all/0/1\">Amy Bearman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damavandi_B/0/1/0/all/0/1\">Babak Damavandi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RedPen: Region- and Reason-Annotated Dataset of Unnatural Speech. (arXiv:2210.14406v1 [eess.AS])","link":"http://arxiv.org/abs/2210.14406","description":"<p>Even with recent advances in speech synthesis models, the evaluation of such\nmodels is based purely on human judgement as a single naturalness score, such\nas the Mean Opinion Score (MOS). The score-based metric does not give any\nfurther information about which parts of speech are unnatural or why human\njudges believe they are unnatural. We present a novel speech dataset, RedPen,\nwith human annotations on unnatural speech regions and their corresponding\nreasons. RedPen consists of 180 synthesized speeches with unnatural regions\nannotated by crowd workers; These regions are then reasoned and categorized by\nerror types, such as voice trembling and background noise. We find that our\ndataset shows a better explanation for unnatural speech regions than the\nmodel-driven unnaturalness prediction. Our analysis also shows that each model\nincludes different types of error types. Summing up, our dataset successfully\nshows the possibility that various error regions and types lie under the single\nnaturalness score. We believe that our dataset will shed light on the\nevaluation and development of more interpretable speech models in the future.\nOur dataset will be publicly available upon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Park_K/0/1/0/all/0/1\">Kyumin Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_K/0/1/0/all/0/1\">Keon Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling the Graphotactics of Low-Resource Languages Using Sequential GANs. (arXiv:2210.14409v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14409","description":"<p>Generative Adversarial Networks (GANs) have been shown to aid in the creation\nof artificial data in situations where large amounts of real data are difficult\nto come by. This issue is especially salient in the computational linguistics\nspace, where researchers are often tasked with modeling the complex morphologic\nand grammatical processes of low-resource languages. This paper will discuss\nthe implementation and testing of a GAN that attempts to model and reproduce\nthe graphotactics of a language using only 100 example strings. These\nartificial, yet graphotactically compliant, strings are meant to aid in\nmodeling the morphological inflection of low-resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wasserman_I/0/1/0/all/0/1\">Isaac Wasserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discourse-Aware Emotion Cause Extraction in Conversations. (arXiv:2210.14419v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14419","description":"<p>Emotion Cause Extraction in Conversations (ECEC) aims to extract the\nutterances which contain the emotional cause in conversations. Most prior\nresearch focuses on modelling conversational contexts with sequential encoding,\nignoring the informative interactions between utterances and\nconversational-specific features for ECEC. In this paper, we investigate the\nimportance of discourse structures in handling utterance interactions and\nconversationspecific features for ECEC. To this end, we propose a\ndiscourse-aware model (DAM) for this task. Concretely, we jointly model ECEC\nwith discourse parsing using a multi-task learning (MTL) framework and\nexplicitly encode discourse structures via gated graph neural network (gated\nGNN), integrating rich utterance interaction information to our model. In\naddition, we use gated GNN to further enhance our ECEC model with\nconversation-specific features. Results on the benchmark corpus show that DAM\noutperform the state-of-theart (SOTA) systems in the literature. This suggests\nthat the discourse structure may contain a potential link between emotional\nutterances and their corresponding cause expressions. It also verifies the\neffectiveness of conversationalspecific features. The codes of this paper will\nbe available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Dexin Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1\">Guohong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chen Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geographic Citation Gaps in NLP Research. (arXiv:2210.14424v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14424","description":"<p>In a fair world, people have equitable opportunities to education, to conduct\nscientific research, to publish, and to get credit for their work, regardless\nof where they live. However, it is common knowledge among researchers that a\nvast number of papers accepted at top NLP venues come from a handful of western\ncountries and (lately) China; whereas, very few papers from Africa and South\nAmerica get published. Similar disparities are also believed to exist for paper\ncitation counts. In the spirit of \"what we do not measure, we cannot improve\",\nthis work asks a series of questions on the relationship between geographical\nlocation and publication success (acceptance in top NLP venues and citation\nimpact). We first created a dataset of 70,000 papers from the ACL Anthology,\nextracted their meta-information, and generated their citation network. We then\nshow that not only are there substantial geographical disparities in paper\nacceptance and citation but also that these disparities persist even when\ncontrolling for a number of variables such as venue of publication and\nsub-field of NLP. Further, despite some steps taken by the NLP community to\nimprove geographical diversity, we show that the disparity in publication\nmetrics across locations is still on an increasing trend since the early 2000s.\nWe release our code and dataset here:\nhttps://github.com/iamjanvijay/acl-cite-net\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rungta_M/0/1/0/all/0/1\">Mukund Rungta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Janvijay Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReSel: N-ary Relation Extraction from Scientific Text and Tables by Learning to Retrieve and Select. (arXiv:2210.14427v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14427","description":"<p>We study the problem of extracting N-ary relation tuples from scientific\narticles. This task is challenging because the target knowledge tuples can\nreside in multiple parts and modalities of the document. Our proposed method\nReSel decomposes this task into a two-stage procedure that first retrieves the\nmost relevant paragraph/table and then selects the target entity from the\nretrieved component. For the high-level retrieval stage, ReSel designs a simple\nand effective feature set, which captures multi-level lexical and semantic\nsimilarities between the query and components. For the low-level selection\nstage, ReSel designs a cross-modal entity correlation graph along with a\nmulti-view architecture, which models both semantic and document-structural\nrelations between entities. Our experiments on three scientific information\nextraction datasets show that ReSel outperforms state-of-the-art baselines\nsignificantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yuchen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jerry Junyang Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_Y/0/1/0/all/0/1\">Yingjun Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Le Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Residual Learning of Neural Text Generation with $n$-gram Language Model. (arXiv:2210.14431v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14431","description":"<p>$N$-gram language models (LM) have been largely superseded by neural LMs as\nthe latter exhibits better performance. However, we find that $n$-gram models\ncan achieve satisfactory performance on a large proportion of testing cases,\nindicating they have already captured abundant knowledge of the language with\nrelatively low computational cost. With this observation, we propose to learn a\nneural LM that fits the residual between an $n$-gram LM and the real-data\ndistribution. The combination of $n$-gram and neural LMs not only allows the\nneural part to focus on the deeper understanding of language but also provides\na flexible way to customize an LM by switching the underlying $n$-gram model\nwithout changing the neural model. Experimental results on three typical\nlanguage tasks (i.e., language modeling, machine translation, and\nsummarization) demonstrate that our approach attains additional performance\ngains over popular standalone neural models consistently. We also show that our\napproach allows for effective domain adaptation by simply switching to a\ndomain-specific $n$-gram model, without any extra training. Our code is\nreleased at https://github.com/ghrua/NgramRes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huayang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1\">Taro Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smart Speech Segmentation using Acousto-Linguistic Features with look-ahead. (arXiv:2210.14446v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14446","description":"<p>Segmentation for continuous Automatic Speech Recognition (ASR) has\ntraditionally used silence timeouts or voice activity detectors (VADs), which\nare both limited to acoustic features. This segmentation is often overly\naggressive, given that people naturally pause to think as they speak.\nConsequently, segmentation happens mid-sentence, hindering both punctuation and\ndownstream tasks like machine translation for which high-quality segmentation\nis critical. Model-based segmentation methods that leverage acoustic features\nare powerful, but without an understanding of the language itself, these\napproaches are limited. We present a hybrid approach that leverages both\nacoustic and language information to improve segmentation. Furthermore, we show\nthat including one word as a look-ahead boosts segmentation quality. On\naverage, our models improve segmentation-F0.5 score by 9.8% over baseline. We\nshow that this approach works for multiple languages. For the downstream task\nof machine translation, it improves the translation BLEU score by an average of\n1.05 points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Behre_P/0/1/0/all/0/1\">Piyush Behre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parihar_N/0/1/0/all/0/1\">Naveen Parihar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Sharman Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Amy Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_E/0/1/0/all/0/1\">Eva Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Geoffrey Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuangyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalil_H/0/1/0/all/0/1\">Hosam Khalil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basoglu_C/0/1/0/all/0/1\">Chris Basoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_S/0/1/0/all/0/1\">Sayan Pathak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Question-Interlocutor Scope Realized Graph Modeling over Key Utterances for Dialogue Reading Comprehension. (arXiv:2210.14456v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14456","description":"<p>In this work, we focus on dialogue reading comprehension (DRC), a task\nextracting answer spans for questions from dialogues. Dialogue context modeling\nin DRC is tricky due to complex speaker information and noisy dialogue context.\nTo solve the two problems, previous research proposes two self-supervised tasks\nrespectively: guessing who a randomly masked speaker is according to the\ndialogue and predicting which utterance in the dialogue contains the answer.\nAlthough these tasks are effective, there are still urging problems: (1)\nrandomly masking speakers regardless of the question cannot map the speaker\nmentioned in the question to the corresponding speaker in the dialogue, and\nignores the speaker-centric nature of utterances. This leads to wrong answer\nextraction from utterances in unrelated interlocutors' scopes; (2) the single\nutterance prediction, preferring utterances similar to the question, is limited\nin finding answer-contained utterances not similar to the question. To\nalleviate these problems, we first propose a new key utterances extracting\nmethod. It performs prediction on the unit formed by several contiguous\nutterances, which can realize more answer-contained utterances. Based on\nutterances in the extracted units, we then propose Question-Interlocutor Scope\nRealized Graph (QuISG) modeling. As a graph constructed on the text of\nutterances, QuISG additionally involves the question and question-mentioning\nspeaker names as nodes. To realize interlocutor scopes, speakers in the\ndialogue are connected with the words in their corresponding utterances.\nExperiments on the benchmarks show that our method can achieve better and\ncompetitive results against previous works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_P/0/1/0/all/0/1\">Peng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bi-Link: Bridging Inductive Link Predictions from Text via Contrastive Learning of Transformers and Prompts. (arXiv:2210.14463v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14463","description":"<p>Inductive knowledge graph completion requires models to comprehend the\nunderlying semantics and logic patterns of relations. With the advance of\npretrained language models, recent research have designed transformers for link\nprediction tasks. However, empirical studies show that linearizing triples\naffects the learning of relational patterns, such as inversion and symmetry. In\nthis paper, we propose Bi-Link, a contrastive learning framework with\nprobabilistic syntax prompts for link predictions. Using grammatical knowledge\nof BERT, we efficiently search for relational prompts according to learnt\nsyntactical patterns that generalize to large knowledge graphs. To better\nexpress symmetric relations, we design a symmetric link prediction model,\nestablishing bidirectional linking between forward prediction and backward\nprediction. This bidirectional linking accommodates flexible self-ensemble\nstrategies at test time. In our experiments, Bi-Link outperforms recent\nbaselines on link prediction datasets (WN18RR, FB15K-237, and Wikidata5M).\nFurthermore, we construct Zeshel-Ind as an in-domain inductive entity linking\nthe environment to evaluate Bi-Link. The experimental results demonstrate that\nour method yields robust representations which can generalize under domain\nshift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Bohua Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shihao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mobarakol Islam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eeny, meeny, miny, moe. How to choose data for morphological inflection. (arXiv:2210.14465v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14465","description":"<p>Data scarcity is a widespread problem in numerous natural language processing\n(NLP) tasks for low-resource languages. Within morphology, the labour-intensive\nwork of tagging/glossing data is a serious bottleneck for both NLP and language\ndocumentation. Active learning (AL) aims to reduce the cost of data annotation\nby selecting data that is most informative for improving the model. In this\npaper, we explore four sampling strategies for the task of morphological\ninflection using a Transformer model: a pair of oracle experiments where data\nis chosen based on whether the model already can or cannot inflect the test\nforms correctly, as well as strategies based on high/low model confidence,\nentropy, as well as random selection. We investigate the robustness of each\nstrategy across 30 typologically diverse languages. We also perform a more\nin-depth case study of Nat\\\"ugu. Our results show a clear benefit to selecting\ndata based on model confidence and entropy. Unsurprisingly, the oracle\nexperiment, where only incorrectly handled forms are chosen for further\ntraining, which is presented as a proxy for linguist/language consultant\nfeedback, shows the most improvement. This is followed closely by choosing\nlow-confidence and high-entropy predictions. We also show that despite the\nconventional wisdom of larger data sets yielding better accuracy, introducing\nmore instances of high-confidence or low-entropy forms, or forms that the model\ncan already inflect correctly, can reduce model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muradoglu_S/0/1/0/all/0/1\">Saliha Muradoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hulden_M/0/1/0/all/0/1\">Mans Hulden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inducer-tuning: Connecting Prefix-tuning and Adapter-tuning. (arXiv:2210.14469v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14469","description":"<p>Prefix-tuning, or more generally continuous prompt tuning, has become an\nessential paradigm of parameter-efficient transfer learning. Using a large\npre-trained language model (PLM), prefix-tuning can obtain strong performance\nby training only a small portion of parameters. In this paper, we propose to\nunderstand and further develop prefix-tuning through the kernel lens.\nSpecifically, we make an analogy between \\textit{prefixes} and \\textit{inducing\nvariables} in kernel methods and hypothesize that \\textit{prefixes} serving as\n\\textit{inducing variables} would improve their overall mechanism. From the\nkernel estimator perspective, we suggest a new variant of prefix-tuning --\n\\textit{inducer-tuning}, which shares the exact mechanism as prefix-tuning\nwhile leveraging the residual form found in adapter-tuning. This mitigates the\ninitialization issue in prefix-tuning. Through comprehensive empirical\nexperiments on natural language understanding and generation tasks, we\ndemonstrate that inducer-tuning can close the performance gap between\nprefix-tuning and fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1\">Devamanyu Hazarika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namazifar_M/0/1/0/all/0/1\">Mahdi Namazifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sinhala Sentence Embedding: A Two-Tiered Structure for Low-Resource Languages. (arXiv:2210.14472v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14472","description":"<p>In the process of numerically modeling natural languages, developing language\nembeddings is a vital step. However, it is challenging to develop functional\nembeddings for resource-poor languages such as Sinhala, for which sufficiently\nlarge corpora, effective language parsers, and any other required resources are\ndifficult to find. In such conditions, the exploitation of existing models to\ncome up with an efficacious embedding methodology to numerically represent text\ncould be quite fruitful. This paper explores the effectivity of several\none-tiered and two-tiered embedding architectures in representing Sinhala text\nin the sentiment analysis domain. With our findings, the two-tiered embedding\narchitecture where the lower-tier consists of a word embedding and the\nupper-tier consists of a sentence embedding has been proven to perform better\nthan one-tier word embeddings, by achieving a maximum F1 score of 88.04% in\ncontrast to the 83.76% achieved by word embedding models. Furthermore,\nembeddings in the hyperbolic space are also developed and compared with\nEuclidean embeddings in terms of performance. A sentiment data set consisting\nof Facebook posts and associated reactions have been used for this research. To\neffectively compare the performance of different embedding systems, the same\ndeep neural network structure has been trained on sentiment data with each of\nthe embedding systems used to encode the text associated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weeraprameshwara_G/0/1/0/all/0/1\">Gihan Weeraprameshwara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayawickrama_V/0/1/0/all/0/1\">Vihanga Jayawickrama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nisansa de Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijeratne_Y/0/1/0/all/0/1\">Yudhanjaya Wijeratne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Language Models for Code Syntax Understanding. (arXiv:2210.14473v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14473","description":"<p>Pre-trained language models have demonstrated impressive performance in both\nnatural language processing and program understanding, which represent the\ninput as a token sequence without explicitly modeling its structure. Some prior\nworks show that pre-trained language models can capture the syntactic rules of\nnatural languages without finetuning on syntax understanding tasks. However,\nthere is limited understanding of how well pre-trained models understand the\ncode structure so far. In this work, we perform the first thorough benchmarking\nof the state-of-the-art pre-trained models for identifying the syntactic\nstructures of programs. Specifically, we introduce CodeSyntax, a large-scale\ndataset of programs annotated with the syntactic relationships in their\ncorresponding abstract syntax trees. Our key observation is that existing\nlanguage models pretrained on code still lack the understanding of code syntax.\nIn fact, these pre-trained programming language models fail to match the\nperformance of simple baselines based on positional offsets and keywords. We\nalso present a natural language benchmark to highlight the differences between\nnatural languages and programming languages in terms of syntactic structure\nunderstanding. Our findings point out key limitations of existing pre-training\nmethods for programming languages, and suggest the importance of modeling code\nsyntactic structures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Da Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_K/0/1/0/all/0/1\">Koushik Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Affirmative Interpretations from Negation Improves Natural Language Understanding. (arXiv:2210.14486v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14486","description":"<p>Negation poses a challenge in many natural language understanding tasks.\nInspired by the fact that understanding a negated statement often requires\nhumans to infer affirmative interpretations, in this paper we show that doing\nso benefits models for three natural language understanding tasks. We present\nan automated procedure to collect pairs of sentences with negation and their\naffirmative interpretations, resulting in over 150,000 pairs. Experimental\nresults show that leveraging these pairs helps (a) T5 generate affirmative\ninterpretations from negations in a previous benchmark, and (b) a RoBERTa-based\nclassifier solve the task of natural language inference. We also leverage our\npairs to build a plug-and-play neural generator that given a negated statement\ngenerates an affirmative interpretation. Then, we incorporate the pretrained\ngenerator into a RoBERTa-based classifier for sentiment analysis and show that\ndoing so improves the results. Crucially, our proposal does not require any\nmanual effort.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Md Mosharaf Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanco_E/0/1/0/all/0/1\">Eduardo Blanco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CS1QA: A Dataset for Assisting Code-based Question Answering in an Introductory Programming Course. (arXiv:2210.14494v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14494","description":"<p>We introduce CS1QA, a dataset for code-based question answering in the\nprogramming education domain. CS1QA consists of 9,237 question-answer pairs\ngathered from chat logs in an introductory programming class using Python, and\n17,698 unannotated chat data with code. Each question is accompanied with the\nstudent's code, and the portion of the code relevant to answering the question.\nWe carefully design the annotation process to construct CS1QA, and analyze the\ncollected dataset in detail. The tasks for CS1QA are to predict the question\ntype, the relevant code snippet given the question and the code and retrieving\nan answer from the annotated corpus. Results for the experiments on several\nbaseline models are reported and thoroughly analyzed. The tasks for CS1QA\nchallenge models to understand both the code and natural language. This unique\ndataset can be used as a benchmark for source code comprehension and question\nanswering in the educational setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Changyoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seonwoo_Y/0/1/0/all/0/1\">Yeon Seonwoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SentBS: Sentence-level Beam Search for Controllable Summarization. (arXiv:2210.14502v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14502","description":"<p>A wide range of control perspectives have been explored in controllable text\ngeneration. Structure-controlled summarization is recently proposed as a useful\nand interesting research direction. However, current structure-controlling\nmethods have limited effectiveness in enforcing the desired structure. To\naddress this limitation, we propose a sentence-level beam search generation\nmethod (SentBS), where evaluation is conducted throughout the generation\nprocess to select suitable sentences for subsequent generations. We experiment\nwith different combinations of decoding methods to be used as subcomponents by\nSentBS and evaluate results on the structure-controlled dataset MReD.\nExperiments show that all explored combinations for SentBS can improve the\nagreement between the generated text and the desired structure, with the best\nmethod significantly reducing the structural discrepancies suffered by the\nexisting model, by approximately 68%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chenhui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Speech-to-Speech Translation Through Unlabeled Text. (arXiv:2210.14514v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14514","description":"<p>Direct speech-to-speech translation (S2ST) is among the most challenging\nproblems in the translation paradigm due to the significant scarcity of S2ST\ndata. While effort has been made to increase the data size from unlabeled\nspeech by cascading pretrained speech recognition (ASR), machine translation\n(MT) and text-to-speech (TTS) models; unlabeled text has remained relatively\nunder-utilized to improve S2ST. We propose an effective way to utilize the\nmassive existing unlabeled text from different languages to create a large\namount of S2ST data to improve S2ST performance by applying various acoustic\neffects to the generated synthetic data. Empirically our method outperforms the\nstate of the art in Spanish-English translation by up to 2 BLEU. Significant\ngains by the proposed method are demonstrated in extremely low-resource\nsettings for both Spanish-English and Russian-English translations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1\">Xuan-Phi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popuri_S/0/1/0/all/0/1\">Sravya Popuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulikov_I/0/1/0/all/0/1\">Ilia Kulikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OTSeq2Set: An Optimal Transport Enhanced Sequence-to-Set Model for Extreme Multi-label Text Classification. (arXiv:2210.14523v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14523","description":"<p>Extreme multi-label text classification (XMTC) is the task of finding the\nmost relevant subset labels from an extremely large-scale label collection.\nRecently, some deep learning models have achieved state-of-the-art results in\nXMTC tasks. These models commonly predict scores for all labels by a fully\nconnected layer as the last layer of the model. However, such models can't\npredict a relatively complete and variable-length label subset for each\ndocument, because they select positive labels relevant to the document by a\nfixed threshold or take top k labels in descending order of scores. A less\npopular type of deep learning models called sequence-to-sequence (Seq2Seq)\nfocus on predicting variable-length positive labels in sequence style. However,\nthe labels in XMTC tasks are essentially an unordered set rather than an\nordered sequence, the default order of labels restrains Seq2Seq models in\ntraining. To address this limitation in Seq2Seq, we propose an autoregressive\nsequence-to-set model for XMTC tasks named OTSeq2Set. Our model generates\npredictions in student-forcing scheme and is trained by a loss function based\non bipartite matching which enables permutation-invariance. Meanwhile, we use\nthe optimal transport distance as a measurement to force the model to focus on\nthe closest labels in semantic label space. Experiments show that OTSeq2Set\noutperforms other competitive baselines on 4 benchmark datasets. Especially, on\nthe Wikipedia dataset with 31k labels, it outperforms the state-of-the-art\nSeq2Seq method by 16.34% in micro-F1 score. The code is available at\nhttps://github.com/caojie54/OTSeq2Set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is MultiWOZ a Solved Task? An Interactive TOD Evaluation Framework with User Simulator. (arXiv:2210.14529v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14529","description":"<p>Task-Oriented Dialogue (TOD) systems are drawing more and more attention in\nrecent studies. Current methods focus on constructing pre-trained models or\nfine-tuning strategies while the evaluation of TOD is limited by a policy\nmismatch problem. That is, during evaluation, the user utterances are from the\nannotated dataset while these utterances should interact with previous\nresponses which can have many alternatives besides annotated texts. Therefore,\nin this work, we propose an interactive evaluation framework for TOD. We first\nbuild a goal-oriented user simulator based on pre-trained models and then use\nthe user simulator to interact with the dialogue system to generate dialogues.\nBesides, we introduce a sentence-level and a session-level score to measure the\nsentence fluency and session coherence in the interactive evaluation.\nExperimental results show that RL-based TOD systems trained by our proposed\nuser simulator can achieve nearly 98% inform and success rates in the\ninteractive evaluation of MultiWOZ dataset and the proposed scores measure the\nresponse quality besides the inform and success rates. We are hoping that our\nwork will encourage simulator-based interactive evaluations in the TOD task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qinyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_G/0/1/0/all/0/1\">Guofeng Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Feng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_X/0/1/0/all/0/1\">Xiaofeng Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unifying Data Perspectivism and Personalization: An Application to Social Norms. (arXiv:2210.14531v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14531","description":"<p>Instead of using a single ground truth for language processing tasks, several\nrecent studies have examined how to represent and predict the labels of the set\nof annotators. However, often little or no information about annotators is\nknown, or the set of annotators is small. In this work, we examine a corpus of\nsocial media posts about conflict from a set of 13k annotators and 210k\njudgements of social norms. We provide a novel experimental setup that applies\npersonalization methods to the modeling of annotators and compare their\neffectiveness for predicting the perception of social norms. We further provide\nan analysis of performance across subsets of social situations that vary by the\ncloseness of the relationship between parties in conflict, and assess where\npersonalization helps the most.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plepi_J/0/1/0/all/0/1\">Joan Plepi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neuendorf_B/0/1/0/all/0/1\">B&#xe9;la Neuendorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flek_L/0/1/0/all/0/1\">Lucie Flek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welch_C/0/1/0/all/0/1\">Charles Welch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Look to the Right: Mitigating Relative Position Bias in Extractive Question Answering. (arXiv:2210.14541v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14541","description":"<p>Extractive question answering (QA) models tend to exploit spurious\ncorrelations to make predictions when a training set has unintended biases.\nThis tendency results in models not being generalizable to examples where the\ncorrelations do not hold. Determining the spurious correlations QA models can\nexploit is crucial in building generalizable QA models in real-world\napplications; moreover, a method needs to be developed that prevents these\nmodels from learning the spurious correlations even when a training set is\nbiased. In this study, we discovered that the relative position of an answer,\nwhich is defined as the relative distance from an answer span to the closest\nquestion-context overlap word, can be exploited by QA models as superficial\ncues for making predictions. Specifically, we find that when the relative\npositions in a training set are biased, the performance on examples with\nrelative positions unseen during training is significantly degraded. To\nmitigate the performance degradation for unseen relative positions, we propose\nan ensemble-based debiasing method that does not require prior knowledge about\nthe distribution of relative positions. We demonstrate that the proposed method\nmitigates the models' reliance on relative positions using the biased and full\nSQuAD dataset. We hope that this study can help enhance the generalization\nability of QA models in real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shinoda_K/0/1/0/all/0/1\">Kazutoshi Shinoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugawara_S/0/1/0/all/0/1\">Saku Sugawara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1\">Akiko Aizawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Robust Bias Mitigation Procedure Based on the Stereotype Content Model. (arXiv:2210.14552v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14552","description":"<p>The Stereotype Content model (SCM) states that we tend to perceive minority\ngroups as cold, incompetent or both. In this paper we adapt existing work to\ndemonstrate that the Stereotype Content model holds for contextualised word\nembeddings, then use these results to evaluate a fine-tuning process designed\nto drive a language model away from stereotyped portrayals of minority groups.\nWe find the SCM terms are better able to capture bias than demographic agnostic\nterms related to pleasantness. Further, we were able to reduce the presence of\nstereotypes in the model through a simple fine-tuning procedure that required\nminimal human and computer resources, without harming downstream performance.\nWe present this work as a prototype of a debiasing procedure that aims to\nremove the need for a priori knowledge of the specifics of bias in the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ungless_E/0/1/0/all/0/1\">Eddie L. Ungless</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafferty_A/0/1/0/all/0/1\">Amy Rafferty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nag_H/0/1/0/all/0/1\">Hrichika Nag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_B/0/1/0/all/0/1\">Bj&#xf6;rn Ross</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Contrastive Learning via Uni-Modal Coding and Cross-Modal Prediction for Multimodal Sentiment Analysis. (arXiv:2210.14556v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14556","description":"<p>Multimodal representation learning is a challenging task in which previous\nwork mostly focus on either uni-modality pre-training or cross-modality fusion.\nIn fact, we regard modeling multimodal representation as building a skyscraper,\nwhere laying stable foundation and designing the main structure are equally\nessential. The former is like encoding robust uni-modal representation while\nthe later is like integrating interactive information among different\nmodalities, both of which are critical to learning an effective multimodal\nrepresentation. Recently, contrastive learning has been successfully applied in\nrepresentation learning, which can be utilized as the pillar of the skyscraper\nand benefit the model to extract the most important features contained in the\nmultimodal data. In this paper, we propose a novel framework named MultiModal\nContrastive Learning (MMCL) for multimodal representation to capture intra- and\ninter-modality dynamics simultaneously. Specifically, we devise uni-modal\ncontrastive coding with an efficient uni-modal feature augmentation strategy to\nfilter inherent noise contained in acoustic and visual modality and acquire\nmore robust uni-modality representations. Besides, a pseudo siamese network is\npresented to predict representation across different modalities, which\nsuccessfully captures cross-modal dynamics. Moreover, we design two contrastive\nlearning tasks, instance- and sentiment-based contrastive learning, to promote\nthe process of prediction and learn more interactive information related to\nsentiment. Extensive experiments conducted on two public datasets demonstrate\nthat our method surpasses the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1\">Ronghao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Haifeng Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty Sentence Sampling by Virtual Adversarial Perturbation. (arXiv:2210.14576v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14576","description":"<p>Active learning for sentence understanding attempts to reduce the annotation\ncost by identifying the most informative examples. Common methods for active\nlearning use either uncertainty or diversity sampling in the pool-based\nscenario. In this work, to incorporate both predictive uncertainty and sample\ndiversity, we propose Virtual Adversarial Perturbation for Active Learning\n(VAPAL) , an uncertainty-diversity combination framework, using virtual\nadversarial perturbation (Miyato et al., 2019) as model uncertainty\nrepresentation. VAPAL consistently performs equally well or even better than\nthe strong baselines on four sentence understanding datasets: AGNEWS, IMDB,\nPUBMED, and SST-2, offering a potential option for active learning on sentence\nunderstanding tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanshan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hongfei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Laws Beyond Backpropagation. (arXiv:2210.14593v1 [cs.LG])","link":"http://arxiv.org/abs/2210.14593","description":"<p>Alternatives to backpropagation have long been studied to better understand\nhow biological brains may learn. Recently, they have also garnered interest as\na way to train neural networks more efficiently. By relaxing constraints\ninherent to backpropagation (e.g., symmetric feedforward and feedback weights,\nsequential updates), these methods enable promising prospects, such as local\nlearning. However, the tradeoffs between different methods in terms of final\ntask performance, convergence speed, and ultimately compute and data\nrequirements are rarely outlined. In this work, we use scaling laws to study\nthe ability of Direct Feedback Alignment~(DFA) to train causal decoder-only\nTransformers efficiently. Scaling laws provide an overview of the tradeoffs\nimplied by a modeling decision, up to extrapolating how it might transfer to\nincreasingly large models. We find that DFA fails to offer more efficient\nscaling than backpropagation: there is never a regime for which the degradation\nin loss incurred by using DFA is worth the potential reduction in compute\nbudget. Our finding comes at variance with previous beliefs in the alternative\ntraining methods community, and highlights the need for holistic empirical\napproaches to better understand modeling decisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Filipovich_M/0/1/0/all/0/1\">Matthew J. Filipovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cappelli_A/0/1/0/all/0/1\">Alessandro Cappelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hesslow_D/0/1/0/all/0/1\">Daniel Hesslow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Launay_J/0/1/0/all/0/1\">Julien Launay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Knowledge Graphs for Automating AI of Digital Twins. (arXiv:2210.14596v1 [cs.AI])","link":"http://arxiv.org/abs/2210.14596","description":"<p>Digital Twins are digital representations of systems in the Internet of\nThings (IoT) that are often based on AI models that are trained on data from\nthose systems. Semantic models are used increasingly to link these datasets\nfrom different stages of the IoT systems life-cycle together and to\nautomatically configure the AI modelling pipelines. This combination of\nsemantic models with AI pipelines running on external datasets raises unique\nchallenges particular if rolled out at scale. Within this paper we will discuss\nthe unique requirements of applying semantic graphs to automate Digital Twins\nin different practical use cases. We will introduce the benchmark dataset DTBM\nthat reflects these characteristics and look into the scaling challenges of\ndifferent knowledge graph technologies. Based on these insights we will propose\na reference architecture that is in-use in multiple products in IBM and derive\nlessons learned for scaling knowledge graphs for configuring AI models for\nDigital Twins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ploennigs_J/0/1/0/all/0/1\">Joern Ploennigs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semertzidis_K/0/1/0/all/0/1\">Konstantinos Semertzidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenzi_F/0/1/0/all/0/1\">Fabio Lorenzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1\">Nandana Mihindukulasooriya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Multi-Task Learning for Abstractive Text Summarization. (arXiv:2210.14606v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14606","description":"<p>Despite the recent success of multi-task learning and pre-finetuning for\nnatural language understanding, few works have studied the effects of task\nfamilies on abstractive text summarization. Task families are a form of task\ngrouping during the pre-finetuning stage to learn common skills, such as\nreading comprehension. To close this gap, we analyze the influence of\nmulti-task learning strategies using task families for the English abstractive\ntext summarization task. We group tasks into one of three strategies, i.e.,\nsequential, simultaneous, and continual multi-task learning, and evaluate\ntrained models through two downstream tasks. We find that certain combinations\nof task families (e.g., advanced reading comprehension and natural language\ninference) positively impact downstream performance. Further, we find that\nchoice and combinations of task families influence downstream performance more\nthan the training scheme, supporting the use of task families for abstractive\ntext summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirstein_F/0/1/0/all/0/1\">Frederic Kirstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1\">Jan Philip Wahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A practical method for occupational skills detection in Vietnamese job listings. (arXiv:2210.14607v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14607","description":"<p>Vietnamese labor market has been under an imbalanced development. The number\nof university graduates is growing, but so is the unemployment rate. This\nsituation is often caused by the lack of accurate and timely labor market\ninformation, which leads to skill miss-matches between worker supply and the\nactual market demands. To build a data monitoring and analytic platform for the\nlabor market, one of the main challenges is to be able to automatically detect\noccupational skills from labor-related data, such as resumes and job listings.\nTraditional approaches rely on existing taxonomy and/or large annotated data to\nbuild Named Entity Recognition (NER) models. They are expensive and require\nhuge manual efforts. In this paper, we propose a practical methodology for\nskill detection in Vietnamese job listings. Rather than viewing the task as a\nNER task, we consider the task as a ranking problem. We propose a pipeline in\nwhich phrases are first extracted and ranked in semantic similarity with the\nphrases' contexts. Then we employ a final classification to detect skill\nphrases. We collected three datasets and conducted extensive experiments. The\nresults demonstrated that our methodology achieved better performance than a\nNER model in scarce datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Viet-Trung Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Hai-Nam Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tuan-Dung Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOCHA: A Multi-Task Training Approach for Coherent Text Generation from Cognitive Perspective. (arXiv:2210.14650v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14650","description":"<p>Teaching neural models to generate narrative coherent texts is a critical\nproblem. Recent pre-trained language models have achieved promising results,\nbut there is still a gap between human written texts and machine-generated\noutputs. In this work, we propose a novel multi-task training strategy for\ncoherent text generation grounded on the cognitive theory of writing, which\nempowers the model to learn essential subskills needed for writing including\nplanning and reviewing besides end-to-end generation. We extensively evaluate\nour model on three open-ended generation tasks including story generation, news\narticle writing and argument generation. Experiments show that our model\nachieves better results on both few-shot and fully-supervised settings than\nstrong baselines, and human evaluations confirm that our model can generate\nmore coherent outputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhe Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Hou Pong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Bilingual Parallel Corpus with Discourse Annotations. (arXiv:2210.14667v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14667","description":"<p>Machine translation (MT) has almost achieved human parity at sentence-level\ntranslation. In response, the MT community has, in part, shifted its focus to\ndocument-level translation. However, the development of document-level MT\nsystems is hampered by the lack of parallel document corpora. This paper\ndescribes BWB, a large parallel corpus first introduced in Jiang et al. (2022),\nalong with an annotated test set. The BWB corpus consists of Chinese novels\ntranslated by experts into English, and the annotated test set is designed to\nprobe the ability of machine translation systems to model various discourse\nphenomena. Our resource is freely available, and we hope it will serve as a\nguide and inspiration for more work in document-level machine translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuchen Eleanor Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Role of Centering Theory in the Context of Neural Coreference Resolution Systems. (arXiv:2210.14678v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14678","description":"<p>Centering theory (CT; Grosz et al., 1995) provides a linguistic analysis of\nthe structure of discourse. According to the theory, local coherence of\ndiscourse arises from the manner and extent to which successive utterances make\nreference to the same entities. In this paper, we investigate the connection\nbetween centering theory and modern coreference resolution systems. We provide\nan operationalization of centering and systematically investigate if neural\ncoreference resolvers adhere to the rules of centering theory by defining\nvarious discourse metrics and developing a search-based methodology. Our\ninformation-theoretic analysis reveals a positive dependence between\ncoreference and centering; but also shows that high-quality neural coreference\nresolvers may not benefit much from explicitly modeling centering ideas. Our\nanalysis further shows that contextualized embeddings contain much of the\ncoherence information, which helps explain why CT can only provide little gains\nto modern neural coreference resolvers which make use of pretrained\nrepresentations. Finally, we discuss factors that contribute to coreference\nwhich are not modeled by CT such as world knowledge and recency bias. We\nformulate a version of CT that also models recency and show that it captures\ncoreference information better compared to vanilla CT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuchen Eleanor Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pronunciation Generation for Foreign Language Words in Intra-Sentential Code-Switching Speech Recognition. (arXiv:2210.14691v1 [cs.SD])","link":"http://arxiv.org/abs/2210.14691","description":"<p>Code-Switching refers to the phenomenon of switching languages within a\nsentence or discourse. However, limited code-switching , different language\nphoneme-sets and high rebuilding costs throw a challenge to make the\nspecialized acoustic model for code-switching speech recognition. In this\npaper, we make use of limited code-switching data as driving materials and\nexplore a shortcut to quickly develop intra-sentential code-switching\nrecognition skill on the commissioned native language acoustic model, where we\npropose a data-driven method to make the seed lexicon which is used to train\ngrapheme-to-phoneme model to predict mapping pronunciations for foreign\nlanguage word in code-switching sentences. The core work of the data-driven\ntechnology in this paper consists of a phonetic decoding method and different\nselection methods. And for imbalanced word-level driving materials problem, we\nhave an internal assistance inspiration that learning the good pronunciation\nrules in the words that possess sufficient materials using the\ngrapheme-to-phoneme model to help the scarce. Our experiments show that the\nMixed Error Rate in intra-sentential Chinese-English code-switching recognition\nreduced from 29.15\\%, acquired on the pure Chinese recognizer, to 12.13\\% by\nadding foreign language words' pronunciation through our data-driven approach,\nand finally get the best result 11.14\\% with the combination of different\nselection methods and internal assistance tactic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaopei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autoregressive Structured Prediction with Language Models. (arXiv:2210.14698v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14698","description":"<p>Recent years have seen a paradigm shift in NLP towards using pretrained\nlanguage models ({PLM}) for a wide range of tasks.\n</p>\n<p>However, there are many difficult design decisions to represent structures\n(e.g. tagged text, coreference chains) in a way such that they can be captured\nby PLMs.\n</p>\n<p>Prior work on structured prediction with PLMs typically flattens the\nstructured output into a sequence, which limits the quality of structural\ninformation being learned and leads to inferior performance compared to classic\ndiscriminative models.\n</p>\n<p>In this work, we describe an approach to model structures as sequences of\nactions in an autoregressive manner with PLMs, allowing in-structure\ndependencies to be learned without any loss.\n</p>\n<p>Our approach achieves the new state-of-the-art on all the structured\nprediction tasks we looked at, namely, named entity recognition, end-to-end\nrelation extraction, and coreference resolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuchen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monath_N/0/1/0/all/0/1\">Nicholas Monath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?. (arXiv:2210.14699v1 [cs.SE])","link":"http://arxiv.org/abs/2210.14699","description":"<p>Language models are promising solutions for tackling increasing complex\nproblems. In software engineering, they recently attracted attention in code\nassistants, with programs automatically written in a given programming language\nfrom a programming task description in natural language. They have the\npotential to save time and effort when writing code. However, these systems are\ncurrently poorly understood, preventing them from being used optimally. In this\npaper, we investigate the various input parameters of two language models, and\nconduct a study to understand if variations of these input parameters (e.g.\nprogramming task description and the surrounding context, creativity of the\nlanguage model, number of generated solutions) can have a significant impact on\nthe quality of the generated programs. We design specific operators for varying\ninput parameters and apply them over two code assistants (Copilot and Codex)\nand two benchmarks representing algorithmic problems (HumanEval and LeetCode).\nOur results showed that varying the input parameters can significantly improve\nthe performance of language models. However, there is a tight dependency when\nvarying the temperature, the prompt and the number of generated solutions,\nmaking potentially hard for developers to properly control the parameters to\nobtain an optimal result. This work opens opportunities to propose (automated)\nstrategies for improving performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doderlein_J/0/1/0/all/0/1\">Jean-Baptiste D&#xf6;derlein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acher_M/0/1/0/all/0/1\">Mathieu Acher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khelladi_D/0/1/0/all/0/1\">Djamel Eddine Khelladi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Combemale_B/0/1/0/all/0/1\">Benoit Combemale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bloom Library: Multimodal Datasets in 300+ Languages for a Variety of Downstream Tasks. (arXiv:2210.14712v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14712","description":"<p>We present Bloom Library, a linguistically diverse set of multimodal and\nmultilingual datasets for language modeling, image captioning, visual\nstorytelling, and speech synthesis/recognition. These datasets represent either\nthe most, or among the most, multilingual datasets for each of the included\ndownstream tasks. In total, the initial release of the Bloom Library datasets\ncovers 363 languages across 32 language families. We train downstream task\nmodels for various languages represented in the data, showing the viability of\nthe data for future work in low-resource, multimodal NLP and establishing the\nfirst known baselines for these downstream tasks in certain languages (e.g.,\nBisu [bzi], with an estimated population of 700 users). Some of these\nfirst-of-their-kind baselines are comparable to state-of-the-art performance\nfor higher-resourced languages. The Bloom Library datasets are released under\nCreative Commons licenses on the Hugging Face datasets hub to catalyze more\nlinguistically diverse research in the included downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leong_C/0/1/0/all/0/1\">Colin Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nemecek_J/0/1/0/all/0/1\">Joshua Nemecek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansdorfer_J/0/1/0/all/0/1\">Jacob Mansdorfer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filighera_A/0/1/0/all/0/1\">Anna Filighera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owodunni_A/0/1/0/all/0/1\">Abraham Owodunni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitenack_D/0/1/0/all/0/1\">Daniel Whitenack</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Imbalanced Text Classification with Dynamic Curriculum Learning. (arXiv:2210.14724v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14724","description":"<p>Recent advances in pre-trained language models have improved the performance\nfor text classification tasks. However, little attention is paid to the\npriority scheduling strategy on the samples during training. Humans acquire\nknowledge gradually from easy to complex concepts, and the difficulty of the\nsame material can also vary significantly in different learning stages.\nInspired by this insights, we proposed a novel self-paced dynamic curriculum\nlearning (SPDCL) method for imbalanced text classification, which evaluates the\nsample difficulty by both linguistic character and model capacity. Meanwhile,\nrather than using static curriculum learning as in the existing research, our\nSPDCL can reorder and resample training data by difficulty criterion with an\nadaptive from easy to hard pace. The extensive experiments on several\nclassification tasks show the effectiveness of SPDCL strategy, especially for\nthe imbalanced dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xulong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Ning Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linguistic-Enhanced Transformer with CTC Embedding for Speech Recognition. (arXiv:2210.14725v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14725","description":"<p>The recent emergence of joint CTC-Attention model shows significant\nimprovement in automatic speech recognition (ASR). The improvement largely lies\nin the modeling of linguistic information by decoder. The decoder\njoint-optimized with an acoustic encoder renders the language model from\nground-truth sequences in an auto-regressive manner during training. However,\nthe training corpus of the decoder is limited to the speech transcriptions,\nwhich is far less than the corpus needed to train an acceptable language model.\nThis leads to poor robustness of decoder. To alleviate this problem, we propose\nlinguistic-enhanced transformer, which introduces refined CTC information to\ndecoder during training process, so that the decoder can be more robust. Our\nexperiments on AISHELL-1 speech corpus show that the character error rate (CER)\nis relatively reduced by up to 7%. We also find that in joint CTC-Attention ASR\nmodel, decoder is more sensitive to linguistic information than acoustic\ninformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xulong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Ning Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monotonic segmental attention for automatic speech recognition. (arXiv:2210.14742v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14742","description":"<p>We introduce a novel segmental-attention model for automatic speech\nrecognition. We restrict the decoder attention to segments to avoid quadratic\nruntime of global attention, better generalize to long sequences, and\neventually enable streaming. We directly compare global-attention and different\nsegmental-attention modeling variants. We develop and compare two separate\ntime-synchronous decoders, one specifically taking the segmental nature into\naccount, yielding further improvements. Using time-synchronous decoding for\nsegmental models is novel and a step towards streaming applications. Our\nexperiments show the importance of a length model to predict the segment\nboundaries. The final best segmental-attention model using segmental decoding\nperforms better than global-attention, in contrast to other monotonic attention\napproaches in the literature. Further, we observe that the segmental model\ngeneralizes much better to long sequences of up to several minutes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeyer_A/0/1/0/all/0/1\">Albert Zeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_R/0/1/0/all/0/1\">Robin Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Task: Deriving Semantic Class Targets for the Physical Sciences. (arXiv:2210.14760v1 [astro-ph.IM])","link":"http://arxiv.org/abs/2210.14760","description":"<p>We define deriving semantic class targets as a novel multi-modal task. By\ndoing so, we aim to improve classification schemes in the physical sciences\nwhich can be severely abstracted and obfuscating. We address this task for\nupcoming radio astronomy surveys and present the derived semantic radio galaxy\nmorphology class targets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Bowles_M/0/1/0/all/0/1\">Micah Bowles</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Tang_H/0/1/0/all/0/1\">Hongming Tang</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Vardoulaki_E/0/1/0/all/0/1\">Eleni Vardoulaki</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Alexander_E/0/1/0/all/0/1\">Emma L. Alexander</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Luo_Y/0/1/0/all/0/1\">Yan Luo</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Rudnick_L/0/1/0/all/0/1\">Lawrence Rudnick</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Walmsley_M/0/1/0/all/0/1\">Mike Walmsley</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Porter_F/0/1/0/all/0/1\">Fiona Porter</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Scaife_A/0/1/0/all/0/1\">Anna M. M. Scaife</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Slijepcevic_I/0/1/0/all/0/1\">Inigo Val Slijepcevic</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Segal_G/0/1/0/all/0/1\">Gary Segal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProSiT! Latent Variable Discovery with PROgressive SImilarity Thresholds. (arXiv:2210.14763v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14763","description":"<p>The most common ways to explore latent document dimensions are topic models\nand clustering methods. However, topic models have several drawbacks: e.g.,\nthey require us to choose the number of latent dimensions a priori, and the\nresults are stochastic. Most clustering methods have the same issues and lack\nflexibility in various ways, such as not accounting for the influence of\ndifferent topics on single documents, forcing word-descriptors to belong to a\nsingle topic (hard-clustering) or necessarily relying on word representations.\nWe propose PROgressive SImilarity Thresholds - ProSiT, a deterministic and\ninterpretable method, agnostic to the input format, that finds the optimal\nnumber of latent dimensions and only has two hyper-parameters, which can be set\nefficiently via grid search. We compare this method with a wide range of topic\nmodels and clustering methods on four benchmark data sets. In most setting,\nProSiT matches or outperforms the other methods in terms six metrics of topic\ncoherence and distinctiveness, producing replicable, deterministic results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fornaciari_T/0/1/0/all/0/1\">Tommaso Fornaciari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Prompt, Search! Mining-based Zero-Shot Learning with Language Models. (arXiv:2210.14803v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14803","description":"<p>Masked language models like BERT can perform text classification in a\nzero-shot fashion by reformulating downstream tasks as text infilling. However,\nthis approach is highly sensitive to the template used to prompt the model, yet\npractitioners are blind when designing them in strict zero-shot settings. In\nthis paper, we propose an alternative mining-based approach for zero-shot\nlearning. Instead of prompting language models, we use regular expressions to\nmine labeled examples from unlabeled corpora, which can optionally be filtered\nthrough prompting, and used to finetune a pretrained model. Our method is more\nflexible and interpretable than prompting, and outperforms it on a wide range\nof tasks when using comparable templates. Our results suggest that the success\nof prompting can partly be explained by the model being exposed to similar\nexamples during pretraining, which can be directly retrieved through regular\nexpressions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kar_M/0/1/0/all/0/1\">Mozes van de Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1\">Mengzhou Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioNLI: Generating a Biomedical NLI Dataset Using Lexico-semantic Constraints for Adversarial Examples. (arXiv:2210.14814v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14814","description":"<p>Natural language inference (NLI) is critical for complex decision-making in\nbiomedical domain. One key question, for example, is whether a given biomedical\nmechanism is supported by experimental evidence. This can be seen as an NLI\nproblem but there are no directly usable datasets to address this. The main\nchallenge is that manually creating informative negative examples for this task\nis difficult and expensive. We introduce a novel semi-supervised procedure that\nbootstraps an NLI dataset from existing biomedical dataset that pairs\nmechanisms with experimental evidence in abstracts. We generate a range of\nnegative examples using nine strategies that manipulate the structure of the\nunderlying mechanisms both with rules, e.g., flip the roles of the entities in\nthe interaction, and, more importantly, as perturbations via logical\nconstraints in a neuro-logical decoding system. We use this procedure to create\na novel dataset for NLI in the biomedical domain, called BioNLI and benchmark\ntwo state-of-the-art biomedical classifiers. The best result we obtain is\naround mid 70s in F1, suggesting the difficulty of the task. Critically, the\nperformance on the different classes of negative examples varies widely, from\n97% F1 on the simple role change negative examples, to barely better than\nchance on the negative examples generated using neuro-logic decoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bastan_M/0/1/0/all/0/1\">Mohaddeseh Bastan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1\">Mihai Surdeanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Curious Case of $\\ell_2$ norm of Sense Embeddings. (arXiv:2210.14815v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14815","description":"<p>We show that the $\\ell_2$ norm of a static sense embedding encodes\ninformation related to the frequency of that sense in the training corpus used\nto learn the sense embeddings. This finding can be seen as an extension of a\npreviously known relationship for word embeddings to sense embeddings. Our\nexperimental results show that, in spite of its simplicity, the $\\ell_2$ norm\nof sense embeddings is a surprisingly effective feature for several word sense\nrelated tasks such as (a) most frequent sense prediction, (b) Word-in-Context\n(WiC), and (c) Word Sense Disambiguation (WSD). In particular, by simply\nincluding the $\\ell_2$ norm of a sense embedding as a feature in a classifier,\nwe show that we can improve WiC and WSD methods that use static sense\nembeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProVe: A Pipeline for Automated Provenance Verification of Knowledge Graphs against Textual Sources. (arXiv:2210.14846v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14846","description":"<p>Knowledge Graphs are repositories of information that gather data from a\nmultitude of domains and sources in the form of semantic triples, serving as a\nsource of structured data for various crucial applications in the modern web\nlandscape, from Wikipedia infoboxes to search engines. Such graphs mainly serve\nas secondary sources of information and depend on well-documented and\nverifiable provenance to ensure their trustworthiness and usability. However,\ntheir ability to systematically assess and assure the quality of this\nprovenance, most crucially whether it properly supports the graph's\ninformation, relies mainly on manual processes that do not scale with size.\nProVe aims at remedying this, consisting of a pipelined approach that\nautomatically verifies whether a Knowledge Graph triple is supported by text\nextracted from its documented provenance. ProVe is intended to assist\ninformation curators and consists of four main steps involving rule-based\nmethods and machine learning models: text extraction, triple verbalisation,\nsentence selection, and claim verification. ProVe is evaluated on a Wikidata\ndataset, achieving promising results overall and excellent performance on the\nbinary classification task of detecting support from provenance, with 87.5%\naccuracy and 82.9% F1-macro on text-rich sources. The evaluation data and\nscripts used in this paper are available on GitHub and Figshare.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amaral_G/0/1/0/all/0/1\">Gabriel Amaral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_O/0/1/0/all/0/1\">Odinaldo Rodrigues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simperl_E/0/1/0/all/0/1\">Elena Simperl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causality Detection using Multiple Annotation Decision. (arXiv:2210.14852v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14852","description":"<p>The paper describes the work that has been submitted to the 5th workshop on\nChallenges and Applications of Automated Extraction of socio-political events\nfrom text (CASE 2022). The work is associated with Subtask 1 of Shared Task 3\nthat aims to detect causality in protest news corpus. The authors used\ndifferent large language models with customized cross-entropy loss functions\nthat exploit annotation information. The experiments showed that\nbert-based-uncased with refined cross-entropy outperformed the others,\nachieving a F1 score of 0.8501 on the Causal News Corpus dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1\">Quynh Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1\">Arka Mitra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Semantic Parsing: From Images to Abstract Meaning Representation. (arXiv:2210.14862v1 [cs.CV])","link":"http://arxiv.org/abs/2210.14862","description":"<p>The success of scene graphs for visual scene understanding has brought\nattention to the benefits of abstracting a visual input (e.g., image) into a\nstructured representation, where entities (people and objects) are nodes\nconnected by edges specifying their relations. Building these representations,\nhowever, requires expensive manual annotation in the form of images paired with\ntheir scene graphs or frames. These formalisms remain limited in the nature of\nentities and relations they can capture. In this paper, we propose to leverage\na widely-used meaning representation in the field of natural language\nprocessing, the Abstract Meaning Representation (AMR), to address these\nshortcomings. Compared to scene graphs, which largely emphasize spatial\nrelationships, our visual AMR graphs are more linguistically informed, with a\nfocus on higher-level semantic concepts extrapolated from visual input.\nMoreover, they allow us to generate meta-AMR graphs to unify information\ncontained in multiple image descriptions under one representation. Through\nextensive experimentation and analysis, we demonstrate that we can re-purpose\nan existing text-to-AMR parser to parse images into AMRs. Our findings point to\nimportant future research directions for improved scene understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelsalam_M/0/1/0/all/0/1\">Mohamed Ashraf Abdelsalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fancellu_F/0/1/0/all/0/1\">Federico Fancellu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basioti_K/0/1/0/all/0/1\">Kalliopi Basioti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_D/0/1/0/all/0/1\">Dhaivat J. Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+pavlovic_v/0/1/0/all/0/1\">vladimir pavlovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazly_A/0/1/0/all/0/1\">Afsaneh Fazly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond English-Centric Bitexts for Better Multilingual Language Representation Learning. (arXiv:2210.14867v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14867","description":"<p>In this paper, we elaborate upon recipes for building multilingual\nrepresentation models that are not only competitive with existing\nstate-of-the-art models but are also more parameter efficient, thereby\npromoting better adoption in resource-constrained scenarios and practical\napplications. We show that going beyond English-centric bitexts, coupled with a\nnovel sampling strategy aimed at reducing under-utilization of training data,\nsubstantially boosts performance across model sizes for both Electra and MLM\npre-training objectives. We introduce XY-LENT: X-Y bitext enhanced Language\nENcodings using Transformers which not only achieves state-of-the-art\nperformance over 5 cross-lingual tasks within all model size bands, is also\ncompetitive across bands. Our XY-LENT XL variant outperforms XLM-RXXL and\nexhibits competitive performance with mT5 XXL while being 5x and 6x smaller\nrespectively. We then show that our proposed method helps ameliorate the curse\nof multilinguality, with the XY-LENT XL achieving 99.3% GLUE performance and\n98.5% SQuAD 2.0 performance compared to a SoTA English only model in the same\nsize band. We then analyze our models performance on extremely low resource\nlanguages and posit that scaling alone may not be sufficient for improving the\nperformance in this scenario\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patra_B/0/1/0/all/0/1\">Barun Patra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1\">Saksham Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zewen Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1\">Vishrav Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-lingual Evaluation of Code Generation Models. (arXiv:2210.14868v1 [cs.LG])","link":"http://arxiv.org/abs/2210.14868","description":"<p>We present MBXP, an execution-based code completion benchmark in 10+\nprogramming languages. This collection of datasets is generated by our\nconversion framework that translates prompts and test cases from the original\nMBPP dataset to the corresponding data in a target language. Based on this\nbenchmark, we are able to evaluate code generation models in a multi-lingual\nfashion, and in particular discover generalization ability of language models\non out-of-domain languages, advantages of large multi-lingual models over\nmono-lingual, benefits of few-shot prompting, and zero-shot translation\nabilities. In addition, we use our code generation model to perform large-scale\nbootstrapping to obtain synthetic canonical solutions in several languages.\nThese solutions can be used for other code-related evaluations such as\ninsertion-based, summarization, or code translation tasks where we demonstrate\nresults and release as part of our benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Athiwaratkun_B/0/1/0/all/0/1\">Ben Athiwaratkun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gouda_S/0/1/0/all/0/1\">Sanjay Krishna Gouda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaopeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuchen Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Ming Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_M/0/1/0/all/0/1\">Mingyue Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonugondla_S/0/1/0/all/0/1\">Sujan Kumar Gonugondla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Hantian Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fulton_N/0/1/0/all/0/1\">Nathan Fulton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farahani_A/0/1/0/all/0/1\">Arash Farahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Siddhartha Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giaquinto_R/0/1/0/all/0/1\">Robert Giaquinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Haifeng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanathan_M/0/1/0/all/0/1\">Murali Krishna Ramanathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nallapati_R/0/1/0/all/0/1\">Ramesh Nallapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_B/0/1/0/all/0/1\">Baishakhi Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_P/0/1/0/all/0/1\">Parminder Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Sudipta Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation. (arXiv:2009.09435v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2009.09435","description":"<p>Bolukbasi et al. (2016) presents one of the first gender bias mitigation\ntechniques for word embeddings. Their method takes pre-trained word embeddings\nas input and attempts to isolate a linear subspace that captures most of the\ngender bias in the embeddings. As judged by an analogical evaluation task,\ntheir method virtually eliminates gender bias in the embeddings. However, an\nimplicit and untested assumption of their method is that the bias sub-space is\nactually linear. In this work, we generalize their method to a kernelized,\nnon-linear version. We take inspiration from kernel principal component\nanalysis and derive a non-linear bias isolation technique. We discuss and\novercome some of the practical drawbacks of our method for non-linear gender\nbias mitigation in word embeddings and analyze empirically whether the bias\nsubspace is actually linear. Our analysis shows that gender bias is in fact\nwell captured by a linear subspace, justifying the assumption of Bolukbasi et\nal. (2016).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vargas_F/0/1/0/all/0/1\">Francisco Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GENIE: A Leaderboard for Human-in-the-Loop Evaluation of Text Generation. (arXiv:2101.06561v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.06561","description":"<p>Leaderboards have eased model development for many NLP datasets by\nstandardizing their evaluation and delegating it to an independent external\nrepository. Their adoption, however, is so far limited to tasks that can be\nreliably evaluated in an automatic manner. This work introduces GENIE, an\nextensible human evaluation leaderboard, which brings the ease of leaderboards\nto text generation tasks. GENIE automatically posts leaderboard submissions to\ncrowdsourcing platforms asking human annotators to evaluate them on various\naxes (e.g., correctness, conciseness, fluency) and compares their answers to\nvarious automatic metrics. We introduce several datasets in English to GENIE,\nrepresenting four core challenges in text generation: machine translation,\nsummarization, commonsense reasoning, and machine comprehension. We provide\nformal granular evaluation metrics and identify areas for future research. We\nmake GENIE publicly available and hope that it will spur progress in language\ngeneration models as well as their automatic and manual evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1\">Jonathan Bragg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lourie_N/0/1/0/all/0/1\">Nicholas Lourie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel S. Weld</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dependency Parsing with Bottom-up Hierarchical Pointer Networks. (arXiv:2105.09611v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.09611","description":"<p>Dependency parsing is a crucial step towards deep language understanding and,\ntherefore, widely demanded by numerous Natural Language Processing\napplications. In particular, left-to-right and top-down transition-based\nalgorithms that rely on Pointer Networks are among the most accurate approaches\nfor performing dependency parsing. Additionally, it has been observed for the\ntop-down algorithm that Pointer Networks' sequential decoding can be improved\nby implementing a hierarchical variant, more adequate to model dependency\nstructures. Considering all this, we develop a bottom-up-oriented Hierarchical\nPointer Network for the left-to-right parser and propose two novel\ntransition-based alternatives: an approach that parses a sentence in\nright-to-left order and a variant that does it from the outside in. We\nempirically test the proposed neural architecture with the different algorithms\non a wide variety of languages, outperforming the original approach in\npractically all of them and setting new state-of-the-art results on the English\nand Chinese Penn Treebanks for non-contextualized and BERT-based embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Gonzalez_D/0/1/0/all/0/1\">Daniel Fern&#xe1;ndez-Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Comparison of Pre-training Language Models. (arXiv:2106.11483v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.11483","description":"<p>Recently, the development of pre-trained language models has brought natural\nlanguage processing (NLP) tasks to the new state-of-the-art. In this paper we\nexplore the efficiency of various pre-trained language models. We pre-train a\nlist of transformer-based models with the same amount of text and the same\ntraining steps. The experimental results shows that the most improvement upon\nthe origin BERT is adding the RNN-layer to capture more contextual information\nfor short text understanding. But there are no remarkable improvement for short\ntext understanding for similar BERT structures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Know Thy Strengths: Comprehensive Dialogue State Tracking Diagnostics. (arXiv:2112.08321v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08321","description":"<p>Recent works that revealed the vulnerability of dialogue state tracking (DST)\nmodels to distributional shifts have made holistic comparisons on robustness\nand qualitative analyses increasingly important for understanding their\nrelative performance. We present our findings from standardized and\ncomprehensive DST diagnoses, which have previously been sparse and\nuncoordinated, using our toolkit, CheckDST, a collection of robustness tests\nand failure mode analytics. We discover that different classes of DST models\nhave clear strengths and weaknesses, where generation models are more promising\nfor handling language variety while span-based classification models are more\nrobust to unseen entities. Prompted by this discovery, we also compare\ncheckpoints from the same model and find that the standard practice of\nselecting checkpoints using validation loss/accuracy is prone to overfitting\nand each model class has distinct patterns of failure. Lastly, we demonstrate\nhow our diagnoses motivate a pre-finetuning procedure with non-dialogue data\nthat offers comprehensive improvements to generation models by alleviating the\nimpact of distributional shifts through transfer learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyundong Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1\">Chinnadhurai Sankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Christopher Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadagopan_K/0/1/0/all/0/1\">Kaushik Ram Sadagopan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shayandeh_S/0/1/0/all/0/1\">Shahin Shayandeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1\">Ahmad Beirami</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Large Scale Language Modeling with Mixtures of Experts. (arXiv:2112.10684v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.10684","description":"<p>Mixture of Experts layers (MoEs) enable efficient scaling of language models\nthrough conditional computation. This paper presents a detailed empirical study\nof how autoregressive MoE language models scale in comparison with dense models\nin a wide range of settings: in- and out-of-domain language modeling, zero- and\nfew-shot priming, and full-shot fine-tuning. With the exception of fine-tuning,\nwe find MoEs to be substantially more compute efficient. At more modest\ntraining budgets, MoEs can match the performance of dense models using $\\sim$4\ntimes less compute. This gap narrows at scale, but our largest MoE model (1.1T\nparameters) consistently outperforms a compute-equivalent dense model (6.7B\nparameters). Overall, this performance gap varies greatly across tasks and\ndomains, suggesting that MoE and dense models generalize differently in ways\nthat are worthy of future study. We make our code and models publicly available\nfor research use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhosale_S/0/1/0/all/0/1\">Shruti Bhosale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Naman Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihaylov_T/0/1/0/all/0/1\">Todor Mihaylov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_M/0/1/0/all/0/1\">Myle Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shleifer_S/0/1/0/all/0/1\">Sam Shleifer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xi Victoria Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jingfei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_S/0/1/0/all/0/1\">Srinivasan Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ramakanth Pasunuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anantharaman_G/0/1/0/all/0/1\">Giri Anantharaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuohui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akin_H/0/1/0/all/0/1\">Halil Akin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baines_M/0/1/0/all/0/1\">Mandeep Baines</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1\">Louis Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koura_P/0/1/0/all/0/1\">Punit Singh Koura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OHoro_B/0/1/0/all/0/1\">Brian O&#x27;Horo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jeff Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1\">Mona Diab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozareva_Z/0/1/0/all/0/1\">Zornitsa Kozareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1\">Ves Stoyanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeRetriever: Unimodal and Bimodal Contrastive Learning for Code Search. (arXiv:2201.10866v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.10866","description":"<p>In this paper, we propose the CodeRetriever model, which learns the\nfunction-level code semantic representations through large-scale code-text\ncontrastive pre-training. We adopt two contrastive learning schemes in\nCodeRetriever: unimodal contrastive learning and bimodal contrastive learning.\nFor unimodal contrastive learning, we design an unsupervised learning approach\nto build semantic-related code pairs based on the documentation and function\nname. For bimodal contrastive learning, we leverage the documentation and\nin-line comments of code to build code-text pairs. Both contrastive objectives\ncan fully leverage large-scale code corpus for pre-training. Extensive\nexperimental results show that CodeRetriever achieves new state-of-the-art with\nsignificant improvement over existing code pre-trained models, on eleven\ndomain/language-specific code search tasks with six programming languages in\ndifferent code granularity (function-level, snippet-level and statement-level).\nThese results demonstrate the effectiveness and robustness of CodeRetriever.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaonan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bolun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1\">Weizhen Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question Answering Evaluation. (arXiv:2202.07654v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.07654","description":"<p>The predictions of question answering (QA)systems are typically evaluated\nagainst manually annotated finite sets of one or more answers. This leads to a\ncoverage limitation that results in underestimating the true performance of\nsystems, and is typically addressed by extending over exact match (EM) with\npre-defined rules or with the token-level F1 measure. In this paper, we present\nthe first systematic conceptual and data-driven analysis to examine the\nshortcomings of token-level equivalence measures.\n</p>\n<p>To this end, we define the asymmetric notion of answer equivalence (AE),\naccepting answers that are equivalent to or improve over the reference, and\npublish over 23k human judgments for candidates produced by multiple QA systems\non SQuAD. Through a careful analysis of this data, we reveal and quantify\nseveral concrete limitations of the F1 measure, such as a false impression of\ngraduality, or missing dependence on the question.\n</p>\n<p>Since collecting AE annotations for each evaluated model is expensive, we\nlearn a BERT matching (BEM) measure to approximate this task. Being a simpler\ntask than QA, we find BEM to provide significantly better AE approximations\nthan F1, and to more accurately reflect the performance of systems.\n</p>\n<p>Finally, we demonstrate the practical utility of AE and BEM on the concrete\napplication of minimal accurate prediction sets, reducing the number of\nrequired answers by up to x2.6.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bulian_J/0/1/0/all/0/1\">Jannis Bulian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buck_C/0/1/0/all/0/1\">Christian Buck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gajewski_W/0/1/0/all/0/1\">Wojciech Gajewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boerschinger_B/0/1/0/all/0/1\">Benjamin Boerschinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"APEACH: Attacking Pejorative Expressions with Analysis on Crowd-Generated Hate Speech Evaluation Datasets. (arXiv:2202.12459v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.12459","description":"<p>In hate speech detection, developing training and evaluation datasets across\nvarious domains is the critical issue. Whereas, major approaches crawl social\nmedia texts and hire crowd-workers to annotate the data. Following this\nconvention often restricts the scope of pejorative expressions to a single\ndomain lacking generalization. Sometimes domain overlap between training corpus\nand evaluation set overestimate the prediction performance when pretraining\nlanguage models on low-data language. To alleviate these problems in Korean, we\npropose APEACH that asks unspecified users to generate hate speech examples\nfollowed by minimal post-labeling. We find that APEACH can collect useful\ndatasets that are less sensitive to the lexical overlaps between the\npretraining corpus and the evaluation set, thereby properly measuring the model\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kichang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_W/0/1/0/all/0/1\">Wonjun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_W/0/1/0/all/0/1\">Won Ik Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RED-ACE: Robust Error Detection for ASR using Confidence Embeddings. (arXiv:2203.07172v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07172","description":"<p>ASR Error Detection (AED) models aim to post-process the output of Automatic\nSpeech Recognition (ASR) systems, in order to detect transcription errors.\nModern approaches usually use text-based input, comprised solely of the ASR\ntranscription hypothesis, disregarding additional signals from the ASR model.\nInstead, we propose to utilize the ASR system's word-level confidence scores\nfor improving AED performance. Specifically, we add an ASR Confidence Embedding\n(ACE) layer to the AED model's encoder, allowing us to jointly encode the\nconfidence scores and the transcribed text into a contextualized\nrepresentation. Our experiments show the benefits of ASR confidence scores for\nAED, their complementary effect over the textual signal, as well as the\neffectiveness and robustness of ACE for combining these signals. To foster\nfurther research, we publish a novel AED dataset consisting of ASR outputs on\nthe LibriSpeech corpus with annotated transcription errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gekhman_Z/0/1/0/all/0/1\">Zorik Gekhman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zverinski_D/0/1/0/all/0/1\">Dina Zverinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallinson_J/0/1/0/all/0/1\">Jonathan Mallinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beryozkin_G/0/1/0/all/0/1\">Genady Beryozkin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Corpus Quality Really Matter for Low-Resource Languages?. (arXiv:2203.08111v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08111","description":"<p>The vast majority of non-English corpora are derived from automatically\nfiltered versions of CommonCrawl. While prior work has identified major issues\non the quality of these datasets (Kreutzer et al., 2021), it is not clear how\nthis impacts downstream performance. Taking representation learning in Basque\nas a case study, we explore tailored crawling (manually identifying and\nscraping websites with high-quality content) as an alternative to filtering\nCommonCrawl. Our new corpus, called EusCrawl, is similar in size to the Basque\nportion of popular multilingual corpora like CC100 and mC4, yet it has a much\nhigher quality according to native annotators. For instance, 66% of documents\nare rated as high-quality for EusCrawl, in contrast with &lt;33% for both mC4 and\nCC100. Nevertheless, we obtain similar results on downstream NLU tasks\nregardless of the corpus used for pre-training. Our work suggests that NLU\nperformance in low-resource languages is not primarily constrained by the\nquality of the data, and other factors like corpus size and domain coverage can\nplay a more important role.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldabe_I/0/1/0/all/0/1\">Itziar Aldabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1\">Rodrigo Agerri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_de_Vinaspre_O/0/1/0/all/0/1\">Olatz Perez-de-Vi&#xf1;aspre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soroa_A/0/1/0/all/0/1\">Aitor Soroa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-Context Learning for Few-Shot Dialogue State Tracking. (arXiv:2203.08568v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08568","description":"<p>Collecting and annotating task-oriented dialogues is time-consuming and\ncostly; thus, zero and few shot learning could greatly benefit dialogue state\ntracking (DST). In this work, we propose an in-context learning (ICL) framework\nfor zero-shot and few-shot learning DST, where a large pre-trained language\nmodel (LM) takes a test instance and a few exemplars as input, and directly\ndecodes the dialogue state without any parameter updates. To better leverage a\ntabular domain description in the LM prompt, we reformulate DST into a\ntext-to-SQL problem. We also propose a novel approach to retrieve annotated\ndialogues as exemplars. Empirical results on MultiWOZ show that our method\nIC-DST substantially outperforms previous fine-tuned state-of-the-art models in\nfew-shot settings. In addition, we test IC-DST in zero-shot settings, in which\nthe model only takes a fixed task instruction as input, finding that it\noutperforms previous zero-shot methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yushi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chia-Hsuan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tianbao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1\">Mari Ostendorf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning. (arXiv:2203.11933v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.11933","description":"<p>Vision-language models can encode societal biases and stereotypes, but there\nare challenges to measuring and mitigating these multimodal harms due to\nlacking measurement robustness and feature degradation. To address these\nchallenges, we investigate bias measures and apply ranking metrics for\nimage-text representations. We then investigate debiasing methods and show that\nprepending learned embeddings to text queries that are jointly trained with\nadversarial debiasing and a contrastive loss reduces various bias measures with\nminimal degradation to the image-text representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berg_H/0/1/0/all/0/1\">Hugo Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_S/0/1/0/all/0/1\">Siobhan Mackenzie Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhalgat_Y/0/1/0/all/0/1\">Yash Bhalgat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wonsuk Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Rose Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shtedritski_A/0/1/0/all/0/1\">Aleksandar Shtedritski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bain_M/0/1/0/all/0/1\">Max Bain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It Takes Two Flints to Make a Fire: Multitask Learning of Neural Relation and Explanation Classifiers. (arXiv:2204.11424v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.11424","description":"<p>We propose an explainable approach for relation extraction that mitigates the\ntension between generalization and explainability by jointly training for the\ntwo goals. Our approach uses a multi-task learning architecture, which jointly\ntrains a classifier for relation extraction, and a sequence model that labels\nwords in the context of the relation that explain the decisions of the relation\nclassifier. We also convert the model outputs to rules to bring global\nexplanations to this approach. This sequence model is trained using a hybrid\nstrategy: supervised, when supervision from pre-existing patterns is available,\nand semi-supervised otherwise. In the latter situation, we treat the sequence\nmodel's labels as latent variables, and learn the best assignment that\nmaximizes the performance of the relation classifier. We evaluate the proposed\napproach on the two datasets and show that the sequence model provides labels\nthat serve as accurate explanations for the relation classifier's decisions,\nand, importantly, that the joint training generally improves the performance of\nthe relation classifier. We also evaluate the performance of the generated\nrules and show that the new rules are great add-on to the manual rules and\nbring the rule-based system much closer to the neural models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1\">Mihai Surdeanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Handling and Presenting Harmful Text in NLP Research. (arXiv:2204.14256v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.14256","description":"<p>Text data can pose a risk of harm. However, the risks are not fully\nunderstood, and how to handle, present, and discuss harmful text in a safe way\nremains an unresolved issue in the NLP community. We provide an analytical\nframework categorising harms on three axes: (1) the harm type (e.g.,\nmisinformation, hate speech or racial stereotypes); (2) whether a harm is\n\\textit{sought} as a feature of the research design if explicitly studying\nharmful content (e.g., training a hate speech classifier), versus\n\\textit{unsought} if harmful content is encountered when working on unrelated\nproblems (e.g., language generation or part-of-speech tagging); and (3) who it\naffects, from people (mis)represented in the data to those handling the data\nand those publishing on the data. We provide advice for practitioners, with\nconcrete steps for mitigating harm in research and in publication. To assist\nimplementation we introduce \\textsc{HarmCheck} -- a documentation standard for\nhandling and presenting harmful text in research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Rose Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birhane_A/0/1/0/all/0/1\">Abeba Birhane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1\">Bertie Vidgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1\">Leon Derczynski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empowering parameter-efficient transfer learning by recognizing the kernel structure in self-attention. (arXiv:2205.03720v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.03720","description":"<p>The massive amount of trainable parameters in the pre-trained language models\n(PLMs) makes them hard to be deployed to multiple downstream tasks. To address\nthis issue, parameter-efficient transfer learning methods have been proposed to\ntune only a few parameters during fine-tuning while freezing the rest. This\npaper looks at existing methods along this line through the \\textit{kernel\nlens}. Motivated by the connection between self-attention in transformer-based\nPLMs and kernel learning, we propose \\textit{kernel-wise adapters}, namely\n\\textit{Kernel-mix}, that utilize the kernel structure in self-attention to\nguide the assignment of the tunable parameters. These adapters use guidelines\nfound in classical kernel learning and enable separate parameter tuning for\neach attention head. Our empirical results, over a diverse set of natural\nlanguage generation and understanding tasks, show that our proposed adapters\ncan attain or improve the strong performance of existing baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1\">Devamanyu Hazarika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namazifar_M/0/1/0/all/0/1\">Mahdi Namazifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RankGen: Improving Text Generation with Large Ranking Models. (arXiv:2205.09726v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09726","description":"<p>Given an input sequence (or prefix), modern language models often assign high\nprobabilities to output sequences that are repetitive, incoherent, or\nirrelevant to the prefix; as such, model-generated text also contains such\nartifacts. To address these issues we present RankGen, a 1.2B parameter encoder\nmodel for English that scores model generations given a prefix. RankGen can be\nflexibly incorporated as a scoring function in beam search and used to decode\nfrom any pretrained language model. We train RankGen using large-scale\ncontrastive learning to map a prefix close to the ground-truth sequence that\nfollows it and far away from two types of negatives: (1) random sequences from\nthe same document as the prefix, and (2) sequences generated from a large\nlanguage model conditioned on the prefix. Experiments across four different\nlanguage models (345M-11B parameters) and two domains show that RankGen\nsignificantly outperforms decoding algorithms like nucleus, top-k, and typical\nsampling on both automatic metrics (85.0 vs 77.3 MAUVE) as well as human\nevaluations with English writers (74.5% human preference over nucleus\nsampling). Analysis reveals that RankGen outputs are more relevant to the\nprefix and improve continuity and coherence compared to baselines. We release\nour model checkpoints, code, and human preference data with explanations to\nfacilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kalpesh Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yapei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieting_J/0/1/0/all/0/1\">John Wieting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Parametric Domain Adaptation for End-to-End Speech Translation. (arXiv:2205.11211v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11211","description":"<p>End-to-End Speech Translation (E2E-ST) has received increasing attention due\nto the potential of its less error propagation, lower latency, and fewer\nparameters. However, the effectiveness of neural-based approaches to this task\nis severely limited by the available training corpus, especially for domain\nadaptation where in-domain triplet training data is scarce or nonexistent. In\nthis paper, we propose a novel non-parametric method that leverages\ndomain-specific text translation corpus to achieve domain adaptation for the\nE2E-ST system. To this end, we first incorporate an additional encoder into the\npre-trained E2E-ST model to realize text translation modelling, and then unify\nthe decoder's output representation for text and speech translation tasks by\nreducing the correspondent representation mismatch in available triplet\ntraining data. During domain adaptation, a k-nearest-neighbor (kNN) classifier\nis introduced to produce the final translation distribution using the external\ndatastore built by the domain-specific text translation corpus, while the\nuniversal output representation is adopted to perform a similarity search.\nExperiments on the Europarl-ST benchmark demonstrate that when in-domain text\ntranslation data is involved only, our proposed approach significantly improves\nbaseline by 12.82 BLEU on average in all translation directions, even\noutperforming the strong in-domain fine-tuning method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yichao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Role of Bidirectionality in Language Model Pre-Training. (arXiv:2205.11726v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11726","description":"<p>Prior work on language model pre-training has explored different\narchitectures and learning objectives, but differences in data, hyperparameters\nand evaluation make a principled comparison difficult. In this work, we focus\non bidirectionality as a key factor that differentiates existing approaches,\nand present a comprehensive study of its role in next token prediction, text\ninfilling, zero-shot priming and fine-tuning. We propose a new framework that\ngeneralizes prior approaches, including fully unidirectional models like GPT,\nfully bidirectional models like BERT, and hybrid models like CM3 and prefix LM.\nOur framework distinguishes between two notions of bidirectionality\n(bidirectional context and bidirectional attention) and allows us to control\neach of them separately. We find that the optimal configuration is largely\napplication-dependent (e.g., bidirectional attention is beneficial for\nfine-tuning and infilling, but harmful for next token prediction and zero-shot\npriming). We train models with up to 6.7B parameters, and find differences to\nremain consistent at scale. While prior work on scaling has focused on\nleft-to-right autoregressive models, our results suggest that this approach\ncomes with some trade-offs, and it might be worthwhile to develop very large\nbidirectional models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jingfei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Naman Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1\">Ves Stoyanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EdiT5: Semi-Autoregressive Text-Editing with T5 Warm-Start. (arXiv:2205.12209v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12209","description":"<p>We present EdiT5 - a novel semi-autoregressive text-editing model designed to\ncombine the strengths of non-autoregressive text-editing and autoregressive\ndecoding. EdiT5 is faster during inference than conventional\nsequence-to-sequence (seq2seq) models, while being capable of modelling\nflexible input-output transformations.\n</p>\n<p>This is achieved by decomposing the generation process into three sub-tasks:\n(1) tagging to decide on the subset of input tokens to be preserved in the\noutput, (2) re-ordering to define their order in the output text, and (3)\ninsertion to infill the missing tokens that are not present in the input. The\ntagging and re-ordering steps, which are responsible for generating the largest\nportion of the output, are non-autoregressive, while the insertion step uses an\nautoregressive decoder.\n</p>\n<p>Depending on the task, EdiT5 on average requires significantly fewer\nautoregressive steps, demonstrating speedups of up to 25x when compared to\nseq2seq models. Quality-wise, EdiT5 is initialized with a pre-trained T5\ncheckpoint yielding comparable performance to T5 in high-resource settings when\nevaluated on three NLG tasks: Sentence Fusion, Grammatical Error Correction,\nand Decontextualization while clearly outperforming T5 in low-resource\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mallinson_J/0/1/0/all/0/1\">Jonathan Mallinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adamek_J/0/1/0/all/0/1\">Jakub Adamek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malmi_E/0/1/0/all/0/1\">Eric Malmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Severyn_A/0/1/0/all/0/1\">Aliaksei Severyn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factorizing Content and Budget Decisions in Abstractive Summarization of Long Documents. (arXiv:2205.12486v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12486","description":"<p>We argue that disentangling content selection from the budget used to cover\nsalient content improves the performance and applicability of abstractive\nsummarizers. Our method, FactorSum, does this disentanglement by factorizing\nsummarization into two steps through an energy function: (1) generation of\nabstractive summary views; (2) combination of these views into a final summary,\nfollowing a budget and content guidance. This guidance may come from different\nsources, including from an advisor model such as BART or BigBird, or in oracle\nmode -- from the reference. This factorization achieves significantly higher\nROUGE scores on multiple benchmarks for long document summarization, namely\nPubMed, arXiv, and GovReport. Most notably, our model is effective for domain\nadaptation. When trained only on PubMed samples, it achieves a 46.29 ROUGE-1\nscore on arXiv, which indicates a strong performance due to more flexible\nbudget adaptation and content selection less dependent on domain-specific\ntextual structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_M/0/1/0/all/0/1\">Marcio Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziser_Y/0/1/0/all/0/1\">Yftah Ziser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Shay B. Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Your Model Classify Entities Reasonably? Diagnosing and Mitigating Spurious Correlations in Entity Typing. (arXiv:2205.12640v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12640","description":"<p>Entity typing aims at predicting one or more words that describe the type(s)\nof a specific mention in a sentence. Due to shortcuts from surface patterns to\nannotated entity labels and biased training, existing entity typing models are\nsubject to the problem of spurious correlations. To comprehensively investigate\nthe faithfulness and reliability of entity typing methods, we first\nsystematically define distinct kinds of model biases that are reflected mainly\nfrom spurious correlations. Particularly, we identify six types of existing\nmodel biases, including mention-context bias, lexical overlapping bias, named\nentity bias, pronoun bias, dependency bias, and overgeneralization bias. To\nmitigate model biases, we then introduce a counterfactual data augmentation\nmethod. By augmenting the original training set with their debiased\ncounterparts, models are forced to fully comprehend sentences and discover the\nfundamental cues for entity typing, rather than relying on spurious\ncorrelations for shortcuts. Experimental results on the UFET dataset show our\ncounterfactual data augmentation approach helps improve generalization of\ndifferent entity typing models with consistently better performance on both the\noriginal and debiased test sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Nan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bangzheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Mingtao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InstructDial: Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning. (arXiv:2205.12673v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12673","description":"<p>Instruction tuning is an emergent paradigm in NLP wherein natural language\ninstructions are leveraged with language models to induce zero-shot performance\non unseen tasks. Instructions have been shown to enable good performance on\nunseen tasks and datasets in both large and small language models. Dialogue is\nan especially interesting area to explore instruction tuning because dialogue\nsystems perform multiple kinds of tasks related to language (e.g., natural\nlanguage understanding and generation, domain-specific interaction), yet\ninstruction tuning has not been systematically explored for dialogue-related\ntasks. We introduce InstructDial, an instruction tuning framework for dialogue,\nwhich consists of a repository of 48 diverse dialogue tasks in a unified\ntext-to-text format created from 59 openly available dialogue datasets. Next,\nwe explore cross-task generalization ability on models tuned on InstructDial\nacross diverse dialogue tasks. Our analysis reveals that InstructDial enables\ngood zero-shot performance on unseen datasets and tasks such as dialogue\nevaluation and intent detection, and even better performance in a few-shot\nsetting. To ensure that models adhere to instructions, we introduce novel\nmeta-tasks. We establish benchmark zero-shot and few-shot performance of models\ntrained using the proposed framework on multiple dialogue tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Prakhar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_C/0/1/0/all/0/1\">Cathy Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1\">Yi-Ting Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehri_S/0/1/0/all/0/1\">Shikib Mehri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eskenazi_M/0/1/0/all/0/1\">Maxine Eskenazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigham_J/0/1/0/all/0/1\">Jeffrey P. Bigham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation. (arXiv:2205.12697v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12697","description":"<p>Logical table-to-text generation is a task that involves generating logically\nfaithful sentences from tables, which requires models to derive logical level\nfacts from table records via logical inference. It raises a new challenge on\nthe logical-level content planning of table-to-text models. However, directly\nlearning the logical inference knowledge from table-text pairs is very\ndifficult for neural models because of the ambiguity of natural language and\nthe scarcity of parallel data. Hence even large-scale pre-trained language\nmodels present low logical fidelity on logical table-to-text. In this work, we\npropose a PLOG (Pretrained Logical Form Generator) framework to improve the\ngeneration fidelity. Specifically, PLOG is first pretrained on a\ntable-to-logic-form generation (table-to-logic) task, then finetuned on\ndownstream table-to-text tasks. The formal definition of logical forms enables\nus to collect large amount of accurate logical forms from tables without human\nannotation. In addition, PLOG can learn logical inference from table-logic\npairs much more definitely than from table-text pairs. To evaluate our model,\nwe further collect a controlled logical table-to-text dataset CONTLOG based on\nan existing dataset. On two benchmarks, LOGICNLG and CONTLOG, PLOG outperforms\nstrong baselines by a large margin on the logical fidelity, demonstrating the\neffectiveness of table-to-logic pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Ao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent Abilities of Large Language Models. (arXiv:2206.07682v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.07682","description":"<p>Scaling up language models has been shown to predictably improve performance\nand sample efficiency on a wide range of downstream tasks. This paper instead\ndiscusses an unpredictable phenomenon that we refer to as emergent abilities of\nlarge language models. We consider an ability to be emergent if it is not\npresent in smaller models but is present in larger models. Thus, emergent\nabilities cannot be predicted simply by extrapolating the performance of\nsmaller models. The existence of such emergence implies that additional scaling\ncould further expand the range of capabilities of language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bommasani_R/0/1/0/all/0/1\">Rishi Bommasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1\">Barret Zoph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1\">Sebastian Borgeaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosma_M/0/1/0/all/0/1\">Maarten Bosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed H. Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dean_J/0/1/0/all/0/1\">Jeff Dean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_W/0/1/0/all/0/1\">William Fedus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Benefits of Free-Form Rationales. (arXiv:2206.11083v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.11083","description":"<p>Free-form rationales aim to aid model interpretability by supplying the\nbackground knowledge that can help understand model decisions. Crowdsourced\nrationales are provided for commonsense QA instances in popular datasets such\nas CoS-E and ECQA, but their utility remains under-investigated. We present\nhuman studies which show that ECQA rationales indeed provide additional\nbackground information to understand a decision, while over 88% of CoS-E\nrationales do not. Inspired by this finding, we ask: can the additional context\nprovided by free-form rationales benefit models, similar to human users? We\ninvestigate the utility of rationales as an additional source of supervision,\nby varying the quantity and quality of rationales during training. After\ncontrolling for instances where rationales leak the correct answer while not\nproviding additional background knowledge, we find that incorporating only 5%\nof rationales during training can boost model performance by 47.22% for CoS-E\nand 57.14% for ECQA during inference. Moreover, we also show that rationale\nquality matters: compared to crowdsourced rationales, T5-generated rationales\nprovide not only weaker supervision to models, but are also not helpful for\nhumans in aiding model interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuezhe Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisFIS: Visual Feature Importance Supervision with Right-for-the-Right-Reason Objectives. (arXiv:2206.11212v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.11212","description":"<p>Many past works aim to improve visual reasoning in models by supervising\nfeature importance (estimated by model explanation techniques) with human\nannotations such as highlights of important image regions. However, recent work\nhas shown that performance gains from feature importance (FI) supervision for\nVisual Question Answering (VQA) tasks persist even with random supervision,\nsuggesting that these methods do not meaningfully align model FI with human FI.\nIn this paper, we show that model FI supervision can meaningfully improve VQA\nmodel accuracy as well as performance on several Right-for-the-Right-Reason\n(RRR) metrics by optimizing for four key model objectives: (1) accurate\npredictions given limited but sufficient information (Sufficiency); (2)\nmax-entropy predictions given no important information (Uncertainty); (3)\ninvariance of predictions to changes in unimportant features (Invariance); and\n(4) alignment between model FI explanations and human FI explanations\n(Plausibility). Our best performing method, Visual Feature Importance\nSupervision (VisFIS), outperforms strong baselines on benchmark VQA datasets in\nterms of both in-distribution and out-of-distribution accuracy. While past work\nsuggests that the mechanism for improved accuracy is through improved\nexplanation plausibility, we show that this relationship depends crucially on\nexplanation faithfulness (whether explanations truly represent the model's\ninternal reasoning). Predictions are more accurate when explanations are\nplausible and faithful, and not when they are plausible but not faithful.\nLastly, we show that, surprisingly, RRR metrics are not predictive of\nout-of-distribution model accuracy when controlling for a model's\nin-distribution accuracy, which calls into question the value of these metrics\nfor evaluating model reasoning. All supporting code is available at\nhttps://github.com/zfying/visfis\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ying_Z/0/1/0/all/0/1\">Zhuofan Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1\">Peter Hase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-Training a Graph Recurrent Network for Language Representation. (arXiv:2209.03834v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.03834","description":"<p>Transformer-based pre-trained models have gained much advance in recent\nyears, becoming one of the most important backbones in natural language\nprocessing. Recent work shows that the attention mechanism inside Transformer\nmay not be necessary, both convolutional neural networks and multi-layer\nperceptron based models have also been investigated as Transformer\nalternatives. In this paper, we consider a graph recurrent network for language\nmodel pre-training, which builds a graph structure for each sequence with local\ntoken-level communications, together with a sentence-level representation\ndecoupled from other tokens. The original model performs well in\ndomain-specific text classification under supervised training, however, its\npotential in learning transfer knowledge by self-supervised way has not been\nfully exploited. We fill this gap by optimizing the architecture and verifying\nits effectiveness in more general language understanding tasks, for both\nEnglish and Chinese languages. As for model efficiency, instead of the\nquadratic complexity in Transformer-based models, our model has linear\ncomplexity and performs more efficiently during inference. Moreover, we find\nthat our model can generate more diverse outputs with less contextualized\nfeature redundancy than existing attention-based models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yile Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1\">Zhiyang Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlexER: Flexible Entity Resolution for Multiple Intents. (arXiv:2209.07569v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.07569","description":"<p>Entity resolution, a longstanding problem of data cleaning and integration,\naims at identifying data records that represent the same real-world entity.\nExisting approaches treat entity resolution as a universal task, assuming the\nexistence of a single interpretation of a real-world entity and focusing only\non finding matched records, separating corresponding from non-corresponding\nones, with respect to this single interpretation. However, in real-world\nscenarios, where entity resolution is part of a more general data project,\ndownstream applications may have varying interpretations of real-world entities\nrelating, for example, to various user needs. In what follows, we introduce the\nproblem of multiple intents entity resolution (MIER), an extension to the\nuniversal (single intent) entity resolution task. As a solution, we propose\nFlexER, utilizing contemporary solutions to universal entity resolution tasks\nto solve multiple intents entity resolution. FlexER addresses the problem as a\nmulti-label classification problem. It combines intent-based representations of\ntuple pairs using a multiplex graph representation that serves as an input to a\ngraph neural network (GNN). FlexER learns intent representations and improves\nthe outcome to multiple resolution problems. A large-scale empirical evaluation\nintroduces a new benchmark and, using also two well-known benchmarks, shows\nthat FlexER effectively solves the MIER problem and outperforms the\nstate-of-the-art for a universal entity resolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Genossar_B/0/1/0/all/0/1\">Bar Genossar</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Shraga_R/0/1/0/all/0/1\">Roee Shraga</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Gal_A/0/1/0/all/0/1\">Avigdor Gal</a> (1) ((1) Technion - Israel Institute of Technology, (2) Northeastern University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Who is GPT-3? An Exploration of Personality, Values and Demographics. (arXiv:2209.14338v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.14338","description":"<p>Language models such as GPT-3 have caused a furore in the research community.\nSome studies found that GPT-3 has some creative abilities and makes mistakes\nthat are on par with human behaviour. This paper answers a related question:\nWho is GPT-3? We administered two validated measurement tools to GPT-3 to\nassess its personality, the values it holds and its self-reported demographics.\nOur results show that GPT-3 scores similarly to human samples in terms of\npersonality and - when provided with a model response memory - in terms of the\nvalues it holds. We provide the first evidence of psychological assessment of\nthe GPT-3 model and thereby add to our understanding of this language model. We\nclose with suggestions for future research that moves social science closer to\nlanguage models and vice versa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miotto_M/0/1/0/all/0/1\">Maril&#xf9; Miotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossberg_N/0/1/0/all/0/1\">Nicola Rossberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinberg_B/0/1/0/all/0/1\">Bennett Kleinberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not All Errors are Equal: Learning Text Generation Metrics using Stratified Error Synthesis. (arXiv:2210.05035v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05035","description":"<p>Is it possible to build a general and automatic natural language generation\n(NLG) evaluation metric? Existing learned metrics either perform\nunsatisfactorily or are restricted to tasks where large human rating data is\nalready available. We introduce SESCORE, a model-based metric that is highly\ncorrelated with human judgements without requiring human annotation, by\nutilizing a novel, iterative error synthesis and severity scoring pipeline.\nThis pipeline applies a series of plausible errors to raw text and assigns\nseverity labels by simulating human judgements with entailment. We evaluate\nSESCORE against existing metrics by comparing how their scores correlate with\nhuman ratings. SESCORE outperforms all prior unsupervised metrics on multiple\ndiverse NLG tasks including machine translation, image captioning, and WebNLG\ntext generation. For WMT 20/21 En-De and Zh-En, SESCORE improve the average\nKendall correlation with human judgement from 0.154 to 0.195. SESCORE even\nachieves comparable performance to the best supervised metric COMET, despite\nreceiving no human-annotated training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenda Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuan_Y/0/1/0/all/0/1\">Yilin Tuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Utility of Self-supervised Models for Prosody-related Tasks. (arXiv:2210.07185v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07185","description":"<p>Self-Supervised Learning (SSL) from speech data has produced models that have\nachieved remarkable performance in many tasks, and that are known to implicitly\nrepresent many aspects of information latently present in speech signals.\nHowever, relatively little is known about the suitability of such models for\nprosody-related tasks or the extent to which they encode prosodic information.\nWe present a new evaluation framework, SUPERB-prosody, consisting of three\nprosody-related downstream tasks and two pseudo tasks. We find that 13 of the\n15 SSL models outperformed the baseline on all the prosody-related tasks. We\nalso show good performance on two pseudo tasks: prosody reconstruction and\nfuture prosody prediction. We further analyze the layerwise contributions of\nthe SSL models. Overall we conclude that SSL speech models are highly effective\nfor prosody-related tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guan-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chi-Luen Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei-Ping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_Y/0/1/0/all/0/1\">Yuan Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tzu-Han Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen-An Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ward_N/0/1/0/all/0/1\">Nigel G. Ward</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models. (arXiv:2210.08933v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08933","description":"<p>Recently, diffusion models have emerged as a new paradigm for generative\nmodels. Despite the success in domains using continuous signals such as vision\nand audio, adapting diffusion models to natural language is difficult due to\nthe discrete nature of text. We tackle this challenge by proposing DiffuSeq: a\ndiffusion model designed for sequence-to-sequence (Seq2Seq) text generation\ntasks. Upon extensive evaluation over a wide range of Seq2Seq tasks, we find\nDiffuSeq achieving comparable or even better performance than six established\nbaselines, including a state-of-the-art model that is based on pre-trained\nlanguage models. Apart from quality, an intriguing property of DiffuSeq is its\nhigh diversity during generation, which is desired in many Seq2Seq tasks. We\nfurther include a theoretical analysis revealing the connection between\nDiffuSeq and autoregressive/non-autoregressive models. Bringing together\ntheoretical analysis and empirical evidence, we demonstrate the great potential\nof diffusion models in complex conditional language generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1\">Shansan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mukai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiangtao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Soft-Labeled Contrastive Pre-training for Function-level Code Representation. (arXiv:2210.09597v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.09597","description":"<p>Code contrastive pre-training has recently achieved significant progress on\ncode-related tasks. In this paper, we present \\textbf{SCodeR}, a\n\\textbf{S}oft-labeled contrastive pre-training framework with two positive\nsample construction methods to learn functional-level \\textbf{Code}\n\\textbf{R}epresentation. Considering the relevance between codes in a\nlarge-scale code corpus, the soft-labeled contrastive pre-training can obtain\nfine-grained soft-labels through an iterative adversarial manner and use them\nto learn better code representation. The positive sample construction is\nanother key for contrastive pre-training. Previous works use\ntransformation-based methods like variable renaming to generate semantically\nequal positive codes. However, they usually result in the generated code with a\nhighly similar surface form, and thus mislead the model to focus on superficial\ncode structure instead of code semantics. To encourage SCodeR to capture\nsemantic information from the code, we utilize code comments and abstract\nsyntax sub-trees of the code to build positive samples. We conduct experiments\non four code-related tasks over seven datasets. Extensive experimental results\nshow that SCodeR achieves new state-of-the-art performance on all of them,\nwhich illustrates the effectiveness of the proposed pre-training method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaonan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Daya Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Document Selection for Efficient Encoder Pretraining. (arXiv:2210.10951v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10951","description":"<p>Building pretrained language models is considered expensive and\ndata-intensive, but must we increase dataset size to achieve better\nperformance? We propose an alternative to larger training sets by automatically\nidentifying smaller yet domain-representative subsets. We extend Cynical Data\nSelection, a statistical sentence scoring method that conditions on a\nrepresentative target domain corpus. As an example, we treat the OntoNotes\ncorpus as a target domain and pretrain a RoBERTa-like encoder from a cynically\nselected subset of the Pile. On both perplexity and across several downstream\ntasks in the target domain, it consistently outperforms random selection with\n20x less data, 3x fewer training iterations, and 2x less estimated cloud\ncompute cost, validating the recipe of automatic document selection for LM\npretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yukun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1\">Patrick Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Spatial Description: Controlled Spatial-Oriented Image-to-Text Generation. (arXiv:2210.11109v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.11109","description":"<p>Image-to-text tasks, such as open-ended image captioning and controllable\nimage description, have received extensive attention for decades. Here, we\nfurther advance this line of work by presenting Visual Spatial Description\n(VSD), a new perspective for image-to-text toward spatial semantics. Given an\nimage and two objects inside it, VSD aims to produce one description focusing\non the spatial perspective between the two objects. Accordingly, we manually\nannotate a dataset to facilitate the investigation of the newly-introduced task\nand build several benchmark encoder-decoder models by using VL-BART and VL-T5\nas backbones. In addition, we investigate pipeline and joint end-to-end\narchitectures for incorporating visual spatial relationship classification\n(VSRC) information into our model. Finally, we conduct experiments on our\nbenchmark dataset to evaluate all our models. Results show that our models are\nimpressive, providing accurate and human-like spatial-oriented text\ndescriptions. Meanwhile, VSRC has great potential for VSD, and the joint\nend-to-end architecture is the better choice for their integration. We make the\ndataset and codes public for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jianguo Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhichao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yueheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empathetic Dialogue Generation via Sensitive Emotion Recognition and Sensible Knowledge Selection. (arXiv:2210.11715v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.11715","description":"<p>Empathy, which is widely used in psychological counselling, is a key trait of\neveryday human conversations. Equipped with commonsense knowledge, current\napproaches to empathetic response generation focus on capturing implicit\nemotion within dialogue context, where the emotions are treated as a static\nvariable throughout the conversations. However, emotions change dynamically\nbetween utterances, which makes previous works difficult to perceive the\nemotion flow and predict the correct emotion of the target response, leading to\ninappropriate response. Furthermore, simply importing commonsense knowledge\nwithout harmonization may trigger the conflicts between knowledge and emotion,\nwhich confuse the model to choose incorrect information to guide the generation\nprocess. To address the above problems, we propose a Serial Encoding and\nEmotion-Knowledge interaction (SEEK) method for empathetic dialogue generation.\nWe use a fine-grained encoding strategy which is more sensitive to the emotion\ndynamics (emotion flow) in the conversations to predict the emotion-intent\ncharacteristic of response. Besides, we design a novel framework to model the\ninteraction between knowledge and emotion to generate more sensible response.\nExtensive experiments on EmpatheticDialogues demonstrate that SEEK outperforms\nthe strong baselines in both automatic and manual evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lanrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenxu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrence Boosts Diversity! Revisiting Recurrent Latent Variable in Transformer-Based Variational AutoEncoder for Diverse Text Generation. (arXiv:2210.12409v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12409","description":"<p>Variational Auto-Encoder (VAE) has been widely adopted in text generation.\nAmong many variants, recurrent VAE learns token-wise latent variables with each\nconditioned on the preceding ones, which captures sequential variability better\nin the era of RNN. However, it is unclear how to incorporate such recurrent\ndynamics into the recently dominant Transformer due to its parallelism. In this\nwork, we propose TRACE, a Transformer-based recurrent VAE structure. TRACE\nimposes recurrence on segment-wise latent variables with arbitrarily separated\ntext segments and constructs the posterior distribution with residual\nparameterization. Besides, we design an acceleration method by approximating\nidempotent matrices, which allows parallelism while maintaining the conditional\ndependence of latent variables. We demonstrate that TRACE could enhance the\nentanglement of each segment and preceding latent variables and deduce a\nnon-zero lower bound of the KL term, providing a theoretical guarantee of\ngeneration diversity. Experiments on two unconditional and one conditional\ngeneration tasks show that TRACE achieves significantly improved diversity\nwhile maintaining satisfactory generation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1\">Xiaoyuan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECTSum: A New Benchmark Dataset For Bullet Point Summarization of Long Earnings Call Transcripts. (arXiv:2210.12467v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12467","description":"<p>Despite tremendous progress in automatic summarization, state-of-the-art\nmethods are predominantly trained to excel in summarizing short newswire\narticles, or documents with strong layout biases such as scientific articles or\ngovernment reports. Efficient techniques to summarize financial documents,\nincluding facts and figures, have largely been unexplored, majorly due to the\nunavailability of suitable datasets. In this work, we present ECTSum, a new\ndataset with transcripts of earnings calls (ECTs), hosted by publicly traded\ncompanies, as documents, and short experts-written telegram-style bullet point\nsummaries derived from corresponding Reuters articles. ECTs are long\nunstructured documents without any prescribed length limit or format. We\nbenchmark our dataset with state-of-the-art summarizers across various metrics\nevaluating the content quality and factual consistency of the generated\nsummaries. Finally, we present a simple-yet-effective approach, ECT-BPS, to\ngenerate a set of bullet points that precisely capture the important facts\ndiscussed in the calls.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_R/0/1/0/all/0/1\">Rajdeep Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohra_A/0/1/0/all/0/1\">Abhinav Bohra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1\">Akash Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Soumya Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_M/0/1/0/all/0/1\">Manjunath Hegde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_A/0/1/0/all/0/1\">Afreen Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_S/0/1/0/all/0/1\">Shivani Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_K/0/1/0/all/0/1\">Koustuv Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_N/0/1/0/all/0/1\">Niloy Ganguly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Saptarshi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focus Is What You Need For Chinese Grammatical Error Correction. (arXiv:2210.12692v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12692","description":"<p>Chinese Grammatical Error Correction (CGEC) aims to automatically detect and\ncorrect grammatical errors contained in Chinese text. In the long term,\nresearchers regard CGEC as a task with a certain degree of uncertainty, that\nis, an ungrammatical sentence may often have multiple references. However, we\nargue that even though this is a very reasonable hypothesis, it is too harsh\nfor the intelligence of the mainstream models in this era. In this paper, we\nfirst discover that multiple references do not actually bring positive gains to\nmodel training. On the contrary, it is beneficial to the CGEC model if the\nmodel can pay attention to small but essential data during the training\nprocess. Furthermore, we propose a simple yet effective training strategy\ncalled OneTarget to improve the focus ability of the CGEC models and thus\nimprove the CGEC performance. Extensive experiments and detailed analyses\ndemonstrate the correctness of our discovery and the effectiveness of our\nproposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jingheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shirong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Rui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity-level Sentiment Analysis in Contact Center Telephone Conversations. (arXiv:2210.13401v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13401","description":"<p>Entity-level sentiment analysis predicts the sentiment about entities\nmentioned in a given text. It is very useful in a business context to\nunderstand user emotions towards certain entities, such as products or\ncompanies. In this paper, we demonstrate how we developed an entity-level\nsentiment analysis system that analyzes English telephone conversation\ntranscripts in contact centers to provide business insight. We present two\napproaches, one entirely based on the transformer-based DistilBERT model, and\nanother that uses a convolutional neural network supplemented with some\nheuristic rules.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xue-Yong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskar_M/0/1/0/all/0/1\">Md Tahmid Rahman Laskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardiner_S/0/1/0/all/0/1\">Shayna Gardiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hiranandani_P/0/1/0/all/0/1\">Pooja Hiranandani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+TN_S/0/1/0/all/0/1\">Shashi Bhushan TN</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapters for Enhanced Modeling of Multilingual Knowledge and Text. (arXiv:2210.13617v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13617","description":"<p>Large language models appear to learn facts from the large text corpora they\nare trained on. Such facts are encoded implicitly within their many parameters,\nmaking it difficult to verify or manipulate what knowledge has been learned.\nLanguage models have recently been extended to multilingual language models\n(MLLMs), enabling knowledge to be learned across hundreds of languages.\nMeanwhile, knowledge graphs contain facts in an explicit triple format, which\nrequire careful and costly curation and are only available in a few\nhigh-resource languages, restricting their research and application. To address\nthese issues, we propose to enhance MLLMs with knowledge from multilingual\nknowledge graphs (MLKGs) so as to tackle language and knowledge graph tasks\nacross many languages, including low-resource ones. Specifically, we introduce\na lightweight adapter set to enhance MLLMs with cross-lingual entity alignment\nand facts from MLKGs for many languages. Experiments on common benchmarks show\nthat such enhancement benefits both MLLMs and MLKGs, achieving: (1) comparable\nor improved performance for knowledge graph completion and entity alignment\nrelative to baselines, especially for low-resource languages (for which\nknowledge graphs are unavailable); and (2) improved MLLM performance on\nlanguage understanding tasks that require multilingual factual knowledge; all\nwhile maintaining performance on other general language tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yifan Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Meizhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allen_C/0/1/0/all/0/1\">Carl Allen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Relation Classification via Efficient and Effective Prompting. (arXiv:2210.13838v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13838","description":"<p>Prompting pre-trained language models has achieved impressive performance on\nvarious NLP tasks, especially in low data regimes. Despite the success of\nprompting in monolingual settings, applying prompt-based methods in\nmultilingual scenarios has been limited to a narrow set of tasks, due to the\nhigh cost of handcrafting multilingual prompts. In this paper, we present the\nfirst work on prompt-based multilingual relation classification (RC), by\nintroducing an efficient and effective method that constructs prompts from\nrelation triples and involves only minimal translation for the class labels. We\nevaluate its performance in fully supervised, few-shot and zero-shot scenarios,\nand analyze its effectiveness across 14 languages, prompt variants, and\nEnglish-task training in cross-lingual settings. We find that in both fully\nsupervised and few-shot scenarios, our prompt method beats competitive\nbaselines: fine-tuning XLM-R_EM and null prompts. It also outperforms the\nrandom baseline by a large margin in zero-shot experiments. Our method requires\nlittle in-language knowledge and can be used as a strong baseline for similar\nmultilingual classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harbecke_D/0/1/0/all/0/1\">David Harbecke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennig_L/0/1/0/all/0/1\">Leonhard Hennig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentially Private Language Models for Secure Data Sharing. (arXiv:2210.13918v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.13918","description":"<p>To protect the privacy of individuals whose data is being shared, it is of\nhigh importance to develop methods allowing researchers and companies to\nrelease textual data while providing formal privacy guarantees to its\noriginators. In the field of NLP, substantial efforts have been directed at\nbuilding mechanisms following the framework of local differential privacy,\nthereby anonymizing individual text samples before releasing them. In practice,\nthese approaches are often dissatisfying in terms of the quality of their\noutput language due to the strong noise required for local differential\nprivacy. In this paper, we approach the problem at hand using global\ndifferential privacy, particularly by training a generative language model in a\ndifferentially private manner and consequently sampling data from it. Using\nnatural language prompts and a new prompt-mismatch loss, we are able to create\nhighly accurate and fluent textual datasets taking on specific desired\nattributes such as sentiment or topic and resembling statistical properties of\nthe training data. We perform thorough experiments indicating that our\nsynthetic datasets do not leak information from our original data and are of\nhigh language quality and highly suitable for training models for further\nanalysis on real-world data. Notably, we also demonstrate that training\nclassifiers on private synthetic data outperforms directly training classifiers\non real data with DP-SGD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mattern_J/0/1/0/all/0/1\">Justus Mattern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weggenmann_B/0/1/0/all/0/1\">Benjamin Weggenmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoelkopf_B/0/1/0/all/0/1\">Bernhard Schoelkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding. (arXiv:2210.14169v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.14169","description":"<p>Dialogue understanding tasks often necessitate abundant annotated data to\nachieve good performance and that presents challenges in low-resource settings.\nTo alleviate this barrier, we explore few-shot data augmentation for dialogue\nunderstanding by prompting large pre-trained language models and present a\nnovel approach that iterates on augmentation quality by applying\nweakly-supervised filters. We evaluate our methods on the emotion and act\nclassification tasks in DailyDialog and the intent classification task in\nFacebook Multilingual Task-Oriented Dialogue. Models fine-tuned on our\naugmented data mixed with few-shot ground truth data are able to approach or\nsurpass existing state-of-the-art performance on both datasets. For DailyDialog\nspecifically, using 10% of the ground truth data we outperform the current\nstate-of-the-art model which uses 100% of the data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Maximillian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papangelis_A/0/1/0/all/0/1\">Alexandros Papangelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chenyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenbaum_A/0/1/0/all/0/1\">Andy Rosenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-10-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}