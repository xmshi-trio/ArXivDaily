{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-04-19T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Researchers eye-view of sarcasm detection in social media textual content. (arXiv:2304.08582v1 [cs.CL])","link":"http://arxiv.org/abs/2304.08582","description":"<p>The enormous use of sarcastic text in all forms of communication in social\nmedia will have a physiological effect on target users. Each user has a\ndifferent approach to misusing and recognising sarcasm. Sarcasm detection is\ndifficult even for users, and this will depend on many things such as\nperspective, context, special symbols. So, that will be a challenging task for\nmachines to differentiate sarcastic sentences from non-sarcastic sentences.\nThere are no exact rules based on which model will accurately detect sarcasm\nfrom many text corpus in the current situation. So, one needs to focus on\noptimistic and forthcoming approaches in the sarcasm detection domain. This\npaper discusses various sarcasm detection techniques and concludes with some\napproaches, related datasets with optimal features, and the researcher's\nchallenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mane_S/0/1/0/all/0/1\">Swapnil Mane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khatavkar_V/0/1/0/all/0/1\">Vaibhav Khatavkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Scene Text Recognition for Character-Level Long-Tailed Distribution. (arXiv:2304.08592v1 [cs.CV])","link":"http://arxiv.org/abs/2304.08592","description":"<p>Despite the recent remarkable improvements in scene text recognition (STR),\nthe majority of the studies focused mainly on the English language, which only\nincludes few number of characters. However, STR models show a large performance\ndegradation on languages with a numerous number of characters (e.g., Chinese\nand Korean), especially on characters that rarely appear due to the long-tailed\ndistribution of characters in such languages. To address such an issue, we\nconducted an empirical analysis using synthetic datasets with different\ncharacter-level distributions (e.g., balanced and long-tailed distributions).\nWhile increasing a substantial number of tail classes without considering the\ncontext helps the model to correctly recognize characters individually,\ntraining with such a synthetic dataset interferes the model with learning the\ncontextual information (i.e., relation among characters), which is also\nimportant for predicting the whole word. Based on this motivation, we propose a\nnovel Context-Aware and Free Experts Network (CAFE-Net) using two experts: 1)\ncontext-aware expert learns the contextual representation trained with a\nlong-tailed dataset composed of common words used in everyday life and 2)\ncontext-free expert focuses on correctly predicting individual characters by\nutilizing a dataset with a balanced number of characters. By training two\nexperts to focus on learning contextual and visual representations,\nrespectively, we propose a novel confidence ensemble method to compensate the\nlimitation of each expert. Through the experiments, we demonstrate that\nCAFE-Net improves the STR performance on languages containing numerous number\nof characters. Moreover, we show that CAFE-Net is easily applicable to various\nSTR models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1\">Sunghyo Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungsoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging Discrete and Backpropagation: Straight-Through and Beyond. (arXiv:2304.08612v1 [cs.LG])","link":"http://arxiv.org/abs/2304.08612","description":"<p>Backpropagation, the cornerstone of deep learning, is limited to computing\ngradients solely for continuous variables. This limitation hinders various\nresearch on problems involving discrete latent variables. To address this\nissue, we propose a novel approach for approximating the gradient of parameters\ninvolved in generating discrete latent variables. First, we examine the widely\nused Straight-Through (ST) heuristic and demonstrate that it works as a\nfirst-order approximation of the gradient. Guided by our findings, we propose a\nnovel method called ReinMax, which integrates Heun's Method, a second-order\nnumerical method for solving ODEs, to approximate the gradient. Our method\nachieves second-order accuracy without requiring Hessian or other second-order\nderivatives. We conduct experiments on structured output prediction and\nunsupervised generative modeling tasks. Our results show that \\ours brings\nconsistent improvements over the state of the art, including ST and\nStraight-Through Gumbel-Softmax. Implementations are released at\nhttps://github.com/microsoft/ReinMax.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chengyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Evaluation on Large Language Model Outputs: Discourse and Memorization. (arXiv:2304.08637v1 [cs.CL])","link":"http://arxiv.org/abs/2304.08637","description":"<p>We present an empirical evaluation of various outputs generated by nine of\nthe most widely-available large language models (LLMs). Our analysis is done\nwith off-the-shelf, readily-available tools. We find a correlation between\npercentage of memorized text, percentage of unique text, and overall output\nquality, when measured with respect to output pathologies such as\ncounterfactual and logically-flawed statements, and general failures like not\nstaying on topic. Overall, 80.0% of the outputs evaluated contained memorized\ndata, but outputs containing the most memorized content were also more likely\nto be considered of high quality. We discuss and evaluate mitigation\nstrategies, showing that, in the models evaluated, the rate of memorized text\nbeing output is reduced. We conclude with a discussion on potential\nimplications around what it means to learn, to memorize, and to evaluate\nquality text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wynter_A/0/1/0/all/0/1\">Adrian de Wynter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolov_A/0/1/0/all/0/1\">Alex Sokolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Qilong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si-Qing Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of US Supreme Court Cases using BERT-Based Techniques. (arXiv:2304.08649v1 [cs.CL])","link":"http://arxiv.org/abs/2304.08649","description":"<p>Models based on bidirectional encoder representations from transformers\n(BERT) produce state of the art (SOTA) results on many natural language\nprocessing (NLP) tasks such as named entity recognition (NER), part-of-speech\n(POS) tagging etc. An interesting phenomenon occurs when classifying long\ndocuments such as those from the US supreme court where BERT-based models can\nbe considered difficult to use on a first-pass or out-of-the-box basis. In this\npaper, we experiment with several BERT-based classification techniques for US\nsupreme court decisions or supreme court database (SCDB) and compare them with\nthe previous SOTA results. We then compare our results specifically with SOTA\nmodels for long documents. We compare our results for two classification tasks:\n(1) a broad classification task with 15 categories and (2) a fine-grained\nclassification task with 279 categories. Our best result produces an accuracy\nof 80\\% on the 15 broad categories and 60\\% on the fine-grained 279 categories\nwhich marks an improvement of 8\\% and 28\\% respectively from previously\nreported SOTA results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vatsal_S/0/1/0/all/0/1\">Shubham Vatsal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyers_A/0/1/0/all/0/1\">Adam Meyers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_J/0/1/0/all/0/1\">John Ortega</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study. (arXiv:2304.08653v1 [cs.CL])","link":"http://arxiv.org/abs/2304.08653","description":"<p>Modern deep models for summarization attains impressive benchmark\nperformance, but they are prone to generating miscalibrated predictive\nuncertainty. This means that they assign high confidence to low-quality\npredictions, leading to compromised reliability and trustworthiness in\nreal-world applications. Probabilistic deep learning methods are common\nsolutions to the miscalibration problem. However, their relative effectiveness\nin complex autoregressive summarization tasks are not well-understood. In this\nwork, we thoroughly investigate different state-of-the-art probabilistic\nmethods' effectiveness in improving the uncertainty quality of the neural\nsummarization models, across three large-scale benchmarks with varying\ndifficulty. We show that the probabilistic methods consistently improve the\nmodel's generation and uncertainty quality, leading to improved selective\ngeneration performance (i.e., abstaining from low-quality summaries) in\npractice. We also reveal notable failure patterns of probabilistic methods\nwidely-adopted in NLP community (e.g., Deep Ensemble and Monte Carlo Dropout),\ncautioning the importance of choosing appropriate method for the data setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zablotskaia_P/0/1/0/all/0/1\">Polina Zablotskaia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_D/0/1/0/all/0/1\">Du Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maynez_J/0/1/0/all/0/1\">Joshua Maynez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_S/0/1/0/all/0/1\">Shashi Narayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jeremiah Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Biomedical Text Summarization with Pre-trained Language Model. (arXiv:2304.08763v1 [cs.CL])","link":"http://arxiv.org/abs/2304.08763","description":"<p>The exponential growth of biomedical texts such as biomedical literature and\nelectronic health records (EHRs), provides a big challenge for clinicians and\nresearchers to access clinical information efficiently. To address the problem,\nbiomedical text summarization has been proposed to support clinical information\nretrieval and management, aiming at generating concise summaries that distill\nkey information from single or multiple biomedical documents. In recent years,\npre-trained language models (PLMs) have been the de facto standard of various\nnatural language processing tasks in the general domain. Most recently, PLMs\nhave been further investigated in the biomedical field and brought new insights\ninto the biomedical text summarization task. In this paper, we systematically\nsummarize recent advances that explore PLMs for biomedical text summarization,\nto help understand recent progress, challenges, and future directions. We\ncategorize PLMs-based approaches according to how they utilize PLMs and what\nPLMs they use. We then review available datasets, recent approaches and\nevaluation metrics of the task. We finally discuss existing challenges and\npromising future directions. To facilitate the research community, we line up\nopen resources including available datasets, recent approaches, codes,\nevaluation metrics, and the leaderboard in a public project:\nhttps://github.com/KenZLuo/Biomedical-Text-Summarization-Survey/tree/master.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zheheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benyou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1\">Sophia Ananiadou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speaker Profiling in Multiparty Conversations. (arXiv:2304.08801v1 [cs.CL])","link":"http://arxiv.org/abs/2304.08801","description":"<p>In conversational settings, individuals exhibit unique behaviors, rendering a\none-size-fits-all approach insufficient for generating responses by dialogue\nagents. Although past studies have aimed to create personalized dialogue agents\nusing speaker persona information, they have relied on the assumption that the\nspeaker's persona is already provided. However, this assumption is not always\nvalid, especially when it comes to chatbots utilized in industries like\nbanking, hotel reservations, and airline bookings. This research paper aims to\nfill this gap by exploring the task of Speaker Profiling in Conversations\n(SPC). The primary objective of SPC is to produce a summary of persona\ncharacteristics for each individual speaker present in a dialogue. To\naccomplish this, we have divided the task into three subtasks: persona\ndiscovery, persona-type identification, and persona-value extraction. Given a\ndialogue, the first subtask aims to identify all utterances that contain\npersona information. Subsequently, the second task evaluates these utterances\nto identify the type of persona information they contain, while the third\nsubtask identifies the specific persona values for each identified type. To\naddress the task of SPC, we have curated a new dataset named SPICE, which comes\nwith specific labels. We have evaluated various baselines on this dataset and\nbenchmarked it with a new neural model, SPOT, which we introduce in this paper.\nFurthermore, we present a comprehensive analysis of SPOT, examining the\nlimitations of individual modules both quantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shivani Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rishabh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the Role of Similarity and Dissimilarity inBest Counter Argument Retrieval. (arXiv:2304.08807v1 [cs.CL])","link":"http://arxiv.org/abs/2304.08807","description":"<p>This paper studies the task of best counter-argument retrieval given an input\nargument. Following the definition that the best counter-argument addresses the\nsame aspects as the input argument while having the opposite stance, we aim to\ndevelop an efficient and effective model for scoring counter-arguments based on\nsimilarity and dissimilarity metrics. We first conduct an experimental study on\nthe effectiveness of available scoring methods, including traditional\nLearning-To-Rank (LTR) and recent neural scoring models. We then propose\nBipolar-encoder, a novel BERT-based model to learn an optimal representation\nfor simultaneous similarity and dissimilarity. Experimental results show that\nour proposed method can achieve the accuracy@1 of 88.9\\%, which significantly\noutperforms other baselines by a large margin. When combined with an\nappropriate caching technique, Bipolar-encoder is comparably efficient at\nprediction time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hongguang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shuirong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cam-Tu Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TTIDA: Controllable Generative Data Augmentation via Text-to-Text and Text-to-Image Models. (arXiv:2304.08821v1 [cs.CV])","link":"http://arxiv.org/abs/2304.08821","description":"<p>Data augmentation has been established as an efficacious approach to\nsupplement useful information for low-resource datasets. Traditional\naugmentation techniques such as noise injection and image transformations have\nbeen widely used. In addition, generative data augmentation (GDA) has been\nshown to produce more diverse and flexible data. While generative adversarial\nnetworks (GANs) have been frequently used for GDA, they lack diversity and\ncontrollability compared to text-to-image diffusion models. In this paper, we\npropose TTIDA (Text-to-Text-to-Image Data Augmentation) to leverage the\ncapabilities of large-scale pre-trained Text-to-Text (T2T) and Text-to-Image\n(T2I) generative models for data augmentation. By conditioning the T2I model on\ndetailed descriptions produced by T2T models, we are able to generate\nphoto-realistic labeled images in a flexible and controllable manner.\nExperiments on in-domain classification, cross-domain classification, and image\ncaptioning tasks show consistent improvements over other data augmentation\nbaselines. Analytical studies in varied settings, including few-shot,\nlong-tail, and adversarial, further reinforce the effectiveness of TTIDA in\nenhancing performance and increasing robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yuwei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1\">Jean Kaddour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yixin Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenguang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer to a Low-Resource Language via Close Relatives: The Case Study on Faroese. (arXiv:2304.08823v1 [cs.CL])","link":"http://arxiv.org/abs/2304.08823","description":"<p>Multilingual language models have pushed state-of-the-art in cross-lingual\nNLP transfer. The majority of zero-shot cross-lingual transfer, however, use\none and the same massively multilingual transformer (e.g., mBERT or XLM-R) to\ntransfer to all target languages, irrespective of their typological,\netymological, and phylogenetic relations to other languages. In particular,\nreadily available data and models of resource-rich sibling languages are often\nignored. In this work, we empirically show, in a case study for Faroese -- a\nlow-resource language from a high-resource language family -- that by\nleveraging the phylogenetic information and departing from the\n'one-size-fits-all' paradigm, one can improve cross-lingual transfer to\nlow-resource languages. In particular, we leverage abundant resources of other\nScandinavian languages (i.e., Danish, Norwegian, Swedish, and Icelandic) for\nthe benefit of Faroese. Our evaluation results show that we can substantially\nimprove the transfer performance to Faroese by exploiting data and models of\nclosely-related high-resource languages. Further, we release a new web corpus\nof Faroese and Faroese datasets for named entity recognition (NER), semantic\ntext similarity (STS), and new language models trained on all Scandinavian\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Snaebjarnarson_V/0/1/0/all/0/1\">V&#xe9;steinn Sn&#xe6;bjarnarson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonsen_A/0/1/0/all/0/1\">Annika Simonsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Approximate Nearest Neighbour Phrase Mining for Contextual Speech Recognition. (arXiv:2304.08862v1 [cs.CL])","link":"http://arxiv.org/abs/2304.08862","description":"<p>This paper presents an extension to train end-to-end Context-Aware\nTransformer Transducer ( CATT ) models by using a simple, yet efficient method\nof mining hard negative phrases from the latent space of the context encoder.\nDuring training, given a reference query, we mine a number of similar phrases\nusing approximate nearest neighbour search. These sampled phrases are then used\nas negative examples in the context list alongside random and ground truth\ncontextual information. By including approximate nearest neighbour phrases\n(ANN-P) in the context list, we encourage the learned representation to\ndisambiguate between similar, but not identical, biasing phrases. This improves\nbiasing accuracy when there are several similar phrases in the biasing\ninventory. We carry out experiments in a large-scale data regime obtaining up\nto 7% relative word error rate reductions for the contextual portion of test\ndata. We also extend and evaluate CATT approach in streaming applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bleeker_M/0/1/0/all/0/1\">Maurits Bleeker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swietojanski_P/0/1/0/all/0/1\">Pawel Swietojanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braun_S/0/1/0/all/0/1\">Stefan Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiaodan Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Romanization-based Large-scale Adaptation of Multilingual Language Models. (arXiv:2304.08865v1 [cs.CL])","link":"http://arxiv.org/abs/2304.08865","description":"<p>Large multilingual pretrained language models (mPLMs) have become the de\nfacto state of the art for cross-lingual transfer in NLP. However, their\nlarge-scale deployment to many languages, besides pretraining data scarcity, is\nalso hindered by the increase in vocabulary size and limitations in their\nparameter budget. In order to boost the capacity of mPLMs to deal with\nlow-resource and unseen languages, we explore the potential of leveraging\ntransliteration on a massive scale. In particular, we explore the UROMAN\ntransliteration tool, which provides mappings from UTF-8 to Latin characters\nfor all the writing systems, enabling inexpensive romanization for virtually\nany language. We first focus on establishing how UROMAN compares against other\nlanguage-specific and manually curated transliterators for adapting\nmultilingual PLMs. We then study and compare a plethora of data- and\nparameter-efficient strategies for adapting the mPLMs to romanized and\nnon-romanized corpora of 14 diverse low-resource languages. Our results reveal\nthat UROMAN-based transliteration can offer strong performance for many\nlanguages, with particular gains achieved in the most challenging setups: on\nlanguages with unseen scripts and with limited training data without any\nvocabulary augmentation. Further analyses reveal that an improved tokenizer\nbased on romanized data can even outperform non-transliteration-based methods\nin the majority of languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Purkayastha_S/0/1/0/all/0/1\">Sukannya Purkayastha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tailoring Domain Adaptation for Machine Translation Quality Estimation. (arXiv:2304.08891v1 [cs.CL])","link":"http://arxiv.org/abs/2304.08891","description":"<p>While quality estimation (QE) can play an important role in the translation\nprocess, its effectiveness relies on the availability and quality of training\ndata. For QE in particular, high-quality labeled data is often lacking due to\nthe high-cost and effort associated with labeling such data. Aside from the\ndata scarcity challenge, QE models should also be generalizable, i.e., they\nshould be able to handle data from different domains, both generic and\nspecific. To alleviate these two main issues -- data scarcity and domain\nmismatch -- this paper combines domain adaptation and data augmentation within\na robust QE system. Our method is to first train a generic QE model and then\nfine-tune it on a specific domain while retaining generic knowledge. Our\nresults show a significant improvement for all the language pairs investigated,\nbetter cross-lingual inference, and a superior performance in zero-shot\nlearning scenarios as compared to state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharami_J/0/1/0/all/0/1\">Javad Pourmostafa Roshan Sharami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shterionov_D/0/1/0/all/0/1\">Dimitar Shterionov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blain_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Blain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanmassenhove_E/0/1/0/all/0/1\">Eva Vanmassenhove</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sisto_M/0/1/0/all/0/1\">Mirella De Sisto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emmery_C/0/1/0/all/0/1\">Chris Emmery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spronck_P/0/1/0/all/0/1\">Pieter Spronck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Zero-Shot Personalized Table-to-Text Generation with Contrastive Persona Distillation. (arXiv:2304.08911v1 [cs.CL])","link":"http://arxiv.org/abs/2304.08911","description":"<p>Existing neural methods have shown great potentials towards generating\ninformative text from structured tabular data as well as maintaining high\ncontent fidelity. However, few of them shed light on generating personalized\nexpressions, which often requires well-aligned persona-table-text datasets that\nare difficult to obtain. To overcome these obstacles, we explore personalized\ntable-to-text generation under a zero-shot setting, by assuming no well-aligned\npersona-table-text triples are required during training. To this end, we\nfirstly collect a set of unpaired persona information and then propose a\nsemi-supervised approach with contrastive persona distillation (S2P-CPD) to\ngenerate personalized context. Specifically, tabular data and persona\ninformation are firstly represented as latent variables separately. Then, we\ndevise a latent space fusion technique to distill persona information into the\ntable representation. Besides, a contrastive-based discriminator is employed to\nguarantee the style consistency between the generated context and its\ncorresponding persona. Experimental results on two benchmarks demonstrate\nS2P-CPD's ability on keeping both content fidelity and personalized\nexpressions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1\">Haolan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xuming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shaobo Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhongzhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haiqing Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Textbooks with Visuals from the Web for Improved Learning. (arXiv:2304.08931v1 [cs.CV])","link":"http://arxiv.org/abs/2304.08931","description":"<p>Textbooks are the primary vehicle for delivering quality education to\nstudents. It has been shown that explanatory or illustrative visuals play a key\nrole in the retention, comprehension and the general transfer of knowledge.\nHowever, many textbooks, especially in the developing world, are low quality\nand lack interesting visuals to support student learning. In this paper, we\ninvestigate the effectiveness of vision-language models to automatically\nenhance textbooks with images from the web. Specifically, we collect a dataset\nof e-textbooks from one of the largest free online publishers in the world. We\nrigorously analyse the dataset, and use the resulting analysis to motivate a\ntask that involves retrieving and appropriately assigning web images to\ntextbooks, which we frame as a novel optimization problem. Through a\ncrowd-sourced evaluation, we verify that (1) while the original textbook images\nare rated higher, automatically assigned ones are not far behind, and (2) the\nchoice of the optimization problem matters. We release the dataset of textbooks\nwith an associated image bank to spur further research in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Janvijay Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zouhar_V/0/1/0/all/0/1\">Vil&#xe9;m Zouhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs. (arXiv:2304.08968v1 [cs.CL])","link":"http://arxiv.org/abs/2304.08968","description":"<p>The self-attention revolution allowed generative language models to scale and\nachieve increasingly impressive abilities. Such models - commonly referred to\nas Large Language Models (LLMs) - have recently gained prominence with the\ngeneral public, thanks to conversational fine-tuning, putting their behavior in\nline with public expectations regarding AI. This prominence amplified prior\nconcerns regarding the misuse of LLMs and led to the emergence of numerous\ntools to detect LLMs in the wild.\n</p>\n<p>Unfortunately, most such tools are critically flawed. While major\npublications in the LLM detectability field suggested that LLMs were easy to\ndetect with fine-tuned autoencoders, the limitations of their results are easy\nto overlook. Specifically, they assumed publicly available generative models\nwithout fine-tunes or non-trivial prompts. While the importance of these\nassumptions has been demonstrated, until now, it remained unclear how well such\ndetection could be countered.\n</p>\n<p>Here, we show that an attacker with access to such detectors' reference human\ntexts and output not only evades detection but can fully frustrate the detector\ntraining - with a reasonable budget and all its outputs labeled as such.\nAchieving it required combining common \"reinforcement from critic\" loss\nfunction modification and AdamW optimizer, which led to surprisingly good\nfine-tuning generalization. Finally, we warn against the temptation to\ntranspose the conclusions obtained in RNN-driven text GANs to LLMs due to their\nbetter representative ability.\n</p>\n<p>These results have critical implications for the detection and prevention of\nmalicious use of generative language models, and we hope they will aid the\ndesigners of generative models and detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henrique_D/0/1/0/all/0/1\">Da Silva Gameiro Henrique</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kucharavy_A/0/1/0/all/0/1\">Andrei Kucharavy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerraoui_R/0/1/0/all/0/1\">Rachid Guerraoui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MER 2023: Multi-label Learning, Modality Robustness, and Semi-Supervised Learning. (arXiv:2304.08981v1 [cs.CL])","link":"http://arxiv.org/abs/2304.08981","description":"<p>Over the past few decades, multimodal emotion recognition has made remarkable\nprogress with the development of deep learning. However, existing technologies\nare difficult to meet the demand for practical applications. To improve the\nrobustness, we launch a Multimodal Emotion Recognition Challenge (MER 2023) to\nmotivate global researchers to build innovative technologies that can further\naccelerate and foster research. For this year's challenge, we present three\ndistinct sub-challenges: (1) MER-MULTI, in which participants recognize both\ndiscrete and dimensional emotions; (2) MER-NOISE, in which noise is added to\ntest videos for modality robustness evaluation; (3) MER-SEMI, which provides\nlarge amounts of unlabeled samples for semi-supervised learning. In this paper,\nwe test a variety of multimodal features and provide a competitive baseline for\neach sub-challenge. Our system achieves 77.57% on the F1 score and 0.82 on the\nmean squared error (MSE) for MER-MULTI, 69.82% on the F1 score and 1.12 on MSE\nfor MER-NOISE, and 86.75% on the F1 score for MER-SEMI, respectively. Baseline\ncode is available at https://github.com/zeroQiaoba/MER2023-Baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zheng Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haiyang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Licai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jiangyan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1\">Jianhua Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D2CSE: Difference-aware Deep continuous prompts for Contrastive Sentence Embeddings. (arXiv:2304.08991v1 [cs.CL])","link":"http://arxiv.org/abs/2304.08991","description":"<p>This paper describes Difference-aware Deep continuous prompt for Contrastive\nSentence Embeddings (D2CSE) that learns sentence embeddings. Compared to\nstate-of-the-art approaches, D2CSE computes sentence vectors that are\nexceptional to distinguish a subtle difference in similar sentences by\nemploying a simple neural architecture for continuous prompts. Unlike existing\narchitectures that require multiple pretrained language models (PLMs) to\nprocess a pair of the original and corrupted (subtly modified) sentences, D2CSE\navoids cumbersome fine-tuning of multiple PLMs by only optimizing continuous\nprompts by performing multiple tasks -- i.e., contrastive learning and\nconditional replaced token detection all done in a self-guided manner. D2CSE\noverloads a single PLM on continuous prompts and greatly saves memory\nconsumption as a result. The number of training parameters in D2CSE is reduced\nto about 1\\% of existing approaches while substantially improving the quality\nof sentence embeddings. We evaluate D2CSE on seven Semantic Textual Similarity\n(STS) benchmarks, using three different metrics, namely, Spearman's rank\ncorrelation, recall@K for a retrieval task, and the anisotropy of an embedding\nspace measured in alignment and uniformity. Our empirical results suggest that\nshallow (not too meticulously devised) continuous prompts can be honed\neffectively for multiple NLP tasks and lead to improvements upon existing\nstate-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyunjae Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Biomedical Entity Extraction Pipeline for Oncology Health Records in Portuguese. (arXiv:2304.08999v1 [cs.CL])","link":"http://arxiv.org/abs/2304.08999","description":"<p>Textual health records of cancer patients are usually protracted and highly\nunstructured, making it very time-consuming for health professionals to get a\ncomplete overview of the patient's therapeutic course. As such limitations can\nlead to suboptimal and/or inefficient treatment procedures, healthcare\nproviders would greatly benefit from a system that effectively summarizes the\ninformation of those records. With the advent of deep neural models, this\nobjective has been partially attained for English clinical texts, however, the\nresearch community still lacks an effective solution for languages with limited\nresources. In this paper, we present the approach we developed to extract\nprocedures, drugs, and diseases from oncology health records written in\nEuropean Portuguese. This project was conducted in collaboration with the\nPortuguese Institute for Oncology which, besides holding over $10$ years of\nduly protected medical records, also provided oncologist expertise throughout\nthe development of the project. Since there is no annotated corpus for\nbiomedical entity extraction in Portuguese, we also present the strategy we\nfollowed in annotating the corpus for the development of the models. The final\nmodels, which combined a neural architecture with entity linking, achieved\n$F_1$ scores of $88.6$, $95.0$, and $55.8$ per cent in the mention extraction\nof procedures, drugs, and diseases, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sousa_H/0/1/0/all/0/1\">Hugo Sousa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasquali_A/0/1/0/all/0/1\">Arian Pasquali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jorge_A/0/1/0/all/0/1\">Al&#xed;pio Jorge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_C/0/1/0/all/0/1\">Catarina Sousa Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopes_M/0/1/0/all/0/1\">M&#xe1;rio Amorim Lopes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeKGC: Code Language Model for Generative Knowledge Graph Construction. (arXiv:2304.09048v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09048","description":"<p>Current generative knowledge graph construction approaches usually fail to\ncapture structural knowledge by simply flattening natural language into\nserialized texts or a specification language. However, large generative\nlanguage model trained on structured data such as code has demonstrated\nimpressive capability in understanding natural language for structural\nprediction and reasoning tasks. Intuitively, we address the task of generative\nknowledge graph construction with code language model: given a code-format\nnatural language input, the target is to generate triples which can be\nrepresented as code completion tasks. Specifically, we develop schema-aware\nprompts that effectively utilize the semantic structure within the knowledge\ngraph. As code inherently possesses structure, such as class and function\ndefinitions, it serves as a useful model for prior semantic structural\nknowledge. Furthermore, we employ a rationale-enhanced generation method to\nboost the performance. Rationales provide intermediate steps, thereby improving\nknowledge extraction abilities. Experimental results indicate that the proposed\napproach can obtain better performance on benchmark datasets compared with\nbaselines. Code and datasets are available in\nhttps://github.com/zjunlp/DeepKE/tree/main/example/llm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yinuo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting k-NN for Pre-trained Language Models. (arXiv:2304.09058v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09058","description":"<p>Pre-trained Language Models (PLMs), as parametric-based eager learners, have\nbecome the de-facto choice for current paradigms of Natural Language Processing\n(NLP). In contrast, k-Nearest-Neighbor (k-NN) classifiers, as the lazy learning\nparadigm, tend to mitigate over-fitting and isolated noise. In this paper, we\nrevisit k-NN classifiers for augmenting the PLMs-based classifiers. From the\nmethodological level, we propose to adopt k-NN with textual representations of\nPLMs in two steps: (1) Utilize k-NN as prior knowledge to calibrate the\ntraining process. (2) Linearly interpolate the probability distribution\npredicted by k-NN with that of the PLMs' classifier. At the heart of our\napproach is the implementation of k-NN-calibrated training, which treats\npredicted results as indicators for easy versus hard examples during the\ntraining process. From the perspective of the diversity of application\nscenarios, we conduct extensive experiments on fine-tuning, prompt-tuning\nparadigms and zero-shot, few-shot and fully-supervised settings, respectively,\nacross eight diverse end-tasks. We hope our exploration will encourage the\ncommunity to revisit the power of classical methods for efficient\nNLP\\footnote{Code and datasets are available in\nhttps://github.com/zjunlp/Revisit-KNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1\">Bozhong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised clustering of file dialects according to monotonic decompositions of mixtures. (arXiv:2304.09082v1 [cs.PL])","link":"http://arxiv.org/abs/2304.09082","description":"<p>This paper proposes an unsupervised classification method that partitions a\nset of files into non-overlapping dialects based upon their behaviors,\ndetermined by messages produced by a collection of programs that consume them.\nThe pattern of messages can be used as the signature of a particular kind of\nbehavior, with the understanding that some messages are likely to co-occur,\nwhile others are not. Patterns of messages can be used to classify files into\ndialects. A dialect is defined by a subset of messages, called the required\nmessages. Once files are conditioned upon dialect and its required messages,\nthe remaining messages are statistically independent.\n</p>\n<p>With this definition of dialect in hand, we present a greedy algorithm that\ndeduces candidate dialects from a dataset consisting of a matrix of\nfile-message data, demonstrate its performance on several file formats, and\nprove conditions under which it is optimal. We show that an analyst needs to\nconsider fewer dialects than distinct message patterns, which reduces their\ncognitive load when studying a complex format.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Robinson_M/0/1/0/all/0/1\">Michael Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altman_T/0/1/0/all/0/1\">Tate Altman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_D/0/1/0/all/0/1\">Denley Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Letitia W. Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Items and Contexts Understanding with Descriptive Graph for Conversational Recommendation. (arXiv:2304.09093v1 [cs.IR])","link":"http://arxiv.org/abs/2304.09093","description":"<p>State-of-the-art methods on conversational recommender systems (CRS) leverage\nexternal knowledge to enhance both items' and contextual words' representations\nto achieve high quality recommendations and responses generation. However, the\nrepresentations of the items and words are usually modeled in two separated\nsemantic spaces, which leads to misalignment issue between them. Consequently,\nthis will cause the CRS to only achieve a sub-optimal ranking performance,\nespecially when there is a lack of sufficient information from the user's\ninput. To address limitations of previous works, we propose a new CRS framework\nKLEVER, which jointly models items and their associated contextual words in the\nsame semantic space. Particularly, we construct an item descriptive graph from\nthe rich items' textual features, such as item description and categories.\nBased on the constructed descriptive graph, KLEVER jointly learns the\nembeddings of the words and items, towards enhancing both recommender and\ndialog generation modules. Extensive experiments on benchmarking CRS dataset\ndemonstrate that KLEVER achieves superior performance, especially when the\ninformation from the users' responses is lacking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dao_H/0/1/0/all/0/1\">Huy Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Dung D. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Cuong Chu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving Math Word Problems by Combining Language Models With Symbolic Solvers. (arXiv:2304.09102v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09102","description":"<p>Automatically generating high-quality step-by-step solutions to math word\nproblems has many applications in education. Recently, combining large language\nmodels (LLMs) with external tools to perform complex reasoning and calculation\nhas emerged as a promising direction for solving math word problems, but prior\napproaches such as Program-Aided Language model (PAL) are biased towards simple\nprocedural problems and less effective for problems that require declarative\nreasoning. We propose an approach that combines an LLM that can incrementally\nformalize word problems as a set of variables and equations with an external\nsymbolic solver that can solve the equations. Our approach achieves comparable\naccuracy to the original PAL on the GSM8K benchmark of math word problems and\noutperforms PAL by an absolute 20% on ALGEBRA, a new dataset of more\nchallenging word problems extracted from Algebra textbooks. Our work highlights\nthe benefits of using declarative and incremental representations when\ninterfacing with an external tool for solving complex math word problems. Our\ndata and prompts are publicly available at\nhttps://github.com/joyheyueya/declarative-math-word-problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Yueya_J/0/1/0/all/0/1\">Joy He-Yueya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poesia_G/0/1/0/all/0/1\">Gabriel Poesia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rose E. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah D. Goodman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT: Applications, Opportunities, and Threats. (arXiv:2304.09103v1 [cs.CY])","link":"http://arxiv.org/abs/2304.09103","description":"<p>Developed by OpenAI, ChatGPT (Conditional Generative Pre-trained Transformer)\nis an artificial intelligence technology that is fine-tuned using supervised\nmachine learning and reinforcement learning techniques, allowing a computer to\ngenerate natural language conversation fully autonomously. ChatGPT is built on\nthe transformer architecture and trained on millions of conversations from\nvarious sources. The system combines the power of pre-trained deep learning\nmodels with a programmability layer to provide a strong base for generating\nnatural language conversations. In this study, after reviewing the existing\nliterature, we examine the applications, opportunities, and threats of ChatGPT\nin 10 main domains, providing detailed examples for the business and industry\nas well as education. We also conducted an experimental study, checking the\neffectiveness and comparing the performances of GPT-3.5 and GPT-4, and found\nthat the latter performs significantly better. Despite its exceptional ability\nto generate natural-sounding responses, the authors believe that ChatGPT does\nnot possess the same level of understanding, empathy, and creativity as a human\nand cannot fully replace them in most situations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bahrini_A/0/1/0/all/0/1\">Aram Bahrini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khamoshifar_M/0/1/0/all/0/1\">Mohammadsadra Khamoshifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbasimehr_H/0/1/0/all/0/1\">Hossein Abbasimehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riggs_R/0/1/0/all/0/1\">Robert J. Riggs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esmaeili_M/0/1/0/all/0/1\">Maryam Esmaeili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majdabadkohne_R/0/1/0/all/0/1\">Rastin Mastali Majdabadkohne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasehvar_M/0/1/0/all/0/1\">Morteza Pasehvar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers. (arXiv:2304.09116v1 [eess.AS])","link":"http://arxiv.org/abs/2304.09116","description":"<p>Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild\ndatasets is important to capture the diversity in human speech such as speaker\nidentities, prosodies, and styles (e.g., singing). Current large TTS systems\nusually quantize speech into discrete tokens and use language models to\ngenerate these tokens one by one, which suffer from unstable prosody, word\nskipping/repeating issue, and poor voice quality. In this paper, we develop\nNaturalSpeech 2, a TTS system that leverages a neural audio codec with residual\nvector quantizers to get the quantized latent vectors and uses a diffusion\nmodel to generate these latent vectors conditioned on text input. To enhance\nthe zero-shot capability that is important to achieve diverse speech synthesis,\nwe design a speech prompting mechanism to facilitate in-context learning in the\ndiffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to\nlarge-scale datasets with 44K hours of speech and singing data and evaluate its\nvoice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS\nsystems by a large margin in terms of prosody/timbre similarity, robustness,\nand voice quality in a zero-shot setting, and performs novel zero-shot singing\nsynthesis with only a speech prompt. Audio samples are available at\nhttps://speechresearch.github.io/naturalspeech2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shen_K/0/1/0/all/0/1\">Kai Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ju_Z/0/1/0/all/0/1\">Zeqian Ju</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yanqing Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task. (arXiv:2304.09138v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09138","description":"<p>Recently, ChatGPT and GPT-4 have emerged and gained immense global attention\ndue to their unparalleled performance in language processing. Despite\ndemonstrating impressive capability in various open-domain tasks, their\nadequacy in highly specific fields like radiology remains untested. Radiology\npresents unique linguistic phenomena distinct from open-domain data due to its\nspecificity and complexity. Assessing the performance of large language models\n(LLMs) in such specific domains is crucial not only for a thorough evaluation\nof their overall performance but also for providing valuable insights into\nfuture model design directions: whether model design should be generic or\ndomain-specific. To this end, in this study, we evaluate the performance of\nChatGPT/GPT-4 on a radiology NLI task and compare it to other models fine-tuned\nspecifically on task-related data samples. We also conduct a comprehensive\ninvestigation on ChatGPT/GPT-4's reasoning ability by introducing varying\nlevels of inference difficulty. Our results show that 1) GPT-4 outperforms\nChatGPT in the radiology NLI task; 2) other specifically fine-tuned models\nrequire significant amounts of data samples to achieve comparable performance\nto ChatGPT/GPT-4. These findings demonstrate that constructing a generic model\nthat is capable of solving various tasks across different domains is feasible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Chao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaowei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Haixing Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Quanzheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. (arXiv:2304.09145v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09145","description":"<p>Quantization of transformer language models faces significant challenges due\nto the existence of detrimental outliers in activations. We observe that these\noutliers are asymmetric and concentrated in specific channels. To address this\nissue, we propose the Outlier Suppression+ framework. First, we introduce\nchannel-wise shifting and scaling operations to eliminate asymmetric\npresentation and scale down problematic channels. We demonstrate that these\noperations can be seamlessly migrated into subsequent modules while maintaining\nequivalence. Second, we quantitatively analyze the optimal values for shifting\nand scaling, taking into account both the asymmetric property and quantization\nerrors of weights in the next layer. Our lightweight framework can incur\nminimal performance degradation under static and standard post-training\nquantization settings. Comprehensive results across various tasks and models\nreveal that our approach achieves near-floating-point performance on both small\nmodels, such as BERT, and large language models (LLMs) including OPTs, BLOOM,\nand BLOOMZ at 8-bit and 6-bit settings. Furthermore, we establish a new state\nof the art for 4-bit BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiuying Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunchen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Ruihao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jinyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining. (arXiv:2304.09151v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09151","description":"<p>Pretrained multilingual large language models have typically used heuristic\ntemperature-based sampling to balance between different languages. However\nprevious work has not systematically evaluated the efficacy of different\npretraining language distributions across model scales. In this paper, we\npropose a new sampling method, UniMax, that delivers more uniform coverage of\nhead languages while mitigating overfitting on tail languages by explicitly\ncapping the number of repeats over each language's corpus. We perform an\nextensive series of ablations testing a range of sampling strategies on a suite\nof multilingual benchmarks, while varying model scale. We find that UniMax\noutperforms standard temperature-based sampling, and the benefits persist as\nscale increases. As part of our contribution, we release: (i) an improved and\nrefreshed mC4 multilingual corpus consisting of 29 trillion characters across\n107 languages, and (ii) a suite of pretrained umT5 model checkpoints trained\nwith UniMax sampling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Media Slant is Contagious. (arXiv:2202.07269v2 [econ.GN] UPDATED)","link":"http://arxiv.org/abs/2202.07269","description":"<p>We examine the diffusion of media slant, specifically how partisan content\nfrom national cable news affects local newspapers in the U.S., 2005-2008. We\nuse a text-based measure of cable news slant trained on content from Fox News\nChannel (FNC), CNN, and MSNBC to analyze how local newspapers adopt FNC's slant\nover CNN/MSNBC's. Our findings show that local news becomes more similar to FNC\ncontent in response to an exogenous increase in local FNC viewership. This\nshift is not limited to borrowing from cable news, but rather, local\nnewspapers' own content changes. Further, cable TV slant polarizes local news\ncontent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/econ/1/au:+Widmer_P/0/1/0/all/0/1\">Philine Widmer</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Galletta_S/0/1/0/all/0/1\">Sergio Galletta</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Ash_E/0/1/0/all/0/1\">Elliott Ash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective. (arXiv:2202.08063v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.08063","description":"<p>Knowledge Extraction (KE), aiming to extract structural information from\nunstructured texts, often suffers from data scarcity and emerging unseen types,\ni.e., low-resource scenarios. Many neural approaches to low-resource KE have\nbeen widely investigated and achieved impressive performance. In this paper, we\npresent a literature review towards KE in low-resource scenarios, and\nsystematically categorize existing works into three paradigms: (1) exploiting\nhigher-resource data, (2) exploiting stronger models, and (3) exploiting data\nand models together. In addition, we highlight promising applications and\noutline some potential directions for future research. We hope that our survey\ncan help both the academic and industrial communities to better understand this\nfield, inspire more ideas, and boost broader applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PMC-Patients: A Large-scale Dataset of Patient Summaries and Relations for Benchmarking Retrieval-based Clinical Decision Support Systems. (arXiv:2202.13876v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13876","description":"<p>Objective: Retrieval-based Clinical Decision Support (ReCDS) can aid clinical\nworkflow by providing relevant literature and similar patients for a given\npatient. However, the development of ReCDS systems has been severely obstructed\nby the lack of diverse patient collections and publicly available large-scale\npatient-level annotation datasets. In this paper, we aim to define and\nbenchmark two ReCDS tasks: Patient-to-Article Retrieval (ReCDS-PAR) and\nPatient-to-Patient Retrieval (ReCDS-PPR) using a novel dataset called\nPMC-Patients.\n</p>\n<p>Methods: We extract patient summaries from PubMed Central articles using\nsimple heuristics and utilize the PubMed citation graph to define\npatient-article relevance and patient-patient similarity. We also implement and\nevaluate several ReCDS systems on the PMC-Patients benchmarks, including sparse\nretrievers, dense retrievers, and nearest neighbor retrievers. We conduct\nseveral case studies to show the clinical utility of PMC-Patients.\n</p>\n<p>Results: PMC-Patients contains 167k patient summaries with 3.1M\npatient-article relevance annotations and 293k patient-patient similarity\nannotations, which is the largest-scale resource for ReCDS and also one of the\nlargest patient collections. Human evaluation and analysis show that\nPMC-Patients is a diverse dataset with high-quality annotations. The evaluation\nof various ReCDS systems shows that the PMC-Patients benchmark is challenging\nand calls for further research.\n</p>\n<p>Conclusion: We present PMC-Patients, a large-scale, diverse, and publicly\navailable patient summary dataset with the largest-scale patient-level relation\nannotations. Based on PMC-Patients, we formally define two benchmark tasks for\nReCDS systems and evaluate various existing retrieval methods. PMC-Patients can\nlargely facilitate methodology research on ReCDS systems and shows real-world\nclinical utility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhengyun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qiao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fangyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1\">Tuorui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sheng Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Adversarial Defences and Robustness in NLP. (arXiv:2203.06414v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06414","description":"<p>In the past few years, it has become increasingly evident that deep neural\nnetworks are not resilient enough to withstand adversarial perturbations in\ninput data, leaving them vulnerable to attack. Various authors have proposed\nstrong adversarial attacks for computer vision and Natural Language Processing\n(NLP) tasks. As a response, many defense mechanisms have also been proposed to\nprevent these networks from failing. The significance of defending neural\nnetworks against adversarial attacks lies in ensuring that the model's\npredictions remain unchanged even if the input data is perturbed. Several\nmethods for adversarial defense in NLP have been proposed, catering to\ndifferent NLP tasks such as text classification, named entity recognition, and\nnatural language inference. Some of these methods not only defend neural\nnetworks against adversarial attacks but also act as a regularization mechanism\nduring training, saving the model from overfitting. This survey aims to review\nthe various methods proposed for adversarial defenses in NLP over the past few\nyears by introducing a novel taxonomy. The survey also highlights the fragility\nof advanced deep neural networks in NLP and the challenges involved in\ndefending them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1\">Shreya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doddapaneni_S/0/1/0/all/0/1\">Sumanth Doddapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M.Khapra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravindran_B/0/1/0/all/0/1\">Balaraman Ravindran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"American cultural regions mapped through the lexical analysis of social media. (arXiv:2208.07649v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.07649","description":"<p>Cultural areas represent a useful concept that cross-fertilizes diverse\nfields in social sciences. Knowledge of how humans organize and relate their\nideas and behavior within a society helps to understand their actions and\nattitudes towards different issues. However, the selection of common traits\nthat shape a cultural area is somewhat arbitrary. What is needed is a method\nthat can leverage the massive amounts of data coming online, especially through\nsocial media, to identify cultural regions without ad-hoc assumptions, biases\nor prejudices. This work takes a crucial step in this direction by introducing\na method to infer cultural regions based on the automatic analysis of large\ndatasets from microblogging posts. The approach presented here is based on the\nprinciple that cultural affiliation can be inferred from the topics that people\ndiscuss among themselves. Specifically, regional variations in written\ndiscourse are measured in American social media. From the frequency\ndistributions of content words in geotagged Tweets, the regional hotspots of\nwords' usage are found, and from there, principal components of regional\nvariation are derived. Through a hierarchical clustering of the data in this\nlower-dimensional space, this method yields clear cultural areas and the topics\nof discussion that define them. It uncovers a manifest North-South separation,\nwhich is primarily influenced by the African American culture, and further\ncontiguous (East-West) and non-contiguous divisions that provide a\ncomprehensive picture of today's cultural areas in the US.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Louf_T/0/1/0/all/0/1\">Thomas Louf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goncalves_B/0/1/0/all/0/1\">Bruno Gon&#xe7;alves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramasco_J/0/1/0/all/0/1\">Jose J. Ramasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_D/0/1/0/all/0/1\">David Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grieve_J/0/1/0/all/0/1\">Jack Grieve</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models. (arXiv:2209.06506v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2209.06506","description":"<p>Neural text ranking models have witnessed significant advancement and are\nincreasingly being deployed in practice. Unfortunately, they also inherit\nadversarial vulnerabilities of general neural models, which have been detected\nbut remain underexplored by prior studies. Moreover, the inherit adversarial\nvulnerabilities might be leveraged by blackhat SEO to defeat better-protected\nsearch engines. In this study, we propose an imitation adversarial attack on\nblack-box neural passage ranking models. We first show that the target passage\nranking model can be transparentized and imitated by enumerating critical\nqueries/candidates and then train a ranking imitation model. Leveraging the\nranking imitation model, we can elaborately manipulate the ranking results and\ntransfer the manipulation attack to the target ranking model. For this purpose,\nwe propose an innovative gradient-based attack method, empowered by the\npairwise objective function, to generate adversarial triggers, which causes\npremeditated disorderliness with very few tokens. To equip the trigger\ncamouflages, we add the next sentence prediction loss and the language model\nfluency constraint to the objective function. Experimental results on passage\nranking demonstrate the effectiveness of the ranking imitation attack model and\nadversarial triggers against various SOTA neural ranking models. Furthermore,\nvarious mitigation analyses and human evaluation show the effectiveness of\ncamouflages when facing potential mitigation approaches. To motivate other\nscholars to further investigate this novel and important problem, we make the\nexperiment data and code publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yangyang Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Di Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaisong Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changlong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaozhong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variable Attention Masking for Configurable Transformer Transducer Speech Recognition. (arXiv:2211.01438v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2211.01438","description":"<p>This work studies the use of attention masking in transformer transducer\nbased speech recognition for building a single configurable model for different\ndeployment scenarios. We present a comprehensive set of experiments comparing\nfixed masking, where the same attention mask is applied at every frame, with\nchunked masking, where the attention mask for each frame is determined by chunk\nboundaries, in terms of recognition accuracy and latency. We then explore the\nuse of variable masking, where the attention masks are sampled from a target\ndistribution at training time, to build models that can work in different\nconfigurations. Finally, we investigate how a single configurable model can be\nused to perform both first pass streaming recognition and second pass acoustic\nrescoring. Experiments show that chunked masking achieves a better accuracy vs\nlatency trade-off compared to fixed masking, both with and without FastEmit. We\nalso show that variable masking improves the accuracy by up to 8% relative in\nthe acoustic re-scoring scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Swietojanski_P/0/1/0/all/0/1\">Pawel Swietojanski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Braun_S/0/1/0/all/0/1\">Stefan Braun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Can_D/0/1/0/all/0/1\">Dogan Can</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Silva_T/0/1/0/all/0/1\">Thiago Fraga da Silva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghoshal_A/0/1/0/all/0/1\">Arnab Ghoshal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hori_T/0/1/0/all/0/1\">Takaaki Hori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsiao_R/0/1/0/all/0/1\">Roger Hsiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mason_H/0/1/0/all/0/1\">Henry Mason</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McDermott_E/0/1/0/all/0/1\">Erik McDermott</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Silovsky_H/0/1/0/all/0/1\">Honza Silovsky</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Travadi_R/0/1/0/all/0/1\">Ruchir Travadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiaodan Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Circling Back to Recurrent Models of Language. (arXiv:2211.01848v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.01848","description":"<p>Just because some purely recurrent models suffer from being hard to optimize\nand inefficient on today's hardware, they are not necessarily bad models of\nlanguage. We demonstrate this by the extent to which these models can still be\nimproved by a combination of a slightly better recurrent cell, architecture,\nobjective, as well as optimization. In the process, we establish a new state of\nthe art for language modelling on small datasets and on Enwik8 with dynamic\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Melis_G/0/1/0/all/0/1\">G&#xe1;bor Melis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge is Power: Understanding Causality Makes Legal judgment Prediction Models More Generalizable and Robust. (arXiv:2211.03046v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.03046","description":"<p>Legal Judgment Prediction (LJP), aiming to predict a judgment based on fact\ndescriptions according to rule of law, serves as legal assistance to mitigate\nthe great work burden of limited legal practitioners. Most existing methods\napply various large-scale pre-trained language models (PLMs) finetuned in LJP\ntasks to obtain consistent improvements. However, we discover the fact that the\nstate-of-the-art (SOTA) model makes judgment predictions according to\nirrelevant (or non-casual) information. The violation of rule of law not only\nweakens the robustness and generalization ability of models but also results in\nsevere social problems like discrimination. In this paper, we use causal\nstructural models (SCMs) to theoretically analyze how LJP models learn to make\ndecisions and why they can succeed in passing the traditional testing paradigm\nwithout learning causality. According to our analysis, we provide two solutions\nintervening on data and model by causality, respectively. In detail, we first\ndistinguish non-causal information by applying the open information extraction\n(OIE) technique. Then, we propose a method named the Causal Information\nEnhanced SAmpling Method (CIESAM) to eliminate the non-causal information from\ndata. To validate our theoretical analysis, we further propose another method\nusing our proposed Causality-Aware Self-Attention Mechanism (CASAM) to guide\nthe model to learn the underlying causality knowledge in legal texts. The\nconfidence of CASAM in learning causal information is higher than that of\nCIESAM. The extensive experimental results show that both our proposed methods\nachieve state-of-the-art (SOTA) performance on three commonly used\nlegal-specific datasets. The stronger performance of CASAM further demonstrates\nthat causality is the key to the robustness and generalization ability of\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haotian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lingwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiran Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fanchao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yang Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InferEM: Inferring the Speaker's Intention for Empathetic Dialogue Generation. (arXiv:2212.06373v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.06373","description":"<p>Current approaches to empathetic response generation typically encode the\nentire dialogue history directly and put the output into a decoder to generate\nfriendly feedback. These methods focus on modelling contextual information but\nneglect capturing the direct intention of the speaker. We argue that the last\nutterance in the dialogue empirically conveys the intention of the speaker.\nConsequently, we propose a novel model named InferEM for empathetic response\ngeneration. We separately encode the last utterance and fuse it with the entire\ndialogue through the multi-head attention based intention fusion module to\ncapture the speaker's intention. Besides, we utilize previous utterances to\npredict the last utterance, which simulates human's psychology to guess what\nthe interlocutor may speak in advance. To balance the optimizing rates of the\nutterance prediction and response generation, a multi-task learning strategy is\ndesigned for InferEM. Experimental results demonstrate the plausibility and\nvalidity of InferEM in improving empathetic expression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lv_G/0/1/0/all/0/1\">Guoqing Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhigang Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Modelling of Swedish Newspaper Articles about Coronavirus: a Case Study using Latent Dirichlet Allocation Method. (arXiv:2301.03029v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.03029","description":"<p>Topic Modelling (TM) is from the research branches of natural language\nunderstanding (NLU) and natural language processing (NLP) that is to facilitate\ninsightful analysis from large documents and datasets, such as a summarisation\nof main topics and the topic changes. This kind of discovery is getting more\npopular in real-life applications due to its impact on big data analytics. In\nthis study, from the social-media and healthcare domain, we apply popular\nLatent Dirichlet Allocation (LDA) methods to model the topic changes in Swedish\nnewspaper articles about Coronavirus. We describe the corpus we created\nincluding 6515 articles, methods applied, and statistics on topic changes over\napproximately 1 year and two months period of time from 17th January 2020 to\n13th March 2021. We hope this work can be an asset for grounding applications\nof topic modelling and can be inspiring for similar case studies in an era with\npandemics, to support socio-economic impact research as well as clinical and\nhealthcare analytics. Our data and source code are openly available at\nhttps://github. com/poethan/Swed_Covid_TM Keywords: Latent Dirichlet Allocation\n(LDA); Topic Modelling; Coronavirus; Pandemics; Natural Language Understanding;\nBERT-topic\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Griciute_B/0/1/0/all/0/1\">Bernadeta Grici&#x16b;t&#x117;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1\">Goran Nenadic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata. (arXiv:2301.04647v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2301.04647","description":"<p>We learn a visual representation that captures information about the camera\nthat recorded a given photo. To do this, we train a multimodal embedding\nbetween image patches and the EXIF metadata that cameras automatically insert\ninto image files. Our model represents this metadata by simply converting it to\ntext and then processing it with a transformer. The features that we learn\nsignificantly outperform other self-supervised and supervised features on\ndownstream image forensics and calibration tasks. In particular, we\nsuccessfully localize spliced image regions \"zero shot\" by clustering the\nvisual embeddings for all of the patches within an image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chenhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Ayush Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owens_A/0/1/0/all/0/1\">Andrew Owens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Erasure of Unaligned Attributes from Neural Representations. (arXiv:2302.02997v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.02997","description":"<p>We present the Assignment-Maximization Spectral Attribute removaL (AMSAL)\nalgorithm, which erases information from neural representations when the\ninformation to be erased is implicit rather than directly being aligned to each\ninput example. Our algorithm works by alternating between two steps. In one, it\nfinds an assignment of the input representations to the information to be\nerased, and in the other, it creates projections of both the input\nrepresentations and the information to be erased into a joint latent space. We\ntest our algorithm on an extensive array of datasets, including a Twitter\ndataset with multiple guarded attributes, the BiasBios dataset and the\nBiasBench benchmark. The last benchmark includes four datasets with various\ntypes of protected attributes. Our results demonstrate that bias can often be\nremoved in our setup. We also discuss the limitations of our approach when\nthere is a strong entanglement between the main task and the information to be\nerased.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1\">Shun Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziser_Y/0/1/0/all/0/1\">Yftah Ziser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Shay Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reception Reader: Exploring Text Reuse in Early Modern British Publications. (arXiv:2302.04084v2 [cs.DL] UPDATED)","link":"http://arxiv.org/abs/2302.04084","description":"<p>The Reception Reader is a web tool for studying text reuse in the Early\nEnglish Books Online (EEBO-TCP) and Eighteenth Century Collections Online\n(ECCO) data. Users can: 1) explore a visual overview of the reception of a\nwork, or its incoming connections, across time based on shared text segments,\n2) interactively survey the details of connected documents, and 3) examine the\ncontext of reused text for \"close reading\". We show examples of how the tool\nstreamlines research and exploration tasks, and discuss the utility and\nlimitations of the user interface along with its current data sources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosson_D/0/1/0/all/0/1\">David Rosson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makela_E/0/1/0/all/0/1\">Eetu M&#xe4;kel&#xe4;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaara_V/0/1/0/all/0/1\">Ville Vaara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahadevan_A/0/1/0/all/0/1\">Ananth Mahadevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryan_Y/0/1/0/all/0/1\">Yann Ryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolonen_M/0/1/0/all/0/1\">Mikko Tolonen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification. (arXiv:2303.07142v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.07142","description":"<p>This case study investigates the task of job classification in a real-world\nsetting, where the goal is to determine whether an English-language job posting\nis appropriate for a graduate or entry-level position. We explore multiple\napproaches to text classification, including supervised approaches such as\ntraditional models like Support Vector Machines (SVMs) and state-of-the-art\ndeep learning methods such as DeBERTa. We compare them with Large Language\nModels (LLMs) used in both few-shot and zero-shot classification settings. To\naccomplish this task, we employ prompt engineering, a technique that involves\ndesigning prompts to guide the LLMs towards the desired output. Specifically,\nwe evaluate the performance of two commercially available state-of-the-art\nGPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also\nconduct a detailed analysis of the impact of different aspects of prompt\nengineering on the model's performance. Our results show that, with a\nwell-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms all\nother models, achieving a 6% increase in Precision@95% Recall compared to the\nbest supervised approach. Furthermore, we observe that the wording of the\nprompt is a critical factor in eliciting the appropriate \"reasoning\" in the\nmodel, and that seemingly minor aspects of the prompt significantly affect the\nmodel's performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clavie_B/0/1/0/all/0/1\">Benjamin Clavi&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciceu_A/0/1/0/all/0/1\">Alexandru Ciceu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naylor_F/0/1/0/all/0/1\">Frederick Naylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soulie_G/0/1/0/all/0/1\">Guillaume Souli&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brightwell_T/0/1/0/all/0/1\">Thomas Brightwell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feedback Effect in User Interaction with Intelligent Assistants: Delayed Engagement, Adaption and Drop-out. (arXiv:2303.10255v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2303.10255","description":"<p>With the growing popularity of intelligent assistants (IAs), evaluating IA\nquality becomes an increasingly active field of research. This paper identifies\nand quantifies the feedback effect, a novel component in IA-user interactions:\nhow the capabilities and limitations of the IA influence user behavior over\ntime. First, we demonstrate that unhelpful responses from the IA cause users to\ndelay or reduce subsequent interactions in the short term via an observational\nstudy. Next, we expand the time horizon to examine behavior changes and show\nthat as users discover the limitations of the IA's understanding and functional\ncapabilities, they learn to adjust the scope and wording of their requests to\nincrease the likelihood of receiving a helpful response from the IA. Our\nfindings highlight the impact of the feedback effect at both the micro and meso\nlevels. We further discuss its macro-level consequences: unsatisfactory\ninteractions continuously reduce the likelihood and diversity of future user\nengagements in a feedback loop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiu_Z/0/1/0/all/0/1\">Zidi Xiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kai-Chen Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">David Q. Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiannan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotek_H/0/1/0/all/0/1\">Hadas Kotek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCarthy_P/0/1/0/all/0/1\">Paul McCarthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_C/0/1/0/all/0/1\">Christopher Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulman_S/0/1/0/all/0/1\">Stephen Pulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1\">Jason D. Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XWikiGen: Cross-lingual Summarization for Encyclopedic Text Generation in Low Resource Languages. (arXiv:2303.12308v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.12308","description":"<p>Lack of encyclopedic text contributors, especially on Wikipedia, makes\nautomated text generation for low resource (LR) languages a critical problem.\nExisting work on Wikipedia text generation has focused on English only where\nEnglish reference articles are summarized to generate English Wikipedia pages.\nBut, for low-resource languages, the scarcity of reference articles makes\nmonolingual summarization ineffective in solving this problem. Hence, in this\nwork, we propose XWikiGen, which is the task of cross-lingual multi-document\nsummarization of text from multiple reference articles, written in various\nlanguages, to generate Wikipedia-style text. Accordingly, we contribute a\nbenchmark dataset, XWikiRef, spanning ~69K Wikipedia articles covering five\ndomains and eight languages. We harness this dataset to train a two-stage\nsystem where the input is a set of citations and a section title and the output\nis a section-specific LR summary. The proposed system is based on a novel idea\nof neural unsupervised extractive summarization to coarsely identify salient\ninformation followed by a neural abstractive model to generate the\nsection-specific text. Extensive experiments show that multi-domain training is\nbetter than the multi-lingual setup on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taunk_D/0/1/0/all/0/1\">Dhaval Taunk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagare_S/0/1/0/all/0/1\">Shivprasad Sagare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_A/0/1/0/all/0/1\">Anupam Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1\">Shivansh Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_V/0/1/0/all/0/1\">Vasudeva Varma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering. (arXiv:2303.12320v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.12320","description":"<p>Commonsense question-answering (QA) methods combine the power of pre-trained\nLanguage Models (LM) with the reasoning provided by Knowledge Graphs (KG). A\ntypical approach collects nodes relevant to the QA pair from a KG to form a\nWorking Graph (WG) followed by reasoning using Graph Neural Networks(GNNs).\nThis faces two major challenges: (i) it is difficult to capture all the\ninformation from the QA in the WG, and (ii) the WG contains some irrelevant\nnodes from the KG. To address these, we propose GrapeQA with two simple\nimprovements on the WG: (i) Prominent Entities for Graph Augmentation\nidentifies relevant text chunks from the QA pair and augments the WG with\ncorresponding latent representations from the LM, and (ii) Context-Aware Node\nPruning removes nodes that are less relevant to the QA pair. We evaluate our\nresults on OpenBookQA, CommonsenseQA and MedQA-USMLE and see that GrapeQA shows\nconsistent improvements over its LM + KG predecessor (QA-GNN in particular) and\nlarge improvements on OpenBookQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taunk_D/0/1/0/all/0/1\">Dhaval Taunk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanna_L/0/1/0/all/0/1\">Lakshya Khanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandru_P/0/1/0/all/0/1\">Pavan Kandru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_V/0/1/0/all/0/1\">Vasudeva Varma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_C/0/1/0/all/0/1\">Charu Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tapaswi_M/0/1/0/all/0/1\">Makarand Tapaswi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quick Dense Retrievers Consume KALE: Post Training Kullback Leibler Alignment of Embeddings for Asymmetrical dual encoders. (arXiv:2304.01016v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01016","description":"<p>In this paper, we consider the problem of improving the inference latency of\nlanguage model-based dense retrieval systems by introducing structural\ncompression and model size asymmetry between the context and query encoders.\nFirst, we investigate the impact of pre and post-training compression on the\nMSMARCO, Natural Questions, TriviaQA, SQUAD, and SCIFACT, finding that\nasymmetry in the dual encoders in dense retrieval can lead to improved\ninference efficiency. Knowing this, we introduce Kullback Leibler Alignment of\nEmbeddings (KALE), an efficient and accurate method for increasing the\ninference efficiency of dense retrieval methods by pruning and aligning the\nquery encoder after training. Specifically, KALE extends traditional Knowledge\nDistillation after bi-encoder training, allowing for effective query encoder\ncompression without full retraining or index generation. Using KALE and\nasymmetric training, we can generate models which exceed the performance of\nDistilBERT despite having 3x faster inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Campos_D/0/1/0/all/0/1\">Daniel Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magnani_A/0/1/0/all/0/1\">Alessandro Magnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tractable Control for Autoregressive Language Generation. (arXiv:2304.07438v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.07438","description":"<p>Despite the success of autoregressive large language models in text\ngeneration, it remains a major challenge to generate text that satisfies\ncomplex constraints: sampling from the conditional distribution\n$\\Pr(\\text{text} | \\alpha)$ is intractable for even the simplest lexical\nconstraints $\\alpha$. To overcome this challenge, we propose to use tractable\nprobabilistic models to impose lexical constraints in autoregressive text\ngeneration, which we refer to as GeLaTo. To demonstrate the effectiveness of\nthis framework, we use distilled hidden Markov models to control autoregressive\ngeneration from GPT2. GeLaTo achieves state-of-the-art performance on\nCommonGen, a challenging benchmark for constrained text generation, beating a\nwide range of strong baselines by a large margin. Our work not only opens up\nnew avenues for controlling large language models but also motivates the\ndevelopment of more expressive tractable probabilistic models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honghua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_M/0/1/0/all/0/1\">Meihua Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1\">Guy Van den Broeck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Canvas: End-to-End Kernel Architecture Search in Neural Networks. (arXiv:2304.07741v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2304.07741","description":"<p>The demands for higher performance and accuracy in neural networks (NNs)\nnever end. Existing tensor compilation and Neural Architecture Search (NAS)\ntechniques orthogonally optimize the two goals but actually share many\nsimilarities in their concrete strategies. We exploit such opportunities by\ncombining the two into one and make a case for Kernel Architecture Search\n(KAS). KAS reviews NAS from a system perspective and zooms into a more\nfine-grained level to generate neural kernels with both high performance and\ngood accuracy. To demonstrate the potential of KAS, we build an end-to-end\nframework, Canvas, to find high-quality kernels as convolution replacements.\nCanvas samples from a rich set of fine-grained primitives to stochastically and\niteratively construct new kernels and evaluate them according to user-specified\nconstraints. Canvas supports freely adjustable tensor dimension sizes inside\nthe kernel and uses two levels of solvers to satisfy structural legality and\nfully utilize model budgets. The evaluation shows that by replacing standard\nconvolutions with generated new kernels in common NNs, Canvas achieves average\n1.5x speedups compared to the previous state-of-the-art with acceptable\naccuracy loss and search efficiency. Canvas verifies the practicability of KAS\nby rediscovering many manually designed kernels in the past and producing new\nstructures that may inspire future machine learning innovations. For source\ncode and implementation, we open-sourced Canvas at\nhttps://github.com/tsinghua-ideal/Canvas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chenggang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Genghan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingyu Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Machine Translation For Low Resource Languages. (arXiv:2304.07869v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.07869","description":"<p>Neural Machine translation is a challenging task due to the inherent complex\nnature and the fluidity that natural languages bring. Nonetheless, in recent\nyears, it has achieved state-of-the-art performance in several language pairs.\nAlthough, a lot of traction can be seen in the areas of multilingual neural\nmachine translation (MNMT) in the recent years, there are no comprehensive\nsurvey done to identify what approaches work well. The goal of this paper is to\ninvestigate the realm of low resource languages and build a Neural Machine\nTranslation model to achieve state-of-the-art results. The paper looks to build\nupon the mBART language model and explore strategies to augment it with various\nNLP and Deep Learning techniques like back translation and transfer learning.\nThis implementation tries to unpack the architecture of the NMT application and\ndetermine the different components which offers us opportunities to amend the\nsaid application within the purview of the low resource languages problem\nspace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyle_V/0/1/0/all/0/1\">Vakul Goyle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnaswamy_P/0/1/0/all/0/1\">Parvathy Krishnaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_K/0/1/0/all/0/1\">Kannan Girija Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_U/0/1/0/all/0/1\">Utsa Chattopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyle_K/0/1/0/all/0/1\">Kartikay Goyle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chinese Open Instruction Generalist: A Preliminary Release. (arXiv:2304.07987v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.07987","description":"<p>Instruction tuning is widely recognized as a key technique for building\ngeneralist language models, which has attracted the attention of researchers\nand the public with the release of InstructGPT~\\citep{ouyang2022training} and\nChatGPT\\footnote{\\url{https://chat.openai.com/}}. Despite impressive progress\nin English-oriented large-scale language models (LLMs), it is still\nunder-explored whether English-based foundation LLMs can perform similarly on\nmultilingual tasks compared to English tasks with well-designed instruction\ntuning and how we can construct the corpora needed for the tuning.\n</p>\n<p>To remedy this gap, we propose the project as an attempt to create a Chinese\ninstruction dataset by various methods adapted to the intrinsic characteristics\nof 4 sub-tasks. We collect around 200k Chinese instruction tuning samples,\nwhich have been manually checked to guarantee high quality. We also summarize\nthe existing English and Chinese instruction corpora and briefly describe some\npotential applications of the newly constructed Chinese instruction corpora.\nThe resulting \\textbf{C}hinese \\textbf{O}pen \\textbf{I}nstruction\n\\textbf{G}eneralist (\\textbf{COIG}) corpora are available in\nHuggingface\\footnote{\\url{https://huggingface.co/datasets/BAAI/COIG}} and\nGithub\\footnote{\\url{https://github.com/FlagOpen/FlagInstruct}}, and will be\ncontinuously updated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Ge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yemin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1\">Ruibin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yizhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Siwei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1\">Yu Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaoqun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zekun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on Chinese Instruction Data for Instruction Following Large Language Model. (arXiv:2304.08109v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.08109","description":"<p>Recently, the instruction-tuning of large language models is a crucial area\nof research in the field of natural language processing. Due to resource and\ncost limitations, several researchers have employed parameter-efficient tuning\ntechniques, such as LoRA, for instruction tuning, and have obtained encouraging\nresults In comparison to full-parameter fine-tuning, LoRA-based tuning\ndemonstrates salient benefits in terms of training costs. In this study, we\nundertook experimental comparisons between full-parameter fine-tuning and\nLoRA-based tuning methods, utilizing LLaMA as the base model. The experimental\nresults show that the selection of the foundational model, training dataset\nscale, learnable parameter quantity, and model training cost are all important\nfactors. We hope that the experimental conclusions of this paper can provide\ninspiration for training large language models, especially in the field of\nChinese, and help researchers find a better trade-off strategy between training\ncost and model performance. To facilitate the reproduction of the paper's\nresults, the dataset, model and code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xianghui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yunjie Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1\">Baochang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-04-18T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}