{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-06-09T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Improving Empathetic Dialogue Generation by Dynamically Infusing Commonsense Knowledge. (arXiv:2306.04657v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04657","description":"<p>In empathetic conversations, individuals express their empathy towards\nothers. Previous work has mainly focused on generating empathetic responses by\nutilizing the speaker's emotion. Besides, external commonsense knowledge has\nbeen applied to enhance the system's understandings of the speaker's situation.\nHowever, given an event, commonsense knowledge base contains various relations,\npotentially leading to confusion for the dialogue system. Consequently,\ninconsistencies arise among the emotion, generated response and speaker's\ncontextual information. To this end, we propose a novel approach for empathetic\nresponse generation, which incorporates an adaptive module for commonsense\nknowledge selection to ensure consistency between the generated empathetic\nresponses and the speaker's situation. This selected knowledge is used to\nrefine the commonsense cognition and empathy expression for generated\nresponses. Experimental results show that our approach significantly\noutperforms baseline models in both automatic and human evaluations, exhibiting\nthe generation of more coherent and empathetic responses. Moreover, case\nstudies highlight the interpretability of knowledge selection in the responses\nand the effectiveness of adaptive module in our model. Code:\nhttps://github.com/Hanscal/DCKS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hua Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuli Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Weilin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaomei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Weifeng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models. (arXiv:2306.04695v1 [cs.CV])","link":"http://arxiv.org/abs/2306.04695","description":"<p>The ability to understand visual concepts and replicate and compose these\nconcepts from images is a central goal for computer vision. Recent advances in\ntext-to-image (T2I) models have lead to high definition and realistic image\nquality generation by learning from large databases of images and their\ndescriptions. However, the evaluation of T2I models has focused on photorealism\nand limited qualitative measures of visual understanding. To quantify the\nability of T2I models in learning and synthesizing novel visual concepts, we\nintroduce ConceptBed, a large-scale dataset that consists of 284 unique visual\nconcepts, 5K unique concept compositions, and 33K composite text prompts. Along\nwith the dataset, we propose an evaluation metric, Concept Confidence Deviation\n(CCD), that uses the confidence of oracle concept classifiers to measure the\nalignment between concepts generated by T2I generators and concepts contained\nin ground truth images. We evaluate visual concepts that are either objects,\nattributes, or styles, and also evaluate four dimensions of compositionality:\ncounting, attributes, relations, and actions. Our human study shows that CCD is\nhighly correlated with human understanding of concepts. Our results point to a\ntrade-off between learning the concepts and preserving the compositionality\nwhich existing approaches struggle to overcome.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_M/0/1/0/all/0/1\">Maitreya Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Open Language Models by Learning from Organic Interactions. (arXiv:2306.04707v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04707","description":"<p>We present BlenderBot 3x, an update on the conversational model BlenderBot 3,\nwhich is now trained using organic conversation and feedback data from\nparticipating users of the system in order to improve both its skills and\nsafety. We are publicly releasing the participating de-identified interaction\ndata for use by the research community, in order to spur further progress.\nTraining models with organic data is challenging because interactions with\npeople \"in the wild\" include both high quality conversations and feedback, as\nwell as adversarial and toxic behavior. We study techniques that enable\nlearning from helpful teachers while avoiding learning from people who are\ntrying to trick the model into unhelpful or toxic responses. BlenderBot 3x is\nboth preferred in conversation to BlenderBot 3, and is shown to produce safer\nresponses in challenging situations. While our current models are still far\nfrom perfect, we believe further improvement can be achieved by continued use\nof the techniques explored in this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_D/0/1/0/all/0/1\">Da Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_J/0/1/0/all/0/1\">Joshua Lane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komeili_M/0/1/0/all/0/1\">Mojtaba Komeili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1\">Eric Michael Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ung_M/0/1/0/all/0/1\">Megan Ung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behrooz_M/0/1/0/all/0/1\">Morteza Behrooz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngan_W/0/1/0/all/0/1\">William Ngan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moritz_R/0/1/0/all/0/1\">Rashel Moritz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1\">Sainbayar Sukhbaatar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boureau_Y/0/1/0/all/0/1\">Y-Lan Boureau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuster_K/0/1/0/all/0/1\">Kurt Shuster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts. (arXiv:2306.04723v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04723","description":"<p>Rapidly increasing quality of AI-generated content makes it difficult to\ndistinguish between human and AI-generated texts, which may lead to undesirable\nconsequences for society. Therefore, it becomes increasingly important to study\nthe properties of human texts that are invariant over text domains and various\nproficiency of human writers, can be easily calculated for any language, and\ncan robustly separate natural and AI-generated texts regardless of the\ngeneration model and sampling method. In this work, we propose such an\ninvariant of human texts, namely the intrinsic dimensionality of the manifold\nunderlying the set of embeddings of a given text sample. We show that the\naverage intrinsic dimensionality of fluent texts in natural language is\nhovering around the value $9$ for several alphabet-based languages and around\n$7$ for Chinese, while the average intrinsic dimensionality of AI-generated\ntexts for each language is $\\approx 1.5$ lower, with a clear statistical\nseparation between human-generated and AI-generated distributions. This\nproperty allows us to build a score-based artificial text detector. The\nproposed detector's accuracy is stable over text domains, generator models, and\nhuman writer proficiency levels, outperforming SOTA detectors in model-agnostic\nand cross-domain scenarios by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tulchinskii_E/0/1/0/all/0/1\">Eduard Tulchinskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_K/0/1/0/all/0/1\">Kristian Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kushnareva_L/0/1/0/all/0/1\">Laida Kushnareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherniavskii_D/0/1/0/all/0/1\">Daniil Cherniavskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1\">Serguei Barannikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1\">Irina Piontkovskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolenko_S/0/1/0/all/0/1\">Sergey Nikolenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompter: Zero-shot Adaptive Prefixes for Dialogue State Tracking Domain Adaptation. (arXiv:2306.04724v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04724","description":"<p>A challenge in the Dialogue State Tracking (DST) field is adapting models to\nnew domains without using any supervised data, zero-shot domain adaptation.\nParameter-Efficient Transfer Learning (PETL) has the potential to address this\nproblem due to its robustness. However, it has yet to be applied to the\nzero-shot scenarios, as it is not clear how to apply it unsupervisedly.\n</p>\n<p>Our method, Prompter, uses descriptions of target domain slots to generate\ndynamic prefixes that are concatenated to the key and values at each layer's\nself-attention mechanism. This allows for the use of prefix-tuning in\nzero-shot. Prompter outperforms previous methods on both the MultiWOZ and SGD\nbenchmarks. In generating prefixes, our analyses find that Prompter not only\nutilizes the semantics of slot descriptions but also how often the slots appear\ntogether in conversation. Moreover, Prompter's gains are due to its improved\nability to distinguish \"none\"-valued dialogue slots, compared against\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aksu_T/0/1/0/all/0/1\">Taha Aksu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Min-Yen Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Soft-prompt Tuning for Large Language Models to Evaluate Bias. (arXiv:2306.04735v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04735","description":"<p>Prompting large language models has gained immense popularity in recent years\ndue to the advantage of producing good results even without the need for\nlabelled data. However, this requires prompt tuning to get optimal prompts that\nlead to better model performances. In this paper, we explore the use of\nsoft-prompt tuning on sentiment classification task to quantify the biases of\nlarge language models (LLMs) such as Open Pre-trained Transformers (OPT) and\nGalactica language model. Since these models are trained on real-world data\nthat could be prone to bias toward certain groups of populations, it is\nimportant to identify these underlying issues. Using soft-prompts to evaluate\nbias gives us the extra advantage of avoiding the human-bias injection that can\nbe caused by manually designed prompts. We check the model biases on different\nsensitive attributes using the group fairness (bias) and find interesting bias\npatterns. Since LLMs have been used in the industry in various applications, it\nis crucial to identify the biases before deploying these models in practice. We\nopen-source our pipeline and encourage industry researchers to adapt our work\nto their use cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jacob-Junqi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emerson_D/0/1/0/all/0/1\">David Emerson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyandoab_S/0/1/0/all/0/1\">Sevil Zanjani Miyandoab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandya_D/0/1/0/all/0/1\">Deval Pandya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seyyed_Kalantari_L/0/1/0/all/0/1\">Laleh Seyyed-Kalantari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khattak_F/0/1/0/all/0/1\">Faiza Khan Khattak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ScienceBenchmark: A Complex Real-World Benchmark for Evaluating Natural Language to SQL Systems. (arXiv:2306.04743v1 [cs.DB])","link":"http://arxiv.org/abs/2306.04743","description":"<p>Natural Language to SQL systems (NL-to-SQL) have recently shown a significant\nincrease in accuracy for natural language to SQL query translation. This\nimprovement is due to the emergence of transformer-based language models, and\nthe popularity of the Spider benchmark - the de-facto standard for evaluating\nNL-to-SQL systems. The top NL-to-SQL systems reach accuracies of up to 85\\%.\nHowever, Spider mainly contains simple databases with few tables, columns, and\nentries, which does not reflect a realistic setting. Moreover, complex\nreal-world databases with domain-specific content have little to no training\ndata available in the form of NL/SQL-pairs leading to poor performance of\nexisting NL-to-SQL systems.\n</p>\n<p>In this paper, we introduce ScienceBenchmark, a new complex NL-to-SQL\nbenchmark for three real-world, highly domain-specific databases. For this new\nbenchmark, SQL experts and domain experts created high-quality NL/SQL-pairs for\neach domain. To garner more data, we extended the small amount of\nhuman-generated data with synthetic data generated using GPT-3. We show that\nour benchmark is highly challenging, as the top performing systems on Spider\nachieve a very low performance on our benchmark. Thus, the challenge is\nmany-fold: creating NL-to-SQL systems for highly complex domains with a small\namount of hand-made training data augmented with synthetic data. To our\nknowledge, ScienceBenchmark is the first NL-to-SQL benchmark designed with\ncomplex real-world scientific databases, containing challenging training and\ntest data carefully validated by domain experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deriu_J/0/1/0/all/0/1\">Jan Deriu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsogiannis_Meimarakis_G/0/1/0/all/0/1\">George Katsogiannis-Meimarakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosten_C/0/1/0/all/0/1\">Catherine Kosten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutrika_G/0/1/0/all/0/1\">Georgia Koutrika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stockinger_K/0/1/0/all/0/1\">Kurt Stockinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Large Language Model Annotations for Valid Downstream Statistical Inference in Social Science: Design-Based Semi-Supervised Learning. (arXiv:2306.04746v1 [stat.ME])","link":"http://arxiv.org/abs/2306.04746","description":"<p>In computational social science (CSS), researchers analyze documents to\nexplain social and political phenomena. In most scenarios, CSS researchers\nfirst obtain labels for documents and then explain labels using interpretable\nregression analyses in the second step. The recent advancements in large\nlanguage models (LLMs) can lower costs for CSS research by annotating documents\ncheaply at scale, but such surrogate labels are often imperfect and biased. We\npresent a new algorithm for using outputs from LLMs for downstream statistical\nanalyses while guaranteeing statistical properties -- like asymptotic\nunbiasedness and proper uncertainty quantification -- which are fundamental to\nCSS research. We show that direct use of LLM-predicted surrogate labels in\ndownstream statistical analyses leads to substantial bias and invalid\nconfidence intervals, even with high surrogate accuracy of 80--90\\%. To address\nthis, we build on debiased machine learning to propose the design-based\nsemi-supervised learning (DSL) estimator. DSL employs a doubly-robust procedure\nto combine surrogate labels with a smaller number of gold-standard labels. Our\napproach guarantees valid inference for downstream statistical analyses, even\nwhen surrogates are arbitrarily biased, without requiring stringent\nassumptions, by controlling the probability of sampling documents for\ngold-standard labeling. Both our theoretical analysis and experimental results\nshow that DSL provides valid statistical inference while achieving root mean\nsquared errors comparable to existing alternatives that focus only on\nprediction without statistical guarantees.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Egami_N/0/1/0/all/0/1\">Naoki Egami</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jacobs_Harukawa_M/0/1/0/all/0/1\">Musashi Jacobs-Harukawa</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Stewart_B/0/1/0/all/0/1\">Brandon M. Stewart</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wei_H/0/1/0/all/0/1\">Hanying Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources. (arXiv:2306.04751v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04751","description":"<p>In this work we explore recent advances in instruction-tuning language models\non a range of open instruction-following datasets. Despite recent claims that\nopen models can be on par with state-of-the-art proprietary models, these\nclaims are often accompanied by limited evaluation, making it difficult to\ncompare models across the board and determine the utility of various resources.\nWe provide a large set of instruction-tuned models from 6.7B to 65B parameters\nin size, trained on 12 instruction datasets ranging from manually curated\n(e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and\nsystematically evaluate them on their factual knowledge, reasoning,\nmultilinguality, coding, and open-ended instruction following abilities through\na collection of automatic, model-based, and human-based metrics. We further\nintroduce T\\\"ulu, our best performing instruction-tuned model suite finetuned\non a combination of high-quality open resources.\n</p>\n<p>Our experiments show that different instruction-tuning datasets can uncover\nor enhance specific skills, while no single dataset (or combination) provides\nthe best performance across all evaluations. Interestingly, we find that model\nand human preference-based evaluations fail to reflect differences in model\ncapabilities exposed by benchmark-based evaluations, suggesting the need for\nthe type of systemic evaluation performed in this work. Our evaluations show\nthat the best model in any given evaluation reaches on average 83% of ChatGPT\nperformance, and 68% of GPT-4 performance, suggesting that further investment\nin building better base models and instruction-tuning data is required to close\nthe gap. We release our instruction-tuned models, including a fully finetuned\n65B T\\\"ulu, along with our code, data, and evaluation framework at\nhttps://github.com/allenai/open-instruct to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivison_H/0/1/0/all/0/1\">Hamish Ivison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasigi_P/0/1/0/all/0/1\">Pradeep Dasigi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1\">Tushar Khot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1\">Khyathi Raghavi Chandu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadden_D/0/1/0/all/0/1\">David Wadden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacMillan_K/0/1/0/all/0/1\">Kelsey MacMillan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models. (arXiv:2306.04757v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04757","description":"<p>Instruction-tuned large language models have revolutionized natural language\nprocessing and have shown great potential in applications such as\nconversational agents. These models, such as GPT-4, can not only master\nlanguage but also solve complex tasks in areas like mathematics, coding,\nmedicine, and law. Despite their impressive capabilities, there is still a lack\nof comprehensive understanding regarding their full potential, primarily due to\nthe black-box nature of many models and the absence of holistic evaluation\nstudies. To address these challenges, we present INSTRUCTEVAL, a more\ncomprehensive evaluation suite designed specifically for instruction-tuned\nlarge language models. Unlike previous works, our evaluation involves a\nrigorous assessment of models based on problem-solving, writing ability, and\nalignment to human values. We take a holistic approach to analyze various\nfactors affecting model performance, including the pretraining foundation,\ninstruction-tuning data, and training methods. Our findings reveal that the\nquality of instruction data is the most crucial factor in scaling model\nperformance. While open-source models demonstrate impressive writing abilities,\nthere is substantial room for improvement in problem-solving and alignment. We\nare encouraged by the rapid development of models by the open-source community,\nbut we also highlight the need for rigorous evaluation to support claims made\nabout these models. Through INSTRUCTEVAL, we aim to foster a deeper\nunderstanding of instruction-tuned models and advancements in their\ncapabilities. INSTRUCTEVAL is publicly available at\nhttps://github.com/declare-lab/instruct-eval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chia_Y/0/1/0/all/0/1\">Yew Ken Chia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1\">Pengfei Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The HCI Aspects of Public Deployment of Research Chatbots: A User Study, Design Recommendations, and Open Challenges. (arXiv:2306.04765v1 [cs.AI])","link":"http://arxiv.org/abs/2306.04765","description":"<p>Publicly deploying research chatbots is a nuanced topic involving necessary\nrisk-benefit analyses. While there have recently been frequent discussions on\nwhether it is responsible to deploy such models, there has been far less focus\non the interaction paradigms and design approaches that the resulting\ninterfaces should adopt, in order to achieve their goals more effectively. We\naim to pose, ground, and attempt to answer HCI questions involved in this\nscope, by reporting on a mixed-methods user study conducted on a recent\nresearch chatbot. We find that abstract anthropomorphic representation for the\nagent has a significant effect on user's perception, that offering AI\nexplainability may have an impact on feedback rates, and that two (diegetic and\nextradiegetic) levels of the chat experience should be intentionally designed.\nWe offer design recommendations and areas of further focus for the research\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Behrooz_M/0/1/0/all/0/1\">Morteza Behrooz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngan_W/0/1/0/all/0/1\">William Ngan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_J/0/1/0/all/0/1\">Joshua Lane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morse_G/0/1/0/all/0/1\">Giuliano Morse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babcock_B/0/1/0/all/0/1\">Benjamin Babcock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuster_K/0/1/0/all/0/1\">Kurt Shuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komeili_M/0/1/0/all/0/1\">Mojtaba Komeili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Moya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kambadur_M/0/1/0/all/0/1\">Melanie Kambadur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boureau_Y/0/1/0/all/0/1\">Y-Lan Boureau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Absformer: Transformer-based Model for Unsupervised Multi-Document Abstractive Summarization. (arXiv:2306.04787v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04787","description":"<p>Multi-document summarization (MDS) refers to the task of summarizing the text\nin multiple documents into a concise summary. The generated summary can save\nthe time of reading many documents by providing the important content in the\nform of a few sentences. Abstractive MDS aims to generate a coherent and fluent\nsummary for multiple documents using natural language generation techniques. In\nthis paper, we consider the unsupervised abstractive MDS setting where there\nare only documents with no groundtruh summaries provided, and we propose\nAbsformer, a new Transformer-based method for unsupervised abstractive summary\ngeneration. Our method consists of a first step where we pretrain a\nTransformer-based encoder using the masked language modeling (MLM) objective as\nthe pretraining task in order to cluster the documents into semantically\nsimilar groups; and a second step where we train a Transformer-based decoder to\ngenerate abstractive summaries for the clusters of documents. To our knowledge,\nwe are the first to successfully incorporate a Transformer-based model to solve\nthe unsupervised abstractive MDS task. We evaluate our approach using three\nreal-world datasets from different domains, and we demonstrate both substantial\nimprovements in terms of evaluation metrics over state-of-the-art\nabstractive-based methods, and generalization to datasets from different\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trabelsi_M/0/1/0/all/0/1\">Mohamed Trabelsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uzunalioglu_H/0/1/0/all/0/1\">Huseyin Uzunalioglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises. (arXiv:2306.04802v1 [cs.AI])","link":"http://arxiv.org/abs/2306.04802","description":"<p>Healthcare knowledge graphs (HKGs) have emerged as a promising tool for\norganizing medical knowledge in a structured and interpretable way, which\nprovides a comprehensive view of medical concepts and their relationships.\nHowever, challenges such as data heterogeneity and limited coverage remain,\nemphasizing the need for further research in the field of HKGs. This survey\npaper serves as the first comprehensive overview of HKGs. We summarize the\npipeline and key techniques for HKG construction (i.e., from scratch and\nthrough integration), as well as the common utilization approaches (i.e.,\nmodel-free and model-based). To provide researchers with valuable resources, we\norganize existing HKGs (The resource is available at\nhttps://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase) based on the\ndata types they capture and application domains, supplemented with pertinent\nstatistical information. In the application section, we delve into the\ntransformative impact of HKGs across various healthcare domains, spanning from\nfine-grained basic science research to high-level clinical decision support.\nLastly, we shed light on the opportunities for creating comprehensive and\naccurate HKGs in the era of large language models, presenting the potential to\nrevolutionize healthcare delivery and enhance the interpretability and\nreliability of clinical prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiaying Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wenjing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shaojun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_X/0/1/0/all/0/1\">Xuan Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1\">Chen Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Joyce Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privately generating tabular data using language models. (arXiv:2306.04803v1 [cs.LG])","link":"http://arxiv.org/abs/2306.04803","description":"<p>Privately generating synthetic data from a table is an important brick of a\nprivacy-first world. We propose and investigate a simple approach of treating\neach row in a table as a sentence and training a language model with\ndifferential privacy. We show this approach obtains competitive results in\nmodelling tabular data across multiple datasets, even at small scales that\nfavor alternative methods based on marginal distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sablayrolles_A/0/1/0/all/0/1\">Alexandre Sablayrolles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karrer_B/0/1/0/all/0/1\">Brian Karrer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Good Data, Large Data, or No Data? Comparing Three Approaches in Developing Research Aspect Classifiers for Biomedical Papers. (arXiv:2306.04820v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04820","description":"<p>The rapid growth of scientific publications, particularly during the COVID-19\npandemic, emphasizes the need for tools to help researchers efficiently\ncomprehend the latest advancements. One essential part of understanding\nscientific literature is research aspect classification, which categorizes\nsentences in abstracts to Background, Purpose, Method, and Finding. In this\nstudy, we investigate the impact of different datasets on model performance for\nthe crowd-annotated CODA-19 research aspect classification task. Specifically,\nwe explore the potential benefits of using the large, automatically curated\nPubMed 200K RCT dataset and evaluate the effectiveness of large language models\n(LLMs), such as LLaMA, GPT-3, ChatGPT, and GPT-4. Our results indicate that\nusing the PubMed 200K RCT dataset does not improve performance for the CODA-19\ntask. We also observe that while GPT-4 performs well, it does not outperform\nthe SciBERT model fine-tuned on the CODA-19 dataset, emphasizing the importance\nof a dedicated and task-aligned datasets dataset for the target task. Our code\nis available at https://github.com/Crowd-AI-Lab/CODA-19-exp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekhar_S/0/1/0/all/0/1\">Shreya Chandrasekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chieh-Yang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Hao &#x27;Kenneth&#x27; Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation for Improving Tail-traffic Robustness in Skill-routing for Dialogue Systems. (arXiv:2306.04823v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04823","description":"<p>Large-scale conversational systems typically rely on a skill-routing\ncomponent to route a user request to an appropriate skill and interpretation to\nserve the request. In such system, the agent is responsible for serving\nthousands of skills and interpretations which create a long-tail distribution\ndue to the natural frequency of requests. For example, the samples related to\nplay music might be a thousand times more frequent than those asking for\ntheatre show times. Moreover, inputs used for ML-based skill routing are often\na heterogeneous mix of strings, embedding vectors, categorical and scalar\nfeatures which makes employing augmentation-based long-tail learning approaches\nchallenging. To improve the skill-routing robustness, we propose an\naugmentation of heterogeneous skill-routing data and training targeted for\nrobust operation in long-tail data regimes. We explore a variety of conditional\nencoder-decoder generative frameworks to perturb original data fields and\ncreate synthetic training data. To demonstrate the effectiveness of the\nproposed method, we conduct extensive experiments using real-world data from a\ncommercial conversational system. Based on the experiment results, the proposed\napproach improves more than 80% (51 out of 63) of intents with less than 10K of\ntraffic instances in the skill-routing replication task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Ting-Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheikholeslami_F/0/1/0/all/0/1\">Fatemeh Sheikholeslami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kachuee_M/0/1/0/all/0/1\">Mohammad Kachuee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_J/0/1/0/all/0/1\">Jaeyoung Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungjin Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Vietnamese Legal Question--Answering System based on Automatic Data Enrichment. (arXiv:2306.04841v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04841","description":"<p>Question answering (QA) in law is a challenging problem because legal\ndocuments are much more complicated than normal texts in terms of terminology,\nstructure, and temporal and logical relationships. It is even more difficult to\nperform legal QA for low-resource languages like Vietnamese where labeled data\nare rare and pre-trained language models are still limited. In this paper, we\ntry to overcome these limitations by implementing a Vietnamese article-level\nretrieval-based legal QA system and introduce a novel method to improve the\nperformance of language models by improving data quality through weak labeling.\nOur hypothesis is that in contexts where labeled data are limited, efficient\ndata enrichment can help increase overall performance. Our experiments are\ndesigned to test multiple aspects, which demonstrate the effectiveness of the\nproposed technique.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vuong_T/0/1/0/all/0/1\">Thi-Hai-Yen Vuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha-Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1\">Quang-Huy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Le-Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_X/0/1/0/all/0/1\">Xuan-Hieu Phan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts. (arXiv:2306.04845v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04845","description":"<p>Weight-sharing supernet has become a vital component for performance\nestimation in the state-of-the-art (SOTA) neural architecture search (NAS)\nframeworks. Although supernet can directly generate different subnetworks\nwithout retraining, there is no guarantee for the quality of these subnetworks\nbecause of weight sharing. In NLP tasks such as machine translation and\npre-trained language modeling, we observe that given the same model\narchitecture, there is a large performance gap between supernet and training\nfrom scratch. Hence, supernet cannot be directly used and retraining is\nnecessary after finding the optimal architectures.\n</p>\n<p>In this work, we propose mixture-of-supernets, a generalized supernet\nformulation where mixture-of-experts (MoE) is adopted to enhance the expressive\npower of the supernet model, with negligible training overhead. In this way,\ndifferent subnetworks do not share the model weights directly, but through an\narchitecture-based routing mechanism. As a result, model weights of different\nsubnetworks are customized towards their specific architectures and the weight\ngeneration is learned by gradient descent. Compared to existing weight-sharing\nsupernet for NLP, our method can minimize the retraining time, greatly\nimproving training efficiency. In addition, the proposed method achieves the\nSOTA performance in NAS for building fast machine translation models, yielding\nbetter latency-BLEU tradeoff compared to HAT, state-of-the-art NAS for MT. We\nalso achieve the SOTA performance in NAS for building memory-efficient\ntask-agnostic BERT models, outperforming NAS-BERT and AutoDistil in various\nmodel sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_G/0/1/0/all/0/1\">Ganesh Jawahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haichuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yunyang Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zechun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dilin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Meng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappu_A/0/1/0/all/0/1\">Aasish Pappu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas Oguz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshmanan_L/0/1/0/all/0/1\">Laks V. S. Lakshmanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamoorthi_R/0/1/0/all/0/1\">Raghuraman Krishnamoorthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1\">Vikas Chandra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expanding Scope: Adapting English Adversarial Attacks to Chinese. (arXiv:2306.04874v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04874","description":"<p>Recent studies have revealed that NLP predictive models are vulnerable to\nadversarial attacks. Most existing studies focused on designing attacks to\nevaluate the robustness of NLP models in the English language alone. Literature\nhas seen an increasing need for NLP solutions for other languages. We,\ntherefore, ask one natural question: whether state-of-the-art (SOTA) attack\nmethods generalize to other languages. This paper investigates how to adapt\nSOTA adversarial attack algorithms in English to the Chinese language. Our\nexperiments show that attack methods previously applied to English NLP can\ngenerate high-quality adversarial examples in Chinese when combined with proper\ntext segmentation and linguistic constraints. In addition, we demonstrate that\nthe generated adversarial examples can achieve high fluency and semantic\nconsistency by focusing on the Chinese language's morphology and phonology,\nwhich in turn can be used to improve the adversarial robustness of Chinese NLP\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chengyuan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yanjun Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-Context Learning through the Bayesian Prism. (arXiv:2306.04891v1 [cs.LG])","link":"http://arxiv.org/abs/2306.04891","description":"<p>In-context learning is one of the surprising and useful features of large\nlanguage models. How it works is an active area of research. Recently, stylized\nmeta-learning-like setups have been devised that train these models on a\nsequence of input-output pairs $(x, f(x))$ from a function class using the\nlanguage modeling loss and observe generalization to unseen functions from the\nsame class. One of the main discoveries in this line of research has been that\nfor several problems such as linear regression, trained transformers learn\nalgorithms for learning functions in context. However, the inductive biases of\nthese models resulting in this behavior are not clearly understood. A model\nwith unlimited training data and compute is a Bayesian predictor: it learns the\npretraining distribution. It has been shown that high-capacity transformers\nmimic the Bayesian predictor for linear regression. In this paper, we show\nempirical evidence of transformers exhibiting the behavior of this ideal\nlearner across different linear and non-linear function classes. We also extend\nthe previous setups to work in the multitask setting and verify that\ntransformers can do in-context learning in this setup as well and the Bayesian\nperspective sheds light on this setting also. Finally, via the example of\nlearning Fourier series, we study the inductive bias for in-context learning.\nWe find that in-context learning may or may not have simplicity bias depending\non the pretraining data distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1\">Kabir Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panwar_M/0/1/0/all/0/1\">Madhur Panwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Navin Goyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NOWJ at COLIEE 2023 -- Multi-Task and Ensemble Approaches in Legal Information Processing. (arXiv:2306.04903v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04903","description":"<p>This paper presents the NOWJ team's approach to the COLIEE 2023 Competition,\nwhich focuses on advancing legal information processing techniques and applying\nthem to real-world legal scenarios. Our team tackles the four tasks in the\ncompetition, which involve legal case retrieval, legal case entailment, statute\nlaw retrieval, and legal textual entailment. We employ state-of-the-art machine\nlearning models and innovative approaches, such as BERT, Longformer,\nBM25-ranking algorithm, and multi-task learning models. Although our team did\nnot achieve state-of-the-art results, our findings provide valuable insights\nand pave the way for future improvements in legal information processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vuong_T/0/1/0/all/0/1\">Thi-Hai-Yen Vuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hai-Long Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tan-Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hoang-Trung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thai-Binh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha-Thanh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prefer to Classify: Improving Text Classifiers via Auxiliary Preference Learning. (arXiv:2306.04925v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04925","description":"<p>The development of largely human-annotated benchmarks has driven the success\nof deep neural networks in various NLP tasks. To enhance the effectiveness of\nexisting benchmarks, collecting new additional input-output pairs is often too\ncostly and challenging, particularly considering their marginal impact on\nimproving the current model accuracy. Instead, additional or complementary\nannotations on the existing input texts in the benchmarks can be preferable as\nan efficient way to pay the additional human cost. In this paper, we\ninvestigate task-specific preferences between pairs of input texts as a new\nalternative way for such auxiliary data annotation. From 'pair-wise'\ncomparisons with respect to the task, the auxiliary preference learning enables\nthe model to learn an additional informative training signal that cannot be\ncaptured with 'instance-wise' task labels. To this end, we propose a novel\nmulti-task learning framework, called prefer-to-classify (P2C), which can enjoy\nthe cooperative effect of learning both the given classification task and the\nauxiliary preferences. Here, we provide three different ways to collect\npreference signals in practice: (a) implicitly extracting from annotation\nrecords (for free, but often unavailable), (b) collecting explicitly from crowd\nworkers (high paid), or (c) pre-trained large language models such as GPT-3\n(low paid). Given existing classification NLP benchmarks, we demonstrate that\nthe proposed auxiliary preference learning via P2C on them is effective in\nimproving text classifiers. Our codes are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaehyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"covLLM: Large Language Models for COVID-19 Biomedical Literature. (arXiv:2306.04926v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04926","description":"<p>The COVID-19 pandemic led to 1.1 million deaths in the United States, despite\nthe explosion of coronavirus research. These new findings are slow to translate\nto clinical interventions, leading to poorer patient outcomes and unnecessary\ndeaths. One reason is that clinicians, overwhelmed by patients, struggle to\nkeep pace with the rate of new coronavirus literature. A potential solution is\ndeveloping a tool for evaluating coronavirus literature using large language\nmodels (LLMs) -- neural networks that are deployed for natural language\nprocessing. LLMs can be used to summarize and extract user-specified\ninformation. The greater availability and advancement of LLMs and pre-processed\ncoronavirus literature databases provide the opportunity to assist clinicians\nin evaluating coronavirus literature through a coronavirus literature specific\nLLM (covLLM), a tool that directly takes an inputted research article and a\nuser query to return an answer. Using the COVID-19 Open Research Dataset\n(CORD-19), we produced two datasets: (1) synCovid, which uses a combination of\nhandwritten prompts and synthetic prompts generated using OpenAI, and (2) real\nabstracts, which contains abstract and title pairs. covLLM was trained with\nLLaMA 7B as a baseline model to produce three models trained on (1) the Alpaca\nand synCovid datasets, (2) the synCovid dataset, and (3) the synCovid and real\nabstract datasets. These models were evaluated by two human evaluators and\nChatGPT. Results demonstrate that training covLLM on the synCovid and abstract\npairs datasets performs competitively with ChatGPT and outperforms covLLM\ntrained primarily using the Alpaca dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_Y/0/1/0/all/0/1\">Yousuf A. Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hokia_C/0/1/0/all/0/1\">Clarisse Hokia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jennifer Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehlert_B/0/1/0/all/0/1\">Ben Ehlert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding. (arXiv:2306.04933v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04933","description":"<p>Soft prompt tuning achieves superior performances across a wide range of\nfew-shot tasks. However, the performances of prompt tuning can be highly\nsensitive to the initialization of the prompts. We also empirically observe\nthat conventional prompt tuning methods cannot encode and learn sufficient\ntask-relevant information from prompt tokens. In this work, we develop an\ninformation-theoretic framework that formulates soft prompt tuning as\nmaximizing mutual information between prompts and other model parameters (or\nencoded representations). This novel view helps us to develop a more efficient,\naccurate and robust soft prompt tuning method InfoPrompt. With this framework,\nwe develop two novel mutual information based loss functions, to (i) discover\nproper prompt initialization for the downstream tasks and learn sufficient\ntask-relevant information from prompt tokens and (ii) encourage the output\nrepresentation from the pretrained language model to be more aware of the\ntask-relevant information captured in the learnt prompt. Extensive experiments\nvalidate that InfoPrompt can significantly accelerate the convergence of the\nprompt tuning and outperform traditional prompt tuning methods. Finally, we\nprovide a formal theoretical result for showing to show that gradient descent\ntype algorithm can be used to train our mutual information loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junda Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruiyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Handong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chaochao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henao_R/0/1/0/all/0/1\">Ricardo Henao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A modified model for topic detection from a corpus and a new metric evaluating the understandability of topics. (arXiv:2306.04941v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04941","description":"<p>This paper presents a modified neural model for topic detection from a corpus\nand proposes a new metric to evaluate the detected topics. The new model builds\nupon the embedded topic model incorporating some modifications such as document\nclustering. Numerical experiments suggest that the new model performs\nfavourably regardless of the document's length. The new metric, which can be\ncomputed more efficiently than widely-used metrics such as topic coherence,\nprovides variable information regarding the understandability of the detected\ntopics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kitano_T/0/1/0/all/0/1\">Tomoya Kitano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyatake_Y/0/1/0/all/0/1\">Yuto Miyatake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furihata_D/0/1/0/all/0/1\">Daisuke Furihata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Set Relation Extraction via Unknown-Aware Training. (arXiv:2306.04950v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04950","description":"<p>The existing supervised relation extraction methods have achieved impressive\nperformance in a closed-set setting, where the relations during both training\nand testing remain the same. In a more realistic open-set setting, unknown\nrelations may appear in the test set. Due to the lack of supervision signals\nfrom unknown relations, a well-performing closed-set relation extractor can\nstill confidently misclassify them into known relations. In this paper, we\npropose an unknown-aware training method, regularizing the model by dynamically\nsynthesizing negative instances. To facilitate a compact decision boundary,\n``difficult'' negative instances are necessary. Inspired by text adversarial\nattacks, we adaptively apply small but critical perturbations to original\ntraining instances and thus synthesizing negative instances that are more\nlikely to be mistaken by the model as known relations. Experimental results\nshow that this method achieves SOTA unknown relation detection without\ncompromising the classification of known relations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1\">Wenyu Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RE-Matching: A Fine-Grained Semantic Matching Method for Zero-Shot Relation Extraction. (arXiv:2306.04954v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04954","description":"<p>Semantic matching is a mainstream paradigm of zero-shot relation extraction,\nwhich matches a given input with a corresponding label description. The\nentities in the input should exactly match their hypernyms in the description,\nwhile the irrelevant contexts should be ignored when matching. However, general\nmatching methods lack explicit modeling of the above matching pattern. In this\nwork, we propose a fine-grained semantic matching method tailored for zero-shot\nrelation extraction. Following the above matching pattern, we decompose the\nsentence-level similarity score into entity and context matching scores. Due to\nthe lack of explicit annotations of the redundant components, we design a\nfeature distillation module to adaptively identify the relation-irrelevant\nfeatures and reduce their negative impact on context matching. Experimental\nresults show that our method achieves higher matching $F_1$ score and has an\ninference speed 10 times faster, when compared with the state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1\">Wenyu Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junzhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1\">Minlong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mingming Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Language Identification to Enhance Code-Mixed Text Classification. (arXiv:2306.04964v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04964","description":"<p>The usage of more than one language in the same text is referred to as Code\nMixed. It is evident that there is a growing degree of adaption of the use of\ncode-mixed data, especially English with a regional language, on social media\nplatforms. Existing deep-learning models do not take advantage of the implicit\nlanguage information in the code-mixed text. Our study aims to improve\nBERT-based models performance on low-resource Code-Mixed Hindi-English Datasets\nby experimenting with language augmentation approaches. We propose a pipeline\nto improve code-mixed systems that comprise data preprocessing, word-level\nlanguage identification, language augmentation, and model training on\ndownstream tasks like sentiment analysis. For language augmentation in BERT\nmodels, we explore word-level interleaving and post-sentence placement of\nlanguage information. We have examined the performance of vanilla BERT-based\nmodels and their code-mixed HingBERT counterparts on respective benchmark\ndatasets, comparing their results with and without using word-level language\ninformation. The models were evaluated using metrics such as accuracy,\nprecision, recall, and F1 score. Our findings show that the proposed language\naugmentation approaches work well across different BERT models. We demonstrate\nthe importance of augmenting code-mixed text with language information on five\ndifferent code-mixed Hindi-English downstream datasets based on sentiment\nanalysis, hate speech detection, and emotion detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Takawane_G/0/1/0/all/0/1\">Gauri Takawane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phaltankar_A/0/1/0/all/0/1\">Abhishek Phaltankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwardhan_V/0/1/0/all/0/1\">Varad Patwardhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_A/0/1/0/all/0/1\">Aryan Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takalikar_M/0/1/0/all/0/1\">Mukta S. Takalikar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Actively Supervised Clustering for Open Relation Extraction. (arXiv:2306.04968v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04968","description":"<p>Current clustering-based Open Relation Extraction (OpenRE) methods usually\nadopt a two-stage pipeline. The first stage simultaneously learns relation\nrepresentations and assignments. The second stage manually labels several\ninstances and thus names the relation for each cluster. However, unsupervised\nobjectives struggle to optimize the model to derive accurate clustering\nassignments, and the number of clusters has to be supplied in advance. In this\npaper, we present a novel setting, named actively supervised clustering for\nOpenRE. Our insight lies in that clustering learning and relation labeling can\nbe alternately performed, providing the necessary guidance for clustering\nwithout a significant increase in human effort. The key to the setting is\nselecting which instances to label. Instead of using classical active labeling\nstrategies designed for fixed known classes, we propose a new strategy, which\nis applicable to dynamically discover clusters of unknown relations.\nExperimental results show that our method is able to discover almost all\nrelational clusters in the data and improve the SOTA methods by 10.3\\% and\n5.2\\%, on two datasets respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1\">Minlong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mingming Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models. (arXiv:2306.04980v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04980","description":"<p>This work introduces approaches to assessing phrase breaks in ESL learners'\nspeech using pre-trained language models (PLMs) and large language models\n(LLMs). There are two tasks: overall assessment of phrase break for a speech\nclip and fine-grained assessment of every possible phrase break position. To\nleverage NLP models, speech input is first force-aligned with texts, and then\npre-processed into a token sequence, including words and phrase break\ninformation. To utilize PLMs, we propose a pre-training and fine-tuning\npipeline with the processed tokens. This process includes pre-training with a\nreplaced break token detection module and fine-tuning with text classification\nand sequence labeling. To employ LLMs, we design prompts for ChatGPT. The\nexperiments show that with the PLMs, the dependence on labeled training data\nhas been greatly reduced, and the performance has improved. Meanwhile, we\nverify that ChatGPT, a renowned LLM, has potential for further advancement in\nthis area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shaoguang Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenshan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tien_J/0/1/0/all/0/1\">Jonathan Tien</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification. (arXiv:2306.04996v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04996","description":"<p>Cross-lingual text classification leverages text classifiers trained in a\nhigh-resource language to perform text classification in other languages with\nno or minimal fine-tuning (zero/few-shots cross-lingual transfer). Nowadays,\ncross-lingual text classifiers are typically built on large-scale, multilingual\nlanguage models (LMs) pretrained on a variety of languages of interest.\nHowever, the performance of these models vary significantly across languages\nand classification tasks, suggesting that the superposition of the language\nmodelling and classification tasks is not always effective. For this reason, in\nthis paper we propose revisiting the classic \"translate-and-test\" pipeline to\nneatly separate the translation and classification stages. The proposed\napproach couples 1) a neural machine translator translating from the targeted\nlanguage to a high-resource language, with 2) a text classifier trained in the\nhigh-resource language, but the neural machine translator generates \"soft\"\ntranslations to permit end-to-end backpropagation during fine-tuning of the\npipeline. Extensive experiments have been carried out over three cross-lingual\ntext classification datasets (XNLI, MLDoc and MultiEURLEX), with the results\nshowing that the proposed approach has significantly improved performance over\na competitive baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Unanue_I/0/1/0/all/0/1\">Inigo Jauregi Unanue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccardi_M/0/1/0/all/0/1\">Massimo Piccardi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Medical Diagnostics with Structured Data Extraction by Large Language Models. (arXiv:2306.05052v1 [cs.LG])","link":"http://arxiv.org/abs/2306.05052","description":"<p>Tabular data is often hidden in text, particularly in medical diagnostic\nreports. Traditional machine learning (ML) models designed to work with tabular\ndata, cannot effectively process information in such form. On the other hand,\nlarge language models (LLMs) which excel at textual tasks, are probably not the\nbest tool for modeling tabular data. Therefore, we propose a novel, simple, and\neffective methodology for extracting structured tabular data from textual\nmedical reports, called TEMED-LLM. Drawing upon the reasoning capabilities of\nLLMs, TEMED-LLM goes beyond traditional extraction techniques, accurately\ninferring tabular features, even when their names are not explicitly mentioned\nin the text. This is achieved by combining domain-specific reasoning guidelines\nwith a proposed data validation and reasoning correction feedback loop. By\napplying interpretable ML models such as decision trees and logistic regression\nover the extracted and validated data, we obtain end-to-end interpretable\npredictions. We demonstrate that our approach significantly outperforms\nstate-of-the-art text classification models in medical diagnostics. Given its\npredictive performance, simplicity, and interpretability, TEMED-LLM underscores\nthe potential of leveraging LLMs to improve the performance and trustworthiness\nof ML models in medical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bisercic_A/0/1/0/all/0/1\">Aleksa Bisercic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolic_M/0/1/0/all/0/1\">Mladen Nikolic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1\">Mihaela van der Schaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delibasic_B/0/1/0/all/0/1\">Boris Delibasic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Lio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrovic_A/0/1/0/all/0/1\">Andrija Petrovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning A Foundation Language Model for Geoscience Knowledge Understanding and Utilization. (arXiv:2306.05064v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05064","description":"<p>Large language models (LLMs)have achieved great success in general domains of\nnatural language processing. In this paper, we bring LLMs to the realm of\ngeoscience, with the objective of advancing research and applications in this\nfield. To this end, we present the first-ever LLM in geoscience, K2, alongside\na suite of resources developed to further promote LLM research within\ngeoscience. For instance, we have curated the first geoscience instruction\ntuning dataset, GeoSignal, which aims to align LLM responses to\ngeoscience-related user queries. Additionally, we have established the first\ngeoscience benchmark, GeoBenchmark, to evaluate LLMs in the context of\ngeoscience. In this work, we experiment with a complete recipe to adapt a\npretrained general-domain LLM to the geoscience domain. Specifically, we\nfurther train the LLaMA-7B model on over 1 million pieces of geoscience\nliterature and utilize GeoSignal's supervised data to fine-tune the model.\nMoreover, we share a protocol that can efficiently gather domain-specific data\nand construct domain-supervised data, even in situations where manpower is\nscarce. Experiments conducted on the GeoBenchmark demonstrate the the\neffectiveness of our approach and datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Cheng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhongmou He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuanyuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Le Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1\">Luoyi Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinbing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chenghu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LCT-1 at SemEval-2023 Task 10: Pre-training and Multi-task Learning for Sexism Detection and Classification. (arXiv:2306.05075v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05075","description":"<p>Misogyny and sexism are growing problems in social media. Advances have been\nmade in online sexism detection but the systems are often uninterpretable.\nSemEval-2023 Task 10 on Explainable Detection of Online Sexism aims at\nincreasing explainability of the sexism detection, and our team participated in\nall the proposed subtasks. Our system is based on further domain-adaptive\npre-training (Gururangan et al., 2020). Building on the Transformer-based\nmodels with the domain adaptation, we compare fine-tuning with multi-task\nlearning and show that each subtask requires a different system configuration.\nIn our experiments, multi-task learning performs on par with standard\nfine-tuning for sexism detection and noticeably better for coarse-grained\nsexism classification, while fine-tuning is preferable for fine-grained\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chernyshev_K/0/1/0/all/0/1\">Konstantin Chernyshev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garanina_E/0/1/0/all/0/1\">Ekaterina Garanina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayram_D/0/1/0/all/0/1\">Duygu Bayram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qiankun Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edman_L/0/1/0/all/0/1\">Lukas Edman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models. (arXiv:2306.05076v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05076","description":"<p>A few benchmarking datasets have been released to evaluate the factual\nknowledge of pretrained language models. These benchmarks (e.g., LAMA, and\nParaRel) are mainly developed in English and later are translated to form new\nmultilingual versions (e.g., mLAMA, and mParaRel). Results on these\nmultilingual benchmarks suggest that using English prompts to recall the facts\nfrom multilingual models usually yields significantly better and more\nconsistent performance than using non-English prompts. Our analysis shows that\nmLAMA is biased toward facts from Western countries, which might affect the\nfairness of probing models. We propose a new framework for curating factual\ntriples from Wikidata that are culturally diverse. A new benchmark DLAMA-v1 is\nbuilt of factual triples from three pairs of contrasting cultures having a\ntotal of 78,259 triples from 20 relation predicates. The three pairs comprise\nfacts representing the (Arab and Western), (Asian and Western), and (South\nAmerican and Western) countries respectively. Having a more balanced benchmark\n(DLAMA-v1) supports that mBERT performs better on Western facts than\nnon-Western ones, while monolingual Arabic, English, and Korean models tend to\nperform better on their culturally proximate facts. Moreover, both monolingual\nand multilingual models tend to make a prediction that is culturally or\ngeographically relevant to the correct label, even if the prediction is wrong.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keleg_A/0/1/0/all/0/1\">Amr Keleg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magdy_W/0/1/0/all/0/1\">Walid Magdy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Language Model Integration for Neural Machine Translation. (arXiv:2306.05077v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05077","description":"<p>The integration of language models for neural machine translation has been\nextensively studied in the past. It has been shown that an external language\nmodel, trained on additional target-side monolingual data, can help improve\ntranslation quality. However, there has always been the assumption that the\ntranslation model also learns an implicit target-side language model during\ntraining, which interferes with the external language model at decoding time.\nRecently, some works on automatic speech recognition have demonstrated that, if\nthe implicit language model is neutralized in decoding, further improvements\ncan be gained when integrating an external language model. In this work, we\ntransfer this concept to the task of machine translation and compare with the\nmost prominent way of including additional monolingual data - namely\nback-translation. We find that accounting for the implicit language model\nsignificantly boosts the performance of language model fusion, although this\napproach is still outperformed by back-translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Herold_C/0/1/0/all/0/1\">Christian Herold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yingbo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1\">Mohammad Zeineldeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Robustness of AI Offensive Code Generators via Data Augmentation. (arXiv:2306.05079v1 [cs.LG])","link":"http://arxiv.org/abs/2306.05079","description":"<p>In this work, we present a method to add perturbations to the code\ndescriptions, i.e., new inputs in natural language (NL) from well-intentioned\ndevelopers, in the context of security-oriented code, and analyze how and to\nwhat extent perturbations affect the performance of AI offensive code\ngenerators. Our experiments show that the performance of the code generators is\nhighly affected by perturbations in the NL descriptions. To enhance the\nrobustness of the code generators, we use the method to perform data\naugmentation, i.e., to increase the variability and diversity of the training\ndata, proving its effectiveness against both perturbed and non-perturbed code\ndescriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Improta_C/0/1/0/all/0/1\">Cristina Improta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liguori_P/0/1/0/all/0/1\">Pietro Liguori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natella_R/0/1/0/all/0/1\">Roberto Natella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cukic_B/0/1/0/all/0/1\">Bojan Cukic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotroneo_D/0/1/0/all/0/1\">Domenico Cotroneo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revealing the Blind Spot of Sentence Encoder Evaluation by HEROS. (arXiv:2306.05083v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05083","description":"<p>Existing sentence textual similarity benchmark datasets only use a single\nnumber to summarize how similar the sentence encoder's decision is to humans'.\nHowever, it is unclear what kind of sentence pairs a sentence encoder (SE)\nwould consider similar. Moreover, existing SE benchmarks mainly consider\nsentence pairs with low lexical overlap, so it is unclear how the SEs behave\nwhen two sentences have high lexical overlap. We introduce a high-quality SE\ndiagnostic dataset, HEROS. HEROS is constructed by transforming an original\nsentence into a new sentence based on certain rules to form a \\textit{minimal\npair}, and the minimal pair has high lexical overlaps. The rules include\nreplacing a word with a synonym, an antonym, a typo, a random word, and\nconverting the original sentence into its negation. Different rules yield\ndifferent subsets of HEROS. By systematically comparing the performance of over\n60 supervised and unsupervised SEs on HEROS, we reveal that most unsupervised\nsentence encoders are insensitive to negation. We find the datasets used to\ntrain the SE are the main determinants of what kind of sentence pairs an SE\nconsiders similar. We also show that even if two SEs have similar performance\non STS benchmarks, they can have very different behavior on HEROS. Our result\nreveals the blind spot of traditional STS benchmarks when evaluating SEs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_C/0/1/0/all/0/1\">Cheng-Han Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yung-Sung Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization. (arXiv:2306.05087v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05087","description":"<p>Instruction tuning large language models (LLMs) remains a challenging task,\nowing to the complexity of hyperparameter selection and the difficulty involved\nin evaluating the tuned models. To determine the optimal hyperparameters, an\nautomatic, robust, and reliable evaluation benchmark is essential. However,\nestablishing such a benchmark is not a trivial task due to the challenges\nassociated with evaluation accuracy and privacy protection. In response to\nthese challenges, we introduce a judge large language model, named PandaLM,\nwhich is trained to distinguish the superior model given several LLMs.\nPandaLM's focus extends beyond just the objective correctness of responses,\nwhich is the main focus of traditional evaluation datasets. It addresses vital\nsubjective factors such as relative conciseness, clarity, adherence to\ninstructions, comprehensiveness, and formality. To ensure the reliability of\nPandaLM, we collect a diverse human-annotated test dataset, where all contexts\nare generated by humans and labels are aligned with human preferences. Our\nresults indicate that PandaLM-7B achieves 93.75% of GPT-3.5's evaluation\nability and 88.28% of GPT-4's in terms of F1-score on our test dataset. PandaLM\nenables the evaluation of LLM to be fairer but with less cost, evidenced by\nsignificant improvements achieved by models tuned through PandaLM compared to\ntheir counterparts trained with default Alpaca's hyperparameters. In addition,\nPandaLM does not depend on API-based evaluations, thus avoiding potential data\nleakage. All resources of PandaLM are released at\nhttps://github.com/WeOpenML/PandaLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhuohao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhengran Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cunxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chaoya Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Rui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shikun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The ART of Conversation: Measuring Phonetic Convergence and Deliberate Imitation in L2-Speech with a Siamese RNN. (arXiv:2306.05088v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05088","description":"<p>Phonetic convergence describes the automatic and unconscious speech\nadaptation of two interlocutors in a conversation. This paper proposes a\nSiamese recurrent neural network (RNN) architecture to measure the convergence\nof the holistic spectral characteristics of speech sounds in an L2-L2\ninteraction. We extend an alternating reading task (the ART) dataset by adding\n20 native Slovak L2 English speakers. We train and test the Siamese RNN model\nto measure phonetic convergence of L2 English speech from three different\nnative language groups: Italian (9 dyads), French (10 dyads) and Slovak (10\ndyads). Our results indicate that the Siamese RNN model effectively captures\nthe dynamics of phonetic convergence and the speaker's imitation ability.\nMoreover, this text-independent model is scalable and capable of handling\nL1-induced speaker variability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Pastore_A/0/1/0/all/0/1\">Aldo Pastore</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Jong_D/0/1/0/all/0/1\">Dorina de Jong</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Fadiga_L/0/1/0/all/0/1\">Luciano Fadiga</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+DAusilio_A/0/1/0/all/0/1\">Alessandro D&#x27;Ausilio</a> (1 and 2) ((1) Istituto Italiano di Tecnologia, Italy, (2) Universit&#xe0; degli Studi di Ferrara, Italy, (3) University of California San Diego, USA)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Closing the Loop: Testing ChatGPT to Generate Model Explanations to Improve Human Labelling of Sponsored Content on Social Media. (arXiv:2306.05115v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05115","description":"<p>Regulatory bodies worldwide are intensifying their efforts to ensure\ntransparency in influencer marketing on social media through instruments like\nthe Unfair Commercial Practices Directive (UCPD) in the European Union, or\nSection 5 of the Federal Trade Commission Act. Yet enforcing these obligations\nhas proven to be highly problematic due to the sheer scale of the influencer\nmarket. The task of automatically detecting sponsored content aims to enable\nthe monitoring and enforcement of such regulations at scale. Current research\nin this field primarily frames this problem as a machine learning task,\nfocusing on developing models that achieve high classification performance in\ndetecting ads. These machine learning tasks rely on human data annotation to\nprovide ground truth information. However, agreement between annotators is\noften low, leading to inconsistent labels that hinder the reliability of\nmodels. To improve annotation accuracy and, thus, the detection of sponsored\ncontent, we propose using chatGPT to augment the annotation process with\nphrases identified as relevant features and brief explanations. Our experiments\nshow that this approach consistently improves inter-annotator agreement and\nannotation accuracy. Additionally, our survey of user experience in the\nannotation task indicates that the explanations improve the annotators'\nconfidence and streamline the process. Our proposed methods can ultimately lead\nto more transparency and alignment with regulatory requirements in sponsored\ncontent detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bertaglia_T/0/1/0/all/0/1\">Thales Bertaglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_S/0/1/0/all/0/1\">Stefan Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goanta_C/0/1/0/all/0/1\">Catalina Goanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spanakis_G/0/1/0/all/0/1\">Gerasimos Spanakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iamnitchi_A/0/1/0/all/0/1\">Adriana Iamnitchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Search Strategies for Document-Level Neural Machine Translation. (arXiv:2306.05116v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05116","description":"<p>Compared to sentence-level systems, document-level neural machine translation\n(NMT) models produce a more consistent output across a document and are able to\nbetter resolve ambiguities within the input. There are many works on\ndocument-level NMT, mostly focusing on modifying the model architecture or\ntraining strategy to better accommodate the additional context-input. On the\nother hand, in most works, the question on how to perform search with the\ntrained model is scarcely discussed, sometimes not mentioned at all. In this\nwork, we aim to answer the question how to best utilize a context-aware\ntranslation model in decoding. We start with the most popular document-level\nNMT approach and compare different decoding schemes, some from the literature\nand others proposed by us. In the comparison, we are using both, standard\nautomatic metrics, as well as specific linguistic phenomena on three standard\ndocument-level translation benchmarks. We find that most commonly used decoding\nstrategies perform similar to each other and that higher quality context\ninformation has the potential to further improve the translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Herold_C/0/1/0/all/0/1\">Christian Herold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework. (arXiv:2306.05119v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05119","description":"<p>Factuality is important to dialogue summarization. Factual error correction\n(FEC) of model-generated summaries is one way to improve factuality. Current\nFEC evaluation that relies on factuality metrics is not reliable and detailed\nenough. To address this problem, we are the first to manually annotate a FEC\ndataset for dialogue summarization containing 4000 items and propose FERRANTI,\na fine-grained evaluation framework based on reference correction that\nautomatically evaluates the performance of FEC models on different error\ncategories. Using this evaluation framework, we conduct sufficient experiments\nwith FEC approaches under a variety of settings and find the best training\nmodes and significant differences in the performance of the existing approaches\non different factual error categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jia Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhefeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huai_B/0/1/0/all/0/1\">Baoxing Huai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mapping Brains with Language Models: A Survey. (arXiv:2306.05126v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05126","description":"<p>Over the years, many researchers have seemingly made the same observation:\nBrain and language model activations exhibit some structural similarities,\nenabling linear partial mappings between features extracted from neural\nrecordings and computational language models. In an attempt to evaluate how\nmuch evidence has been accumulated for this observation, we survey over 30\nstudies spanning 10 datasets and 8 metrics. How much evidence has been\naccumulated, and what, if anything, is missing before we can draw conclusions?\nOur analysis of the evaluation methods used in the literature reveals that some\nof the metrics are less conservative. We also find that the accumulated\nevidence, for now, remains ambiguous, but correlations with model size and\nquality provide grounds for cautious optimism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karamolegkou_A/0/1/0/all/0/1\">Antonia Karamolegkou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdou_M/0/1/0/all/0/1\">Mostafa Abdou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RRWKV: Capturing Long-range Dependencies in RWKV. (arXiv:2306.05176v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05176","description":"<p>Owing to the impressive dot-product attention, the Transformers have been the\ndominant architectures in various natural language processing (NLP) tasks.\nRecently, the Receptance Weighted Key Value (RWKV) architecture follows a\nnon-transformer architecture to eliminate the drawbacks of dot-product\nattention, where memory and computational complexity exhibits quadratic scaling\nwith sequence length. Although RWKV has exploited a linearly tensor-product\nattention mechanism and achieved parallelized computations by deploying the\ntime-sequential mode, it fails to capture long-range dependencies because of\nits limitation on looking back at previous information, compared with full\ninformation obtained by direct interactions in the standard transformer.\nTherefore, the paper devises the Retrospected Receptance Weighted Key Value\n(RRWKV) architecture via incorporating the retrospecting ability into the RWKV\nto effectively absorb information, which maintains memory and computational\nefficiency as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Leilei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models. (arXiv:2306.05179v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05179","description":"<p>Despite the existence of various benchmarks for evaluating natural language\nprocessing models, we argue that human exams are a more suitable means of\nevaluating general intelligence for large language models (LLMs), as they\ninherently demand a much wider range of abilities such as language\nunderstanding, domain knowledge, and problem-solving skills. To this end, we\nintroduce M3Exam, a novel benchmark sourced from real and official human exam\nquestions for evaluating LLMs in a multilingual, multimodal, and multilevel\ncontext. M3Exam exhibits three unique characteristics: (1) multilingualism,\nencompassing questions from multiple countries that require strong multilingual\nproficiency and cultural knowledge; (2) multimodality, accounting for the\nmultimodal nature of many exam questions to test the model's multimodal\nunderstanding capability; and (3) multilevel structure, featuring exams from\nthree critical educational periods to comprehensively assess a model's\nproficiency at different levels. In total, M3Exam contains 12,317 questions in\n9 diverse languages with three educational levels, where about 23\\% of the\nquestions require processing images for successful solving. We assess the\nperformance of top-performing LLMs on M3Exam and find that current models,\nincluding GPT-4, still struggle with multilingual text, particularly in\nlow-resource and non-Latin script languages. Multimodal LLMs also perform\npoorly with complex multimodal questions. We believe that M3Exam can be a\nvaluable resource for comprehensively evaluating LLMs by examining their\nmultilingual and multimodal abilities and tracking their development. Data and\nevaluation code is available at \\url{https://github.com/DAMO-NLP-SG/M3Exam}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aljunied_S/0/1/0/all/0/1\">Sharifah Mahani Aljunied</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chia_Y/0/1/0/all/0/1\">Yew Ken Chia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Long Context Document-Level Machine Translation. (arXiv:2306.05183v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05183","description":"<p>Document-level context for neural machine translation (NMT) is crucial to\nimprove the translation consistency and cohesion, the translation of ambiguous\ninputs, as well as several other linguistic phenomena. Many works have been\npublished on the topic of document-level NMT, but most restrict the system to\nonly local context, typically including just the one or two preceding sentences\nas additional information. This might be enough to resolve some ambiguous\ninputs, but it is probably not sufficient to capture some document-level\ninformation like the topic or style of a conversation. When increasing the\ncontext size beyond just the local context, there are two challenges: (i)\nthe~memory usage increases exponentially (ii) the translation performance\nstarts to degrade. We argue that the widely-used attention mechanism is\nresponsible for both issues. Therefore, we propose a constrained attention\nvariant that focuses the attention on the most relevant parts of the sequence,\nwhile simultaneously reducing the memory consumption. For evaluation, we\nutilize targeted test sets in combination with novel evaluation techniques to\nanalyze the translations in regards to specific discourse-related phenomena. We\nfind that our approach is a good compromise between sentence-level NMT vs\nattending to the full context, especially in low resource scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Herold_C/0/1/0/all/0/1\">Christian Herold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dealing with Semantic Underspecification in Multimodal NLP. (arXiv:2306.05240v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05240","description":"<p>Intelligent systems that aim at mastering language as humans do must deal\nwith its semantic underspecification, namely, the possibility for a linguistic\nsignal to convey only part of the information needed for communication to\nsucceed. Consider the usages of the pronoun they, which can leave the gender\nand number of its referent(s) underspecified. Semantic underspecification is\nnot a bug but a crucial language feature that boosts its storage and processing\nefficiency. Indeed, human speakers can quickly and effortlessly integrate\nsemantically-underspecified linguistic signals with a wide range of\nnon-linguistic information, e.g., the multimodal context, social or cultural\nconventions, and shared knowledge. Standard NLP models have, in principle, no\nor limited access to such extra information, while multimodal systems grounding\nlanguage into other modalities, such as vision, are naturally equipped to\naccount for this phenomenon. However, we show that they struggle with it, which\ncould negatively affect their performance and lead to harmful consequences when\nused for applications. In this position paper, we argue that our community\nshould be aware of semantic underspecification if it aims to develop language\ntechnology that can successfully interact with human users. We discuss some\napplications where mastering it is crucial and outline a few directions toward\nachieving this goal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pezzelle_S/0/1/0/all/0/1\">Sandro Pezzelle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factorized Contrastive Learning: Going Beyond Multi-view Redundancy. (arXiv:2306.05268v1 [cs.LG])","link":"http://arxiv.org/abs/2306.05268","description":"<p>In a wide range of multimodal tasks, contrastive learning has become a\nparticularly appealing approach since it can successfully learn representations\nfrom abundant unlabeled data with only pairing information (e.g., image-caption\nor video-audio pairs). Underpinning these approaches is the assumption of\nmulti-view redundancy - that shared information between modalities is necessary\nand sufficient for downstream tasks. However, in many real-world settings,\ntask-relevant information is also contained in modality-unique regions:\ninformation that is only present in one modality but still relevant to the\ntask. How can we learn self-supervised multimodal representations to capture\nboth shared and unique information relevant to downstream tasks? This paper\nproposes FactorCL, a new multimodal representation learning method to go beyond\nmulti-view redundancy. FactorCL is built from three new contributions: (1)\nfactorizing task-relevant information into shared and unique representations,\n(2) capturing task-relevant information via maximizing MI lower bounds and\nremoving task-irrelevant information via minimizing MI upper bounds, and (3)\nmultimodal data augmentations to approximate task relevance without labels. On\nlarge-scale real-world datasets, FactorCL captures both shared and unique\ninformation and achieves state-of-the-art results on six benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zihao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Martin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overview of the Problem List Summarization (ProbSum) 2023 Shared Task on Summarizing Patients' Active Diagnoses and Problems from Electronic Health Record Progress Notes. (arXiv:2306.05270v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05270","description":"<p>The BioNLP Workshop 2023 initiated the launch of a shared task on Problem\nList Summarization (ProbSum) in January 2023. The aim of this shared task is to\nattract future research efforts in building NLP models for real-world\ndiagnostic decision support applications, where a system generating relevant\nand accurate diagnoses will augment the healthcare providers decision-making\nprocess and improve the quality of care for patients. The goal for participants\nis to develop models that generated a list of diagnoses and problems using\ninput from the daily care notes collected from the hospitalization of\ncritically ill patients. Eight teams submitted their final systems to the\nshared task leaderboard. In this paper, we describe the tasks, datasets,\nevaluation metrics, and baseline systems. Additionally, the techniques and\nresults of the evaluation of the different approaches tried by the\nparticipating teams are summarized.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yanjun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dligach_D/0/1/0/all/0/1\">Dmitriy Dligach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_T/0/1/0/all/0/1\">Timothy Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Churpek_M/0/1/0/all/0/1\">Matthew M. Churpek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afshar_M/0/1/0/all/0/1\">Majid Afshar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extensive Evaluation of Transformer-based Architectures for Adverse Drug Events Extraction. (arXiv:2306.05276v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05276","description":"<p>Adverse Event (ADE) extraction is one of the core tasks in digital\npharmacovigilance, especially when applied to informal texts. This task has\nbeen addressed by the Natural Language Processing community using large\npre-trained language models, such as BERT. Despite the great number of\nTransformer-based architectures used in the literature, it is unclear which of\nthem has better performances and why. Therefore, in this paper we perform an\nextensive evaluation and analysis of 19 Transformer-based models for ADE\nextraction on informal texts. We compare the performance of all the considered\nmodels on two datasets with increasing levels of informality (forums posts and\ntweets). We also combine the purely Transformer-based models with two\ncommonly-used additional processing layers (CRF and LSTM), and analyze their\neffect on the models performance. Furthermore, we use a well-established\nfeature importance technique (SHAP) to correlate the performance of the models\nwith a set of features that describe them: model category (AutoEncoding,\nAutoRegressive, Text-to-Text), pretraining domain, training from scratch, and\nmodel size in number of parameters. At the end of our analyses, we identify a\nlist of take-home messages that can be derived from the experimental data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scaboro_S/0/1/0/all/0/1\">Simone Scaboro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portellia_B/0/1/0/all/0/1\">Beatrice Portellia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chersoni_E/0/1/0/all/0/1\">Emmanuele Chersoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santus_E/0/1/0/all/0/1\">Enrico Santus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serra_G/0/1/0/all/0/1\">Giuseppe Serra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisit Few-shot Intent Classification with PLMs: Direct Fine-tuning vs. Continual Pre-training. (arXiv:2306.05278v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05278","description":"<p>We consider the task of few-shot intent detection, which involves training a\ndeep learning model to classify utterances based on their underlying intents\nusing only a small amount of labeled data. The current approach to address this\nproblem is through continual pre-training, i.e., fine-tuning pre-trained\nlanguage models (PLMs) on external resources (e.g., conversational corpora,\npublic intent detection datasets, or natural language understanding datasets)\nbefore using them as utterance encoders for training an intent classifier. In\nthis paper, we show that continual pre-training may not be essential, since the\noverfitting problem of PLMs on this task may not be as serious as expected.\nSpecifically, we find that directly fine-tuning PLMs on only a handful of\nlabeled examples already yields decent results compared to methods that employ\ncontinual pre-training, and the performance gap diminishes rapidly as the\nnumber of labeled data increases. To maximize the utilization of the limited\navailable data, we propose a context augmentation method and leverage\nsequential self-distillation to boost performance. Comprehensive experiments on\nreal-world benchmarks show that given only two or more labeled samples per\nclass, direct fine-tuning outperforms many strong baselines that utilize\nexternal data sources for continual pre-training. The code can be found at\nhttps://github.com/hdzhang-code/DFTPlus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haode Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Haowen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1\">Liming Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Ming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_A/0/1/0/all/0/1\">Albert Y.S. Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases. (arXiv:2306.05301v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05301","description":"<p>Enabling large language models to effectively utilize real-world tools is\ncrucial for achieving embodied intelligence. Existing approaches to tool\nlearning have primarily relied on either extremely large language models, such\nas GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or\nhave utilized supervised learning to train limited types of tools on compact\nmodels. However, it remains uncertain whether smaller language models can\nachieve generalized tool-use abilities without specific tool-specific training.\nTo address this question, this paper introduces ToolAlpaca, a novel framework\ndesigned to automatically generate a tool-use corpus and learn generalized\ntool-use abilities on compact language models with minimal human intervention.\nSpecifically, ToolAlpaca first collects a comprehensive dataset by building a\nmulti-agent simulation environment, which contains 3938 tool-use instances from\nmore than 400 real-world tool APIs spanning 50 distinct categories.\nSubsequently, the constructed corpus is employed to fine-tune compact language\nmodels, resulting in two models, namely ToolAlpaca-7B and ToolAlpaca-13B,\nrespectively. Finally, we evaluate the ability of these models to utilize\npreviously unseen tools without specific training. Experimental results\ndemonstrate that ToolAlpaca achieves effective generalized tool-use\ncapabilities comparable to those of extremely large language models like\nGPT-3.5. This validation supports the notion that learning generalized tool-use\nabilities is feasible for compact language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1\">Qiaoyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Ziliang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Q/0/1/0/all/0/1\">Qiao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are fairness metric scores enough to assess discrimination biases in machine learning?. (arXiv:2306.05307v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05307","description":"<p>This paper presents novel experiments shedding light on the shortcomings of\ncurrent metrics for assessing biases of gender discrimination made by machine\nlearning algorithms on textual data. We focus on the Bios dataset, and our\nlearning task is to predict the occupation of individuals, based on their\nbiography. Such prediction tasks are common in commercial Natural Language\nProcessing (NLP) applications such as automatic job recommendations. We address\nan important limitation of theoretical discussions dealing with group-wise\nfairness metrics: they focus on large datasets, although the norm in many\nindustrial NLP applications is to use small to reasonably large linguistic\ndatasets for which the main practical constraint is to get a good prediction\naccuracy. We then question how reliable are different popular measures of bias\nwhen the size of the training set is simply sufficient to learn reasonably\naccurate predictions. Our experiments sample the Bios dataset and learn more\nthan 200 models on different sample sizes. This allows us to statistically\nstudy our results and to confirm that common gender bias indices provide\ndiverging and sometimes unreliable results when applied to relatively small\ntraining and test samples. This highlights the crucial importance of variance\ncalculations for providing sound results in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jourdan_F/0/1/0/all/0/1\">Fanny Jourdan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risser_L/0/1/0/all/0/1\">Laurent Risser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loubes_J/0/1/0/all/0/1\">Jean-Michel Loubes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asher_N/0/1/0/all/0/1\">Nicholas Asher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CUED at ProbSum 2023: Hierarchical Ensemble of Summarization Models. (arXiv:2306.05317v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05317","description":"<p>In this paper, we consider the challenge of summarizing patients' medical\nprogress notes in a limited data setting. For the Problem List Summarization\n(shared task 1A) at the BioNLP Workshop 2023, we demonstrate that Clinical-T5\nfine-tuned to 765 medical clinic notes outperforms other extractive,\nabstractive and zero-shot baselines, yielding reasonable baseline systems for\nmedical note summarization. Further, we introduce Hierarchical Ensemble of\nSummarization Models (HESM), consisting of token-level ensembles of diverse\nfine-tuned Clinical-T5 models, followed by Minimum Bayes Risk (MBR) decoding.\nOur HESM approach lead to a considerable summarization performance boost, and\nwhen evaluated on held-out challenge data achieved a ROUGE-L of 32.77, which\nwas the best-performing system at the top of the shared task leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manakul_P/0/1/0/all/0/1\">Potsawee Manakul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathullah_Y/0/1/0/all/0/1\">Yassir Fathullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liusie_A/0/1/0/all/0/1\">Adian Liusie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vyas Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vatsal Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark Gales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KIT's Multilingual Speech Translation System for IWSLT 2023. (arXiv:2306.05320v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05320","description":"<p>Many existing speech translation benchmarks focus on native-English speech in\nhigh-quality recording conditions, which often do not match the conditions in\nreal-life use-cases. In this paper, we describe our speech translation system\nfor the multilingual track of IWSLT 2023, which focuses on the translation of\nscientific conference talks. The test condition features accented input speech\nand terminology-dense contents. The tasks requires translation into 10\nlanguages of varying amounts of resources. In absence of training data from the\ntarget domain, we use a retrieval-based approach (kNN-MT) for effective\nadaptation (+0.8 BLEU for speech translation). We also use adapters to easily\nintegrate incremental training data from data augmentation, and show that it\nmatches the performance of re-training. We observe that cascaded systems are\nmore easily adaptable towards specific target domains, due to their separate\nmodules. Our cascaded speech system substantially outperforms its end-to-end\ncounterpart on scientific talk translation, although their performance remains\nsimilar on TED talks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Danni Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thai Binh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koneru_S/0/1/0/all/0/1\">Sai Koneru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ugan_E/0/1/0/all/0/1\">Enes Yavuz Ugan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1\">Ngoc-Quan Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan-Nam Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1\">Tu Anh Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullov_C/0/1/0/all/0/1\">Carlos Mullov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alexander Waibel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1\">Jan Niehues</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advancing Italian Biomedical Information Extraction with Large Language Models: Methodological Insights and Multicenter Practical Application. (arXiv:2306.05323v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05323","description":"<p>The introduction of computerized medical records in hospitals has reduced\nburdensome operations like manual writing and information fetching. However,\nthe data contained in medical records are still far underutilized, primarily\nbecause extracting them from unstructured textual medical records takes time\nand effort. Information Extraction, a subfield of Natural Language Processing,\ncan help clinical practitioners overcome this limitation, using automated\ntext-mining pipelines. In this work, we created the first Italian\nneuropsychiatric Named Entity Recognition dataset, PsyNIT, and used it to\ndevelop a Large Language Model for this task. Moreover, we conducted several\nexperiments with three external independent datasets to implement an effective\nmulticenter model, with overall F1-score 84.77%, Precision 83.16%, Recall\n86.44%. The lessons learned are: (i) the crucial role of a consistent\nannotation process and (ii) a fine-tuning strategy that combines classical\nmethods with a \"few-shot\" approach. This allowed us to establish methodological\nguidelines that pave the way for future implementations in this field and allow\nItalian hospitals to tap into important research opportunities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Crema_C/0/1/0/all/0/1\">Claudio Crema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buonocore_T/0/1/0/all/0/1\">Tommaso Mario Buonocore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fostinelli_S/0/1/0/all/0/1\">Silvia Fostinelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parimbelli_E/0/1/0/all/0/1\">Enea Parimbelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verde_F/0/1/0/all/0/1\">Federico Verde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fundaro_C/0/1/0/all/0/1\">Cira Fundar&#xf2;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manera_M/0/1/0/all/0/1\">Marina Manera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramusino_M/0/1/0/all/0/1\">Matteo Cotta Ramusino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capelli_M/0/1/0/all/0/1\">Marco Capelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1\">Alfredo Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binetti_G/0/1/0/all/0/1\">Giuliano Binetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellazzi_R/0/1/0/all/0/1\">Riccardo Bellazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redolfi_A/0/1/0/all/0/1\">Alberto Redolfi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The ADAIO System at the BEA-2023 Shared Task on Generating AI Teacher Responses in Educational Dialogues. (arXiv:2306.05360v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05360","description":"<p>This paper presents the ADAIO team's system entry in the Building Educational\nApplications (BEA) 2023 Shared Task on Generating AI Teacher Responses in\nEducational Dialogues. The task aims to assess the performance of\nstate-of-the-art generative models as AI teachers in producing suitable\nresponses within a student-teacher dialogue. Our system comprises evaluating\nvarious baseline models using OpenAI GPT-3 and designing diverse prompts to\nprompt the OpenAI models for teacher response generation. After the challenge,\nour system achieved second place by employing a few-shot prompt-based approach\nwith the OpenAI text-davinci-003 model. The results highlight the few-shot\nlearning capabilities of large-language models, particularly OpenAI's GPT-3, in\nthe role of AI teachers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adigwe_A/0/1/0/all/0/1\">Adaeze Adigwe</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a> (2 and 3) ((1) University of Edinburgh, United Kingdom, (2) Istituto Italiano di Tecnologia, Italy, (3) Universit&#xe0; di Ferrara, Italy)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Human Rights Violations on Social Media during Russia-Ukraine War. (arXiv:2306.05370v1 [cs.CY])","link":"http://arxiv.org/abs/2306.05370","description":"<p>The present-day Russia-Ukraine military conflict has exposed the pivotal role\nof social media in enabling the transparent and unbridled sharing of\ninformation directly from the frontlines. In conflict zones where freedom of\nexpression is constrained and information warfare is pervasive, social media\nhas emerged as an indispensable lifeline. Anonymous social media platforms, as\npublicly available sources for disseminating war-related information, have the\npotential to serve as effective instruments for monitoring and documenting\nHuman Rights Violations (HRV). Our research focuses on the analysis of data\nfrom Telegram, the leading social media platform for reading independent news\nin post-Soviet regions. We gathered a dataset of posts sampled from 95 public\nTelegram channels that cover politics and war news, which we have utilized to\nidentify potential occurrences of HRV. Employing a mBERT-based text classifier,\nwe have conducted an analysis to detect any mentions of HRV in the Telegram\ndata. Our final approach yielded an $F_2$ score of 0.71 for HRV detection,\nrepresenting an improvement of 0.38 over the multilingual BERT base model. We\nrelease two datasets that contains Telegram posts: (1) large corpus with over\n2.3 millions posts and (2) annotated at the sentence-level dataset to indicate\nHRVs. The Telegram posts are in the context of the Russia-Ukraine war. We posit\nthat our findings hold significant implications for NGOs, governments, and\nresearchers by providing a means to detect and document possible human rights\nviolations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nemkova_P/0/1/0/all/0/1\">Poli Nemkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ubani_S/0/1/0/all/0/1\">Solomon Ubani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polat_S/0/1/0/all/0/1\">Suleyman Olcay Polat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Nayeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_R/0/1/0/all/0/1\">Rodney D. Nielsen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Utterance Emotion Dynamics in Children's Poems: Emotional Changes Across Age. (arXiv:2306.05387v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05387","description":"<p>Emerging psychopathology studies are showing that patterns of changes in\nemotional state -- emotion dynamics -- are associated with overall well-being\nand mental health. More recently, there has been some work in tracking emotion\ndynamics through one's utterances, allowing for data to be collected on a\nlarger scale across time and people. However, several questions about how\nemotion dynamics change with age, especially in children, and when determined\nthrough children's writing, remain unanswered. In this work, we use both a\nlexicon and a machine learning based approach to quantify characteristics of\nemotion dynamics determined from poems written by children of various ages. We\nshow that both approaches point to similar trends: consistent increasing\nintensities for some emotions (e.g., anger, fear, joy, sadness, arousal, and\ndominance) with age and a consistent decreasing valence with age. We also find\nincreasing emotional variability, rise rates (i.e., emotional reactivity), and\nrecovery rates (i.e., emotional regulation) with age. These results act as a\nuseful baselines for further research in how patterns of emotions expressed by\nchildren change with age, and their association with mental health.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teodorescu_D/0/1/0/all/0/1\">Daniela Teodorescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fyshe_A/0/1/0/all/0/1\">Alona Fyshe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modular Visual Question Answering via Code Generation. (arXiv:2306.05392v1 [cs.CL])","link":"http://arxiv.org/abs/2306.05392","description":"<p>We present a framework that formulates visual question answering as modular\ncode generation. In contrast to prior work on modular approaches to VQA, our\napproach requires no additional training and relies on pre-trained language\nmodels (LMs), visual models pre-trained on image-caption pairs, and fifty VQA\nexamples used for in-context learning. The generated Python programs invoke and\ncompose the outputs of the visual models using arithmetic and conditional\nlogic. Our approach improves accuracy on the COVR dataset by at least 3% and on\nthe GQA dataset by roughly 2% compared to the few-shot baseline that does not\nemploy code generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1\">Sanjay Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_M/0/1/0/all/0/1\">Medhini Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khangaonkar_K/0/1/0/all/0/1\">Kushal Khangaonkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1\">Arsha Nagrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Andy Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridge the Gap Between CV and NLP! A Gradient-based Textual Adversarial Attack Framework. (arXiv:2110.15317v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.15317","description":"<p>Despite recent success on various tasks, deep learning techniques still\nperform poorly on adversarial examples with small perturbations. While\noptimization-based methods for adversarial attacks are well-explored in the\nfield of computer vision, it is impractical to directly apply them in natural\nlanguage processing due to the discrete nature of the text. To address the\nproblem, we propose a unified framework to extend the existing\noptimization-based adversarial attack methods in the vision domain to craft\ntextual adversarial samples. In this framework, continuously optimized\nperturbations are added to the embedding layer and amplified in the forward\npropagation process. Then the final perturbed latent representations are\ndecoded with a masked language model head to obtain potential adversarial\nsamples. In this paper, we instantiate our framework with an attack algorithm\nnamed Textual Projected Gradient Descent (T-PGD). We find our algorithm\neffective even using proxy gradient information. Therefore, we perform the more\nchallenging transfer black-box attack and conduct comprehensive experiments to\nevaluate our attack algorithm with several models on three benchmark datasets.\nExperimental results demonstrate that our method achieves overall better\nperformance and produces more fluent and grammatical adversarial samples\ncompared to strong baseline methods. The code and data are available at\n\\url{https://github.com/Phantivia/T-PGD}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lifan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Small Character Models Match Large Word Models for Autocomplete Under Memory Constraints. (arXiv:2210.03251v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03251","description":"<p>Autocomplete is a task where the user inputs a piece of text, termed prompt,\nwhich is conditioned by the model to generate semantically coherent\ncontinuation. Existing works for this task have primarily focused on datasets\n(e.g., email, chat) with high frequency user prompt patterns (or focused\nprompts) where word-based language models have been quite effective. In this\nwork, we study the more challenging open-domain setting consisting of low\nfrequency user prompt patterns (or broad prompts, e.g., prompt about 93rd\nacademy awards) and demonstrate the effectiveness of character-based language\nmodels. We study this problem under memory-constrained settings (e.g., edge\ndevices and smartphones), where character-based representation is effective in\nreducing the overall model size (in terms of parameters). We use WikiText-103\nbenchmark to simulate broad prompts and demonstrate that character models rival\nword models in exact match accuracy for the autocomplete task, when controlled\nfor the model size. For instance, we show that a 20M parameter character model\nperforms similar to an 80M parameter word model in the vanilla setting. We\nfurther propose novel methods to improve character models by incorporating\ninductive bias in the form of compositional information and representation\ntransfer from large word models. Datasets and code used in this work are\navailable at https://github.com/UBC-NLP/char_autocomplete.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_G/0/1/0/all/0/1\">Ganesh Jawahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_D/0/1/0/all/0/1\">Debadeepta Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshmanan_L/0/1/0/all/0/1\">Laks V.S. Lakshmanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendes_C/0/1/0/all/0/1\">Caio Cesar Teodoro Mendes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosa_G/0/1/0/all/0/1\">Gustavo Henrique de Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Shital Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks. (arXiv:2210.06379v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.06379","description":"<p>Current multimodal models, aimed at solving Vision and Language (V+L) tasks,\npredominantly repurpose Vision Encoders (VE) as feature extractors. While many\nVEs -- of different architectures, trained on different data and objectives --\nare publicly available, they are not designed for the downstream V+L tasks.\nNonetheless, most current work assumes that a \\textit{single} pre-trained VE\ncan serve as a general-purpose encoder. In this work, we focus on analysis and\naim to understand whether the information stored within different VEs is\ncomplementary, i.e. if providing the model with features from multiple VEs can\nimprove the performance on a target task, and how they are combined. We\nexhaustively experiment with three popular VEs on six downstream V+L tasks and\nanalyze the attention and VE-dropout patterns. Our analyses suggest that\ndiverse VEs complement each other, resulting in improved downstream V+L task\nperformance, where the improvements are not due to simple ensemble effects\n(i.e. the performance does not always improve when increasing the number of\nencoders). We demonstrate that future VEs, which are not \\textit{repurposed},\nbut explicitly \\textit{designed} for V+L tasks, have the potential of improving\nperformance on the target V+L tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geigle_G/0/1/0/all/0/1\">Gregor Geigle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chen Cecilia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation. (arXiv:2210.07535v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07535","description":"<p>Mixture-of-Expert (MoE) models have obtained state-of-the-art performance in\nNeural Machine Translation (NMT) tasks. Existing works in MoE mostly consider a\nhomogeneous design where the same number of experts of the same size are placed\nuniformly throughout the network. Furthermore, existing MoE works do not\nconsider computational constraints (e.g., FLOPs, latency) to guide their\ndesign. To this end, we develop AutoMoE -- a framework for designing\nheterogeneous MoE's under computational constraints. AutoMoE leverages Neural\nArchitecture Search (NAS) to obtain efficient sparse MoE sub-transformers with\n4x inference speedup (CPU) and FLOPs reduction over manually designed\nTransformers, with parity in BLEU score over dense Transformer and within 1\nBLEU point of MoE SwitchTransformer, on aggregate over benchmark datasets for\nNMT. Heterogeneous search space with dense and sparsely activated Transformer\nmodules (e.g., how many experts? where to place them? what should be their\nsizes?) allows for adaptive compute -- where different amounts of computations\nare used for different tokens in the input. Adaptivity comes naturally from\nrouting decisions which send tokens to experts of different sizes. AutoMoE\ncode, data, and trained models are available at https://aka.ms/AutoMoE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_G/0/1/0/all/0/1\">Ganesh Jawahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshmanan_L/0/1/0/all/0/1\">Laks V. S. Lakshmanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1\">Sebastien Bubeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Semi-supervised Approach for a Better Translation of Sentiment in Dialectical Arabic UGT. (arXiv:2210.11899v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.11899","description":"<p>In the online world, Machine Translation (MT) systems are extensively used to\ntranslate User-Generated Text (UGT) such as reviews, tweets, and social media\nposts, where the main message is often the author's positive or negative\nattitude towards the topic of the text. However, MT systems still lack accuracy\nin some low-resource languages and sometimes make critical translation errors\nthat completely flip the sentiment polarity of the target word or phrase and\nhence delivers a wrong affect message. This is particularly noticeable in texts\nthat do not follow common lexico-grammatical standards such as the dialectical\nArabic (DA) used on online platforms. In this research, we aim to improve the\ntranslation of sentiment in UGT written in the dialectical versions of the\nArabic language to English. Given the scarcity of gold-standard parallel data\nfor DA-EN in the UGT domain, we introduce a semi-supervised approach that\nexploits both monolingual and parallel data for training an NMT system\ninitialised by a cross-lingual language model trained with supervised and\nunsupervised modeling objectives. We assess the accuracy of sentiment\ntranslation by our proposed system through a numerical 'sentiment-closeness'\nmeasure as well as human evaluation. We will show that our semi-supervised MT\nsystem can significantly help with correcting sentiment errors detected in the\nonline translation of dialectical Arabic UGT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saadany_H/0/1/0/all/0/1\">Hadeel Saadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orasan_C/0/1/0/all/0/1\">Constantin Orasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1\">Emad Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tantawy_A/0/1/0/all/0/1\">Ashraf Tantawy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models. (arXiv:2210.12023v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12023","description":"<p>We have recently witnessed a number of impressive results on hard\nmathematical reasoning problems with language models. At the same time, the\nrobustness of these models has also been called into question; recent works\nhave shown that models can rely on shallow patterns in the problem description\nwhen generating a solution. Building on the idea of behavioral testing, we\npropose a novel framework, which pins down the causal effect of various factors\nin the input, e.g., the surface form of the problem text, the operands, and\nmath operators on the output solution. By grounding the behavioral analysis in\na causal graph describing an intuitive reasoning process, we study the behavior\nof language models in terms of robustness and sensitivity to direct\ninterventions in the input space. We apply our framework on a test bed of math\nword problems. Our analysis shows that robustness does not appear to\ncontinuously improve as a function of size, but the GPT-3 Davinci models (175B)\nachieve a dramatic improvement in both robustness and sensitivity compared to\nall other GPT variants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stolfo_A/0/1/0/all/0/1\">Alessandro Stolfo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_K/0/1/0/all/0/1\">Kumar Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explicit Knowledge Transfer for Weakly-Supervised Code Generation. (arXiv:2211.16740v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.16740","description":"<p>Large language models (LLMs) can acquire strong code-generation capabilities\nthrough few-shot learning. In contrast, supervised fine-tuning is still needed\nfor smaller models to achieve good performance. Such fine-tuning demands a\nlarge number of task-specific NL-code pairs, which are expensive to obtain. In\nthis paper, we attempt to transfer the code generation ability of an LLM to a\nsmaller model with the aid of weakly-supervised data. More specifically, we\npropose explicit knowledge transfer (EKT), which uses the few-shot capabilities\nof a teacher LLM to create NL-code pairs that we then filter for correctness\nand fine-tune the student on. We evaluate EKT on the task of generating code\nsolutions to math word problems from the GSM8k dataset. We find that EKT not\nonly yields better performance than training with expert iteration, but also\noutperforms knowledge distillation, another form of knowledge transfer. A\nGPT-Neo 1.3B model trained using EKT with a GPT-J teacher achieves a 12.4%\npass@100 on GSM8k, while the same student and teacher trained with knowledge\ndistillation yield only a 3.7% pass@100. We also show that it is possible for a\nstudent model to outperform the teacher using EKT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azerbayev_Z/0/1/0/all/0/1\">Zhangir Azerbayev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1\">Ansong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoelkopf_H/0/1/0/all/0/1\">Hailey Schoelkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relational Sentence Embedding for Flexible Semantic Matching. (arXiv:2212.08802v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08802","description":"<p>We present Relational Sentence Embedding (RSE), a new paradigm to further\ndiscover the potential of sentence embeddings. Prior work mainly models the\nsimilarity between sentences based on their embedding distance. Because of the\ncomplex semantic meanings conveyed, sentence pairs can have various relation\ntypes, including but not limited to entailment, paraphrasing, and\nquestion-answer. It poses challenges to existing embedding methods to capture\nsuch relational information. We handle the problem by learning associated\nrelational embeddings. Specifically, a relation-wise translation operation is\napplied to the source sentence to infer the corresponding target sentence with\na pre-trained Siamese-based encoder. The fine-grained relational similarity\nscores can be computed from learned embeddings. We benchmark our method on 19\ndatasets covering a wide range of tasks, including semantic textual similarity,\ntransfer, and domain-specific tasks. Experimental results show that our method\nis effective and flexible in modeling sentence relations and outperforms a\nseries of state-of-the-art sentence embedding methods.\nhttps://github.com/BinWang28/RSE\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do language models have coherent mental models of everyday things?. (arXiv:2212.10029v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10029","description":"<p>When people think of everyday things like an egg, they typically have a\nmental image associated with it. This allows them to correctly judge, for\nexample, that \"the yolk surrounds the shell\" is a false statement. Do language\nmodels similarly have a coherent picture of such everyday things? To\ninvestigate this, we propose a benchmark dataset consisting of 100 everyday\nthings, their parts, and the relationships between these parts, expressed as\n11,720 \"X relation Y?\" true/false questions. Using these questions as probes,\nwe observe that state-of-the-art pre-trained language models (LMs) like GPT-3\nand Macaw have fragments of knowledge about these everyday things, but do not\nhave fully coherent \"parts mental models\" (54-59% accurate, 19-43% conditional\nconstraint violation). We propose an extension where we add a constraint\nsatisfaction layer on top of the LM's raw predictions to apply commonsense\nconstraints. As well as removing inconsistencies, we find that this also\nsignificantly improves accuracy (by 16-20%), suggesting how the incoherence of\nthe LM's pictures of everyday things can be significantly reduced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuling Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1\">Bhavana Dalvi Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoRRPUS: Code-based Structured Prompting for Neurosymbolic Story Understanding. (arXiv:2212.10754v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10754","description":"<p>Story generation and understanding -- as with all NLG/NLU tasks -- has seen a\nsurge in neurosymbolic work. Researchers have recognized that, while large\nlanguage models (LLMs) have tremendous utility, they can be augmented with\nsymbolic means to be even better and to make up for any flaws that the neural\nnetworks might have. However, symbolic methods are extremely costly in terms of\nthe amount of time and expertise needed to create them. In this work, we\ncapitalize on state-of-the-art Code-LLMs, such as Codex, to bootstrap the use\nof symbolic methods for tracking the state of stories and aiding in story\nunderstanding. We show that our CoRRPUS system and abstracted prompting\nprocedures can beat current state-of-the-art structured LLM techniques on\npre-existing story understanding tasks (bAbI Task 2 and Re^3) with minimal hand\nengineering. We hope that this work can help highlight the importance of\nsymbolic representations and specialized prompting for LLMs as these models\nrequire some guidance for performing reasoning tasks properly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yijiang River Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1\">Lara J. Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning. (arXiv:2212.10773v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10773","description":"<p>Instruction tuning, a new learning paradigm that fine-tunes pre-trained\nlanguage models on tasks specified through instructions, has shown promising\nzero-shot performance on various natural language processing tasks. However,\nit's still not explored for vision and multimodal tasks. In this work, we\nintroduce MultiInstruct, the first multimodal instruction tuning benchmark\ndataset that consists of 47 diverse multimodal tasks covering 11 broad\ncategories. Each task is designed at least with 5,000 instances (input-out\npairs) from existing open-source datasets and 5 expert-written instructions. We\ntake OFA as the base pre-trained model for multimodal instruction tuning, and\nto improve its performance, we explore multiple transfer learning strategies to\nleverage the large-scale Natural Instructions dataset. Experimental results\ndemonstrate its strong zero-shot performance on various unseen multimodal tasks\nand the benefit of transfer learning from text-only instructions. We also\ndesign a new evaluation metric: Sensitivity, to evaluate how sensitive the\nmodel is to the variety of instructions. Our results indicate that the model is\nless sensitive to the varying instructions after finetuning on a diverse set of\ntasks and instructions for each task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Think Twice: A Human-like Two-stage Conversational Agent for Emotional Response Generation. (arXiv:2301.04907v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.04907","description":"<p>Towards human-like dialogue systems, current emotional dialogue approaches\njointly model emotion and semantics with a unified neural network. This\nstrategy tends to generate safe responses due to the mutual restriction between\nemotion and semantics, and requires rare emotion-annotated large-scale dialogue\ncorpus. Inspired by the \"think twice\" behavior in human dialogue, we propose a\ntwo-stage conversational agent for the generation of emotional dialogue.\nFirstly, a dialogue model trained without the emotion-annotated dialogue corpus\ngenerates a prototype response that meets the contextual semantics. Secondly,\nthe first-stage prototype is modified by a controllable emotion refiner with\nthe empathy hypothesis. Experimental results on the DailyDialog and\nEmpatheticDialogues datasets demonstrate that the proposed conversational\noutperforms the comparison models in emotion generation and maintains the\nsemantic performance in automatic and human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yushan Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shangzhao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bin_W/0/1/0/all/0/1\">Wu Bin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yuexian Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two Stage Contextual Word Filtering for Context bias in Unified Streaming and Non-streaming Transducer. (arXiv:2301.06735v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2301.06735","description":"<p>It is difficult for an E2E ASR system to recognize words such as entities\nappearing infrequently in the training data. A widely used method to mitigate\nthis issue is feeding contextual information into the acoustic model. Previous\nworks have proven that a compact and accurate contextual list can boost the\nperformance significantly. In this paper, we propose an efficient approach to\nobtain a high quality contextual list for a unified streaming/non-streaming\nbased E2E model. Specifically, we make use of the phone-level streaming output\nto first filter the predefined contextual word list then fuse it into\nnon-casual encoder and decoder to generate the final recognition results. Our\napproach improve the accuracy of the contextual ASR system and speed up the\ninference process. Experiments on two datasets demonstrates over 20% CER\nreduction comparing to the baseline system. Meanwhile, the RTF of our system\ncan be stabilized within 0.15 when the size of the contextual word list grows\nover 6,000.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhanheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Sining Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yike Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Long Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. (arXiv:2304.03279v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2304.03279","description":"<p>Artificial agents have traditionally been trained to maximize reward, which\nmay incentivize power-seeking and deception, analogous to how next-token\nprediction in language models (LMs) may incentivize toxicity. So do agents\nnaturally learn to be Machiavellian? And how do we measure these behaviors in\ngeneral-purpose models such as GPT-4? Towards answering these questions, we\nintroduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games\ncontaining over half a million rich, diverse scenarios that center on social\ndecision-making. Scenario labeling is automated with LMs, which are more\nperformant than human annotators. We mathematize dozens of harmful behaviors\nand use our annotations to evaluate agents' tendencies to be power-seeking,\ncause disutility, and commit ethical violations. We observe some tension\nbetween maximizing reward and behaving ethically. To improve this trade-off, we\ninvestigate LM-based methods to steer agents' towards less harmful behaviors.\nOur results show that agents can both act competently and morally, so concrete\nprogress can currently be made in machine ethics--designing agents that are\nPareto improvements in both safety and capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_A/0/1/0/all/0/1\">Alexander Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1\">Jun Shern Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1\">Andy Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nathaniel Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodside_T/0/1/0/all/0/1\">Thomas Woodside</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_J/0/1/0/all/0/1\">Jonathan Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emmons_S/0/1/0/all/0/1\">Scott Emmons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlled Text Generation with Natural Language Instructions. (arXiv:2304.14293v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.14293","description":"<p>Large language models generate fluent texts and can follow natural language\ninstructions to solve a wide range of tasks without task-specific training.\nNevertheless, it is notoriously difficult to control their generation to\nsatisfy the various constraints required by different applications. In this\nwork, we present InstructCTG, a controlled text generation framework that\nincorporates different constraints by conditioning on natural language\ndescriptions and demonstrations of the constraints. In particular, we first\nextract the underlying constraints of natural texts through a combination of\noff-the-shelf NLP tools and simple heuristics. We then verbalize the\nconstraints into natural language instructions to form weakly supervised\ntraining data. By prepending natural language descriptions of the constraints\nand a few demonstrations, we fine-tune a pre-trained language model to\nincorporate various types of constraints. Compared to existing search-based or\nscore-based methods, InstructCTG is more flexible to different constraint types\nand has a much smaller impact on the generation quality and speed because it\ndoes not modify the decoding procedure. Additionally, InstructCTG allows the\nmodel to adapt to new constraints without re-training through the use of\nfew-shot task generalization and in-context learning abilities of\ninstruction-tuned language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuchen Eleanor Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilcox_E/0/1/0/all/0/1\">Ethan Wilcox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from Book Reviews. (arXiv:2305.06595v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06595","description":"<p>The analysis of consumer sentiment, as expressed through reviews, can provide\na wealth of insight regarding the quality of a product. While the study of\nsentiment analysis has been widely explored in many popular languages,\nrelatively less attention has been given to the Bangla language, mostly due to\na lack of relevant data and cross-domain adaptability. To address this\nlimitation, we present BanglaBook, a large-scale dataset of Bangla book reviews\nconsisting of 158,065 samples classified into three broad categories: positive,\nnegative, and neutral. We provide a detailed statistical analysis of the\ndataset and employ a range of machine learning models to establish baselines\nincluding SVM, LSTM, and Bangla-BERT. Our findings demonstrate a substantial\nperformance advantage of pre-trained models over models that rely on manually\ncrafted features, emphasizing the necessity for additional training resources\nin this domain. Additionally, we conduct an in-depth error analysis by\nexamining sentiment unigrams, which may provide insight into common\nclassification errors in under-resourced languages like Bangla. Our codes and\ndata are publicly available at https://github.com/mohsinulkabir14/BanglaBook.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Mohsinul Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahfuz_O/0/1/0/all/0/1\">Obayed Bin Mahfuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raiyan_S/0/1/0/all/0/1\">Syed Rifat Raiyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmud_H/0/1/0/all/0/1\">Hasan Mahmud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Md Kamrul Hasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asymmetric feature interaction for interpreting model predictions. (arXiv:2305.07224v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.07224","description":"<p>In natural language processing (NLP), deep neural networks (DNNs) could model\ncomplex interactions between context and have achieved impressive results on a\nrange of NLP tasks. Prior works on feature interaction attribution mainly focus\non studying symmetric interaction that only explains the additional influence\nof a set of words in combination, which fails to capture asymmetric influence\nthat contributes to model prediction. In this work, we propose an asymmetric\nfeature interaction attribution explanation model that aims to explore\nasymmetric higher-order feature interactions in the inference of deep neural\nNLP models. By representing our explanation with an directed interaction graph,\nwe experimentally demonstrate interpretability of the graph to discover\nasymmetric feature interactions. Experimental results on two sentiment\nclassification datasets show the superiority of our model against the\nstate-of-the-art feature interaction attribution methods in identifying\ninfluential features for model predictions. Our code is available at\nhttps://github.com/StillLu/ASIV.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaolei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jianghong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haode Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Hidden Mystery of OCR in Large Multimodal Models. (arXiv:2305.07895v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.07895","description":"<p>Large models have recently played a dominant role in natural language\nprocessing and multimodal vision-language learning. It remains less explored\nabout their efficacy in text-related visual tasks. We conducted a comprehensive\nstudy of existing publicly available multimodal models, evaluating their\nperformance in text recognition (document text, artistic text, handwritten\ntext, scene text), text-based visual question answering (document text, scene\ntext, and bilingual text), key information extraction (receipts, documents, and\nnutrition facts) and handwritten mathematical expression recognition. Our\nfindings reveal strengths and weaknesses in these models, which primarily rely\non semantic understanding for word recognition and exhibit inferior perception\nof individual character shapes. They also display indifference towards text\nlength and have limited capabilities in detecting finegrained features in\nimages. Consequently, these results demonstrate that even the current most\npowerful large multimodal models cannot match domain-specific methods in\ntraditional text tasks and face greater challenges in more complex tasks. Most\nimportantly, the baseline results showcased in this study could provide a\nfoundational framework for the conception and assessment of innovative\nstrategies targeted at enhancing zero-shot multimodal techniques. Evaluation\npipeline is available at https://github.com/Yuliang-Liu/MultimodalOCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenwen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Mingxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Dezhi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingrui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cheng-lin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sensitivity and Robustness of Large Language Models to Prompt Template in Japanese Text Classification Tasks. (arXiv:2305.08714v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08714","description":"<p>Prompt engineering relevance research has seen a notable surge in recent\nyears, primarily driven by advancements in pre-trained language models and\nlarge language models. However, a critical issue has been identified within\nthis domain: the inadequate of sensitivity and robustness of these models\ntowards Prompt Templates, particularly in lesser-studied languages such as\nJapanese. This paper explores this issue through a comprehensive evaluation of\nseveral representative Large Language Models (LLMs) and a widely-utilized\npre-trained model(PLM). These models are scrutinized using a benchmark dataset\nin Japanese, with the aim to assess and analyze the performance of the current\nmultilingual models in this context. Our experimental results reveal startling\ndiscrepancies. A simple modification in the sentence structure of the Prompt\nTemplate led to a drastic drop in the accuracy of GPT-4 from 49.21 to 25.44.\nThis observation underscores the fact that even the highly performance GPT-4\nmodel encounters significant stability issues when dealing with diverse\nJapanese prompt templates, rendering the consistency of the model's output\nresults questionable. In light of these findings, we conclude by proposing\npotential research trajectories to further enhance the development and\nperformance of Large Language Models in their current stage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chengguang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_T/0/1/0/all/0/1\">Tatsunori Mori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners. (arXiv:2305.14825v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14825","description":"<p>The emergent few-shot reasoning capabilities of Large Language Models (LLMs)\nhave excited the natural language and machine learning community over recent\nyears. Despite of numerous successful applications, the underlying mechanism of\nsuch in-context capabilities still remains unclear. In this work, we\nhypothesize that the learned \\textit{semantics} of language tokens do the most\nheavy lifting during the reasoning process. Different from human's symbolic\nreasoning process, the semantic representations of LLMs could create strong\nconnections among tokens, thus composing a superficial logical chain. To test\nour hypothesis, we decouple semantics from the language reasoning process and\nevaluate three kinds of reasoning abilities, i.e., deduction, induction and\nabduction. Our findings reveal that semantics play a vital role in LLMs'\nin-context reasoning -- LLMs perform significantly better when semantics are\nconsistent with commonsense but struggle to solve symbolic or\ncounter-commonsense reasoning tasks by leveraging in-context new knowledge. The\nsurprising observations question whether modern LLMs have mastered the\ninductive, deductive and abductive reasoning abilities as in human\nintelligence, and motivate research on unveiling the magic existing within the\nblack-box LLMs. On the whole, our analysis provides a novel perspective on the\nrole of semantics in developing and evaluating language models' reasoning\nabilities. Code is available at {\\url{https://github.com/XiaojuanTang/ICSR}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaojuan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zilong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fanxu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yitao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Muhan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BUCA: A Binary Classification Approach to Unsupervised Commonsense Question Answering. (arXiv:2305.15932v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.15932","description":"<p>Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as\nthe construction of commonsense reasoning datasets is expensive, and they are\ninevitably limited in their scope. A popular approach to UCR is to fine-tune\nlanguage models with external knowledge (e.g., knowledge graphs), but this\nusually requires a large number of training examples. In this paper, we propose\nto transform the downstream multiple choice question answering task into a\nsimpler binary classification task by ranking all candidate answers according\nto their reasonableness. To this end, for training the model, we convert the\nknowledge graph triples into reasonable and unreasonable texts. Extensive\nexperimental results show the effectiveness of our approach on various multiple\nchoice question answering benchmarks. Furthermore, compared with existing UCR\napproaches using KGs, ours is less data hungry. Our code is available at\nhttps://github.com/probe2/BUCA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jie He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+U_S/0/1/0/all/0/1\">Simon Chi Lok U</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Basulto_V/0/1/0/all/0/1\">V&#xed;ctor Guti&#xe9;rrez-Basulto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Whitening-based Contrastive Learning of Sentence Embeddings. (arXiv:2305.17746v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17746","description":"<p>This paper presents a whitening-based contrastive learning method for\nsentence embedding learning (WhitenedCSE), which combines contrastive learning\nwith a novel shuffled group whitening. Generally, contrastive learning pulls\ndistortions of a single sample (i.e., positive samples) close and push negative\nsamples far away, correspondingly facilitating the alignment and uniformity in\nthe feature space. A popular alternative to the \"pushing'' operation is\nwhitening the feature space, which scatters all the samples for uniformity.\nSince the whitening and the contrastive learning have large redundancy w.r.t.\nthe uniformity, they are usually used separately and do not easily work\ntogether. For the first time, this paper integrates whitening into the\ncontrastive learning scheme and facilitates two benefits. 1) Better uniformity.\nWe find that these two approaches are not totally redundant but actually have\nsome complementarity due to different uniformity mechanism. 2) Better\nalignment. We randomly divide the feature into multiple groups along the\nchannel axis and perform whitening independently within each group. By\nshuffling the group division, we derive multiple distortions of a single sample\nand thus increase the positive sample diversity. Consequently, using multiple\npositive samples with enhanced diversity further improves contrastive learning\ndue to better alignment. Extensive experiments on seven semantic textual\nsimilarity tasks show our method achieves consistent improvement over the\ncontrastive learning baseline and sets new states of the art, e.g., 78.78\\%\n(+2.53\\% based on BERT\\ba) Spearman correlation on STS tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1\">Wenjie Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18486","description":"<p>The development of large language models (LLMs) such as ChatGPT has brought a\nlot of attention recently. However, their evaluation in the benchmark academic\ndatasets remains under-explored due to the difficulty of evaluating the\ngenerative outputs produced by this model against the ground truth. In this\npaper, we aim to present a thorough evaluation of ChatGPT's performance on\ndiverse academic datasets, covering tasks like question-answering, text\nsummarization, code generation, commonsense reasoning, mathematical\nproblem-solving, machine translation, bias detection, and ethical\nconsiderations. Specifically, we evaluate ChatGPT across 140 tasks and analyze\n255K responses it generates in these datasets. This makes our work the largest\nevaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate\nthe strengths and weaknesses of ChatGPT in various tasks and provide insights\nfor future research using LLMs. We also report a new emergent ability to follow\nmulti-query instructions that we mostly found in ChatGPT and other\ninstruction-tuned models. Our extensive evaluation shows that even though\nChatGPT is capable of performing a wide variety of tasks, and may obtain\nimpressive performance in several benchmark datasets, it is still far from\nachieving the ability to reliably solve many challenging tasks. By providing a\nthorough assessment of ChatGPT's performance across diverse NLP tasks, this\npaper sets the stage for a targeted deployment of ChatGPT-like LLMs in\nreal-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laskar_M/0/1/0/all/0/1\">Md Tahmid Rahman Laskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1\">M Saiful Bari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mizanur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhuiyan_M/0/1/0/all/0/1\">Md Amran Hossen Bhuiyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jimmy Xiangji Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supplementary Features of BiLSTM for Enhanced Sequence Labeling. (arXiv:2305.19928v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.19928","description":"<p>Sequence labeling tasks require the computation of sentence representations\nfor each word within a given sentence. With the rise of advanced pretrained\nlanguage models; one common approach involves incorporating a BiLSTM layer to\nenhance the sequence structure information at the output level. Nevertheless,\nit has been empirically demonstrated (P.-H. Li, 2020) that BiLSTM's potential\nfor generating sentence representations for sequence labeling tasks is\nconstrained, primarily due to the integration of fragments from past and future\nsentence representations to form a complete sentence representation. In this\nstudy, we observed that the entire sentence representation, found in both the\nfirst and last cells of BiLSTM, can supplement each cell's sentence\nrepresentation. Accordingly, we devised a global context mechanism to integrate\nentire future and past sentence representations into each cell's sentence\nrepresentation within BiLSTM, leading to a significant improvement in both F1\nscore and accuracy. By embedding the BERT model within BiLSTM as a\ndemonstration, and conducting exhaustive experiments on nine datasets for\nsequence labeling tasks, including named entity recognition (NER), part of\nspeech (POS) tagging and End-to-End Aspect-Based sentiment analysis (E2E-ABSA).\nWe noted significant improvements in F1 scores and accuracy across all examined\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Conglei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1\">Kun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongguang Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study on Challenging Math Problem Solving with GPT-4. (arXiv:2306.01337v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.01337","description":"<p>Employing Large Language Models (LLMs) to address mathematical problems is an\nintriguing research endeavor, considering the abundance of math problems\nexpressed in natural language across numerous science and engineering fields.\nWhile several prior works have investigated solving elementary mathematics\nusing LLMs, this work explores the frontier of using GPT-4 for solving more\ncomplex and challenging math problems. We evaluate various ways of using GPT-4.\nSome of them are adapted from existing work, and one is MathChat, a\nconversational problem-solving framework newly proposed in this work. We\nperform the evaluation on difficult high school competition problems from the\nMATH dataset, which shows the advantage of the proposed conversational\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yiran Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1\">Feiran Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaokun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hangyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1\">Erkang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yin Tat Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1\">Richard Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qingyun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models. (arXiv:2306.01506v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.01506","description":"<p>Self-supervised techniques for learning speech representations have been\nshown to develop linguistic competence from exposure to speech without the need\nfor human labels. In order to fully realize the potential of these approaches\nand further our understanding of how infants learn language, simulations must\nclosely emulate real-life situations by training on developmentally plausible\ncorpora and benchmarking against appropriate test sets. To this end, we propose\na language-acquisition-friendly benchmark to probe spoken language models at\nthe lexical and syntactic levels, both of which are compatible with the\nvocabulary typical of children's language experiences. This paper introduces\nthe benchmark and summarizes a range of experiments showing its usefulness. In\naddition, we highlight two exciting challenges that need to be addressed for\nfurther progress: bridging the gap between text and speech and between clean\nspeech and in-the-wild speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lavechin_M/0/1/0/all/0/1\">Marvin Lavechin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sy_Y/0/1/0/all/0/1\">Yaya Sy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titeux_H/0/1/0/all/0/1\">Hadrien Titeux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blandon_M/0/1/0/all/0/1\">Mar&#xed;a Andrea Cruz Bland&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bredin_H/0/1/0/all/0/1\">Herv&#xe9; Bredin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristia_A/0/1/0/all/0/1\">Alejandrina Cristia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNIDECOR: A Unified Deception Corpus for Cross-Corpus Deception Detection. (arXiv:2306.02827v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.02827","description":"<p>Verbal deception has been studied in psychology, forensics, and computational\nlinguistics for a variety of reasons, like understanding behaviour patterns,\nidentifying false testimonies, and detecting deception in online communication.\nVarying motivations across research fields lead to differences in the domain\nchoices to study and in the conceptualization of deception, making it hard to\ncompare models and build robust deception detection systems for a given\nlanguage. With this paper, we improve this situation by surveying available\nEnglish deception datasets which include domains like social media reviews,\ncourt testimonials, opinion statements on specific topics, and deceptive\ndialogues from online strategy games. We consolidate these datasets into a\nsingle unified corpus. Based on this resource, we conduct a correlation\nanalysis of linguistic cues of deception across datasets to understand the\ndifferences and perform cross-corpus modeling experiments which show that a\ncross-domain generalization is challenging to achieve. The unified deception\ncorpus (UNIDECOR) can be obtained from\nhttps://www.ims.uni-stuttgart.de/data/unidecor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Velutharambath_A/0/1/0/all/0/1\">Aswathy Velutharambath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese Medical Exam Dataset. (arXiv:2306.03030v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.03030","description":"<p>Recent advancements in large language models (LLMs) have transformed the\nfield of question answering (QA). However, evaluating LLMs in the medical field\nis challenging due to the lack of standardized and comprehensive datasets. To\naddress this gap, we introduce CMExam, sourced from the Chinese National\nMedical Licensing Examination. CMExam consists of 60K+ multiple-choice\nquestions for standardized and objective evaluations, as well as solution\nexplanations for model reasoning evaluation in an open-ended manner. For\nin-depth analyses of LLMs, we invited medical professionals to label five\nadditional question-wise annotations, including disease groups, clinical\ndepartments, medical disciplines, areas of competency, and question difficulty\nlevels. Alongside the dataset, we further conducted thorough experiments with\nrepresentative LLMs and QA algorithms on CMExam. The results show that GPT-4\nhad the best accuracy of 61.6% and a weighted F1 score of 0.617. These results\nhighlight a great disparity when compared to human accuracy, which stood at\n71.6%. For explanation tasks, while LLMs could generate relevant reasoning and\ndemonstrate improved performance after finetuning, they fall short of a desired\nstandard, indicating ample room for improvement. To the best of our knowledge,\nCMExam is the first Chinese medical exam dataset to provide comprehensive\nmedical annotations. The experiments and findings of LLM evaluation also\nprovide valuable insights into the challenges and potential solutions in\ndeveloping Chinese medical QA systems and LLM evaluation pipelines. The dataset\nand relevant code are available at https://github.com/williamliujl/CMExam.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Peilin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yining Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_D/0/1/0/all/0/1\">Dading Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhongyu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Andrew Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Helin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenhua Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Michael Lingzhi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT Self-Supervision for a Better Data Annotator. (arXiv:2306.04349v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.04349","description":"<p>The task of annotating data into concise summaries poses a significant\nchallenge across various domains, frequently requiring the allocation of\nsignificant time and specialized knowledge by human experts. Despite existing\nefforts to use large language models for annotation tasks, significant problems\nsuch as limited applicability to unlabeled data, the absence of self-supervised\nmethods, and the lack of focus on complex structured data still persist. In\nthis work, we propose a GPT self-supervision annotation method, which embodies\na generating-recovering paradigm that leverages the one-shot learning\ncapabilities of the Generative Pretrained Transformer (GPT). The proposed\napproach comprises a one-shot tuning phase followed by a generation phase. In\nthe one-shot tuning phase, we sample a data from the support set as part of the\nprompt for GPT to generate a textual summary, which is then used to recover the\noriginal data. The alignment score between the recovered and original data\nserves as a self-supervision navigator to refine the process. In the generation\nstage, the optimally selected one-shot sample serves as a template in the\nprompt and is applied to generating summaries from challenging datasets. The\nannotation performance is evaluated by tuning several human feedback reward\nnetworks and by calculating alignment scores between original and recovered\ndata at both sentence and structure levels. Our self-supervised annotation\nmethod consistently achieves competitive scores, convincingly demonstrating its\nrobust strength in various data-to-summary annotation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_X/0/1/0/all/0/1\">Xiaohuan Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning. (arXiv:2306.04387v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2306.04387","description":"<p>Instruction tuning has significantly advanced large language models (LLMs)\nsuch as ChatGPT, enabling them to align with human instructions across diverse\ntasks. However, progress in open vision-language models (VLMs) has been limited\ndue to the scarcity of high-quality instruction datasets. To tackle this\nchallenge and promote research in the vision-language field, we introduce the\nMulti-Modal, Multilingual Instruction Tuning (M$^3$IT) dataset, designed to\noptimize VLM alignment with human instructions. Our M$^3$IT dataset comprises\n40 carefully curated datasets, including 2.4 million instances and 400 manually\nwritten task instructions, reformatted into a vision-to-text structure. Key\ntasks are translated into 80 languages with an advanced translation system,\nensuring broader accessibility. M$^3$IT surpasses previous datasets regarding\ntask coverage, instruction number and instance scale. Moreover, we develop\nYing-VLM, a VLM model trained on our M$^3$IT dataset, showcasing its potential\nto answer complex questions requiring world knowledge, generalize to unseen\nvideo tasks, and comprehend unseen instructions in Chinese. We have\nopen-sourced the dataset to encourage further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yuwei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shicheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuhuai Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mukai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yazheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Conversational Recommendation Systems via Counterfactual Data Simulation. (arXiv:2306.02842v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2306.02842","description":"<p>Conversational recommender systems (CRSs) aim to provide recommendation\nservices via natural language conversations. Although a number of approaches\nhave been proposed for developing capable CRSs, they typically rely on\nsufficient training data for training. Since it is difficult to annotate\nrecommendation-oriented dialogue datasets, existing CRS approaches often suffer\nfrom the issue of insufficient training due to the scarcity of training data.\nTo address this issue, in this paper, we propose a CounterFactual data\nsimulation approach for CRS, named CFCRS, to alleviate the issue of data\nscarcity in CRSs. Our approach is developed based on the framework of\ncounterfactual data augmentation, which gradually incorporates the rewriting to\nthe user preference from a real dialogue without interfering with the entire\nconversation flow. To develop our approach, we characterize user preference and\norganize the conversation flow by the entities involved in the dialogue, and\ndesign a multi-stage recommendation dialogue simulator based on a conversation\nflow language model. Under the guidance of the learned user preference and\ndialogue schema, the flow language model can produce reasonable, coherent\nconversation flows, which can be further realized into complete dialogues.\nBased on the simulator, we perform the intervention at the representations of\nthe interacted entities of target users, and design an adversarial training\nmethod with a curriculum schedule that can gradually optimize the data\naugmentation strategy. Extensive experiments show that our approach can\nconsistently boost the performance of several competitive CRSs, and outperform\nother data augmentation methods, especially when the training data is limited.\nOur code is publicly available at https://github.com/RUCAIBox/CFCRS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xinyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Fan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-06-08T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}