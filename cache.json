{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-01-26T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Audience-Centric Natural Language Generation via Style Infusion. (arXiv:2301.10283v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10283","description":"<p>Adopting contextually appropriate, audience-tailored linguistic styles is\ncritical to the success of user-centric language generation systems (e.g.,\nchatbots, computer-aided writing, dialog systems). While existing approaches\ndemonstrate textual style transfer with large volumes of parallel or\nnon-parallel data, we argue that grounding style on audience-independent\nexternal factors is innately limiting for two reasons. First, it is difficult\nto collect large volumes of audience-specific stylistic data. Second, some\nstylistic objectives (e.g., persuasiveness, memorability, empathy) are hard to\ndefine without audience feedback.\n</p>\n<p>In this paper, we propose the novel task of style infusion - infusing the\nstylistic preferences of audiences in pretrained language generation models.\nSince humans are better at pairwise comparisons than direct scoring - i.e., is\nSample-A more persuasive/polite/empathic than Sample-B - we leverage limited\npairwise human judgments to bootstrap a style analysis model and augment our\nseed set of judgments. We then infuse the learned textual style in a GPT-2\nbased text generator while balancing fluency and style adoption. With\nquantitative and qualitative assessments, we show that our infusion approach\ncan generate compelling stylized examples with generic text prompts. The code\nand data are accessible at https://github.com/CrowdDynamicsLab/StyleInfusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moorjani_S/0/1/0/all/0/1\">Samraj Moorjani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_A/0/1/0/all/0/1\">Adit Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaram_H/0/1/0/all/0/1\">Hari Sundaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maslowska_E/0/1/0/all/0/1\">Ewa Maslowska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_A/0/1/0/all/0/1\">Aravind Sankar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large language models can segment narrative events similarly to humans. (arXiv:2301.10297v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10297","description":"<p>Humans perceive discrete events such as \"restaurant visits\" and \"train rides\"\nin their continuous experience. One important prerequisite for studying human\nevent perception is the ability of researchers to quantify when one event ends\nand another begins. Typically, this information is derived by aggregating\nbehavioral annotations from several observers. Here we present an alternative\ncomputational approach where event boundaries are derived using a large\nlanguage model, GPT-3, instead of using human annotations. We demonstrate that\nGPT-3 can segment continuous narrative text into events. GPT-3-annotated events\nare significantly correlated with human event annotations. Furthermore, these\nGPT-derived annotations achieve a good approximation of the \"consensus\"\nsolution (obtained by averaging across human annotations); the boundaries\nidentified by GPT-3 are closer to the consensus, on average, than boundaries\nidentified by individual human annotators. This finding suggests that GPT-3\nprovides a feasible solution for automated event annotations, and it\ndemonstrates a further parallel between human cognition and prediction in large\nlanguage models. In the future, GPT-3 may thereby help to elucidate the\nprinciples underlying human event perception.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michelmann_S/0/1/0/all/0/1\">Sebastian Michelmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">Manoj Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norman_K/0/1/0/all/0/1\">Kenneth A. Norman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toneva_M/0/1/0/all/0/1\">Mariya Toneva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive-Chain-Prompting: Ambiguity Resolution for Crosslingual Conditional Generation with Interaction. (arXiv:2301.10309v1 [cs.LG])","link":"http://arxiv.org/abs/2301.10309","description":"<p>Crosslingual conditional generation (e.g., machine translation) has long\nenjoyed the benefits of scaling. Nonetheless, there are still issues that scale\nalone may not overcome. A source query in one language, for instance, may yield\nseveral translation options in another language without any extra context. Only\none translation could be acceptable however, depending on the translator's\npreferences and goals. Choosing the incorrect option might significantly affect\ntranslation usefulness and quality. We propose a novel method interactive-chain\nprompting -- a series of question, answering and generation intermediate steps\nbetween a Translator model and a User model -- that reduces translations into a\nlist of subproblems addressing ambiguities and then resolving such subproblems\nbefore producing the final text to be translated. To check ambiguity resolution\ncapabilities and evaluate translation quality, we create a dataset exhibiting\ndifferent linguistic phenomena which leads to ambiguities at inference for four\nlanguages. To encourage further exploration in this direction, we release all\ndatasets. We note that interactive-chain prompting, using eight interactions as\nexemplars, consistently surpasses prompt-based methods with direct access to\nbackground information to resolve ambiguities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pilault_J/0/1/0/all/0/1\">Jonathan Pilault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brazinskas_A/0/1/0/all/0/1\">Arthur Bra&#x17e;inskas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model Detoxification in Dialogue with Contextualized Stance Control. (arXiv:2301.10368v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10368","description":"<p>To reduce the toxic degeneration in a pretrained Language Model (LM),\nprevious work on Language Model detoxification has focused on reducing the\ntoxicity of the generation itself (self-toxicity) without consideration of the\ncontext. As a result, a type of implicit offensive language where the\ngenerations support the offensive language in the context is ignored. Different\nfrom the LM controlling tasks in previous work, where the desired attributes\nare fixed for generation, the desired stance of the generation depends on the\noffensiveness of the context. Therefore, we propose a novel control method to\ndo context-dependent detoxification with the stance taken into consideration.\nWe introduce meta prefixes to learn the contextualized stance control strategy\nand to generate the stance control prefix according to the input context. The\ngenerated stance prefix is then combined with the toxicity control prefix to\nguide the response generation. Experimental results show that our proposed\nmethod can effectively learn the context-dependent stance control strategies\nwhile keeping a low self-toxicity of the underlying LM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jing Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xifeng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Headline Dependency Parsing. (arXiv:2301.10371v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10371","description":"<p>English news headlines form a register with unique syntactic properties that\nhave been documented in linguistics literature since the 1930s. However,\nheadlines have received surprisingly little attention from the NLP syntactic\nparsing community. We aim to bridge this gap by providing the first news\nheadline corpus of Universal Dependencies annotated syntactic dependency trees,\nwhich enables us to evaluate existing state-of-the-art dependency parsers on\nnews headlines. To improve English news headline parsing accuracies, we develop\na projection method to bootstrap silver training data from unlabeled news\nheadline-article lead sentence pairs. Models trained on silver headline parses\ndemonstrate significant improvements in performance over models trained solely\non gold-annotated long-form texts. Ultimately, we find that, although projected\nsilver training data improves parser performance across different news outlets,\nthe improvement is moderated by constructions idiosyncratic to outlet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benton_A/0/1/0/all/0/1\">Adrian Benton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1\">Tianze Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irsoy_O/0/1/0/all/0/1\">Ozan &#x130;rsoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malioutov_I/0/1/0/all/0/1\">Igor Malioutov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XNLI: Explaining and Diagnosing NLI-based Visual Data Analysis. (arXiv:2301.10385v1 [cs.HC])","link":"http://arxiv.org/abs/2301.10385","description":"<p>Natural language interfaces (NLIs) enable users to flexibly specify\nanalytical intentions in data visualization. However, diagnosing the\nvisualization results without understanding the underlying generation process\nis challenging. Our research explores how to provide explanations for NLIs to\nhelp users locate the problems and further revise the queries. We present XNLI,\nan explainable NLI system for visual data analysis. The system introduces a\nProvenance Generator to reveal the detailed process of visual transformations,\na suite of interactive widgets to support error adjustments, and a Hint\nGenerator to provide query revision hints based on the analysis of user queries\nand interactions. Two usage scenarios of XNLI and a user study verify the\neffectiveness and usability of the system. Results suggest that XNLI can\nsignificantly enhance task accuracy without interrupting the NLI-based analysis\nprocess.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yingchaojie Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1\">Bo Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam Kwai Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zihan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuxin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10405","description":"<p>Recently decades have witnessed the empirical success of framing Knowledge\nGraph (KG) embeddings via language models. However, language model-based KG\nembeddings are usually deployed as static artifacts, which are challenging to\nmodify without re-training after deployment. To address this issue, we propose\na new task of editing language model-based KG embeddings in this paper. The\nproposed task aims to enable data-efficient and fast updates to KG embeddings\nwithout damaging the performance of the rest. We build four new datasets:\nE-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge\nediting baselines demonstrating the limited ability of previous models to\nhandle the proposed challenging task. We further propose a simple yet strong\nbaseline dubbed KGEditor, which utilizes additional parametric layers of the\nhyper network to edit/add facts. Comprehensive experimental results demonstrate\nthat KGEditor can perform better when updating specific facts while not\naffecting the rest with low training resources. Code and datasets will be\navailable in https://github.com/zjunlp/PromptKG/tree/main/deltaKG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1\">Bozhong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zelin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10410","description":"<p>Cross-domain NER is a challenging task to address the low-resource problem in\npractical scenarios. Previous typical solutions mainly obtain a NER model by\npre-trained language models (PLMs) with data from a rich-resource domain and\nadapt it to the target domain. Owing to the mismatch issue among entity types\nin different domains, previous approaches normally tune all parameters of PLMs,\nending up with an entirely new NER model for each domain. Moreover, current\nmodels only focus on leveraging knowledge in one general source domain while\nfailing to successfully transfer knowledge from multiple sources to the target.\nTo address these issues, we introduce Collaborative Domain-Prefix Tuning for\ncross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically,\nwe present text-to-text generation grounding domain-related instructors to\ntransfer knowledge to new domain NER tasks without structural modifications. We\nutilize frozen PLMs and conduct collaborative domain-prefix tuning to stimulate\nthe potential of PLMs to handle NER tasks across various domains. Experimental\nresults on the Cross-NER benchmark show that the proposed approach has flexible\ntransfer ability and performs better on both one-source and multiple-source\ncross-domain NER tasks. Codes will be available in\nhttps://github.com/zjunlp/DeepKE/tree/main/example/ner/cross.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Q/0/1/0/all/0/1\">Qiaoshuo Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BDMMT: Backdoor Sample Detection for Language Models through Model Mutation Testing. (arXiv:2301.10412v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10412","description":"<p>Deep neural networks (DNNs) and natural language processing (NLP) systems\nhave developed rapidly and have been widely used in various real-world fields.\nHowever, they have been shown to be vulnerable to backdoor attacks.\nSpecifically, the adversary injects a backdoor into the model during the\ntraining phase, so that input samples with backdoor triggers are classified as\nthe target class. Some attacks have achieved high attack success rates on the\npre-trained language models (LMs), but there have yet to be effective defense\nmethods. In this work, we propose a defense method based on deep model mutation\ntesting. Our main justification is that backdoor samples are much more robust\nthan clean samples if we impose random mutations on the LMs and that backdoors\nare generalizable. We first confirm the effectiveness of model mutation testing\nin detecting backdoor samples and select the most appropriate mutation\noperators. We then systematically defend against three extensively studied\nbackdoor attack levels (i.e., char-level, word-level, and sentence-level) by\ndetecting backdoor samples. We also make the first attempt to defend against\nthe latest style-level backdoor attacks. We evaluate our approach on three\nbenchmark datasets (i.e., IMDB, Yelp, and AG news) and three style transfer\ndatasets (i.e., SST-2, Hate-speech, and AG news). The extensive experimental\nresults demonstrate that our approach can detect backdoor samples more\nefficiently and accurately than the three state-of-the-art defense approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jiali Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1\">Ming Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenjing Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Wuxia Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is This Abstract Generated by AI? A Research for the Gap between AI-generated Scientific Text and Human-written Scientific Text. (arXiv:2301.10416v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10416","description":"<p>BACKGROUND: Recent neural language models have taken a significant step\nforward in producing remarkably controllable, fluent, and grammatical text.\nAlthough some recent works have found that AI-generated text is not\ndistinguishable from human-authored writing for crowd-sourcing workers, there\nstill exist errors in AI-generated text which are even subtler and harder to\nspot. METHOD: In this paper, we investigate the gap between scientific content\ngenerated by AI and written by humans. Specifically, we first adopt several\npublicly available tools or models to investigate the performance for detecting\nGPT-generated scientific text. Then we utilize features from writing style to\nanalyze the similarities and differences between the two types of content.\nFurthermore, more complex and deep perspectives, such as consistency,\ncoherence, language redundancy, and factual errors, are also taken into\nconsideration for in-depth analysis. RESULT: The results suggest that while AI\nhas the potential to generate scientific content that is as accurate as\nhuman-written content, there is still a gap in terms of depth and overall\nquality. AI-generated scientific content is more likely to contain errors in\nlanguage redundancy and factual issues. CONCLUSION: We find that there exists a\n``writing style'' gap between AI-generated scientific text and human-written\nscientific text. Moreover, based on the analysis result, we summarize a series\nof model-agnostic or distribution-agnostic features, which could be utilized to\nunknown or novel domain distribution and different generation methods. Future\nresearch should focus on not only improving the capabilities of AI models to\nproduce high-quality content but also examining and addressing ethical and\nsecurity concerns related to the generation and the use of AI-generated\ncontent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yongqiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_F/0/1/0/all/0/1\">Fan Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViDeBERTa: A powerful pre-trained language model for Vietnamese. (arXiv:2301.10439v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10439","description":"<p>This paper presents ViDeBERTa, a new pre-trained monolingual language model\nfor Vietnamese, with three versions - ViDeBERTa_xsmall, ViDeBERTa_base, and\nViDeBERTa_large, which are pre-trained on a large-scale corpus of high-quality\nand diverse Vietnamese texts using DeBERTa architecture. Although many\nsuccessful pre-trained language models based on Transformer have been widely\nproposed for the English language, there are still few pre-trained models for\nVietnamese, a low-resource language, that perform good results on downstream\ntasks, especially Question answering. We fine-tune and evaluate our model on\nthree important natural language downstream tasks, Part-of-speech tagging,\nNamed-entity recognition, and Question answering. The empirical results\ndemonstrate that ViDeBERTa with far fewer parameters surpasses the previous\nstate-of-the-art models on multiple Vietnamese-specific natural language\nunderstanding tasks. Notably, ViDeBERTa_base with 86M parameters, which is only\nabout 23% of PhoBERT_large with 370M parameters, still performs the same or\nbetter results than the previous state-of-the-art model. Our ViDeBERTa models\nare available at: https://github.com/HySonLab/ViDeBERTa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1\">Cong Dao Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1\">Nhut Huy Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hy_T/0/1/0/all/0/1\">Truong Son Hy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tu Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Experimental Study on Pretraining Transformers from Scratch for IR. (arXiv:2301.10444v1 [cs.IR])","link":"http://arxiv.org/abs/2301.10444","description":"<p>Finetuning Pretrained Language Models (PLM) for IR has been de facto the\nstandard practice since their breakthrough effectiveness few years ago. But, is\nthis approach well understood? In this paper, we study the impact of the\npretraining collection on the final IR effectiveness. In particular, we\nchallenge the current hypothesis that PLM shall be trained on a large enough\ngeneric collection and we show that pretraining from scratch on the collection\nof interest is surprisingly competitive with the current approach. We benchmark\nfirst-stage ranking rankers and cross-encoders for reranking on the task of\ngeneral passage retrieval on MSMARCO, Mr-Tydi for Arabic, Japanese and Russian,\nand TripClick for specific domain. Contrary to popular belief, we show that,\nfor finetuning first-stage rankers, models pretrained solely on their\ncollection have equivalent or better effectiveness compared to more general\nmodels. However, there is a slight effectiveness drop for rerankers pretrained\nonly on the target collection. Overall, our study sheds a new light on the role\nof the pretraining collection and should make our community ponder on building\nspecialized models by pretraining from scratch. Last but not least, doing so\ncould enable better control of efficiency, data bias and replicability, which\nare key research questions for the IR community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lassance_C/0/1/0/all/0/1\">Carlos Lassance</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dejean_H/0/1/0/all/0/1\">Herv&#xe9; D&#xe9;jean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clinchant_S/0/1/0/all/0/1\">St&#xe9;phane Clinchant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-computed memory or on-the-fly encoding? A hybrid approach to retrieval augmentation makes the most of your compute. (arXiv:2301.10448v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10448","description":"<p>Retrieval-augmented language models such as Fusion-in-Decoder are powerful,\nsetting the state of the art on a variety of knowledge-intensive tasks.\nHowever, they are also expensive, due to the need to encode a large number of\nretrieved passages. Some work avoids this cost by pre-encoding a text corpus\ninto a memory and retrieving dense representations directly. However,\npre-encoding memory incurs a severe quality penalty as the memory\nrepresentations are not conditioned on the current input. We propose LUMEN, a\nhybrid between these two extremes, pre-computing the majority of the retrieval\nrepresentation and completing the encoding on the fly using a live encoder that\nis conditioned on the question and fine-tuned for the task. We show that LUMEN\nsignificantly outperforms pure memory on multiple question-answering tasks\nwhile being much cheaper than FiD, and outperforms both for any given compute\nbudget. Moreover, the advantage of LUMEN over FiD increases with model size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jong_M/0/1/0/all/0/1\">Michiel de Jong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemlyanskiy_Y/0/1/0/all/0/1\">Yury Zemlyanskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+FitzGerald_N/0/1/0/all/0/1\">Nicholas FitzGerald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghai_S/0/1/0/all/0/1\">Sumit Sanghai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1\">Fei Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-augmented Graph Neural Networks with Concept-aware Attention for Adverse Drug Event Detection. (arXiv:2301.10451v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10451","description":"<p>Adverse drug events (ADEs) are an important aspect of drug safety. Various\ntexts such as biomedical literature, drug reviews, and user posts on social\nmedia and medical forums contain a wealth of information about ADEs. Recent\nstudies have applied word embedding and deep learning -based natural language\nprocessing to automate ADE detection from text. However, they did not explore\nincorporating explicit medical knowledge about drugs and adverse reactions or\nthe corresponding feature learning. This paper adopts the heterogenous text\ngraph which describes relationships between documents, words and concepts,\naugments it with medical knowledge from the Unified Medical Language System,\nand proposes a concept-aware attention mechanism which learns features\ndifferently for the different types of nodes in the graph. We further utilize\ncontextualized embeddings from pretrained language models and convolutional\ngraph neural networks for effective feature representation and relational\nlearning. Experiments on four public datasets show that our model achieves\nperformance competitive to the recent advances and the concept-aware attention\nconsistently outperforms other attention mechanisms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Ya Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marttinen_P/0/1/0/all/0/1\">Pekka Marttinen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Stock Price Movement Classification Using News Articles Based on Embeddings and Label Smoothing. (arXiv:2301.10458v1 [cs.LG])","link":"http://arxiv.org/abs/2301.10458","description":"<p>Stock price movement prediction is a challenging and essential problem in\nfinance. While it is well established in modern behavioral finance that the\nshare prices of related stocks often move after the release of news via\nreactions and overreactions of investors, how to capture the relationships\nbetween price movements and news articles via quantitative models is an active\narea research; existing models have achieved success with variable degrees. In\nthis paper, we propose to improve stock price movement classification using\nnews articles by incorporating regularization and optimization techniques from\ndeep learning. More specifically, we capture the dependencies between news\narticles and stocks through embeddings and bidirectional recurrent neural\nnetworks as in recent models. We further incorporate weight decay, batch\nnormalization, dropout, and label smoothing to improve the generalization of\nthe trained models. To handle high fluctuations of validation accuracy of batch\nnormalization, we propose dual-phase training to realize the improvements\nreliably. Our experimental results on a commonly used dataset show significant\nimprovements, achieving average accuracy of 80.7% on the test set, which is\nmore than 10.0% absolute improvement over existing models. Our ablation studies\nshow batch normalization and label smoothing are most effective, leading to\n6.0% and 3.4% absolute improvement, respectively on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Villamil_L/0/1/0/all/0/1\">Luis Villamil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bausback_R/0/1/0/all/0/1\">Ryan Bausback</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salman_S/0/1/0/all/0/1\">Shaeke Salman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting L. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horn_C/0/1/0/all/0/1\">Conrad Horn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiuwen Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models. (arXiv:2301.10472v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10472","description":"<p>Large multilingual language models typically rely on a single vocabulary\nshared across 100+ languages. As these models have increased in parameter count\nand depth, vocabulary size has remained largely unchanged. This vocabulary\nbottleneck limits the representational capabilities of multilingual models like\nXLM-R. In this paper, we introduce a new approach for scaling to very large\nmultilingual vocabularies by de-emphasizing token sharing between languages\nwith little lexical overlap and assigning vocabulary capacity to achieve\nsufficient coverage for each individual language. Tokenizations using our\nvocabulary are typically more semantically meaningful and shorter compared to\nXLM-R. Leveraging this improved vocabulary, we train XLM-V, a multilingual\nlanguage model with a one million token vocabulary. XLM-V outperforms XLM-R on\nevery task we tested on ranging from natural language inference (XNLI),\nquestion answering (MLQA, XQuAD, TyDiQA), and named entity recognition\n(WikiAnn) to low-resource tasks (Americas NLI, MasakhaNER).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Davis Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonen_H/0/1/0/all/0/1\">Hila Gonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1\">Rui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Naman Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghazvininejad_M/0/1/0/all/0/1\">Marjan Ghazvininejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1\">Madian Khabsa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FewShotTextGCN: K-hop neighborhood regularization for few-shot learning on graphs. (arXiv:2301.10481v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10481","description":"<p>We present FewShotTextGCN, a novel method designed to effectively utilize the\nproperties of word-document graphs for improved learning in low-resource\nsettings. We introduce K-hop Neighbourhood Regularization, a regularizer for\nheterogeneous graphs, and show that it stabilizes and improves learning when\nonly a few training samples are available. We furthermore propose a\nsimplification in the graph-construction method, which results in a graph that\nis $\\sim$7 times less dense and yields better performance in little-resource\nsettings while performing on par with the state of the art in high-resource\nsettings. Finally, we introduce a new variant of Adaptive Pseudo-Labeling\ntailored for word-document graphs. When using as little as 20 samples for\ntraining, we outperform a strong TextGCN baseline with 17% in absolute accuracy\non average over eight languages. We demonstrate that our method can be applied\nto document classification without any language model pretraining on a wide\nrange of typologically diverse languages while performing on par with large\npretrained language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heijden_N/0/1/0/all/0/1\">Niels van der Heijden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1\">Ekaterina Shutova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannakoudakis_H/0/1/0/all/0/1\">Helen Yannakoudakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SWING: Balancing Coverage and Faithfulness for Dialogue Summarization. (arXiv:2301.10483v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10483","description":"<p>Missing information is a common issue of dialogue summarization where some\ninformation in the reference summaries is not covered in the generated\nsummaries. To address this issue, we propose to utilize natural language\ninference (NLI) models to improve coverage while avoiding introducing factual\ninconsistencies. Specifically, we use NLI to compute fine-grained training\nsignals to encourage the model to generate content in the reference summaries\nthat have not been covered, as well as to distinguish between factually\nconsistent and inconsistent generated sentences. Experiments on the DialogSum\nand SAMSum datasets confirm the effectiveness of the proposed approach in\nbalancing coverage and faithfulness, validated with automatic metrics and human\nevaluations. Additionally, we compute the correlation between commonly used\nautomatic metrics with human judgments in terms of three different dimensions\nregarding coverage and factual consistency to provide insight into the most\nsuitable metric for evaluating dialogue summaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kung-Hsiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Siffi Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaofei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_F/0/1/0/all/0/1\">Feng Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dingwall_N/0/1/0/all/0/1\">Nicholas Dingwall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Tenant Optimization For Few-Shot Task-Oriented FAQ Retrieval. (arXiv:2301.10517v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10517","description":"<p>Business-specific Frequently Asked Questions (FAQ) retrieval in task-oriented\ndialog systems poses unique challenges vis-\\`a-vis community based FAQs. Each\nFAQ question represents an intent which is usually an umbrella term for many\nrelated user queries. We evaluate performance for such Business FAQs both with\nstandard FAQ retrieval techniques using query-Question (q-Q) similarity and\nfew-shot intent detection techniques. Implementing a real world solution for\nFAQ retrieval in order to support multiple tenants (FAQ sets) entails\noptimizing speed, accuracy and cost. We propose a novel approach to scale\nmulti-tenant FAQ applications in real-world context by contrastive fine-tuning\nof the last layer in sentence Bi-Encoders along with tenant-specific weight\nswitching.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vishwanathan_A/0/1/0/all/0/1\">Asha Vishwanathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warrier_R/0/1/0/all/0/1\">Rajeev Unnikrishnan Warrier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_G/0/1/0/all/0/1\">Gautham Vadakkekara Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandpal_C/0/1/0/all/0/1\">Chandra Shekhar Kandpal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ExaRanker: Explanation-Augmented Neural Ranker. (arXiv:2301.10521v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10521","description":"<p>Recent work has shown that inducing a large language model (LLM) to generate\nexplanations prior to outputting an answer is an effective strategy to improve\nperformance on a wide range of reasoning tasks. In this work, we show that\nneural rankers also benefit from explanations. We use LLMs such as GPT-3.5 to\naugment retrieval datasets with explanations and train a sequence-to-sequence\nranking model to output a relevance label and an explanation for a given\nquery-document pair. Our model, dubbed ExaRanker, finetuned on a few thousand\nexamples with synthetic explanations performs on par with models finetuned on\n3x more examples without explanations. Furthermore, the ExaRanker model incurs\nno additional computational cost during ranking and allows explanations to be\nrequested on demand.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferraretto_F/0/1/0/all/0/1\">Fernando Ferraretto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laitz_T/0/1/0/all/0/1\">Thiago Laitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1\">Roberto Lotufo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Argument Mining in the Medical Domain. (arXiv:2301.10527v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10527","description":"<p>Nowadays the medical domain is receiving more and more attention in\napplications involving Artificial Intelligence. Clinicians have to deal with an\nenormous amount of unstructured textual data to make a conclusion about\npatients' health in their everyday life. Argument mining helps to provide a\nstructure to such data by detecting argumentative components in the text and\nclassifying the relations between them. However, as it is the case for many\ntasks in Natural Language Processing in general and in medical text processing\nin particular, the large majority of the work on computational argumentation\nhas been done only for English. This is also the case with the only dataset\navailable for argumentation in the medical domain, namely, the annotated\nmedical data of abstracts of Randomized Controlled Trials (RCT) from the\nMEDLINE database. In order to mitigate the lack of annotated data for other\nlanguages, we empirically investigate several strategies to perform argument\nmining and classification in medical texts for a language for which no\nannotated data is available. This project shows that automatically translating\nand project annotations from English to a target language (Spanish) is an\neffective way to generate annotated data without manual intervention.\nFurthermore, our experiments demonstrate that the translation and projection\napproach outperforms zero-shot cross-lingual approaches using a large masked\nmultilingual language model. Finally, we show how the automatically generated\ndata in Spanish can also be used to improve results in the original English\nevaluation setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeginbergenova_A/0/1/0/all/0/1\">Anar Yeginbergenova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1\">Rodrigo Agerri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Backward Compatibility During Data Updates by Weight Interpolation. (arXiv:2301.10546v1 [cs.LG])","link":"http://arxiv.org/abs/2301.10546","description":"<p>Backward compatibility of model predictions is a desired property when\nupdating a machine learning driven application. It allows to seamlessly improve\nthe underlying model without introducing regression bugs. In classification\ntasks these bugs occur in the form of negative flips. This means an instance\nthat was correctly classified by the old model is now classified incorrectly by\nthe updated model. This has direct negative impact on the user experience of\nsuch systems e.g. a frequently used voice assistant query is suddenly\nmisclassified. A common reason to update the model is when new training data\nbecomes available and needs to be incorporated. Simply retraining the model\nwith the updated data introduces the unwanted negative flips. We study the\nproblem of regression during data updates and propose Backward Compatible\nWeight Interpolation (BCWI). This method interpolates between the weights of\nthe old and new model and we show in extensive experiments that it reduces\nnegative flips without sacrificing the improved accuracy of the new model. BCWI\nis straight forward to implement and does not increase inference cost. We also\nexplore the use of importance weighting during interpolation and averaging the\nweights of multiple new models in order to further reduce negative flips.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schumann_R/0/1/0/all/0/1\">Raphael Schumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansimov_E/0/1/0/all/0/1\">Elman Mansimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yi-An Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_N/0/1/0/all/0/1\">Nikolaos Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xibin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study on FGSM Adversarial Training for Neural Retrieval. (arXiv:2301.10576v1 [cs.IR])","link":"http://arxiv.org/abs/2301.10576","description":"<p>Neural retrieval models have acquired significant effectiveness gains over\nthe last few years compared to term-based methods. Nevertheless, those models\nmay be brittle when faced to typos, distribution shifts or vulnerable to\nmalicious attacks. For instance, several recent papers demonstrated that such\nvariations severely impacted models performances, and then tried to train more\nresilient models. Usual approaches include synonyms replacements or typos\ninjections -- as data-augmentation -- and the use of more robust tokenizers\n(characterBERT, BPE-dropout). To further complement the literature, we\ninvestigate in this paper adversarial training as another possible solution to\nthis robustness issue. Our comparison includes the two main families of\nBERT-based neural retrievers, i.e. dense and sparse, with and without\ndistillation techniques. We then demonstrate that one of the most simple\nadversarial training techniques -- the Fast Gradient Sign Method (FGSM) -- can\nimprove first stage rankers robustness and effectiveness. In particular, FGSM\nincreases models performances on both in-domain and out-of-domain\ndistributions, and also on queries with typos, for multiple neural retrievers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lupart_S/0/1/0/all/0/1\">Simon Lupart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clinchant_S/0/1/0/all/0/1\">St&#xe9;phane Clinchant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARDIAS: AI-Enhanced Research Management, Discovery, and Advisory System. (arXiv:2301.10577v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10577","description":"<p>In this work, we present ARDIAS, a web-based application that aims to provide\nresearchers with a full suite of discovery and collaboration tools. ARDIAS\ncurrently allows searching for authors and articles by name and gaining\ninsights into the research topics of a particular researcher. With the aid of\nAI-based tools, ARDIAS aims to recommend potential collaborators and topics to\nresearchers. In the near future, we aim to add tools that allow researchers to\ncommunicate with each other and start new projects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1\">Debayan Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yimam_S/0/1/0/all/0/1\">Seid Muhie Yimam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awale_S/0/1/0/all/0/1\">Sushil Awale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biemann_C/0/1/0/all/0/1\">Chris Biemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Text into Circuits. (arXiv:2301.10595v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10595","description":"<p>This paper concerns the structure of meanings within natural language.\nEarlier, a framework named DisCoCirc was sketched that (1) is compositional and\ndistributional (a.k.a. vectorial); (2) applies to general text; (3) captures\nlinguistic `connections' between meanings (cf. grammar) (4) updates word\nmeanings as text progresses; (5) structures sentence types; (6) accommodates\nambiguity. Here, we realise DisCoCirc for a substantial fragment of English.\n</p>\n<p>When passing to DisCoCirc's text circuits, some `grammatical bureaucracy' is\neliminated, that is, DisCoCirc displays a significant degree of (7) inter- and\nintra-language independence. That is, e.g., independence from word-order\nconventions that differ across languages, and independence from choices like\nmany short sentences vs. few long sentences. This inter-language independence\nmeans our text circuits should carry over to other languages, unlike the\nlanguage-specific typings of categorial grammars. Hence, text circuits are a\nlean structure for the `actual substance of text', that is, the inner-workings\nof meanings within text across several layers of expressiveness (cf. words,\nsentences, text), and may capture that what is truly universal beneath grammar.\nThe elimination of grammatical bureaucracy also explains why DisCoCirc: (8)\napplies beyond language, e.g. to spatial, visual and other cognitive modes.\nWhile humans could not verbally communicate in terms of text circuits, machines\ncan.\n</p>\n<p>We first define a `hybrid grammar' for a fragment of English, i.e. a\npurpose-built, minimal grammatical formalism needed to obtain text circuits. We\nthen detail a translation process such that all text generated by this grammar\nyields a text circuit. Conversely, for any text circuit obtained by freely\ncomposing the generators, there exists a text (with hybrid grammar) that gives\nrise to it. Hence: (9) text circuits are generative for text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Mascianica_V/0/1/0/all/0/1\">Vincent Wang-Mascianica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jonathon Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coecke_B/0/1/0/all/0/1\">Bob Coecke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated multilingual detection of Pro-Kremlin propaganda in newspapers and Telegram posts. (arXiv:2301.10604v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10604","description":"<p>The full-scale conflict between the Russian Federation and Ukraine generated\nan unprecedented amount of news articles and social media data reflecting\nopposing ideologies and narratives. These polarized campaigns have led to\nmutual accusations of misinformation and fake news, shaping an atmosphere of\nconfusion and mistrust for readers worldwide. This study analyses how the media\naffected and mirrored public opinion during the first month of the war using\nnews articles and Telegram news channels in Ukrainian, Russian, Romanian and\nEnglish. We propose and compare two methods of multilingual automated\npro-Kremlin propaganda identification, based on Transformers and linguistic\nfeatures. We analyse the advantages and disadvantages of both methods, their\nadaptability to new genres and languages, and ethical considerations of their\nusage for content moderation. With this work, we aim to lay the foundation for\nfurther development of moderation tools tailored to the current conflict.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Solopova_V/0/1/0/all/0/1\">Veronika Solopova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_O/0/1/0/all/0/1\">Oana-Iuliana Popescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benzmuller_C/0/1/0/all/0/1\">Christoph Benzm&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landgraf_T/0/1/0/all/0/1\">Tim Landgraf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Holistic Cascade System, benchmark, and Human Evaluation Protocol for Expressive Speech-to-Speech Translation. (arXiv:2301.10606v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10606","description":"<p>Expressive speech-to-speech translation (S2ST) aims to transfer prosodic\nattributes of source speech to target speech while maintaining translation\naccuracy. Existing research in expressive S2ST is limited, typically focusing\non a single expressivity aspect at a time. Likewise, this research area lacks\nstandard evaluation protocols and well-curated benchmark datasets. In this\nwork, we propose a holistic cascade system for expressive S2ST, combining\nmultiple prosody transfer techniques previously considered only in isolation.\nWe curate a benchmark expressivity test set in the TV series domain and\nexplored a second dataset in the audiobook domain. Finally, we present a human\nevaluation protocol to assess multiple expressive dimensions across speech\npairs. Experimental results indicate that bi-lingual annotators can assess the\nquality of expressive preservation in S2ST systems, and the holistic modeling\napproach outperforms single-aspect systems. Audio samples can be accessed\nthrough our demo webpage:\nhttps://facebookresearch.github.io/speech_translation/cascade_expressive_s2st.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wen-Chin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peloquin_B/0/1/0/all/0/1\">Benjamin Peloquin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_J/0/1/0/all/0/1\">Justine Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salesky_E/0/1/0/all/0/1\">Elizabeth Salesky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng-Jen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Taxonomic and Thematic Embeddings for Taxonomic Information. (arXiv:2301.10656v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10656","description":"<p>Modelling taxonomic and thematic relatedness is important for building AI\nwith comprehensive natural language understanding. The goal of this paper is to\nlearn more about how taxonomic information is structurally encoded in\nembeddings. To do this, we design a new hypernym-hyponym probing task and\nperform a comparative probing study of taxonomic and thematic SGNS and GloVe\nembeddings. Our experiments indicate that both types of embeddings encode some\ntaxonomic information, but the amount, as well as the geometric properties of\nthe encodings, are independently related to both the encoder architecture, as\nwell as the embedding training data. Specifically, we find that only taxonomic\nembeddings carry taxonomic information in their norm, which is determined by\nthe underlying distribution in the data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klubicka_F/0/1/0/all/0/1\">Filip Klubi&#x10d;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelleher_J/0/1/0/all/0/1\">John D. Kelleher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistency is Key: Disentangling Label Variation in Natural Language Processing with Intra-Annotator Agreement. (arXiv:2301.10684v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10684","description":"<p>We commonly use agreement measures to assess the utility of judgements made\nby human annotators in Natural Language Processing (NLP) tasks. While\ninter-annotator agreement is frequently used as an indication of label\nreliability by measuring consistency between annotators, we argue for the\nadditional use of intra-annotator agreement to measure label stability over\ntime. However, in a systematic review, we find that the latter is rarely\nreported in this field. Calculating these measures can act as important quality\ncontrol and provide insights into why annotators disagree. We propose\nexploratory annotation experiments to investigate the relationships between\nthese measures and perceptions of subjectivity and ambiguity in text items.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abercrombie_G/0/1/0/all/0/1\">Gavin Abercrombie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1\">Verena Rieser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Strategies for Clause Recommendation. (arXiv:2301.10716v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10716","description":"<p>Clause recommendation is the problem of recommending a clause to a legal\ncontract, given the context of the contract in question and the clause type to\nwhich the clause should belong. With not much prior work being done toward the\ngeneration of legal contracts, this problem was proposed as a first step toward\nthe bigger problem of contract generation. As an open-ended text generation\nproblem, the distinguishing characteristics of this problem lie in the nature\nof legal language as a sublanguage and the considerable similarity of textual\ncontent within the clauses of a specific type. This similarity aspect in legal\nclauses drives us to investigate the importance of similar contracts'\nrepresentation for recommending clauses. In our work, we experiment with\ngenerating clauses for 15 commonly occurring clause types in contracts\nexpanding upon the previous work on this problem and analyzing clause\nrecommendations in varying settings using information derived from similar\ncontracts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Sagar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balaji_S/0/1/0/all/0/1\">Sumanth Balaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_J/0/1/0/all/0/1\">Jerrin Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garimella_A/0/1/0/all/0/1\">Aparna Garimella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_V/0/1/0/all/0/1\">Vasudeva Varma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fillers in Spoken Language Understanding: Computational and Psycholinguistic Perspectives. (arXiv:2301.10761v1 [cs.CL])","link":"http://arxiv.org/abs/2301.10761","description":"<p>Disfluencies (i.e. interruptions in the regular flow of speech), are\nubiquitous to spoken discourse. Fillers (\"uh\", \"um\") are disfluencies that\noccur the most frequently compared to other kinds of disfluencies. Yet, to the\nbest of our knowledge, there isn't a resource that brings together the research\nperspectives influencing Spoken Language Understanding (SLU) on these speech\nevents. This aim of this article is to synthesise a breadth of perspectives in\na holistic way; i.e. from considering underlying (psycho)linguistic theory, to\ntheir annotation and consideration in Automatic Speech Recognition (ASR) and\nSLU systems, to lastly, their study from a generation standpoint. This article\naims to present the perspectives in an approachable way to the SLU and\nConversational AI community, and discuss moving forward, what we believe are\nthe trends and challenges in each area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dinkar_T/0/1/0/all/0/1\">Tanvi Dinkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1\">Chlo&#xe9; Clavel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilescu_I/0/1/0/all/0/1\">Ioana Vasilescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patient Outcome and Zero-shot Diagnosis Prediction with Hypernetwork-guided Multitask Learning. (arXiv:2109.03062v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03062","description":"<p>Multitask deep learning has been applied to patient outcome prediction from\ntext, taking clinical notes as input and training deep neural networks with a\njoint loss function of multiple tasks. However, the joint training scheme of\nmultitask learning suffers from inter-task interference, and diagnosis\nprediction among the multiple tasks has the generalizability issue due to rare\ndiseases or unseen diagnoses. To solve these challenges, we propose a\nhypernetwork-based approach that generates task-conditioned parameters and\ncoefficients of multitask prediction heads to learn task-specific prediction\nand balance the multitask learning. We also incorporate semantic task\ninformation to improves the generalizability of our task-conditioned multitask\nmodel. Experiments on early and discharge notes extracted from the real-world\nMIMIC database show our method can achieve better performance on multitask\npatient outcome prediction than strong baselines in most cases. Besides, our\nmethod can effectively handle the scenario with limited information and improve\nzero-shot prediction on unseen diagnosis categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marttinen_P/0/1/0/all/0/1\">Pekka Marttinen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TextRGNN: Residual Graph Neural Networks for Text Classification. (arXiv:2112.15060v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.15060","description":"<p>Recently, text classification model based on graph neural network (GNN) has\nattracted more and more attention. Most of these models adopt a similar network\nparadigm, that is, using pre-training node embedding initialization and\ntwo-layer graph convolution. In this work, we propose TextRGNN, an improved GNN\nstructure that introduces residual connection to deepen the convolution network\ndepth. Our structure can obtain a wider node receptive field and effectively\nsuppress the over-smoothing of node features. In addition, we integrate the\nprobabilistic language model into the initialization of graph node embedding,\nso that the non-graph semantic information of can be better extracted. The\nexperimental results show that our model is general and efficient. It can\nsignificantly improve the classification accuracy whether in corpus level or\ntext level, and achieve SOTA performance on a wide range of text classification\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiayuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Boyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Review of Deep Learning for Automated Medical Coding. (arXiv:2201.02797v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.02797","description":"<p>Automated medical coding, an essential task for healthcare operation and\ndelivery, makes unstructured data manageable by predicting medical codes from\nclinical documents. Recent advances in deep learning models in natural language\nprocessing have been widely applied to this task. However, it lacks a unified\nview of the design of neural network architectures for medical coding. This\nreview proposes a unified framework to provide a general understanding of the\nbuilding blocks of medical coding models and summarizes recent advanced models\nunder the proposed framework. Our unified framework decomposes medical coding\ninto four main components, i.e., encoder modules for text feature extraction,\nmechanisms for building deep encoder architectures, decoder modules for\ntransforming hidden representations into medical codes, and the usage of\nauxiliary information. Finally, we discuss key research challenges and future\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Honghan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marttinen_P/0/1/0/all/0/1\">Pekka Marttinen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Learning-based Stance Classifier for COVID-19-related Health Policies. (arXiv:2209.04631v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.04631","description":"<p>The ongoing COVID-19 pandemic has caused immeasurable losses for people\nworldwide. To contain the spread of the virus and further alleviate the crisis,\nvarious health policies (e.g., stay-at-home orders) have been issued which\nspark heated discussions as users turn to share their attitudes on social\nmedia. In this paper, we consider a more realistic scenario on stance detection\n(i.e., cross-target and zero-shot settings) for the pandemic and propose an\nadversarial learning-based stance classifier to automatically identify the\npublic's attitudes toward COVID-19-related health policies. Specifically, we\nadopt adversarial learning that allows the model to train on a large amount of\nlabeled data and capture transferable knowledge from source topics, so as to\nenable generalize to the emerging health policies with sparse labeled data. To\nfurther enhance the model's deeper understanding, we incorporate policy\ndescriptions as external knowledge into the model. Meanwhile, a GeoEncoder is\ndesigned which encourages the model to capture unobserved background factors\nspecified by each region and then represent them as non-text information. We\nevaluate the performance of a broad range of baselines on the stance detection\ntask for COVID-19-related health policies, and experimental results show that\nour proposed method achieves state-of-the-art performance in both cross-target\nand zero-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1\">Feng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xuechen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haiyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Jiaying Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Lei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yusong Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Fidelity Assessment for Strategy Training in Inpatient Rehabilitation using Natural Language Processing. (arXiv:2209.06727v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.06727","description":"<p>Strategy training is a multidisciplinary rehabilitation approach that teaches\nskills to reduce disability among those with cognitive impairments following a\nstroke. Strategy training has been shown in randomized, controlled clinical\ntrials to be a more feasible and efficacious intervention for promoting\nindependence than traditional rehabilitation approaches. A standardized\nfidelity assessment is used to measure adherence to treatment principles by\nexamining guided and directed verbal cues in video recordings of rehabilitation\nsessions. Although the fidelity assessment for detecting guided and directed\nverbal cues is valid and feasible for single-site studies, it can become labor\nintensive, time consuming, and expensive in large, multi-site pragmatic trials.\nTo address this challenge to widespread strategy training implementation, we\nleveraged natural language processing (NLP) techniques to automate the strategy\ntraining fidelity assessment, i.e., to automatically identify guided and\ndirected verbal cues from video recordings of rehabilitation sessions. We\ndeveloped a rule-based NLP algorithm, a long-short term memory (LSTM) model,\nand a bidirectional encoder representation from transformers (BERT) model for\nthis task. The best performance was achieved by the BERT model with a 0.8075\nF1-score. This BERT model was verified on an external validation dataset\ncollected from a separate major regional health system and achieved an F1 score\nof 0.8259, which shows that the BERT model generalizes well. The findings from\nthis study hold widespread promise in psychology and rehabilitation\nintervention research and practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Osterhoudt_H/0/1/0/all/0/1\">Hunter Osterhoudt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_C/0/1/0/all/0/1\">Courtney E. Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_H/0/1/0/all/0/1\">Haneef A Mohammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shih_M/0/1/0/all/0/1\">Minmei Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harper_A/0/1/0/all/0/1\">Alexandra E. Harper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Leming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skidmore_E/0/1/0/all/0/1\">Elizabeth R Skidmore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanshan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Generation of Interpretable Inference Rules in a Neuro-Symbolic Expert System. (arXiv:2209.07662v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.07662","description":"<p>We present an approach for systematic reasoning that produces human\ninterpretable proof trees grounded in a factbase. Our solution evokes classic\nProlog-based inference engines, where we replace handcrafted rules through a\ncombination of neural language modeling, guided generation, and semiparametric\ndense retrieval. This novel reasoning engine, NELLIE, dynamically instantiates\ninterpretable inference rules that capture and score entailment\n(de)compositions over natural language statements. NELLIE shows competitive\nperformance on scientific QA datasets requiring structured explanations over\nmultiple facts while fully grounding justification proofs in verified\nknowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weir_N/0/1/0/all/0/1\">Nathaniel Weir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generate rather than Retrieve: Large Language Models are Strong Context Generators. (arXiv:2209.10063v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.10063","description":"<p>Knowledge-intensive tasks, such as open-domain question answering (QA),\nrequire access to a large amount of world or domain knowledge. A common\napproach for knowledge-intensive tasks is to employ a retrieve-then-read\npipeline that first retrieves a handful of relevant contextual documents from\nan external corpus such as Wikipedia and then predicts an answer conditioned on\nthe retrieved documents. In this paper, we present a novel perspective for\nsolving knowledge-intensive tasks by replacing document retrievers with large\nlanguage model generators. We call our method generate-then-read (GenRead),\nwhich first prompts a large language model to generate contextutal documents\nbased on a given question, and then reads the generated documents to produce\nthe final answer. Furthermore, we propose a novel clustering-based prompting\nmethod that selects distinct prompts, resulting in the generated documents that\ncover different perspectives, leading to better recall over acceptable answers.\nWe conduct extensive experiments on three different knowledge-intensive tasks,\nincluding open-domain QA, fact checking, and dialogue system. Notably, GenRead\nachieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly\noutperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0\nand +3.9, without retrieving any documents from any external knowledge source.\nLastly, we demonstrate the model performance can be further improved by\ncombining retrieval and generation. Our code and generated documents can be\nfound at https://github.com/wyu97/GenRead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iter_D/0/1/0/all/0/1\">Dan Iter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_M/0/1/0/all/0/1\">Mingxuan Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1\">Soumya Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Analogical Reasoning over Knowledge Graphs. (arXiv:2210.00312v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.00312","description":"<p>Analogical reasoning is fundamental to human cognition and holds an important\nplace in various fields. However, previous studies mainly focus on single-modal\nanalogical reasoning and ignore taking advantage of structure knowledge.\nNotably, the research in cognitive psychology has demonstrated that information\nfrom multimodal sources always brings more powerful cognitive transfer than\nsingle modality sources. To this end, we introduce the new task of multimodal\nanalogical reasoning over knowledge graphs, which requires multimodal reasoning\nability with the help of background knowledge. Specifically, we construct a\nMultimodal Analogical Reasoning dataSet (MARS) and a multimodal knowledge graph\nMarKG. We evaluate with multimodal knowledge graph embedding and pre-trained\nTransformer baselines, illustrating the potential challenges of the proposed\ntask. We further propose a novel model-agnostic Multimodal analogical reasoning\nframework with Transformer (MarT) motivated by the structure mapping theory,\nwhich can obtain better performance. Code and datasets are available in\nhttps://github.com/zjunlp/MKG_Analogy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought. (arXiv:2210.01240v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.01240","description":"<p>Large language models (LLMs) have shown remarkable reasoning capabilities\ngiven chain-of-thought prompts (examples with intermediate reasoning steps).\nExisting benchmarks measure reasoning ability indirectly, by evaluating\naccuracy on downstream tasks such as mathematical reasoning. However, it is\nunclear how these models obtain the answers and whether they rely on simple\nheuristics rather than the generated chain-of-thought. To enable systematic\nexploration of the reasoning ability of LLMs, we present a new synthetic\nquestion-answering dataset called PrOntoQA, where each example is generated\nfrom a synthetic world model represented in first-order logic. This allows us\nto parse the generated chain-of-thought into symbolic proofs for formal\nanalysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite\ncapable of making correct individual deduction steps, and so are generally\ncapable of reasoning, even in fictional contexts. However, they have difficulty\nwith proof planning: When multiple valid deduction steps are available, they\nare not able to systematically explore the different options.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saparov_A/0/1/0/all/0/1\">Abulhair Saparov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks. (arXiv:2210.05664v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05664","description":"<p>Dialogue systems capable of social influence such as persuasion, negotiation,\nand therapy, are essential for extending the use of technology to numerous\nrealistic scenarios. However, existing research primarily focuses on either\ntask-oriented or open-domain scenarios, a categorization that has been\ninadequate for capturing influence skills systematically. There exists no\nformal definition or category for dialogue systems with these skills and\ndata-driven efforts in this direction are highly limited. In this work, we\nformally define and introduce the category of social influence dialogue systems\nthat influence users' cognitive and emotional responses, leading to changes in\nthoughts, opinions, and behaviors through natural conversations. We present a\nsurvey of various tasks, datasets, and methods, compiling the progress across\nseven diverse domains. We discuss the commonalities and differences between the\nexamined systems, identify limitations, and recommend future directions. This\nstudy serves as a comprehensive reference for social influence dialogue systems\nto inspire more dedicated research and discussion in this emerging area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chawla_K/0/1/0/all/0/1\">Kushal Chawla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weiyan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_G/0/1/0/all/0/1\">Gale Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gratch_J/0/1/0/all/0/1\">Jonathan Gratch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context Variance Evaluation of Pretrained Language Models for Prompt-based Biomedical Knowledge Probing. (arXiv:2211.10265v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.10265","description":"<p>Pretrained language models (PLMs) have motivated research on what kinds of\nknowledge these models learn. Fill-in-the-blanks problem (e.g., cloze tests) is\na natural approach for gauging such knowledge. BioLAMA generates prompts for\nbiomedical factual knowledge triples and uses the Top-k accuracy metric to\nevaluate different PLMs' knowledge. However, existing research has shown that\nsuch prompt-based knowledge probing methods can only probe a lower bound of\nknowledge. Many factors like prompt-based probing biases make the LAMA\nbenchmark unreliable and unstable. This problem is more prominent in BioLAMA.\nThe severe long-tailed distribution in vocabulary and large-N-M relation make\nthe performance gap between LAMA and BioLAMA remain notable. To address these,\nwe introduce context variance into the prompt generation and propose a new\nrank-change-based evaluation metric. Different from the previous known-unknown\nevaluation criteria, we propose the concept of \"Misunderstand\" in LAMA for the\nfirst time. Through experiments on 12 PLMs, our context variance prompts and\nUnderstand-Confuse-Misunderstand (UCM) metric makes BioLAMA more friendly to\nlarge-N-M relations and rare relations. We also conducted a set of control\nexperiments to disentangle \"understand\" from just \"read and copy\".\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zonghai Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Talking About Large Language Models. (arXiv:2212.03551v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.03551","description":"<p>Thanks to rapid progress in artificial intelligence, we have entered an era\nwhen technology and philosophy intersect in interesting ways. Sitting squarely\nat the centre of this intersection are large language models (LLMs). The more\nadept LLMs become at mimicking human language, the more vulnerable we become to\nanthropomorphism, to seeing the systems in which they are embedded as more\nhuman-like than they really are. This trend is amplified by the natural\ntendency to use philosophically loaded terms, such as \"knows\", \"believes\", and\n\"thinks\", when describing these systems. To mitigate this trend, this paper\nadvocates the practice of repeatedly stepping back to remind ourselves of how\nLLMs, and the systems of which they form a part, actually work. The hope is\nthat increased scientific precision will encourage more philosophical nuance in\nthe discourse around artificial intelligence, both within the field and in the\npublic sphere.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shanahan_M/0/1/0/all/0/1\">Murray Shanahan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Removing Non-Stationary Knowledge From Pre-Trained Language Models for Entity-Level Sentiment Classification in Finance. (arXiv:2301.03136v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.03136","description":"<p>Extraction of sentiment signals from news text, stock message boards, and\nbusiness reports, for stock movement prediction, has been a rising field of\ninterest in finance. Building upon past literature, the most recent works\nattempt to better capture sentiment from sentences with complex syntactic\nstructures by introducing aspect-level sentiment classification (ASC). Despite\nthe growing interest, however, fine-grained sentiment analysis has not been\nfully explored in non-English literature due to the shortage of annotated\nfinance-specific data. Accordingly, it is necessary for non-English languages\nto leverage datasets and pre-trained language models (PLM) of different\ndomains, languages, and tasks to best their performance. To facilitate\nfinance-specific ASC research in the Korean language, we build KorFinASC, a\nKorean aspect-level sentiment classification dataset for finance consisting of\n12,613 human-annotated samples, and explore methods of intermediate transfer\nlearning. Our experiments indicate that past research has been ignorant towards\nthe potentially wrong knowledge of financial entities encoded during the\ntraining phase, which has overestimated the predictive power of PLMs. In our\nwork, we use the term \"non-stationary knowledge'' to refer to information that\nwas previously correct but is likely to change, and present \"TGT-Masking'', a\nnovel masking pattern to restrict PLMs from speculating knowledge of the kind.\nFinally, through a series of transfer learning with TGT-Masking applied we\nimprove 22.63% of classification accuracy compared to standalone models on\nKorFinASC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Son_G/0/1/0/all/0/1\">Guijin Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hanwool Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_N/0/1/0/all/0/1\">Nahyeon Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahm_M/0/1/0/all/0/1\">Moonjeong Hahm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Same Words, Different Meanings: Semantic Polarization in Broadcast Media Language Forecasts Polarization on Social Media Discourse. (arXiv:2301.08832v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.08832","description":"<p>With the growth of online news over the past decade, empirical studies on\npolitical discourse and news consumption have focused on the phenomenon of\nfilter bubbles and echo chambers. Yet recently, scholars have revealed limited\nevidence around the impact of such phenomenon, leading some to argue that\npartisan segregation across news audiences cannot be fully explained by online\nnews consumption alone and that the role of traditional legacy media may be as\nsalient in polarizing public discourse around current events. In this work, we\nexpand the scope of analysis to include both online and more traditional media\nby investigating the relationship between broadcast news media language and\nsocial media discourse. By analyzing a decade's worth of closed captions (2\nmillion speaker turns) from CNN and Fox News along with topically corresponding\ndiscourse from Twitter, we provide a novel framework for measuring semantic\npolarization between America's two major broadcast networks to demonstrate how\nsemantic polarization between these outlets has evolved (Study 1), peaked\n(Study 2) and influenced partisan discussions on Twitter (Study 3) across the\nlast decade. Our results demonstrate a sharp increase in polarization in how\ntopically important keywords are discussed between the two channels, especially\nafter 2016, with overall highest peaks occurring in 2020. The two stations\ndiscuss identical topics in drastically distinct contexts in 2020, to the\nextent that there is barely any linguistic overlap in how identical keywords\nare contextually discussed. Further, we demonstrate at scale, how such partisan\ndivision in broadcast media language significantly shapes semantic polarity\ntrends on Twitter (and vice-versa), empirically linking for the first time, how\nonline discussions are influenced by televised media.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaohan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horning_M/0/1/0/all/0/1\">Mike Horning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rho_E/0/1/0/all/0/1\">Eugenia H. Rho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PrimeQA: The Prime Repository for State-of-the-Art Multilingual Question Answering Research and Development. (arXiv:2301.09715v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.09715","description":"<p>The field of Question Answering (QA) has made remarkable progress in recent\nyears, thanks to the advent of large pre-trained language models, newer\nrealistic benchmark datasets with leaderboards, and novel algorithms for key\ncomponents such as retrievers and readers. In this paper, we introduce PRIMEQA:\na one-stop and open-source QA repository with an aim to democratize QA\nre-search and facilitate easy replication of state-of-the-art (SOTA) QA\nmethods. PRIMEQA supports core QA functionalities like retrieval and reading\ncomprehension as well as auxiliary capabilities such as question generation.It\nhas been designed as an end-to-end toolkit for various use cases: building\nfront-end applications, replicating SOTA methods on pub-lic benchmarks, and\nexpanding pre-existing methods. PRIMEQA is available at :\nhttps://github.com/primeqa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1\">Avirup Sil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_J/0/1/0/all/0/1\">Jaydeep Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_B/0/1/0/all/0/1\">Bhavani Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franz_M/0/1/0/all/0/1\">Martin Franz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fadnis_K/0/1/0/all/0/1\">Kshitij Fadnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bornea_M/0/1/0/all/0/1\">Mihaela Bornea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenthal_S/0/1/0/all/0/1\">Sara Rosenthal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCarley_S/0/1/0/all/0/1\">Scott McCarley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vishwajeet Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yulong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sultan_M/0/1/0/all/0/1\">Md Arafat Sultan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1\">Riyaz Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florian_R/0/1/0/all/0/1\">Radu Florian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roukos_S/0/1/0/all/0/1\">Salim Roukos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-tail Relation Extraction via Knowledge Graph Embeddings and Graph Convolution Networks. (arXiv:1903.01306v1 [cs.IR] CROSS LISTED)","link":"http://arxiv.org/abs/1903.01306","description":"<p>We propose a distance supervised relation extraction approach for\nlong-tailed, imbalanced data which is prevalent in real-world settings. Here,\nthe challenge is to learn accurate \"few-shot\" models for classes existing at\nthe tail of the class distribution, for which little data is available.\nInspired by the rich semantic correlations between classes at the long tail and\nthose at the head, we take advantage of the knowledge from data-rich classes at\nthe head of the distribution to boost the performance of the data-poor classes\nat the tail. First, we propose to leverage implicit relational knowledge among\nclass labels from knowledge graph embeddings and learn explicit relational\nknowledge using graph convolution networks. Second, we integrate that\nrelational knowledge into relation extraction model by coarse-to-fine\nknowledge-aware attention mechanism. We demonstrate our results for a\nlarge-scale benchmark dataset which show that our approach significantly\noutperforms other baselines, especially for long-tail relations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhanlin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-aware Deep Model for Entity Recommendation in Search Engine at Alibaba. (arXiv:1909.04493v1 [cs.IR] CROSS LISTED)","link":"http://arxiv.org/abs/1909.04493","description":"<p>Entity recommendation, providing search users with an improved experience via\nassisting them in finding related entities for a given query, has become an\nindispensable feature of today's search engines. Existing studies typically\nonly consider the queries with explicit entities. They usually fail to handle\ncomplex queries that without entities, such as \"what food is good for cold\nweather\", because their models could not infer the underlying meaning of the\ninput text. In this work, we believe that contexts convey valuable evidence\nthat could facilitate the semantic modeling of queries, and take them into\nconsideration for entity recommendation. In order to better model the semantics\nof queries and entities, we learn the representation of queries and entities\njointly with attentive deep neural networks. We evaluate our approach using\nlarge-scale, real-world search logs from a widely used commercial Chinese\nsearch engine. Our system has been deployed in ShenMa Search Engine and you can\nfetch it in UC Browser of Alibaba. Results from online A/B test suggest that\nthe impression efficiency of click-through rate increased by 5.1% and page view\nincreased by 5.5%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Q/0/1/0/all/0/1\">Qianghuai Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_N/0/1/0/all/0/1\">Nengwei Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation Adversarial Network for Low Resource Knowledge Graph Completion. (arXiv:1911.03091v6 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/1911.03091","description":"<p>Knowledge Graph Completion (KGC) has been proposed to improve Knowledge\nGraphs by filling in missing connections via link prediction or relation\nextraction. One of the main difficulties for KGC is a low resource problem.\nPrevious approaches assume sufficient training triples to learn versatile\nvectors for entities and relations, or a satisfactory number of labeled\nsentences to train a competent relation extraction model. However, low resource\nrelations are very common in KGs, and those newly added relations often do not\nhave many known samples for training. In this work, we aim at predicting new\nfacts under a challenging setting where only limited training instances are\navailable. We propose a general framework called Weighted Relation Adversarial\nNetwork, which utilizes an adversarial procedure to help adapt\nknowledge/features learned from high resource relations to different but\nrelated low resource relations. Specifically, the framework takes advantage of\na relation discriminator to distinguish between samples from different\nrelations, and help learn relation-invariant features more transferable from\nsource relations to target relations. Experimental results show that the\nproposed approach outperforms previous methods regarding low resource settings\nfor both link prediction and relation extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhanlin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoayan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conceptualized Representation Learning for Chinese Biomedical Text Mining. (arXiv:2008.10813v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2008.10813","description":"<p>Biomedical text mining is becoming increasingly important as the number of\nbiomedical documents and web data rapidly grows. Recently, word representation\nmodels such as BERT has gained popularity among researchers. However, it is\ndifficult to estimate their performance on datasets containing biomedical texts\nas the word distributions of general and biomedical corpora are quite\ndifferent. Moreover, the medical domain has long-tail concepts and\nterminologies that are difficult to be learned via language models. For the\nChinese biomedical text, it is more difficult due to its complex structure and\nthe variety of phrase combinations. In this paper, we investigate how the\nrecently introduced pre-trained language model BERT can be adapted for Chinese\nbiomedical corpora and propose a novel conceptualized representation learning\napproach. We also release a new Chinese Biomedical Language Understanding\nEvaluation benchmark (\\textbf{ChineseBLUE}). We examine the effectiveness of\nChinese pre-trained models: BERT, BERT-wwm, RoBERTa, and our approach.\nExperimental results on the benchmark show that our approach could bring\nsignificant gain. We release the pre-trained model on GitHub:\nhttps://github.com/alibaba-research/ChineseBLUE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Q/0/1/0/all/0/1\">Qianghuai Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kangping Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Liang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Feng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_N/0/1/0/all/0/1\">Nengwei Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Robustness and Bias Analysis of BERT-based Relation Extraction. (arXiv:2009.06206v5 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2009.06206","description":"<p>Fine-tuning pre-trained models have achieved impressive performance on\nstandard natural language processing benchmarks. However, the resultant model\ngeneralizability remains poorly understood. We do not know, for example, how\nexcellent performance can lead to the perfection of generalization models. In\nthis study, we analyze a fine-tuned BERT model from different perspectives\nusing relation extraction. We also characterize the differences in\ngeneralization techniques according to our proposed improvements. From\nempirical experimentation, we find that BERT suffers a bottleneck in terms of\nrobustness by way of randomizations, adversarial and counterfactual tests, and\nbiases (i.e., selection and semantic). These findings highlight opportunities\nfor future improvements. Our open-sourced testbed DiagnoseRE is available in\n\\url{https://github.com/zjunlp/DiagnoseRE}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Triple Extraction with Generative Transformer. (arXiv:2009.06207v8 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2009.06207","description":"<p>Triple extraction is an essential task in information extraction for natural\nlanguage processing and knowledge graph construction. In this paper, we revisit\nthe end-to-end triple extraction task for sequence generation. Since generative\ntriple extraction may struggle to capture long-term dependencies and generate\nunfaithful triples, we introduce a novel model, contrastive triple extraction\nwith a generative transformer. Specifically, we introduce a single shared\ntransformer module for encoder-decoder-based generation. To generate faithful\nresults, we propose a novel triplet contrastive training object. Moreover, we\nintroduce two mechanisms to further improve model performance (i.e., batch-wise\ndynamic attention-masking and triple-wise calibration). Experimental results on\nthree datasets (i.e., NYT, WebNLG, and MIE) show that our approach achieves\nbetter performance than that of baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Devil is the Classifier: Investigating Long Tail Relation Classification with Decoupling Analysis. (arXiv:2009.07022v1 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2009.07022","description":"<p>Long-tailed relation classification is a challenging problem as the head\nclasses may dominate the training phase, thereby leading to the deterioration\nof the tail performance. Existing solutions usually address this issue via\nclass-balancing strategies, e.g., data re-sampling and loss re-weighting, but\nall these methods adhere to the schema of entangling learning of the\nrepresentation and classifier. In this study, we conduct an in-depth empirical\ninvestigation into the long-tailed problem and found that pre-trained models\nwith instance-balanced sampling already capture the well-learned\nrepresentations for all classes; moreover, it is possible to achieve better\nlong-tailed classification ability at low cost by only adjusting the\nclassifier. Inspired by this observation, we propose a robust classifier with\nattentive relation routing, which assigns soft weights by automatically\naggregating the relations. Extensive experiments on two datasets demonstrate\nthe effectiveness of our proposed approach. Code and datasets are available in\nhttps://github.com/zjunlp/deepke.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zonggang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yantao Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging Text and Knowledge with Multi-Prototype Embedding for Few-Shot Relational Triple Extraction. (arXiv:2010.16059v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2010.16059","description":"<p>Current supervised relational triple extraction approaches require huge\namounts of labeled data and thus suffer from poor performance in few-shot\nsettings. However, people can grasp new knowledge by learning a few instances.\nTo this end, we take the first step to study the few-shot relational triple\nextraction, which has not been well understood. Unlike previous single-task\nfew-shot problems, relational triple extraction is more challenging as the\nentities and relations have implicit correlations. In this paper, We propose a\nnovel multi-prototype embedding network model to jointly extract the\ncomposition of relational triples, namely, entity pairs and corresponding\nrelations. To be specific, we design a hybrid prototypical learning mechanism\nthat bridges text and knowledge concerning both entities and relations. Thus,\nimplicit correlations between entities and relations are injected.\nAdditionally, we propose a prototype-aware regularization to learn more\nrepresentative prototypes. Experimental results demonstrate that the proposed\nmethod can improve the performance of the few-shot triple extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZJUKLAB at SemEval-2021 Task 4: Negative Augmentation with Language Model for Reading Comprehension of Abstract Meaning. (arXiv:2102.12828v3 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2102.12828","description":"<p>This paper presents our systems for the three Subtasks of SemEval Task4:\nReading Comprehension of Abstract Meaning (ReCAM). We explain the algorithms\nused to learn our models and the process of tuning the algorithms and selecting\nthe best model. Inspired by the similarity of the ReCAM task and the language\npre-training, we propose a simple yet effective technology, namely, negative\naugmentation with language model. Evaluation results demonstrate the\neffectiveness of our proposed approach. Our models achieve the 4th rank on both\nofficial test sets of Subtask 1 and Subtask 2 with an accuracy of 87.9% and an\naccuracy of 92.8%, respectively. We further conduct comprehensive model\nanalysis and observe interesting error cases, which may promote future\nresearches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangnan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Normal vs. Adversarial: Salience-based Analysis of Adversarial Samples for Relation Extraction. (arXiv:2104.00312v4 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2104.00312","description":"<p>Recent neural-based relation extraction approaches, though achieving\npromising improvement on benchmark datasets, have reported their vulnerability\ntowards adversarial attacks. Thus far, efforts mostly focused on generating\nadversarial samples or defending adversarial attacks, but little is known about\nthe difference between normal and adversarial samples. In this work, we take\nthe first step to leverage the salience-based method to analyze those\nadversarial samples. We observe that salience tokens have a direct correlation\nwith adversarial perturbations. We further find the adversarial perturbations\nare either those tokens not existing in the training set or superficial cues\nassociated with relation labels. To some extent, our approach unveils the\ncharacters against adversarial samples. We release an open-source testbed,\n\"DiagnoseAdv\" in https://github.com/zjunlp/DiagnoseAdv.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled Contrastive Learning for Learning Robust Textual Representations. (arXiv:2104.04907v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2104.04907","description":"<p>Although the self-supervised pre-training of transformer models has resulted\nin the revolutionizing of natural language processing (NLP) applications and\nthe achievement of state-of-the-art results with regard to various benchmarks,\nthis process is still vulnerable to small and imperceptible permutations\noriginating from legitimate inputs. Intuitively, the representations should be\nsimilar in the feature space with subtle input permutations, while large\nvariations occur with different meanings. This motivates us to investigate the\nlearning of robust textual representation in a contrastive manner. However, it\nis non-trivial to obtain opposing semantic instances for textual samples. In\nthis study, we propose a disentangled contrastive learning method that\nseparately optimizes the uniformity and alignment of representations without\nnegative sampling. Specifically, we introduce the concept of momentum\nrepresentation consistency to align features and leverage power normalization\nwhile conforming the uniformity. Our experimental results for the NLP\nbenchmarks demonstrate that our approach can obtain better results compared\nwith the baselines, as well as achieve promising improvements with invariance\ntests and adversarial attacks. The code is available in\nhttps://github.com/zxlzr/DCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Document-level Relation Extraction as Semantic Segmentation. (arXiv:2106.03618v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2106.03618","description":"<p>Document-level relation extraction aims to extract relations among multiple\nentity pairs from a document. Previously proposed graph-based or\ntransformer-based models utilize the entities independently, regardless of\nglobal information among relational triples. This paper approaches the problem\nby predicting an entity-level relation matrix to capture local and global\ninformation, parallel to the semantic segmentation task in computer vision.\nHerein, we propose a Document U-shaped Network for document-level relation\nextraction. Specifically, we leverage an encoder module to capture the context\ninformation of entities and a U-shaped segmentation module over the image-style\nfeature map to capture global interdependency among triples. Experimental\nresults show that our approach can obtain state-of-the-art performance on three\nbenchmark datasets DocRED, CDR, and GDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners. (arXiv:2108.13161v7 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2108.13161","description":"<p>Large-scale pre-trained language models have contributed significantly to\nnatural language processing by demonstrating remarkable abilities as few-shot\nlearners. However, their effectiveness depends mainly on scaling the model\nparameters and prompt design, hindering their implementation in most real-world\napplications. This study proposes a novel pluggable, extensible, and efficient\napproach named DifferentiAble pRompT (DART), which can convert small language\nmodels into better few-shot learners without any prompt engineering. The main\nprinciple behind this approach involves reformulating potential natural\nlanguage processing tasks into the task of a pre-trained language model and\ndifferentially optimizing the prompt template as well as the target label with\nbackpropagation. Furthermore, the proposed approach can be: (i) Plugged to any\npre-trained language models; (ii) Extended to widespread classification tasks.\nA comprehensive evaluation of standard NLP tasks demonstrates that the proposed\napproach achieves a better few-shot performance. Code is available in\nhttps://github.com/zjunlp/DART.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LightNER: A Lightweight Tuning Paradigm for Low-resource NER via Pluggable Prompting. (arXiv:2109.00720v5 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2109.00720","description":"<p>Most NER methods rely on extensive labeled data for model training, which\nstruggles in the low-resource scenarios with limited training data. Existing\ndominant approaches usually suffer from the challenge that the target domain\nhas different label sets compared with a resource-rich source domain, which can\nbe concluded as class transfer and domain transfer. In this paper, we propose a\nlightweight tuning paradigm for low-resource NER via pluggable prompting\n(LightNER). Specifically, we construct the unified learnable verbalizer of\nentity categories to generate the entity span sequence and entity categories\nwithout any label-specific classifiers, thus addressing the class transfer\nissue. We further propose a pluggable guidance module by incorporating\nlearnable parameters into the self-attention layer as guidance, which can\nre-modulate the attention and adapt pre-trained weights. Note that we only tune\nthose inserted module with the whole parameter of the pre-trained language\nmodel fixed, thus, making our approach lightweight and flexible for\nlow-resource scenarios and can better transfer knowledge across domains.\nExperimental results show that LightNER can obtain comparable performance in\nthe standard supervised setting and outperform strong baselines in low-resource\nsettings. Code is in\nhttps://github.com/zjunlp/DeepKE/tree/main/example/ner/few-shot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Ask for Data-Efficient Event Argument Extraction. (arXiv:2110.00479v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2110.00479","description":"<p>Event argument extraction (EAE) is an important task for information\nextraction to discover specific argument roles. In this study, we cast EAE as a\nquestion-based cloze task and empirically analyze fixed discrete token template\nperformance. As generating human-annotated question templates is often\ntime-consuming and labor-intensive, we further propose a novel approach called\n\"Learning to Ask,\" which can learn optimized question templates for EAE without\nhuman annotations. Experiments using the ACE-2005 dataset demonstrate that our\nmethod based on optimized questions achieves state-of-the-art performance in\nboth the few-shot and supervised settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LOGEN: Few-shot Logical Knowledge-Conditioned Text Generation with Self-training. (arXiv:2112.01404v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2112.01404","description":"<p>Natural language generation from structured data mainly focuses on\nsurface-level descriptions, suffering from uncontrollable content selection and\nlow fidelity. Previous works leverage logical forms to facilitate logical\nknowledge-conditioned text generation. Though achieving remarkable progress,\nthey are data-hungry, which makes the adoption for real-world applications\nchallenging with limited data. To this end, this paper proposes a unified\nframework for logical knowledge-conditioned text generation in the few-shot\nsetting. With only a few seeds logical forms (e.g., 20/100 shot), our approach\nleverages self-training and samples pseudo logical forms based on content and\nstructure consistency. Experimental results demonstrate that our approach can\nobtain better few-shot performance than baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiacheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings. (arXiv:2201.05575v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2201.05575","description":"<p>Previous knowledge graph embedding approaches usually map entities to\nrepresentations and utilize score functions to predict the target entities, yet\nthey struggle to reason rare or emerging unseen entities. In this paper, we\npropose kNN-KGE, a new knowledge graph embedding approach with pre-trained\nlanguage models, by linearly interpolating its entity distribution with\nk-nearest neighbors. We compute the nearest neighbors based on the distance in\nthe entity embedding space from the knowledge store. Our approach can allow\nrare or emerging entities to be memorized explicitly rather than implicitly in\nmodel parameters. Experimental results demonstrate that our approach can\nimprove inductive and transductive link prediction results and yield better\nperformance for low-resource settings with only a few triples, which might be\neasier to reason via explicit memory. Code is available at\nhttps://github.com/zjunlp/KNN-KG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kformer: Knowledge Injection in Transformer Feed-Forward Layers. (arXiv:2201.05742v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2201.05742","description":"<p>Recent days have witnessed a diverse set of knowledge injection models for\npre-trained language models (PTMs); however, most previous studies neglect the\nPTMs' own ability with quantities of implicit knowledge stored in parameters. A\nrecent study has observed knowledge neurons in the Feed Forward Network (FFN),\nwhich are responsible for expressing factual knowledge. In this work, we\npropose a simple model, Kformer, which takes advantage of the knowledge stored\nin PTMs and external knowledge via knowledge injection in Transformer FFN\nlayers. Empirically results on two knowledge-intensive tasks, commonsense\nreasoning (i.e., SocialIQA) and medical question answering (i.e., MedQA-USMLE),\ndemonstrate that Kformer can yield better performance than other knowledge\ninjection technologies such as concatenation or attention-based injection. We\nthink the proposed simple model and empirical findings may be helpful for the\ncommunity to develop more powerful knowledge injection methods. Code available\nin https://github.com/zjunlp/Kformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptKG: A Prompt Learning Framework for Knowledge Graph Representation Learning and Application. (arXiv:2210.00305v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2210.00305","description":"<p>Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph\nstructure and text-rich entity/relation information. KG representation models\nshould consider graph structures and text semantics, but no comprehensive\nopen-sourced framework is mainly designed for KG regarding informative text\ndescription. In this paper, we present PromptKG, a prompt learning framework\nfor KG representation learning and application that equips the cutting-edge\ntext-based methods, integrates a new prompt learning model and supports various\ntasks (e.g., knowledge graph completion, question answering, recommendation,\nand knowledge probing). PromptKG is publicly open-sourced at\nhttps://github.com/zjunlp/PromptKG with long-term technical support.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Schema-aware Reference as Prompt Improves Data-Efficient Relational Triple and Event Extraction. (arXiv:2210.10709v3 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2210.10709","description":"<p>Information Extraction, which aims to extract structural relational triple or\nevent from unstructured texts, often suffers from data scarcity issues. With\nthe development of pre-trained language models, many prompt-based approaches to\ndata-efficient information extraction have been proposed and achieved\nimpressive performance. However, existing prompt learning methods for\ninformation extraction are still susceptible to several potential limitations:\n(i) semantic gap between natural language and output structure knowledge with\npre-defined schema; (ii) representation learning with locally individual\ninstances limits the performance given the insufficient features. In this\npaper, we propose a novel approach of schema-aware Reference As Prompt (RAP),\nwhich dynamically leverage schema and knowledge inherited from global\n(few-shot) training data for each sample. Specifically, we propose a\nschema-aware reference store, which unifies symbolic schema and relevant\ntextual instances. Then, we employ a dynamic reference integration module to\nretrieve pertinent knowledge from the datastore as prompts during training and\ninference. Experimental results demonstrate that RAP can be plugged into\nvarious existing models and outperforms baselines in low-resource settings on\nfour datasets of relational triple extraction and event extraction. In\naddition, we provide comprehensive empirical ablations and case analysis\nregarding different types and scales of knowledge in order to better understand\nthe mechanisms of RAP. Code is available in https://github.com/zjunlp/RAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shengyu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-01-25T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}