{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-10-18T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"TestAug: A Framework for Augmenting Capability-based NLP Tests. (arXiv:2210.08097v1 [cs.SE])","link":"http://arxiv.org/abs/2210.08097","description":"<p>The recently proposed capability-based NLP testing allows model developers to\ntest the functional capabilities of NLP models, revealing functional failures\nthat cannot be detected by the traditional heldout mechanism. However, existing\nwork on capability-based testing requires extensive manual efforts and domain\nexpertise in creating the test cases. In this paper, we investigate a low-cost\napproach for the test case generation by leveraging the GPT-3 engine. We\nfurther propose to use a classifier to remove the invalid outputs from GPT-3\nand expand the outputs into templates to generate more test cases. Our\nexperiments show that TestAug has three advantages over the existing work on\nbehavioral testing: (1) TestAug can find more bugs than existing work; (2) The\ntest cases in TestAug are more diverse; and (3) TestAug largely saves the\nmanual efforts in creating the test suites. The code and data for TestAug can\nbe found at our project website (https://guanqun-yang.github.io/testaug/) and\nGitHub (https://github.com/guanqun-yang/testaug).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guanqun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haque_M/0/1/0/all/0/1\">Mirazul Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qiaochu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xueqing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TweetNERD -- End to End Entity Linking Benchmark for Tweets. (arXiv:2210.08129v1 [cs.CL])","link":"http://arxiv.org/abs/2210.08129","description":"<p>Named Entity Recognition and Disambiguation (NERD) systems are foundational\nfor information retrieval, question answering, event detection, and other\nnatural language processing (NLP) applications. We introduce TweetNERD, a\ndataset of 340K+ Tweets across 2010-2021, for benchmarking NERD systems on\nTweets. This is the largest and most temporally diverse open sourced dataset\nbenchmark for NERD on Tweets and can be used to facilitate research in this\narea. We describe evaluation setup with TweetNERD for three NERD tasks: Named\nEntity Recognition (NER), Entity Linking with True Spans (EL), and End to End\nEntity Linking (End2End); and provide performance of existing publicly\navailable methods on specific TweetNERD splits. TweetNERD is available at:\nhttps://doi.org/10.5281/zenodo.6617192 under Creative Commons Attribution 4.0\nInternational (CC BY 4.0) license. Check out more details at\nhttps://github.com/twitter-research/TweetNERD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shubhanshu Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saini_A/0/1/0/all/0/1\">Aman Saini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makki_R/0/1/0/all/0/1\">Raheleh Makki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Sneha Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haghighi_A/0/1/0/all/0/1\">Aria Haghighi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mollahosseini_A/0/1/0/all/0/1\">Ali Mollahosseini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Repetition in Abstractive Neural Summarizers. (arXiv:2210.08145v1 [cs.CL])","link":"http://arxiv.org/abs/2210.08145","description":"<p>We provide a quantitative and qualitative analysis of self-repetition in the\noutput of neural summarizers. We measure self-repetition as the number of\nn-grams of length four or longer that appear in multiple outputs of the same\nsystem. We analyze the behavior of three popular architectures (BART, T5, and\nPegasus), fine-tuned on five datasets. In a regression analysis, we find that\nthe three architectures have different propensities for repeating content\nacross output summaries for inputs, with BART being particularly prone to\nself-repetition. Fine-tuning on more abstractive data, and on data featuring\nformulaic language, is associated with a higher rate of self-repetition. In\nqualitative analysis we find systems produce artefacts such as ads and\ndisclaimers unrelated to the content being summarized, as well as formulaic\nphrases common in the fine-tuning domain. Our approach to corpus-level analysis\nof self-repetition may help practitioners clean up training data for\nsummarizers and ultimately support methods for minimizing the amount of\nself-repetition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salkar_N/0/1/0/all/0/1\">Nikita Salkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trikalinos_T/0/1/0/all/0/1\">Thomas Trikalinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenkova_A/0/1/0/all/0/1\">Ani Nenkova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Synthetic Speech from SpokenVocab for Speech Translation. (arXiv:2210.08174v1 [cs.CL])","link":"http://arxiv.org/abs/2210.08174","description":"<p>Training end-to-end speech translation (ST) systems requires sufficiently\nlarge-scale data, which is unavailable for most language pairs and domains. One\npractical solution to the data scarcity issue is to convert machine translation\ndata (MT) to ST data via text-to-speech (TTS) systems. Yet, using TTS systems\ncan be tedious and slow, as the conversion needs to be done for each MT\ndataset. In this work, we propose a simple, scalable and effective data\naugmentation technique, i.e., SpokenVocab, to convert MT data to ST data\non-the-fly. The idea is to retrieve and stitch audio snippets from a\nSpokenVocab bank according to words in an MT sequence. Our experiments on\nmultiple language pairs from Must-C show that this method outperforms strong\nbaselines by an average of 1.83 BLEU scores, and it performs equally well as\nTTS-generated speech. We also showcase how SpokenVocab can be applied in\ncode-switching ST for which often no TTS systems exit. Our code is available at\nhttps://github.com/mingzi151/SpokenVocab\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffar_G/0/1/0/all/0/1\">Gholamreza Haffar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shareghi_E/0/1/0/all/0/1\">Ehsan Shareghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Invariant Representation and Risk Minimized for Unsupervised Accent Domain Adaptation. (arXiv:2210.08182v1 [cs.SD])","link":"http://arxiv.org/abs/2210.08182","description":"<p>Unsupervised representation learning for speech audios attained impressive\nperformances for speech recognition tasks, particularly when annotated speech\nis limited. However, the unsupervised paradigm needs to be carefully designed\nand little is known about what properties these representations acquire. There\nis no guarantee that the model learns meaningful representations for valuable\ninformation for recognition. Moreover, the adaptation ability of the learned\nrepresentations to other domains still needs to be estimated. In this work, we\nexplore learning domain-invariant representations via a direct mapping of\nspeech representations to their corresponding high-level linguistic\ninformations. Results prove that the learned latents not only capture the\narticulatory feature of each phoneme but also enhance the adaptation ability,\noutperforming the baseline largely on accented benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chendong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoyang Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Word Meaning Disambiguation using TimeLMs. (arXiv:2210.08207v1 [cs.CL])","link":"http://arxiv.org/abs/2210.08207","description":"<p>Meaning of words constantly changes given the events in modern civilization.\nLarge Language Models use word embeddings, which are often static and thus\ncannot cope with this semantic change. Thus,it is important to resolve\nambiguity in word meanings. This paper is an effort in this direction, where we\nexplore methods for word sense disambiguation for the EvoNLP shared task. We\nconduct rigorous ablations for two solutions to this problem. We see that an\napproach using time-aware language models helps this task. Furthermore, we\nexplore possible future directions to this problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Godbole_M/0/1/0/all/0/1\">Mihir Godbole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandavate_P/0/1/0/all/0/1\">Parth Dandavate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kane_A/0/1/0/all/0/1\">Aditya Kane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models for Multi-label Propaganda Detection. (arXiv:2210.08209v1 [cs.CL])","link":"http://arxiv.org/abs/2210.08209","description":"<p>The spread of propaganda through the internet has increased drastically over\nthe past years. Lately, propaganda detection has started gaining importance\nbecause of the negative impact it has on society. In this work, we describe our\napproach for the WANLP 2022 shared task which handles the task of propaganda\ndetection in a multi-label setting. The task demands the model to label the\ngiven text as having one or more types of propaganda techniques. There are a\ntotal of 22 propaganda techniques to be detected. We show that an ensemble of\nfive models performs the best on the task, scoring a micro-F1 score of 59.73%.\nWe also conduct comprehensive ablations and propose various future directions\nfor this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chavan_T/0/1/0/all/0/1\">Tanmay Chavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kane_A/0/1/0/all/0/1\">Aditya Kane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Few-Shot Relation Extraction Pipeline Based on Adaptive Prototype Fusion. (arXiv:2210.08242v1 [cs.CL])","link":"http://arxiv.org/abs/2210.08242","description":"<p>Few-shot relation extraction (FSRE) aims at recognizing unseen relations by\nlearning with merely a handful of annotated instances. To more effectively\ngeneralize to new relations, this paper proposes a novel pipeline for the FSRE\ntask based on adaptive prototype fusion. Specifically, for each relation class,\nthe pipeline fully explores the relation information by concatenating two types\nof embedding, and then elaborately combine the relation representation with the\nadaptive prototype fusion mechanism. The whole framework can be effectively and\nefficiently optimized in an end-to-end fashion. Experiments on the benchmark\ndataset FewRel 1.0 show a significant improvement of our method against\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuzhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cen_M/0/1/0/all/0/1\">Min Cen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongzhou Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniRPG: Unified Discrete Reasoning over Table and Text as Program Generation. (arXiv:2210.08249v1 [cs.CL])","link":"http://arxiv.org/abs/2210.08249","description":"<p>Question answering requiring discrete reasoning, e.g., arithmetic computing,\ncomparison, and counting, over knowledge is a challenging task. In this paper,\nwe propose UniRPG, a semantic-parsing-based approach advanced in\ninterpretability and scalability, to perform unified discrete reasoning over\nheterogeneous knowledge resources, i.e., table and text, as program generation.\nConcretely, UniRPG consists of a neural programmer and a symbolic program\nexecutor, where a program is the composition of a set of pre-defined general\natomic and higher-order operations and arguments extracted from table and text.\nFirst, the programmer parses a question into a program by generating operations\nand copying arguments, and then the executor derives answers from table and\ntext based on the program. To alleviate the costly program annotation issue, we\ndesign a distant supervision approach for programmer learning, where pseudo\nprograms are automatically constructed without annotated derivations. Extensive\nexperiments on the TAT-QA dataset show that UniRPG achieves tremendous\nimprovements and enhances interpretability and scalability compared with\nstate-of-the-art methods, even without derivation annotation. Moreover, it\nachieves promising performance on the textual dataset DROP without derivations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yongwei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chaoqun Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AraLegal-BERT: A pretrained language model for Arabic Legal text. (arXiv:2210.08284v1 [cs.CL])","link":"http://arxiv.org/abs/2210.08284","description":"<p>The effectiveness of the BERT model on multiple linguistic tasks has been\nwell documented. On the other hand, its potentials for narrow and specific\ndomains such as Legal, have not been fully explored. In this paper, we examine\nhow BERT can be used in the Arabic legal domain and try customizing this\nlanguage model for several downstream tasks using several different\ndomain-relevant training and testing datasets to train BERT from scratch. We\nintroduce the AraLegal-BERT, a bidirectional encoder Transformer-based model\nthat have been thoroughly tested and carefully optimized with the goal to\namplify the impact of NLP-driven solution concerning jurisprudence, legal\ndocuments, and legal practice. We fine-tuned AraLegal-BERT and evaluated it\nagainst three BERT variations for Arabic language in three natural languages\nunderstanding (NLU) tasks. The results show that the base version of\nAraLegal-BERT achieve better accuracy than the general and original BERT over\nthe Legal text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+AL_Qurishi_M/0/1/0/all/0/1\">Muhammad AL-Qurishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AlQaseemi_S/0/1/0/all/0/1\">Sarah AlQaseemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soussi_R/0/1/0/all/0/1\">Riad Soussi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Radiology Summarization with Radiograph and Anatomy Prompts. (arXiv:2210.08303v1 [cs.CV])","link":"http://arxiv.org/abs/2210.08303","description":"<p>The impression is crucial for the referring physicians to grasp key\ninformation since it is concluded from the findings and reasoning of\nradiologists. To alleviate the workload of radiologists and reduce repetitive\nhuman labor in impression writing, many researchers have focused on automatic\nimpression generation. However, recent works on this task mainly summarize the\ncorresponding findings and pay less attention to the radiology images. In\nclinical, radiographs can provide more detailed valuable observations to\nenhance radiologists' impression writing, especially for complicated cases.\nBesides, each sentence in findings usually focuses on single anatomy, so they\nonly need to be matched to corresponding anatomical regions instead of the\nwhole image, which is beneficial for textual and visual features alignment.\nTherefore, we propose a novel anatomy-enhanced multimodal model to promote\nimpression generation. In detail, we first construct a set of rules to extract\nanatomies and put these prompts into each sentence to highlight anatomy\ncharacteristics. Then, two separate encoders are applied to extract features\nfrom the radiograph and findings. Afterward, we utilize a contrastive learning\nmodule to align these two representations at the overall level and use a\nco-attention to fuse them at the sentence level with the help of\nanatomy-enhanced sentence representation. Finally, the decoder takes the fused\ninformation as the input to generate impressions. The experimental results on\ntwo benchmark datasets confirm the effectiveness of the proposed method, which\nachieves state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinpeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhihong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tsung-Hui Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Construction Repetition Reduces Information Rate in Dialogue. (arXiv:2210.08321v1 [cs.CL])","link":"http://arxiv.org/abs/2210.08321","description":"<p>Speakers repeat constructions frequently in dialogue. Due to their peculiar\ninformation-theoretic properties, repetitions can be thought of as a strategy\nfor cost-effective communication. In this study, we focus on the repetition of\nlexicalised constructions -- i.e., recurring multi-word units -- in English\nopen-domain spoken dialogues. We hypothesise that speakers use construction\nrepetition to mitigate information rate, leading to an overall decrease in\nutterance information content over the course of a dialogue. We conduct a\nquantitative analysis, measuring the information content of constructions and\nthat of their containing utterances, estimating information content with an\nadaptive neural language model. We observe that construction usage lowers the\ninformation content of utterances. This facilitating effect (i) increases\nthroughout dialogues, (ii) is boosted by repetition, (iii) grows as a function\nof repetition frequency and density, and (iv) is stronger for repetitions of\nreferential constructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giulianelli_M/0/1/0/all/0/1\">Mario Giulianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinclair_A/0/1/0/all/0/1\">Arabella Sinclair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_R/0/1/0/all/0/1\">Raquel Fern&#xe1;ndez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combination Of Convolution Neural Networks And Deep Neural Networks For Fake News Detection. (arXiv:2210.08331v1 [cs.IR])","link":"http://arxiv.org/abs/2210.08331","description":"<p>Nowadays, People prefer to follow the latest news on social media, as it is\ncheap, easily accessible, and quickly disseminated. However, it can spread fake\nor unreliable, low-quality news that intentionally contains false information.\nThe spread of fake news can have a negative effect on people and society. Given\nthe seriousness of such a problem, researchers did their best to identify\npatterns and characteristics that fake news may exhibit to design a system that\ncan detect fake news before publishing. In this paper, we have described the\nFake News Challenge stage #1 (FNC-1) dataset and given an overview of the\ncompetitive attempts to build a fake news detection system using the FNC-1\ndataset. The proposed model was evaluated with the FNC-1 dataset. A competitive\ndataset is considered an open problem and a challenge worldwide. This system's\nprocedure implies processing the text in the headline and body text columns\nwith different natural language processing techniques. After that, the\nextracted features are reduced using the elbow truncated method, finding the\nsimilarity between each pair using the soft cosine similarity method. The new\nfeature is entered into CNN and DNN deep learning approaches. The proposed\nsystem detects all the categories with high accuracy except the disagree\ncategory. As a result, the system achieves up to 84.6 % accuracy, classifying\nit as the second ranking based on other competitive studies regarding this\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jawad_Z/0/1/0/all/0/1\">Zainab A. Jawad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obaid_A/0/1/0/all/0/1\">Ahmed J. Obaid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Affect Intensities. (arXiv:1704.08798v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1704.08798","description":"<p>Words often convey affect -- emotions, feelings, and attitudes. Further,\ndifferent words can convey affect to various degrees (intensities). However,\nexisting manually created lexicons for basic emotions (such as anger and fear)\nindicate only coarse categories of affect association (for example, associated\nwith anger or not associated with anger). Automatic lexicons of affect provide\nfine degrees of association, but they tend not to be accurate as human-created\nlexicons. Here, for the first time, we present a manually created affect\nintensity lexicon with real-valued scores of intensity for four basic emotions:\nanger, fear, joy, and sadness. (We will subsequently add entries for more\nemotions such as disgust, anticipation, trust, and surprise.) We refer to this\ndataset as the NRC Affect Intensity Lexicon, or AIL for short. AIL has entries\nfor close to 6,000 English words. We used a technique called best-worst scaling\n(BWS) to create the lexicon. BWS improves annotation consistency and obtains\nreliable fine-grained scores (split-half reliability &gt; 0.91). We also compare\nthe entries in AIL with the entries in the NRC VAD Lexicon, which has valence,\narousal, and dominance (VAD) scores for 20K English words. We find that anger,\nfear, and sadness words, on average, have very similar VAD scores. However,\nsadness words tend to have slightly lower dominance scores than fear and anger\nwords. The Affect Intensity Lexicon has applications in automatic emotion\nanalysis in a number of domains such as commerce, education, intelligence, and\npublic health. AIL is also useful in the building of natural language\ngeneration systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InsNet: An Efficient, Flexible, and Performant Insertion-based Text Generation Model. (arXiv:2102.11008v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.11008","description":"<p>We propose InsNet, an expressive insertion-based text generator with\nefficient training and flexible decoding (parallel or sequential). Unlike most\nexisting insertion-based text generation works that require re-encoding of the\ncontext after each insertion operation and thus are inefficient to train,\nInsNet only requires one pass of context encoding for the entire sequence\nduring training by introducing a novel insertion-oriented position encoding and\na light-weighted slot representation strategy to enable computation sharing.\nFurthermore, we propose an algorithm InsNet-Dinic to better determine the\nparallelization of insertion operations that provides a controllable switch\nbetween parallel and sequential decoding, making it flexible to handle more\nparallelizable tasks such as machine translation with efficient decoding, or\nless parallelizable tasks such as open-domain text generation to guarantee\nhigh-quality outputs. Experiments on two lexically constrained text generation\ndatasets and three machine translation datasets demonstrate InsNet's advantages\nover previous insertion-based methods in terms of training speed, inference\nefficiency, and generation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Sidi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_T/0/1/0/all/0/1\">Tao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Picard understanding Darmok: A Dataset and Model for Metaphor-Rich Translation in a Constructed Language. (arXiv:2107.08146v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.08146","description":"<p>Tamarian, a fictional language introduced in the Star Trek episode Darmok,\ncommunicates meaning through utterances of metaphorical references, such as\n\"Darmok and Jalad at Tanagra\" instead of \"We should work together.\" This work\nassembles a Tamarian-English dictionary of utterances from the original episode\nand several follow-on novels, and uses this to construct a parallel corpus of\n456 English-Tamarian utterances. A machine translation system based on a large\nlanguage model (T5) is trained using this parallel corpus, and is shown to\nproduce an accuracy of 76% when translating from English to Tamarian on known\nutterances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jansen_P/0/1/0/all/0/1\">Peter Jansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1\">Jordan Boyd-Graber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTraffic: BERT-based Joint Speaker Role and Speaker Change Detection for Air Traffic Control Communications. (arXiv:2110.05781v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.05781","description":"<p>Automatic speech recognition (ASR) allows transcribing the communications\nbetween air traffic controllers (ATCOs) and aircraft pilots. The transcriptions\nare used later to extract ATC named entities, e.g., aircraft callsigns. One\ncommon challenge is speech activity detection (SAD) and speaker diarization\n(SD). In the failure condition, two or more segments remain in the same\nrecording, jeopardizing the overall performance. We propose a system that\ncombines SAD and a BERT model to perform speaker change detection and speaker\nrole detection (SRD) by chunking ASR transcripts, i.e., SD with a defined\nnumber of speakers together with SRD. The proposed model is evaluated on\nreal-life public ATC databases. Our BERT SD model baseline reaches up to 10%\nand 20% token-based Jaccard error rate (JER) in public and private ATC\ndatabases. We also achieved relative improvements of 32% and 7.7% in JERs and\nSD error rate (DER), respectively, compared to VBx, a well-known SD system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarfjoo_S/0/1/0/all/0/1\">Seyyed Saeed Sarfjoo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasad_A/0/1/0/all/0/1\">Amrutha Prasad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nigmatulina_I/0/1/0/all/0/1\">Iuliia Nigmatulina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ondrej_K/0/1/0/all/0/1\">Karel Ondrej</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ohneiser_O/0/1/0/all/0/1\">Oliver Ohneiser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Helmke_H/0/1/0/all/0/1\">Hartmut Helmke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical Benefits of Feature Feedback Under Distribution Shift. (arXiv:2110.07566v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07566","description":"<p>In attempts to develop sample-efficient and interpretable algorithms,\nresearcher have explored myriad mechanisms for collecting and exploiting\nfeature feedback (or rationales) auxiliary annotations provided for training\n(but not test) instances that highlight salient evidence. Examples include\nbounding boxes around objects and salient spans in text. Despite its intuitive\nappeal, feature feedback has not delivered significant gains in practical\nproblems as assessed on iid holdout sets. However, recent works on\ncounterfactually augmented data suggest an alternative benefit of supplemental\nannotations, beyond interpretability: lessening sensitivity to spurious\npatterns and consequently delivering gains in out-of-domain evaluations. We\nspeculate that while existing methods for incorporating feature feedback have\ndelivered negligible in-sample performance gains, they may nevertheless provide\nout-of-domain benefits. Our experiments addressing sentiment analysis, show\nthat feature feedback methods perform significantly better on various natural\nout-of-domain datasets despite comparable in-domain evaluations. By contrast,\nperformance on natural language inference remains comparable. Finally, we\ncompare those tasks where feature feedback does (and does not) help.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katakkar_A/0/1/0/all/0/1\">Anurag Katakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1\">Clay H. Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaushik_D/0/1/0/all/0/1\">Divyansh Kaushik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discourse Comprehension: A Question Answering Framework to Represent Sentence Connections. (arXiv:2111.00701v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.00701","description":"<p>While there has been substantial progress in text comprehension through\nsimple factoid question answering, more holistic comprehension of a discourse\nstill presents a major challenge (Dunietz et al., 2020). Someone critically\nreflecting on a text as they read it will pose curiosity-driven, often\nopen-ended questions, which reflect deep understanding of the content and\nrequire complex reasoning to answer (Ko et al., 2020; Westera et al., 2020). A\nkey challenge in building and evaluating models for this type of discourse\ncomprehension is the lack of annotated data, especially since collecting\nanswers to such questions requires high cognitive load for annotators. This\npaper presents a novel paradigm that enables scalable data collection targeting\nthe comprehension of news documents, viewing these questions through the lens\nof discourse. The resulting corpus, DCQA (Discourse Comprehension by Question\nAnswering), captures both discourse and semantic links between sentences in the\nform of free-form, open-ended questions. On an evaluation set that we annotated\non questions from Ko et al. (2020), we show that DCQA provides valuable\nsupervision for answering open-ended questions. We additionally design\npre-training methods utilizing existing question-answering resources, and use\nsynthetic data to accommodate unanswerable questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ko_W/0/1/0/all/0/1\">Wei-Jen Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalton_C/0/1/0/all/0/1\">Cutter Dalton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simmons_M/0/1/0/all/0/1\">Mark Simmons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_E/0/1/0/all/0/1\">Eliza Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples. (arXiv:2111.10962v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.10962","description":"<p>Knowledge-enhanced language representation learning has shown promising\nresults across various knowledge-intensive NLP tasks. However, prior methods\nare limited in efficient utilization of multilingual knowledge graph (KG) data\nfor language model (LM) pretraining. They often train LMs with KGs in indirect\nways, relying on extra entity/relation embeddings to facilitate knowledge\ninjection. In this work, we explore methods to make better use of the\nmultilingual annotation and language agnostic property of KG triples, and\npresent novel knowledge based multilingual language models (KMLMs) trained\ndirectly on the knowledge triples. We first generate a large amount of\nmultilingual synthetic sentences using the Wikidata KG triples. Then based on\nthe intra- and inter-sentence structures of the generated data, we design\npretraining tasks to enable the LMs to not only memorize the factual knowledge\nbut also learn useful logical patterns. Our pretrained KMLMs demonstrate\nsignificant performance improvements on a wide range of knowledge-intensive\ncross-lingual tasks, including named entity recognition (NER), factual\nknowledge retrieval, relation classification, and a newly designed logical\nreasoning task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruidan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DKPLM: Decomposable Knowledge-enhanced Pre-trained Language Model for Natural Language Understanding. (arXiv:2112.01047v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.01047","description":"<p>Knowledge-Enhanced Pre-trained Language Models (KEPLMs) are pre-trained\nmodels with relation triples injecting from knowledge graphs to improve\nlanguage understanding abilities. To guarantee effective knowledge injection,\nprevious studies integrate models with knowledge encoders for representing\nknowledge retrieved from knowledge graphs. The operations for knowledge\nretrieval and encoding bring significant computational burdens, restricting the\nusage of such models in real-world applications that require high inference\nspeed. In this paper, we propose a novel KEPLM named DKPLM that Decomposes\nKnowledge injection process of the Pre-trained Language Models in pre-training,\nfine-tuning and inference stages, which facilitates the applications of KEPLMs\nin real-world scenarios. Specifically, we first detect knowledge-aware\nlong-tail entities as the target for knowledge injection, enhancing the KEPLMs'\nsemantic understanding abilities and avoiding injecting redundant information.\nThe embeddings of long-tail entities are replaced by \"pseudo token\nrepresentations\" formed by relevant knowledge triples. We further design the\nrelational knowledge decoding task for pre-training to force the models to\ntruly understand the injected knowledge by relation triple reconstruction.\nExperiments show that our model outperforms other KEPLMs significantly over\nzero-shot knowledge probing tasks and multiple knowledge-aware language\nunderstanding tasks. We further show that DKPLM has a higher inference speed\nthan other competing models due to the decomposing mechanism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Taolin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_N/0/1/0/all/0/1\">Nan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1\">Minghui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chengguang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Context-Word Biases in Lexical Semantic Datasets. (arXiv:2112.06733v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06733","description":"<p>State-of-the-art pretrained contextualized models (PCM) eg. BERT use tasks\nsuch as WiC and WSD to evaluate their word-in-context representations. This\ninherently assumes that performance in these tasks reflect how well a model\nrepresents the coupled word and context semantics. We question this assumption\nby presenting the first quantitative analysis on the context-word interaction\nbeing tested in major contextual lexical semantic tasks. To achieve this, we\nrun probing baselines on masked input, and propose measures to calculate and\nvisualize the degree of context or word biases in existing datasets. The\nanalysis was performed on both models and humans. Our findings demonstrate that\nmodels are usually not being tested for word-in-context semantics in the same\nway as humans are in these tasks, which helps us better understand the\nmodel-human gap. Specifically, to PCMs, most existing datasets fall into the\nextreme ends (the retrieval-based tasks exhibit strong target word bias while\nWiC-style tasks and WSD show strong context bias); In comparison, humans are\nless biased and achieve much better performance when both word and context are\navailable than with masked input. We recommend our framework for understanding\nand controlling these biases for model interpretation and future task design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qianchu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCarthy_D/0/1/0/all/0/1\">Diana McCarthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilled Dual-Encoder Model for Vision-Language Understanding. (arXiv:2112.08723v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08723","description":"<p>We propose a cross-modal attention distillation framework to train a\ndual-encoder model for vision-language understanding tasks, such as visual\nreasoning and visual question answering. Dual-encoder models have a faster\ninference speed than fusion-encoder models and enable the pre-computation of\nimages and text during inference. However, the shallow interaction module used\nin dual-encoder models is insufficient to handle complex vision-language\nunderstanding tasks. In order to learn deep interactions of images and text, we\nintroduce cross-modal attention distillation, which uses the image-to-text and\ntext-to-image attention distributions of a fusion-encoder model to guide the\ntraining of our dual-encoder model. In addition, we show that applying the\ncross-modal attention distillation for both pre-training and fine-tuning stages\nachieves further improvements. Experimental results demonstrate that the\ndistilled dual-encoder model achieves competitive performance for visual\nreasoning, visual entailment and visual question answering tasks while enjoying\na much faster inference speed than fusion-encoder models. Our code and models\nwill be publicly available at https://github.com/kugwzk/Distilled-DualEncoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zekun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Haichao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings. (arXiv:2201.05575v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05575","description":"<p>Previous knowledge graph embedding approaches usually map entities to\nrepresentations and utilize score functions to predict the target entities, yet\nthey struggle to reason rare or emerging unseen entities. In this paper, we\npropose kNN-KGE, a new knowledge graph embedding approach with pre-trained\nlanguage models, by linearly interpolating its entity distribution with\nk-nearest neighbors. We compute the nearest neighbors based on the distance in\nthe entity embedding space from the knowledge store. Our approach can allow\nrare or emerging entities to be memorized explicitly rather than implicitly in\nmodel parameters. Experimental results demonstrate that our approach can\nimprove inductive and transductive link prediction results and yield better\nperformance for low-resource settings with only a few triples, which might be\neasier to reason via explicit memory. Code is available at\nhttps://github.com/zjunlp/KNN-KG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CsFEVER and CTKFacts: Acquiring Czech data for fact verification. (arXiv:2201.11115v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11115","description":"<p>In this paper, we examine several methods of acquiring Czech data for\nautomated fact-checking, which is a task commonly modeled as a classification\nof textual claim veracity w.r.t. a corpus of trusted ground truths. We attempt\nto collect sets of data in form of a factual claim, evidence within the ground\ntruth corpus, and its veracity label (supported, refuted or not enough info).\nAs a first attempt, we generate a Czech version of the large-scale FEVER\ndataset built on top of Wikipedia corpus. We take a hybrid approach of machine\ntranslation and document alignment; the approach and the tools we provide can\nbe easily applied to other languages. We discuss its weaknesses and\ninaccuracies, propose a future approach for their cleaning and publish the 127k\nresulting translations, as well as a version of such dataset reliably\napplicable for the Natural Language Inference task - the CsFEVER-NLI.\nFurthermore, we collect a novel dataset of 3,097 claims, which is annotated\nusing the corpus of 2.2M articles of Czech News Agency. We present its extended\nannotation methodology based on the FEVER approach, and, as the underlying\ncorpus is kept a trade secret, we also publish a standalone version of the\ndataset for the task of Natural Language Inference we call CTKFactsNLI. We\nanalyze both acquired datasets for spurious cues - annotation patterns leading\nto model overfitting. CTKFacts is further examined for inter-annotator\nagreement, thoroughly cleaned, and a typology of common annotator errors is\nextracted. Finally, we provide baseline models for all stages of the\nfact-checking pipeline and publish the NLI datasets, as well as our annotation\nplatform and other experimental data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ullrich_H/0/1/0/all/0/1\">Herbert Ullrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drchal_J/0/1/0/all/0/1\">Jan Drchal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rypar_M/0/1/0/all/0/1\">Martin R&#xfd;par</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincourova_H/0/1/0/all/0/1\">Hana Vincourov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moravec_V/0/1/0/all/0/1\">V&#xe1;clav Moravec</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"POTATO: exPlainable infOrmation exTrAcTion framewOrk. (arXiv:2201.13230v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.13230","description":"<p>We present POTATO, a task- and languageindependent framework for\nhuman-in-the-loop (HITL) learning of rule-based text classifiers using\ngraph-based features. POTATO handles any type of directed graph and supports\nparsing text into Abstract Meaning Representations (AMR), Universal\nDependencies (UD), and 4lang semantic graphs. A streamlit-based user interface\nallows users to build rule systems from graph patterns, provides real-time\nevaluation based on ground truth data, and suggests rules by ranking graph\nfeatures using interpretable machine learning models. Users can also provide\npatterns over graphs using regular expressions, and POTATO can recommend\nrefinements of such rules. POTATO is applied in projects across domains and\nlanguages, including classification tasks on German legal text and English\nsocial media data. All components of our system are written in Python, can be\ninstalled via pip, and are released under an MIT License on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kovacs_A/0/1/0/all/0/1\">&#xc1;d&#xe1;m Kov&#xe1;cs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemes_K/0/1/0/all/0/1\">Kinga G&#xe9;mes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iklodi_E/0/1/0/all/0/1\">Eszter Ikl&#xf3;di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recski_G/0/1/0/all/0/1\">G&#xe1;bor Recski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Locally Typical Sampling. (arXiv:2202.00666v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00666","description":"<p>Today's probabilistic language generators fall short when it comes to\nproducing coherent and fluent text despite the fact that the underlying models\nperform well under standard metrics, e.g., perplexity. This discrepancy has\npuzzled the language generation community for the last few years. In this work,\nwe posit that the abstraction of natural language generation as a discrete\nstochastic process--which allows for an information-theoretic analysis--can\nprovide new insights into the behavior of probabilistic language generators,\ne.g., why high-probability texts can be dull or repetitive. Humans use language\nas a means of communicating information, aiming to do so in a simultaneously\nefficient and error-minimizing manner; in fact, psycholinguistics research\nsuggests humans choose each word in a string with this subconscious goal in\nmind. We formally define the set of strings that meet this criterion: those for\nwhich each word has an information content close to the expected information\ncontent, i.e., the conditional entropy of our model. We then propose a simple\nand efficient procedure for enforcing this criterion when generating from\nprobabilistic models, which we call locally typical sampling. Automatic and\nhuman evaluations show that, in comparison to nucleus and top-k sampling,\nlocally typical sampling offers competitive performance (in both abstractive\nsummarization and story generation) in terms of quality while consistently\nreducing degenerate repetitions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiher_G/0/1/0/all/0/1\">Gian Wiher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClidSum: A Benchmark Dataset for Cross-Lingual Dialogue Summarization. (arXiv:2202.05599v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.05599","description":"<p>We present ClidSum, a benchmark dataset for building cross-lingual\nsummarization systems on dialogue documents. It consists of 67k+ dialogue\ndocuments from two subsets (i.e., SAMSum and MediaSum) and 112k+ annotated\nsummaries in different target languages. Based on the proposed ClidSum, we\nintroduce two benchmark settings for supervised and semi-supervised scenarios,\nrespectively. We then build various baseline systems in different paradigms\n(pipeline and end-to-end) and conduct extensive experiments on ClidSum to\nprovide deeper analyses. Furthermore, we propose mDialBART which extends\nmBART-50 (a multi-lingual BART) via further pre-training. The multiple\nobjectives used in the further pre-training stage help the pre-trained model\ncapture the structural characteristics as well as important content in\ndialogues and the transformation from source to the target language.\nExperimental results show the superiority of mDialBART, as an end-to-end model,\noutperforms strong pipeline models on ClidSum. Finally, we discuss specific\nchallenges that current approaches faced with this task and give multiple\npromising directions for future research. We have released the dataset and code\nat https://github.com/krystalan/ClidSum.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Ziyao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1\">Duo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1\">Jianfeng Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image Captioning. (arXiv:2202.06574v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.06574","description":"<p>Image Captioning is a popular vision-and-language task to generate the\nlanguage description of an image. Recent advances focus on scaling up the model\nsize and the number of training data, significantly increasing the cost of\ntraining. As an alternative to these heavy-cost models, we introduce I-Tuning,\na lightweight image captioning framework, which contains only a small number of\ntrainable parameters. The novel I-Tuning cross-attention module connects the\nnon-trainable pre-trained language decoder GPT2 and vision encoder CLIP-ViT.\nSince most parameters are not updated during training, our framework is\nlightweight and fast. Experimental results on three image captioning benchmarks\nreveal that our framework achieves comparable or better performance than the\nlarge-scale baseline systems. At the same time, our models require up to 10\ntimes fewer trainable parameters and much fewer training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Ziyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1\">Yadong Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongsheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Cross-lingual Transfer of Prompt-based Tuning with a Unified Multilingual Prompt. (arXiv:2202.11451v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.11451","description":"<p>Prompt-based tuning has been proven effective for pretrained language models\n(PLMs). While most of the existing work focuses on the monolingual prompts, we\nstudy the multilingual prompts for multilingual PLMs, especially in the\nzero-shot cross-lingual setting. To alleviate the effort of designing different\nprompts for multiple languages, we propose a novel model that uses a unified\nprompt for all languages, called UniPrompt. Different from the discrete prompts\nand soft prompts, the unified prompt is model-based and language-agnostic.\nSpecifically, the unified prompt is initialized by a multilingual PLM to\nproduce language-independent representation, after which is fused with the text\ninput. During inference, the prompts can be pre-computed so that no extra\ncomputation cost is needed. To collocate with the unified prompt, we propose a\nnew initialization method for the target label word to further improve the\nmodel's transferability across languages. Extensive experiments show that our\nproposed methods can significantly outperform the strong baselines across\ndifferent languages. We release data and code to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lianzhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Houfeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning. (arXiv:2203.11933v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.11933","description":"<p>Vision-language models can encode societal biases and stereotypes, but there\nare challenges to measuring and mitigating these multimodal harms due to\nlacking measurement robustness and feature degradation. To address these\nchallenges, we investigate bias measures and apply ranking metrics for\nimage-text representations. We then investigate debiasing methods and show that\nprepending learned embeddings to text queries that are jointly trained with\nadversarial debiasing and a contrastive loss reduces various bias measures with\nminimal degradation to the image-text representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berg_H/0/1/0/all/0/1\">Hugo Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_S/0/1/0/all/0/1\">Siobhan Mackenzie Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhalgat_Y/0/1/0/all/0/1\">Yash Bhalgat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wonsuk Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Rose Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shtedritski_A/0/1/0/all/0/1\">Aleksandar Shtedritski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bain_M/0/1/0/all/0/1\">Max Bain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Does Pre-trained Wav2Vec 2.0 Perform on Domain Shifted ASR? An Extensive Benchmark on Air Traffic Control Communications. (arXiv:2203.16822v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2203.16822","description":"<p>Recent work on self-supervised pre-training focus on leveraging large-scale\nunlabeled speech data to build robust end-to-end (E2E) acoustic models (AM)\nthat can be later fine-tuned on downstream tasks e.g., automatic speech\nrecognition (ASR). Yet, few works investigated the impact on performance when\nthe data properties substantially differ between the pre-training and\nfine-tuning phases, termed domain shift. We target this scenario by analyzing\nthe robustness of Wav2Vec 2.0 and XLS-R models on downstream ASR for a\ncompletely unseen domain, air traffic control (ATC) communications. We\nbenchmark these two models on several open-source and challenging ATC databases\nwith signal-to-noise ratio between 5 and 20 dB. Relative word error rate (WER)\nreductions between 20% to 40% are obtained in comparison to hybrid-based ASR\nbaselines by only fine-tuning E2E acoustic models with a smaller fraction of\nlabeled data. We analyze WERs on the low-resource scenario and gender bias\ncarried by one ATC dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasad_A/0/1/0/all/0/1\">Amrutha Prasad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nigmatulina_I/0/1/0/all/0/1\">Iuliia Nigmatulina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarfjoo_S/0/1/0/all/0/1\">Saeed Sarfjoo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kleinert_M/0/1/0/all/0/1\">Matthias Kleinert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Helmke_H/0/1/0/all/0/1\">Hartmut Helmke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ohneiser_O/0/1/0/all/0/1\">Oliver Ohneiser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhan_Q/0/1/0/all/0/1\">Qingran Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Show, Don't Tell: Demonstrations Outperform Descriptions for Schema-Guided Task-Oriented Dialogue. (arXiv:2204.04327v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04327","description":"<p>Building universal dialogue systems that operate across multiple domains/APIs\nand generalize to new ones with minimal overhead is a critical challenge.\nRecent works have leveraged natural language descriptions of schema elements to\nenable such systems; however, descriptions only indirectly convey schema\nsemantics. In this work, we propose Show, Don't Tell, which prompts seq2seq\nmodels with a labeled example dialogue to show the semantics of schema elements\nrather than tell the model through descriptions. While requiring similar effort\nfrom service developers as generating descriptions, we show that using short\nexamples as schema representations with large language models results in\nstate-of-the-art performance on two popular dialogue state tracking benchmarks\ndesigned to measure zero-shot generalization - the Schema-Guided Dialogue\ndataset and the MultiWOZ leave-one-out benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Raghav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Harrison Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jeffrey Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1\">Abhinav Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Experimental Standards for Deep Learning in Natural Language Processing Research. (arXiv:2204.06251v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.06251","description":"<p>The field of Deep Learning (DL) has undergone explosive growth during the\nlast decade, with a substantial impact on Natural Language Processing (NLP) as\nwell. Yet, compared to more established disciplines, a lack of common\nexperimental standards remains an open challenge to the field at large.\nStarting from fundamental scientific principles, we distill ongoing discussions\non experimental standards in NLP into a single, widely-applicable methodology.\nFollowing these best practices is crucial to strengthen experimental evidence,\nimprove reproducibility and support scientific progress. These standards are\nfurther collected in a public repository to help them transparently adapt to\nfuture needs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ulmer_D/0/1/0/all/0/1\">Dennis Ulmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bassignana_E/0/1/0/all/0/1\">Elisa Bassignana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_Eberstein_M/0/1/0/all/0/1\">Max M&#xfc;ller-Eberstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varab_D/0/1/0/all/0/1\">Daniel Varab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mike Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goot_R/0/1/0/all/0/1\">Rob van der Goot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardmeier_C/0/1/0/all/0/1\">Christian Hardmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back to the Future: Bidirectional Information Decoupling Network for Multi-turn Dialogue Modeling. (arXiv:2204.08152v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08152","description":"<p>Multi-turn dialogue modeling as a challenging branch of natural language\nunderstanding (NLU), aims to build representations for machines to understand\nhuman dialogues, which provides a solid foundation for multiple downstream\ntasks. Recent studies of dialogue modeling commonly employ pre-trained language\nmodels (PrLMs) to encode the dialogue history as successive tokens, which is\ninsufficient in capturing the temporal characteristics of dialogues. Therefore,\nwe propose Bidirectional Information Decoupling Network (BiDeN) as a universal\ndialogue encoder, which explicitly incorporates both the past and future\ncontexts and can be generalized to a wide range of dialogue-related tasks.\nExperimental results on datasets of different downstream tasks demonstrate the\nuniversality and effectiveness of our BiDeN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Fast Post-Training Pruning Framework for Transformers. (arXiv:2204.09656v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.09656","description":"<p>Pruning is an effective way to reduce the huge inference cost of Transformer\nmodels. However, prior work on pruning Transformers requires retraining the\nmodels. This can add high training cost and high complexity to model\ndeployment, making it difficult to use in many practical situations. To address\nthis, we propose a fast post-training pruning framework for Transformers that\ndoes not require any retraining. Given a resource constraint and a sample\ndataset, our framework automatically prunes the Transformer model using\nstructured sparsity methods. To retain high accuracy without retraining, we\nintroduce three novel techniques: (i) a lightweight mask search algorithm that\nfinds which heads and filters to prune based on the Fisher information; (ii)\nmask rearrangement that complements the search algorithm; and (iii) mask tuning\nthat reconstructs the output activations for each layer. We apply our method to\nBERT-base and DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD\nbenchmarks. Our framework achieves up to 2.0x reduction in FLOPs and 1.56x\nspeedup in inference latency, while maintaining &lt; 1% loss in accuracy.\nImportantly, our framework prunes Transformers in less than 3 minutes on a\nsingle GPU, which is over two orders of magnitude faster than existing pruning\napproaches that retrain the models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_W/0/1/0/all/0/1\">Woosuk Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassoun_J/0/1/0/all/0/1\">Joseph Hassoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building for Tomorrow: Assessing the Temporal Persistence of Text Classifiers. (arXiv:2205.05435v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.05435","description":"<p>Performance of text classification models tends to drop over time due to\nchanges in data, which limits the lifetime of a pretrained model. Therefore an\nability to predict a model's ability to persist over time can help design\nmodels that can be effectively used over a longer period of time. In this\npaper, we look at this problem from a practical perspective by assessing the\nability of a wide range of language models and classification algorithms to\npersist over time, as well as how dataset characteristics can help predict the\ntemporal stability of different models. We perform longitudinal classification\nexperiments on three datasets spanning between 6 and 19 years, and involving\ndiverse tasks and types of data. We find that one can estimate how a model will\nretain its performance over time based on (i) how well the model performs over\na restricted time period and its extrapolation to a longer time period, and\n(ii) the linguistic characteristics of the dataset, such as the familiarity\nscore between subsets from different years.Findings from these experiments have\nimportant implications for the design of text classification models with the\naim of preserving performance over time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alkhalifa_R/0/1/0/all/0/1\">Rabab Alkhalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochkina_E/0/1/0/all/0/1\">Elena Kochkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How sensitive are translation systems to extra contexts? Mitigating gender bias in Neural Machine Translation models through relevant contexts. (arXiv:2205.10762v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10762","description":"<p>Neural Machine Translation systems built on top of Transformer-based\narchitectures are routinely improving the state-of-the-art in translation\nquality according to word-overlap metrics. However, a growing number of studies\nalso highlight the inherent gender bias that these models incorporate during\ntraining, which reflects poorly in their translations. In this work, we\ninvestigate whether these models can be instructed to fix their bias during\ninference using targeted, guided instructions as contexts. By translating\nrelevant contextual sentences during inference along with the input, we observe\nlarge improvements in reducing the gender bias in translations, across three\npopular test suites (WinoMT, BUG, SimpleGen). We further propose a novel metric\nto assess several large pre-trained models (OPUS-MT, M2M-100) on their\nsensitivity towards using contexts during translation to correct their biases.\nOur approach requires no fine-tuning and thus can be used easily in production\nsystems to de-bias translations from stereotypical gender-occupation bias 1. We\nhope our method, along with our metric, can be used to build better, bias-free\ntranslation systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shanya Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_M/0/1/0/all/0/1\">Manan Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_K/0/1/0/all/0/1\">Koustuv Sinha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relphormer: Relational Graph Transformer for Knowledge Graph Representations. (arXiv:2205.10852v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10852","description":"<p>Transformers have achieved remarkable performance in widespread fields,\nincluding natural language processing, computer vision and graph mining.\nHowever, vanilla Transformer architectures have not yielded promising\nimprovements in the Knowledge Graph (KG) representations, where the\ntranslational distance paradigm dominates this area. Note that vanilla\nTransformer architectures struggle to capture the intrinsically heterogeneous\nsemantic and structural information of knowledge graphs. To this end, we\npropose a new variant of Transformer for knowledge graph representations dubbed\nRelphormer. Specifically, we introduce Triple2Seq which can dynamically sample\ncontextualized sub-graph sequences as the input to alleviate the heterogeneity\nissue. We propose a novel structure-enhanced self-attention mechanism to encode\nthe relational information and keep the globally semantic information among\nsub-graphs. Moreover, we propose masked knowledge modeling as a new paradigm\nfor knowledge graph representation learning. We apply Relphormer to three\ntasks, namely, knowledge graph completion, KG-based question answering and\nKG-based recommendation for evaluation. Experimental results show that\nRelphormer can obtain better performance on benchmark datasets compared with\nbaselines. Code is available in https://github.com/zjunlp/Relphormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder. (arXiv:2205.12035v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12035","description":"<p>Despite pre-training's progress in many important NLP tasks, it remains to\nexplore effective pre-training strategies for dense retrieval. In this paper,\nwe propose RetroMAE, a new retrieval oriented pre-training paradigm based on\nMasked Auto-Encoder (MAE). RetroMAE is highlighted by three critical designs.\n1) A novel MAE workflow, where the input sentence is polluted for encoder and\ndecoder with different masks. The sentence embedding is generated from the\nencoder's masked input; then, the original sentence is recovered based on the\nsentence embedding and the decoder's masked input via masked language modeling.\n2) Asymmetric model structure, with a full-scale BERT like transformer as\nencoder, and a one-layer transformer as decoder. 3) Asymmetric masking ratios,\nwith a moderate ratio for encoder: 15~30%, and an aggressive ratio for decoder:\n50~70%. Our framework is simple to realize and empirically competitive: the\npre-trained models dramatically improve the SOTA performances on a wide range\nof dense retrieval benchmarks, like BEIR and MS MARCO. The source code and\npre-trained models are made publicly available at\nhttps://github.com/staoxiao/RetroMAE so as to inspire more interesting\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shitao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yingxia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhao Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Certified Robustness Against Natural Language Attacks by Causal Intervention. (arXiv:2205.12331v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.12331","description":"<p>Deep learning models have achieved great success in many fields, yet they are\nvulnerable to adversarial examples. This paper follows a causal perspective to\nlook into the adversarial vulnerability and proposes Causal Intervention by\nSemantic Smoothing (CISS), a novel framework towards robustness against natural\nlanguage attacks. Instead of merely fitting observational data, CISS learns\ncausal effects p(y|do(x)) by smoothing in the latent semantic space to make\nrobust predictions, which scales to deep architectures and avoids tedious\nconstruction of noise customized for specific attacks. CISS is provably robust\nagainst word substitution attacks, as well as empirically robust even when\nperturbations are strengthened by unknown attack algorithms. For example, on\nYELP, CISS surpasses the runner-up by 6.7% in terms of certified robustness\nagainst word substitutions, and achieves 79.4% empirical robustness when\nsyntactic attacks are integrated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haiteng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xinshuai Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1\">Anh Tuan Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhi-Hong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FLUTE: Figurative Language Understanding through Textual Explanations. (arXiv:2205.12404v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12404","description":"<p>Figurative language understanding has been recently framed as a recognizing\ntextual entailment (RTE) task (a.k.a. natural language inference, or NLI).\nHowever, similar to classical RTE/NLI datasets, the current benchmarks suffer\nfrom spurious correlations and annotation artifacts. To tackle this problem,\nwork on NLI has built explanation-based datasets such as e-SNLI, allowing us to\nprobe whether language models are right for the right reasons.Yet no such data\nexists for figurative language, making it harder to assess genuine\nunderstanding of such expressions. To address this issue, we release FLUTE, a\ndataset of 9,000 figurative NLI instances with explanations, spanning four\ncategories: Sarcasm, Simile, Metaphor, and Idioms. We collect the data through\na model-in-the-loop framework based on GPT-3, crowd workers, and expert\nannotators. We show how utilizing GPT-3 in conjunction with human annotators\n(novices and experts) can aid in scaling up the creation of datasets even for\nsuch complex linguistic phenomena as figurative language. The baseline\nperformance of the T5 model fine-tuned on FLUTE shows that our dataset can\nbring us a step closer to developing models that understand figurative language\nthrough textual explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saakyan_A/0/1/0/all/0/1\">Arkadiy Saakyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1\">Debanjan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Character-Level Length-Control Algorithm for Non-Autoregressive Sentence Summarization. (arXiv:2205.14522v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.14522","description":"<p>Sentence summarization aims at compressing a long sentence into a short one\nthat keeps the main gist, and has extensive real-world applications such as\nheadline generation. In previous work, researchers have developed various\napproaches to improve the ROUGE score, which is the main evaluation metric for\nsummarization, whereas controlling the summary length has not drawn much\nattention. In our work, we address a new problem of explicit character-level\nlength control for summarization, and propose a dynamic programming algorithm\nbased on the Connectionist Temporal Classification (CTC) model. Results show\nthat our approach not only achieves higher ROUGE scores but also yields more\ncomplete sentences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Puyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lili Mou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Squeezeformer: An Efficient Transformer for Automatic Speech Recognition. (arXiv:2206.00888v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2206.00888","description":"<p>The recently proposed Conformer model has become the de facto backbone model\nfor various downstream speech tasks based on its hybrid attention-convolution\narchitecture that captures both local and global features. However, through a\nseries of systematic studies, we find that the Conformer architecture's design\nchoices are not optimal. After re-examining the design choices for both the\nmacro and micro-architecture of Conformer, we propose Squeezeformer which\nconsistently outperforms the state-of-the-art ASR models under the same\ntraining schemes. In particular, for the macro-architecture, Squeezeformer\nincorporates (i) the Temporal U-Net structure which reduces the cost of the\nmulti-head attention modules on long sequences, and (ii) a simpler block\nstructure of multi-head attention or convolution modules followed up by\nfeed-forward module instead of the Macaron structure proposed in Conformer.\nFurthermore, for the micro-architecture, Squeezeformer (i) simplifies the\nactivations in the convolutional block, (ii) removes redundant Layer\nNormalization operations, and (iii) incorporates an efficient depthwise\ndown-sampling layer to efficiently sub-sample the input signal. Squeezeformer\nachieves state-of-the-art results of 7.5%, 6.5%, and 6.0% word-error-rate (WER)\non LibriSpeech test-other without external language models, which are 3.1%,\n1.4%, and 0.6% better than Conformer-CTC with the same number of FLOPs. Our\ncode is open-sourced and available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Sehoon Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shaw_A/0/1/0/all/0/1\">Albert Shaw</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_N/0/1/0/all/0/1\">Nicholas Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mangalam_K/0/1/0/all/0/1\">Karttikeya Mangalam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Sequence Interface for Vision Tasks. (arXiv:2206.07669v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.07669","description":"<p>While language tasks are naturally expressed in a single, unified, modeling\nframework, i.e., generating sequences of tokens, this has not been the case in\ncomputer vision. As a result, there is a proliferation of distinct\narchitectures and loss functions for different vision tasks. In this work we\nshow that a diverse set of \"core\" computer vision tasks can also be unified if\nformulated in terms of a shared pixel-to-sequence interface. We focus on four\ntasks, namely, object detection, instance segmentation, keypoint detection, and\nimage captioning, all with diverse types of outputs, e.g., bounding boxes or\ndense masks. Despite that, by formulating the output of each task as a sequence\nof discrete tokens with a unified interface, we show that one can train a\nneural network with a single model architecture and loss function on all these\ntasks, with no task-specific customization. To solve a specific task, we use a\nshort prompt as task description, and the sequence output adapts to the prompt\nso it can produce task-specific output. We show that such a model can achieve\ncompetitive performance compared to well-established task-specific models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_S/0/1/0/all/0/1\">Saurabh Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lala Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinton_G/0/1/0/all/0/1\">Geoffrey Hinton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IDIAPers @ Causal News Corpus 2022: Efficient Causal Relation Identification Through a Prompt-based Few-shot Approach. (arXiv:2209.03895v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.03895","description":"<p>In this paper, we describe our participation in the subtask 1 of CASE-2022,\nEvent Causality Identification with Casual News Corpus. We address the Causal\nRelation Identification (CRI) task by exploiting a set of simple yet\ncomplementary techniques for fine-tuning language models (LMs) on a small\nnumber of annotated examples (i.e., a few-shot configuration). We follow a\nprompt-based prediction approach for fine-tuning LMs in which the CRI task is\ntreated as a masked language modeling problem (MLM). This approach allows LMs\nnatively pre-trained on MLM problems to directly generate textual responses to\nCRI-specific prompts. We compare the performance of this method against\nensemble techniques trained on the entire dataset. Our best-performing\nsubmission was fine-tuned with only 256 instances per class, 15.7% of the all\navailable data, and yet obtained the second-best precision (0.82), third-best\naccuracy (0.82), and an F1-score (0.85) very close to what was reported by the\nwinner team (0.86).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burdisso_S/0/1/0/all/0/1\">Sergio Burdisso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villatoro_Tello_E/0/1/0/all/0/1\">Esau Villatoro-Tello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fajcik_M/0/1/0/all/0/1\">Martin Fajcik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Muskaan Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smrz_P/0/1/0/all/0/1\">Pavel Smrz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering. (arXiv:2209.09513v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.09513","description":"<p>When answering a question, humans utilize the information available across\ndifferent modalities to synthesize a consistent and complete chain of thought\n(CoT). This process is normally a black box in the case of deep learning models\nlike large-scale language models. Recently, science question benchmarks have\nbeen used to diagnose the multi-hop reasoning ability and interpretability of\nan AI system. However, existing datasets fail to provide annotations for the\nanswers, or are restricted to the textual-only modality, small scales, and\nlimited domain diversity. To this end, we present Science Question Answering\n(ScienceQA), a new benchmark that consists of ~21k multimodal multiple choice\nquestions with a diverse set of science topics and annotations of their answers\nwith corresponding lectures and explanations. We further design language models\nto learn to generate lectures and explanations as the chain of thought (CoT) to\nmimic the multi-hop reasoning process when answering ScienceQA questions.\nScienceQA demonstrates the utility of CoT in language models, as CoT improves\nthe question answering performance by 1.20% in few-shot GPT-3 and 3.99% in\nfine-tuned UnifiedQA. We also explore the upper bound for models to leverage\nexplanations by feeding those in the input; we observe that it improves the\nfew-shot performance of GPT-3 by 18.96%. Our analysis further shows that\nlanguage models, similar to humans, benefit from explanations to learn from\nfewer data and achieve the same performance with just 40% of the data. The data\nand code are available at https://scienceqa.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1\">Tony Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tafjord_O/0/1/0/all/0/1\">Oyvind Tafjord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1\">Ashwin Kalyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re-contextualizing Fairness in NLP: The Case of India. (arXiv:2209.12226v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.12226","description":"<p>Recent research has revealed undesirable biases in NLP data and models.\nHowever, these efforts focus of social disparities in West, and are not\ndirectly portable to other geo-cultural contexts. In this paper, we focus on\nNLP fair-ness in the context of India. We start with a brief account of the\nprominent axes of social disparities in India. We build resources for fairness\nevaluation in the Indian context and use them to demonstrate prediction biases\nalong some of the axes. We then delve deeper into social stereotypes for Region\nandReligion, demonstrating its prevalence in corpora and models. Finally, we\noutline a holistic research agenda to re-contextualize NLP fairness research\nfor the Indian context, ac-counting for Indian societal context, bridging\ntechnological gaps in NLP capabilities and re-sources, and adapting to Indian\ncultural values. While we focus on India, this framework can be generalized to\nother geo-cultural contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_S/0/1/0/all/0/1\">Shaily Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dave_S/0/1/0/all/0/1\">Shachi Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1\">Vinodkumar Prabhakaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment is all you need to win US Presidential elections. (arXiv:2209.13487v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.13487","description":"<p>Election speeches play an integral role in communicating the vision and\nmission of the candidates. From lofty promises to mud-slinging, the electoral\ncandidate accounts for all. However, there remains an open question about what\nexactly wins over the voters. In this work, we used state-of-the-art natural\nlanguage processing methods to study the speeches and sentiments of the\nRepublican candidate, Donald Trump, and Democratic candidate, Joe Biden,\nfighting for the 2020 US Presidential election. Comparing the racial dichotomy\nof the United States, we analyze what led to the victory and defeat of the\ndifferent candidates. We believe this work will inform the election campaigning\nstrategy and provide a basis for communicating to diverse crowds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohapatra_S/0/1/0/all/0/1\">Sovesh Mohapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohapatra_S/0/1/0/all/0/1\">Somesh Mohapatra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keyword Extraction from Short Texts with a Text-To-Text Transfer Transformer. (arXiv:2209.14008v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.14008","description":"<p>The paper explores the relevance of the Text-To-Text Transfer Transformer\nlanguage model (T5) for Polish (plT5) to the task of intrinsic and extrinsic\nkeyword extraction from short text passages. The evaluation is carried out on\nthe new Polish Open Science Metadata Corpus (POSMAC), which is released with\nthis paper: a collection of 216,214 abstracts of scientific publications\ncompiled in the CURLICAT project. We compare the results obtained by four\ndifferent methods, i.e. plT5kw, extremeText, TermoPL, KeyBERT and conclude that\nthe plT5kw model yields particularly promising results for both frequent and\nsparsely represented keywords. Furthermore, a plT5kw keyword generation model\ntrained on the POSMAC also seems to produce highly useful results in\ncross-domain text labelling scenarios. We discuss the performance of the model\non news stories and phone-based dialog transcripts which represent text genres\nand domains extrinsic to the dataset of scientific abstracts. Finally, we also\nattempt to characterize the challenges of evaluating a text-to-text model on\nboth intrinsic and extrinsic keyword extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pezik_P/0/1/0/all/0/1\">Piotr P&#x119;zik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikolajczyk_Barela_A/0/1/0/all/0/1\">Agnieszka Miko&#x142;ajczyk-Bare&#x142;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wawrzynski_A/0/1/0/all/0/1\">Adam Wawrzy&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niton_B/0/1/0/all/0/1\">Bart&#x142;omiej Nito&#x144;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogrodniczuk_M/0/1/0/all/0/1\">Maciej Ogrodniczuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent Communication: Generalization and Overfitting in Lewis Games. (arXiv:2209.15342v2 [cs.MA] UPDATED)","link":"http://arxiv.org/abs/2209.15342","description":"<p>Lewis signaling games are a class of simple communication games for\nsimulating the emergence of language. In these games, two agents must agree on\na communication protocol in order to solve a cooperative task. Previous work\nhas shown that agents trained to play this game with reinforcement learning\ntend to develop languages that display undesirable properties from a linguistic\npoint of view (lack of generalization, lack of compositionality, etc). In this\npaper, we aim to provide better understanding of this phenomenon by\nanalytically studying the learning problem in Lewis games. As a core\ncontribution, we demonstrate that the standard objective in Lewis games can be\ndecomposed in two components: a co-adaptation loss and an information loss.\nThis decomposition enables us to surface two potential sources of overfitting,\nwhich we show may undermine the emergence of a structured communication\nprotocol. In particular, when we control for overfitting on the co-adaptation\nloss, we recover desired properties in the emergent languages: they are more\ncompositional and generalize better.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rita_M/0/1/0/all/0/1\">Mathieu Rita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tallec_C/0/1/0/all/0/1\">Corentin Tallec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michel_P/0/1/0/all/0/1\">Paul Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grill_J/0/1/0/all/0/1\">Jean-Bastien Grill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietquin_O/0/1/0/all/0/1\">Olivier Pietquin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strub_F/0/1/0/all/0/1\">Florian Strub</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Multi-Modal Sarcasm Detection via Hierarchical Congruity Modeling with Knowledge Enhancement. (arXiv:2210.03501v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03501","description":"<p>Sarcasm is a linguistic phenomenon indicating a discrepancy between literal\nmeanings and implied intentions. Due to its sophisticated nature, it is usually\nchallenging to be detected from the text itself. As a result, multi-modal\nsarcasm detection has received more attention in both academia and industries.\nHowever, most existing techniques only modeled the atomic-level inconsistencies\nbetween the text input and its accompanying image, ignoring more complex\ncompositions for both modalities. Moreover, they neglected the rich information\ncontained in external knowledge, e.g., image captions. In this paper, we\npropose a novel hierarchical framework for sarcasm detection by exploring both\nthe atomic-level congruity based on multi-head cross attention mechanism and\nthe composition-level congruity based on graph neural networks, where a post\nwith low congruity can be identified as sarcasm. In addition, we exploit the\neffect of various knowledge resources for sarcasm detection. Evaluation results\non a public multi-modal sarcasm detection dataset based on Twitter demonstrate\nthe superiority of our proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenya Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoliang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Teachers Can Be Dense with Knowledge. (arXiv:2210.03923v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03923","description":"<p>Recent advances in distilling pretrained language models have discovered\nthat, besides the expressiveness of knowledge, the student-friendliness should\nbe taken into consideration to realize a truly knowledgable teacher. Based on a\npilot study, we find that over-parameterized teachers can produce expressive\nyet student-unfriendly knowledge and are thus limited in overall\nknowledgableness. To remove the parameters that result in\nstudent-unfriendliness, we propose a sparse teacher trick under the guidance of\nan overall knowledgable score for each teacher parameter. The knowledgable\nscore is essentially an interpolation of the expressiveness and\nstudent-friendliness scores. The aim is to ensure that the expressive\nparameters are retained while the student-unfriendly ones are removed.\nExtensive experiments on the GLUE benchmark show that the proposed sparse\nteachers can be dense with knowledge and lead to students with compelling\nperformance in comparison with a series of competitive baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spread Love Not Hate: Undermining the Importance of Hateful Pre-training for Hate Speech Detection. (arXiv:2210.04267v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04267","description":"<p>Pre-training large neural language models, such as BERT, has led to\nimpressive gains on many natural language processing (NLP) tasks. Although this\nmethod has proven to be effective for many domains, it might not always provide\ndesirable benefits. In this paper, we study the effects of hateful pre-training\non low-resource hate speech classification tasks. While previous studies on the\nEnglish language have emphasized its importance, we aim to augment their\nobservations with some non-obvious insights. We evaluate different variations\nof tweet-based BERT models pre-trained on hateful, non-hateful, and mixed\nsubsets of a 40M tweet dataset. This evaluation is carried out for the Indian\nlanguages Hindi and Marathi. This paper is empirical evidence that hateful\npre-training is not the best pre-training option for hate speech detection. We\nshow that pre-training on non-hateful text from the target domain provides\nsimilar or better results. Further, we introduce HindTweetBERT and\nMahaTweetBERT, the first publicly available BERT models pre-trained on Hindi\nand Marathi tweets, respectively. We show that they provide state-of-the-art\nperformance on hate speech classification tasks. We also release hateful BERT\nfor the two languages and a gold hate speech evaluation benchmark HateEval-Hi\nand HateEval-Mr consisting of manually labeled 2000 tweets each. The models and\ndata are available at https://github.com/l3cube-pune/MarathiNLP .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_O/0/1/0/all/0/1\">Omkar Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kane_A/0/1/0/all/0/1\">Aditya Kane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patankar_S/0/1/0/all/0/1\">Shantanu Patankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chavan_T/0/1/0/all/0/1\">Tanmay Chavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REV: Information-Theoretic Evaluation of Free-Text Rationales. (arXiv:2210.04982v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04982","description":"<p>Free-text rationales are a promising step towards explainable AI, yet their\nevaluation remains an open research problem. While existing metrics have mostly\nfocused on measuring the direct association between the rationale and a given\nlabel, we argue that an ideal metric should also be able to focus on the new\ninformation uniquely provided in the rationale that is otherwise not provided\nin the input or the label. We investigate this research problem from an\ninformation-theoretic perspective using the conditional V-information. More\nconcretely, we propose a metric called REV (Rationale Evaluation with\nconditional V-information), that can quantify the new information in a\nrationale supporting a given label beyond the information already available in\nthe input or the label. Experiments on reasoning tasks across four benchmarks,\nincluding few-shot prompting with GPT-3, demonstrate the effectiveness of REV\nin evaluating different types of rationale-label pairs, compared to existing\nmetrics. Through several quantitative comparisons, we demonstrate the\ncapability of REV in providing more sensitive measurements of new information\nin free-text rationales with respect to a label. Furthermore, REV is consistent\nwith human judgments on rationale evaluations. Overall, when used alongside\ntraditional performance metrics, REV provides deeper insights into a models'\nreasoning and prediction processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Language Maps for Robot Navigation. (arXiv:2210.05714v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2210.05714","description":"<p>Grounding language to the visual observations of a navigating agent can be\nperformed using off-the-shelf visual-language models pretrained on\nInternet-scale data (e.g., image captions). While this is useful for matching\nimages to natural language descriptions of object goals, it remains disjoint\nfrom the process of mapping the environment, so that it lacks the spatial\nprecision of classic geometric maps. To address this problem, we propose\nVLMaps, a spatial map representation that directly fuses pretrained\nvisual-language features with a 3D reconstruction of the physical world. VLMaps\ncan be autonomously built from video feed on robots using standard exploration\napproaches and enables natural language indexing of the map without additional\nlabeled data. Specifically, when combined with large language models (LLMs),\nVLMaps can be used to (i) translate natural language commands into a sequence\nof open-vocabulary navigation goals (which, beyond prior work, can be spatial\nby construction, e.g., \"in between the sofa and TV\" or \"three meters to the\nright of the chair\") directly localized in the map, and (ii) can be shared\namong multiple robots with different embodiments to generate new obstacle maps\non-the-fly (by using a list of obstacle categories). Extensive experiments\ncarried out in simulated and real world environments show that VLMaps enable\nnavigation according to more complex language instructions than existing\nmethods. Videos are available at https://vlmaps.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chenguang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mees_O/0/1/0/all/0/1\">Oier Mees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Andy Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1\">Wolfram Burgard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hate-CLIPper: Multimodal Hateful Meme Classification based on Cross-modal Interaction of CLIP Features. (arXiv:2210.05916v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05916","description":"<p>Hateful memes are a growing menace on social media. While the image and its\ncorresponding text in a meme are related, they do not necessarily convey the\nsame meaning when viewed individually. Hence, detecting hateful memes requires\ncareful consideration of both visual and textual information. Multimodal\npre-training can be beneficial for this task because it effectively captures\nthe relationship between the image and the text by representing them in a\nsimilar feature space. Furthermore, it is essential to model the interactions\nbetween the image and text features through intermediate fusion. Most existing\nmethods either employ multimodal pre-training or intermediate fusion, but not\nboth. In this work, we propose the Hate-CLIPper architecture, which explicitly\nmodels the cross-modal interactions between the image and text representations\nobtained using Contrastive Language-Image Pre-training (CLIP) encoders via a\nfeature interaction matrix (FIM). A simple classifier based on the FIM\nrepresentation is able to achieve state-of-the-art performance on the Hateful\nMemes Challenge (HMC) dataset with an AUROC of 85.8, which even surpasses the\nhuman performance of 82.65. Experiments on other meme datasets such as\nPropaganda Memes and TamilMemes also demonstrate the generalizability of the\nproposed approach. Finally, we analyze the interpretability of the FIM\nrepresentation and show that cross-modal interactions can indeed facilitate the\nlearning of meaningful concepts. The code for this work is available at\nhttps://github.com/gokulkarthik/hateclipper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1\">Gokul Karthik Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandakumar_K/0/1/0/all/0/1\">Karthik Nandakumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting the clinical citation count of biomedical papers using multilayer perceptron neural network. (arXiv:2210.06346v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06346","description":"<p>The number of clinical citations received from clinical guidelines or\nclinical trials has been considered as one of the most appropriate indicators\nfor quantifying the clinical impact of biomedical papers. Therefore, the early\nprediction of the clinical citation count of biomedical papers is critical to\nscientific activities in biomedicine, such as research evaluation, resource\nallocation, and clinical translation. In this study, we designed a four-layer\nmultilayer perceptron neural network (MPNN) model to predict the clinical\ncitation count of biomedical papers in the future by using 9,822,620 biomedical\npapers published from 1985 to 2005. We extracted ninety-one paper features from\nthree dimensions as the input of the model, including twenty-one features in\nthe paper dimension, thirty-five in the reference dimension, and thirty-five in\nthe citing paper dimension. In each dimension, the features can be classified\ninto three categories, i.e., the citation-related features, the clinical\ntranslation-related features, and the topic-related features. Besides, in the\npaper dimension, we also considered the features that have previously been\ndemonstrated to be related to the citation counts of research papers. The\nresults showed that the proposed MPNN model outperformed the other five\nbaseline models, and the features in the reference dimension were the most\nimportant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xuli Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qikai Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SubeventWriter: Iterative Sub-event Sequence Generation with Coherence Controller. (arXiv:2210.06694v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06694","description":"<p>In this paper, we propose a new task of sub-event generation for an unseen\nprocess to evaluate the understanding of the coherence of sub-event actions and\nobjects. To solve the problem, we design SubeventWriter, a sub-event sequence\ngeneration framework with a coherence controller. Given an unseen process, the\nframework can iteratively construct the sub-event sequence by generating one\nsub-event at each iteration. We also design a very effective coherence\ncontroller to decode more coherent sub-events. As our extensive experiments and\nanalysis indicate, SubeventWriter can generate more reliable and meaningful\nsub-event sequences for unseen processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1\">Tianqing Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_G/0/1/0/all/0/1\">Ginny Y. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_S/0/1/0/all/0/1\">Simon See</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re3: Generating Longer Stories With Recursive Reprompting and Revision. (arXiv:2210.06774v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06774","description":"<p>We consider the problem of automatically generating longer stories of over\ntwo thousand words. Compared to prior work on shorter stories, long-range plot\ncoherence and relevance are more central challenges here. We propose the\nRecursive Reprompting and Revision framework (Re3) to address these challenges\nby (a) prompting a general-purpose language model to construct a structured\noverarching plan, and (b) generating story passages by repeatedly injecting\ncontextual information from both the plan and current story state into a\nlanguage model prompt. We then revise by (c) reranking different continuations\nfor plot coherence and premise relevance, and finally (d) editing the best\ncontinuation for factual consistency. Compared to similar-length stories\ngenerated directly from the same base model, human evaluators judged\nsubstantially more of Re3's stories as having a coherent overarching plot (by\n14% absolute increase), and relevant to the given initial premise (by 20%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence Ambiguity, Grammaticality and Complexity Probes. (arXiv:2210.06928v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06928","description":"<p>It is unclear whether, how and where large pre-trained language models\ncapture subtle linguistic traits like ambiguity, grammaticality and sentence\ncomplexity. We present results of automatic classification of these traits and\ncompare their viability and patterns across representation types. We\ndemonstrate that template-based datasets with surface-level artifacts should\nnot be used for probing, careful comparisons with baselines should be done and\nthat t-SNE plots should not be used to determine the presence of a feature\namong dense vectors representations. We also show how features might be highly\nlocalized in the layers for these models and get lost in the upper layers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Sunit Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zouhar_V/0/1/0/all/0/1\">Vil&#xe9;m Zouhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-based Connective Prediction Method for Fine-grained Implicit Discourse Relation Recognition. (arXiv:2210.07032v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07032","description":"<p>Due to the absence of connectives, implicit discourse relation recognition\n(IDRR) is still a challenging and crucial task in discourse analysis. Most of\nthe current work adopted multi-task learning to aid IDRR through explicit\ndiscourse relation recognition (EDRR) or utilized dependencies between\ndiscourse relation labels to constrain model predictions. But these methods\nstill performed poorly on fine-grained IDRR and even utterly misidentified on\nmost of the few-shot discourse relation classes. To address these problems, we\npropose a novel Prompt-based Connective Prediction (PCP) method for IDRR. Our\nmethod instructs large-scale pre-trained models to use knowledge relevant to\ndiscourse relation and utilizes the strong correlation between connectives and\ndiscourse relation to help the model recognize implicit discourse relations.\nExperimental results show that our method surpasses the current\nstate-of-the-art model and achieves significant improvements on those\nfine-grained few-shot discourse relation. Moreover, our approach is able to be\ntransferred to EDRR and obtain acceptable results. Our code is released in\nhttps://github.com/zh-i9/PCP-for-IDRR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_M/0/1/0/all/0/1\">Man Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuanbin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Meirong Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SS-BERT: Mitigating Identity Terms Bias in Toxic Comment Classification by Utilising the Notion of \"Subjectivity\" and \"Identity Terms\". (arXiv:2109.02691v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2109.02691","description":"<p>Toxic comment classification models are often found biased toward identity\nterms which are terms characterizing a specific group of people such as\n\"Muslim\" and \"black\". Such bias is commonly reflected in false-positive\npredictions, i.e. non-toxic comments with identity terms. In this work, we\npropose a novel approach to tackle such bias in toxic comment classification,\nleveraging the notion of subjectivity level of a comment and the presence of\nidentity terms. We hypothesize that when a comment is made about a group of\npeople that is characterized by an identity term, the likelihood of that\ncomment being toxic is associated with the subjectivity level of the comment,\ni.e. the extent to which the comment conveys personal feelings and opinions.\nBuilding upon the BERT model, we propose a new structure that is able to\nleverage these features, and thoroughly evaluate our model on 4 datasets of\nvarying sizes and representing different social media platforms. The results\nshow that our model can consistently outperform BERT and a SOTA model devised\nto address identity term bias in a different way, with a maximum improvement in\nF1 of 2.43% and 1.91% respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhixue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hopfgartner_F/0/1/0/all/0/1\">Frank Hopfgartner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-10-17T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}