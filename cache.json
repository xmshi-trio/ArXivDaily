{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-12-05T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Generalizing Math Word Problem Solvers via Solution Diversification. (arXiv:2212.00833v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00833","description":"<p>Current math word problem (MWP) solvers are usually Seq2Seq models trained by\nthe (one-problem; one-solution) pairs, each of which is made of a problem\ndescription and a solution showing reasoning flow to get the correct answer.\nHowever, one MWP problem naturally has multiple solution equations. The\ntraining of an MWP solver with (one-problem; one-solution) pairs excludes other\ncorrect solutions, and thus limits the generalizability of the MWP solver. One\nfeasible solution to this limitation is to augment multiple solutions to a\ngiven problem. However, it is difficult to collect diverse and accurate augment\nsolutions through human efforts. In this paper, we design a new training\nframework for an MWP solver by introducing a solution buffer and a solution\ndiscriminator. The buffer includes solutions generated by an MWP solver to\nencourage the training data diversity. The discriminator controls the quality\nof buffered solutions to participate in training. Our framework is flexibly\napplicable to a wide setting of fully, semi-weakly and weakly supervised\ntraining for all Seq2Seq MWP solvers. We conduct extensive experiments on a\nbenchmark dataset Math23k and a new dataset named Weak12k, and show that our\nframework improves the performance of various MWP solvers under different\nsettings by generating correct and diverse solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhenwen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jie Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analogical Math Word Problems Solving with Enhanced Problem-Solution Association. (arXiv:2212.00837v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00837","description":"<p>Math word problem (MWP) solving is an important task in question answering\nwhich requires human-like reasoning ability. Analogical reasoning has long been\nused in mathematical education, as it enables students to apply common\nrelational structures of mathematical situations to solve new problems. In this\npaper, we propose to build a novel MWP solver by leveraging analogical MWPs,\nwhich advance the solver's generalization ability across different kinds of\nMWPs. The key idea, named analogy identification, is to associate the\nanalogical MWP pairs in a latent space, i.e., encoding an MWP close to another\nanalogical MWP, while moving away from the non-analogical ones. Moreover, a\nsolution discriminator is integrated into the MWP solver to enhance the\nassociation between the representations of MWPs and their true solutions. The\nevaluation results verify that our proposed analogical learning strategy\npromotes the performance of MWP-BERT on Math23k over the state-of-the-art model\nGenerate2Rank, with 5 times fewer parameters in the encoder. We also find that\nour model has a stronger generalization ability in solving difficult MWPs due\nto the analogical learning from easy MWPs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhenwen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focus! Relevant and Sufficient Context Selection for News Image Captioning. (arXiv:2212.00843v1 [cs.CV])","link":"http://arxiv.org/abs/2212.00843","description":"<p>News Image Captioning requires describing an image by leveraging additional\ncontext from a news article. Previous works only coarsely leverage the article\nto extract the necessary context, which makes it challenging for models to\nidentify relevant events and named entities. In our paper, we first demonstrate\nthat by combining more fine-grained context that captures the key named\nentities (obtained via an oracle) and the global context that summarizes the\nnews, we can dramatically improve the model's ability to generate accurate news\ncaptions. This begs the question, how to automatically extract such key\nentities from an image? We propose to use the pre-trained vision and language\nretrieval model CLIP to localize the visually grounded entities in the news\narticle and then capture the non-visual entities via an open relation\nextraction model. Our experiments demonstrate that by simply selecting a better\ncontext from the article, we can significantly improve the performance of\nexisting models and achieve new state-of-the-art performance on multiple\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Grace Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SOLD: Sinhala Offensive Language Dataset. (arXiv:2212.00851v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00851","description":"<p>The widespread of offensive content online, such as hate speech and\ncyber-bullying, is a global phenomenon. This has sparked interest in the\nartificial intelligence (AI) and natural language processing (NLP) communities,\nmotivating the development of various systems trained to detect potentially\nharmful content automatically. These systems require annotated datasets to\ntrain the machine learning (ML) models. However, with a few notable exceptions,\nmost datasets on this topic have dealt with English and a few other\nhigh-resource languages. As a result, the research in offensive language\nidentification has been limited to these languages. This paper addresses this\ngap by tackling offensive language identification in Sinhala, a low-resource\nIndo-Aryan language spoken by over 17 million people in Sri Lanka. We introduce\nthe Sinhala Offensive Language Dataset (SOLD) and present multiple experiments\non this dataset. SOLD is a manually annotated dataset containing 10,000 posts\nfrom Twitter annotated as offensive and not offensive at both sentence-level\nand token-level, improving the explainability of the ML models. SOLD is the\nfirst large publicly available offensive language dataset compiled for Sinhala.\nWe also introduce SemiSOLD, a larger dataset containing more than 145,000\nSinhala tweets, annotated following a semi-supervised approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anuradha_I/0/1/0/all/0/1\">Isuri Anuradha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Premasiri_D/0/1/0/all/0/1\">Damith Premasiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_K/0/1/0/all/0/1\">Kanishka Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hettiarachchi_H/0/1/0/all/0/1\">Hansi Hettiarachchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uyangodage_L/0/1/0/all/0/1\">Lasitha Uyangodage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"a survey on GPT-3. (arXiv:2212.00857v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00857","description":"<p>This paper provides an introductory survey to GPT-3. We cover some of the\nhistorical development behind this technology, some of the key features of\nGPT-3, and discuss the machine learning model and the datasets used. We survey\nboth academic and commercial efforts applying GPT-3 in diverse domains such as\ndeveloping conversational AI chatbots, software development, creative work,\ndomain knowledge, and business productivity. We discuss some of the challenges\nthat GPT-3 faces such as the problems of training complexity, bias, and\nhallucination/incorrect answers. We also discuss the future research\nopportunities in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zong_M/0/1/0/all/0/1\">Mingyu Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamachari_B/0/1/0/all/0/1\">Bhaskar Krishnamachari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AGRO: Adversarial Discovery of Error-prone groups for Robust Optimization. (arXiv:2212.00921v1 [cs.LG])","link":"http://arxiv.org/abs/2212.00921","description":"<p>Models trained via empirical risk minimization (ERM) are known to rely on\nspurious correlations between labels and task-independent input features,\nresulting in poor generalization to distributional shifts. Group\ndistributionally robust optimization (G-DRO) can alleviate this problem by\nminimizing the worst-case loss over a set of pre-defined groups over training\ndata. G-DRO successfully improves performance of the worst-group, where the\ncorrelation does not hold. However, G-DRO assumes that the spurious\ncorrelations and associated worst groups are known in advance, making it\nchallenging to apply it to new tasks with potentially multiple unknown spurious\ncorrelations. We propose AGRO -- Adversarial Group discovery for\nDistributionally Robust Optimization -- an end-to-end approach that jointly\nidentifies error-prone groups and improves accuracy on them. AGRO equips G-DRO\nwith an adversarial slicing model to find a group assignment for training\nexamples which maximizes worst-case loss over the discovered groups. On the\nWILDS benchmark, AGRO results in 8% higher model performance on average on\nknown worst-groups, compared to prior group discovery approaches used with\nG-DRO. AGRO also improves out-of-distribution performance on SST2, QQP, and\nMS-COCO -- datasets where potential spurious correlations are as yet\nuncharacterized. Human evaluation of ARGO groups shows that they contain\nwell-defined, yet previously unstudied spurious correlations that lead to model\nerrors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paranjape_B/0/1/0/all/0/1\">Bhargavi Paranjape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasigi_P/0/1/0/all/0/1\">Pradeep Dasigi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Nested Named Entity Recognition. (arXiv:2212.00953v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00953","description":"<p>While Named Entity Recognition (NER) is a widely studied task, making\ninferences of entities with only a few labeled data has been challenging,\nespecially for entities with nested structures. Unlike flat entities, entities\nand their nested entities are more likely to have similar semantic feature\nrepresentations, drastically increasing difficulties in classifying different\nentity categories in the few-shot setting. Although prior work has briefly\ndiscussed nested structures in the context of few-shot learning, to our best\nknowledge, this paper is the first one specifically dedicated to studying the\nfew-shot nested NER task. Leveraging contextual dependency to distinguish\nnested entities, we propose a Biaffine-based Contrastive Learning (BCL)\nframework. We first design a Biaffine span representation module for learning\nthe contextual span dependency representation for each entity span rather than\nonly learning its semantic representation. We then merge these two\nrepresentations by the residual connection to distinguish nested entities.\nFinally, we build a contrastive learning framework to adjust the representation\ndistribution for larger margin boundaries and more generalized domain transfer\nlearning ability. We conducted experimental studies on three English, German,\nand Russian nested NER datasets. The results show that the BCL outperformed\nthree baseline models on the 1-shot and 5-shot tasks in terms of F1 score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ming_H/0/1/0/all/0/1\">Hong Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaoyun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lili Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_N/0/1/0/all/0/1\">Ning An</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph. (arXiv:2212.00959v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00959","description":"<p>Multi-hop Question Answering over Knowledge Graph~(KGQA) aims to find the\nanswer entities that are multiple hops away from the topic entities mentioned\nin a natural language question on a large-scale Knowledge Graph (KG). To cope\nwith the vast search space, existing work usually adopts a two-stage approach:\nit firstly retrieves a relatively small subgraph related to the question and\nthen performs the reasoning on the subgraph to accurately find the answer\nentities. Although these two stages are highly related, previous work employs\nvery different technical solutions for developing the retrieval and reasoning\nmodels, neglecting their relatedness in task essence. In this paper, we propose\nUniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and\nreasoning in both model architecture and parameter learning. For model\narchitecture, UniKGQA consists of a semantic matching module based on a\npre-trained language model~(PLM) for question-relation semantic matching, and a\nmatching information propagation module to propagate the matching information\nalong the edges on KGs. For parameter learning, we design a shared pre-training\ntask based on question-relation matching for both retrieval and reasoning\nmodels, and then propose retrieval- and reasoning-oriented fine-tuning\nstrategies. Compared with previous studies, our approach is more unified,\ntightly relating the retrieval and reasoning stages. Extensive experiments on\nthree benchmark datasets have demonstrated the effectiveness of our method on\nthe multi-hop KGQA task. Our codes and data are publicly available at\nhttps://github.com/RUCAIBox/UniKGQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jinhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation-aware Language-Graph Transformer for Question Answering. (arXiv:2212.00975v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00975","description":"<p>Question Answering (QA) is a task that entails reasoning over natural\nlanguage contexts, and many relevant works augment language models (LMs) with\ngraph neural networks (GNNs) to encode the Knowledge Graph (KG) information.\nHowever, most existing GNN-based modules for QA do not take advantage of rich\nrelational information of KGs and depend on limited information interaction\nbetween the LM and the KG. To address these issues, we propose Question\nAnswering Transformer (QAT), which is designed to jointly reason over language\nand graphs with respect to entity relations in a unified manner. Specifically,\nQAT constructs Meta-Path tokens, which learn relation-centric embeddings based\non diverse structural and semantic relations. Then, our Relation-Aware\nSelf-Attention module comprehensively integrates different modalities via the\nCross-Modal Relative Position Bias, which guides information exchange between\nrelevant entities of different modalities. We validate the effectiveness of QAT\non commonsense question answering datasets like CommonsenseQA and OpenBookQA,\nand on a medical question answering dataset, MedQA-USMLE. On all the datasets,\nour method achieves state-of-the-art performance. Our code is available at\n<a href=\"http://github.com/mlvlab/QAT.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinyoung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hyeong Kyu Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1\">Juyeon Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyeonjin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Ji-Hoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jisu Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyungmin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunwoo J. Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Contrastive Pre-Training for Efficient Video-Text Retrieval. (arXiv:2212.00986v1 [cs.CV])","link":"http://arxiv.org/abs/2212.00986","description":"<p>We present a simple yet effective end-to-end Video-language Pre-training\n(VidLP) framework, Masked Contrastive Video-language Pretraining (MAC), for\nvideo-text retrieval tasks. Our MAC aims to reduce video representation's\nspatial and temporal redundancy in the VidLP model by a mask sampling mechanism\nto improve pre-training efficiency. Comparing conventional temporal sparse\nsampling, we propose to randomly mask a high ratio of spatial regions and only\nfeed visible regions into the encoder as sparse spatial sampling. Similarly, we\nadopt the mask sampling technique for text inputs for consistency. Instead of\nblindly applying the mask-then-prediction paradigm from MAE, we propose a\nmasked-then-alignment paradigm for efficient video-text alignment. The\nmotivation is that video-text retrieval tasks rely on high-level alignment\nrather than low-level reconstruction, and multimodal alignment with masked\nmodeling encourages the model to learn a robust and general multimodal\nrepresentation from incomplete and unstable inputs. Coupling these designs\nenables efficient end-to-end pre-training: reduce FLOPs (60% off), accelerate\npre-training (by 3x), and improve performance. Our MAC achieves\nstate-of-the-art results on various video-text retrieval datasets, including\nMSR-VTT, DiDeMo, and ActivityNet. Our approach is omnivorous to input\nmodalities. With minimal modifications, we achieve competitive results on\nimage-text retrieval tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shu_F/0/1/0/all/0/1\">Fangxun Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Biaolong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yue Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1\">Ke Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shuwen Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wenyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaobo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yousong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinqiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AGO: Boosting Mobile AI Inference Performance by Removing Constraints on Graph Optimization. (arXiv:2212.01005v1 [cs.LG])","link":"http://arxiv.org/abs/2212.01005","description":"<p>Traditional deep learning compilers rely on heuristics for subgraph\ngeneration, which impose extra constraints on graph optimization, e.g., each\nsubgraph can only contain at most one complex operator. In this paper, we\npropose AGO, a framework for graph optimization with arbitrary structures to\nboost the inference performance of deep models by removing such constraints. To\ncreate new optimization opportunities for complicated subgraphs, we propose\nintensive operator fusion, which can effectively stitch multiple complex\noperators together for better performance. Further, we design a graph\npartitioning scheme that allows an arbitrary structure for each subgraph while\nguaranteeing the acyclic property among all generated subgraphs. Additionally,\nto enable efficient performance tuning on complicated subgraphs, we devise a\nnovel divide-and-conquer tuning mechanism to orchestrate different system\ncomponents. Through extensive experiments on various neural networks and mobile\ndevices, we show that our system can improve the inference performance by up to\n3.3x when compared with state-of-the-art deep compilers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiying Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hongding Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"General Framework for Self-Supervised Model Priming for Parameter-Efficient Fine-tuning. (arXiv:2212.01032v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01032","description":"<p>Parameter-efficient methods (like Prompt or Adapters) for adapting\npre-trained language models to downstream tasks have been popular recently.\nHowever, hindrances still prevent these methods from reaching their full\npotential. For example, two significant challenges are few-shot adaptation and\ncross-task generalization ability. To tackle these issues, we propose a general\nframework to enhance the few-shot adaptation and cross-domain generalization\nability of parameter-efficient methods. In our framework, we prime the\nself-supervised model for parameter-efficient methods to rapidly adapt to\nvarious downstream few-shot tasks. To evaluate the authentic generalization\nability of these parameter-efficient methods, we conduct experiments on a\nfew-shot cross-domain benchmark containing 160 diverse NLP tasks. The\nexperiment result reveals that priming by tuning PLM only with extra training\ntasks leads to the best performance. Also, we perform a comprehensive analysis\nof various parameter-efficient methods under few-shot cross-domain scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shih-Cheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shih-Heng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shih_M/0/1/0/all/0/1\">Min-Han Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahay_S/0/1/0/all/0/1\">Saurav Sahay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SoftCorrect: Error Correction with Soft Detection for Automatic Speech Recognition. (arXiv:2212.01039v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01039","description":"<p>Error correction in automatic speech recognition (ASR) aims to correct those\nincorrect words in sentences generated by ASR models. Since recent ASR models\nusually have low word error rate (WER), to avoid affecting originally correct\ntokens, error correction models should only modify incorrect words, and\ntherefore detecting incorrect words is important for error correction. Previous\nworks on error correction either implicitly detect error words through\ntarget-source attention or CTC (connectionist temporal classification) loss, or\nexplicitly locate specific deletion/substitution/insertion errors. However,\nimplicit error detection does not provide clear signal about which tokens are\nincorrect and explicit error detection suffers from low detection accuracy. In\nthis paper, we propose SoftCorrect with a soft error detection mechanism to\navoid the limitations of both explicit and implicit error detection.\nSpecifically, we first detect whether a token is correct or not through a\nprobability produced by a dedicatedly designed language model, and then design\na constrained CTC loss that only duplicates the detected incorrect tokens to\nlet the decoder focus on the correction of error tokens. Compared with implicit\nerror detection with CTC loss, SoftCorrect provides explicit signal about which\nwords are incorrect and thus does not need to duplicate every token but only\nincorrect tokens; compared with explicit error detection, SoftCorrect does not\ndetect specific deletion/substitution/insertion errors but just leaves it to\nCTC loss. Experiments on AISHELL-1 and Aidatatang datasets show that\nSoftCorrect achieves 26.1% and 9.4% CER reduction respectively, outperforming\nprevious works by a large margin, while still enjoying fast speed of parallel\ngeneration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang-Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1\">Edward Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Faithful Rationale for Multi-hop Fact Verification via Salience-Aware Graph Learning. (arXiv:2212.01060v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01060","description":"<p>The opaqueness of the multi-hop fact verification model imposes imperative\nrequirements for explainability. One feasible way is to extract rationales, a\nsubset of inputs, where the performance of prediction drops dramatically when\nbeing removed. Though being explainable, most rationale extraction methods for\nmulti-hop fact verification explore the semantic information within each piece\nof evidence individually, while ignoring the topological information\ninteraction among different pieces of evidence. Intuitively, a faithful\nrationale bears complementary information being able to extract other\nrationales through the multi-hop reasoning process. To tackle such\ndisadvantages, we cast explainable multi-hop fact verification as subgraph\nextraction, which can be solved based on graph convolutional network (GCN) with\nsalience-aware graph learning. In specific, GCN is utilized to incorporate the\ntopological interaction information among multiple pieces of evidence for\nlearning evidence representation. Meanwhile, to alleviate the influence of\nnoisy evidence, the salience-aware graph perturbation is induced into the\nmessage passing of GCN. Moreover, the multi-task model with three diagnostic\nproperties of rationale is elaborately designed to improve the quality of an\nexplanation without any explicit annotations. Experimental results on the\nFEVEROUS benchmark show significant gains over previous state-of-the-art\nmethods for both rationale extraction and fact verification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_J/0/1/0/all/0/1\">Jiasheng Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yingjie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Deyu Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modal Mutual Learning for Cued Speech Recognition. (arXiv:2212.01083v1 [cs.CV])","link":"http://arxiv.org/abs/2212.01083","description":"<p>Automatic Cued Speech Recognition (ACSR) provides an intelligent\nhuman-machine interface for visual communications, where the Cued Speech (CS)\nsystem utilizes lip movements and hand gestures to code spoken language for\nhearing-impaired people. Previous ACSR approaches often utilize direct feature\nconcatenation as the main fusion paradigm. However, the asynchronous modalities\n(\\textit{i.e.}, lip, hand shape and hand position) in CS may cause interference\nfor feature concatenation. To address this challenge, we propose a transformer\nbased cross-modal mutual learning framework to prompt multi-modal interaction.\nCompared with the vanilla self-attention, our model forces modality-specific\ninformation of different modalities to pass through a modality-invariant\ncodebook, collating linguistic representations for tokens of each modality.\nThen the shared linguistic knowledge is used to re-synchronize multi-modal\nsequences. Moreover, we establish a novel large-scale multi-speaker CS dataset\nfor Mandarin Chinese. To our knowledge, this is the first work on ACSR for\nMandarin Chinese. Extensive experiments are conducted for different languages\n(\\textit{i.e.}, Chinese, French, and British English). Results demonstrate that\nour model exhibits superior recognition performance to the state-of-the-art by\na large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Role Labeling Meets Definition Modeling: Using Natural Language to Describe Predicate-Argument Structures. (arXiv:2212.01094v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01094","description":"<p>One of the common traits of past and present approaches for Semantic Role\nLabeling (SRL) is that they rely upon discrete labels drawn from a predefined\nlinguistic inventory to classify predicate senses and their arguments. However,\nwe argue this need not be the case. In this paper, we present an approach that\nleverages Definition Modeling to introduce a generalized formulation of SRL as\nthe task of describing predicate-argument structures using natural language\ndefinitions instead of discrete labels. Our novel formulation takes a first\nstep towards placing interpretability and flexibility foremost, and yet our\nexperiments and analyses on PropBank-style and FrameNet-style, dependency-based\nand span-based SRL also demonstrate that a flexible model with an interpretable\noutput does not necessarily come at the expense of performance. We release our\nsoftware for research purposes at https://github.com/SapienzaNLP/dsrl.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Conia_S/0/1/0/all/0/1\">Simone Conia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barba_E/0/1/0/all/0/1\">Edoardo Barba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scire_A/0/1/0/all/0/1\">Alessandro Scir&#xe8;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navigli_R/0/1/0/all/0/1\">Roberto Navigli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning. (arXiv:2212.01117v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01117","description":"<p>The spread of rumors along with breaking events seriously hinders the truth\nin the era of social media. Previous studies reveal that due to the lack of\nannotated resources, rumors presented in minority languages are hard to be\ndetected. Furthermore, the unforeseen breaking events not involved in\nyesterday's news exacerbate the scarcity of data resources. In this work, we\npropose a novel zero-shot framework based on prompt learning to detect rumors\nfalling in different domains or presented in different languages. More\nspecifically, we firstly represent rumor circulated on social media as diverse\npropagation threads, then design a hierarchical prompt encoding mechanism to\nlearn language-agnostic contextual representations for both prompts and rumor\ndata. To further enhance domain adaptation, we model the domain-invariant\nstructural features from the propagation threads, to incorporate structural\nposition representations of influential community response. In addition, a new\nvirtual response augmentation method is used to improve model training.\nExtensive experiments conducted on three real-world datasets demonstrate that\nour proposed model achieves much better performance than state-of-the-art\nmethods and exhibits a superior capacity for detecting rumors at early stages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongzhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_P/0/1/0/all/0/1\">Pengyao Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Ziyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruifang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tackling Low-Resourced Sign Language Translation: UPC at WMT-SLT 22. (arXiv:2212.01140v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01140","description":"<p>This paper describes the system developed at the Universitat Polit\\`ecnica de\nCatalunya for the Workshop on Machine Translation 2022 Sign Language\nTranslation Task, in particular, for the sign-to-text direction. We use a\nTransformer model implemented with the Fairseq modeling toolkit. We have\nexperimented with the vocabulary size, data augmentation techniques and\npretraining the model with the PHOENIX-14T dataset. Our system obtains 0.50\nBLEU score for the test set, improving the organizers' baseline by 0.38 BLEU.\nWe remark the poor results for both the baseline and our system, and thus, the\nunreliability of our findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tarres_L/0/1/0/all/0/1\">Laia Tarr&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe0;llego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1\">Xavier Gir&#xf3;-i-Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torres_J/0/1/0/all/0/1\">Jordi Torres</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Diverse, Relevant and Coherent Open-Domain Dialogue Generation via Hybrid Latent Variables. (arXiv:2212.01145v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01145","description":"<p>Conditional variational models, using either continuous or discrete latent\nvariables, are powerful for open-domain dialogue response generation. However,\nprevious works show that continuous latent variables tend to reduce the\ncoherence of generated responses. In this paper, we also found that discrete\nlatent variables have difficulty capturing more diverse expressions. To tackle\nthese problems, we combine the merits of both continuous and discrete latent\nvariables and propose a Hybrid Latent Variable (HLV) method. Specifically, HLV\nconstrains the global semantics of responses through discrete latent variables\nand enriches responses with continuous latent variables. Thus, we diversify the\ngenerated responses while maintaining relevance and coherence. In addition, we\npropose Conditional Hybrid Variational Transformer (CHVT) to construct and to\nutilize HLV with transformers for dialogue generation. Through fine-grained\nsymbolic-level semantic information and additive Gaussian mixing, we construct\nthe distribution of continuous variables, prompting the generation of diverse\nexpressions. Meanwhile, to maintain the relevance and coherence, the discrete\nlatent variable is optimized by self-separation training. Experimental results\non two dialogue generation datasets (DailyDialog and Opensubtitles) show that\nCHVT is superior to traditional transformer-based variational mechanism w.r.t.\ndiversity, relevance and coherence metrics. Moreover, we also demonstrate the\nbenefit of applying HLV to fine-tuning two pre-trained dialogue models (PLATO\nand BART-base).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SumREN: Summarizing Reported Speech about Events in News. (arXiv:2212.01146v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01146","description":"<p>A primary objective of news articles is to establish the factual record for\nan event, frequently achieved by conveying both the details of the specified\nevent (i.e., the 5 Ws; Who, What, Where, When and Why regarding the event) and\nhow people reacted to it (i.e., reported statements). However, existing work on\nnews summarization almost exclusively focuses on the event details. In this\nwork, we propose the novel task of summarizing the reactions of different\nspeakers, as expressed by their reported statements, to a given event. To this\nend, we create a new multi-document summarization benchmark, SUMREN, comprising\n745 summaries of reported statements from various public figures obtained from\n633 news articles discussing 132 events. We propose an automatic silver\ntraining data generation approach for our task, which helps smaller models like\nBART achieve GPT-3 level performance on this task. Finally, we introduce a\npipeline-based framework for summarizing reported speech, which we empirically\nshow to generate summaries that are more abstractive and factual than baseline\nquery-focused summarization approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1\">Revanth Gangi Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elfardy_H/0/1/0/all/0/1\">Heba Elfardy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Hou Pong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Small_K/0/1/0/all/0/1\">Kevin Small</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surrogate Gradient Spiking Neural Networks as Encoders for Large Vocabulary Continuous Speech Recognition. (arXiv:2212.01187v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01187","description":"<p>Compared to conventional artificial neurons that produce dense and\nreal-valued responses, biologically-inspired spiking neurons transmit sparse\nand binary information, which can also lead to energy-efficient\nimplementations. Recent research has shown that spiking neural networks can be\ntrained like standard recurrent neural networks using the surrogate gradient\nmethod. They have shown promising results on speech command recognition tasks.\nUsing the same technique, we show that they are scalable to large vocabulary\ncontinuous speech recognition, where they are capable of replacing LSTMs in the\nencoder with only minor loss of performance. This suggests that they may be\napplicable to more involved sequence-to-sequence tasks. Moreover, in contrast\nto their recurrent non-spiking counterparts, they show robustness to exploding\ngradient problems without the need to use gates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bittar_A/0/1/0/all/0/1\">Alexandre Bittar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garner_P/0/1/0/all/0/1\">Philip N. Garner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Simultaneous Machine Translation with Monolingual Data. (arXiv:2212.01188v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01188","description":"<p>Simultaneous machine translation (SiMT) is usually done via sequence-level\nknowledge distillation (Seq-KD) from a full-sentence neural machine translation\n(NMT) model. However, there is still a significant performance gap between NMT\nand SiMT. In this work, we propose to leverage monolingual data to improve\nSiMT, which trains a SiMT student on the combination of bilingual data and\nexternal monolingual data distilled by Seq-KD. Preliminary experiments on En-Zh\nand En-Ja news domain corpora demonstrate that monolingual data can\nsignificantly improve translation quality (e.g., +3.15 BLEU on En-Zh). Inspired\nby the behavior of human simultaneous interpreters, we propose a novel\nmonolingual sampling strategy for SiMT, considering both chunk length and\nmonotonicity. Experimental results show that our sampling strategy consistently\noutperforms the random sampling strategy (and other conventional typical NMT\nmonolingual sampling strategies) by avoiding the key problem of SiMT --\nhallucination, and has better scalability. We achieve +0.72 BLEU improvements\non average against random sampling on En-Zh and En-Ja. Data and codes can be\nfound at https://github.com/hexuandeng/Mono4SiMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Hexuan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Open Knowledge Base Canonicalization and Linking. (arXiv:2212.01207v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01207","description":"<p>Open Information Extraction (OIE) methods extract a large number of OIE\ntriples (noun phrase, relation phrase, noun phrase) from text, which compose\nlarge Open Knowledge Bases (OKBs). However, noun phrases (NPs) and relation\nphrases (RPs) in OKBs are not canonicalized and often appear in different\nparaphrased textual variants, which leads to redundant and ambiguous facts. To\naddress this problem, there are two related tasks: OKB canonicalization (i.e.,\nconvert NPs and RPs to canonicalized form) and OKB linking (i.e., link NPs and\nRPs with their corresponding entities and relations in a curated Knowledge Base\n(e.g., DBPedia). These two tasks are tightly coupled, and one task can benefit\nsignificantly from the other. However, they have been studied in isolation so\nfar. In this paper, we explore the task of joint OKB canonicalization and\nlinking for the first time, and propose a novel framework JOCL based on factor\ngraph model to make them reinforce each other. JOCL is flexible enough to\ncombine different signals from both tasks, and able to extend to fit any new\nsignals. A thorough experimental study over two large scale OIE triple data\nsets shows that our framework outperforms all the baseline methods for the task\nof OKB canonicalization (OKB linking) in terms of average F1 (accuracy).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuanfei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenglu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaojie Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Large Pre-Trained Language Model to Assist FDA in Premarket Medical Device. (arXiv:2212.01217v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01217","description":"<p>This paper proposes a possible method using natural language processing that\nmight assist in the FDA medical device marketing process. Actual device\ndescriptions are taken and matched with the device description in FDA Title 21\nof CFR to determine their corresponding device type. Both pre-trained word\nembeddings such as FastText and large pre-trained sentence embedding models\nsuch as sentence transformers are evaluated on their accuracy in characterizing\na piece of device description. An experiment is also done to test whether these\nmodels can identify the devices wrongly classified in the FDA database. The\nresult shows that sentence transformer with T5 and MPNet and GPT-3 semantic\nsearch embedding show high accuracy in identifying the correct classification\nby narrowing down the correct label to be contained in the first 15 most likely\nresults, as compared to 2585 types of device descriptions that must be manually\nsearched through. On the other hand, all methods demonstrate high accuracy in\nidentifying completely incorrectly labeled devices, but all fail to identify\nfalse device classifications that are wrong but closely related to the true\nlabel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zongzhe Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Answer ranking in Community Question Answering: a deep learning approach. (arXiv:2212.01218v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01218","description":"<p>Community Question Answering is the field of computational linguistics that\ndeals with problems derived from the questions and answers posted to websites\nsuch as Quora or Stack Overflow. Among some of these problems we find the issue\nof ranking the multiple answers posted in reply to each question by how\ninformative they are in the attempt to solve the original question. This work\ntries to advance the state of the art on answer ranking for community Question\nAnswering by proceeding with a deep learning approach. We started off by\ncreating a large data set of questions and answers posted to the Stack Overflow\nwebsite.\n</p>\n<p>We then leveraged the natural language processing capabilities of dense\nembeddings and LSTM networks to produce a prediction for the accepted answer\nattribute, and present the answers in a ranked form ordered by how likely they\nare to be marked as accepted by the question asker. We also produced a set of\nnumerical features to assist with the answer ranking task. These numerical\nfeatures were either extracted from metadata found in the Stack Overflow posts\nor derived from the questions and answers texts. We compared the performance of\nour deep learning models against a set of forest and boosted trees ensemble\nmethods and found that our models could not improve the best baseline results.\nWe speculate that this lack of performance improvement versus the baseline\nmodels may be caused by the large number of out of vocabulary words present in\nthe programming code snippets found in the questions and answers text. We\nconclude that while a deep learning approach may be helpful in answer ranking\nproblems new methods should be developed to assist with the large number of out\nof vocabulary words present in the programming code snippets\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valentin_L/0/1/0/all/0/1\">Lucas Valentin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CHAPTER: Exploiting Convolutional Neural Network Adapters for Self-supervised Speech Models. (arXiv:2212.01282v1 [eess.AS])","link":"http://arxiv.org/abs/2212.01282","description":"<p>Self-supervised learning (SSL) is a powerful technique for learning\nrepresentations from unlabeled data. Transformer based models such as HuBERT,\nwhich consist a feature extractor and transformer layers, are leading the field\nin the speech domain. SSL models are fine-tuned on a wide range of downstream\ntasks, which involves re-training the majority of the model for each task.\nPrevious studies have introduced applying adapters, which are small lightweight\nmodules commonly used in Natural Language Processing (NLP) to adapt pre-trained\nmodels to new tasks. However, such efficient tuning techniques only provide\nadaptation at the transformer layer, but failed to perform adaptation at the\nfeature extractor. In this paper, we propose CHAPTER, an efficient tuning\nmethod specifically designed for SSL speech model, by applying CNN adapters at\nthe feature extractor. Using this method, we can only fine-tune fewer than 5%\nof parameters per task compared to fully fine-tuning and achieve better and\nmore stable performance. We empirically found that adding CNN adapters to the\nfeature extractor can help the adaptation on emotion and speaker tasks. For\ninstance, the accuracy of SID is improved from 87.71 to 91.56, and the accuracy\nof ER is improved by 5%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zih-Ching Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sung_Y/0/1/0/all/0/1\">Yu-Shun Sung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subword-Delimited Downsampling for Better Character-Level Translation. (arXiv:2212.01304v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01304","description":"<p>Subword-level models have been the dominant paradigm in NLP. However,\ncharacter-level models have the benefit of seeing each character individually,\nproviding the model with more detailed information that ultimately could lead\nto better models. Recent works have shown character-level models to be\ncompetitive with subword models, but costly in terms of time and computation.\nCharacter-level models with a downsampling component alleviate this, but at the\ncost of quality, particularly for machine translation. This work analyzes the\nproblems of previous downsampling methods and introduces a novel downsampling\nmethod which is informed by subwords. This new downsampling method not only\noutperforms existing downsampling methods, showing that downsampling characters\ncan be done without sacrificing quality, but also leads to promising\nperformance compared to subword models for translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Edman_L/0/1/0/all/0/1\">Lukas Edman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toral_A/0/1/0/all/0/1\">Antonio Toral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noord_G/0/1/0/all/0/1\">Gertjan van Noord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Legal Prompting: Teaching a Language Model to Think Like a Lawyer. (arXiv:2212.01326v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01326","description":"<p>Large language models that are capable of zero or few-shot prompting\napproaches have given rise to the new research area of prompt engineering.\nRecent advances showed that for example Chain-of-Thought (CoT) prompts can\nimprove arithmetic or common sense tasks significantly. We explore how such\napproaches fair with legal reasoning tasks and take the COLIEE entailment task\nbased on the Japanese Bar exam for testing zero-shot/few-shot and fine-tuning\napproaches. Our findings show that while CoT prompting and fine-tuning with\nexplanations approaches show improvements, the best results are produced by\nprompts that are derived from specific legal reasoning techniques such as IRAC\n(Issue, Rule, Application, Conclusion). Based on our experiments we improve the\n2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 best\nsystem of 0.6789 accuracy with an accuracy of 0.7431.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fangyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quartey_L/0/1/0/all/0/1\">Lee Quartey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schilder_F/0/1/0/all/0/1\">Frank Schilder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Moving Beyond Downstream Task Accuracy for Information Retrieval Benchmarking. (arXiv:2212.01340v1 [cs.IR])","link":"http://arxiv.org/abs/2212.01340","description":"<p>Neural information retrieval (IR) systems have progressed rapidly in recent\nyears, in large part due to the release of publicly available benchmarking\ntasks. Unfortunately, some dimensions of this progress are illusory: the\nmajority of the popular IR benchmarks today focus exclusively on downstream\ntask accuracy and thus conceal the costs incurred by systems that trade away\nefficiency for quality. Latency, hardware cost, and other efficiency\nconsiderations are paramount to the deployment of IR systems in user-facing\nsettings. We propose that IR benchmarks structure their evaluation methodology\nto include not only metrics of accuracy, but also efficiency considerations\nsuch as a query latency and the corresponding cost budget for a reproducible\nhardware setting. For the popular IR benchmarks MS MARCO and XOR-TyDi, we show\nhow the best choice of IR system varies according to how these efficiency\nconsiderations are chosen and weighed. We hope that future benchmarks will\nadopt these guidelines toward more holistic IR evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santhanam_K/0/1/0/all/0/1\">Keshav Santhanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saad_Falcon_J/0/1/0/all/0/1\">Jon Saad-Falcon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franz_M/0/1/0/all/0/1\">Martin Franz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1\">Omar Khattab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1\">Avirup Sil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florian_R/0/1/0/all/0/1\">Radu Florian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sultan_M/0/1/0/all/0/1\">Md Arafat Sultan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roukos_S/0/1/0/all/0/1\">Salim Roukos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nonparametric Masked Language Modeling. (arXiv:2212.01349v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01349","description":"<p>Existing language models (LMs) predict tokens with a softmax over a finite\nvocabulary, which can make it difficult to predict rare tokens or phrases. We\nintroduce NPM, the first nonparametric masked language model that replaces this\nsoftmax with a nonparametric distribution over every phrase in a reference\ncorpus. We show that NPM can be efficiently trained with a contrastive\nobjective and an in-batch approximation to full corpus retrieval. Zero-shot\nevaluation on 9 closed-set tasks and 7 open-set tasks demonstrates that NPM\noutperforms significantly larger parametric models, either with or without a\nretrieve-and-generate approach. It is particularly better on dealing with rare\npatterns (word senses or facts), and predicting rare or nearly unseen words\n(e.g., non-Latin script). We release the model and code at\ngithub.com/facebookresearch/NPM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weijia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Iterative Text Revision by Learning Where to Edit from Other Revision Tasks. (arXiv:2212.01350v1 [cs.CL])","link":"http://arxiv.org/abs/2212.01350","description":"<p>Iterative text revision improves text quality by fixing grammatical errors,\nrephrasing for better readability or contextual appropriateness, or\nreorganizing sentence structures throughout a document. Most recent research\nhas focused on understanding and classifying different types of edits in the\niterative revision process from human-written text instead of building accurate\nand robust systems for iterative text revision. In this work, we aim to build\nan end-to-end text revision system that can iteratively generate helpful edits\nby explicitly detecting editable spans (where-to-edit) with their corresponding\nedit intents and then instructing a revision model to revise the detected edit\nspans. Leveraging datasets from other related text editing NLP tasks, combined\nwith the specification of editable spans, leads our system to more accurately\nmodel the process of iterative text refinement, as evidenced by empirical\nresults and human evaluations. Our system significantly outperforms previous\nbaselines on our text revision tasks and other standard text revision tasks,\nincluding grammatical error correction, text simplification, sentence fusion,\nand style transfer. Through extensive qualitative and quantitative analysis, we\nmake vital connections between edit intentions and writing quality, and better\ncomputational modeling of iterative text revisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Z/0/1/0/all/0/1\">Zae Myung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wanyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raheja_V/0/1/0/all/0/1\">Vipul Raheja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1\">Dhruv Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning. (arXiv:2212.01378v1 [cs.LG])","link":"http://arxiv.org/abs/2212.01378","description":"<p>Pretraining has been shown to scale well with compute, data size and data\ndiversity. Multitask learning trains on a mixture of supervised datasets and\nproduces improved performance compared to self-supervised pretraining. Until\nnow, massively multitask learning required simultaneous access to all datasets\nin the mixture and heavy compute resources that are only available to\nwell-resourced teams.\n</p>\n<p>In this paper, we propose ColD Fusion, a method that provides the benefits of\nmultitask learning but leverages distributed computation and requires limited\ncommunication and no sharing of data. Consequentially, ColD Fusion can create a\nsynergistic loop, where finetuned models can be recycled to continually improve\nthe pretrained model they are based on. We show that ColD Fusion yields\ncomparable benefits to multitask pretraining by producing a model that (a)\nattains strong performance on all of the datasets it was multitask trained on\nand (b) is a better starting point for finetuning on unseen datasets. We find\nColD Fusion outperforms RoBERTa and even previous multitask models.\nSpecifically, when training and testing on 35 diverse datasets, ColD\nFusion-based model outperforms RoBERTa by 2.45 points in average without any\nchanges to the architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Don_Yehiya_S/0/1/0/all/0/1\">Shachar Don-Yehiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venezian_E/0/1/0/all/0/1\">Elad Venezian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1\">Yoav Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Watch Those Words: Video Falsification Detection Using Word-Conditioned Facial Motion. (arXiv:2112.10936v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10936","description":"<p>In today's era of digital misinformation, we are increasingly faced with new\nthreats posed by video falsification techniques. Such falsifications range from\ncheapfakes (e.g., lookalikes or audio dubbing) to deepfakes (e.g.,\nsophisticated AI media synthesis methods), which are becoming perceptually\nindistinguishable from real videos. To tackle this challenge, we propose a\nmulti-modal semantic forensic approach to discover clues that go beyond\ndetecting discrepancies in visual quality, thereby handling both simpler\ncheapfakes and visually persuasive deepfakes. In this work, our goal is to\nverify that the purported person seen in the video is indeed themselves by\ndetecting anomalous facial movements corresponding to the spoken words. We\nleverage the idea of attribution to learn person-specific biometric patterns\nthat distinguish a given speaker from others. We use interpretable Action Units\n(AUs) to capture a person's face and head movement as opposed to deep CNN\nfeatures, and we are the first to use word-conditioned facial motion analysis.\nWe further demonstrate our method's effectiveness on a range of fakes not seen\nin training including those without video manipulation, that were not addressed\nin prior work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Shruti Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Liwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_E/0/1/0/all/0/1\">Evonne Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation. (arXiv:2204.06674v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06674","description":"<p>Recent improvements in KG-to-text generation are due to additional auxiliary\npre-training tasks designed to give the fine-tune task a boost in performance.\nThese tasks require extensive computational resources while only suggesting\nmarginal improvements. Here, we demonstrate that by fusing graph-aware elements\ninto existing pre-trained language models, we are able to outperform\nstate-of-the-art models and close the gap imposed by additional pre-training\ntasks. We do so by proposing a mask structure to capture neighborhood\ninformation and a novel type encoder that adds a bias to the graph-attention\nweights depending on the connection type. Experiments on two KG-to-text\nbenchmark datasets show our models are competitive while involving fewer\nparameters and no additional pre-training tasks. By formulating the problem as\na framework, we can interchange the various proposed components and begin\ninterpreting KG-to-text generative models based on the topological and type\ninformation found in a graph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colas_A/0/1/0/all/0/1\">Anthony Colas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvandipour_M/0/1/0/all/0/1\">Mehrdad Alvandipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daisy Zhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PASH at TREC 2021 Deep Learning Track: Generative Enhanced Model for Multi-stage Ranking. (arXiv:2205.11245v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2205.11245","description":"<p>This paper describes the PASH participation in TREC 2021 Deep Learning Track.\nIn the recall stage, we adopt a scheme combining sparse and dense retrieval\nmethod. In the multi-stage ranking phase, point-wise and pair-wise ranking\nstrategies are used one after another based on model continual pre-trained on\ngeneral knowledge and document-level data. Compared to TREC 2020 Deep Learning\nTrack, we have additionally introduced the generative model T5 to further\nenhance the performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yixuan Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yongquan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tuozhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xianbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1\">Rui Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Wenfeng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guotong Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chunk-aware Alignment and Lexical Constraint for Visual Entailment with Natural Language Explanations. (arXiv:2207.11401v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.11401","description":"<p>Visual Entailment with natural language explanations aims to infer the\nrelationship between a text-image pair and generate a sentence to explain the\ndecision-making process. Previous methods rely mainly on a pre-trained\nvision-language model to perform the relation inference and a language model to\ngenerate the corresponding explanation. However, the pre-trained\nvision-language models mainly build token-level alignment between text and\nimage yet ignore the high-level semantic alignment between the phrases (chunks)\nand visual contents, which is critical for vision-language reasoning. Moreover,\nthe explanation generator based only on the encoded joint representation does\nnot explicitly consider the critical decision-making points of relation\ninference. Thus the generated explanations are less faithful to visual-language\nreasoning. To mitigate these problems, we propose a unified Chunk-aware\nAlignment and Lexical Constraint based method, dubbed as CALeC. It contains a\nChunk-aware Semantic Interactor (arr. CSI), a relation inferrer, and a Lexical\nConstraint-aware Generator (arr. LeCG). Specifically, CSI exploits the sentence\nstructure inherent in language and various image regions to build chunk-aware\nsemantic alignment. Relation inferrer uses an attention-based reasoning network\nto incorporate the token-level and chunk-level vision-language representations.\nLeCG utilizes lexical constraints to expressly incorporate the words or chunks\nfocused by the relation inferrer into explanation generation, improving the\nfaithfulness and informativeness of the explanations. We conduct extensive\nexperiments on three datasets, and experimental results indicate that CALeC\nsignificantly outperforms other competitor models on inference accuracy and\nquality of generated explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Baotian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuxing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConTextual Masked Auto-Encoder for Dense Passage Retrieval. (arXiv:2208.07670v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.07670","description":"<p>Dense passage retrieval aims to retrieve the relevant passages of a query\nfrom a large corpus based on dense representations (i.e., vectors) of the query\nand the passages. Recent studies have explored improving pre-trained language\nmodels to boost dense retrieval performance. This paper proposes CoT-MAE\n(ConTextual Masked Auto-Encoder), a simple yet effective generative\npre-training method for dense passage retrieval. CoT-MAE employs an asymmetric\nencoder-decoder architecture that learns to compress the sentence semantics\ninto a dense vector through self-supervised and context-supervised masked\nauto-encoding. Precisely, self-supervised masked auto-encoding learns to model\nthe semantics of the tokens inside a text span, and context-supervised masked\nauto-encoding learns to model the semantical correlation between the text\nspans. We conduct experiments on large-scale passage retrieval benchmarks and\nshow considerable improvements over strong baselines, demonstrating the high\nefficiency of CoT-MAE. Our code is available at\nhttps://github.com/caskcsg/ir/tree/main/cotmae.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1\">Guangyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Meng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zijia Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songlin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Textual Entailment Recognition with Semantic Features from Empirical Text Representation. (arXiv:2210.09723v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.09723","description":"<p>Textual entailment recognition is one of the basic natural language\nunderstanding(NLU) tasks. Understanding the meaning of sentences is a\nprerequisite before applying any natural language processing(NLP) techniques to\nautomatically recognize the textual entailment. A text entails a hypothesis if\nand only if the true value of the hypothesis follows the text. Classical\napproaches generally utilize the feature value of each word from word embedding\nto represent the sentences. In this paper, we propose a novel approach to\nidentifying the textual entailment relationship between text and hypothesis,\nthereby introducing a new semantic feature focusing on empirical\nthreshold-based semantic text representation. We employ an element-wise\nManhattan distance vector-based feature that can identify the semantic\nentailment relationship between the text-hypothesis pair. We carried out\nseveral experiments on a benchmark entailment classification(SICK-RTE) dataset.\nWe train several machine learning(ML) algorithms applying both semantic and\nlexical features to classify the text-hypothesis pair as entailment, neutral,\nor contradiction. Our empirical sentence representation technique enriches the\nsemantic information of the texts and hypotheses found to be more efficient\nthan the classical ones. In the end, our approach significantly outperforms\nknown methods in understanding the meaning of the sentences for the textual\nentailment classification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shajalal_M/0/1/0/all/0/1\">Md Shajalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atabuzzaman_M/0/1/0/all/0/1\">Md Atabuzzaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baby_M/0/1/0/all/0/1\">Maksuda Bilkis Baby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Md Rezaul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boden_A/0/1/0/all/0/1\">Alexander Boden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eye-tracking based classification of Mandarin Chinese readers with and without dyslexia using neural sequence models. (arXiv:2210.09819v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.09819","description":"<p>Eye movements are known to reflect cognitive processes in reading, and\npsychological reading research has shown that eye gaze patterns differ between\nreaders with and without dyslexia. In recent years, researchers have attempted\nto classify readers with dyslexia based on their eye movements using Support\nVector Machines (SVMs). However, these approaches (i) are based on highly\naggregated features averaged over all words read by a participant, thus\ndisregarding the sequential nature of the eye movements, and (ii) do not\nconsider the linguistic stimulus and its interaction with the reader's eye\nmovements. In the present work, we propose two simple sequence models that\nprocess eye movements on the entire stimulus without the need of aggregating\nfeatures across the sentence. Additionally, we incorporate the linguistic\nstimulus into the model in two ways -- contextualized word embeddings and\nmanually extracted linguistic features. The models are evaluated on a Mandarin\nChinese dataset containing eye movements from children with and without\ndyslexia. Our results show that (i) even for a logographic script such as\nChinese, sequence models are able to classify dyslexia on eye gaze sequences,\nreaching state-of-the-art performance, and (ii) incorporating the linguistic\nstimulus does not help to improve classification performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haller_P/0/1/0/all/0/1\">Patrick Haller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sauberli_A/0/1/0/all/0/1\">Andreas S&#xe4;uberli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiener_S/0/1/0/all/0/1\">Sarah Elisabeth Kiener</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jinger Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jager_L/0/1/0/all/0/1\">Lena J&#xe4;ger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Converge to the Truth: Factual Error Correction via Iterative Constrained Editing. (arXiv:2211.12130v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.12130","description":"<p>Given a possibly false claim sentence, how can we automatically correct it\nwith minimal editing? Existing methods either require a large number of pairs\nof false and corrected claims for supervised training or do not handle well\nerrors spanning over multiple tokens within an utterance. In this paper, we\npropose VENCE, a novel method for factual error correction (FEC) with minimal\nedits. VENCE formulates the FEC problem as iterative sampling editing actions\nwith respect to a target density function. We carefully design the target\nfunction with predicted truthfulness scores from an offline trained fact\nverification model. VENCE samples the most probable editing positions based on\nback-calculated gradients of the truthfulness score concerning input tokens and\nthe editing actions using a distantly-supervised language model (T5).\nExperiments on a public dataset show that VENCE improves the well-adopted SARI\nmetric by 5.3 (or a relative improvement of 11.8%) over the previous best\ndistantly-supervised methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiangjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Rui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenxuan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changzhi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Navigation as the Attacker Wishes? Towards Building Byzantine-Robust Embodied Agents under Federated Learning. (arXiv:2211.14769v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2211.14769","description":"<p>Federated embodied agent learning protects the data privacy of individual\nvisual environments by keeping data locally at each client (the individual\nenvironment) during training. However, since the local data is inaccessible to\nthe server under federated learning, attackers may easily poison the training\ndata of the local client to build a backdoor in the agent without notice.\nDeploying such an agent raises the risk of potential harm to humans, as the\nattackers may easily navigate and control the agent as they wish via the\nbackdoor. Towards Byzantine-robust federated embodied agent learning, in this\npaper, we study the attack and defense for the task of vision-and-language\nnavigation (VLN), where the agent is required to follow natural language\ninstructions to navigate indoor environments. First, we introduce a simple but\neffective attack strategy, Navigation as Wish (NAW), in which the malicious\nclient manipulates local trajectory data to implant a backdoor into the global\nmodel. Results on two VLN datasets (R2R and RxR) show that NAW can easily\nnavigate the deployed VLN agent regardless of the language instruction, without\naffecting its performance on normal test sets. Then, we propose a new\nPrompt-Based Aggregation (PBA) to defend against the NAW attack in federated\nVLN, which provides the server with a ''prompt'' of the vision-and-language\nalignment variance between the benign and malicious clients so that they can be\ndistinguished during training. We validate the effectiveness of the PBA method\non protecting the global model from the NAW attack, which outperforms other\nstate-of-the-art defense methods by a large margin in the defense metrics on\nR2R and RxR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Di_Z/0/1/0/all/0/1\">Zonglin Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiwen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cihang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClueWeb22: 10 Billion Web Documents with Visual and Semantic Information. (arXiv:2211.15848v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2211.15848","description":"<p>ClueWeb22, the newest iteration of the ClueWeb line of datasets, provides 10\nbillion web pages affiliated with rich information. Its design was influenced\nby the need for a high quality, large scale web corpus to support a range of\nacademic and industry research, for example, in information systems,\nretrieval-augmented AI systems, and model pretraining. Compared with earlier\nClueWeb corpora, the ClueWeb22 corpus is larger, more varied, of\nhigher-quality, and aligned with the document distributions in commercial web\nsearch. Besides raw HTML, ClueWeb22 includes rich information about the web\npages provided by industry-standard document understanding systems, including\nthe visual representation of pages rendered by a web browser, parsed HTML\nstructure information from a neural network parser, and pre-processed cleaned\ndocument text to lower the barrier to entry. Many of these signals have been\nwidely used in industry but are available to the research community for the\nfirst time at this scale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Overwijk_A/0/1/0/all/0/1\">Arnold Overwijk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+VandenBerg_C/0/1/0/all/0/1\">Cameron VandenBerg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callan_J/0/1/0/all/0/1\">Jamie Callan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topological Data Analysis for Speech Processing. (arXiv:2211.17223v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2211.17223","description":"<p>We apply topological data analysis (TDA) to speech classification problems\nand to the introspection of a pretrained speech model, HuBERT. To this end, we\nintroduce a number of topological and algebraic features derived from\nTransformer attention maps and embeddings. We show that a simple linear\nclassifier built on top of such features outperforms a fine-tuned\nclassification head. In particular, we achieve an improvement of about $9\\%$\naccuracy and $5\\%$ ERR on four common datasets; on CREMA-D, the proposed\nfeature set reaches a new state of the art performance with accuracy $80.155$.\nWe also show that topological features are able to reveal functional roles of\nspeech Transformer heads; e.g., we find the heads capable to distinguish\nbetween pairs of sample sources (natural/synthetic) or voices without any\ndownstream fine-tuning. Our results demonstrate that TDA is a promising new\napproach for speech analysis, especially for tasks that require structural\nprediction. Appendices, an introduction to TDA, and other additional materials\nare available here - https://topohubert.github.io/speech-topology-webpages/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tulchinskii_E/0/1/0/all/0/1\">Eduard Tulchinskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_K/0/1/0/all/0/1\">Kristian Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kushnareva_L/0/1/0/all/0/1\">Laida Kushnareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherniavskii_D/0/1/0/all/0/1\">Daniil Cherniavskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1\">Serguei Barannikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1\">Irina Piontkovskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolenko_S/0/1/0/all/0/1\">Sergey Nikolenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inference of Media Bias and Content Quality Using Natural-Language Processing. (arXiv:2212.00237v1 [physics.soc-ph] CROSS LISTED)","link":"http://arxiv.org/abs/2212.00237","description":"<p>Media bias can significantly impact the formation and development of opinions\nand sentiments in a population. It is thus important to study the emergence and\ndevelopment of partisan media and political polarization. However, it is\nchallenging to quantitatively infer the ideological positions of media outlets.\nIn this paper, we present a quantitative framework to infer both political bias\nand content quality of media outlets from text, and we illustrate this\nframework with empirical experiments with real-world data. We apply a\nbidirectional long short-term memory (LSTM) neural network to a data set of\nmore than 1 million tweets to generate a two-dimensional ideological-bias and\ncontent-quality measurement for each tweet. We then infer a ``media-bias\nchart'' of (bias, quality) coordinates for the media outlets by integrating the\n(bias, quality) measurements of the tweets of the media outlets. We also apply\na variety of baseline machine-learning methods, such as a naive-Bayes method\nand a support-vector machine (SVM), to infer the bias and quality values for\neach tweet. All of these baseline approaches are based on a bag-of-words\napproach. We find that the LSTM-network approach has the best performance of\nthe examined methods. Our results illustrate the importance of leveraging word\norder into machine-learning methods in text analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Chao_Z/0/1/0/all/0/1\">Zehan Chao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Molitor_D/0/1/0/all/0/1\">Denali Molitor</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Needell_D/0/1/0/all/0/1\">Deanna Needell</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Porter_M/0/1/0/all/0/1\">Mason A. Porter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-12-04T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/"}}]}]}