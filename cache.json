{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-01-06T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation. (arXiv:2301.01768v1 [cs.CL])","link":"http://arxiv.org/abs/2301.01768","description":"<p>Conversational artificial intelligence (AI) disrupts how humans interact with\ntechnology. Recently, OpenAI introduced ChatGPT, a state-of-the-art dialogue\nmodel that can converse with its human counterparts with unprecedented\ncapabilities. ChatGPT has witnessed tremendous attention from the media,\nacademia, industry, and the general public, attracting more than a million\nusers within days of its release. However, its explosive adoption for\ninformation search and as an automated decision aid underscores the importance\nto understand its limitations and biases. This paper focuses on one of\ndemocratic society's most important decision-making processes: political\nelections. Prompting ChatGPT with 630 political statements from two leading\nvoting advice applications and the nation-agnostic political compass test in\nthree pre-registered experiments, we uncover ChatGPT's pro-environmental,\nleft-libertarian ideology. For example, ChatGPT would impose taxes on flights,\nrestrict rent increases, and legalize abortion. In the 2021 elections, it would\nhave voted most likely for the Greens both in Germany (B\\\"undnis 90/Die\nGr\\\"unen) and in the Netherlands (GroenLinks). Our findings are robust when\nnegating the prompts, reversing the order of the statements, varying prompt\nformality, and across languages (English, German, Dutch, and Spanish). We\nconclude by discussing the implications of politically biased conversational AI\non society.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_J/0/1/0/all/0/1\">Jochen Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwenzow_J/0/1/0/all/0/1\">Jasper Schwenzow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witte_M/0/1/0/all/0/1\">Maximilian Witte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MessageNet: Message Classification using Natural Language Processing and Meta-data. (arXiv:2301.01808v1 [cs.LG])","link":"http://arxiv.org/abs/2301.01808","description":"<p>In this paper we propose a new Deep Learning (DL) approach for message\nclassification. Our method is based on the state-of-the-art Natural Language\nProcessing (NLP) building blocks, combined with a novel technique for infusing\nthe meta-data input that is typically available in messages such as the sender\ninformation, timestamps, attached image, audio, affiliations, and more. As we\ndemonstrate throughout the paper, going beyond the mere text by leveraging all\navailable channels in the message, could yield an improved representation and\nhigher classification accuracy. To achieve message representation, each type of\ninput is processed in a dedicated block in the neural network architecture that\nis suitable for the data type. Such an implementation enables training all\nblocks together simultaneously, and forming cross channels features in the\nnetwork. We show in the Experiments Section that in some cases, message's\nmeta-data holds an additional information that cannot be extracted just from\nthe text, and when using this information we achieve better performance.\nFurthermore, we demonstrate that our multi-modality block approach outperforms\nother approaches for injecting the meta data to the the text classifier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kahana_A/0/1/0/all/0/1\">Adar Kahana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elisha_O/0/1/0/all/0/1\">Oren Elisha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Fine-Tuning Design Spaces. (arXiv:2301.01821v1 [cs.CL])","link":"http://arxiv.org/abs/2301.01821","description":"<p>Parameter-efficient fine-tuning aims to achieve performance comparable to\nfine-tuning, using fewer trainable parameters. Several strategies (e.g.,\nAdapters, prefix tuning, BitFit, and LoRA) have been proposed. However, their\ndesigns are hand-crafted separately, and it remains unclear whether certain\ndesign patterns exist for parameter-efficient fine-tuning. Thus, we present a\nparameter-efficient fine-tuning design paradigm and discover design patterns\nthat are applicable to different experimental settings. Instead of focusing on\ndesigning another individual tuning strategy, we introduce parameter-efficient\nfine-tuning design spaces that parameterize tuning structures and tuning\nstrategies. Specifically, any design space is characterized by four components:\nlayer grouping, trainable parameter allocation, tunable groups, and strategy\nassignment. Starting from an initial design space, we progressively refine the\nspace based on the model quality of each design choice and make greedy\nselection at each stage over these four components. We discover the following\ndesign patterns: (i) group layers in a spindle pattern; (ii) allocate the\nnumber of trainable parameters to layers uniformly; (iii) tune all the groups;\n(iv) assign proper tuning strategies to different groups. These design patterns\nresult in new parameter-efficient fine-tuning methods. We show experimentally\nthat these methods consistently and significantly outperform investigated\nparameter-efficient fine-tuning strategies across different backbone models and\ndifferent tasks in natural language processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xingjian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alex Smola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Critical Perspectives: A Benchmark Revealing Pitfalls in PerspectiveAPI. (arXiv:2301.01874v1 [cs.CL])","link":"http://arxiv.org/abs/2301.01874","description":"<p>Detecting \"toxic\" language in internet content is a pressing social and\ntechnical challenge. In this work, we focus on PERSPECTIVE from Jigsaw, a\nstate-of-the-art tool that promises to score the \"toxicity\" of text, with a\nrecent model update that claims impressive results (Lees et al., 2022). We seek\nto challenge certain normative claims about toxic language by proposing a new\nbenchmark, Selected Adversarial SemanticS, or SASS. We evaluate PERSPECTIVE on\nSASS, and compare to low-effort alternatives, like zero-shot and few-shot GPT-3\nprompt models, in binary classification settings. We find that PERSPECTIVE\nexhibits troubling shortcomings across a number of our toxicity categories.\nSASS provides a new tool for evaluating performance on previously undetected\ntoxic language that avoids common normative pitfalls. Our work leads us to\nemphasize the importance of questioning assumptions made by tools already in\ndeployment for toxicity detection in order to anticipate and prevent disparate\nharms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piedras_L/0/1/0/all/0/1\">Lorena Piedras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenblatt_L/0/1/0/all/0/1\">Lucas Rosenblatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilkins_J/0/1/0/all/0/1\">Julia Wilkins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GIVL: Improving Geographical Inclusivity of Vision-Language Models with Pre-Training Methods. (arXiv:2301.01893v1 [cs.CV])","link":"http://arxiv.org/abs/2301.01893","description":"<p>A key goal for the advancement of AI is to develop technologies that serve\nthe needs not just of one group but of all communities regardless of their\ngeographical region. In fact, a significant proportion of knowledge is locally\nshared by people from certain regions but may not apply equally in other\nregions because of cultural differences. If a model is unaware of regional\ncharacteristics, it may lead to performance disparity across regions and result\nin bias against underrepresented groups. We propose GIVL, a Geographically\nInclusive Vision-and-Language Pre-trained model. There are two attributes of\ngeo-diverse visual concepts which can help to learn geo-diverse knowledge: 1)\nconcepts under similar categories have unique knowledge and visual\ncharacteristics, 2) concepts with similar visual features may fall in\ncompletely different categories. Motivated by the attributes, we design new\npre-training objectives Image Knowledge Matching (IKM) and Image Edit Checking\n(IEC) to pre-train GIVL. Compared with similar-size models pre-trained with\nsimilar scale of data, GIVL achieves state-of-the-art (SOTA) and more balanced\nperformance on geo-diverse V&amp;L tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Da Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Feng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1\">Govind Thattai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnston_M/0/1/0/all/0/1\">Michael Johnston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Segmentation Model Focusing on Local Context. (arXiv:2301.01935v1 [cs.CL])","link":"http://arxiv.org/abs/2301.01935","description":"<p>Topic segmentation is important in understanding scientific documents since\nit can not only provide better readability but also facilitate downstream tasks\nsuch as information retrieval and question answering by creating appropriate\nsections or paragraphs. In the topic segmentation task, topic coherence is\ncritical in predicting segmentation boundaries. Most of the existing models\nhave tried to exploit as many contexts as possible to extract useful\ntopic-related information. However, additional context does not always bring\npromising results, because the local context between sentences becomes\nincoherent despite more sentences being supplemented. To alleviate this issue,\nwe propose siamese sentence embedding layers which process two input sentences\nindependently to get appropriate amount of information without being hampered\nby excessive information. Also, we adopt multi-task learning techniques\nincluding Same Topic Prediction (STP), Topic Classification (TC) and Next\nSentence Prediction (NSP). When these three classification layers are combined\nin a multi-task manner, they can make up for each other's limitations,\nimproving performance in all three tasks. We experiment different combinations\nof the three layers and report how each layer affects other layers in the same\ncombination as well as the overall segmentation performance. The model we\nproposed achieves the state-of-the-art result in the WikiSection dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jeonghwan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiyeong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1\">Sunghoon Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Min Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph. (arXiv:2301.01949v1 [cs.CL])","link":"http://arxiv.org/abs/2301.01949","description":"<p>Existing multimodal conversation agents have shown impressive abilities to\nlocate absolute positions or retrieve attributes in simple scenarios, but they\nfail to perform well when complex relative positions and information alignments\nare involved, which poses a bottleneck in response quality. In this paper, we\npropose a Situated Conversation Agent Petrained with Multimodal Questions from\nINcremental Layout Graph (SPRING) with abilities of reasoning multi-hops\nspatial relations and connecting them with visual attributes in crowded\nsituated scenarios. Specifically, we design two types of Multimodal Question\nAnswering (MQA) tasks to pretrain the agent. All QA pairs utilized during\npretraining are generated from novel Incremental Layout Graphs (ILG). QA pair\ndifficulty labels automatically annotated by ILG are used to promote MQA-based\nCurriculum Learning. Experimental results verify the SPRING's effectiveness,\nshowing that it significantly outperforms state-of-the-art approaches on both\nSIMMC 1.0 and SIMMC 2.0 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yuxing Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1\">Binyuan Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fulong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhuoxin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Caixia Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies. (arXiv:2301.01967v1 [cs.CL])","link":"http://arxiv.org/abs/2301.01967","description":"<p>The analysis of data in which multiple languages are represented has gained\npopularity among computational linguists in recent years. So far, much of this\nresearch focuses mainly on the improvement of computational methods and largely\nignores linguistic and social aspects of C-S discussed across a wide range of\nlanguages within the long-established literature in linguistics. To fill this\ngap, we offer a survey of code-switching (C-S) covering the literature in\nlinguistics with a reflection on the key issues in language technologies. From\nthe linguistic perspective, we provide an overview of structural and functional\npatterns of C-S focusing on the literature from European and Indian contexts as\nhighly multilingual areas. From the language technologies perspective, we\ndiscuss how massive language models fail to represent diverse C-S types due to\nlack of appropriate training data, lack of robust evaluation benchmarks for C-S\n(across multilingual situations and types of C-S) and lack of end-to-end\nsystems that cover sociolinguistic aspects of C-S as well. Our survey will be a\nstep towards an outcome of mutual benefit for computational scientists and\nlinguists with a shared interest in multilingualism and C-S.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dogruoz_A/0/1/0/all/0/1\">A.Seza Do&#x11f;ru&#xf6;z</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitaram_S/0/1/0/all/0/1\">Sunayana Sitaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bullock_B/0/1/0/all/0/1\">Barbara E. Bullock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toribio_A/0/1/0/all/0/1\">Almeida Jacqueline Toribio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion-Cause Pair Extraction as Question Answering. (arXiv:2301.01982v1 [cs.CL])","link":"http://arxiv.org/abs/2301.01982","description":"<p>The task of Emotion-Cause Pair Extraction (ECPE) aims to extract all\npotential emotion-cause pairs of a document without any annotation of emotion\nor cause clauses. Previous approaches on ECPE have tried to improve\nconventional two-step processing schemes by using complex architectures for\nmodeling emotion-cause interaction. In this paper, we cast the ECPE task to the\nquestion answering (QA) problem and propose simple yet effective BERT-based\nsolutions to tackle it. Given a document, our Guided-QA model first predicts\nthe best emotion clause using a fixed question. Then the predicted emotion is\nused as a question to predict the most potential cause for the emotion. We\nevaluate our model on a standard ECPE corpus. The experimental results show\nthat despite its simplicity, our Guided-QA achieves promising results and is\neasy to reproduce. The code of Guided-QA is also provided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huu-Hiep Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh-Tien Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HIT-SCIR at MMNLU-22: Consistency Regularization for Multilingual Spoken Language Understanding. (arXiv:2301.02010v1 [cs.CL])","link":"http://arxiv.org/abs/2301.02010","description":"<p>Multilingual spoken language understanding (SLU) consists of two sub-tasks,\nnamely intent detection and slot filling. To improve the performance of these\ntwo sub-tasks, we propose to use consistency regularization based on a hybrid\ndata augmentation strategy. The consistency regularization enforces the\npredicted distributions for an example and its semantically equivalent\naugmentation to be consistent. We conduct experiments on the MASSIVE dataset\nunder both full-dataset and zero-shot settings. Experimental results\ndemonstrate that our proposed method improves the performance on both intent\ndetection and slot filling tasks. Our system\\footnote{The code will be\navailable at \\url{https://github.com/bozheng-hit/MMNLU-22-HIT-SCIR}.} ranked\n1st in the MMNLU-22 competition under the full-dataset setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhouyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Fuxuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiguang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TextDescriptives: A Python package for calculating a large variety of statistics from text. (arXiv:2301.02057v1 [cs.CL])","link":"http://arxiv.org/abs/2301.02057","description":"<p>TextDescriptives is a Python package for calculating a large variety of\nstatistics from text. It is built on top of spaCy and can be easily integrated\ninto existing workflows. The package has already been used for analysing the\nlinguistic stability of clinical texts, creating features for predicting\nneuropsychiatric conditions, and analysing linguistic goals of primary school\nstudents. This paper describes the package and its features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hansen_L/0/1/0/all/0/1\">Lasse Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Enevoldsen_K/0/1/0/all/0/1\">Kenneth Enevoldsen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Table-to-Text Generation with Pretrained Language Model: A Table Structure Understanding and Text Deliberating Approach. (arXiv:2301.02071v1 [cs.CL])","link":"http://arxiv.org/abs/2301.02071","description":"<p>Although remarkable progress on the neural table-to-text methods has been\nmade, the generalization issues hinder the applicability of these models due to\nthe limited source tables. Large-scale pretrained language models sound like a\npromising solution to tackle such issues. However, how to effectively bridge\nthe gap between the structured table and the text input by fully leveraging\ntable information to fuel the pretrained model is still not well explored.\nBesides, another challenge of integrating the deliberation mechanism into the\ntext-to-text pretrained model for solving the table-to-text task remains seldom\nstudied. In this paper, to implement the table-to-text generation with\npretrained language model, we propose a table structure understanding and text\ndeliberating approach, namely TASD. Specifically, we devise a three-layered\nmulti-head attention network to realize the table-structure-aware text\ngeneration model with the help of the pretrained language model. Furthermore, a\nmulti-pass decoder framework is adopted to enhance the capability of polishing\ngenerated text for table descriptions. The empirical studies, as well as human\nevaluation, on two public datasets, validate that our approach can generate\nfaithful and fluent descriptive texts for different types of tables.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Miao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xinjiang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers. (arXiv:2301.02111v1 [cs.CL])","link":"http://arxiv.org/abs/2301.02111","description":"<p>We introduce a language modeling approach for text to speech synthesis (TTS).\nSpecifically, we train a neural codec language model (called Vall-E) using\ndiscrete codes derived from an off-the-shelf neural audio codec model, and\nregard TTS as a conditional language modeling task rather than continuous\nsignal regression as in previous work. During the pre-training stage, we scale\nup the TTS training data to 60K hours of English speech which is hundreds of\ntimes larger than existing systems. Vall-E emerges in-context learning\ncapabilities and can be used to synthesize high-quality personalized speech\nwith only a 3-second enrolled recording of an unseen speaker as an acoustic\nprompt. Experiment results show that Vall-E significantly outperforms the\nstate-of-the-art zero-shot TTS system in terms of speech naturalness and\nspeaker similarity. In addition, we find Vall-E could preserve the speaker's\nemotion and acoustic environment of the acoustic prompt in synthesis. See\nhttps://aka.ms/valle for demos of our work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sanyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huaming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anaphora Resolution in Dialogue: System Description (CODI-CRAC 2022 Shared Task). (arXiv:2301.02113v1 [cs.CL])","link":"http://arxiv.org/abs/2301.02113","description":"<p>We describe three models submitted for the CODI-CRAC 2022 shared task. To\nperform identity anaphora resolution, we test several combinations of the\nincremental clustering approach based on the Workspace Coreference System (WCS)\nwith other coreference models. The best result is achieved by adding the\n''cluster merging'' version of the coref-hoi model, which brings up to 10.33%\nimprovement 1 over vanilla WCS clustering. Discourse deixis resolution is\nimplemented as multi-task learning: we combine the learning objective of\ncorefhoi with anaphor type classification. We adapt the higher-order resolution\nmodel introduced in Joshi et al. (2019) for bridging resolution given gold\nmentions and anaphors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anikina_T/0/1/0/all/0/1\">Tatiana Anikina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skachkova_N/0/1/0/all/0/1\">Natalia Skachkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renner_J/0/1/0/all/0/1\">Joseph Renner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_P/0/1/0/all/0/1\">Priyansh Trivedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reprogramming Pretrained Language Models for Protein Sequence Representation Learning. (arXiv:2301.02120v1 [cs.LG])","link":"http://arxiv.org/abs/2301.02120","description":"<p>Machine Learning-guided solutions for protein learning tasks have made\nsignificant headway in recent years. However, success in scientific discovery\ntasks is limited by the accessibility of well-defined and labeled in-domain\ndata. To tackle the low-data constraint, recent adaptions of deep learning\nmodels pretrained on millions of protein sequences have shown promise; however,\nthe construction of such domain-specific large-scale model is computationally\nexpensive. Here, we propose Representation Learning via Dictionary Learning\n(R2DL), an end-to-end representation learning framework in which we reprogram\ndeep models for alternate-domain tasks that can perform well on protein\nproperty prediction with significantly fewer training samples. R2DL reprograms\na pretrained English language model to learn the embeddings of protein\nsequences, by learning a sparse linear mapping between English and protein\nsequence vocabulary embeddings. Our model can attain better accuracy and\nsignificantly improve the data efficiency by up to $10^5$ times over the\nbaselines set by pretrained and standard supervised methods. To this end, we\nreprogram an off-the-shelf pre-trained English language transformer and\nbenchmark it on a set of protein physicochemical prediction tasks (secondary\nstructure, stability, homology, stability) as well as on a biomedically\nrelevant set of protein function prediction tasks (antimicrobial, toxicity,\nantibody affinity).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vinod_R/0/1/0/all/0/1\">Ria Vinod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Payel Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Autoformalization of Mathematics and Code Correctness: Experiments with Elementary Proofs. (arXiv:2301.02195v1 [cs.CL])","link":"http://arxiv.org/abs/2301.02195","description":"<p>The ever-growing complexity of mathematical proofs makes their manual\nverification by mathematicians very cognitively demanding. Autoformalization\nseeks to address this by translating proofs written in natural language into a\nformal representation that is computer-verifiable via interactive theorem\nprovers. In this paper, we introduce a semantic parsing approach, based on the\nUniversal Transformer architecture, that translates elementary mathematical\nproofs into an equivalent formalization in the language of the Coq interactive\ntheorem prover. The same architecture is also trained to translate simple\nimperative code decorated with Hoare triples into formally verifiable proofs of\ncorrectness in Coq. Experiments on a limited domain of artificial and\nhuman-written proofs show that the models generalize well to intermediate\nlengths not seen during training and variations in natural language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cunningham_G/0/1/0/all/0/1\">Garett Cunningham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bunescu_R/0/1/0/all/0/1\">Razvan C. Bunescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juedes_D/0/1/0/all/0/1\">David Juedes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training. (arXiv:2301.02228v1 [eess.IV])","link":"http://arxiv.org/abs/2301.02228","description":"<p>In this paper, we consider the problem of enhancing self-supervised\nvisual-language pre-training (VLP) with medical-specific knowledge, by\nexploiting the paired image-text reports from the radiological daily practice.\nIn particular, we make the following contributions: First, unlike existing\nworks that directly process the raw reports, we adopt a novel report filter to\nextract the medical entities, avoiding unnecessary complexity from language\ngrammar and enhancing the supervision signals; Second, we propose a novel\nentity embedding module by querying an external knowledge description base, to\nexploit the rich context of additional information that the medical domain\naffords, and implicitly build relationships between entities in the language\nembedding space; Third, we propose a novel Transformer-based fusion model for\nspatially aligning the entity description with visual signals at the image\npatch level only with self-supervised learning, thus enabling the ability for\nspatial grounding; Fourth, we conduct thorough experiments to validate the\neffectiveness of our proposed architecture, and benchmark on numerous public\nbenchmarks e.g., ChestX-ray14, RSNA Pneumonia, SIIM-ACR Pneumothorax, COVIDx\nCXR-2, COVID Rural, and EdemaSeverity. In both zero-shot and fine-tuning\nsettings, our model has demonstrated strong performance compared with the\nformer methods on disease classification and grounding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1\">Chaoyi Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoman Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CiT: Curation in Training for Effective Vision-Language Data. (arXiv:2301.02241v1 [cs.CV])","link":"http://arxiv.org/abs/2301.02241","description":"<p>Large vision-language models are generally applicable to many downstream\ntasks, but come at an exorbitant training cost that only large institutions can\nafford. This paper trades generality for efficiency and presents Curation in\nTraining (CiT), a simple and efficient vision-text learning algorithm that\ncouples a data objective into training. CiT automatically yields quality data\nto speed-up contrastive image-text training and alleviates the need for an\noffline data filtering pipeline, allowing broad data sources (including raw\nimage-text pairs from the web). CiT contains two loops: an outer loop curating\nthe training data and an inner loop consuming the curated training data. The\ntext encoder connects the two loops. Given metadata for tasks of interest,\ne.g., class names, and a large pool of image-text pairs, CiT alternatively\nselects relevant training data from the pool by measuring the similarity of\ntheir text embeddings and embeddings of the metadata. In our experiments, we\nobserve that CiT can speed up training by over an order of magnitude,\nespecially if the raw data size is large.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Saining Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Po-Yao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Licheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howes_R/0/1/0/all/0/1\">Russell Howes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1\">Christoph Feichtenhofer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Text Classification As Sub-Hierarchy Sequence Generation. (arXiv:2111.11104v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.11104","description":"<p>Hierarchical text classification (HTC) is essential for various real\napplications. However, HTC models are challenging to develop because they often\nrequire processing a large volume of documents and labels with hierarchical\ntaxonomy. Recent HTC models based on deep learning have attempted to\nincorporate hierarchy information into a model structure. Consequently, these\nmodels are challenging to implement when the model parameters increase for a\nlarge-scale hierarchy because the model structure depends on the hierarchy\nsize. To solve this problem, we formulate HTC as a sub-hierarchy sequence\ngeneration to incorporate hierarchy information into a target label sequence\ninstead of the model structure. Subsequently, we propose the Hierarchy DECoder\n(HiDEC), which decodes a text sequence into a sub-hierarchy sequence using\nrecursive hierarchy decoding, classifying all parents at the same level into\nchildren at once. In addition, HiDEC is trained to use hierarchical path\ninformation from a root to each leaf in a sub-hierarchy composed of the labels\nof a target document via an attention mechanism and hierarchy-aware masking.\nHiDEC achieved state-of-the-art performance with significantly fewer model\nparameters than existing models on benchmark datasets, such as RCV1-v2, NYT,\nand EURLEX57K.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1\">SangHun Im</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gibaeg Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1\">Heung-Seon Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_S/0/1/0/all/0/1\">Seongung Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghwan Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaVocoder: Adaptive Vocoder for Custom Voice. (arXiv:2203.09825v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.09825","description":"<p>Custom voice is to construct a personal speech synthesis system by adapting\nthe source speech synthesis model to the target model through the target few\nrecordings. The solution to constructing a custom voice is to combine an\nadaptive acoustic model with a robust vocoder. However, training a robust\nvocoder usually requires a multi-speaker dataset, which should include various\nage groups and various timbres, so that the trained vocoder can be used for\nunseen speakers. Collecting such a multi-speaker dataset is difficult, and the\ndataset distribution always has a mismatch with the distribution of the target\nspeaker dataset. This paper proposes an adaptive vocoder for custom voice from\nanother novel perspective to solve the above problems. The adaptive vocoder\nmainly uses a cross-domain consistency loss to solve the overfitting problem\nencountered by the GAN-based neural vocoder in the transfer learning of\nfew-shot scenes. We construct two adaptive vocoders, AdaMelGAN and AdaHiFi-GAN.\nFirst, We pre-train the source vocoder model on AISHELL3 and CSMSC datasets,\nrespectively. Then, fine-tune it on the internal dataset VXI-children with few\nadaptation data. The empirical results show that a high-quality custom voice\nsystem can be built by combining a adaptive acoustic model with a adaptive\nvocoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yongbing Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Mingming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuo_C/0/1/0/all/0/1\">Cheng Tuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minghang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sarcasm Detection Framework Using Context, Emotion and Sentiment Features. (arXiv:2211.13014v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.13014","description":"<p>Sarcasm detection is an essential task that can help identify the actual\nsentiment in user-generated data, such as discussion forums or tweets. Sarcasm\nis a sophisticated form of linguistic expression because its surface meaning\nusually contradicts its inner, deeper meaning. Such incongruity is the\nessential component of sarcasm, however, it makes sarcasm detection quite a\nchallenging task. In this paper, we propose a model, that incorporates\ndifferent features to capture the incongruity intrinsic to sarcasm. We use a\npre-trained transformer and CNN to capture context features, and we use\ntransformers pre-trained on emotions detection and sentiment analysis tasks.\nOur approach outperformed previous state-of-the-art results on four datasets\nfrom social networking platforms and online media.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vitman_O/0/1/0/all/0/1\">Oxana Vitman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kostiuk_Y/0/1/0/all/0/1\">Yevhen Kostiuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidorov_G/0/1/0/all/0/1\">Grigori Sidorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1\">Alexander Gelbukh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Scales Data Augmentation Approach In Natural Language Inference For Artifacts Mitigation And Pre-Trained Model Optimization. (arXiv:2212.08756v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08756","description":"<p>Machine learning models can reach high performance on benchmark natural\nlanguage processing (NLP) datasets but fail in more challenging settings. We\nstudy this issue when a pre-trained model learns dataset artifacts in natural\nlanguage inference (NLI), the topic of studying the logical relationship\nbetween a pair of text sequences. We provide a variety of techniques for\nanalyzing and locating dataset artifacts inside the crowdsourced Stanford\nNatural Language Inference (SNLI) corpus. We study the stylistic pattern of\ndataset artifacts in the SNLI. To mitigate dataset artifacts, we employ a\nunique multi-scale data augmentation technique with two distinct frameworks: a\nbehavioral testing checklist at the sentence level and lexical synonym criteria\nat the word level. Specifically, our combination method enhances our model's\nresistance to perturbation testing, enabling it to continuously outperform the\npre-trained baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhenyuan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MicroBERT: Effective Training of Low-resource Monolingual BERTs through Parameter Reduction and Multitask Learning. (arXiv:2212.12510v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.12510","description":"<p>Transformer language models (TLMs) are critical for most NLP tasks, but they\nare difficult to create for low-resource languages because of how much\npretraining data they require. In this work, we investigate two techniques for\ntraining monolingual TLMs in a low-resource setting: greatly reducing TLM size,\nand complementing the masked language modeling objective with two\nlinguistically rich supervised tasks (part-of-speech tagging and dependency\nparsing). Results from 7 diverse languages indicate that our model, MicroBERT,\nis able to produce marked improvements in downstream task evaluations relative\nto a typical monolingual TLM pretraining approach. Specifically, we find that\nmonolingual MicroBERT models achieve gains of up to 18% for parser LAS and 11%\nfor NER F1 compared to a multilingual baseline, mBERT, while having less than\n1% of its parameter count. We conclude reducing TLM parameter count and using\nlabeled data for pretraining low-resource TLMs can yield large quality benefits\nand in some cases produce models that outperform multilingual approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gessler_L/0/1/0/all/0/1\">Luke Gessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeldes_A/0/1/0/all/0/1\">Amir Zeldes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Second Thoughts are Best: Learning to Re-Align With Human Values from Text Edits. (arXiv:2301.00355v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.00355","description":"<p>We present Second Thought, a new learning paradigm that enables language\nmodels (LMs) to re-align with human values. By modeling the chain-of-edits\nbetween value-unaligned and value-aligned text, with LM fine-tuning and\nadditional refinement through reinforcement learning, Second Thought not only\nachieves superior performance in three value alignment benchmark datasets but\nalso shows strong human-value transfer learning ability in few-shot scenarios.\nThe generated editing steps also offer better interpretability and ease for\ninteractive error correction. Extensive human evaluations further confirm its\neffectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Chenyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Ge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1\">Ziyu Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tony X Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models as Corporate Lobbyists. (arXiv:2301.01181v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.01181","description":"<p>We demonstrate a proof-of-concept of a large language model conducting\ncorporate lobbying related activities. An autoregressive large language model\n(OpenAI's text-davinci-003) determines if proposed U.S. Congressional bills are\nrelevant to specific public companies and provides explanations and confidence\nlevels. For the bills the model deems as relevant, the model drafts a letter to\nthe sponsor of the bill in an attempt to persuade the congressperson to make\nchanges to the proposed legislation. We use hundreds of ground-truth labels of\nthe relevance of a bill to a company to benchmark the performance of the model,\nwhich outperforms the baseline of predicting the most common outcome of\nirrelevance. We also benchmark the performance of the previous OpenAI GPT-3\nmodel (text-davinci-002), which was state-of-the-art on many language tasks\nuntil text-davinci-003 was recently released. The performance of\ntext-davinci-002 is worse than simply always predicting that a bill is\nirrelevant to a company. These results suggest that, as large language models\ncontinue to exhibit improved core natural language understanding capabilities,\nperformance on corporate lobbying related tasks will continue to improve. If AI\nbegins to influence law in a manner that is not a direct extension of human\nintentions, this threatens the critical role that law as information could play\nin aligning AI with humans. This paper explores how this is increasingly a\npossibility. Initially, AI is being used to simply augment human lobbyists.\nHowever, there may be a slow creep of less and less human oversight over\nautomated assessments of policy ideas and the written communication to\nregulatory agencies and Congressional staffers. The core question raised is\nwhere to draw the line between human-driven and AI-driven policy influence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nay_J/0/1/0/all/0/1\">John J. Nay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterated Decomposition: Improving Science Q&A by Supervising Reasoning Processes. (arXiv:2301.01751v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.01751","description":"<p>Language models (LMs) can perform complex reasoning either end-to-end, with\nhidden latent state, or compositionally, with transparent intermediate state.\nComposition offers benefits for interpretability and safety, but may need\nworkflow support and infrastructure to remain competitive. We describe iterated\ndecomposition, a human-in-the-loop workflow for developing and refining\ncompositional LM programs. We improve the performance of compositions by\nzooming in on failing components and refining them through decomposition,\nadditional context, chain of thought, etc. To support this workflow, we develop\nICE, an open-source tool for visualizing the execution traces of LM programs.\nWe apply iterated decomposition to three real-world tasks and improve the\naccuracy of LM programs over less compositional baselines: describing the\nplacebo used in a randomized controlled trial (25% to 65%), evaluating\nparticipant adherence to a medical intervention (53% to 70%), and answering NLP\nquestions on the Qasper dataset (38% to 69%). These applications serve as case\nstudies for a workflow that, if automated, could keep ML systems interpretable\nand safe even as they scale to increasingly complex tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reppert_J/0/1/0/all/0/1\">Justin Reppert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rachbach_B/0/1/0/all/0/1\">Ben Rachbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+George_C/0/1/0/all/0/1\">Charlie George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stebbing_L/0/1/0/all/0/1\">Luke Stebbing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_J/0/1/0/all/0/1\">Jungwon Byun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Appleton_M/0/1/0/all/0/1\">Maggie Appleton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuhlmuller_A/0/1/0/all/0/1\">Andreas Stuhlm&#xfc;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniHD at TSAR-2022 Shared Task: Is Compute All We Need for Lexical Simplification?. (arXiv:2301.01764v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.01764","description":"<p>Previous state-of-the-art models for lexical simplification consist of\ncomplex pipelines with several components, each of which requires deep\ntechnical knowledge and fine-tuned interaction to achieve its full potential.\nAs an alternative, we describe a frustratingly simple pipeline based on\nprompted GPT-3 responses, beating competing approaches by a wide margin in\nsettings with few training instances. Our best-performing submission to the\nEnglish language track of the TSAR-2022 shared task consists of an ``ensemble''\nof six different prompt templates with varying context levels. As a\nlate-breaking result, we further detail a language transfer technique that\nallows simplification in languages other than English. Applied to the Spanish\nand Portuguese subset, we achieve state-of-the-art results with only minor\nmodification to the original prompts. Aside from detailing the implementation\nand setup, we spend the remainder of this work discussing the particularities\nof prompting and implications for future work. Code for the experiments is\navailable online at https://github.com/dennlinger/TSAR-2022-Shared-Task\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aumiller_D/0/1/0/all/0/1\">Dennis Aumiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gertz_M/0/1/0/all/0/1\">Michael Gertz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-01-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}