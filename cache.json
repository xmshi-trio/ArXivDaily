{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-10-17T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"SODAPOP: Open-Ended Discovery of Social Biases in Social Commonsense Reasoning Models. (arXiv:2210.07269v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07269","description":"<p>A common limitation of diagnostic tests for detecting social biases in NLP\nmodels is that they may only detect stereotypic associations that are\npre-specified by the designer of the test. Since enumerating all possible\nproblematic associations is infeasible, it is likely these tests fail to detect\nbiases that are present in a model but not pre-specified by the designer. To\naddress this limitation, we propose SODAPOP (SOcial bias Discovery from Answers\nabout PeOPle) in social commonsense question-answering. Our pipeline generates\nmodified instances from the Social IQa dataset (Sap et al., 2019) by (1)\nsubstituting names associated with different demographic groups, and (2)\ngenerating many distractor answers from a masked language model. By using a\nsocial commonsense model to score the generated distractors, we are able to\nuncover the model's stereotypic associations between demographic groups and an\nopen set of words. We also test SODAPOP on debiased models and show the\nlimitations of multiple state-of-the-art debiasing algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_H/0/1/0/all/0/1\">Haozhe An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zongxia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jieyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudinger_R/0/1/0/all/0/1\">Rachel Rudinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Learning for Joint Semantic Role and Proto-Role Labeling. (arXiv:2210.07270v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07270","description":"<p>We put forward an end-to-end multi-step machine learning model which jointly\nlabels semantic roles and the proto-roles of Dowty (1991), given a sentence and\nthe predicates therein. Our best architecture first learns argument spans\nfollowed by learning the argument's syntactic heads. This information is shared\nwith the next steps for predicting the semantic roles and proto-roles. We also\nexperiment with transfer learning from argument and head prediction to role and\nproto-role labeling. We compare using static and contextual embeddings for\nwords, arguments, and sentences. Unlike previous work, our model does not\nrequire pre-training or fine-tuning on additional tasks, beyond using\noff-the-shelf (static or contextual) embeddings and supervision. It also does\nnot require argument spans, their semantic roles, and/or their gold syntactic\nheads as additional input, because it learns to predict all these during\ntraining. Our multi-task learning model raises the state-of-the-art predictions\nfor most proto-roles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Aashish Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malireddi_H/0/1/0/all/0/1\">Harshitha Malireddi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_D/0/1/0/all/0/1\">Daniel Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayeed_A/0/1/0/all/0/1\">Asad Sayeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marton_Y/0/1/0/all/0/1\">Yuval Marton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Reasoning on Hybrid-knowledge sources for Task-Oriented Dialog. (arXiv:2210.07295v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07295","description":"<p>Traditional systems designed for task oriented dialog utilize knowledge\npresent only in structured knowledge sources to generate responses. However,\nrelevant information required to generate responses may also reside in\nunstructured sources, such as documents. Recent state of the art models such as\nHyKnow and SeKnow aimed at overcoming these challenges make limiting\nassumptions about the knowledge sources. For instance, these systems assume\nthat certain types of information, such as a phone number, is always present in\na structured KB while information about aspects such as entrance ticket prices\nwould always be available in documents.\n</p>\n<p>In this paper, we create a modified version of the MutliWOZ based dataset\nprepared by SeKnow to demonstrate how current methods have significant\ndegradation in performance when strict assumptions about the source of\ninformation are removed. Then, in line with recent work exploiting pre-trained\nlanguage models, we fine-tune a BART based model using prompts for the tasks of\nquerying knowledge sources, as well as, for response generation, without making\nassumptions about the information present in each knowledge source. Through a\nseries of experiments, we demonstrate that our model is robust to perturbations\nto knowledge modality (source of information), and that it can fuse information\nfrom structured as well as unstructured knowledge to generate responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_M/0/1/0/all/0/1\">Mayank Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Contractor_D/0/1/0/all/0/1\">Danish Contractor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghu_D/0/1/0/all/0/1\">Dinesh Raghu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bootstrapping Multilingual Semantic Parsers using Large Language Models. (arXiv:2210.07313v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07313","description":"<p>Despite cross-lingual generalization demonstrated by pre-trained multilingual\nmodels, the translate-train paradigm of transferring English datasets across\nmultiple languages remains to be the key ingredient for training task-specific\nmultilingual models. However, for many low-resource languages, the availability\nof a reliable translation service entails significant amounts of costly\nhuman-annotated translation pairs. Further, the translation services for\nlow-resource languages may continue to be brittle due to domain mismatch\nbetween the task-specific input text and the general-purpose text used while\ntraining the translation models. We consider the task of multilingual semantic\nparsing and demonstrate the effectiveness and flexibility offered by large\nlanguage models (LLMs) for translating English datasets into several languages\nvia few-shot prompting. We provide (i) Extensive comparisons with prior\ntranslate-train methods across 50 languages demonstrating that LLMs can serve\nas highly effective data translators, outperforming prior translation based\nmethods on 40 out of 50 languages; (ii) A comprehensive study of the key design\nchoices that enable effective data translation via prompted LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Awasthi_A/0/1/0/all/0/1\">Abhijeet Awasthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1\">Nitish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samanta_B/0/1/0/all/0/1\">Bidisha Samanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dave_S/0/1/0/all/0/1\">Shachi Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1\">Sunita Sarawagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MTEB: Massive Text Embedding Benchmark. (arXiv:2210.07316v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07316","description":"<p>Text embeddings are commonly evaluated on a small set of datasets from a\nsingle task not covering their possible applications to other tasks. It is\nunclear whether state-of-the-art embeddings on semantic textual similarity\n(STS) can be equally well applied to other tasks like clustering or reranking.\nThis makes progress in the field difficult to track, as various models are\nconstantly being proposed without proper evaluation. To solve this problem, we\nintroduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding\ntasks covering a total of 56 datasets and 112 languages. Through the\nbenchmarking of 33 models on MTEB, we establish the most comprehensive\nbenchmark of text embeddings to date. We find that no particular text embedding\nmethod dominates across all tasks. This suggests that the field has yet to\nconverge on a universal text embedding method and scale it up sufficiently to\nprovide state-of-the-art results on all embedding tasks. MTEB comes with\nopen-source code and a public leaderboard at\nhttps://huggingface.co/spaces/mteb/leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1\">Niklas Muennighoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tazi_N/0/1/0/all/0/1\">Nouamane Tazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magne_L/0/1/0/all/0/1\">Lo&#xef;c Magne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods. (arXiv:2210.07321v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07321","description":"<p>Advances in natural language generation (NLG) have resulted in machine\ngenerated text that is increasingly difficult to distinguish from human\nauthored text. Powerful open-source models are freely available, and\nuser-friendly tools democratizing access to generative models are\nproliferating. The great potential of state-of-the-art NLG systems is tempered\nby the multitude of avenues for abuse. Detection of machine generated text is a\nkey countermeasure for reducing abuse of NLG models, with significant technical\nchallenges and numerous open problems. We provide a survey that includes both\n1) an extensive analysis of threat models posed by contemporary NLG systems,\nand 2) the most complete review of machine generated text detection methods to\ndate. This survey places machine generated text within its cybersecurity and\nsocial context, and provides strong guidance for future work addressing the\nmost critical threat models, and ensuring detection systems themselves\ndemonstrate trustworthiness through fairness, robustness, and accountability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Crothers_E/0/1/0/all/0/1\">Evan Crothers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Japkowicz_N/0/1/0/all/0/1\">Nathalie Japkowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viktor_H/0/1/0/all/0/1\">Herna Viktor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HuBERT-TR: Reviving Turkish Automatic Speech Recognition with Self-supervised Speech Representation Learning. (arXiv:2210.07323v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07323","description":"<p>While the Turkish language is listed among low-resource languages, literature\non Turkish automatic speech recognition (ASR) is relatively old. In this paper,\nwe present HuBERT-TR, a speech representation model for Turkish based on\nHuBERT. HuBERT-TR achieves state-of-the-art results on several Turkish ASR\ndatasets. We investigate pre-training HuBERT for Turkish with large-scale data\ncurated from online resources. We pre-train HuBERT-TR using over 6,500 hours of\nspeech data curated from YouTube that includes extensive variability in terms\nof quality and genre. We show that pre-trained models within a multi-lingual\nsetup are inferior to language-specific models, where our Turkish model\nHuBERT-TR base performs better than its x10 times larger multi-lingual\ncounterpart XLS-R-1B. Moreover, we study the effect of scaling on ASR\nperformance by scaling our models up to 1B parameters. Our best model yields a\nstate-of-the-art word error rate of 4.97% on the Turkish Broadcast News\ndataset. Models are available at huggingface.co/asafaya .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Safaya_A/0/1/0/all/0/1\">Ali Safaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erzin_E/0/1/0/all/0/1\">Engin Erzin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Fine-Tuning Performance with Probing. (arXiv:2210.07352v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07352","description":"<p>Large NLP models have recently shown impressive performance in language\nunderstanding tasks, typically evaluated by their fine-tuned performance.\nAlternatively, probing has received increasing attention as being a lightweight\nmethod for interpreting the intrinsic mechanisms of large NLP models. In\nprobing, post-hoc classifiers are trained on \"out-of-domain\" datasets that\ndiagnose specific abilities. While probing the language models has led to\ninsightful findings, they appear disjointed from the development of models.\nThis paper explores the utility of probing deep NLP models to extract a proxy\nsignal widely used in model development -- the fine-tuning performance. We find\nthat it is possible to use the accuracies of only three probing tests to\npredict the fine-tuning performance with errors $40\\%$ - $80\\%$ smaller than\nbaselines. We further discuss possible avenues where probing can empower the\ndevelopment of deep NLP models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zining Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahtalebi_S/0/1/0/all/0/1\">Soroosh Shahtalebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1\">Frank Rudzicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JOIST: A Joint Speech and Text Streaming Model For ASR. (arXiv:2210.07353v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07353","description":"<p>We present JOIST, an algorithm to train a streaming, cascaded, encoder\nend-to-end (E2E) model with both speech-text paired inputs, and text-only\nunpaired inputs. Unlike previous works, we explore joint training with both\nmodalities, rather than pre-training and fine-tuning. In addition, we explore\nJOIST using a streaming E2E model with an order of magnitude more data, which\nare also novelties compared to previous works. Through a series of ablation\nstudies, we explore different types of text modeling, including how to model\nthe length of the text sequence and the appropriate text sub-word unit\nrepresentation. We find that best text representation for JOIST improves WER\nacross a variety of search and rare-word test sets by 4-14% relative, compared\nto a model not trained with text. In addition, we quantitatively show that\nJOIST maintains streaming capabilities, which is important for good user-level\nexperience.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1\">Zhouyuan Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhehuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Demographic Factors Improve Text Classification? Revisiting Demographic Adaptation in the Age of Transformers. (arXiv:2210.07362v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07362","description":"<p>Demographic factors (e.g., gender or age) shape our language. Previous work\nshowed that incorporating demographic factors can consistently improve\nperformance for various NLP tasks with traditional NLP models. In this work, we\ninvestigate whether these previous findings still hold with state-of-the-art\npretrained Transformer-based language models (PLMs). We use three common\nspecialization methods proven effective for incorporating external knowledge\ninto pretrained Transformers (e.g., domain-specific or geographic knowledge).\nWe adapt the language representations for the demographic dimensions of gender\nand age, using continuous language modeling and dynamic multi-task learning for\nadaptation, where we couple language modeling objectives with the prediction of\ndemographic classes. Our results when employing a multilingual PLM show\nsubstantial performance gains across four languages (English, German, French,\nand Danish), which is consistent with the results of previous work. However,\ncontrolling for confounding factors -- primarily domain and language\nproficiency of Transformer-based PLMs -- shows that downstream performance\ngains from our demographic adaptation do not actually stem from demographic\nknowledge. Our results indicate that demographic specialization of PLMs, while\nholding promise for positive societal impact, still represents an unsolved\nproblem for (modern) NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hung_C/0/1/0/all/0/1\">Chia-Chien Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponzetto_S/0/1/0/all/0/1\">Simone Paolo Ponzetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is It Worth the (Environmental) Cost? Limited Evidence for the Benefits of Diachronic Continuous Training. (arXiv:2210.07365v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07365","description":"<p>Language is constantly changing and evolving, leaving language models to\nquickly become outdated, both factually and linguistically. Recent research\nproposes we continuously update our models using new data. Continuous training\nallows us to teach language models about new events and facts and changing\nnorms. However, continuous training also means continuous costs. We show there\nis currently limited evidence for the benefits of continuous training, be it\nfor the actual downstream performance or the environmental cost. Our results\nshow continuous training does not significantly improve performance. While it\nis clear that, sooner or later, our language models need to be updated, it is\nunclear when this effort is worth the cost. We call for a critical reflection\nabout when and how to use continuous training and for more benchmarks to\nsupport this research direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Attanasio_G/0/1/0/all/0/1\">Giuseppe Attanasio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nozza_D/0/1/0/all/0/1\">Debora Nozza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M2D2: A Massively Multi-domain Language Modeling Dataset. (arXiv:2210.07370v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07370","description":"<p>We present M2D2, a fine-grained, massively multi-domain corpus for studying\ndomain adaptation in language models (LMs). M2D2 consists of 8.5B tokens and\nspans 145 domains extracted from Wikipedia and Semantic Scholar. Using\nontologies derived from Wikipedia and ArXiv categories, we organize the domains\nin each data source into 22 groups. This two-level hierarchy enables the study\nof relationships between domains and their effects on in- and out-of-domain\nperformance after adaptation. We also present a number of insights into the\nnature of effective domain adaptation in LMs, as examples of the new types of\nstudies M2D2 enables. To improve in-domain performance, we show the benefits of\nadapting the LM along a domain hierarchy; adapting to smaller amounts of\nfine-grained domain-specific data can lead to larger in-domain performance\ngains than larger amounts of weakly relevant data. We further demonstrate a\ntrade-off between in-domain specialization and out-of-domain generalization\nwithin and across ontologies, as well as a strong correlation between\nout-of-domain performance and lexical overlap between domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1\">Machel Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_V/0/1/0/all/0/1\">Victor Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1\">Suchin Gururangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mind the Labels: Describing Relations in Knowledge Graphs With Pretrained Models. (arXiv:2210.07373v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07373","description":"<p>Pretrained language models (PLMs) for data-to-text (D2T) generation can use\nhuman-readable data labels such as column headings, keys, or relation names to\ngeneralize to out-of-domain examples. However, the models are well-known in\nproducing semantically inaccurate outputs if these labels are ambiguous or\nincomplete, which is often the case in D2T datasets. In this paper, we expose\nthis issue on the task of descibing a relation between two entities. For our\nexperiments, we collect a novel dataset for verbalizing a diverse set of 1,522\nunique relations from three large-scale knowledge graphs (Wikidata, DBPedia,\nYAGO). We find that although PLMs for D2T generation expectedly fail on unclear\ncases, models trained with a large variety of relation labels are surprisingly\nrobust in verbalizing novel, unseen relations. We argue that using data with a\ndiverse set of clear and meaningful labels is key to training D2T generation\nsystems capable of generalizing to novel domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasner_Z/0/1/0/all/0/1\">Zden&#x11b;k Kasner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konstas_I/0/1/0/all/0/1\">Ioannis Konstas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1\">Ond&#x159;ej Du&#x161;ek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frustratingly Easy Sentiment Analysis of Text Streams: Generating High-Quality Emotion Arcs Using Emotion Lexicons. (arXiv:2210.07381v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07381","description":"<p>Automatically generated emotion arcs -- that capture how an individual or a\npopulation feels over time -- are widely used in industry and research.\nHowever, there is little work on evaluating the generated arcs. This is in part\ndue to the difficulty of establishing the true (gold) emotion arc. Our work,\nfor the first time, systematically and quantitatively evaluates automatically\ngenerated emotion arcs. We also compare two common ways of generating emotion\narcs: Machine-Learning (ML) models and Lexicon-Only (LexO) methods. Using a\nnumber of diverse datasets, we systematically study the relationship between\nthe quality of an emotion lexicon and the quality of the emotion arc that can\nbe generated with it. We also study the relationship between the quality of an\ninstance-level emotion detection system (say from an ML model) and the quality\nof emotion arcs that can be generated with it. We show that despite being\nmarkedly poor at instance level, LexO methods are highly accurate at generating\nemotion arcs by aggregating information from hundreds of instances. This has\nwide-spread implications for commercial development, as well as research in\npsychology, public health, digital humanities, etc. that values simple\ninterpretable methods and disprefers the need for domain-specific training\ndata, programming expertise, and high-carbon-footprint models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teodorescu_D/0/1/0/all/0/1\">Daniela Teodorescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Behavior Cloned Transformers are Neurosymbolic Reasoners. (arXiv:2210.07382v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07382","description":"<p>In this work, we explore techniques for augmenting interactive agents with\ninformation from symbolic modules, much like humans use tools like calculators\nand GPS systems to assist with arithmetic and navigation. We test our agent's\nabilities in text games -- challenging benchmarks for evaluating the multi-step\nreasoning abilities of game agents in grounded, language-based environments.\nOur experimental study indicates that injecting the actions from these symbolic\nmodules into the action space of a behavior cloned transformer agent increases\nperformance on four text game benchmarks that test arithmetic, navigation,\nsorting, and common sense reasoning by an average of 22%, allowing an agent to\nreach the highest possible performance on unseen games. This action injection\ntechnique is easily extended to new agents, environments, and symbolic modules.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruoyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jansen_P/0/1/0/all/0/1\">Peter Jansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1\">Marc-Alexandre C&#xf4;t&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Early Discovery of Disappearing Entities in Microblogs. (arXiv:2210.07404v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07404","description":"<p>We make decisions by reacting to changes in the real world, in particular,\nthe emergence and disappearance of impermanent entities such as events,\nrestaurants, and services. Because we want to avoid missing out on\nopportunities or making fruitless actions after they have disappeared, it is\nimportant to know when entities disappear as early as possible. We thus tackle\nthe task of detecting disappearing entities from microblogs, whose posts\nmention various entities, in a timely manner. The major challenge is detecting\nuncertain contexts of disappearing entities from noisy microblog posts. To\ncollect these disappearing contexts, we design time-sensitive distant\nsupervision, which utilizes entities from the knowledge base and time-series\nposts, for this task to build large-scale Twitter datasets\\footnote{We will\nrelease the datasets (tweet IDs) used in the experiments to promote\nreproducibility.} for English and Japanese. To ensure robust detection in noisy\nenvironments, we refine pretrained word embeddings of the detection model on\nmicroblog streams of the target day. Experimental results on the Twitter\ndatasets confirmed the effectiveness of the collected labeled data and refined\nword embeddings; more than 70\\% of the detected disappearing entities in\nWikipedia are discovered earlier than the update on Wikipedia, and the average\nlead-time is over one month.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akasaki_S/0/1/0/all/0/1\">Satoshi Akasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshinaga_N/0/1/0/all/0/1\">Naoki Yoshinaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toyoda_M/0/1/0/all/0/1\">Masashi Toyoda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noise Audits Improve Moral Foundation Classification. (arXiv:2210.07415v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07415","description":"<p>Morality plays an important role in culture, identity, and emotion. Recent\nadvances in natural language processing have shown that it is possible to\nclassify moral values expressed in text at scale. Morality classification\nrelies on human annotators to label the moral expressions in text, which\nprovides training data to achieve state-of-the-art performance. However, these\nannotations are inherently subjective and some of the instances are hard to\nclassify, resulting in noisy annotations due to error or lack of agreement. The\npresence of noise in training data harms the classifier's ability to accurately\nrecognize moral foundations from text. We propose two metrics to audit the\nnoise of annotations. The first metric is entropy of instance labels, which is\na proxy measure of annotator disagreement about how the instance should be\nlabeled. The second metric is the silhouette coefficient of a label assigned by\nan annotator to an instance. This metric leverages the idea that instances with\nthe same label should have similar latent representations, and deviations from\ncollective judgments are indicative of errors. Our experiments on three widely\nused moral foundations datasets show that removing noisy annotations based on\nthe proposed metrics improves classification performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mokhberian_N/0/1/0/all/0/1\">Negar Mokhberian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hopp_F/0/1/0/all/0/1\">Frederic R. Hopp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harandizadeh_B/0/1/0/all/0/1\">Bahareh Harandizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morstatter_F/0/1/0/all/0/1\">Fred Morstatter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_K/0/1/0/all/0/1\">Kristina Lerman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PCFG-based Natural Language Interface Improves Generalization for Controlled Text Generation. (arXiv:2210.07431v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07431","description":"<p>Existing work on controlled text generation (CTG) assumes a control interface\nof categorical attributes. In this work, we propose a natural language (NL)\ninterface, where we craft a PCFG to embed the control attributes into natural\nlanguage commands, and propose variants of existing CTG models that take\ncommands as input. In our experiments, we design tailored setups to test\nmodel's generalization abilities. We find our PCFG-based command generation\napproach is effective for handling unseen commands compared to fix-set\ntemplates; our proposed NL models can effectively generalize to unseen\nattributes, a new ability enabled by the NL interface, as well as unseen\nattribute combinations. Interestingly, we discover that the simple conditional\ngeneration approach, enhanced with our proposed NL interface, is a strong\nbaseline in those challenging settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianxing He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InterFair: Debiasing with Natural Language Feedback for Fair Interpretable Predictions. (arXiv:2210.07440v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07440","description":"<p>Debiasing methods in NLP models traditionally focus on isolating information\nrelated to a sensitive attribute (like gender or race). We instead argue that a\nfavorable debiasing method should use sensitive information 'fairly,' with\nexplanations, rather than blindly eliminating it. This fair balance is often\nsubjective and can be challenging to achieve algorithmically. We show that an\ninteractive setup with users enabled to provide feedback can achieve a better\nand fair balance between task performance and bias mitigation, supported by\nfaithful explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Majumder_B/0/1/0/all/0/1\">Bodhisattwa Prasad Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zexue He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Word Sense Disambiguation with Unified Sense Representation. (arXiv:2210.07447v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07447","description":"<p>As a key natural language processing (NLP) task, word sense disambiguation\n(WSD) evaluates how well NLP models can understand the lexical semantics of\nwords under specific contexts. Benefited from the large-scale annotation,\ncurrent WSD systems have achieved impressive performances in English by\ncombining supervised learning with lexical knowledge. However, such success is\nhard to be replicated in other languages, where we only have limited\nannotations.In this paper, based on the multilingual lexicon BabelNet\ndescribing the same set of concepts across languages, we propose building\nknowledge and supervised-based Multilingual Word Sense Disambiguation (MWSD)\nsystems. We build unified sense representations for multiple languages and\naddress the annotation scarcity problem for MWSD by transferring annotations\nfrom rich-sourced languages to poorer ones. With the unified sense\nrepresentations, annotations from multiple languages can be jointly trained to\nbenefit the MWSD tasks. Evaluations of SemEval-13 and SemEval-15 datasets\ndemonstrate the effectiveness of our methodology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Ying Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Out-of-Distribution Performance on Document Image Classifiers. (arXiv:2210.07448v1 [cs.CV])","link":"http://arxiv.org/abs/2210.07448","description":"<p>The ability of a document classifier to handle inputs that are drawn from a\ndistribution different from the training distribution is crucial for robust\ndeployment and generalizability. The RVL-CDIP corpus is the de facto standard\nbenchmark for document classification, yet to our knowledge all studies that\nuse this corpus do not include evaluation on out-of-distribution documents. In\nthis paper, we curate and release a new out-of-distribution benchmark for\nevaluating out-of-distribution performance for document classifiers. Our new\nout-of-distribution benchmark consists of two types of documents: those that\nare not part of any of the 16 in-domain RVL-CDIP categories (RVL-CDIP-O), and\nthose that are one of the 16 in-domain categories yet are drawn from a\ndistribution different from that of the original RVL-CDIP dataset (RVL-CDIP-N).\nWhile prior work on document classification for in-domain RVL-CDIP documents\nreports high accuracy scores, we find that these models exhibit accuracy drops\nof between roughly 15-30% on our new out-of-domain RVL-CDIP-N benchmark, and\nfurther struggle to distinguish between in-domain RVL-CDIP-N and out-of-domain\nRVL-CDIP-O inputs. Our new benchmark provides researchers with a valuable new\nresource for analyzing out-of-distribution performance on document classifiers.\nOur new out-of-distribution data can be found at https://tinyurl.com/4he6my23.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Larson_S/0/1/0/all/0/1\">Stefan Larson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_G/0/1/0/all/0/1\">Gordon Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_Y/0/1/0/all/0/1\">Yutong Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_D/0/1/0/all/0/1\">David Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leach_K/0/1/0/all/0/1\">Kevin Leach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlling Bias Exposure for Fair Interpretable Predictions. (arXiv:2210.07455v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07455","description":"<p>Recent work on reducing bias in NLP models usually focuses on protecting or\nisolating information related to a sensitive attribute (like gender or race).\nHowever, when sensitive information is semantically entangled with the task\ninformation of the input, e.g., the gender information is predictive for a\nprofession, a fair trade-off between task performance and bias mitigation is\ndifficult to achieve. Existing approaches perform this trade-off by eliminating\nbias information from the latent space, lacking control over how much bias is\nnecessarily required to be removed. We argue that a favorable debiasing method\nshould use sensitive information 'fairly' rather than blindly eliminating it\n(Caliskan et al., 2017; Sun et al., 2019). In this work, we provide a novel\ndebiasing algorithm by adjusting the predictive model's belief to (1) ignore\nthe sensitive information if it is not useful for the task; (2) use sensitive\ninformation minimally as necessary for the prediction (while also incurring a\npenalty). Experimental results on two text classification tasks (influenced by\ngender) and an open-ended generation task (influenced by race) indicate that\nour model achieves a desirable trade-off between debiasing and task performance\nalong with producing debiased rationales as evidence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zexue He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_B/0/1/0/all/0/1\">Bodhisattwa Prasad Majumder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptable Claim Rewriting with Offline Reinforcement Learning for Effective Misinformation Discovery. (arXiv:2210.07467v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07467","description":"<p>We propose a novel system to help fact-checkers formulate search queries for\nknown misinformation claims and effectively search across multiple social media\nplatforms. We introduce an adaptable rewriting strategy, where editing actions\n(e.g., swap a word with its synonym; change verb tense into present simple) for\nqueries containing claims are automatically learned through offline\nreinforcement learning. Specifically, we use a decision transformer to learn a\nsequence of editing actions that maximize query retrieval metrics such as mean\naverage precision. Through several experiments, we show that our approach can\nincrease the effectiveness of the queries by up to 42\\% relatively, while\nproducing editing action sequences that are human readable, thus making the\nsystem easy to use and explain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kazemi_A/0/1/0/all/0/1\">Ashkan Kazemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abzaliev_A/0/1/0/all/0/1\">Artem Abzaliev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_N/0/1/0/all/0/1\">Naihao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1\">Rui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Davis Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1\">Scott A. Hale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Rosas_V/0/1/0/all/0/1\">Ver&#xf3;nica P&#xe9;rez-Rosas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transparency Helps Reveal When Language Models Learn Meaning. (arXiv:2210.07468v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07468","description":"<p>Many current NLP systems are built from language models trained to optimize\nunsupervised objectives on large amounts of raw text. Under what conditions\nmight such a procedure acquire meaning? Our systematic experiments with\nsynthetic data reveal that, with languages where all expressions have\ncontext-independent denotations (i.e., languages with strong transparency),\nboth autoregressive and masked language models successfully learn to emulate\nsemantic relations between expressions. However, when denotations are changed\nto be context-dependent with the language otherwise unmodified, this ability\ndegrades. Turning to natural language, our experiments with a specific\nphenomenon -- referential opacity -- add to the growing body of evidence that\ncurrent language models do not well-represent natural language semantics. We\nshow this failure relates to the context-dependent nature of natural language\nform-meaning mappings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhaofeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyLEx: Explaining Styles with Lexicon-Based Human Perception. (arXiv:2210.07469v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07469","description":"<p>Style plays a significant role in how humans express themselves and\ncommunicate with others. Large pre-trained language models produce impressive\nresults on various style classification tasks. However, they often learn\nspurious domain-specific words to make predictions. This incorrect word\nimportance learned by the model often leads to ambiguous token-level\nexplanations which do not align with human perception of linguistic styles. To\ntackle this challenge, we introduce StyLEx, a model that learns annotated human\nperceptions of stylistic lexica and uses these stylistic words as additional\ninformation for predicting the style of a sentence. Our experiments show that\nStyLEx can provide human-like stylistic lexical explanations without\nsacrificing the performance of sentence-level style prediction on both original\nand out-of-domain datasets. Explanations from StyLEx show higher sufficiency,\nand plausibility when compared to human annotations, and are also more\nunderstandable by human judges compared to the existing widely-used saliency\nbaseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hayati_S/0/1/0/all/0/1\">Shirley Anugrah Hayati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kyumin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopal_D/0/1/0/all/0/1\">Dheeraj Rajagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1\">Lyle Ungar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"John is 50 years old, can his son be 65?\" Evaluating NLP Models' Understanding of Feasibility. (arXiv:2210.07471v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07471","description":"<p>In current NLP research, large-scale language models and their abilities are\nwidely being discussed. Some recent works have also found notable failures of\nthese models. Often these failure examples involve complex reasoning abilities.\nThis work focuses on a simple commonsense ability, reasoning about when an\naction (or its effect) is feasible. We introduce FeasibilityQA, a\nquestion-answering dataset involving binary classification (BCQ) and\nmulti-choice multi-correct questions (MCQ) that test understanding of\nfeasibility. We show that even state-of-the-art models such as GPT-3 struggle\nto answer the feasibility questions correctly. Specifically, on (MCQ, BCQ)\nquestions, GPT-3 achieves accuracy of just (19%, 62%) and (25%, 64%) in\nzero-shot and few-shot settings, respectively. We also evaluate models by\nproviding relevant knowledge statements required to answer the question and\nfind that the additional knowledge leads to a 7% gain in performance, but the\noverall performance still remains low. These results make one wonder how much\ncommonsense knowledge about action feasibility is encoded in GPT-3 and how well\nthe model can reason about it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_K/0/1/0/all/0/1\">Kuntal Kumar Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawant_S/0/1/0/all/0/1\">Saurabh Arjun Sawant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaria_K/0/1/0/all/0/1\">Kevin Scaria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1\">Siddharth Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Candidate Generation for Entity Linking on Short Social Media Texts. (arXiv:2210.07472v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07472","description":"<p>Entity Linking (EL) is the gateway into Knowledge Bases. Recent advances in\nEL utilize dense retrieval approaches for Candidate Generation, which addresses\nsome of the shortcomings of the Lookup based approach of matching NER mentions\nagainst pre-computed dictionaries. In this work, we show that in the domain of\nTweets, such methods suffer as users often include informal spelling, limited\ncontext, and lack of specificity, among other issues. We investigate these\nchallenges on a large and recent Tweets benchmark for EL, empirically evaluate\nlookup and dense retrieval approaches, and demonstrate a hybrid solution using\nlong contextual representation from Wikipedia is necessary to achieve\nconsiderable gains over previous work, achieving 0.93 recall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hebert_L/0/1/0/all/0/1\">Liam Hebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makki_R/0/1/0/all/0/1\">Raheleh Makki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shubhanshu Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saghir_H/0/1/0/all/0/1\">Hamidreza Saghir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_A/0/1/0/all/0/1\">Anusha Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merhav_Y/0/1/0/all/0/1\">Yuval Merhav</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SQA3D: Situated Question Answering in 3D Scenes. (arXiv:2210.07474v1 [cs.CV])","link":"http://arxiv.org/abs/2210.07474","description":"<p>We propose a new task to benchmark scene understanding of embodied agents:\nSituated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g.,\n3D scan), SQA3D requires the tested agent to first understand its situation\n(position, orientation, etc.) in the 3D scene as described by text, then reason\nabout its surrounding environment and answer a question under that situation.\nBased upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k\nunique situations, along with 20.4k descriptions and 33.4k diverse reasoning\nquestions for these situations. These questions examine a wide spectrum of\nreasoning capabilities for an intelligent agent, ranging from spatial relation\ncomprehension to commonsense understanding, navigation, and multi-hop\nreasoning. SQA3D imposes a significant challenge to current multi-modal\nespecially 3D reasoning models. We evaluate various state-of-the-art approaches\nand find that the best one only achieves an overall score of 47.20%, while\namateur human participants can reach 90.06%. We believe SQA3D could facilitate\nfuture embodied AI research with stronger situation understanding and reasoning\ncapability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yong_S/0/1/0/all/0/1\">Silong Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zilong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yitao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Holistic Sentence Embeddings for Better Out-of-Distribution Detection. (arXiv:2210.07485v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07485","description":"<p>Detecting out-of-distribution (OOD) instances is significant for the safe\ndeployment of NLP models. Among recent textual OOD detection works based on\npretrained language models (PLMs), distance-based methods have shown superior\nperformance. However, they estimate sample distance scores in the last-layer\nCLS embedding space and thus do not make full use of linguistic information\nunderlying in PLMs. To address the issue, we propose to boost OOD detection by\nderiving more holistic sentence embeddings. On the basis of the observations\nthat token averaging and layer combination contribute to improving OOD\ndetection, we propose a simple embedding approach named Avg-Avg, which averages\nall token representations from each intermediate layer as the sentence\nembedding and significantly surpasses the state-of-the-art on a comprehensive\nsuite of benchmarks by a 9.33% FAR95 margin. Furthermore, our analysis\ndemonstrates that it indeed helps preserve general linguistic knowledge in\nfine-tuned PLMs and substantially benefits detecting background shifts. The\nsimple yet effective embedding method can be applied to fine-tuned PLMs with\nnegligible extra costs, providing a free gain in OOD detection. Our code is\navailable at https://github.com/lancopku/Avg-Avg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sishuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_X/0/1/0/all/0/1\">Xiaohan Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Rundong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaFill: Text Infilling for Meta-Path Generation on Heterogeneous Information Networks. (arXiv:2210.07488v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07488","description":"<p>Heterogeneous Information Network (HIN) is essential to study complicated\nnetworks containing multiple edge types and node types. Meta-path, a sequence\nof node types and edge types, is the core technique to embed HINs. Since\nmanually curating meta-paths is time-consuming, there is a pressing need to\ndevelop automated meta-path generation approaches. Existing meta-path\ngeneration approaches cannot fully exploit the rich textual information in\nHINs, such as node names and edge type names. To address this problem, we\npropose MetaFill, a text-infilling-based approach for meta-path generation. The\nkey idea of MetaFill is to formulate meta-path identification problem as a word\nsequence infilling problem, which can be advanced by Pretrained Language Models\n(PLMs). We observed the superior performance of MetaFill against existing\nmeta-path generation methods and graph embedding methods that do not leverage\nmeta-paths in both link prediction and node classification on two real-world\nHIN datasets. We further demonstrated how MetaFill can accurately classify\nedges in the zero-shot setting, where existing approaches cannot generate any\nmeta-paths. MetaFill exploits PLMs to generate meta-paths for graph embedding,\nopening up new avenues for language model applications in graph analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zequn Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_K/0/1/0/all/0/1\">Kefei Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Junwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hanwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Psychology-guided Controllable Story Generation. (arXiv:2210.07493v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07493","description":"<p>Controllable story generation is a challenging task in the field of NLP,\nwhich has attracted increasing research interest in recent years. However, most\nexisting works generate a whole story conditioned on the appointed keywords or\nemotions, ignoring the psychological changes of the protagonist. Inspired by\npsychology theories, we introduce global psychological state chains, which\ninclude the needs and emotions of the protagonists, to help a story generation\nsystem create more controllable and well-planned stories. In this paper, we\npropose a Psychology-guIded Controllable Story Generation System (PICS) to\ngenerate stories that adhere to the given leading context and desired\npsychological state chains for the protagonist. Specifically, psychological\nstate trackers are employed to memorize the protagonist's local psychological\nstates to capture their inner temporal relationships. In addition,\npsychological state planners are adopted to gain the protagonist's global\npsychological states for story planning. Eventually, a psychology controller is\ndesigned to integrate the local and global psychological states into the story\ncontext representation for composing psychology-guided stories. Automatic and\nmanual evaluations demonstrate that PICS outperforms baselines, and each part\nof PICS shows effectiveness for writing stories with more consistent\npsychological changes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuqiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_G/0/1/0/all/0/1\">Guanqun Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Luxi Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayes risk CTC: Controllable CTC alignment in Sequence-to-Sequence tasks. (arXiv:2210.07499v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07499","description":"<p>Sequence-to-Sequence (seq2seq) tasks transcribe the input sequence to a\ntarget sequence. The Connectionist Temporal Classification (CTC) criterion is\nwidely used in multiple seq2seq tasks. Besides predicting the target sequence,\na side product of CTC is to predict the alignment, which is the most probable\ninput-long sequence that specifies a hard aligning relationship between the\ninput and target units. As there are multiple potential aligning sequences\n(called paths) that are equally considered in CTC formulation, the choice of\nwhich path will be most probable and become the predicted alignment is always\nuncertain. In addition, it is usually observed that the alignment predicted by\nvanilla CTC will drift compared with its reference and rarely provides\npractical functionalities. Thus, the motivation of this work is to make the CTC\nalignment prediction controllable and thus equip CTC with extra\nfunctionalities. The Bayes risk CTC (BRCTC) criterion is then proposed in this\nwork, in which a customizable Bayes risk function is adopted to enforce the\ndesired characteristics of the predicted alignment. With the risk function, the\nBRCTC is a general framework to adopt some customizable preference over the\npaths in order to concentrate the posterior into a particular subset of the\npaths. In applications, we explore one particular preference which yields\nmodels with the down-sampling ability and reduced inference costs. By using\nBRCTC with another preference for early emissions, we obtain an improved\nperformance-latency trade-off for online models. Experimentally, the proposed\nBRCTC reduces the inference cost of offline models by up to 47% without\nperformance degradation and cuts down the overall latency of online systems to\nan unseen level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jinchuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_C/0/1/0/all/0/1\">Chao Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Language Representation Models Think in Bets?. (arXiv:2210.07519v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07519","description":"<p>In recent years, transformer-based language representation models (LRMs) have\nachieved state-of-the-art results on difficult natural language understanding\nproblems, such as question answering and text summarization. As these models\nare integrated into real-world applications, evaluating their ability to make\nrational decisions is an important research agenda, with practical\nramifications. This article investigates LRMs' rational decision-making ability\nthrough a carefully designed set of decision-making benchmarks and experiments.\nInspired by classic work in cognitive science, we model the decision-making\nproblem as a bet. We then investigate an LRM's ability to choose outcomes that\nhave optimal, or at minimum, positive expected gain. Through a robust body of\nexperiments on four established LRMs, we show that a model is only able to\n`think in bets' if it is first fine-tuned on bet questions with an identical\nstructure. Modifying the bet question's structure, while still retaining its\nfundamental characteristics, decreases an LRM's performance by more than 25\\%,\non average, although absolute performance remains well above random. LRMs are\nalso found to be more rational when selecting outcomes with non-negative\nexpected gain, rather than optimal or strictly positive expected gain. Our\nresults suggest that LRMs could potentially be applied to tasks that rely on\ncognitive decision-making skills, but that more research is necessary before\nthey can robustly make rational decisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhisheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kejriwal_M/0/1/0/all/0/1\">Mayank Kejriwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Adaptive Named Entity Recognition by Retrieving Unstructured Knowledge. (arXiv:2210.07523v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07523","description":"<p>Although named entity recognition (NER) helps us to extract various\ndomain-specific entities from text (e.g., artists in the music domain), it is\ncostly to create a large amount of training data or a structured knowledge base\nto perform accurate NER in the target domain. Here, we propose self-adaptive\nNER, where the model retrieves the external knowledge from unstructured text to\nlearn the usage of entities that has not been learned well. To retrieve useful\nknowledge for NER, we design an effective two-stage model that retrieves\nunstructured knowledge using uncertain entities as queries. Our model first\npredicts the entities in the input and then finds the entities of which the\nprediction is not confident. Then, our model retrieves knowledge by using these\nuncertain entities as queries and concatenates the retrieved text to the\noriginal input to revise the prediction. Experiments on CrossNER datasets\ndemonstrated that our model outperforms the strong NERBERT baseline by 2.45\npoints on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nishida_K/0/1/0/all/0/1\">Kosuke Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshinaga_N/0/1/0/all/0/1\">Naoki Yoshinaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishida_K/0/1/0/all/0/1\">Kyosuke Nishida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoMoE: Neural Architecture Search for Efficient Sparsely Activated Transformers. (arXiv:2210.07535v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07535","description":"<p>Neural architecture search (NAS) has demonstrated promising results on\nidentifying efficient Transformer architectures which outperform manually\ndesigned ones for natural language tasks like neural machine translation (NMT).\nExisting NAS methods operate on a space of dense architectures, where all of\nthe sub-architecture weights are activated for every input. Motivated by the\nrecent advances in sparsely activated models like the Mixture-of-Experts (MoE)\nmodel, we introduce sparse architectures with conditional computation into the\nNAS search space. Given this expressive search space which subsumes prior\ndensely activated architectures, we develop a new framework AutoMoE to search\nfor efficient sparsely activated sub-Transformers. AutoMoE-generated sparse\nmodels obtain (i) 3x FLOPs reduction over manually designed dense Transformers\nand (ii) 23% FLOPs reduction over state-of-the-art NAS-generated dense\nsub-Transformers with parity in BLEU score on benchmark datasets for NMT.\nAutoMoE consists of three training phases: (a) Heterogeneous search space\ndesign with dense and sparsely activated Transformer modules (e.g., how many\nexperts? where to place them? what should be their sizes?); (b) SuperNet\ntraining that jointly trains several subnetworks sampled from the large search\nspace by weight-sharing; (c) Evolutionary search for the architecture with the\noptimal trade-off between task performance and computational constraint like\nFLOPs and latency. AutoMoE code, data and trained models are available at\nhttps://github.com/microsoft/AutoMoE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_G/0/1/0/all/0/1\">Ganesh Jawahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshmanan_L/0/1/0/all/0/1\">Laks V. S. Lakshmanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1\">Sebastien Bubeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The User-Aware Arabic Gender Rewriter. (arXiv:2210.07538v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07538","description":"<p>We introduce the User-Aware Arabic Gender Rewriter, a user-centric web-based\nsystem for Arabic gender rewriting in contexts involving two users. The system\ntakes either Arabic or English sentences as input, and provides users with the\nability to specify their desired first and/or second person target genders. The\nsystem outputs gender rewritten alternatives of the Arabic input sentences (or\ntheir Arabic translations in case of English input) to match the target users'\ngender preferences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alhafni_B/0/1/0/all/0/1\">Bashar Alhafni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obeid_O/0/1/0/all/0/1\">Ossama Obeid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Watermarking Pre-trained Language Models with Backdooring. (arXiv:2210.07543v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07543","description":"<p>Large pre-trained language models (PLMs) have proven to be a crucial\ncomponent of modern natural language processing systems. PLMs typically need to\nbe fine-tuned on task-specific downstream datasets, which makes it hard to\nclaim the ownership of PLMs and protect the developer's intellectual property\ndue to the catastrophic forgetting phenomenon. We show that PLMs can be\nwatermarked with a multi-task learning framework by embedding backdoors\ntriggered by specific inputs defined by the owners, and those watermarks are\nhard to remove even though the watermarked PLMs are fine-tuned on multiple\ndownstream tasks. In addition to using some rare words as triggers, we also\nshow that the combination of common words can be used as backdoor triggers to\navoid them being easily detected. Extensive experiments on multiple datasets\ndemonstrate that the embedded watermarks can be robustly extracted with a high\nsuccess rate and less influenced by the follow-up fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_C/0/1/0/all/0/1\">Chenxi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chengsong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Legal Case Document Summarization: Extractive and Abstractive Methods and their Evaluation. (arXiv:2210.07544v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07544","description":"<p>Summarization of legal case judgement documents is a challenging problem in\nLegal NLP. However, not much analyses exist on how different families of\nsummarization models (e.g., extractive vs. abstractive) perform when applied to\nlegal case documents. This question is particularly important since many recent\ntransformer-based abstractive summarization models have restrictions on the\nnumber of input tokens, and legal documents are known to be very long. Also, it\nis an open question on how best to evaluate legal case document summarization\nsystems. In this paper, we carry out extensive experiments with several\nextractive and abstractive summarization methods (both supervised and\nunsupervised) over three legal summarization datasets that we have developed.\nOur analyses, that includes evaluation by law practitioners, lead to several\ninteresting insights on legal summarization in specific and long document\nsummarization in general.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shukla_A/0/1/0/all/0/1\">Abhay Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_P/0/1/0/all/0/1\">Paheli Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poddar_S/0/1/0/all/0/1\">Soham Poddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_R/0/1/0/all/0/1\">Rajdeep Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_K/0/1/0/all/0/1\">Kripabandhu Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Saptarshi Ghosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kernel-Whitening: Overcome Dataset Bias with Isotropic Sentence Embedding. (arXiv:2210.07547v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07547","description":"<p>Dataset bias has attracted increasing attention recently for its detrimental\neffect on the generalization ability of fine-tuned models. The current\nmainstream solution is designing an additional shallow model to pre-identify\nbiased instances. However, such two-stage methods scale up the computational\ncomplexity of training process and obstruct valid feature information while\nmitigating bias. To address this issue, we utilize the representation\nnormalization method which aims at disentangling the correlations between\nfeatures of encoded sentences. We find it also promising in eliminating the\nbias problem by providing isotropic data distribution. We further propose\nKernel-Whitening, a Nystrom kernel approximation method to achieve more\nthorough debiasing on nonlinear spurious correlations. Our framework is\nend-to-end with similar time consumption to fine-tuning. Experiments show that\nKernel-Whitening significantly improves the performance of BERT on\nout-of-distribution datasets while maintaining in-distribution accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Songyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1\">Shihan Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation. (arXiv:2210.07558v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07558","description":"<p>With the ever-growing size of pre-trained models (PMs), fine-tuning them has\nbecome more expensive and resource-hungry. As a remedy, low-rank adapters\n(LoRA) keep the main pre-trained weights of the model frozen and just introduce\nsome learnable truncated SVD modules (so-called LoRA blocks) to the model.\nWhile LoRA blocks are parameter efficient, they suffer from two major problems:\nfirst, the size of these blocks is fixed and cannot be modified after training\n(for example, if we need to change the rank of LoRA blocks, then we need to\nre-train them from scratch); second, optimizing their rank requires an\nexhaustive search and effort. In this work, we introduce a dynamic low-rank\nadaptation (DyLoRA) technique to address these two problems together. Our\nDyLoRA method trains LoRA blocks for a range of ranks instead of a single rank\nby sorting out the representation learned by the adapter module at different\nranks during training. We evaluate our solution on different tasks of the GLUE\nbenchmark using the RoBERTa model. Our results show that we can train dynamic\nsearch-free models with DyLoRA at least $7\\times$ faster than LoRA without\nsignificantly compromising performance. Moreover, our models can perform\nconsistently well on a much larger range of ranks compared to LoRA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valipour_M/0/1/0/all/0/1\">Mojtaba Valipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobyzev_I/0/1/0/all/0/1\">Ivan Kobyzev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empirical Study Incorporating Linguistic Knowledge on Filled Pauses for Personalized Spontaneous Speech Synthesis. (arXiv:2210.07559v1 [cs.SD])","link":"http://arxiv.org/abs/2210.07559","description":"<p>We present a comprehensive empirical study for personalized spontaneous\nspeech synthesis on the basis of linguistic knowledge. With the advent of voice\ncloning for reading-style speech synthesis, a new voice cloning paradigm for\nhuman-like and spontaneous speech synthesis is required. We, therefore, focus\non personalized spontaneous speech synthesis that can clone both the\nindividual's voice timbre and speech disfluency. Specifically, we deal with\nfilled pauses, a major source of speech disfluency, which is known to play an\nimportant role in speech generation and communication in psychology and\nlinguistics. To comparatively evaluate personalized filled pause insertion and\nnon-personalized filled pause prediction methods, we developed a speech\nsynthesis method with a non-personalized external filled pause predictor\ntrained with a multi-speaker corpus. The results clarify the position-word\nentanglement of filled pauses, i.e., the necessity of precisely predicting\npositions for naturalness and the necessity of precisely predicting words for\nindividuality on the evaluation of synthesized speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsunaga_Y/0/1/0/all/0/1\">Yuta Matsunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeki_T/0/1/0/all/0/1\">Takaaki Saeki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takamichi_S/0/1/0/all/0/1\">Shinnosuke Takamichi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saruwatari_H/0/1/0/all/0/1\">Hiroshi Saruwatari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Q-TOD: A Query-driven Task-oriented Dialogue System. (arXiv:2210.07564v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07564","description":"<p>Existing pipelined task-oriented dialogue systems usually have difficulties\nadapting to unseen domains, whereas end-to-end systems are plagued by\nlarge-scale knowledge bases in practice. In this paper, we introduce a novel\nquery-driven task-oriented dialogue system, namely Q-TOD. The essential\ninformation from the dialogue context is extracted into a query, which is\nfurther employed to retrieve relevant knowledge records for response\ngeneration. Firstly, as the query is in the form of natural language and not\nconfined to the schema of the knowledge base, the issue of domain adaption is\nalleviated remarkably in Q-TOD. Secondly, as the query enables the decoupling\nof knowledge retrieval from the generation, Q-TOD gets rid of the issue of\nknowledge base scalability. To evaluate the effectiveness of the proposed\nQ-TOD, we collect query annotations for three publicly available task-oriented\ndialogue datasets. Comprehensive experiments verify that Q-TOD outperforms\nstrong baselines and establishes a new state-of-the-art performance on these\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingzhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mengfei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1\">Siqi Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Huang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shuqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Pre-Training of Modular Prompt for Few-Shot Learning. (arXiv:2210.07565v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07565","description":"<p>Prompt tuning is a parameter-efficient approach to adapting pre-trained\nlanguage models to downstream tasks. Although prompt tuning has been shown to\nmatch the performance of full model tuning when training data is sufficient, it\ntends to struggle in few-shot learning settings. In this paper, we present\nMulti-task Pre-trained Modular Prompt (MP2) to boost prompt tuning for few-shot\nlearning. MP2 is a set of combinable prompts pre-trained on 38 Chinese tasks.\nOn downstream tasks, the pre-trained prompts are selectively activated and\ncombined, leading to strong compositional generalization to unseen tasks. To\nbridge the gap between pre-training and fine-tuning, we formulate upstream and\ndownstream tasks into a unified machine reading comprehension task. Extensive\nexperiments under two learning paradigms, i.e., gradient descent and black-box\ntuning, show that MP2 significantly outperforms prompt tuning, full model\ntuning, and prior prompt pre-training methods in few-shot settings. In\naddition, we demonstrate that MP2 can achieve surprisingly fast and strong\nadaptation to downstream tasks by merely learning 8 parameters to combine the\npre-trained modular prompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhengfu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Parameters Associated with the Quality of Benchmarks in NLP. (arXiv:2210.07566v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07566","description":"<p>Several benchmarks have been built with heavy investment in resources to\ntrack our progress in NLP. Thousands of papers published in response to those\nbenchmarks have competed to top leaderboards, with models often surpassing\nhuman performance. However, recent studies have shown that models triumph over\nseveral popular benchmarks just by overfitting on spurious biases, without\ntruly learning the desired task. Despite this finding, benchmarking, while\ntrying to tackle bias, still relies on workarounds, which do not fully utilize\nthe resources invested in benchmark creation, due to the discarding of low\nquality data, and cover limited sets of bias. A potential solution to these\nissues -- a metric quantifying quality -- remains underexplored. Inspired by\nsuccessful quality indices in several domains such as power, food, and water,\nwe take the first step towards a metric by identifying certain language\nproperties that can represent various possible interactions leading to biases\nin a benchmark. We look for bias related parameters which can potentially help\npave our way towards the metric. We survey existing works and identify\nparameters capturing various properties of bias, their origins, types and\nimpact on performance, generalization, and robustness. Our analysis spans over\ndatasets and a hierarchy of tasks ranging from NLI to Summarization, ensuring\nthat our parameters are generic and are not overfitted towards a specific task\nor dataset. We also develop certain parameters in this process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arunkumar_A/0/1/0/all/0/1\">Anjana Arunkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bryan_C/0/1/0/all/0/1\">Chris Bryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MICO: A Multi-alternative Contrastive Learning Framework for Commonsense Knowledge Representation. (arXiv:2210.07570v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07570","description":"<p>Commonsense reasoning tasks such as commonsense knowledge graph completion\nand commonsense question answering require powerful representation learning. In\nthis paper, we propose to learn commonsense knowledge representation by MICO, a\nMulti-alternative contrastve learning framework on COmmonsense knowledge graphs\n(MICO). MICO generates the commonsense knowledge representation by contextual\ninteraction between entity nodes and relations with multi-alternative\ncontrastive learning. In MICO, the head and tail entities in an $(h,r,t)$\nknowledge triple are converted to two relation-aware sequence pairs (a premise\nand an alternative) in the form of natural language. Semantic representations\ngenerated by MICO can benefit the following two tasks by simply comparing the\ndistance score between the representations: 1) zero-shot commonsense question\nanswering task; 2) inductive commonsense knowledge graph completion task.\nExtensive experiments show the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Ying Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1\">Tianqing Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations. (arXiv:2210.07586v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07586","description":"<p>Most weakly supervised named entity recognition (NER) models rely on\ndomain-specific dictionaries provided by experts. This approach is infeasible\nin many domains where dictionaries do not exist. While a phrase retrieval model\nwas used to construct pseudo-dictionaries with entities retrieved from\nWikipedia automatically in a recent study, these dictionaries often have\nlimited coverage because the retriever is likely to retrieve popular entities\nrather than rare ones. In this study, a phrase embedding search to efficiently\ncreate high-coverage dictionaries is presented. Specifically, the reformulation\nof natural language queries into phrase representations allows the retriever to\nsearch a space densely populated with various entities. In addition, we present\na novel framework, HighGEN, that generates NER datasets with high-coverage\ndictionaries obtained using the phrase embedding search. HighGEN generates weak\nlabels based on the distance between the embeddings of a candidate phrase and\ntarget entity type to reduce the noise in high-coverage dictionaries. We\ncompare HighGEN with current weakly supervised NER models on six NER benchmarks\nand demonstrate the superiority of our models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jaehyo Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConEntail: An Entailment-based Framework for Universal Zero and Few Shot Classification with Supervised Contrastive Pretraining. (arXiv:2210.07587v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07587","description":"<p>A universal classification model aims to generalize to diverse classification\ntasks in both zero and few shot settings. A promising way toward universal\nclassification is to cast heterogeneous data formats into a dataset-agnostic\n\"meta-task\" (e.g., textual entailment, question answering) then pretrain a\nmodel on the combined meta dataset. The existing work is either pretrained on\nspecific subsets of classification tasks, or pretrained on both classification\nand generation data but the model could not fulfill its potential in\nuniversality and reliability. These also leave a massive amount of annotated\ndata under-exploited. To fill these gaps, we propose ConEntail, a new framework\nfor universal zero and few shot classification with supervised contrastive\npretraining. Our unified meta-task for classification is based on nested\nentailment. It can be interpreted as \"Does sentence a entails [sentence b\nentails label c]\". This formulation enables us to make better use of 57\nannotated classification datasets for supervised contrastive pretraining and\nuniversal evaluation. In this way, ConEntail helps the model (1) absorb\nknowledge from different datasets, and (2) gain consistent performance gain\nwith more pretraining data. In experiments, we compare our model with\ndiscriminative and generative models pretrained on the same dataset. The\nresults confirm that our framework effectively exploits existing annotated data\nand consistently outperforms baselines in both zero (9.4% average improvement)\nand few shot settings (3.5% average improvement).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1\">Aysa Xuemo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The State of Profanity Obfuscation in Natural Language Processing. (arXiv:2210.07595v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07595","description":"<p>Work on hate speech has made the consideration of rude and harmful examples\nin scientific publications inevitable. This raises various problems, such as\nwhether or not to obscure profanities. While science must accurately disclose\nwhat it does, the unwarranted spread of hate speech is harmful to readers, and\nincreases its internet frequency. While maintaining publications' professional\nappearance, obfuscating profanities makes it challenging to evaluate the\ncontent, especially for non-native speakers. Surveying 150 ACL papers, we\ndiscovered that obfuscation is usually employed for English but not other\nlanguages, and even so quite uneven. We discuss the problems with obfuscation\nand suggest a multilingual community resource called PrOf that has a Python\nmodule to standardize profanity obfuscation processes. We believe PrOf can help\nscientific publication policies to make hate speech work accessible and\ncomparable, irrespective of language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nozza_D/0/1/0/all/0/1\">Debora Nozza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mention Annotations Alone Enable Efficient Domain Adaptation for Coreference Resolution. (arXiv:2210.07602v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07602","description":"<p>Although, recent advances in neural network models for coreference resolution\nhave led to substantial improvements on benchmark datasets, it remains a\nchallenge to successfully transfer those models to new target domains\ncontaining many out-of-vocabulary spans and requiring differing annotation\nschemes. Typical approaches for domain adaptation involve continued training on\ncoreference annotations in the target domain, but obtaining those annotations\nis costly and time-consuming. In this work, we show that adapting mention\ndetection is the key component to successful domain adaptation of coreference\nmodels, rather than antecedent linking. Through timed annotation experiments,\nwe also show annotating mentions alone is nearly twice as fast as annotating\nfull coreference chains. Based on these insights, we propose a method for\neffectively adapting coreference models that requires only mention annotations\nin the target domain. We use an auxiliary mention detection objective trained\nwith mention examples in the target domain resulting in higher mention\nprecision. We demonstrate that our approach facilitates sample- and\ntime-efficient transfer to new annotation schemes and lexicons in extensive\nevaluation across three English coreference datasets: CoNLL-2012\n(news/conversation), i2b2/VA (medical case notes), and a dataset of child\nwelfare case notes. We show that annotating mentions results in 7-14%\nimprovement in average F1 over annotating coreference over an equivalent amount\nof time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_N/0/1/0/all/0/1\">Nupoor Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Field_A/0/1/0/all/0/1\">Anjalie Field</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1\">Emma Strubell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense-ATOMIC: Construction of Densely-connected and Multi-hop Commonsense Knowledge Graph upon ATOMIC. (arXiv:2210.07621v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07621","description":"<p>ATOMIC is a large-scale commonsense knowledge graph (CSKG) containing\neveryday if-then knowledge triplets, i.e., {head event, relation, tail event}.\nThe one-hop annotation manner made ATOMIC a set of independent bipartite\ngraphs, which ignored the numerous missing links between events in different\nbipartite graphs and consequently caused shortcomings in knowledge coverage and\nmulti-hop reasoning. To address these issues, we propose a CSKG completion\napproach by training a relation prediction model based on a set of existing\ntriplets, and infer the missing links on ATOMIC. On this basis, we construct\nDense-ATOMIC, a densely-connected and multi-hop commonsense knowledge graph.\nThe experimental results on an annotated dense subgraph demonstrate the\neffectiveness of our CSKG completion approach upon ATOMIC. The evaluation on a\ndownstream commonsense reasoning task also proves the advantage of Dense-ATOMIC\nagainst conventional ATOMIC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiangqing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Siwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_R/0/1/0/all/0/1\">Rui Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation. (arXiv:2210.07626v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07626","description":"<p>Automatic evaluation metrics are crucial to the development of generative\nsystems. In recent years, pre-trained language model (PLM) based metrics, such\nas BERTScore, have been commonly adopted in various generation tasks. However,\nit has been demonstrated that PLMs encode a range of stereotypical societal\nbiases, leading to a concern on the fairness of PLMs as metrics. To that end,\nthis work presents the first systematic study on the social bias in PLM-based\nmetrics. We demonstrate that popular PLM-based metrics exhibit significantly\nhigher social bias than traditional metrics on 6 sensitive attributes, namely\nrace, gender, religion, physical appearance, age, and socioeconomic status.\nIn-depth analysis suggests that choosing paradigms (matching, regression, or\ngeneration) of the metric has a greater impact on fairness than choosing PLMs.\nIn addition, we develop debiasing adapters that are injected into PLM layers,\nmitigating bias in PLM-based metrics while retaining high performance for\nevaluating text generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hardness of Samples Need to be Quantified for a Reliable Evaluation System: Exploring Potential Opportunities with a New Task. (arXiv:2210.07631v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07631","description":"<p>Evaluation of models on benchmarks is unreliable without knowing the degree\nof sample hardness; this subsequently overestimates the capability of AI\nsystems and limits their adoption in real world applications. We propose a Data\nScoring task that requires assignment of each unannotated sample in a benchmark\na score between 0 to 1, where 0 signifies easy and 1 signifies hard. Use of\nunannotated samples in our task design is inspired from humans who can\ndetermine a question difficulty without knowing its correct answer. This also\nrules out the use of methods involving model based supervision (since they\nrequire sample annotations to get trained), eliminating potential biases\nassociated with models in deciding sample difficulty. We propose a method based\non Semantic Textual Similarity (STS) for this task; we validate our method by\nshowing that existing models are more accurate with respect to the easier\nsample-chunks than with respect to the harder sample-chunks. Finally we\ndemonstrate five novel applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arunkumar_A/0/1/0/all/0/1\">Anjana Arunkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bryan_C/0/1/0/all/0/1\">Chris Bryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training speech emotion classifier without categorical annotations. (arXiv:2210.07642v1 [cs.SD])","link":"http://arxiv.org/abs/2210.07642","description":"<p>There are two paradigms of emotion representation, categorical labeling and\ndimensional description in continuous space. Therefore, the emotion recognition\ntask can be treated as a classification or regression. The main aim of this\nstudy is to investigate the relation between these two representations and\npropose a classification pipeline that uses only dimensional annotation. The\nproposed approach contains a regressor model which is trained to predict a\nvector of continuous values in dimensional representation for given speech\naudio. The output of this model can be interpreted as an emotional category\nusing a mapping algorithm. We investigated the performances of a combination of\nthree feature extractors, three neural network architectures, and three mapping\nalgorithms on two different corpora. Our study shows the advantages and\nlimitations of the classification via regression approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shamsi_M/0/1/0/all/0/1\">Meysam Shamsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tahon_M/0/1/0/all/0/1\">Marie Tahon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enabling Classifiers to Make Judgements Explicitly Aligned with Human Values. (arXiv:2210.07652v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07652","description":"<p>Many NLP classification tasks, such as sexism/racism detection or toxicity\ndetection, are based on human values. Yet, human values can vary under diverse\ncultural conditions. Therefore, we introduce a framework for value-aligned\nclassification that performs prediction based on explicitly written human\nvalues in the command. Along with the task, we propose a practical approach\nthat distills value-aligned knowledge from large-scale language models (LLMs)\nto construct value-aligned classifiers in two steps. First, we generate\nvalue-aligned training data from LLMs by prompt-based few-shot learning. Next,\nwe fine-tune smaller classification models with the generated data for the\ntask. Empirical results show that our VA-Models surpass multiple baselines by\nat least 15.56% on the F1-score, including few-shot learning with OPT-175B and\nexisting text augmentation methods. We suggest that using classifiers with\nexplicit human value input improves both inclusivity &amp; explainability in AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bang_Y/0/1/0/all/0/1\">Yejin Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhaojiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1\">Mona Diab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pretrained Transformers Do not Always Improve Robustness. (arXiv:2210.07663v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07663","description":"<p>Pretrained Transformers (PT) have been shown to improve Out of Distribution\n(OOD) robustness than traditional models such as Bag of Words (BOW), LSTMs,\nConvolutional Neural Networks (CNN) powered by Word2Vec and Glove embeddings.\nHow does the robustness comparison hold in a real world setting where some part\nof the dataset can be noisy? Do PT also provide more robust representation than\ntraditional models on exposure to noisy data? We perform a comparative study on\n10 models and find an empirical evidence that PT provide less robust\nrepresentation than traditional models on exposure to noisy data. We\ninvestigate further and augment PT with an adversarial filtering (AF) mechanism\nthat has been shown to improve OOD generalization. However, increase in\ngeneralization does not necessarily increase robustness, as we find that noisy\ndata fools the AF method powered by PT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_B/0/1/0/all/0/1\">Bhavdeep Singh Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training. (arXiv:2210.07688v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07688","description":"<p>Large-scale vision-language pre-trained (VLP) models are prone to hallucinate\nnon-existent visual objects when generating text based on visual information.\nIn this paper, we exhaustively probe the object hallucination problem from\nthree aspects. First, we examine various state-of-the-art VLP models, showing\nthat models achieving better scores on standard metrics(e.g., BLEU-4, CIDEr)\ncould hallucinate objects more frequently. Second, we investigate how different\ntypes of visual features in VLP influence hallucination, including\nregion-based, grid-based, and patch-based. Surprisingly, we find that\npatch-based features perform the best and smaller patch resolution yields a\nnon-trivial reduction in object hallucination. Third, we decouple various VLP\nobjectives and demonstrate their effectiveness in alleviating object\nhallucination. Based on that, we propose a new pre-training loss, object masked\nlanguage modeling, to further reduce object hallucination. We evaluate models\non both COCO (in-domain) and NoCaps (out-of-domain) datasets with our improved\nCHAIR metric. Furthermore, we investigate the effects of various text decoding\nstrategies and image augmentation methods on object hallucination.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Ziwei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey. (arXiv:2210.07700v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07700","description":"<p>Recent advances in the capacity of large language models to generate\nhuman-like text have resulted in their increased adoption in user-facing\nsettings. In parallel, these improvements have prompted a heated discourse\naround the risks of societal harms they introduce, whether inadvertent or\nmalicious. Several studies have identified potential causes of these harms and\ncalled for their mitigation via development of safer and fairer models. Going\nbeyond enumerating the risks of harms, this work provides a survey of practical\nmethods for addressing potential threats and societal harms from language\ngeneration models. We draw on several prior works' taxonomies of language model\nrisks to present a structured overview of strategies for detecting and\nameliorating different kinds of risks/harms of language generators. Bridging\ndiverse strands of research, this survey aims to serve as a practical guide for\nboth LM researchers and practitioners with explanations of motivations behind\ndifferent mitigation strategies, their limitations, and open problems for\nfuture research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sachin Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandran_V/0/1/0/all/0/1\">Vidhisha Balachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Njoo_L/0/1/0/all/0/1\">Lucille Njoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Category Discovery under Coarse-grained supervision with Hierarchical Weighted Self-contrastive Learning. (arXiv:2210.07733v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07733","description":"<p>Novel category discovery aims at adapting models trained on known categories\nto novel categories. Previous works only focus on the scenario where known and\nnovel categories are of the same granularity. In this paper, we investigate a\nnew practical scenario called Fine-grained Category Discovery under\nCoarse-grained supervision (FCDC). FCDC aims at discovering fine-grained\ncategories with only coarse-grained labeled data, which can adapt models to\ncategories of different granularity from known ones and reduce significant\nlabeling cost. It is also a challenging task since supervised training on\ncoarse-grained categories tends to focus on inter-class distance (distance\nbetween coarse-grained classes) but ignore intra-class distance (distance\nbetween fine-grained sub-classes) which is essential for separating\nfine-grained categories. Considering most current methods cannot transfer\nknowledge from coarse-grained level to fine-grained level, we propose a\nhierarchical weighted self-contrastive network by building a novel weighted\nself-contrastive module and combining it with supervised learning in a\nhierarchical manner. Extensive experiments on public datasets show both\neffectiveness and efficiency of our model over compared methods. Code and data\nare available at https://github.com/Lackel/Hierarchical_Weighted_SCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1\">Wenbin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_F/0/1/0/all/0/1\">Feng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Ping Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qinghua Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">QianYing Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Confidence estimation of classification based on the distribution of the neural network output layer. (arXiv:2210.07745v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07745","description":"<p>One of the most common problems preventing the application of prediction\nmodels in the real world is lack of generalization: The accuracy of models,\nmeasured in the benchmark does repeat itself on future data, e.g. in the\nsettings of real business. There is relatively little methods exist that\nestimate the confidence of prediction models. In this paper, we propose novel\nmethods that, given a neural network classification model, estimate uncertainty\nof particular predictions generated by this model. Furthermore, we propose a\nmethod that, given a model and a confidence level, calculates a threshold that\nseparates prediction generated by this model into two subsets, one of them\nmeets the given confidence level. In contrast to other methods, the proposed\nmethods do not require any changes on existing neural networks, because they\nsimply build on the output logit layer of a common neural network. In\nparticular, the methods infer the confidence of a particular prediction based\non the distribution of the logit values corresponding to this prediction. The\nproposed methods constitute a tool that is recommended for filtering\npredictions in the process of knowledge extraction, e.g. based on web\nscrapping, where predictions subsets are identified that maximize the precision\non cost of the recall, which is less important due to the availability of data.\nThe method has been tested on different tasks including relation extraction,\nnamed entity recognition and image classification to show the significant\nincrease of accuracy achieved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taha_A/0/1/0/all/0/1\">Abdel Aziz Taha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennig_L/0/1/0/all/0/1\">Leonhard Hennig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoth_P/0/1/0/all/0/1\">Petr Knoth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Cultural Commonsense Knowledge at Scale. (arXiv:2210.07763v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07763","description":"<p>Structured knowledge is important for many AI applications. Commonsense\nknowledge, which is crucial for robust human-centric AI, is covered by a small\nnumber of structured knowledge projects. However, they lack knowledge about\nhuman traits and behaviors conditioned on socio-cultural contexts, which is\ncrucial for situative AI. This paper presents CANDLE, an end-to-end methodology\nfor extracting high-quality cultural commonsense knowledge (CCSK) at scale.\nCANDLE extracts CCSK assertions from a huge web corpus and organizes them into\ncoherent clusters, for 3 domains of subjects (geography, religion, occupation)\nand several cultural facets (food, drinks, clothing, traditions, rituals,\nbehaviors). CANDLE includes judicious techniques for classification-based\nfiltering and scoring of interestingness. Experimental evaluations show the\nsuperiority of the CANDLE CCSK collection over prior works, and an extrinsic\nuse case demonstrates the benefits of CCSK for the GPT-3 language model. Code\nand data can be accessed at https://cultural-csk.herokuapp.com/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan-Phong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1\">Simon Razniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varde_A/0/1/0/all/0/1\">Aparna Varde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Jointly Transcribe and Subtitle for End-to-End Spontaneous Speech Recognition. (arXiv:2210.07771v1 [eess.AS])","link":"http://arxiv.org/abs/2210.07771","description":"<p>TV subtitles are a rich source of transcriptions of many types of speech,\nranging from read speech in news reports to conversational and spontaneous\nspeech in talk shows and soaps. However, subtitles are not verbatim (i.e.\nexact) transcriptions of speech, so they cannot be used directly to improve an\nAutomatic Speech Recognition (ASR) model. We propose a multitask dual-decoder\nTransformer model that jointly performs ASR and automatic subtitling. The ASR\ndecoder (possibly pre-trained) predicts the verbatim output and the subtitle\ndecoder generates a subtitle, while sharing the encoder. The two decoders can\nbe independent or connected. The model is trained to perform both tasks\njointly, and is able to effectively use subtitle data. We show improvements on\nregular ASR and on spontaneous and conversational ASR by incorporating the\nadditional subtitle decoder. The method does not require preprocessing\n(aligning, filtering, pseudo-labeling, ...) of the subtitles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Poncelet_J/0/1/0/all/0/1\">Jakob Poncelet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+hamme_H/0/1/0/all/0/1\">Hugo Van hamme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEATHER: A Framework for Learning to Generate Human-like Text in Dialogue. (arXiv:2210.07777v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07777","description":"<p>Algorithms for text-generation in dialogue can be misguided. For example, in\ntask-oriented settings, reinforcement learning that optimizes only task-success\ncan lead to abysmal lexical diversity. We hypothesize this is due to poor\ntheoretical understanding of the objectives in text-generation and their\nrelation to the learning process (i.e., model training). To this end, we\npropose a new theoretical framework for learning to generate text in dialogue.\nCompared to existing theories of learning, our framework allows for analysis of\nthe multi-faceted goals inherent to text-generation. We use our framework to\ndevelop theoretical guarantees for learners that adapt to unseen data. As an\nexample, we apply our theory to study data-shift within a cooperative learning\nalgorithm proposed for the GuessWhat?! visual dialogue game. From this insight,\nwe propose a new algorithm, and empirically, we demonstrate our proposal\nimproves both task-success and human-likeness of the generated text. Finally,\nwe show statistics from our theory are empirically predictive of multiple\nqualities of the generated dialogue, suggesting our theory is useful for\nmodel-selection when human evaluations are not available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sicilia_A/0/1/0/all/0/1\">Anthony Sicilia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt Conditioned VAE: Enhancing Generative Replay for Lifelong Learning in Task-Oriented Dialogue. (arXiv:2210.07783v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07783","description":"<p>Lifelong learning (LL) is vital for advanced task-oriented dialogue (ToD)\nsystems. To address the catastrophic forgetting issue of LL, generative replay\nmethods are widely employed to consolidate past knowledge with generated pseudo\nsamples. However, most existing generative replay methods use only a single\ntask-specific token to control their models. This scheme is usually not strong\nenough to constrain the generative model due to insufficient information\ninvolved. In this paper, we propose a novel method, prompt conditioned VAE for\nlifelong learning (PCLL), to enhance generative replay by incorporating tasks'\nstatistics. PCLL captures task-specific distributions with a conditional\nvariational autoencoder, conditioned on natural language prompts to guide the\npseudo-sample generation. Moreover, it leverages a distillation process to\nfurther consolidate past knowledge by alleviating the noise in pseudo samples.\nExperiments on natural language understanding tasks of ToD systems demonstrate\nthat PCLL significantly outperforms competitive baselines in building LL\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yingxiu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhiliang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bowen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Nevin L. Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Preference Learning for Storytelling via Contrastive Reinforcement Learning. (arXiv:2210.07792v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07792","description":"<p>Controlled automated story generation seeks to generate natural language\nstories satisfying constraints from natural language critiques or preferences.\nExisting methods to control for story preference utilize prompt engineering\nwhich is labor intensive and often inconsistent. They may also use\nlogit-manipulation methods which require annotated datasets to exist for the\ndesired attributes. To address these issues, we first train a contrastive\nbi-encoder model to align stories with corresponding human critiques, named\nCARP, building a general purpose preference model. This is subsequently used as\na reward function to fine-tune a generative language model via reinforcement\nlearning. However, simply fine-tuning a generative language model with a\ncontrastive reward model does not always reliably result in a story generation\nsystem capable of generating stories that meet user preferences. To increase\nstory generation robustness we further fine-tune the contrastive reward model\nusing a prompt-learning technique. A human participant study is then conducted\ncomparing generations from our full system, ablations, and two baselines. We\nshow that the full fine-tuning pipeline results in a story generator preferred\nover a LLM 20x as large as well as logit-based methods. This motivates the use\nof contrastive learning for general purpose human preference modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Castricato_L/0/1/0/all/0/1\">Louis Castricato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Havrilla_A/0/1/0/all/0/1\">Alexander Havrilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matiana_S/0/1/0/all/0/1\">Shahbuland Matiana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pieler_M/0/1/0/all/0/1\">Michael Pieler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_A/0/1/0/all/0/1\">Anbang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_I/0/1/0/all/0/1\">Ian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frazier_S/0/1/0/all/0/1\">Spencer Frazier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark Riedl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EfficientVLM: Fast and Accurate Vision-Language Models via Knowledge Distillation and Modal-adaptive Pruning. (arXiv:2210.07795v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07795","description":"<p>Pre-trained vision-language models (VLMs) have achieved impressive results in\na range of vision-language tasks. However, popular VLMs usually consist of\nhundreds of millions of parameters which brings challenges for fine-tuning and\ndeployment in real-world applications due to space, memory, and latency\nconstraints. In this work, we introduce a distilling then pruning framework to\ncompress large vision-language models into smaller, faster, and more accurate\nones. We first shrink the size of a pre-trained large VLM and apply knowledge\ndistillation in the vision-language pre-training stage to obtain a\ntask-agnostic compact VLM. Then we propose a modal-adaptive pruning algorithm\nto automatically infer the importance of vision and language modalities for\ndifferent downstream tasks and adaptively remove redundant structures and\nneurons in different encoders with controllable target sparsity. We apply our\nframework to train EfficientVLM, a fast and accurate vision-language model\nconsisting of 6 vision layers, 3 text layers, and 3 cross-modal fusion layers,\naccounting for only 93 million parameters in total, which is 44.3% of the\nteacher model. EfficientVLM retains 98.4% performance of the teacher model and\naccelerates its inference speed by 2.2x. EfficientVLM achieves a large absolute\nimprovement over previous SoTA efficient VLMs of similar sizes by a large\nmargin on various vision-language tasks, including VQAv2 (+4.9%), NLVR2\n(+5.6%), ITR (R@1 on TR +17.2%, on IR + 15.6% ) and COCO caption generation\n(CIDEr +6.5), demonstrating a large potential on training lightweight VLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tiannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinsong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bringing NURC/SP to Digital Life: the Role of Open-source Automatic Speech Recognition Models. (arXiv:2210.07852v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07852","description":"<p>The NURC Project that started in 1969 to study the cultured linguistic urban\nnorm spoken in five Brazilian capitals, was responsible for compiling a large\ncorpus for each capital. The digitized NURC/SP comprises 375 inquiries in 334\nhours of recordings taken in S\\~ao Paulo capital. Although 47 inquiries have\ntranscripts, there was no alignment between the audio-transcription, and 328\ninquiries were not transcribed. This article presents an evaluation and error\nanalysis of three automatic speech recognition models trained with spontaneous\nspeech in Portuguese and one model trained with prepared speech. The evaluation\nallowed us to choose the best model, using WER and CER metrics, in a manually\naligned sample of NURC/SP, to automatically transcribe 284 hours.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gris_L/0/1/0/all/0/1\">Lucas Rafael Stefanel Gris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_A/0/1/0/all/0/1\">Arnaldo Candido Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_V/0/1/0/all/0/1\">Vin&#xed;cius G. dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dias_B/0/1/0/all/0/1\">Bruno A. Papa Dias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leite_M/0/1/0/all/0/1\">Marli Quadros Leite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svartman_F/0/1/0/all/0/1\">Flaviane Romani Fernandes Svartman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aluisio_S/0/1/0/all/0/1\">Sandra Alu&#xed;sio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Graph to Rule them All: Using NLP and Graph Neural Networks to analyse Tolkien's Legendarium. (arXiv:2210.07871v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07871","description":"<p>Natural Language Processing and Machine Learning have considerably advanced\nComputational Literary Studies. Similarly, the construction of co-occurrence\nnetworks of literary characters, and their analysis using methods from social\nnetwork analysis and network science, have provided insights into the micro-\nand macro-level structure of literary texts. Combining these perspectives, in\nthis work we study character networks extracted from a text corpus of J.R.R.\nTolkien's Legendarium. We show that this perspective helps us to analyse and\nvisualise the narrative style that characterises Tolkien's works. Addressing\ncharacter classification, embedding and co-occurrence prediction, we further\ninvestigate the advantages of state-of-the-art Graph Neural Networks over a\npopular word embedding method. Our results highlight the large potential of\ngraph learning in Computational Literary Studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perri_V/0/1/0/all/0/1\">Vincenzo Perri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qarkaxhija_L/0/1/0/all/0/1\">Lisi Qarkaxhija</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zehe_A/0/1/0/all/0/1\">Albin Zehe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hotho_A/0/1/0/all/0/1\">Andreas Hotho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholtes_I/0/1/0/all/0/1\">Ingo Scholtes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Second Wave of UD Hebrew Treebanking and Cross-Domain Parsing. (arXiv:2210.07873v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07873","description":"<p>Foundational Hebrew NLP tasks such as segmentation, tagging and parsing, have\nrelied to date on various versions of the Hebrew Treebank (HTB, Sima'an et al.\n2001). However, the data in HTB, a single-source newswire corpus, is now over\n30 years old, and does not cover many aspects of contemporary Hebrew on the\nweb. This paper presents a new, freely available UD treebank of Hebrew\nstratified from a range of topics selected from Hebrew Wikipedia. In addition\nto introducing the corpus and evaluating the quality of its annotations, we\ndeploy automatic validation tools based on grew (Guillaume, 2021), and conduct\nthe first cross domain parsing experiments in Hebrew. We obtain new\nstate-of-the-art (SOTA) results on UD NLP tasks, using a combination of the\nlatest language modelling and some incremental improvements to existing\ntransformer based approaches. We also release a new version of the UD HTB\nmatching annotation scheme updates from our new corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeldes_A/0/1/0/all/0/1\">Amir Zeldes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howell_N/0/1/0/all/0/1\">Nick Howell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ordan_N/0/1/0/all/0/1\">Noam Ordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moshe_Y/0/1/0/all/0/1\">Yifat Ben Moshe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HashFormers: Towards Vocabulary-independent Pre-trained Transformers. (arXiv:2210.07904v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07904","description":"<p>Transformer-based pre-trained language models are vocabulary-dependent,\nmapping by default each token to its corresponding embedding. This one-to-one\nmapping results into embedding matrices that occupy a lot of memory (i.e.\nmillions of parameters) and grow linearly with the size of the vocabulary.\nPrevious work on on-device transformers dynamically generate token embeddings\non-the-fly without embedding matrices using locality-sensitive hashing over\nmorphological information. These embeddings are subsequently fed into\ntransformer layers for text classification. However, these methods are not\npre-trained. Inspired by this line of work, we propose HashFormers, a new\nfamily of vocabulary-independent pre-trained transformers that support an\nunlimited vocabulary (i.e. all possible tokens in a corpus) given a\nsubstantially smaller fixed-sized embedding matrix. We achieve this by first\nintroducing computationally cheap hashing functions that bucket together\nindividual tokens to embeddings. We also propose three variants that do not\nrequire an embedding matrix at all, further reducing the memory requirements.\nWe empirically demonstrate that HashFormers are more memory efficient compared\nto standard pre-trained transformers while achieving comparable predictive\nperformance when fine-tuned on multiple text classification tasks. For example,\nour most efficient HashFormer variant has a negligible performance degradation\n(0.4\\% on GLUE) using only 99.1K parameters for representing the embeddings\ncompared to 12.3-38M parameters of state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Huiyin Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expose Backdoors on the Way: A Feature-Based Efficient Defense against Textual Backdoor Attacks. (arXiv:2210.07907v1 [cs.CL])","link":"http://arxiv.org/abs/2210.07907","description":"<p>Natural language processing (NLP) models are known to be vulnerable to\nbackdoor attacks, which poses a newly arisen threat to NLP models. Prior online\nbackdoor defense methods for NLP models only focus on the anomalies at either\nthe input or output level, still suffering from fragility to adaptive attacks\nand high computational cost. In this work, we take the first step to\ninvestigate the unconcealment of textual poisoned samples at the\nintermediate-feature level and propose a feature-based efficient online defense\nmethod. Through extensive experiments on existing attacking methods, we find\nthat the poisoned samples are far away from clean samples in the intermediate\nfeature space of a poisoned NLP model. Motivated by this observation, we devise\na distance-based anomaly score (DAN) to distinguish poisoned samples from clean\nsamples at the feature level. Experiments on sentiment analysis and offense\ndetection tasks demonstrate the superiority of DAN, as it substantially\nsurpasses existing online defense methods in terms of defending performance and\nenjoys lower inference costs. Moreover, we show that DAN is also resistant to\nadaptive attacks based on feature-level regularization. Our code is available\nat https://github.com/lancopku/DAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sishuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenkai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_X/0/1/0/all/0/1\">Xiaohan Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Measures of Biases and Harms in NLP. (arXiv:2108.03362v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.03362","description":"<p>Recent studies show that Natural Language Processing (NLP) technologies\npropagate societal biases about demographic groups associated with attributes\nsuch as gender, race, and nationality. To create interventions and mitigate\nthese biases and associated harms, it is vital to be able to detect and measure\nsuch biases. While existing works propose bias evaluation and mitigation\nmethods for various tasks, there remains a need to cohesively understand the\nbiases and the specific harms they measure, and how different measures compare\nwith each other. To address this gap, this work presents a practical framework\nof harms and a series of questions that practitioners can answer to guide the\ndevelopment of bias measures. As a validation of our framework and\ndocumentation questions, we also present several case studies of how existing\nbias measures in NLP -- both intrinsic measures of bias in representations and\nextrinsic measures of bias of downstream applications -- can be aligned with\ndifferent harms and how our proposed documentation questions facilitates more\nholistic understanding of what bias measures are measuring.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_E/0/1/0/all/0/1\">Emily Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jieyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amstutz_A/0/1/0/all/0/1\">Aubrie Amstutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanseverino_M/0/1/0/all/0/1\">Mattie Sanseverino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishi_A/0/1/0/all/0/1\">Akihiro Nishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Do Things without Words: Modeling Semantic Drift of Emoji. (arXiv:2110.04093v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04093","description":"<p>Emoji have become a significant part of our informal textual communication.\nPrevious work addressing the societal and linguistic functions of emoji\noverlook the evolving meaning of the symbol. This evolution could be addressed\nthrough the framework of semantic drifts. In this paper we model and analyze\nthe semantic drift of emoji and discuss the features that may be contributing\nto the drift, some are unique to emoji and some are more general.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arviv_E/0/1/0/all/0/1\">Eyal Arviv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsur_O/0/1/0/all/0/1\">Oren Tsur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Template Filling for Controllable Commonsense Reasoning. (arXiv:2111.00539v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.00539","description":"<p>Large-scale sequence-to-sequence models have shown to be adept at both\nmultiple-choice and open-domain commonsense reasoning tasks. However, the\ncurrent systems do not provide the ability to control the various attributes of\nthe reasoning chain. To enable better controllability, we propose to study the\ncommonsense reasoning as a template filling task (TemplateCSR) -- where the\nlanguage models fills reasoning templates with the given constraints as control\nfactors. As an approach to TemplateCSR, we (i) propose a dataset of commonsense\nreasoning template-expansion pairs and (ii) introduce POTTER, a pretrained\nsequence-to-sequence model using prompts to perform commonsense reasoning\nacross concepts. Our experiments show that our approach outperforms baselines\nboth in generation metrics and factuality metrics. We also present a detailed\nerror analysis on our approach's ability to reliably perform commonsense\nreasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajagopal_D/0/1/0/all/0/1\">Dheeraj Rajagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khetan_V/0/1/0/all/0/1\">Vivek Khetan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sacaleanu_B/0/1/0/all/0/1\">Bogdan Sacaleanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gershman_A/0/1/0/all/0/1\">Anatole Gershman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fano_A/0/1/0/all/0/1\">Andrew Fano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P4E: Few-Shot Event Detection as Prompt-Guided Identification and Localization. (arXiv:2202.07615v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.07615","description":"<p>We propose P4E, an identify-and-localize event detection framework that\nintegrates the best of few-shot prompting and structured prediction. Our\nframework decomposes event detection into an unstructured identification task\nand a structured localization task. For the unstructured identification task,\nwe leverage prompting to elicit knowledge from pretrained language models,\nallowing our model to adapt to new event types quickly. We then employ a\ntype-agnostic sequence labeling model to localize the event trigger conditioned\non the identification output. This heterogeneous model design allows P4E to\nmake fast adaptation without sacrificing the ability to make structured\npredictions. Our experiments demonstrate the effectiveness of our proposed\ndesign, and P4E achieves the new state-of-the-art on few-shot entity detection\nacross multiple datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yiqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics. (arXiv:2202.11705v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.11705","description":"<p>Many applications of text generation require incorporating different\nconstraints to control the semantics or style of generated text. These\nconstraints can be hard (e.g., ensuring certain keywords are included in the\noutput) and soft (e.g., contextualizing the output with the left- or right-hand\ncontext). In this paper, we present Energy-based Constrained Decoding with\nLangevin Dynamics (COLD), a decoding framework which unifies constrained\ngeneration as specifying constraints through an energy function, then\nperforming efficient differentiable reasoning over the constraints through\ngradient-based sampling. COLD decoding is a flexible framework that can be\napplied directly to off-the-shelf left-to-right language models without the\nneed for any task-specific fine-tuning, as demonstrated through three\nchallenging text generation applications: lexically-constrained generation,\nabductive reasoning, and counterfactual reasoning. Our experiments on these\nconstrained generation tasks point to the effectiveness of our approach, both\nin terms of automatic and human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Lianhui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning of Sociopragmatic Meaning in Social Media. (arXiv:2203.07648v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07648","description":"<p>Recent progress in representation and contrastive learning in NLP has not\nwidely considered the class of \\textit{sociopragmatic meaning} (i.e., meaning\nin interaction within different language communities). To bridge this gap, we\npropose a novel framework for learning task-agnostic representations\ntransferable to a wide range of sociopragmatic tasks (e.g., emotion, hate\nspeech, humor, sarcasm). Our framework outperforms other contrastive learning\nframeworks for both in-domain and out-of-domain data, across both the general\nand few-shot settings. For example, compared to two popular pre-trained\nlanguage models, our method obtains an improvement of $11.66$ average $F_1$ on\n$16$ datasets when fine-tuned on only $20$ training samples per dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_G/0/1/0/all/0/1\">Ganesh Jawahar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastKASSIM: A Fast Tree Kernel-Based Syntactic Similarity Metric. (arXiv:2203.08299v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08299","description":"<p>Syntax is a fundamental component of language, yet few metrics have been\nemployed to capture syntactic similarity or coherence at the utterance- and\ndocument-level. The existing standard document-level syntactic similarity\nmetric is computationally expensive and performs inconsistently when faced with\nsyntactically dissimilar documents. To address these challenges, we present\nFastKASSIM, a metric for utterance- and document-level syntactic similarity\nwhich pairs and averages the most similar constituency parse trees between a\npair of documents based on tree kernels. FastKASSIM is more robust to syntactic\ndissimilarities and runs up to to 5.32 times faster than its predecessor over\ndocuments in the r/ChangeMyView corpus. FastKASSIM's improvements allow us to\nexamine hypotheses in two settings with large documents. We find that\nsyntactically similar arguments on r/ChangeMyView tend to be more persuasive,\nand that syntax is predictive of authorship attribution in the Australian High\nCourt Judgment corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Maximillian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Caitlyn Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CorrectSpeech: A Fully Automated System for Speech Correction and Accent Reduction. (arXiv:2204.05460v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2204.05460","description":"<p>This study propose a fully automated system for speech correction and accent\nreduction. Consider the application scenario that a recorded speech audio\ncontains certain errors, e.g., inappropriate words, mispronunciations, that\nneed to be corrected. The proposed system, named CorrectSpeech, performs the\ncorrection in three steps: recognizing the recorded speech and converting it\ninto time-stamped symbol sequence, aligning recognized symbol sequence with\ntarget text to determine locations and types of required edit operations, and\ngenerating the corrected speech. Experiments show that the quality and\nnaturalness of corrected speech depend on the performance of speech recognition\nand alignment modules, as well as the granularity level of editing operations.\nThe proposed system is evaluated on two corpora: a manually perturbed version\nof VCTK and L2-ARCTIC. The results demonstrate that our system is able to\ncorrect mispronunciation and reduce accent in speech recordings. Audio samples\nare available online for demonstration\nhttps://daxintan-cuhk.github.io/CorrectSpeech/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tan_D/0/1/0/all/0/1\">Daxin Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_L/0/1/0/all/0/1\">Liqun Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_N/0/1/0/all/0/1\">Nianzu Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yeung_Y/0/1/0/all/0/1\">Yu Ting Yeung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_T/0/1/0/all/0/1\">Tan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Markovian Generative Architectures over Pretrained LM Backbones for Efficient Task-Oriented Dialog Systems. (arXiv:2204.06452v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06452","description":"<p>Recently, Transformer based pretrained language models (PLMs), such as GPT2\nand T5, have been leveraged to build generative task-oriented dialog (TOD)\nsystems. A drawback of existing PLM-based models is their non-Markov\narchitectures across turns, i.e., the whole history is used as the conditioning\ninput at each turn. First, this brings inefficiencies in memory and\ncomputation. Furthermore, using the whole history increases model complexity\nand may hurt the training efficiency, especially when facing small amounts of\nlabeled training data (the low-resource setting). In this paper, motivated by\nthe observation that dialog states could be viewed as Markov states, we propose\nto build Markovian Generative Architectures (MGA) over PLM backbones for\nefficient TOD systems. Experiments on MultiWOZ2.1 show that in the\nrich-resource setting, the proposed Markov models reduce memory and time costs\nwithout performance degradation; in the low-resource setting, the training\nefficiency of the Markov models is more significant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yucheng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zhijian Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Junlan Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FETA: A Benchmark for Few-Sample Task Transfer in Open-Domain Dialogue. (arXiv:2205.06262v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.06262","description":"<p>Task transfer, transferring knowledge contained in related tasks, holds the\npromise of reducing the quantity of labeled data required to fine-tune language\nmodels. Dialogue understanding encompasses many diverse tasks, yet task\ntransfer has not been thoroughly studied in conversational AI. This work\nexplores conversational task transfer by introducing FETA: a benchmark for\nfew-sample task transfer in open-domain dialogue. FETA contains two underlying\nsets of conversations upon which there are 10 and 7 tasks annotated, enabling\nthe study of intra-dataset task transfer; task transfer without domain\nadaptation. We utilize three popular language models and three learning\nalgorithms to analyze the transferability between 132 source-target task pairs\nand create a baseline for future work. We run experiments in the single- and\nmulti-source settings and report valuable findings, e.g., most performance\ntrends are model-specific, and span extraction and multiple-choice tasks\nbenefit the most from task transfer. In addition to task transfer, FETA can be\na valuable resource for future research into the efficiency and\ngeneralizability of pre-training datasets and model architectures, as well as\nfor learning settings such as continual and multitask learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1\">Alon Albalak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuan_Y/0/1/0/all/0/1\">Yi-Lin Tuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jandaghi_P/0/1/0/all/0/1\">Pegah Jandaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pryor_C/0/1/0/all/0/1\">Connor Pryor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoffe_L/0/1/0/all/0/1\">Luke Yoffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_D/0/1/0/all/0/1\">Deepak Ramachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Getoor_L/0/1/0/all/0/1\">Lise Getoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Rule Induction for Interpretable Semi-Supervised Learning. (arXiv:2205.09067v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09067","description":"<p>Semi-supervised learning has shown promise in allowing NLP models to\ngeneralize from small amounts of labeled data. Meanwhile, pretrained\ntransformer models act as black-box correlation engines that are difficult to\nexplain and sometimes behave unreliably. In this paper, we propose tackling\nboth of these challenges via Automatic Rule Induction (ARI), a simple and\ngeneral-purpose framework for the automatic discovery and integration of\nsymbolic rules into pretrained transformer models. First, we extract weak\nsymbolic rules from low-capacity machine learning models trained on small\namounts of labeled data. Next, we use an attention mechanism to integrate these\nrules into high-capacity pretrained transformer models. Last, the\nrule-augmented system becomes part of a self-training framework to boost\nsupervision signal on unlabeled data. These steps can be layered beneath a\nvariety of existing weak supervision and semi-supervised NLP algorithms in\norder to improve performance and interpretability. Experiments across nine\nsequence classification and relation extraction tasks suggest that ARI can\nimprove state-of-the-art methods with no manual effort and minimal\ncomputational overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1\">Reid Pryzant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BBTv2: Towards a Gradient-Free Future with Large Language Models. (arXiv:2205.11200v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11200","description":"<p>Most downstream adaptation methods tune all or part of the parameters of\npre-trained models (PTMs) through gradient descent, where the tuning cost\nincreases linearly with the growth of the model size. By contrast,\ngradient-free methods only require the forward computation of the PTM to tune\nthe prompt, retaining the benefits of efficient tuning and deployment. Though,\npast work on gradient-free tuning often introduces gradient descent to seek a\ngood initialization of prompt and lacks versatility across tasks and PTMs. In\nthis paper, we present BBTv2, an improved version of Black-Box Tuning, to drive\nPTMs for few-shot learning. We prepend continuous prompts to every layer of the\nPTM and propose a divide-and-conquer gradient-free algorithm to optimize the\nprompts at different layers alternately. Extensive experiments across various\ntasks and PTMs show that BBTv2 can achieve comparable performance to full model\ntuning and state-of-the-art parameter-efficient methods (e.g., Adapter, LoRA,\nBitFit, etc.) under few-shot settings while maintaining much fewer tunable\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhengfu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Hong Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yunhua Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Tokenization Learning. (arXiv:2205.11443v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11443","description":"<p>In the presented study, we discover that the so-called \"transition freedom\"\nmetric appears superior for unsupervised tokenization purposes in comparison to\nstatistical metrics such as mutual information and conditional probability,\nproviding F-measure scores in range from $0.71$ to $1.0$ across explored\nmultilingual corpora. We find that different languages require different\noffshoots of that metric (such as derivative, variance, and \"peak values\") for\nsuccessful tokenization. Larger training corpora do not necessarily result in\nbetter tokenization quality, while compressing the models by eliminating\nstatistically weak evidence tends to improve performance. The proposed\nunsupervised tokenization technique provides quality better than or comparable\nto lexicon-based ones, depending on the language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolonin_A/0/1/0/all/0/1\">Anton Kolonin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_V/0/1/0/all/0/1\">Vignav Ramesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LingMess: Linguistically Informed Multi Expert Scorers for Coreference Resolution. (arXiv:2205.12644v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12644","description":"<p>While coreference resolution typically involves various linguistic\nchallenges, recent models are based on a single pairwise scorer for all types\nof pairs. We present LingMess, a new coreference model that defines different\ncategories of coreference cases and optimize multiple pairwise scorers, where\neach scorer learns a specific set of linguistic challenges. Our model\nsubstantially improves pairwise scores for most categories and outperforms\ncluster-level performance on Ontonotes and 5 additional datasets. Our model is\navailable in https://github.com/shon-otmazgin/lingmess-coref\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Otmazgin_S/0/1/0/all/0/1\">Shon Otmazgin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cattan_A/0/1/0/all/0/1\">Arie Cattan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoNT: Contrastive Neural Text Generation. (arXiv:2205.14690v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.14690","description":"<p>Recently, contrastive learning attracts increasing interests in neural text\ngeneration as a new solution to alleviate the exposure bias problem. It\nintroduces a sequence-level training signal which is crucial to generation\ntasks that always rely on auto-regressive decoding. However, previous methods\nusing contrastive learning in neural text generation usually lead to inferior\nperformance. In this paper, we analyse the underlying reasons and propose a new\nContrastive Neural Text generation framework, CoNT. CoNT addresses bottlenecks\nthat prevent contrastive learning from being widely adopted in generation tasks\nfrom three aspects -- the construction of contrastive examples, the choice of\nthe contrastive loss, and the strategy in decoding. We validate CoNT on five\ngeneration tasks with ten benchmarks, including machine translation,\nsummarization, code comment generation, data-to-text generation and commonsense\ngeneration. Experimental results show that CoNT clearly outperforms the\nconventional training framework on all the ten benchmarks with a convincing\nmargin. Especially, CoNT surpasses previous the most competitive contrastive\nlearning method for text generation, by 1.50 BLEU on machine translation and\n1.77 ROUGE-1 on summarization, respectively. It achieves new state-of-the-art\non summarization, code comment generation (without external data) and\ndata-to-text generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_C/0/1/0/all/0/1\">Chenxin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiangtao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_K/0/1/0/all/0/1\">Kai Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EcoFormer: Energy-Saving Attention with Linear Complexity. (arXiv:2209.09004v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2209.09004","description":"<p>Transformer is a transformative framework that models sequential data and has\nachieved remarkable performance on a wide range of tasks, but with high\ncomputational and energy cost. To improve its efficiency, a popular choice is\nto compress the models via binarization which constrains the floating-point\nvalues into binary ones to save resource consumption owing to cheap bitwise\noperations significantly. However, existing binarization methods only aim at\nminimizing the information loss for the input distribution statistically, while\nignoring the pairwise similarity modeling at the core of the attention. To this\nend, we propose a new binarization paradigm customized to high-dimensional\nsoftmax attention via kernelized hashing, called EcoFormer, to map the original\nqueries and keys into low-dimensional binary codes in Hamming space. The\nkernelized hash functions are learned to match the ground-truth similarity\nrelations extracted from the attention map in a self-supervised way. Based on\nthe equivalence between the inner product of binary codes and the Hamming\ndistance as well as the associative property of matrix multiplication, we can\napproximate the attention in linear complexity by expressing it as a\ndot-product of binary codes. Moreover, the compact binary representations of\nqueries and keys enable us to replace most of the expensive multiply-accumulate\noperations in attention with simple accumulations to save considerable on-chip\nenergy footprint on edge devices. Extensive experiments on both vision and\nlanguage tasks show that EcoFormer consistently achieves comparable performance\nwith standard attentions while consuming much fewer resources. For example,\nbased on PVTv2-B0 and ImageNet-1K, Ecoformer achieves a 73% on-chip energy\nfootprint reduction with only a 0.33% performance drop compared to the standard\nattention. Code is available at https://github.com/ziplab/EcoFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zizheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Haoyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physical computation and compositionality. (arXiv:2210.00392v2 [quant-ph] UPDATED)","link":"http://arxiv.org/abs/2210.00392","description":"<p>Developments in quantum computing and, more in general, non-standard\ncomputing systems, represent a clear indication that the very notion of what a\nphysical computing device is and does should be recast in a rigorous and sound\nframework. Physical computing has opened a whole stream of new research aimed\nto understand and control how information is processed by several types of\nphysical devices. Therefore, classical definitions and entire frameworks need\nto be adapted in order to fit a broader notion of what physical computing\nsystems really are. Recent studies have proposed a formalism that can be used\nto carve out a more proper notion of physical computing. In this paper we\npresent a framework which capture such results in a very natural way via some\nbasic constructions in Category Theory. Furthermore, we show that, within our\nframework, the compositional nature of physical computing systems is naturally\nformalized, and that it can be organized in coherent structures by the means of\ntheir relational nature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Dehghani_N/0/1/0/all/0/1\">Nima Dehghani</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Caterina_G/0/1/0/all/0/1\">Gianluca Caterina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COMPS: Conceptual Minimal Pair Sentences for testing Property Knowledge and Inheritance in Pre-trained Language Models. (arXiv:2210.01963v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.01963","description":"<p>A characteristic feature of human semantic memory is its ability to not only\nstore and retrieve the properties of concepts observed through experience, but\nto also facilitate the inheritance of properties (can breathe) from\nsuperordinate concepts (animal) to their subordinates (dog) -- i.e. demonstrate\nproperty inheritance. In this paper, we present COMPS, a collection of minimal\npair sentences that jointly tests pre-trained language models (PLMs) on their\nability to attribute properties to concepts and their ability to demonstrate\nproperty inheritance behavior. Analyses of 22 different PLMs on COMPS reveal\nthat they can easily distinguish between concepts on the basis of a property\nwhen they are trivially different, but find it relatively difficult when\nconcepts are related on the basis of nuanced knowledge representations.\nFurthermore, we find that PLMs can demonstrate behavior consistent with\nproperty inheritance to a great extent, but fail in the presence of distracting\ninformation, which decreases the performance of many models, sometimes even\nbelow chance. This lack of robustness in demonstrating simple reasoning raises\nimportant questions about PLMs' capacity to make correct inferences even when\nthey appear to possess the prerequisite knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Misra_K/0/1/0/all/0/1\">Kanishka Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayz_J/0/1/0/all/0/1\">Julia Taylor Rayz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ettinger_A/0/1/0/all/0/1\">Allyson Ettinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not another Negation Benchmark: The NaN-NLI Test Suite for Sub-clausal Negation. (arXiv:2210.03256v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03256","description":"<p>Negation is poorly captured by current language models, although the extent\nof this problem is not widely understood. We introduce a natural language\ninference (NLI) test suite to enable probing the capabilities of NLP methods,\nwith the aim of understanding sub-clausal negation. The test suite contains\npremise--hypothesis pairs where the premise contains sub-clausal negation and\nthe hypothesis is constructed by making minimal modifications to the premise in\norder to reflect different possible interpretations. Aside from adopting\nstandard NLI labels, our test suite is systematically constructed under a\nrigorous linguistic framework. It includes annotation of negation types and\nconstructions grounded in linguistic theory, as well as the operations used to\nconstruct hypotheses. This facilitates fine-grained analysis of model\nperformance. We conduct experiments using pre-trained language models to\ndemonstrate that our test suite is more challenging than existing benchmarks\nfocused on negation, and show how our annotation supports a deeper\nunderstanding of the current NLI capabilities in terms of negation and\nquantification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Thinh Hung Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otmakhova_Y/0/1/0/all/0/1\">Yulia Otmakhova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verspoor_K/0/1/0/all/0/1\">Karin Verspoor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models Are Poor Learners of Directional Inference. (arXiv:2210.04695v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04695","description":"<p>We examine LMs' competence of directional predicate entailments by supervised\nfine-tuning with prompts. Our analysis shows that contrary to their apparent\nsuccess on standard NLI, LMs show limited ability to learn such directional\ninference; moreover, existing datasets fail to test directionality, and/or are\ninfested by artefacts that can be learnt as proxy for entailments, yielding\nover-optimistic results. In response, we present BoOQA (Boolean Open QA), a\nrobust multi-lingual evaluation benchmark for directional predicate\nentailments, extrinsic to existing training sets. On BoOQA, we establish\nbaselines and show evidence of existing LM-prompting models being incompetent\ndirectional entailment learners, in contrast to entailment graphs, however\nlimited by sparsity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1\">Mohammad Javad Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_S/0/1/0/all/0/1\">Sabine Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1\">Mark Steedman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis. (arXiv:2210.04714v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04714","description":"<p>Pre-trained language models (PLMs) have gained increasing popularity due to\ntheir compelling prediction performance in diverse natural language processing\n(NLP) tasks. When formulating a PLM-based prediction pipeline for NLP tasks, it\nis also crucial for the pipeline to minimize the calibration error, especially\nin safety-critical applications. That is, the pipeline should reliably indicate\nwhen we can trust its predictions. In particular, there are various\nconsiderations behind the pipeline: (1) the choice and (2) the size of PLM, (3)\nthe choice of uncertainty quantifier, (4) the choice of fine-tuning loss, and\nmany more. Although prior work has looked into some of these considerations,\nthey usually draw conclusions based on a limited scope of empirical studies.\nThere still lacks a holistic analysis on how to compose a well-calibrated\nPLM-based prediction pipeline. To fill this void, we compare a wide range of\npopular options for each consideration based on three prevalent NLP\nclassification tasks and the setting of domain shift. In response, we recommend\nthe following: (1) use ELECTRA for PLM encoding, (2) use larger PLMs if\npossible, (3) use Temp Scaling as the uncertainty quantifier, and (4) use Focal\nLoss for fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yuxin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_U/0/1/0/all/0/1\">Umang Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1\">Willie Neiswanger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIGAT: Modeling News Recommendation with Dual-Graph Interaction. (arXiv:2210.05196v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05196","description":"<p>News recommendation (NR) is essential for online news services. Existing NR\nmethods typically adopt a news-user representation learning framework, facing\ntwo potential limitations. First, in news encoder, single candidate news\nencoding suffers from an insufficient semantic information problem. Second,\nexisting graph-based NR methods are promising but lack effective news-user\nfeature interaction, rendering the graph-based recommendation suboptimal. To\novercome these limitations, we propose dual-interactive graph attention\nnetworks (DIGAT) consisting of news- and user-graph channels. In the news-graph\nchannel, we enrich the semantics of single candidate news by incorporating the\nsemantically relevant news information with a semantic-augmented graph (SAG).\nIn the user-graph channel, multi-level user interests are represented with a\nnews-topic graph. Most notably, we design a dual-graph interaction process to\nperform effective feature interaction between the news and user graphs, which\nfacilitates accurate news-user representation matching. Experiment results on\nthe benchmark dataset MIND show that DIGAT outperforms existing news\nrecommendation methods. Further ablation studies and analyses validate the\neffectiveness of (1) semantic-augmented news graph modeling and (2) dual-graph\ninteraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhiming Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xingshan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding. (arXiv:2210.06155v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06155","description":"<p>Recent years have witnessed the rise and success of pre-training techniques\nin visually-rich document understanding. However, most existing methods lack\nthe systematic mining and utilization of layout-centered knowledge, leading to\nsub-optimal performances. In this paper, we propose ERNIE-Layout, a novel\ndocument pre-training solution with layout knowledge enhancement in the whole\nworkflow, to learn better representations that combine the features from text,\nlayout, and image. Specifically, we first rearrange input sequences in the\nserialization stage, and then present a correlative pre-training task, reading\norder prediction, to learn the proper reading order of documents. To improve\nthe layout awareness of the model, we integrate a spatial-aware disentangled\nattention into the multi-modal transformer and a replaced regions prediction\ntask into the pre-training phase. Experimental results show that ERNIE-Layout\nachieves superior performance on various downstream tasks, setting new\nstate-of-the-art on key information extraction, document image classification,\nand document question answering datasets. The code and models are publicly\navailable at\n<a href=\"http://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/ernie-layout.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qiming Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yinxu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenjin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1\">Bin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhengjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Teng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Weichong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yongfeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shikun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1\">Hao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InfoCSE: Information-aggregated Contrastive Learning of Sentence Embeddings. (arXiv:2210.06432v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06432","description":"<p>Contrastive learning has been extensively studied in sentence embedding\nlearning, which assumes that the embeddings of different views of the same\nsentence are closer. The constraint brought by this assumption is weak, and a\ngood sentence representation should also be able to reconstruct the original\nsentence fragments. Therefore, this paper proposes an information-aggregated\ncontrastive learning framework for learning unsupervised sentence embeddings,\ntermed InfoCSE. InfoCSE forces the representation of [CLS] positions to\naggregate denser sentence information by introducing an additional Masked\nlanguage model task and a well-designed network. We evaluate the proposed\nInfoCSE on several benchmark datasets w.r.t the semantic text similarity (STS)\ntask. Experimental results show that InfoCSE outperforms SimCSE by an average\nSpearman correlation of 2.60% on BERT-base, and 1.77% on BERT-large, achieving\nstate-of-the-art results among unsupervised sentence representation learning\nmethods. Our code are available at\nhttps://github.com/caskcsg/sentemb/tree/main/InfoCSE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chaochen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zijia Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jizhong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songlin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study on Finding Spans. (arXiv:2210.06824v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06824","description":"<p>We present an empirical study on methods for span finding, the selection of\nconsecutive tokens in text for some downstream tasks. We focus on approaches\nthat can be employed in training end-to-end information extraction systems, and\nfind there is no definitive solution without considering task properties, and\nprovide our observations to help with future design choices: 1) a tagging\napproach often yields higher precision while span enumeration and boundary\nprediction provide higher recall; 2) span type information can benefit a\nboundary prediction approach; 3) additional contextualization does not help\nspan finding in most cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1\">Weiwei Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Boyuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunmo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tongfei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anonymizing Speech with Generative Adversarial Networks to Preserve Speaker Privacy. (arXiv:2210.07002v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2210.07002","description":"<p>In order to protect the privacy of speech data, speaker anonymization aims\nfor hiding the identity of a speaker by changing the voice in speech\nrecordings. This typically comes with a privacy-utility trade-off between\nprotection of individuals and usability of the data for downstream\napplications. One of the challenges in this context is to create non-existent\nvoices that sound as natural as possible.\n</p>\n<p>In this work, we propose to tackle this issue by generating speaker\nembeddings using a generative adversarial network with Wasserstein distance as\ncost function. By incorporating these artificial embeddings into a\nspeech-to-text-to-speech pipeline, we outperform previous approaches in terms\nof privacy and utility. According to standard objective metrics and human\nevaluation, our approach generates intelligible and content-preserving yet\nprivacy-protecting versions of the original recordings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meyer_S/0/1/0/all/0/1\">Sarina Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tilli_P/0/1/0/all/0/1\">Pascal Tilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denisov_P/0/1/0/all/0/1\">Pavel Denisov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lux_F/0/1/0/all/0/1\">Florian Lux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_J/0/1/0/all/0/1\">Julia Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing. (arXiv:2210.07074v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07074","description":"<p>A bottleneck to developing Semantic Parsing (SP) models is the need for a\nlarge volume of human-labeled training data. Given the complexity and cost of\nhuman annotation for SP, labeled data is often scarce, particularly in\nmultilingual settings. Large Language Models (LLMs) excel at SP given only a\nfew examples, however LLMs are unsuitable for runtime systems which require low\nlatency. In this work, we propose CLASP, a simple method to improve\nlow-resource SP for moderate-sized models: we generate synthetic data from\nAlexaTM 20B to augment the training set for a model 40x smaller (500M\nparameters). We evaluate on two datasets in low-resource settings: English\nPIZZA, containing either 348 or 16 real examples, and mTOP cross-lingual\nzero-shot, where training data is available only in English, and the model must\ngeneralize to four new languages. On both datasets, we show significant\nimprovements over strong baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosenbaum_A/0/1/0/all/0/1\">Andy Rosenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltan_S/0/1/0/all/0/1\">Saleh Soltan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamza_W/0/1/0/all/0/1\">Wael Hamza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffari_A/0/1/0/all/0/1\">Amir Saffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damonte_M/0/1/0/all/0/1\">Marco Damonte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groves_I/0/1/0/all/0/1\">Isabel Groves</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Compressing Sequences for Self-Supervised Speech Models. (arXiv:2210.07189v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07189","description":"<p>Compressing self-supervised models has become increasingly necessary, as\nself-supervised models become larger. While previous approaches have primarily\nfocused on compressing the model size, shortening sequences is also effective\nin reducing the computational cost. In this work, we study fixed-length and\nvariable-length subsampling along the time axis in self-supervised learning. We\nexplore how individual downstream tasks are sensitive to input frame rates.\nSubsampling while training self-supervised models not only improves the overall\nperformance on downstream tasks under certain frame rates, but also brings\nsignificant speed-up in inference. Variable-length subsampling performs\nparticularly well under low frame rates. In addition, if we have access to\nphonetic boundaries, we find no degradation in performance for an average frame\nrate as low as 10 Hz.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yen Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hsuan-Jui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_P/0/1/0/all/0/1\">Paola Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-10-16T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}