{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-10-10T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A Brief History of Prompt: Leveraging Language Models. (arXiv:2310.04438v1 [cs.CL])","link":"http://arxiv.org/abs/2310.04438","description":"<p>This paper presents a comprehensive exploration of the evolution of prompt\nengineering and generation in the field of natural language processing (NLP).\nStarting from the early language models and information retrieval systems, we\ntrace the key developments that have shaped prompt engineering over the years.\nThe introduction of attention mechanisms in 2015 revolutionized language\nunderstanding, leading to advancements in controllability and\ncontext-awareness. Subsequent breakthroughs in reinforcement learning\ntechniques further enhanced prompt engineering, addressing issues like exposure\nbias and biases in generated text. We examine the significant contributions in\n2018 and 2019, focusing on fine-tuning strategies, control codes, and\ntemplate-based generation. The paper also discusses the growing importance of\nfairness, human-AI collaboration, and low-resource adaptation. In 2020 and\n2021, contextual prompting and transfer learning gained prominence, while 2022\nand 2023 witnessed the emergence of advanced techniques like unsupervised\npre-training and novel reward shaping. Throughout the paper, we reference\nspecific research studies that exemplify the impact of various developments on\nprompt engineering. The journey of prompt engineering continues, with ethical\nconsiderations being paramount for the responsible and inclusive future of AI\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muktadir_G/0/1/0/all/0/1\">Golam Md Muktadir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Mobility Question Answering (Vision Paper). (arXiv:2310.04443v1 [cs.CL])","link":"http://arxiv.org/abs/2310.04443","description":"<p>Question answering (QA) systems have attracted much attention from the\nartificial intelligence community as they can learn to answer questions based\non the given knowledge source (e.g., images in visual question answering).\nHowever, the research into question answering systems with human mobility data\nremains unexplored. Mining human mobility data is crucial for various\napplications such as smart city planning, pandemic management, and personalised\nrecommendation system. In this paper, we aim to tackle this gap and introduce a\nnovel task, that is, human mobility question answering (MobQA). The aim of the\ntask is to let the intelligent system learn from mobility data and answer\nrelated questions. This task presents a new paradigm change in mobility\nprediction research and further facilitates the research of human mobility\nrecommendation systems. To better support this novel research topic, this\nvision paper also proposes an initial design of the dataset and a potential\ndeep learning model framework for the introduced MobQA task. We hope that this\npaper will provide novel insights and open new directions in human mobility\nresearch and question answering research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1\">Flora D. Salim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What's the Magic Word? A Control Theory of LLM Prompting. (arXiv:2310.04444v1 [cs.CL])","link":"http://arxiv.org/abs/2310.04444","description":"<p>Prompt engineering is effective and important in the deployment of LLMs but\nis poorly understood mathematically. Here, we formalize prompt engineering as\nan optimal control problem on LLMs -- where the prompt is considered a control\nvariable for modulating the output distribution of the LLM. Within this\nframework, we ask a simple question: given a sequence of tokens, does there\nalways exist a prompt we can prepend that will steer the LLM toward accurately\npredicting the final token? We call such an optimal prompt the magic word since\nprepending the prompt causes the LLM to output the correct answer. If magic\nwords exist, can we find them? If so, what are their properties? We offer\nanalytic analysis on the controllability of the self-attention head where we\nprove a bound on controllability as a function of the singular values of its\nweight matrices. We take inspiration from control theory to propose a metric\ncalled $k-\\epsilon$ controllability to characterize LLM steerability. We\ncompute the $k-\\epsilon$ controllability of a panel of large language models,\nincluding Falcon-7b, Llama-7b, and Falcon-40b on 5000 WikiText causal language\nmodeling tasks. Remarkably, we find that magic words of 10 tokens or less exist\nfor over 97% of WikiText instances surveyed for each model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhargava_A/0/1/0/all/0/1\">Aman Bhargava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witkowski_C/0/1/0/all/0/1\">Cameron Witkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Manav Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomson_M/0/1/0/all/0/1\">Matt Thomson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LoFT: Local Proxy Fine-tuning For Improving Transferability Of Adversarial Attacks Against Large Language Model. (arXiv:2310.04445v1 [cs.CL])","link":"http://arxiv.org/abs/2310.04445","description":"<p>It has been shown that Large Language Model (LLM) alignments can be\ncircumvented by appending specially crafted attack suffixes with harmful\nqueries to elicit harmful responses. To conduct attacks against private target\nmodels whose characterization is unknown, public models can be used as proxies\nto fashion the attack, with successful attacks being transferred from public\nproxies to private target models. The success rate of attack depends on how\nclosely the proxy model approximates the private model. We hypothesize that for\nattacks to be transferrable, it is sufficient if the proxy can approximate the\ntarget model in the neighborhood of the harmful query. Therefore, in this\npaper, we propose \\emph{Local Fine-Tuning (LoFT)}, \\textit{i.e.}, fine-tuning\nproxy models on similar queries that lie in the lexico-semantic neighborhood of\nharmful queries to decrease the divergence between the proxy and target models.\nFirst, we demonstrate three approaches to prompt private target models to\nobtain similar queries given harmful queries. Next, we obtain data for local\nfine-tuning by eliciting responses from target models for the generated similar\nqueries. Then, we optimize attack suffixes to generate attack prompts and\nevaluate the impact of our local fine-tuning on the attack's success rate.\nExperiments show that local fine-tuning of proxy models improves attack\ntransferability and increases attack success rate by $39\\%$, $7\\%$, and $0.5\\%$\n(absolute) on target models ChatGPT, GPT-4, and Claude respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Muhammad Ahmed Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Roshan Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamyal_H/0/1/0/all/0/1\">Hira Dhamyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olivier_R/0/1/0/all/0/1\">Raphael Olivier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Ankit Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alharthi_D/0/1/0/all/0/1\">Dareen Alharthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bukhari_H/0/1/0/all/0/1\">Hazim T Bukhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baali_M/0/1/0/all/0/1\">Massa Baali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshmukh_S/0/1/0/all/0/1\">Soham Deshmukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhlmann_M/0/1/0/all/0/1\">Michael Kuhlmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rita Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Large Language Models' Perception of Emotion Using Appraisal Theory. (arXiv:2310.04450v1 [cs.CL])","link":"http://arxiv.org/abs/2310.04450","description":"<p>Large Language Models (LLM) like ChatGPT have significantly advanced in\nrecent years and are now being used by the general public. As more people\ninteract with these systems, improving our understanding of these black box\nmodels is crucial, especially regarding their understanding of human\npsychological aspects. In this work, we investigate their emotion perception\nthrough the lens of appraisal and coping theory using the Stress and Coping\nProcess Questionaire (SCPQ). SCPQ is a validated clinical instrument consisting\nof multiple stories that evolve over time and differ in key appraisal variables\nsuch as controllability and changeability. We applied SCPQ to three recent LLMs\nfrom OpenAI, davinci-003, ChatGPT, and GPT-4 and compared the results with\npredictions from the appraisal theory and human data. The results show that\nLLMs' responses are similar to humans in terms of dynamics of appraisal and\ncoping, but their responses did not differ along key appraisal dimensions as\npredicted by the theory and data. The magnitude of their responses is also\nquite different from humans in several variables. We also found that GPTs can\nbe quite sensitive to instruction and how questions are asked. This work adds\nto the growing literature evaluating the psychological aspects of LLMs and\nhelps enrich our understanding of the current models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yongsatianchot_N/0/1/0/all/0/1\">Nutchanon Yongsatianchot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torshizi_P/0/1/0/all/0/1\">Parisa Ghanad Torshizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marsella_S/0/1/0/all/0/1\">Stacy Marsella</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models. (arXiv:2310.04451v1 [cs.CL])","link":"http://arxiv.org/abs/2310.04451","description":"<p>The aligned Large Language Models (LLMs) are powerful language understanding\nand decision-making tools that are created through extensive alignment with\nhuman feedback. However, these large models remain susceptible to jailbreak\nattacks, where adversaries manipulate prompts to elicit malicious outputs that\nshould not be given by aligned LLMs. Investigating jailbreak prompts can lead\nus to delve into the limitations of LLMs and further guide us to secure them.\nUnfortunately, existing jailbreak techniques suffer from either (1) scalability\nissues, where attacks heavily rely on manual crafting of prompts, or (2)\nstealthiness problems, as attacks depend on token-based algorithms to generate\nprompts that are often semantically meaningless, making them susceptible to\ndetection through basic perplexity testing. In light of these challenges, we\nintend to answer this question: Can we develop an approach that can\nautomatically generate stealthy jailbreak prompts? In this paper, we introduce\nAutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can\nautomatically generate stealthy jailbreak prompts by the carefully designed\nhierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN\nnot only automates the process while preserving semantic meaningfulness, but\nalso demonstrates superior attack strength in cross-model transferability, and\ncross-sample universality compared with the baseline. Moreover, we also compare\nAutoDAN with perplexity-based defense methods and show that AutoDAN can bypass\nthem effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaogeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Nan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Short text classification with machine learning in the social sciences: The case of climate change on Twitter. (arXiv:2310.04452v1 [cs.CL])","link":"http://arxiv.org/abs/2310.04452","description":"<p>To analyse large numbers of texts, social science researchers are\nincreasingly confronting the challenge of text classification. When manual\nlabeling is not possible and researchers have to find automatized ways to\nclassify texts, computer science provides a useful toolbox of machine-learning\nmethods whose performance remains understudied in the social sciences. In this\narticle, we compare the performance of the most widely used text classifiers by\napplying them to a typical research scenario in social science research: a\nrelatively small labeled dataset with infrequent occurrence of categories of\ninterest, which is a part of a large unlabeled dataset. As an example case, we\nlook at Twitter communication regarding climate change, a topic of increasing\nscholarly interest in interdisciplinary social science research. Using a novel\ndataset including 5,750 tweets from various international organizations\nregarding the highly ambiguous concept of climate change, we evaluate the\nperformance of methods in automatically classifying tweets based on whether\nthey are about climate change or not. In this context, we highlight two main\nfindings. First, supervised machine-learning methods perform better than\nstate-of-the-art lexicons, in particular as class balance increases. Second,\ntraditional machine-learning methods, such as logistic regression and random\nforest, perform similarly to sophisticated deep-learning methods, whilst\nrequiring much less training time and computational resources. The results have\nimportant implications for the analysis of short texts in social science\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shyrokykh_K/0/1/0/all/0/1\">Karina Shyrokykh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girnyk_M/0/1/0/all/0/1\">Maksym Girnyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dellmuth_L/0/1/0/all/0/1\">Lisa Dellmuth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 South African Vaccine Hesitancy Models Show Boost in Performance Upon Fine-Tuning on M-pox Tweets. (arXiv:2310.04453v1 [cs.CL])","link":"http://arxiv.org/abs/2310.04453","description":"<p>Very large numbers of M-pox cases have, since the start of May 2022, been\nreported in non-endemic countries leading many to fear that the M-pox Outbreak\nwould rapidly transition into another pandemic, while the COVID-19 pandemic\nravages on. Given the similarities of M-pox with COVID-19, we chose to test the\nperformance of COVID-19 models trained on South African twitter data on a\nhand-labelled M-pox dataset before and after fine-tuning. More than 20k\nM-pox-related tweets from South Africa were hand-labelled as being either\npositive, negative or neutral. After fine-tuning these COVID-19 models on the\nM-pox dataset, the F1-scores increased by more than 8% falling just short of\n70%, but still outperforming state-of-the-art models and well-known\nclassification algorithms. An LDA-based topic modelling procedure was used to\ncompare the miss-classified M-pox tweets of the original COVID-19 RoBERTa model\nwith its fine-tuned version, and from this analysis, we were able to draw\nconclusions on how to build more sophisticated models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perikli_N/0/1/0/all/0/1\">Nicholas Perikli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Srimoy Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogbuokiri_B/0/1/0/all/0/1\">Blessing Ogbuokiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nia_Z/0/1/0/all/0/1\">Zahra Movahedi Nia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lieberman_B/0/1/0/all/0/1\">Benjamin Lieberman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_N/0/1/0/all/0/1\">Nidhi Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahbi_S/0/1/0/all/0/1\">Salah-Eddine Dahbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevenson_F/0/1/0/all/0/1\">Finn Stevenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bragazzi_N/0/1/0/all/0/1\">Nicola Bragazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_J/0/1/0/all/0/1\">Jude Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mellado_B/0/1/0/all/0/1\">Bruce Mellado</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On migration to Perpetual Enterprise System. (arXiv:2104.04844v4 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2104.04844","description":"<p>This document describes a pragmatic approach on how to migrate an\norganisation computer system towards a new system that could evolve forever,\naddresses the whole organisation and it is integrated.\n</p>\n<p>Governance aspects are as important, if not more, than purely technical IT\naspects: human resources, call for tenders, and similar. Migration implies that\none is not starting from a green field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benitez_M/0/1/0/all/0/1\">Manuel Tomas Carrasco Benitez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph. (arXiv:2105.02605v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.02605","description":"<p>The representation learning on textual graph is to generate low-dimensional\nembeddings for the nodes based on the individual textual features and the\nneighbourhood information. Recent breakthroughs on pretrained language models\nand graph neural networks push forward the development of corresponding\ntechniques. The existing works mainly rely on the cascaded model architecture:\nthe textual features of nodes are independently encoded by language models at\nfirst; the textual embeddings are aggregated by graph neural networks\nafterwards. However, the above architecture is limited due to the independent\nmodeling of textual features. In this work, we propose GraphFormers, where\nlayerwise GNN components are nested alongside the transformer blocks of\nlanguage models. With the proposed architecture, the text encoding and the\ngraph aggregation are fused into an iterative workflow, {making} each node's\nsemantic accurately comprehended from the global perspective. In addition, a\n{progressive} learning strategy is introduced, where the model is successively\ntrained on manipulated data and original data to reinforce its capability of\nintegrating information on graph. Extensive evaluations are conducted on three\nlarge-scale benchmark datasets, where GraphFormers outperform the SOTA\nbaselines with comparable running efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Junhan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shitao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chaozhuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Defu Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sanjay Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amit Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangzhong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Defending Against Backdoor Attacks in Natural Language Generation. (arXiv:2106.01810v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.01810","description":"<p>The frustratingly fragile nature of neural network models make current\nnatural language generation (NLG) systems prone to backdoor attacks and\ngenerate malicious sequences that could be sexist or offensive. Unfortunately,\nlittle effort has been invested to how backdoor attacks can affect current NLG\nmodels and how to defend against these attacks. In this work, by giving a\nformal definition of backdoor attack and defense, we investigate this problem\non two important NLG tasks, machine translation and dialog generation. Tailored\nto the inherent nature of NLG models (e.g., producing a sequence of coherent\nwords given contexts), we design defending strategies against attacks. We find\nthat testing the backward probability of generating sources given targets\nyields effective defense performance against all different types of attacks,\nand is able to handle the {\\it one-to-many} issue in many NLG tasks such as\ndialog generation. We hope that this work can raise the awareness of backdoor\nrisks concealed in deep NLG systems and inspire more future work (both attack\nand defense) towards this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1\">Xiang Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Small-Text: Active Learning for Text Classification in Python. (arXiv:2107.10314v7 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.10314","description":"<p>We introduce small-text, an easy-to-use active learning library, which offers\npool-based active learning for single- and multi-label text classification in\nPython. It features numerous pre-implemented state-of-the-art query strategies,\nincluding some that leverage the GPU. Standardized interfaces allow the\ncombination of a variety of classifiers, query strategies, and stopping\ncriteria, facilitating a quick mix and match, and enabling a rapid and\nconvenient development of both active learning experiments and applications.\nWith the objective of making various classifiers and query strategies\naccessible for active learning, small-text integrates several well-known\nmachine learning libraries, namely scikit-learn, PyTorch, and Hugging Face\ntransformers. The latter integrations are optionally installable extensions, so\nGPUs can be used but are not required. Using this new library, we investigate\nthe performance of the recently published SetFit training paradigm, which we\ncompare to vanilla transformer fine-tuning, finding that it matches the latter\nin classification accuracy while outperforming it in area under the curve. The\nlibrary is available under the MIT License at\nhttps://github.com/webis-de/small-text, in version 1.3.0 at the time of\nwriting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schroder_C/0/1/0/all/0/1\">Christopher Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1\">Lydia M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekler_A/0/1/0/all/0/1\">Andreas Niekler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Meet Harry Potter: A Bilingual Dataset for Aligning Dialogue Agents with Characters. (arXiv:2211.06869v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.06869","description":"<p>In recent years, Dialogue-style Large Language Models (LLMs) such as ChatGPT\nand GPT4 have demonstrated immense potential in constructing open-domain\ndialogue agents. However, aligning these agents with specific characters or\nindividuals remains a considerable challenge due to the complexities of\ncharacter representation and the lack of comprehensive annotations. In this\npaper, we introduce the Harry Potter Dialogue (HPD) dataset, designed to\nadvance the study of dialogue agents and character alignment. The dataset\nencompasses all dialogue sessions (in both English and Chinese) from the Harry\nPotter series and is annotated with vital background information, including\ndialogue scenes, speakers, character relationships, and attributes. These\nextensive annotations may empower LLMs to unlock character-driven dialogue\ncapabilities. Furthermore, it can serve as a universal benchmark for evaluating\nhow well can a LLM aligning with a specific character. We benchmark LLMs on HPD\nusing both fine-tuning and in-context learning settings. Evaluation results\nreveal that although there is substantial room for improvement in generating\nhigh-quality, character-aligned responses, the proposed dataset is valuable in\nguiding models toward responses that better align with the character of Harry\nPotter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Similarity Models for Depression Severity Estimation. (arXiv:2211.07624v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07624","description":"<p>Depressive disorders constitute a severe public health issue worldwide.\nHowever, public health systems have limited capacity for case detection and\ndiagnosis. In this regard, the widespread use of social media has opened up a\nway to access public information on a large scale. Computational methods can\nserve as support tools for rapid screening by exploiting this user-generated\nsocial media content. This paper presents an efficient semantic pipeline to\nstudy depression severity in individuals based on their social media writings.\nWe select test user sentences for producing semantic rankings over an index of\nrepresentative training sentences corresponding to depressive symptoms and\nseverity levels. Then, we use the sentences from those results as evidence for\npredicting users' symptom severity. For that, we explore different aggregation\nmethods to answer one of four Beck Depression Inventory (BDI) options per\nsymptom. We evaluate our methods on two Reddit-based benchmarks, achieving 30\\%\nimprovement over state of the art in terms of measuring depression severity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_A/0/1/0/all/0/1\">Anxo P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warikoo_N/0/1/0/all/0/1\">Neha Warikoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parapar_J/0/1/0/all/0/1\">Javier Parapar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Extraction Attack against Self-supervised Speech Models. (arXiv:2211.16044v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2211.16044","description":"<p>Self-supervised learning (SSL) speech models generate meaningful\nrepresentations of given clips and achieve incredible performance across\nvarious downstream tasks. Model extraction attack (MEA) often refers to an\nadversary stealing the functionality of the victim model with only query\naccess. In this work, we study the MEA problem against SSL speech model with a\nsmall number of queries. We propose a two-stage framework to extract the model.\nIn the first stage, SSL is conducted on the large-scale unlabeled corpus to\npre-train a small speech model. Secondly, we actively sample a small portion of\nclips from the unlabeled corpus and query the target model with these clips to\nacquire their representations as labels for the small model's second-stage\ntraining. Experiment results show that our sampling methods can effectively\nextract the target model without knowing any information about its model\narchitecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_T/0/1/0/all/0/1\">Tsu-Yuan Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen-An Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tung-Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Inconsistencies of Conditionals Learned by Masked Language Models. (arXiv:2301.00068v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.00068","description":"<p>Learning to predict masked tokens in a sequence has been shown to be a\npowerful pretraining objective for large language models. After training, such\nmasked language models can provide distributions of tokens conditioned on\nbidirectional context.\n</p>\n<p>In this paper, we show that contrary to popular assumptions, such\nbidirectional conditionals often demonstrate considerable inconsistencies,\ni.e., they cannot be derived from a coherent joint distribution when considered\ntogether. We empirically quantify such inconsistencies in the simple scenario\nof bigram comparison for two common styles of masked language models: T5-style\nand BERT-style. For example, we show that T5 models often confuse their own\npreference regarding two similar bigrams. We show that inconsistencies exist\nubiquitously in masked language models of diverse sizes and configurations,\nfrom RoBERTa-base to GLM-130B.\n</p>\n<p>As an initial attempt to address this issue during the inference phase, we\npropose Ensemble of Conditionals, a self-ensemble algorithm that jointly\nconsiders many inconsistent conditionals directly produced by the MLM to\nsynthesize a distribution that is used as the model's final output. Such\nensembling improves open-source SOTA results on LAMBADA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Young_T/0/1/0/all/0/1\">Tom Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Inferential Reproducibility of Machine Learning Research. (arXiv:2302.04054v7 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.04054","description":"<p>Reliability of machine learning evaluation -- the consistency of observed\nevaluation scores across replicated model training runs -- is affected by\nseveral sources of nondeterminism which can be regarded as measurement noise.\nCurrent tendencies to remove noise in order to enforce reproducibility of\nresearch results neglect inherent nondeterminism at the implementation level\nand disregard crucial interaction effects between algorithmic noise factors and\ndata properties. This limits the scope of conclusions that can be drawn from\nsuch experiments. Instead of removing noise, we propose to incorporate several\nsources of variance, including their interaction with data properties, into an\nanalysis of significance and reliability of machine learning evaluation, with\nthe aim to draw inferences beyond particular instances of trained models. We\nshow how to use linear mixed effects models (LMEMs) to analyze performance\nevaluation scores, and to conduct statistical inference with a generalized\nlikelihood ratio test (GLRT). This allows us to incorporate arbitrary sources\nof noise like meta-parameter variations into statistical significance testing,\nand to assess performance differences conditional on data properties.\nFurthermore, a variance component analysis (VCA) enables the analysis of the\ncontribution of noise sources to overall variance and the computation of a\nreliability coefficient by the ratio of substantial to total variance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagmann_M/0/1/0/all/0/1\">Michael Hagmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meier_P/0/1/0/all/0/1\">Philipp Meier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1\">Stefan Riezler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PK-ICR: Persona-Knowledge Interactive Context Retrieval for Grounded Dialogue. (arXiv:2302.06674v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.06674","description":"<p>Identifying relevant persona or knowledge for conversational systems is\ncritical to grounded dialogue response generation. However, each grounding has\nbeen mostly researched in isolation with more practical multi-context dialogue\ntasks introduced in recent works. We define Persona and Knowledge Dual Context\nIdentification as the task to identify persona and knowledge jointly for a\ngiven dialogue, which could be of elevated importance in complex multi-context\ndialogue settings. We develop a novel grounding retrieval method that utilizes\nall contexts of dialogue simultaneously. Our method requires less computational\npower via utilizing neural QA retrieval models. We further introduce our novel\nnull-positive rank test which measures ranking performance on semantically\ndissimilar samples (i.e. hard negatives) in relation to data augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1\">Minsik Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joosung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Response Generation for Chinese Reading Comprehension. (arXiv:2302.08817v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.08817","description":"<p>Machine reading comprehension (MRC) is an important area of conversation\nagents and draws a lot of attention. However, there is a notable limitation to\ncurrent MRC benchmarks: The labeled answers are mostly either spans extracted\nfrom the target corpus or the choices of the given candidates, ignoring the\nnatural aspect of high-quality responses. As a result, MRC models trained on\nthese datasets can not generate human-like responses in real QA scenarios. To\nthis end, we construct a new dataset called Penguin to promote the research of\nMRC, providing a training and test bed for natural response generation to real\nscenarios. Concretely, Penguin consists of 200k training data with high-quality\nfluent, and well-informed responses. Penguin is the first benchmark towards\nnatural response generation in Chinese MRC on a relatively large scale. To\naddress the challenges in Penguin, we develop two strong baselines: end-to-end\nand two-stage frameworks. Following that, we further design Prompt-BART:\nfine-tuning the pre-trained generative language models with a mixture of prefix\nprompts in Penguin. Extensive experiments validated the effectiveness of this\ndesign.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yinan Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baoyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RETVec: Resilient and Efficient Text Vectorizer. (arXiv:2302.09207v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.09207","description":"<p>This paper describes RETVec, an efficient, resilient, and multilingual text\nvectorizer designed for neural-based text processing. RETVec combines a novel\ncharacter encoding with an optional small embedding model to embed words into a\n256-dimensional vector space. The RETVec embedding model is pre-trained using\npair-wise metric learning to be robust against typos and character-level\nadversarial attacks. In this paper, we evaluate and compare RETVec to\nstate-of-the-art vectorizers and word embeddings on popular model architectures\nand datasets. These comparisons demonstrate that RETVec leads to competitive,\nmultilingual models that are significantly more resilient to typos and\nadversarial text attacks. RETVec is available under the Apache 2 license at\nhttps://github.com/google-research/retvec.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bursztein_E/0/1/0/all/0/1\">Elie Bursztein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Marina Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vallis_O/0/1/0/all/0/1\">Owen Vallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xinyu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurakin_A/0/1/0/all/0/1\">Alexey Kurakin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?. (arXiv:2302.11713v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2302.11713","description":"<p>Pre-trained vision and language models have demonstrated state-of-the-art\ncapabilities over existing tasks involving images and texts, including visual\nquestion answering. However, it remains unclear whether these models possess\nthe capability to answer questions that are not only querying visual content\nbut knowledge-intensive and information-seeking. In this study, we introduce\nInfoSeek, a visual question answering dataset tailored for information-seeking\nquestions that cannot be answered with only common sense knowledge. Using\nInfoSeek, we analyze various pre-trained visual question answering models and\ngain insights into their characteristics. Our findings reveal that\nstate-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.)\nface challenges in answering visual information-seeking questions, but\nfine-tuning on the InfoSeek dataset elicits models to use fine-grained\nknowledge that was learned during their pre-training. Furthermore, we show that\naccurate visual entity recognition can be used to improve performance on\nInfoSeek by retrieving relevant documents, showing a significant space for\nimprovement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_Y/0/1/0/all/0/1\">Yi Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haitian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Changpinyo_S/0/1/0/all/0/1\">Soravit Changpinyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding Support Examples for In-Context Learning. (arXiv:2302.13539v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.13539","description":"<p>Additionally, the strong dependency among in-context examples makes it an\nNP-hard combinatorial optimization problem and enumerating all permutations is\ninfeasible. Hence we propose LENS, a fiLter-thEN-Search method to tackle this\nchallenge in two stages: First we filter the dataset to obtain informative\nin-context examples individually. Specifically, we propose a novel metric,\nInfoScore, to evaluate the example's in-context informativeness based on the\nlanguage model's feedback, and further propose a progressive filtering process\nto filter out uninformative examples. Then we propose diversity-guided example\nsearch which iteratively refines and evaluates the selected example\npermutations, to find examples that fully depict the task. The experimental\nresults show that LENS significantly outperforms a wide range of baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaonan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OptBA: Optimizing Hyperparameters with the Bees Algorithm for Improved Medical Text Classification. (arXiv:2303.08021v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.08021","description":"<p>One of the challenges that artificial intelligence engineers face,\nspecifically in the field of deep learning is obtaining the optimal model\nhyperparameters. The search for optimal hyperparameters usually hinders the\nprogress of solutions to real-world problems such as healthcare. To overcome\nthis hurdle, the proposed work introduces a novel mechanism called ``OptBA\" to\nautomatically fine-tune the hyperparameters of deep learning models by\nleveraging the Bees Algorithm, which is a recent promising swarm intelligence\nalgorithm. In this paper, the optimization problem of OptBA is to maximize the\naccuracy in classifying ailments using medical text, where initial\nhyperparameters are iteratively adjusted by specific criteria. Experimental\nresults demonstrate a noteworthy enhancement in accuracy with approximately\n1.4%. This outcome highlights the effectiveness of the proposed mechanism in\naddressing the critical issue of hyperparameter optimization and its potential\nimpact on advancing solutions for healthcare and other societal challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaaban_M/0/1/0/all/0/1\">Mai A. Shaaban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashkash_M/0/1/0/all/0/1\">Mariam Kashkash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alghfeli_M/0/1/0/all/0/1\">Maryam Alghfeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_A/0/1/0/all/0/1\">Adham Ibrahim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeltaScore: Story Evaluation with Perturbations. (arXiv:2303.08991v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.08991","description":"<p>Numerous evaluation metrics have been developed for natural language\ngeneration tasks, but their effectiveness in evaluating stories is limited as\nthey are not specifically tailored to assess intricate aspects of storytelling,\nsuch as fluency and interestingness. In this paper, we introduce DELTASCORE, a\nnovel methodology that employs perturbation techniques for the evaluation of\nnuanced story aspects. Our central proposition posits that the extent to which\na story excels in a specific aspect (e.g., fluency) correlates with the\nmagnitude of its susceptibility to particular perturbations (e.g., the\nintroduction of typos). Given this, we measure the quality of an aspect by\ncalculating the likelihood difference between pre- and post-perturbation states\nusing pre-trained language models. We compare DELTASCORE with existing metrics\non storytelling datasets from two domains in five fine-grained story aspects:\nfluency, coherence, relatedness, logicality, and interestingness. DELTASCORE\ndemonstrates remarkable performance, revealing a surprising finding that a\nspecific perturbation proves highly effective in capturing multiple aspects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhuohan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Miao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieving Multimodal Information for Augmented Generation: A Survey. (arXiv:2303.10868v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.10868","description":"<p>As Large Language Models (LLMs) become popular, there emerged an important\ntrend of using multimodality to augment the LLMs' generation ability, which\nenables LLMs to better interact with the world. However, there lacks a unified\nperception of at which stage and how to incorporate different modalities. In\nthis survey, we review methods that assist and augment generative models by\nretrieving multimodal knowledge, whose formats range from images, codes,\ntables, graphs, to audio. Such methods offer a promising solution to important\nconcerns such as factuality, reasoning, interpretability, and robustness. By\nproviding an in-depth review, this survey is expected to provide scholars with\na deeper understanding of the methods' applications and encourage them to adapt\nexisting techniques to the fast-growing field of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruochen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hailin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weishi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_F/0/1/0/all/0/1\">Fangkai Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_X/0/1/0/all/0/1\">Xuan Long Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chengwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bosheng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaobao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How does Transformer model evolve to learn diverse chemical structures?. (arXiv:2303.11593v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2303.11593","description":"<p>Recent years have seen rapid development of descriptor generation based on\nrepresentation learning of extremely diverse molecules, especially those that\napply natural language processing (NLP) models to SMILES, a literal\nrepresentation of molecular structure. However, little research has been done\non how these models understand chemical structure. To address this black box,\nwe investigated the relationship between the learning progress of SMILES and\nchemical structure using a representative NLP model, the Transformer. The\nresults suggest that while the Transformer learns partial structures of\nmolecules quickly, it requires extended training to understand overall\nstructures. Consistently, the accuracy of molecular property predictions using\ndescriptors generated from models at different learning steps was similar from\nthe beginning to the end of training. Furthermore, we found that the\nTransformer requires particularly long training to learn chirality and\nsometimes stagnates with low translation accuracy due to misunderstanding of\nenantiomers. These findings are expected to deepen the understanding of NLP\nmodels in chemistry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoshikai_Y/0/1/0/all/0/1\">Yasuhiro Yoshikai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mizuno_T/0/1/0/all/0/1\">Tadahaya Mizuno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nemoto_S/0/1/0/all/0/1\">Shumpei Nemoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusuhara_H/0/1/0/all/0/1\">Hiroyuki Kusuhara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT. (arXiv:2303.13809v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.13809","description":"<p>Generative large language models (LLMs), e.g., ChatGPT, have demonstrated\nremarkable proficiency across several NLP tasks, such as machine translation,\ntext summarization. Recent research (Kocmi and Federmann, 2023) has shown that\nutilizing ChatGPT for assessing the quality of machine translation (MT)\nachieves state-of-the-art performance at the system level but performs poorly\nat the segment level. To further improve the performance of LLMs on MT quality\nassessment, we conduct an investigation into several prompting methods, and\npropose a new prompting method called Error Analysis Prompting (EAPrompt) by\ncombining Chain-of-Thoughts (Wei et al., 2022) and Error Analysis (Lu et al.,\n2022). Our results on WMT22 indicate that prompting LLMs like ChatGPT with\nerror analysis can generate human-like MT evaluations at both the system and\nsegment level. Additionally, we first discover some limitations of ChatGPT as\nan MT evaluator, such as changing the order of input may significantly\ninfluence the judgment when providing multiple translations in a single query.\nThis work provides a preliminary experience of prompting LLMs as an evaluator\nto improve the reliability of translation evaluation metrics under the error\nanalysis paradigm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qingyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_B/0/1/0/all/0/1\">Baopu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kanjian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocmi_T/0/1/0/all/0/1\">Tom Kocmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Debiasing Scores and Prompts of 2D Diffusion for View-consistent Text-to-3D Generation. (arXiv:2303.15413v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2303.15413","description":"<p>Existing score-distilling text-to-3D generation techniques, despite their\nconsiderable promise, often encounter the view inconsistency problem. One of\nthe most notable issues is the Janus problem, where the most canonical view of\nan object (\\textit{e.g}., face or head) appears in other views. In this work,\nwe explore existing frameworks for score-distilling text-to-3D generation and\nidentify the main causes of the view inconsistency problem -- the embedded bias\nof 2D diffusion models. Based on these findings, we propose two approaches to\ndebias the score-distillation frameworks for view-consistent text-to-3D\ngeneration. Our first approach, called score debiasing, involves cutting off\nthe score estimated by 2D diffusion models and gradually increasing the\ntruncation value throughout the optimization process. Our second approach,\ncalled prompt debiasing, identifies conflicting words between user prompts and\nview prompts using a language model, and adjusts the discrepancy between view\nprompts and the viewing direction of an object. Our experimental results show\nthat our methods improve the realism of the generated 3D objects by\nsignificantly reducing artifacts and achieve a good trade-off between\nfaithfulness to the 2D diffusion models and 3D consistency with little\noverhead. Our project page is available\nat~\\url{https://susunghong.github.io/Debiased-Score-Distillation-Sampling/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Susung Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_D/0/1/0/all/0/1\">Donghoon Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.15714","description":"<p>Language models have been shown to perform remarkably well on a wide range of\nnatural language processing tasks. In this paper, we propose LEAP, a novel\nsystem that uses language models to perform multi-step logical reasoning and\nincorporates explicit planning into the inference procedure. Explicit planning\nenables the system to make more informed reasoning decisions at each step by\nlooking ahead into their future effects. Moreover, we propose a training\nstrategy that safeguards the planning process from being led astray by spurious\nfeatures. Our full system significantly outperforms other competing methods on\nmultiple standard datasets. When using small T5 models as its core selection\nand deduction components, our system performs competitively compared to GPT-3\ndespite having only about 1B parameters (i.e., 175 times smaller than GPT-3).\nWhen using GPT-3.5, it significantly outperforms chain-of-thought prompting on\nthe challenging PrOntoQA dataset. We have conducted extensive empirical studies\nto demonstrate that explicit planning plays a crucial role in the system's\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hongyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kangrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1\">Hongyuan Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models. (arXiv:2304.01933v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01933","description":"<p>The success of large language models (LLMs), like GPT-4 and ChatGPT, has led\nto the development of numerous cost-effective and accessible alternatives that\nare created by finetuning open-access LLMs with task-specific data (e.g.,\nChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning\nmethods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly\none of the most attractive topics, as it only requires fine-tuning a few\nexternal parameters instead of the entire LLMs while achieving comparable or\neven better performance. To enable further research on PEFT methods of LLMs,\nthis paper presents LLM-Adapters, an easy-to-use framework that integrates\nvarious adapters into LLMs and can execute these adapter-based PEFT methods of\nLLMs for different tasks. The framework includes state-of-the-art open-access\nLLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such as\nSeries adapters, Parallel adapter, Prompt-based learning and\nReparametrization-based methods. Moreover, we conduct extensive empirical\nstudies on the impact of adapter types, placement locations, and\nhyper-parameters to the best design for each adapter-based methods. We evaluate\nthe effectiveness of the adapters on fourteen datasets from two different\nreasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results\ndemonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few\nextra trainable parameters yields comparable, and in some cases superior,\nperformance to powerful LLMs (175B) in zero-shot inference on both reasoning\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiqiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yihuai Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wanyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1\">Ee-Peng Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GEMINI: Controlling the Sentence-level Writing Style for Abstractive Text Summarization. (arXiv:2304.03548v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.03548","description":"<p>Human experts write summaries using different techniques, including\nextracting a sentence from the document and rewriting it, or fusing various\ninformation from the document to abstract it. These techniques are flexible and\nthus difficult to be imitated by any single method. To address this issue, we\npropose an adaptive model, GEMINI, that integrates a rewriter and a generator\nto mimic the sentence rewriting and abstracting techniques, respectively.\nGEMINI adaptively chooses to rewrite a specific document sentence or generate a\nsummary sentence from scratch. Experiments demonstrate that our adaptive\napproach outperforms the pure abstractive and rewriting baselines on three\nbenchmark datasets, achieving the best results on WikiHow. Interestingly,\nempirical results show that the human summary styles of summary sentences are\nconsistently predictable given their context. We release our code and model at\n\\url{https://github.com/baoguangsheng/gemini}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_G/0/1/0/all/0/1\">Guangsheng Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zebin Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition. (arXiv:2304.04704v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2304.04704","description":"<p>This work proposes POMP, a prompt pre-training method for vision-language\nmodels. Being memory and computation efficient, POMP enables the learned prompt\nto condense semantic information for a rich set of visual concepts with over\ntwenty-thousand classes. Once pre-trained, the prompt with a strong\ntransferable ability can be directly plugged into a variety of visual\nrecognition tasks including image classification, semantic segmentation, and\nobject detection, to boost recognition performances in a zero-shot manner.\nEmpirical evaluation shows that POMP achieves state-of-the-art performances on\n21 datasets, e.g., 67.0% average accuracy on 10 classification datasets (+3.1%\ncompared to CoOp) and 84.4 hIoU on open-vocabulary Pascal VOC segmentation\n(+6.9 compared to ZSSeg). Our code is available at\nhttps://github.com/amazon-science/prompt-pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuhuai Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alex Smola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RRHF: Rank Responses to Align Language Models with Human Feedback without tears. (arXiv:2304.05302v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.05302","description":"<p>Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment\nof large language models with human preferences, significantly enhancing the\nquality of interactions between humans and models. InstructGPT implements RLHF\nthrough several stages, including Supervised Fine-Tuning (SFT), reward model\ntraining, and Proximal Policy Optimization (PPO). However, PPO is sensitive to\nhyperparameters and requires multiple models in its standard implementation,\nmaking it hard to train and scale up to larger parameter counts. In contrast,\nwe propose a novel learning paradigm called RRHF, which scores sampled\nresponses from different sources via a logarithm of conditional probabilities\nand learns to align these probabilities with human preferences through ranking\nloss. RRHF can leverage sampled responses from various sources including the\nmodel responses from itself, other large language model responses, and human\nexpert responses to learn to rank them. RRHF only needs 1 to 2 models during\ntuning and can efficiently align language models with human preferences\nrobustly without complex hyperparameter tuning. Additionally, RRHF can be\nconsidered an extension of SFT and reward model training while being simpler\nthan PPO in terms of coding, model counts, and hyperparameters. We evaluate\nRRHF on the Helpful and Harmless dataset, demonstrating comparable alignment\nperformance with PPO by reward model score and human labeling. Extensive\nexperiments show that the performance of RRHF is highly related to sampling\nquality which suggests RRHF is a best-of-n learner. Codes available at\nhttps://github.com/GanjinZero/RRHF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongyi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-NER: Named Entity Recognition via Large Language Models. (arXiv:2304.10428v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.10428","description":"<p>Despite the fact that large-scale Language Models (LLM) have achieved SOTA\nperformances on a variety of NLP tasks, its performance on NER is still\nsignificantly below supervised baselines. This is due to the gap between the\ntwo tasks the NER and LLMs: the former is a sequence labeling task in nature\nwhile the latter is a text-generation model.\n</p>\n<p>In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the\ngap by transforming the sequence labeling task to a generation task that can be\neasily adapted by LLMs e.g., the task of finding location entities in the input\ntext \"Columbus is a city\" is transformed to generate the text sequence\n\"@@Columbus## is a city\", where special tokens @@## marks the entity to\nextract. To efficiently address the \"hallucination\" issue of LLMs, where LLMs\nhave a strong inclination to over-confidently label NULL inputs as entities, we\npropose a self-verification strategy by prompting LLMs to ask itself whether\nthe extracted entities belong to a labeled entity tag.\n</p>\n<p>We conduct experiments on five widely adopted NER datasets, and GPT-NER\nachieves comparable performances to fully supervised baselines, which is the\nfirst time as far as we are concerned. More importantly, we find that GPT-NER\nexhibits a greater ability in the low-resource and few-shot setups, when the\namount of training data is extremely scarce, GPT-NER performs significantly\nbetter than supervised models. This demonstrates the capabilities of GPT-NER in\nreal-world NER applications where the number of labeled examples is limited.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_R/0/1/0/all/0/1\">Rongbin Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summarizing Multiple Documents with Conversational Structure for Meta-Review Generation. (arXiv:2305.01498v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01498","description":"<p>We present PeerSum, a novel dataset for generating meta-reviews of scientific\npapers. The meta-reviews can be interpreted as abstractive summaries of\nreviews, multi-turn discussions and the paper abstract. These source documents\nhave rich inter-document relationships with an explicit hierarchical\nconversational structure, cross-references and (occasionally) conflicting\ninformation. To introduce the structural inductive bias into pre-trained\nlanguage models, we introduce Rammer ( Relationship-aware Multi-task\nMeta-review Generator), a model that uses sparse attention based on the\nconversational structure and a multi-task training objective that predicts\nmetadata features (e.g., review ratings). Our experimental results show that\nRammer outperforms other strong baseline models in terms of a suite of\nautomatic evaluation metrics. Further analyses, however, reveal that RAMMER and\nother models struggle to handle conflicts in source documents of PeerSum,\nsuggesting meta-review generation is a challenging task and a promising avenue\nfor further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Miao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoT: Memory-of-Thought Enables ChatGPT to Self-Improve. (arXiv:2305.05181v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.05181","description":"<p>Large Language Models (LLMs) have shown impressive abilities in various\ntasks. However, fundamentally improving them depends on high-quality datasets\nor computationally expensive fine-tuning. On the contrary, humans can easily\nimprove themselves by self-thinking and memory, without external resources. In\nthis paper, we propose a framework, MoT, to let the LLM self-improve through\nMemory-of-Thought, without annotated datasets and parameter updates.\nSpecifically, MoT is divided into two stages: 1. before the test stage, the LLM\npre-thinks on the unlabeled dataset and saves the high-confidence thoughts as\nexternal memory; 2. During the test stage, given a test question, the LLM\nrecalls relevant memory to help itself reason and answer it. Experimental\nresults show that MoT can help ChatGPT significantly improve its abilities in\narithmetic reasoning, commonsense reasoning, factual reasoning, and natural\nlanguage inference. Further analyses show that each component contributes\ncritically to the improvements and MoT can lead to consistent improvements\nacross various CoT methods and LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaonan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Learning for Person or Entity-centric Knowledge Graphs: An Application in Healthcare. (arXiv:2305.05640v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2305.05640","description":"<p>Knowledge graphs (KGs) are a popular way to organise information based on\nontologies or schemas and have been used across a variety of scenarios from\nsearch to recommendation. Despite advances in KGs, representing knowledge\nremains a non-trivial task across industries and it is especially challenging\nin the biomedical and healthcare domains due to complex interdependent\nrelations between entities, heterogeneity, lack of standardization, and\nsparseness of data. KGs are used to discover diagnoses or prioritize genes\nrelevant to disease, but they often rely on schemas that are not centred around\na node or entity of interest, such as a person. Entity-centric KGs are\nrelatively unexplored but hold promise in representing important facets\nconnected to a central node and unlocking downstream tasks beyond graph\ntraversal and reasoning, such as generating graph embeddings and training graph\nneural networks for a wide range of predictive tasks. This paper presents an\nend-to-end representation learning framework to extract entity-centric KGs from\nstructured and unstructured data. We introduce a star-shaped ontology to\nrepresent the multiple facets of a person and use it to guide KG creation.\nCompact representations of the graphs are created leveraging graph neural\nnetworks and experiments are conducted using different levels of heterogeneity\nor explicitness. A readmission prediction task is used to evaluate the results\nof the proposed framework, showing a stable system, robust to missing data,\nthat outperforms a range of baseline machine learning classifiers. We highlight\nthat this approach has several potential applications across domains and is\nopen-sourced. Lastly, we discuss lessons learned, challenges, and next steps\nfor the adoption of the framework in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Theodoropoulos_C/0/1/0/all/0/1\">Christos Theodoropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulligan_N/0/1/0/all/0/1\">Natasha Mulligan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stappenbeck_T/0/1/0/all/0/1\">Thaddeus Stappenbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bettencourt_Silva_J/0/1/0/all/0/1\">Joao Bettencourt-Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning Language Models with Generative Adversarial Feedback. (arXiv:2305.06176v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06176","description":"<p>Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to\nsignificantly enhance the performance of large language models (LLMs) by\naligning their outputs with desired human values through instruction tuning.\nHowever, RLHF is constrained by the expertise and productivity limitations of\nhuman evaluators. A response to this downside is to fall back to supervised\nfine-tuning (SFT) with additional carefully selected expert demonstrations.\nHowever, while this method has been proven to be effective, it invariably also\nleads to increased human-in-the-loop overhead. In this study, we propose\nanother alternative approach: Reinforcement Learning with Generative\nAdversarial Feedback (RLGAF) to RLHF and SFT, which uses a generative\nadversarial training style to enable the LLMs to learn useful human expert\ndemonstrations without being directly exposed to the training examples, thus\nenabling good generalization capabilities while preserving sample efficiency.\nOur preliminary findings indicate that RLGAF can help align LLMs outputs with\ncompetitive performance against RLHF and SFT, while not suffering from their\nrespective inherent restrictions, suggesting promising avenues for further\nresearch on automating AI alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhang Ze Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaw_L/0/1/0/all/0/1\">Lau Jia Jaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wong Qin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_Z/0/1/0/all/0/1\">Zhang Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Low_B/0/1/0/all/0/1\">Bryan Kian Hsiang Low</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Evaluation of Attribution by Large Language Models. (arXiv:2305.06311v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06311","description":"<p>A recent focus of large language model (LLM) development, as exemplified by\ngenerative search engines, is to incorporate external references to generate\nand support its claims. However, evaluating the attribution, i.e., verifying\nwhether the generated statement is fully supported by the cited reference,\nremains an open problem. Although human evaluation is common practice, it is\ncostly and time-consuming. In this paper, we investigate the automatic\nevaluation of attribution given by LLMs. We begin by defining different types\nof attribution errors, and then explore two approaches for automatic\nevaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is\nrepurposed from related tasks such as question answering, fact-checking,\nnatural language inference, and summarization. We manually curate a set of test\nexamples covering 12 domains from a generative search engine, New Bing. Our\nresults on this curated test set and simulated examples from existing\nbenchmarks highlight both promising signals and challenges. We hope our problem\nformulation, testbeds, and findings will help lay the foundation for future\nstudies on this important problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boshi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziru Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Classification via Large Language Models. (arXiv:2305.08377v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08377","description":"<p>Despite the remarkable success of large-scale Language Models (LLMs) such as\nGPT-3, their performances still significantly underperform fine-tuned models in\nthe task of text classification. This is due to (1) the lack of reasoning\nability in addressing complex linguistic phenomena (e.g., intensification,\ncontrast, irony etc); (2) limited number of tokens allowed in in-context\nlearning.\n</p>\n<p>In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adopts\na progressive reasoning strategy tailored to addressing the complex linguistic\nphenomena involved in text classification: CARP first prompts LLMs to find\nsuperficial clues (e.g., keywords, tones, semantic relations, references, etc),\nbased on which a diagnostic reasoning process is induced for final decisions.\nTo further address the limited-token issue, CARP uses a fine-tuned model on the\nsupervised dataset for $k$NN demonstration search in the in-context learning,\nallowing the model to take the advantage of both LLM's generalization ability\nand the task-specific evidence provided by the full labeled dataset.\nRemarkably, CARP yields new SOTA performances on 4 out of 5 widely-used\ntext-classification benchmarks, 97.39 (+1.24) on SST-2, 96.40 (+0.72) on\nAGNews, 98.78 (+0.25) on R8 and 96.95 (+0.6) on R52, and a performance\ncomparable to SOTA on MR (92.39 v.s. 93.3). More importantly, we find that CARP\ndelivers impressive abilities on low-resource and domain-adaptation setups.\nSpecifically, using 16 examples per class, CARP achieves comparable\nperformances to supervised models with 1,024 examples per class.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shangwei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Rumination for Pre-trained Language Models. (arXiv:2305.08732v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08732","description":"<p>Previous studies have revealed that vanilla pre-trained language models\n(PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus,\nseveral works have attempted to integrate external knowledge into PLMs.\nHowever, despite the promising outcome, we empirically observe that PLMs may\nhave already encoded rich knowledge in their pre-trained parameters but fail to\nfully utilize them when applying them to knowledge-intensive tasks. In this\npaper, we propose a new paradigm dubbed Knowledge Rumination to help the\npre-trained language model utilize that related latent knowledge without\nretrieving it from the external corpus. By simply adding a prompt like \"As far\nas I know\" to the PLMs, we try to review related latent knowledge and inject\nthem back into the model for knowledge consolidation. We apply the proposed\nknowledge rumination to various language models, including RoBERTa, DeBERTa,\nand GPT-3. Experimental results on six commonsense reasoning tasks and GLUE\nbenchmarks demonstrate the effectiveness of our proposed approach, which proves\nthat the knowledge stored in PLMs can be better exploited to enhance\nperformance. Code is available in\nhttps://github.com/zjunlp/knowledge-rumination.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shengyu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Knowledge Graph Forecasting Using In-Context Learning. (arXiv:2305.10613v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10613","description":"<p>Temporal knowledge graph (TKG) forecasting benchmarks challenge models to\npredict future facts using knowledge of past facts. In this paper, we apply\nlarge language models (LLMs) to these benchmarks using in-context learning\n(ICL). We investigate whether and to what extent LLMs can be used for TKG\nforecasting, especially without any fine-tuning or explicit modules for\ncapturing structural and temporal information. For our experiments, we present\na framework that converts relevant historical facts into prompts and generates\nranked predictions using token probabilities. Surprisingly, we observe that\nLLMs, out-of-the-box, perform on par with state-of-the-art TKG models carefully\ndesigned and trained for TKG forecasting. Our extensive evaluation presents\nperformances across several models and datasets with different characteristics,\ncompares alternative heuristics for preparing contextual information, and\ncontrasts to prominent TKG methods and simple frequency and recency baselines.\nWe also discover that using numerical indices instead of entity/relation names,\ni.e., hiding semantic information, does not significantly affect the\nperformance ($\\pm$0.4\\% Hit@1). This shows that prior semantic knowledge is\nunnecessary; instead, LLMs can leverage the existing patterns in the context to\nachieve such performance. Our analysis also reveals that ICL enables LLMs to\nlearn irregular patterns from the historical context, going beyond simple\npredictions based on common or recent information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-Ho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahrabian_K/0/1/0/all/0/1\">Kian Ahrabian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Woojeong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morstatter_F/0/1/0/all/0/1\">Fred Morstatter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Norm Violations in Live-Stream Chat. (arXiv:2305.10731v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10731","description":"<p>Toxic language, such as hate speech, can deter users from participating in\nonline communities and enjoying popular platforms. Previous approaches to\ndetecting toxic language and norm violations have been primarily concerned with\nconversations from online forums and social media, such as Reddit and Twitter.\nThese approaches are less effective when applied to conversations on\nlive-streaming platforms, such as Twitch and YouTube Live, as each comment is\nonly visible for a limited time and lacks a thread structure that establishes\nits relationship with other comments. In this work, we share the first NLP\nstudy dedicated to detecting norm violations in conversations on live-streaming\nplatforms. We define norm violation categories in live-stream chats and\nannotate 4,583 moderated comments from Twitch. We articulate several facets of\nlive-stream data that differ from other forums, and demonstrate that existing\nmodels perform poorly in this setting. By conducting a user study, we identify\nthe informational context humans use in live-stream moderation, and train\nmodels leveraging context to identify norm violations. Our results show that\nappropriate contextual information can boost moderation performance by 35\\%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1\">Jihyung Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-Ho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyundong Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Woojeong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chan Young Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungjoon Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoTrial: Prompting Language Models for Clinical Trial Design. (arXiv:2305.11366v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11366","description":"<p>Clinical trials are critical for drug development. Constructing the\nappropriate eligibility criteria (i.e., the inclusion/exclusion criteria for\npatient recruitment) is essential for the trial's success. Proper design of\nclinical trial protocols should consider similar precedent trials and their\neligibility criteria to ensure sufficient patient coverage. In this paper, we\npresent a method named AutoTrial to aid the design of clinical eligibility\ncriteria using language models. It allows (1) controllable generation under\ninstructions via a hybrid of discrete and neural prompting, (2) scalable\nknowledge incorporation via in-context learning, and (3) explicit reasoning\nchains to provide rationales for understanding the outputs. Experiments on over\n70K clinical trials verify that AutoTrial generates high-quality criteria texts\nthat are fluent and coherent and with high accuracy in capturing the relevant\nclinical concepts to the target trial. It is noteworthy that our method, with a\nmuch smaller parameter size, gains around 60% winning rate against the GPT-3.5\nbaselines via human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Cao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jimeng Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Autoregressive Document-Level Machine Translation. (arXiv:2305.12878v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12878","description":"<p>Non-autoregressive translation (NAT) models achieve comparable performance\nand superior speed compared to auto-regressive translation (AT) models in the\ncontext of sentence-level machine translation (MT). However, their abilities\nare unexplored in document-level MT, hindering their usage in real scenarios.\nIn this paper, we conduct a comprehensive examination of typical NAT models in\nthe context of document-level MT and further propose a simple but effective\ndesign of sentence alignment between source and target. Experiments show that\nNAT models achieve high acceleration on documents, and sentence alignment\nsignificantly enhances their performance.\n</p>\n<p>However, current NAT models still have a significant performance gap compared\nto their AT counterparts. Further investigation reveals that NAT models suffer\nmore from the multi-modality and misalignment issues in the context of\ndocument-level MT, and current NAT models struggle with exploiting document\ncontext and handling discourse phenomena. We delve into these challenges and\nprovide our code at \\url{https://github.com/baoguangsheng/nat-on-doc}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_G/0/1/0/all/0/1\">Guangsheng Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1\">Zhiyang Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jianhao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A Preliminary Study on Writing Assistance. (arXiv:2305.13225v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13225","description":"<p>Proprietary Large Language Models (LLMs), such as ChatGPT, have garnered\nsignificant attention due to their exceptional capabilities in handling a\ndiverse range of tasks. Recent studies demonstrate that open-sourced smaller\nfoundational models, such as 7B-size LLaMA, can also display remarkable\nproficiency in tackling diverse tasks when fine-tuned using instruction-driven\ndata. In this work, we investigate a practical problem setting where the\nprimary focus is on one or a few particular tasks rather than general-purpose\ninstruction following, and explore whether LLMs can be beneficial and further\nimproved for such targeted scenarios. We choose the writing-assistant scenario\nas the testbed, which includes seven writing tasks. We collect training data\nfor these tasks, reframe them in an instruction-following format, and\nsubsequently refine the LLM, specifically LLaMA, via instruction tuning.\nExperimental results show that fine-tuning LLaMA on writing instruction data\nsignificantly improves its ability on writing tasks. We also conduct more\nexperiments and analyses to offer insights for future work on effectively\nfine-tuning LLaMA for specific scenarios. Finally, we initiate a discussion\nregarding the necessity of employing LLMs for only one targeted task, taking\ninto account the efforts required for tuning and the resources consumed during\ndeployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Leyang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinting Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1\">Tao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1\">Wei Bi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis. (arXiv:2305.13230v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.13230","description":"<p>Recent research has highlighted the importance of dataset size in scaling\nlanguage models. However, large language models (LLMs) are notoriously\ntoken-hungry during pre-training, and high-quality text data on the web is\napproaching its scaling limit for LLMs. To further enhance LLMs, a\nstraightforward approach is to repeat the pre-training data for additional\nepochs. In this study, we empirically investigate three key aspects under this\napproach. First, we explore the consequences of repeating pre-training data,\nrevealing that the model is susceptible to overfitting, leading to multi-epoch\ndegradation. Second, we examine the key factors contributing to multi-epoch\ndegradation, finding that significant factors include dataset size, model\nparameters, and training objectives, while less influential factors consist of\ndataset quality and model FLOPs. Finally, we explore whether widely used\nregularization can alleviate multi-epoch degradation. Most regularization\ntechniques do not yield significant improvements, except for dropout, which\ndemonstrates remarkable effectiveness but requires careful tuning when scaling\nup the model size. Additionally, we discover that leveraging mixture-of-experts\n(MoE) enables cost-effective and efficient hyper-parameter tuning for\ncomputationally intensive dense LLMs with comparable trainable parameters,\npotentially impacting efficient LLM development on a broader scale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zangwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Mistakes via Interactive Study Assistant for Large Language Models. (arXiv:2305.13829v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13829","description":"<p>Large language models (LLMs) have shown promising capabilities to refine\ntheir generation based on feedback. However, LLM refinement based on feedback\nis not always robust and may produce incorrect answers. In this paper, we\npropose Large LAnguage Model (SALAM) to learn and correct from their mistakes.\nOur method introduces a study assistant agent to analyze mistakes and generate\nimprovement guidelines from the main LLM. During inference, it identifies\ncommon misunderstandings based on the mistake collections and provides\nguidelines for LLMs to help them avoid similar mistakes. We further finetune\nthe study assistant using imitation learning with successful feedback\ninteraction. Our experiments on two challenging frameworks (BBH and BBQ)\ndemonstrate that SALAM outperforms baselines by a margin of up to 10.7 in\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Danqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Condensing Multilingual Knowledge with Lightweight Language-Specific Modules. (arXiv:2305.13993v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13993","description":"<p>Incorporating language-specific (LS) modules is a proven method to boost\nperformance in multilingual machine translation. This approach bears similarity\nto Mixture-of-Experts (MoE) because it does not inflate FLOPs. However, the\nscalability of this approach to hundreds of languages (experts) tends to be\nunmanageable due to the prohibitive number of parameters introduced by\nfull-rank matrices in fully-connected layers. In this work, we introduce the\nLanguage-Specific Matrix Synthesis (LMS) method. This approach constructs LS\nmodules by generating low-rank matrices from two significantly smaller matrices\nto approximate the full-rank matrix. Furthermore, we condense multilingual\nknowledge from multiple LS modules into a single shared module with the Fuse\nDistillation (FD) technique to improve the efficiency of inference and model\nserialization. We show that our LMS method significantly outperforms previous\nLS methods and MoE methods with the same amount of extra parameters, e.g., 1.73\nBLEU points over the Switch Transformer on many-to-many multilingual machine\ntranslation. Importantly, LMS is able to have comparable translation\nperformance with much fewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Weiting Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuyue Stella Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunmo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_K/0/1/0/all/0/1\">Kenton Murray</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14259","description":"<p>Literature-Based Discovery (LBD) aims to discover new scientific knowledge by\nmining papers and generating hypotheses. Standard LBD is limited to predicting\npairwise relations between discrete concepts (e.g., drug-disease links). LBD\nalso ignores critical contexts like experimental settings (e.g., a specific\npatient population where a drug is evaluated) and background knowledge and\nmotivations that human scientists consider (e.g., to find a drug candidate\nwithout specific side effects). We address these limitations with a novel\nformulation of contextualized-LBD (C-LBD): generating scientific hypotheses in\nnatural language, while grounding them in a context that controls the\nhypothesis search space. We present a modeling framework using retrieval of\n``inspirations'' from a heterogeneous network of citations and knowledge graph\nrelations, and create a new dataset derived from papers. Our evaluations with\npowerful large language models (LLMs) reveal that GPT4 tends to generate ideas\nwith overall low technical depth and novelty, while our inspiration prompting\napproaches partially mitigate this issue. However, fundamental challenges\nremain on the road to building machines that generate new scientific knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingyun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback. (arXiv:2305.14282v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14282","description":"<p>Automatically evaluating the quality of language generation is critical.\nAlthough recent learned metrics show high correlation with human judgement,\nthese metrics can not explain their verdict or associate the scores with\ndefects in generated text. To address this limitation, we present\nInstructScore, an explainable evaluation metric for text generation. By\nharnessing both explicit human instruction and the implicit knowledge of GPT-4,\nwe fine-tune a text evaluation metric based on LLaMA, producing both a score\nfor generated text and a human readable diagnostic report. We evaluate\nInstructScore on a variety of generation tasks, including translation,\ncaptioning, data-to-text and commonsense generation. Experiments show that our\n7B model surpasses all other unsupervised metrics, including those based on\n175B GPT-3 and GPT-4. Surprisingly, our InstructScore, even without direct\nsupervision from human-rated data, achieves performance levels on par with\nstate-of-the-art metrics like COMET22, which were fine-tuned on human ratings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenda Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Danqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liangming Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhenqiao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models. (arXiv:2305.14318v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14318","description":"<p>Large Language Models (LLMs) have made significant progress in utilizing\ntools, but their ability is limited by API availability and the instability of\nimplicit reasoning, particularly when both planning and execution are involved.\nTo overcome these limitations, we propose CREATOR, a novel framework that\nenables LLMs to create their own tools using documentation and code\nrealization. CREATOR disentangles abstract tool creation and concrete decision\nexecution, resulting in improved performance. We evaluate CREATOR on MATH and\nTabMWP benchmarks, respectively consisting of challenging math competition\nproblems and diverse tabular contents. Remarkably, CREATOR outperforms existing\nchain-of-thought, program-of-thought, and tool-using baselines. Additionally,\nwe introduce the Creation Challenge dataset, featuring 2K diverse questions, to\nemphasize the necessity and benefits of LLMs' tool creation ability. Further\nresearch demonstrates that leveraging LLMs as tool creators facilitates\nknowledge transfer, and LLMs exhibit varying levels of tool creation abilities,\nenabling them to adapt to diverse situations. The tool creation ability\nrevolutionizes the LLM's problem-solving paradigm, driving us closer to the\nnext frontier of artificial intelligence. All the codes and data are released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Cheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_Y/0/1/0/all/0/1\">Yi R. Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt position really matters in few-shot and zero-shot NLU tasks. (arXiv:2305.14493v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14493","description":"<p>Prompt-based models have made remarkable advancements in the fields of\nzero-shot and few-shot learning, attracting a lot of attention from\nresearchers. Developing an effective prompt template plays a critical role.\nHowever, prior studies have mainly focused on prompt vocabulary selection or\nembedding initialization with the reserved prompt position fixed. In this\nempirical study, we conduct the most comprehensive analysis to date of prompt\nposition option for natural language understanding tasks. Our findings quantify\nthe substantial impact prompt position has on model performance. We observe\nthat the prompt position used in prior studies is often sub-optimal for both\nzero-shot and few-shot settings. These findings suggest prompt position\noptimisation as an interesting research direction alongside the existing focus\non prompt engineering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Junyu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Middleton_S/0/1/0/all/0/1\">Stuart E. Middleton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niranjan_M/0/1/0/all/0/1\">Mahesan Niranjan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning. (arXiv:2305.14761v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14761","description":"<p>Charts are very popular for analyzing data, visualizing key insights and\nanswering complex reasoning questions about data. To facilitate chart-based\ndata analysis using natural language, several downstream tasks have been\nintroduced recently such as chart question answering and chart summarization.\nHowever, most of the methods that solve these tasks use pretraining on language\nor vision-language tasks that do not attempt to explicitly model the structure\nof the charts (e.g., how data is visually encoded and how chart elements are\nrelated to each other). To address this, we first build a large corpus of\ncharts covering a wide variety of topics and visual styles. We then present\nUniChart, a pretrained model for chart comprehension and reasoning. UniChart\nencodes the relevant text, data, and visual elements of charts and then uses a\nchart-grounded text decoder to generate the expected output in natural\nlanguage. We propose several chart-specific pretraining tasks that include: (i)\nlow-level tasks to extract the visual elements (e.g., bars, lines) and data\nfrom charts, and (ii) high-level tasks to acquire chart understanding and\nreasoning skills. We find that pretraining the model on a large corpus with\nchart-specific low- and high-level tasks followed by finetuning on three\ndown-streaming tasks results in state-of-the-art performance on three\ndownstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Masry_A/0/1/0/all/0/1\">Ahmed Masry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavehzadeh_P/0/1/0/all/0/1\">Parsa Kavehzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_X/0/1/0/all/0/1\">Xuan Long Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoque_E/0/1/0/all/0/1\">Enamul Hoque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SummIt: Iterative Text Summarization via ChatGPT. (arXiv:2305.14835v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14835","description":"<p>Text summarization systems have made significant progress in recent years,\nbut typically generate summaries in one single step. However, the one-shot\nsummarization setting is sometimes inadequate, as the generated summary may\ncontain hallucinations or overlook essential details related to the reader's\ninterests. This paper addresses this limitation by proposing SummIt, an\niterative text summarization framework based on large language models like\nChatGPT. Our framework enables the model to refine the generated summary\niteratively through self-evaluation and feedback, resembling humans' iterative\nprocess when drafting and revising summaries. Furthermore, we explore the\npotential benefits of integrating knowledge and topic extractors into the\nframework to enhance summary faithfulness and controllability. We automatically\nevaluate the performance of our framework on three benchmark summarization\ndatasets. We also conduct a human evaluation to validate the effectiveness of\nthe iterative refinements and identify a potential issue of over-correction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning. (arXiv:2305.16646v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16646","description":"<p>Large language models have shown astonishing performance on a wide range of\nreasoning tasks. In this paper, we investigate whether they could reason about\nreal-world events and help improve the prediction performance of event sequence\nmodels. We design LAMP, a framework that integrates a large language model in\nevent prediction. Particularly, the language model performs abductive reasoning\nto assist an event sequence model: the event model proposes predictions on\nfuture events given the past; instructed by a few expert-annotated\ndemonstrations, the language model learns to suggest possible causes for each\nproposal; a search module finds out the previous events that match the causes;\na scoring function learns to examine whether the retrieved events could\nactually cause the proposal. Through extensive experiments on several\nchallenging real-world datasets, we demonstrate that our framework -- thanks to\nthe reasoning capabilities of large language models -- could significantly\noutperform the state-of-the-art event sequence models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaoming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_S/0/1/0/all/0/1\">Siqiao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kangrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Fan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">James Y. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1\">Hongyuan Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset. (arXiv:2305.18500v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.18500","description":"<p>Vision and text have been fully explored in contemporary video-text\nfoundational models, while other modalities such as audio and subtitles in\nvideos have not received sufficient attention. In this paper, we resort to\nestablish connections between multi-modality video tracks, including Vision,\nAudio, and Subtitle, and Text by exploring an automatically generated\nlarge-scale omni-modality video caption dataset called VAST-27M. Specifically,\nwe first collect 27 million open-domain video clips and separately train a\nvision and an audio captioner to generate vision and audio captions. Then, we\nemploy an off-the-shelf Large Language Model (LLM) to integrate the generated\ncaptions, together with subtitles and instructional prompts into omni-modality\ncaptions. Based on the proposed VAST-27M dataset, we train an omni-modality\nvideo-text foundational model named VAST, which can perceive and process\nvision, audio, and subtitle modalities from video, and better support various\ntasks including vision-text, audio-text, and multi-modal video-text tasks\n(retrieval, captioning and QA). Extensive experiments have been conducted to\ndemonstrate the effectiveness of our proposed VAST-27M corpus and VAST\nfoundation model. VAST achieves 22 new state-of-the-art results on various\ncross-modality benchmarks. Code, model and dataset will be released at\nhttps://github.com/TXH-mercury/VAST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sihan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Handong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qunbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zijia Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mingzhen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinxin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models. (arXiv:2305.18507v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18507","description":"<p>Large language models (LLMs) have scaled up to unlock a wide range of complex\nreasoning tasks with the aid of various prompting methods. However, current\nprompting methods generate natural language intermediate steps to help\nreasoning, which can cause imperfect task reduction and confusion. To mitigate\nsuch limitations, we explore code prompting, a neural symbolic prompting method\nwith both zero-shot and few-shot versions which triggers code as intermediate\nsteps. We conduct experiments on 7 widely-used benchmarks involving symbolic\nreasoning and arithmetic reasoning. Code prompting generally outperforms\nchain-of-thought (CoT) prompting. To further understand the performance and\nlimitations of code prompting, we perform extensive ablation studies and error\nanalyses, and identify several exclusive advantages of using symbolic\npromptings compared to natural language. We also consider the ensemble of code\nprompting and CoT prompting to combine the strengths of both. Finally, we show\nthrough experiments how code annotations and their locations affect code\nprompting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haotong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Muhan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multitask learning for recognizing stress and depression in social media. (arXiv:2305.18907v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18907","description":"<p>Stress and depression are prevalent nowadays across people of all ages due to\nthe quick paces of life. People use social media to express their feelings.\nThus, social media constitute a valuable form of information for the early\ndetection of stress and depression. Although many research works have been\nintroduced targeting the early recognition of stress and depression, there are\nstill limitations. There have been proposed multi-task learning settings, which\nuse depression and emotion (or figurative language) as the primary and\nauxiliary tasks respectively. However, although stress is inextricably linked\nwith depression, researchers face these two tasks as two separate tasks. To\naddress these limitations, we present the first study, which exploits two\ndifferent datasets collected under different conditions, and introduce two\nmultitask learning frameworks, which use depression and stress as the main and\nauxiliary tasks respectively. Specifically, we use a depression dataset and a\nstressful dataset including stressful posts from ten subreddits of five\ndomains. In terms of the first approach, each post passes through a shared BERT\nlayer, which is updated by both tasks. Next, two separate BERT encoder layers\nare exploited, which are updated by each task separately. Regarding the second\napproach, it consists of shared and task-specific layers weighted by attention\nfusion networks. We conduct a series of experiments and compare our approaches\nwith existing research initiatives, single-task learning, and transfer\nlearning. Experiments show multiple advantages of our approaches over\nstate-of-the-art ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ilias_L/0/1/0/all/0/1\">Loukas Ilias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askounis_D/0/1/0/all/0/1\">Dimitris Askounis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models. (arXiv:2305.19187v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.19187","description":"<p>Large language models (LLMs) specializing in natural language generation\n(NLG) have recently started exhibiting promising capabilities across a variety\nof domains. However, gauging the trustworthiness of responses generated by LLMs\nremains an open challenge, with limited research on uncertainty quantification\n(UQ) for NLG. Furthermore, existing literature typically assumes white-box\naccess to language models, which is becoming unrealistic either due to the\nclosed-source nature of the latest LLMs or computational constraints. In this\nwork, we investigate UQ in NLG for black-box LLMs. We first differentiate\nuncertainty vs confidence: the former refers to the \"dispersion\" of the\npotential predictions for a fixed input, and the latter refers to the\nconfidence on a particular prediction/generation. We then propose and compare\nseveral confidence/uncertainty metrics, applying them to selective NLG where\nunreliable results could either be ignored or yielded for further assessment.\nExperiments were carried out with several popular LLMs on question-answering\ndatasets (for evaluation purposes). Results reveal that a simple metric for the\nsemantic dispersion can be a reliable predictor of the quality of LLM\nresponses, providing valuable insights for practitioners on uncertainty\nmanagement when adopting LLMs. The code to replicate our experiments is\navailable at https://github.com/zlin7/UQ-NLG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_S/0/1/0/all/0/1\">Shubhendu Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jimeng Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting Pretrained ASR Models to Low-resource Clinical Speech using Epistemic Uncertainty-based Data Selection. (arXiv:2306.02105v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.02105","description":"<p>While there has been significant progress in ASR, African-accented clinical\nASR has been understudied due to a lack of training datasets. Building robust\nASR systems in this domain requires large amounts of annotated or labeled data,\nfor a wide variety of linguistically and morphologically rich accents, which\nare expensive to create. Our study aims to address this problem by reducing\nannotation expenses through informative uncertainty-based data selection. We\nshow that incorporating epistemic uncertainty into our adaptation rounds\noutperforms several baseline results, established using state-of-the-art (SOTA)\nASR models, while reducing the required amount of labeled data, and hence\nreducing annotation costs. Our approach also improves out-of-distribution\ngeneralization for very low-resource accents, demonstrating the viability of\nour approach for building generalizable ASR models in the context of accented\nAfrican clinical ASR, where training datasets are predominantly scarce.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dossou_B/0/1/0/all/0/1\">Bonaventure F. P. Dossou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonja_A/0/1/0/all/0/1\">Atnafu Lambebo Tonja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Chinenye Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olatunji_T/0/1/0/all/0/1\">Tobi Olatunji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etori_N/0/1/0/all/0/1\">Naome A Etori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osei_S/0/1/0/all/0/1\">Salomey Osei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1\">Tosin Adewumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sahib Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Valley: Video Assistant with Large Language model Enhanced abilitY. (arXiv:2306.07207v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2306.07207","description":"<p>Large language models (LLMs), with their remarkable conversational\ncapabilities, have demonstrated impressive performance across various\napplications and have emerged as formidable AI assistants. In view of this, it\nraises an intuitive question: Can we harness the power of LLMs to build\nmultimodal AI assistants for visual applications? Recently, several multi-modal\nmodels have been developed for this purpose. They typically pre-train an\nadaptation module to align the semantics of the vision encoder and language\nmodel, followed by fine-tuning on instruction-following data. However, despite\nthe success of this pipeline in image and language understanding, its\neffectiveness in joint video and language understanding has not been widely\nexplored. In this paper, we aim to develop a novel multi-modal foundation model\ncapable of comprehending video, image, and language within a general framework.\nTo achieve this goal, we introduce Valley, a Video Assistant with Large\nLanguage model Enhanced abilitY. The Valley consists of a LLM, a temporal\nmodeling module, a visual encoder, and a simple projection module designed to\nbridge visual and textual modes. To empower Valley with video comprehension and\ninstruction-following capabilities, we construct a video instruction dataset\nand adopt a two-stage tuning procedure to train it. Specifically, we employ\nChatGPT to facilitate the construction of task-oriented conversation data\nencompassing various tasks, including multi-shot captions, long video\ndescriptions, action recognition, causal relationship inference, etc.\nSubsequently, we adopt a pre-training-then-instructions-tuned pipeline to align\nvisual and textual modalities and improve the instruction-following capability\nof Valley. Qualitative experiments demonstrate that Valley has the potential to\nfunction as a highly effective video assistant that can make complex video\nunderstanding scenarios easy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Ruipu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziwang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Junwei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Da Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pengcheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Linmei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1\">Minghui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Operationalising Representation in Natural Language Processing. (arXiv:2306.08193v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.08193","description":"<p>Despite its centrality in the philosophy of cognitive science, there has been\nlittle prior philosophical work engaging with the notion of representation in\ncontemporary NLP practice. This paper attempts to fill that lacuna: drawing on\nideas from cognitive science, I introduce a framework for evaluating the\nrepresentational claims made about components of neural NLP models, proposing\nthree criteria with which to evaluate whether a component of a model represents\na property and operationalising these criteria using probing classifiers, a\npopular analysis technique in NLP (and deep learning more broadly).\n</p>\n<p>The project of operationalising a philosophically-informed notion of\nrepresentation should be of interest to both philosophers of science and NLP\npractitioners. It affords philosophers a novel testing-ground for claims about\nthe nature of representation, and helps NLPers organise the large literature on\nprobing experiments, suggesting novel avenues for empirical research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harding_J/0/1/0/all/0/1\">Jacqueline Harding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PEACE: Cross-Platform Hate Speech Detection- A Causality-guided Framework. (arXiv:2306.08804v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.08804","description":"<p>Hate speech detection refers to the task of detecting hateful content that\naims at denigrating an individual or a group based on their religion, gender,\nsexual orientation, or other characteristics. Due to the different policies of\nthe platforms, different groups of people express hate in different ways.\nFurthermore, due to the lack of labeled data in some platforms it becomes\nchallenging to build hate speech detection models. To this end, we revisit if\nwe can learn a generalizable hate speech detection model for the cross platform\nsetting, where we train the model on the data from one (source) platform and\ngeneralize the model across multiple (target) platforms. Existing\ngeneralization models rely on linguistic cues or auxiliary information, making\nthem biased towards certain tags or certain kinds of words (e.g., abusive\nwords) on the source platform and thus not applicable to the target platforms.\nInspired by social and psychological theories, we endeavor to explore if there\nexist inherent causal cues that can be leveraged to learn generalizable\nrepresentations for detecting hate speech across these distribution shifts. To\nthis end, we propose a causality-guided framework, PEACE, that identifies and\nleverages two intrinsic causal cues omnipresent in hateful content: the overall\nsentiment and the aggression in the text. We conduct extensive experiments\nacross multiple platforms (representing the distribution shift) showing if\ncausal cues can help cross-platform generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheth_P/0/1/0/all/0/1\">Paras Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumarage_T/0/1/0/all/0/1\">Tharindu Kumarage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moraffah_R/0/1/0/all/0/1\">Raha Moraffah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pushing the Limits of ChatGPT on NLP Tasks. (arXiv:2306.09719v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.09719","description":"<p>Despite the success of ChatGPT, its performances on most NLP tasks are still\nwell below the supervised baselines. In this work, we looked into the causes,\nand discovered that its subpar performance was caused by the following factors:\n(1) token limit in the prompt does not allow for the full utilization of the\nsupervised datasets; (2) mismatch between the generation nature of ChatGPT and\nNLP tasks; (3) intrinsic pitfalls of LLMs models, e.g., hallucination, overly\nfocus on certain keywords, etc.\n</p>\n<p>In this work, we propose a collection of general modules to address these\nissues, in an attempt to push the limits of ChatGPT on NLP tasks. Our proposed\nmodules include (1) a one-input-multiple-prompts strategy that employs multiple\nprompts for one input to accommodate more demonstrations; (2) using fine-tuned\nmodels for better demonstration retrieval; (3) transforming tasks to formats\nthat are more tailored to the generation nature; (4) employing reasoning\nstrategies that are tailored to addressing the task-specific complexity; (5)\nthe self-verification strategy to address the hallucination issue of LLMs; (6)\nthe paraphrase strategy to improve the robustness of model predictions.\n</p>\n<p>We conduct experiments on 21 datasets of 10 representative NLP tasks,\nincluding question answering, commonsense reasoning, natural language\ninference, sentiment analysis, named entity recognition, entity-relation\nextraction, event extraction, dependency parsing, semantic role labeling, and\npart-of-speech tagging. Using the proposed assemble of techniques, we are able\nto significantly boost the performance of ChatGPT on the selected NLP tasks,\nachieving performances comparable to or better than supervised baselines, or\neven existing SOTA performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Linfeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zhen Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1\">Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Counterfactual Data Augmentation Method for Aspect-Based Sentiment Analysis. (arXiv:2306.11260v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.11260","description":"<p>Aspect-based-sentiment-analysis (ABSA) is a fine-grained sentiment evaluation\ntask, which analyzes the emotional polarity of the evaluation aspects.\nGenerally, the emotional polarity of an aspect exists in the corresponding\nopinion expression, whose diversity has great impact on model's performance. To\nmitigate this problem, we propose a novel and simple counterfactual data\naugmentation method to generate opinion expressions with reversed sentiment\npolarity. In particular, the integrated gradients are calculated to locate and\nmask the opinion expression. Then, a prompt combined with the reverse\nexpression polarity is added to the original text, and a Pre-trained language\nmodel (PLM), T5, is finally was employed to predict the masks. The experimental\nresults shows the proposed counterfactual data augmentation method performs\nbetter than current augmentation methods on three ABSA datasets, i.e. Laptop,\nRestaurant, and MAMS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dongming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lulu Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhaoshu Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Art of Embedding Fusion: Optimizing Hate Speech Detection. (arXiv:2306.14939v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.14939","description":"<p>Hate speech detection is a challenging natural language processing task that\nrequires capturing linguistic and contextual nuances. Pre-trained language\nmodels (PLMs) offer rich semantic representations of text that can improve this\ntask. However there is still limited knowledge about ways to effectively\ncombine representations across PLMs and leverage their complementary strengths.\nIn this work, we shed light on various combination techniques for several PLMs\nand comprehensively analyze their effectiveness. Our findings show that\ncombining embeddings leads to slight improvements but at a high computational\ncost and the choice of combination has marginal effect on the final outcome. We\nalso make our codebase public at\nhttps://github.com/aflah02/The-Art-of-Embedding-Fusion-Optimizing-Hate-Speech-Detection .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Aflah Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_N/0/1/0/all/0/1\">Neemesh Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1\">Mohit Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1\">Sanyam Goyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RECAP-KG: Mining Knowledge Graphs from Raw GP Notes for Remote COVID-19 Assessment in Primary Care. (arXiv:2306.17175v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.17175","description":"<p>Clinical decision-making is a fundamental stage in delivering appropriate\ncare to patients. In recent years several decision-making systems designed to\naid the clinician in this process have been developed. However, technical\nsolutions currently in use are based on simple regression models and are only\nable to take into account simple pre-defined multiple-choice features, such as\npatient age, pre-existing conditions, smoker status, etc. One particular source\nof patient data, that available decision-making systems are incapable of\nprocessing is the collection of patient consultation GP notes. These contain\ncrucial signs and symptoms - the information used by clinicians in order to\nmake a final decision and direct the patient to the appropriate care.\nExtracting information from GP notes is a technically challenging problem, as\nthey tend to include abbreviations, typos, and incomplete sentences.\n</p>\n<p>This paper addresses this open challenge. We present a framework that\nperforms knowledge graph construction from raw GP medical notes written during\nor after patient consultations. By relying on support phrases mined from the\nSNOMED ontology, as well as predefined supported facts from values used in the\nRECAP (REmote COVID-19 Assessment in Primary Care) patient risk prediction\ntool, our graph generative framework is able to extract structured knowledge\ngraphs from the highly unstructured and inconsistent format that consultation\nnotes are written in. Our knowledge graphs include information about existing\npatient symptoms, their duration, and their severity.\n</p>\n<p>We apply our framework to consultation notes of COVID-19 patients in the UK\nCOVID-19 Clinical Assesment Servcie (CCAS) patient dataset. We provide a\nquantitative evaluation of the performance of our framework, demonstrating that\nour approach has better accuracy than traditional NLP methods when answering\nquestions about patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mekhtieva_R/0/1/0/all/0/1\">Rakhilya Lee Mekhtieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forbes_B/0/1/0/all/0/1\">Brandon Forbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alrajeh_D/0/1/0/all/0/1\">Dalal Alrajeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delaney_B/0/1/0/all/0/1\">Brendan Delaney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russo_A/0/1/0/all/0/1\">Alessandra Russo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading. (arXiv:2307.00782v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.00782","description":"<p>While state-of-the-art Text-to-Speech systems can generate natural speech of\nvery high quality at sentence level, they still meet great challenges in speech\ngeneration for paragraph / long-form reading. Such deficiencies are due to i)\nignorance of cross-sentence contextual information, and ii) high computation\nand memory cost for long-form synthesis. To address these issues, this work\ndevelops a lightweight yet effective TTS system, ContextSpeech. Specifically,\nwe first design a memory-cached recurrence mechanism to incorporate global text\nand speech context into sentence encoding. Then we construct\nhierarchically-structured textual semantics to broaden the scope for global\ncontext enhancement. Additionally, we integrate linearized self-attention to\nimprove model efficiency. Experiments show that ContextSpeech significantly\nimproves the voice quality and prosody expressiveness in paragraph reading with\ncompetitive model efficiency. Audio samples are available at:\nhttps://contextspeech.github.io/demo/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yujia Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaofei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soong_F/0/1/0/all/0/1\">Frank K. Soong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models. (arXiv:2307.01379v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.01379","description":"<p>While Large Language Models (LLMs) have demonstrated remarkable potential in\nnatural language generation and instruction following, a persistent challenge\nlies in their susceptibility to \"hallucinations\", which erodes trust in their\noutputs. Although Uncertainty Quantification (UQ) presents a promising\nsolution, its accurate implementation within the context of LLMs remains a\nsignificant hurdle. To address this critical roadblock, our research originates\nfrom a fundamental heuristic insight: tokens within auto-regressive\nLLM-generated text do not equally reflect the underlying meaning. Some tokens\ncarry greater relevance and representativeness than others, owing to the\nphenomenon of \"linguistic redundancy\", wherein a select few keywords suffice to\nconvey the essence of lengthy sentences. Regrettably, existing methodologies\ntreat all tokens with equal importance when estimating uncertainty,\ndisregarding these inherent generative inequalities. Our analysis reveals a\nsignificant issue with state-of-the-art: numerous tokens (and sentences) of\nlimited semantic significance receive equal or even excessive weighting during\nuncertainty estimation. To rectify this bias, we propose to jointly Shifting\nAttention to more Relevant (SAR) components, at both the token- and the\nsentence-levels for accurate uncertainty estimation. We conduct extensive\nexperiments involving a range of popular \"off-the-shelf\" LLMs, including\ninstruction-tuned LLMs such as Vicuna, WizardLM, and LLaMA-2-chat, as well as\npretrained LLMs like OPT and LLaMA, with model sizes extending up to 33B\nparameters. We carry out evaluation across various free-form question-answering\ntasks, encompassing domains such as reading comprehension, science Q&amp;A, and\nmedical Q&amp;A. Our experimental results demonstrate the superior performance of\nSAR in addressing the challenges of uncertainty estimation within the realm of\nLLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jinhao Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zavalny_A/0/1/0/all/0/1\">Alex Zavalny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaidi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Divide & Bind Your Attention for Improved Generative Semantic Nursing. (arXiv:2307.10864v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2307.10864","description":"<p>Emerging large-scale text-to-image generative models, e.g., Stable Diffusion\n(SD), have exhibited overwhelming results with high fidelity. Despite the\nmagnificent progress, current state-of-the-art models still struggle to\ngenerate images fully adhering to the input prompt. Prior work, Attend &amp;\nExcite, has introduced the concept of Generative Semantic Nursing (GSN), aiming\nto optimize cross-attention during inference time to better incorporate the\nsemantics. It demonstrates promising results in generating simple prompts,\ne.g., ``a cat and a dog''. However, its efficacy declines when dealing with\nmore complex prompts, and it does not explicitly address the problem of\nimproper attribute binding. To address the challenges posed by complex prompts\nor scenarios involving multiple entities and to achieve improved attribute\nbinding, we propose Divide &amp; Bind. We introduce two novel loss objectives for\nGSN: a novel attendance loss and a binding loss. Our approach stands out in its\nability to faithfully synthesize desired objects with improved attribute\nalignment from complex prompts and exhibits superior performance across\nmultiple evaluation benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yumeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1\">Margret Keuper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khoreva_A/0/1/0/all/0/1\">Anna Khoreva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding. (arXiv:2307.15337v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.15337","description":"<p>This work aims at decreasing the end-to-end generation latency of large\nlanguage models (LLMs). One of the major causes of the high generation latency\nis the sequential decoding approach adopted by almost all state-of-the-art\nLLMs. In this work, motivated by the thinking and writing process of humans, we\npropose Skeleton-of-Thought (SoT), which first guides LLMs to generate the\nskeleton of the answer, and then conducts parallel API calls or batched\ndecoding to complete the contents of each skeleton point in parallel. Not only\ndoes SoT provide considerable speed-ups across 12 LLMs, but it can also\npotentially improve the answer quality on several question categories. SoT is\nan initial attempt at data-centric optimization for inference efficiency, and\nfurther underscores the potential of pushing LLMs to think more like a human\nfor answer quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1\">Xuefei Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zinan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zixuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zifu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huazhong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Private Watermark for Large Language Models. (arXiv:2307.16230v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.16230","description":"<p>Recently, text watermarking algorithms for large language models (LLMs) have\nbeen mitigating the potential harms of text generated by the LLMs, including\nfake news and copyright issues. However, the watermark detection of current\ntext algorithms requires the key from the generation process, making them\nsusceptible to breaches and counterfeiting. In this work, we propose the first\nprivate watermarking algorithm, which extends the current text watermarking\nalgorithms by using two different neural networks respectively for watermark\ngeneration and detection, rather than using the same key at both stages.\nMeanwhile, part of the parameters of the watermark generation and detection\nnetworks are shared, which makes the detection network achieve a high accuracy\nvery efficiently. Experiments show that our algorithm ensures high detection\naccuracy with minimal impact on generation and detection speed, due to the\nsmall parameter size of both networks. Additionally, our subsequent analysis\ndemonstrates the difficulty of reverting the watermark generation rules from\nthe detection network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Leyi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shu&#x27;ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"More Context, Less Distraction: Zero-shot Visual Classification by Inferring and Conditioning on Contextual Attributes. (arXiv:2308.01313v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2308.01313","description":"<p>Vision-language models like CLIP are widely used in zero-shot image\nclassification due to their ability to understand various visual concepts and\nnatural language descriptions. However, how to fully leverage CLIP's\nunprecedented human-like understanding capabilities to achieve better\nperformance is still an open question. This paper draws inspiration from the\nhuman visual perception process: when classifying an object, humans first infer\ncontextual attributes (e.g., background and orientation) which help separate\nthe foreground object from the background, and then classify the object based\non this information. Inspired by it, we observe that providing CLIP with\ncontextual attributes improves zero-shot image classification and mitigates\nreliance on spurious features. We also observe that CLIP itself can reasonably\ninfer the attributes from an image. With these observations, we propose a\ntraining-free, two-step zero-shot classification method PerceptionCLIP. Given\nan image, it first infers contextual attributes (e.g., background) and then\nperforms object classification conditioning on them. Our experiments show that\nPerceptionCLIP achieves better generalization, group robustness, and\ninterpretability. For example, PerceptionCLIP with ViT-L/14 improves the worst\ngroup accuracy by 16.5% on the Waterbirds dataset and by 3.5% on CelebA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1\">Bang An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Sicheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panaitescu_Liess_M/0/1/0/all/0/1\">Michael-Andrei Panaitescu-Liess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mummadi_C/0/1/0/all/0/1\">Chaithanya Kumar Mummadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Furong Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extrapolating Large Language Models to Non-English by Aligning Languages. (arXiv:2308.04948v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.04948","description":"<p>Existing large language models show disparate capability across different\nlanguages, due to the imbalance in the training data. Their performances on\nEnglish tasks are often stronger than on tasks of other languages. In this\npaper, we empower pre-trained LLMs on non-English languages by building\nsemantic alignment across languages. We start from targeting individual\nlanguages by performing cross-lingual instruction-tuning (CoIT) on LLaMA, i.e.\ntuning it with translation task data and cross-lingual general task data to\nobtain cross-lingual models (x-LLaMAs), and formulate underlying scaling laws\nto investigate the advantages of using scalable translation data. Then we\nperform multilingual instruction-tuning (MuIT) with mixed resources to build\nmultilingual m-LLaMA. We also illustrate how we leverage the scaling laws to\noptimize data allocation in a resource-constrained setting. Experiment results\non cross-lingual benchmarks XQUAD and MLQA show that x-LLaMAs surpass the\nEnglish instruction-tuned counterpart (Alpaca) by an average of 27.83% across\nsix non-English languages. Evaluation results on translation dataset Flores-101\nshow that x-LLaMAs outperform previous LLaMA-based models by an average of\n18.89%. Encouragingly, m-LLaMA achieves comparable performance to x-LLaMAs on\nindividual languages and demonstrates the ability to follow multilingual\ninstructions. Further analysis on response content and representation space\nreveals the alignment of the multilingual semantic space within the middle\nlayers of m-LLaMA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenhao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yunzhe Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qingxiu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1\">Fei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.10792","description":"<p>This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), a crucial technique to enhance the capabilities and\ncontrollability of large language models (LLMs). Instruction tuning refers to\nthe process of further training LLMs on a dataset consisting of\n\\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the\ngap between the next-word prediction objective of LLMs and the users' objective\nof having LLMs adhere to human instructions. In this work, we make a systematic\nreview of the literature, including the general methodology of IT, the\nconstruction of IT datasets, the training of IT models, and applications to\ndifferent modalities, domains and applications, along with an analysis on\naspects that influence the outcome of IT (e.g., generation of instruction\noutputs, size of the instruction dataset, etc). We also review the potential\npitfalls of IT along with criticism against it, along with efforts pointing out\ncurrent deficiencies of existing strategies and suggest some avenues for\nfruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Linfeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Runyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. (arXiv:2308.10848v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.10848","description":"<p>Autonomous agents empowered by Large Language Models (LLMs) have undergone\nsignificant improvements, enabling them to generalize across a broad spectrum\nof tasks. However, in real-world scenarios, cooperation among individuals is\noften required to enhance the efficiency and effectiveness of task\naccomplishment. Hence, inspired by human group dynamics, we propose a\nmulti-agent framework \\framework that can collaboratively and dynamically\nadjust its composition as a greater-than-the-sum-of-its-parts system. Our\nexperiments demonstrate that \\framework framework can effectively deploy\nmulti-agent groups that outperform a single agent. Furthermore, we delve into\nthe emergence of social behaviors among individual agents within a group during\ncollaborative task accomplishment. In view of these behaviors, we discuss some\npossible strategies to leverage positive ones and mitigate negative ones for\nimproving the collaborative potential of multi-agent groups. Our codes for\n\\framework will soon be released at\n\\url{https://github.com/OpenBMB/AgentVerse}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weize Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yusheng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_J/0/1/0/all/0/1\">Jingwei Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chenfei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Chi-Min Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Heyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yaxi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_Y/0/1/0/all/0/1\">Yi-Hsin Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1\">Xin Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Ruobing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PlatoLM: Teaching LLMs via a Socratic Questioning User Simulator. (arXiv:2308.11534v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.11534","description":"<p>The unparalleled performance of closed-sourced ChatGPT has sparked efforts\ntowards its democratization, with notable strides made by leveraging real user\nand ChatGPT conversations, as evidenced by Vicuna. However, due to challenges\nin gathering conversations involving human participation, current endeavors\nlike Baize and UltraChat aim to automatically generate conversational data.\nThey primarily rely on ChatGPT conducting roleplay to simulate human behaviors\nbased on instructions rather than genuine learning from humans, resulting in\nlimited scope, diminished diversity, and an absence of genuine multi-round\nconversational dynamics. To address the above issues, we target human questions\nextracted from genuine human-machine conversations as a learning goal and train\na user simulator called `Socratic' to produce a high-quality human-centric\nsynthetic conversation dataset. Subsequently, this dataset was used to train\nour assistant model, named `PlatoLM'. Experimentally, PlatoLM outpaces baseline\nmodels in both Vicuna-Bench and MT-Bench by pairwise comparison when\nconsidering equivalent training set sizes, and manual evaluation also shows\nthat our model is highly competitive. Impressively, when fine-tuned with the\nlatest LLaMA 2 model, PlatoLM achieves the SOTA performance among 7B models\n(including LLaMA-2-7B-chat and Vicuna-7B) in MT-Bench benchmark and in\nAlpaca-Eval benchmark, it ranks second among 7B models, even beating some\nlarger scale models (including LLaMA-2-13B-chat and GPT-3.5). Further in-depth\nanalysis demonstrates the scalability and transferability of our approach. The\ncode is available at https://github.com/FreedomIntelligence/PlatoLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Chuyi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yaxin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1\">Feng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benyou Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpikeBERT: A Language Spikformer Learned from BERT with Knowledge Distillation. (arXiv:2308.15122v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.15122","description":"<p>Spiking neural networks (SNNs) offer a promising avenue to implement deep\nneural networks in a more energy-efficient way. However, the network\narchitectures of existing SNNs for language tasks are still simplistic and\nrelatively shallow, and deep architectures have not been fully explored,\nresulting in a significant performance gap compared to mainstream\ntransformer-based networks such as BERT. To this end, we improve a\nrecently-proposed spiking Transformer (i.e., Spikformer) to make it possible to\nprocess language tasks and propose a two-stage knowledge distillation method\nfor training it, which combines pre-training by distilling knowledge from BERT\nwith a large collection of unlabelled texts and fine-tuning with task-specific\ninstances via knowledge distillation again from the BERT fine-tuned on the same\ntraining examples. Through extensive experimentation, we show that the models\ntrained with our method, named SpikeBERT, outperform state-of-the-art SNNs and\neven achieve comparable results to BERTs on text classification tasks for both\nEnglish and Chinese with much less energy consumption. Our code is available at\nhttps://github.com/Lvchangze/SpikeBERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1\">Changze Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jianhan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_C/0/1/0/all/0/1\">Chenxi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zixuan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cenyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SememeASR: Boosting Performance of End-to-End Speech Recognition against Domain and Long-Tailed Data Shift with Sememe Semantic Knowledge. (arXiv:2309.01437v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2309.01437","description":"<p>Recently, excellent progress has been made in speech recognition. However,\npure data-driven approaches have struggled to solve the problem in\ndomain-mismatch and long-tailed data. Considering that knowledge-driven\napproaches can help data-driven approaches alleviate their flaws, we introduce\nsememe-based semantic knowledge information to speech recognition (SememeASR).\nSememe, according to the linguistic definition, is the minimum semantic unit in\na language and is able to represent the implicit semantic information behind\neach word very well. Our experiments show that the introduction of sememe\ninformation can improve the effectiveness of speech recognition. In addition,\nour further experiments show that sememe knowledge can improve the model's\nrecognition of long-tailed data and enhance the model's domain generalization\nability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiaxu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Changhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wordle: A Microcosm of Life. Luck, Skill, Cheating, Loyalty, and Influence!. (arXiv:2309.02110v3 [math.HO] UPDATED)","link":"http://arxiv.org/abs/2309.02110","description":"<p>Wordle is a popular, online word game offered by the New York Times\n(nytimes.com). Currently there are some 2 million players of the English\nversion worldwide. Players have 6 attempts to guess the daily word (target\nword) and after each attempt, the player receives color-coded information about\nthe correctness and position of each letter in the guess. After either a\nsuccessful completion of the puzzle or the final unsuccessful attempt, software\ncan assess the player's luck and skill using Information Theory and can display\ndata for the first, second, ..., sixth guesses of a random sample of all\nplayers. Recently, I discovered that the latter data is presented in a format\nthat can easily be copied and pasted into a spreadsheet. I compiled data on\nWordle players' first guesses from May 2023 - August 2023 and inferred some\ninteresting information about Wordle players. A) Every day, about 0.2-0.5% of\nplayers solve the puzzle in one attempt. Because the odds of guessing the one\nof 2,315 possible target words at random is 0.043%, this implies that 4,000 -\n10,000 players cheat by obtaining the target word outside of playing the game!\nB) At least 1/3 of the players have a favorite starting word, or cycle through\nseveral. And even though players should be aware that target words are never\nrepeated, most players appear to remain loyal to their starting word even after\nits appearance as a target word. C) On August 15, 2023, about 30,000 players\nabruptly changed their starting word, presumably based on a crossword puzzle\nclue! Wordle players can be influenced! This study goes beyond social media\npostings, surveys, and Google Trends to provide solid, quantitative evidence\nabout cheating in Wordle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Dilger_J/0/1/0/all/0/1\">James P. Dilger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-Only Domain Adaptation for End-to-End Speech Recognition through Down-Sampling Acoustic Representation. (arXiv:2309.02459v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2309.02459","description":"<p>Mapping two modalities, speech and text, into a shared representation space,\nis a research topic of using text-only data to improve end-to-end automatic\nspeech recognition (ASR) performance in new domains. However, the length of\nspeech representation and text representation is inconsistent. Although the\nprevious method up-samples the text representation to align with acoustic\nmodality, it may not match the expected actual duration. In this paper, we\nproposed novel representations match strategy through down-sampling acoustic\nrepresentation to align with text modality. By introducing a continuous\nintegrate-and-fire (CIF) module generating acoustic representations consistent\nwith token length, our ASR model can learn unified representations from both\nmodalities better, allowing for domain adaptation using text-only data of the\ntarget domain. Experiment results of new domain data demonstrate the\neffectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiaxu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_W/0/1/0/all/0/1\">Weinan Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yaoxun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Changhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1\">Zhao You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Resource Hallucination Prevention for Large Language Models. (arXiv:2309.02654v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.02654","description":"<p>The prevalent use of large language models (LLMs) in various domains has\ndrawn attention to the issue of \"hallucination,\" which refers to instances\nwhere LLMs generate factually inaccurate or ungrounded information. Existing\ntechniques for hallucination detection in language assistants rely on intricate\nfuzzy, specific free-language-based chain of thought (CoT) techniques or\nparameter-based methods that suffer from interpretability issues. Additionally,\nthe methods that identify hallucinations post-generation could not prevent\ntheir occurrence and suffer from inconsistent performance due to the influence\nof the instruction format and model style. In this paper, we introduce a novel\npre-detection self-evaluation technique, referred to as SELF-FAMILIARITY, which\nfocuses on evaluating the model's familiarity with the concepts present in the\ninput instruction and withholding the generation of response in case of\nunfamiliar concepts. This approach emulates the human ability to refrain from\nresponding to unfamiliar topics, thus reducing hallucinations. We validate\nSELF-FAMILIARITY across four different large language models, demonstrating\nconsistently superior performance compared to existing techniques. Our findings\npropose a significant shift towards preemptive strategies for hallucination\nmitigation in LLM assistants, promising improvements in reliability,\napplicability, and interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Junyu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Cao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fenglong Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta predictive learning model of languages in neural circuits. (arXiv:2309.04106v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.04106","description":"<p>Large language models based on self-attention mechanisms have achieved\nastonishing performances not only in natural language itself, but also in a\nvariety of tasks of different nature. However, regarding processing language,\nour human brain may not operate using the same principle. Then, a debate is\nestablished on the connection between brain computation and artificial\nself-supervision adopted in large language models. One of most influential\nhypothesis in brain computation is the predictive coding framework, which\nproposes to minimize the prediction error by local learning. However, the role\nof predictive coding and the associated credit assignment in language\nprocessing remains unknown. Here, we propose a mean-field learning model within\nthe predictive coding framework, assuming that the synaptic weight of each\nconnection follows a spike and slab distribution, and only the distribution,\nrather than specific weights, is trained. This meta predictive learning is\nsuccessfully validated on classifying handwritten digits where pixels are input\nto the network in sequence, and moreover on the toy and real language corpus.\nOur model reveals that most of the connections become deterministic after\nlearning, while the output connections have a higher level of variability. The\nperformance of the resulting network ensemble changes continuously with data\nload, further improving with more training data, in analogy with the emergent\nbehavior of large language models. Therefore, our model provides a starting\npoint to investigate the connection among brain computation, next-token\nprediction and general intelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Junbin Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haiping Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAIN: Your Language Models Can Align Themselves without Finetuning. (arXiv:2309.07124v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.07124","description":"<p>Large language models (LLMs) often demonstrate inconsistencies with human\npreferences. Previous research typically gathered human preference data and\nthen aligned the pre-trained models using reinforcement learning or instruction\ntuning, a.k.a. the finetuning step. In contrast, aligning frozen LLMs without\nrequiring alignment data is more appealing. This work explores the potential of\nthe latter setting. We discover that by integrating self-evaluation and rewind\nmechanisms, unaligned LLMs can directly produce responses consistent with human\npreferences via self-boosting. We introduce a novel inference method,\nRewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to\nevaluate their own generation and use the evaluation results to guide rewind\nand generation for AI safety. Notably, RAIN operates without the need of extra\ndata for model alignment and abstains from any training, gradient computation,\nor parameter updates. Experimental results evaluated by GPT-4 and humans\ndemonstrate the effectiveness of RAIN: on the HH dataset, RAIN improves the\nharmlessness rate of LLaMA 30B from 82% of vanilla inference to 97%, while\nmaintaining the helpfulness rate. On the TruthfulQA dataset, RAIN improves the\ntruthfulness of the already-well-aligned LLaMA-2-chat 13B model by 5%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Fangyun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinjing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongyang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Agents: An Open-source Framework for Autonomous Language Agents. (arXiv:2309.07870v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.07870","description":"<p>Recent advances on large language models (LLMs) enable researchers and\ndevelopers to build autonomous language agents that can automatically solve\nvarious tasks and interact with environments, humans, and other agents using\nnatural language interfaces. We consider language agents as a promising\ndirection towards artificial general intelligence and release Agents, an\nopen-source library with the goal of opening up these advances to a wider\nnon-specialist audience. Agents is carefully engineered to support important\nfeatures including planning, memory, tool usage, multi-agent communication, and\nfine-grained symbolic control. Agents is user-friendly as it enables\nnon-specialists to build, customize, test, tune, and deploy state-of-the-art\nautonomous language agents without much coding. The library is also\nresearch-friendly as its modularized design makes it easily extensible for\nresearchers. Agents is available at https://github.com/aiwaves-cn/agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuchen Eleanor Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Long Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jialong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tiannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jintian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ruipu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shiding Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wentao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. (arXiv:2309.12284v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.12284","description":"<p>Large language models (LLMs) have pushed the limits of natural language\nunderstanding and exhibited excellent problem-solving ability. Despite the\ngreat success, most existing open-source LLMs (e.g., LLaMA-2) are still far\naway from satisfactory for solving mathematical problem due to the complex\nreasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned\nlanguage model that specializes in mathematical reasoning. Specifically, we\nstart by bootstrapping mathematical questions by rewriting the question from\nmultiple perspectives without extra knowledge, which results in a new dataset\ncalled MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA.\nExperimental results on two popular benchmarks (i.e., GSM8K and MATH) for\nmathematical reasoning demonstrate that MetaMath outperforms a suite of\nopen-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4%\non GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same\nsize by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of\n82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the\nMetaMathQA dataset, the MetaMath models with different model sizes and the\ntraining code for public use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Longhui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Weisen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Han Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jincheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwok_J/0/1/0/all/0/1\">James T. Kwok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing the Moral Development of Large Language Models through Defining Issues Test. (arXiv:2309.13356v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.13356","description":"<p>In this study, we measure the moral reasoning ability of LLMs using the\nDefining Issues Test - a psychometric instrument developed for measuring the\nmoral development stage of a person according to the Kohlberg's Cognitive Moral\nDevelopment Model. DIT uses moral dilemmas followed by a set of ethical\nconsiderations that the respondent has to judge for importance in resolving the\ndilemma, and then rank-order them by importance. A moral development stage\nscore of the respondent is then computed based on the relevance rating and\nranking.\n</p>\n<p>Our study shows that early LLMs such as GPT-3 exhibit a moral reasoning\nability no better than that of a random baseline, while ChatGPT, Llama2-Chat,\nPaLM-2 and GPT-4 show significantly better performance on this task, comparable\nto adult humans. GPT-4, in fact, has the highest post-conventional moral\nreasoning score, equivalent to that of typical graduate school students.\nHowever, we also observe that the models do not perform consistently across all\ndilemmas, pointing to important gaps in their understanding and reasoning\nabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tanmay_K/0/1/0/all/0/1\">Kumar Tanmay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1\">Aditi Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_U/0/1/0/all/0/1\">Utkarsh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models. (arXiv:2309.14717v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2309.14717","description":"<p>Recently years have witnessed a rapid development of large language models\n(LLMs). Despite the strong ability in many language-understanding tasks, the\nheavy computational burden largely restricts the application of LLMs especially\nwhen one needs to deploy them onto edge devices. In this paper, we propose a\nquantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies\nin the imbalanced degrees of freedom of quantization and adaptation, and the\nsolution is to use group-wise operators which increase the degree of freedom of\nquantization meanwhile decreasing that of adaptation. QA-LoRA is easily\nimplemented with a few lines of code, and it equips the original LoRA with\ntwo-fold abilities: (i) during fine-tuning, the LLM's weights are quantized\n(e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the\nLLM and auxiliary weights are naturally integrated into a quantized model\nwithout loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model\nfamilies and validate its effectiveness in different fine-tuning datasets and\ndownstream scenarios. Code will be made available at\nhttps://github.com/yuhuixu1993/qa-lora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuhui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiaotao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hengheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengsu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLPBench: Evaluating Large Language Models on Solving NLP Problems. (arXiv:2309.15630v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.15630","description":"<p>Recent developments in large language models (LLMs) have shown promise in\nenhancing the capabilities of natural language processing (NLP). Despite these\nsuccesses, there remains a dearth of research dedicated to the NLP\nproblem-solving abilities of LLMs. To fill the gap in this area, we present a\nunique benchmarking dataset, NLPBench, comprising 378 college-level NLP\nquestions spanning various NLP topics sourced from Yale University's prior\nfinal exams. NLPBench includes questions with context, in which multiple\nsub-questions share the same public information, and diverse question types,\nincluding multiple choice, short answer, and math. Our evaluation, centered on\nLLMs such as GPT-3.5/4, PaLM-2, and LLAMA-2, incorporates advanced prompting\nstrategies like the chain-of-thought (CoT) and tree-of-thought (ToT). Our study\nreveals that the effectiveness of the advanced prompting strategies can be\ninconsistent, occasionally damaging LLM performance, especially in smaller\nmodels like the LLAMA-2 (13b). Furthermore, our manual assessment illuminated\nspecific shortcomings in LLMs' scientific problem-solving skills, with\nweaknesses in logical decomposition and reasoning notably affecting results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linxin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jieyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lechao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pengyuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lyra: Orchestrating Dual Correction in Automated Theorem Proving. (arXiv:2309.15806v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.15806","description":"<p>Large Language Models (LLMs) present an intriguing avenue for exploration in\nthe field of formal theorem proving. Nevertheless, their full potential,\nparticularly concerning the mitigation of hallucinations and refinement through\nprover error messages, remains an area that has yet to be thoroughly\ninvestigated. To enhance the effectiveness of LLMs in the field, we introduce\nthe Lyra, a new framework that employs two distinct correction mechanisms: Tool\nCorrection (TC) and Conjecture Correction (CC). To implement Tool Correction in\nthe post-processing of formal proofs, we leverage prior knowledge to utilize\npredefined prover tools (e.g., Sledgehammer) for guiding the replacement of\nincorrect tools. Tool Correction significantly contributes to mitigating\nhallucinations, thereby improving the overall accuracy of the proof. In\naddition, we introduce Conjecture Correction, an error feedback mechanism\ndesigned to interact with prover to refine formal proof conjectures with prover\nerror messages. Compared to the previous refinement framework, the proposed\nConjecture Correction refines generation with instruction but does not collect\npaired (generation, error &amp; refinement) prompts. Our method has achieved\nstate-of-the-art (SOTA) performance on both miniF2F validation (48.0% -&gt; 55.3%)\nand test (45.5% -&gt; 51.2%). We also present 3 IMO problems solved by Lyra. We\nbelieve Tool Correction (post-process for hallucination mitigation) and\nConjecture Correction (subgoal adjustment from interaction with environment)\ncould provide a promising avenue for future research in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chuanyang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiankai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_H/0/1/0/all/0/1\">Huajian Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianhao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Image-text Multimodal Models. (arXiv:2309.15857v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.15857","description":"<p>Amidst the evolving landscape of artificial intelligence, the convergence of\nvisual and textual information has surfaced as a crucial frontier, leading to\nthe advent of image-text multimodal models. This paper provides a comprehensive\nreview of the evolution and current state of image-text multimodal models,\nexploring their application value, challenges, and potential research\ntrajectories. Initially, we revisit the basic concepts and developmental\nmilestones of these models, introducing a novel classification that segments\ntheir evolution into three distinct phases, based on their time of introduction\nand subsequent impact on the discipline. Furthermore, based on the tasks'\nsignificance and prevalence in the academic landscape, we propose a\ncategorization of the tasks associated with image-text multimodal models into\nfive major types, elucidating the recent progress and key technologies within\neach category. Despite the remarkable accomplishments of these models, numerous\nchallenges and issues persist. This paper delves into the inherent challenges\nand limitations of image-text multimodal models, fostering the exploration of\nprospective research directions. Our objective is to offer an exhaustive\noverview of the present research landscape of image-text multimodal models and\nto serve as a valuable reference for future scholarly endeavors. We extend an\ninvitation to the broader community to collaborate in enhancing the image-text\nmultimodal model community, accessible at:\n\\href{https://github.com/i2vec/A-survey-on-image-text-multimodal-models}{https://github.com/i2vec/A-survey-on-image-text-multimodal-models}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Ruifeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jingxuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Linzhuang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bihui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_G/0/1/0/all/0/1\">Guiyong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sibo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhengbing Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingjun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_L/0/1/0/all/0/1\">Liping Bu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment. (arXiv:2310.00212v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2310.00212","description":"<p>Large Language Models (LLMs) can acquire extensive world knowledge through\npre-training on large corpora. However, due to exposure to low-quality data,\nLLMs may exhibit harmful behavior without aligning with human values. The\ndominant approach for steering LLMs towards beneficial behavior involves\nReinforcement Learning with Human Feedback (RLHF), with Proximal Policy\nOptimization (PPO) serving as the default RL optimizer. Despite its\neffectiveness, PPO has limitations when optimizing rewards trained from\ncomparison-based loss. Primarily, PPO is not invariant to equivalent reward\nfunctions containing identical preference information due to the need to\ncalibrate the reward scale. Additionally, PPO's necessity for token-wise\nupdates introduces complexity in both function approximation and algorithm\ndesign compared to trajectory-wise optimization. This paper proposes a new\nframework, reinforcement learning with relative feedback, and a novel\ntrajectory-wise policy gradient algorithm, Pairwise Proximal Policy\nOptimization (P3O) that operates directly on comparative rewards. We show\ntheoretically that P3O is invariant to equivalent rewards and avoids the\ncomplexity of PPO. Empirical evaluations demonstrate that P3O outperforms PPO\nin the KL-Reward trade-off and can align with human preferences as well as or\nbetter than prior methods. In summary, this work introduces a simpler yet\neffective approach for aligning LLMs to human preferences through relative\nfeedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Banghua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruoyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zhaojin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramchandran_K/0/1/0/all/0/1\">Kannan Ramchandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jiantao Jiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RelBERT: Embedding Relations with Language Models. (arXiv:2310.00299v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.00299","description":"<p>Many applications need access to background knowledge about how different\nconcepts and entities are related. Although Knowledge Graphs (KG) and Large\nLanguage Models (LLM) can address this need to some extent, KGs are inevitably\nincomplete and their relational schema is often too coarse-grained, while LLMs\nare inefficient and difficult to control. As an alternative, we propose to\nextract relation embeddings from relatively small language models. In\nparticular, we show that masked language models such as RoBERTa can be\nstraightforwardly fine-tuned for this purpose, using only a small amount of\ntraining data. The resulting model, which we call RelBERT, captures relational\nsimilarity in a surprisingly fine-grained way, allowing us to set a new\nstate-of-the-art in analogy benchmarks. Crucially, RelBERT is capable of\nmodelling relations that go well beyond what the model has seen during\ntraining. For instance, we obtained strong results on relations between named\nentities with a model that was only trained on lexical relations between\nconcepts, and we observed that RelBERT can recognise morphological analogies\ndespite not being trained on such examples. Overall, we find that RelBERT\nsignificantly outperforms strategies based on prompting language models that\nare several orders of magnitude larger, including recent GPT-based models and\nopen source models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ushio_A/0/1/0/all/0/1\">Asahi Ushio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1\">Steven Schockaert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Value Understanding in Language Models through Discriminator-Critique Gap. (arXiv:2310.00378v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.00378","description":"<p>Recent advancements in Large Language Models (LLMs) have heightened concerns\nabout their potential misalignment with human values. However, evaluating their\ngrasp of these values is complex due to their intricate and adaptable nature.\nWe argue that truly understanding values in LLMs requires considering both\n\"know what\" and \"know why\". To this end, we present the Value Understanding\nMeasurement (VUM) framework that quantitatively assess both \"know what\" and\n\"know why\" by measuring the discriminator-critique gap related to human values.\nUsing the Schwartz Value Survey, we specify our evaluation values and develop a\nthousand-level dialogue dataset with GPT-4. Our assessment looks at both the\nvalue alignment of LLM's outputs compared to baseline answers and how LLM\nresponses align with reasons for value recognition versus GPT-4's annotations.\nWe evaluate five representative LLMs and provide strong evidence that the\nscaling law significantly impacts \"know what\" but not much on \"know why\", which\nhas consistently maintained a high level. This may further suggest that LLMs\nmight craft plausible explanations based on the provided context without truly\nunderstanding their inherent value, indicating potential risks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaowei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_F/0/1/0/all/0/1\">Fengshuo Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaodong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SELF: Language-Driven Self-Evolution for Large Language Model. (arXiv:2310.00533v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.00533","description":"<p>Large Language Models (LLMs) have showcased remarkable versatility across\ndiverse domains. However, the pathway toward autonomous model development, a\ncornerstone for achieving human-level learning and advancing autonomous AI,\nremains largely uncharted. We introduce an innovative approach, termed \"SELF\"\n(Self-Evolution with Language Feedback). This methodology empowers LLMs to\nundergo continual self-evolution. Furthermore, SELF employs language-based\nfeedback as a versatile and comprehensive evaluative tool, pinpointing areas\nfor response refinement and bolstering the stability of self-evolutionary\ntraining. Initiating with meta-skill learning, SELF acquires foundational\nmeta-skills with a focus on self-feedback and self-refinement. These\nmeta-skills are critical, guiding the model's subsequent self-evolution through\na cycle of perpetual training with self-curated data, thereby enhancing its\nintrinsic abilities. Given unlabeled instructions, SELF equips the model with\nthe capability to autonomously generate and interactively refine responses.\nThis synthesized training data is subsequently filtered and utilized for\niterative fine-tuning, enhancing the model's capabilities. Experimental results\non representative benchmarks substantiate that SELF can progressively advance\nits inherent abilities without the requirement of human intervention, thereby\nindicating a viable pathway for autonomous model evolution. Additionally, SELF\ncan employ online self-refinement strategy to produce responses of superior\nquality. In essence, the SELF framework signifies a progressive step towards\nautonomous LLM development, transforming the LLM from a mere passive recipient\nof information into an active participant in its own evolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jianqiao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wanjun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenyong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baojun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GeRA: Label-Efficient Geometrically Regularized Alignment. (arXiv:2310.00672v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2310.00672","description":"<p>Pretrained unimodal encoders incorporate rich semantic information into\nembedding space structures. To be similarly informative, multi-modal encoders\ntypically require massive amounts of paired data for alignment and training. We\nintroduce a semi-supervised Geometrically Regularized Alignment (GeRA) method\nto align the embedding spaces of pretrained unimodal encoders in a\nlabel-efficient way. Our method leverages the manifold geometry of unpaired\n(unlabeled) data to improve alignment performance. To prevent distortions to\nlocal geometry during the alignment process, potentially disrupting semantic\nneighborhood structures and causing misalignment of unobserved pairs, we\nintroduce a geometric loss term. This term is built upon a diffusion operator\nthat captures the local manifold geometry of the unimodal pretrained encoders.\nGeRA is modality-agnostic and thus can be used to align pretrained encoders\nfrom any data modalities. We provide empirical evidence to the effectiveness of\nour method in the domains of speech-text and image-text alignment. Our\nexperiments demonstrate significant improvement in alignment quality compared\nto a variaty of leading baselines, especially with a small amount of paired\ndata, using our proposed geometric regularization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klebe_D/0/1/0/all/0/1\">Dustin Klebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shnitzer_T/0/1/0/all/0/1\">Tal Shnitzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yurochkin_M/0/1/0/all/0/1\">Mikhail Yurochkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1\">Leonid Karlinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solomon_J/0/1/0/all/0/1\">Justin Solomon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back to the Future: Towards Explainable Temporal Reasoning with Large Language Models. (arXiv:2310.01074v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.01074","description":"<p>Temporal reasoning is a crucial NLP task, providing a nuanced understanding\nof time-sensitive contexts within textual data. Although recent advancements in\nLLMs have demonstrated their potential in temporal reasoning, the predominant\nfocus has been on tasks such as temporal expression and temporal relation\nextraction. These tasks are primarily designed for the extraction of direct and\npast temporal cues and to engage in simple reasoning processes. A significant\ngap remains when considering complex reasoning tasks such as event forecasting,\nwhich requires multi-step temporal reasoning on events and prediction on the\nfuture timestamp. Another notable limitation of existing methods is their\nincapability to provide an illustration of their reasoning process, hindering\nexplainability. In this paper, we introduce the first task of explainable\ntemporal reasoning, to predict an event's occurrence at a future timestamp\nbased on context which requires multiple reasoning over multiple events, and\nsubsequently provide a clear explanation for their prediction. Our task offers\na comprehensive evaluation of both the LLMs' complex temporal reasoning\nability, the future event prediction ability, and explainability-a critical\nattribute for AI applications. To support this task, we present the first\nmulti-source instruction-tuning dataset of explainable temporal reasoning\n(ExpTime) with 26k derived from the temporal knowledge graph datasets and their\ntemporal reasoning paths, using a novel knowledge-graph-instructed-generation\nstrategy. Based on the dataset, we propose the first open-source LLM series\nTimeLlaMA based on the foundation LlaMA2, with the ability of instruction\nfollowing for explainable temporal reasoning. We compare the performance of our\nmethod and a variety of LLMs, where our method achieves the state-of-the-art\nperformance of temporal prediction and explanation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chenhan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jimin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1\">Sophia Ananiadou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RA-DIT: Retrieval-Augmented Dual Instruction Tuning. (arXiv:2310.01352v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.01352","description":"<p>Retrieval-augmented language models (RALMs) improve performance by accessing\nlong-tail and up-to-date knowledge from external data stores, but are\nchallenging to build. Existing approaches require either expensive\nretrieval-specific modifications to LM pre-training or use post-hoc integration\nof the data store that leads to suboptimal performance. We introduce\nRetrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning\nmethodology that provides a third option by retrofitting any LLM with retrieval\ncapabilities. Our approach operates in two distinct fine-tuning steps: (1) one\nupdates a pre-trained LM to better use retrieved information, while (2) the\nother updates the retriever to return more relevant results, as preferred by\nthe LM. By fine-tuning over tasks that require both knowledge utilization and\ncontextual awareness, we demonstrate that each stage yields significant\nperformance improvements, and using both leads to additional gains. Our best\nmodel, RA-DIT 65B, achieves state-of-the-art performance across a range of\nknowledge-intensive zero- and few-shot learning benchmarks, significantly\noutperforming existing in-context RALM approaches by up to +8.9% in 0-shot\nsetting and +1.4% in 5-shot setting on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xi Victoria Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingda Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weijia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomeli_M/0/1/0/all/0/1\">Maria Lomeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_R/0/1/0/all/0/1\">Rich James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1\">Pedro Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahn_J/0/1/0/all/0/1\">Jacob Kahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szilvasy_G/0/1/0/all/0/1\">Gergely Szilvasy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_S/0/1/0/all/0/1\">Scott Yih</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Split and Merge: Aligning Position Biases in Large Language Model based Evaluators. (arXiv:2310.01432v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.01432","description":"<p>Large language models (LLMs) have shown promise as automated evaluators for\nassessing the quality of answers generated by AI systems. However, these\nLLM-based evaluators exhibit position bias, or inconsistency, when used to\nevaluate candidate answers in pairwise comparisons, favoring either the first\nor second answer regardless of content. To address this limitation, we propose\nPORTIA, an alignment-based system designed to mimic human comparison strategies\nto calibrate position bias in a lightweight yet effective manner. Specifically,\nPORTIA splits the answers into multiple segments, aligns similar content across\ncandidate answers, and then merges them back into a single prompt for\nevaluation by LLMs. We conducted extensive experiments with six diverse LLMs to\nevaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances\nthe consistency rates for all the models and comparison forms tested, achieving\nan average relative improvement of 47.46%. Remarkably, PORTIA enables less\nadvanced GPT models to achieve 88% agreement with the state-of-the-art GPT-4\nmodel at just 10% of the cost. Furthermore, it rectifies around 80% of the\nposition bias instances within the GPT-4 model, elevating its consistency rate\nup to 98%. Subsequent human evaluations indicate that the PORTIA-enhanced\nGPT-3.5 model can even surpass the standalone GPT-4 in terms of alignment with\nhuman evaluators. These findings highlight PORTIA's ability to correct position\nbias, improve LLM consistency, and boost performance while keeping\ncost-efficiency. This represents a valuable step toward a more reliable and\nscalable use of LLMs for automated evaluations across diverse applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zongjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaozheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1\">Pingchuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Daoyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Cuiyun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs. (arXiv:2310.01801v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.01801","description":"<p>In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Suyu Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minjia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extraction of Medication and Temporal Relation from Clinical Text using Neural Language Models. (arXiv:2310.02229v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.02229","description":"<p>Clinical texts, represented in electronic medical records (EMRs), contain\nrich medical information and are essential for disease prediction, personalised\ninformation recommendation, clinical decision support, and medication pattern\nmining and measurement. Relation extractions between medication mentions and\ntemporal information can further help clinicians better understand the\npatients' treatment history. To evaluate the performances of deep learning (DL)\nand large language models (LLMs) in medication extraction and temporal\nrelations classification, we carry out an empirical investigation of\n\\textbf{MedTem} project using several advanced learning structures including\nBiLSTM-CRF and CNN-BiLSTM for a clinical domain named entity recognition (NER),\nand BERT-CNN for temporal relation extraction (RE), in addition to the\nexploration of different word embedding techniques. Furthermore, we also\ndesigned a set of post-processing roles to generate structured output on\nmedications and the temporal relation. Our experiments show that CNN-BiLSTM\nslightly wins the BiLSTM-CRF model on the i2b2-2009 clinical NER task yielding\n75.67, 77.83, and 78.17 for precision, recall, and F1 scores using Macro\nAverage. BERT-CNN model also produced reasonable evaluation scores 64.48,\n67.17, and 65.03 for P/R/F1 using Macro Avg on the temporal relation extraction\ntest set from i2b2-2012 challenges. Code and Tools from MedTem will be hosted\nat \\url{https://github.com/HECTA-UoM/MedTem}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_H/0/1/0/all/0/1\">Hangyu Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1\">Goran Nenadic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning. (arXiv:2310.03094v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.03094","description":"<p>Large language models (LLMs) such as GPT-4 have exhibited remarkable\nperformance in a variety of tasks, but this strong performance often comes with\nthe high expense of using paid API services. In this paper, we are motivated to\nstudy building an LLM cascade to save the cost of using LLMs, particularly for\nperforming reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline\nfollows the intuition that simpler questions can be addressed by a weaker but\nmore affordable LLM, whereas only the challenging questions necessitate the\nstronger and more expensive LLM. To realize this decision-making, we consider\nthe \"answer consistency\" of the weaker LLM as a signal of the question\ndifficulty and propose several methods for the answer sampling and consistency\nchecking, including one leveraging a mixture of two thought representations\n(i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six\nreasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and\nstronger LLMs, respectively, we demonstrate that our proposed LLM cascades can\nachieve performance comparable to using solely the stronger LLM but require\nonly 40% of its cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_M/0/1/0/all/0/1\">Murong Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Liang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Ziyu Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Self-Supervised Speech Representations for Indigenous American Languages. (arXiv:2310.03639v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.03639","description":"<p>The application of self-supervision to speech representation learning has\ngarnered significant interest in recent years, due to its scalability to large\namounts of unlabeled data. However, much progress, both in terms of\npre-training and downstream evaluation, has remained concentrated in\nmonolingual models that only consider English. Few models consider other\nlanguages, and even fewer consider indigenous ones. In our submission to the\nNew Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASR\ncorpus for Quechua, an indigenous South American Language. We benchmark the\nefficacy of large SSL models on Quechua, along with 6 other indigenous\nlanguages such as Guarani and Bribri, on low-resource ASR. Our results show\nsurprisingly strong performance by state-of-the-art SSL models, showing the\npotential generalizability of large-scale models to real-world data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chih-Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">William Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zevallos_R/0/1/0/all/0/1\">Rodolfo Zevallos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_J/0/1/0/all/0/1\">John E. Ortega</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models. (arXiv:2310.03965v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2310.03965","description":"<p>Large Language Models (LLMs) have achieved remarkable success in reasoning\ntasks with the development of prompting methods. However, existing prompting\napproaches cannot reuse insights of solving similar problems and suffer from\naccumulated errors in multi-step reasoning, since they prompt LLMs to reason\n\\textit{from scratch}. To address these issues, we propose\n\\textbf{\\textit{Thought Propagation} (TP)}, which explores the analogous\nproblems and leverages their solutions to enhance the complex reasoning ability\nof LLMs. These analogous problems are related to the input one, with reusable\nsolutions and problem-solving strategies. Thus, it is promising to propagate\ninsights of solving previous analogous problems to inspire new problem-solving.\nTo achieve this, TP first prompts LLMs to propose and solve a set of analogous\nproblems that are related to the input one. Then, TP reuses the results of\nanalogous problems to directly yield a new solution or derive a\nknowledge-intensive plan for execution to amend the initial solution obtained\nfrom scratch. TP is compatible with existing prompting approaches, allowing\nplug-and-play generalization and enhancement in a wide range of tasks without\nmuch labor in task-specific prompt engineering. Experiments across three\nchallenging tasks demonstrate TP enjoys a substantial improvement over the\nbaselines by an average of 12\\% absolute increase in finding the optimal\nsolutions in Shortest-path Reasoning, 13\\% improvement of human preference in\nCreative Writing, and 15\\% enhancement in the task completion rate of LLM-Agent\nPlanning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Junchi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ran He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1\">Rex Ying</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Can Be Good Privacy Protection Learners. (arXiv:2310.02469v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2310.02469","description":"<p>The proliferation of Large Language Models (LLMs) has driven considerable\ninterest in fine-tuning them with domain-specific data to create specialized\nlanguage models. Nevertheless, such domain-specific fine-tuning data often\ncontains sensitive personally identifiable information (PII). Direct\nfine-tuning LLMs on this data without privacy protection poses a risk of\nleakage. To address this challenge, we introduce Privacy Protection Language\nModels (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects\ndomain-specific knowledge while safeguarding data privacy. Our work offers a\ntheoretical analysis for model design and delves into various techniques such\nas corpus curation, penalty-based unlikelihood in training loss, and\ninstruction-based tuning, etc. Extensive experiments across diverse datasets\nand scenarios demonstrate the effectiveness of our approaches. In particular,\ninstruction tuning with both positive and negative examples, stands out as a\npromising method, effectively protecting private data while enhancing the\nmodel's knowledge. Our work underscores the potential for Large Language Models\nas robust privacy protection learners.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yijia Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yiqiao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yushi Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xianjun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenchao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xujiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wei Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-10-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}