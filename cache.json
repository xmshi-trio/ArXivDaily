{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-11-02T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?. (arXiv:2311.00047v1 [cs.AI])","link":"http://arxiv.org/abs/2311.00047","description":"<p>Vision-Language Models (VLMs) are trained on vast amounts of data captured by\nhumans emulating our understanding of the world. However, known as visual\nillusions, human's perception of reality isn't always faithful to the physical\nworld. This raises a key question: do VLMs have the similar kind of illusions\nas humans do, or do they faithfully learn to represent reality? To investigate\nthis question, we build a dataset containing five types of visual illusions and\nformulate four tasks to examine visual illusions in state-of-the-art VLMs. Our\nfindings have shown that although the overall alignment is low, larger models\nare closer to human perception and more susceptible to visual illusions. Our\ndataset and initial findings will promote a better understanding of visual\nillusions in humans and machines and provide a stepping stone for future\ncomputational models that can better align humans and machines in perceiving\nand communicating about the shared visual world. The code and data are\navailable at https://github.com/vl-illusion/dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jiayi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuchen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_R/0/1/0/all/0/1\">Rui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Generative AI Paradox: \"What It Can Create, It May Not Understand\". (arXiv:2311.00059v1 [cs.AI])","link":"http://arxiv.org/abs/2311.00059","description":"<p>The recent wave of generative AI has sparked unprecedented global attention,\nwith both excitement and concern over potentially superhuman levels of\nartificial intelligence: models now take only seconds to produce outputs that\nwould challenge or exceed the capabilities even of expert humans. At the same\ntime, models still show basic errors in understanding that would not be\nexpected even in non-expert humans. This presents us with an apparent paradox:\nhow do we reconcile seemingly superhuman capabilities with the persistence of\nerrors that few humans would make? In this work, we posit that this tension\nreflects a divergence in the configuration of intelligence in today's\ngenerative models relative to intelligence in humans. Specifically, we propose\nand test the Generative AI Paradox hypothesis: generative models, having been\ntrained directly to reproduce expert-like outputs, acquire generative\ncapabilities that are not contingent upon -- and can therefore exceed -- their\nability to understand those same types of outputs. This contrasts with humans,\nfor whom basic understanding almost always precedes the ability to generate\nexpert-level outputs. We test this hypothesis through controlled experiments\nanalyzing generation vs. understanding in generative models, across both\nlanguage and image modalities. Our results show that although models can\noutperform humans in generation, they consistently fall short of human\ncapabilities in measures of understanding, as well as weaker correlation\nbetween generation and understanding performance, and more brittleness to\nadversarial inputs. Our findings support the hypothesis that models' generative\ncapability may not be contingent upon understanding capability, and call for\ncaution in interpreting artificial intelligence by analogy to human\nintelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1\">Nouha Dziri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_J/0/1/0/all/0/1\">Jillian Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichander_A/0/1/0/all/0/1\">Abhilasha Ravichander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1\">Khyathi Chandu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newman_B/0/1/0/all/0/1\">Benjamin Newman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1\">Pang Wei Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ettinger_A/0/1/0/all/0/1\">Allyson Ettinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTwich: Extending BERT's Capabilities to Model Dialectal and Noisy Text. (arXiv:2311.00116v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00116","description":"<p>Real-world NLP applications often deal with nonstandard text (e.g.,\ndialectal, informal, or misspelled text). However, language models like BERT\ndeteriorate in the face of dialect variation or noise. How do we push BERT's\nmodeling capabilities to encompass nonstandard text? Fine-tuning helps, but it\nis designed for specializing a model to a task and does not seem to bring about\nthe deeper, more pervasive changes needed to adapt a model to nonstandard\nlanguage. In this paper, we introduce the novel idea of sandwiching BERT's\nencoder stack between additional encoder layers trained to perform masked\nlanguage modeling on noisy text. We find that our approach, paired with recent\nwork on including character-level noise in fine-tuning data, can promote\nzero-shot transfer to dialectal text, as well as reduce the distance in the\nembedding space between words and their noisy counterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Aarohi Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B. (arXiv:2311.00117v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00117","description":"<p>Llama 2-Chat is a collection of large language models that Meta developed and\nreleased to the public. While Meta fine-tuned Llama 2-Chat to refuse to output\nharmful content, we hypothesize that public access to model weights enables bad\nactors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's\ncapabilities for malicious purposes. We demonstrate that it is possible to\neffectively undo the safety fine-tuning from Llama 2-Chat 13B with less than\n$200, while retaining its general capabilities. Our results demonstrate that\nsafety-fine tuning is ineffective at preventing misuse when model weights are\nreleased publicly. Given that future models will likely have much greater\nability to cause harm at scale, it is essential that AI developers address\nthreats from fine-tuning when considering whether to publicly release their\nmodel weights.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gade_P/0/1/0/all/0/1\">Pranav Gade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lermen_S/0/1/0/all/0/1\">Simon Lermen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_Smith_C/0/1/0/all/0/1\">Charlie Rogers-Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ladish_J/0/1/0/all/0/1\">Jeffrey Ladish</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the effect of curriculum learning with developmental data for grammar acquisition. (arXiv:2311.00128v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00128","description":"<p>This work explores the degree to which grammar acquisition is driven by\nlanguage `simplicity' and the source modality (speech vs. text) of data. Using\nBabyBERTa as a probe, we find that grammar acquisition is largely driven by\nexposure to speech data, and in particular through exposure to two of the\nBabyLM training corpora: AO-Childes and Open Subtitles. We arrive at this\nfinding by examining various ways of presenting input data to our model. First,\nwe assess the impact of various sequence-level complexity based curricula. We\nthen examine the impact of learning over `blocks' -- covering spans of text\nthat are balanced for the number of tokens in each of the source corpora\n(rather than number of lines). Finally, we explore curricula that vary the\ndegree to which the model is exposed to different corpora. In all cases, we\nfind that over-exposure to AO-Childes and Open Subtitles significantly drives\nperformance. We verify these findings through a comparable control dataset in\nwhich exposure to these corpora, and speech more generally, is limited by\ndesign. Our findings indicate that it is not the proportion of tokens occupied\nby high-utility data that aids acquisition, but rather the proportion of\ntraining steps assigned to such data. We hope this encourages future research\ninto the use of more developmentally plausible linguistic data (which tends to\nbe more scarce) to augment general purpose pre-training regimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Opper_M/0/1/0/all/0/1\">Mattia Opper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morrison_J/0/1/0/all/0/1\">J. Morrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1\">N. Siddharth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Stage Classifier for Campaign Negativity Detection using Axis Embeddings: A Case Study on Tweets of Political Users during 2021 Presidential Election in Iran. (arXiv:2311.00143v1 [cs.LG])","link":"http://arxiv.org/abs/2311.00143","description":"<p>In elections around the world, the candidates may turn their campaigns toward\nnegativity due to the prospect of failure and time pressure. In the digital\nage, social media platforms such as Twitter are rich sources of political\ndiscourse. Therefore, despite the large amount of data that is published on\nTwitter, the automatic system for campaign negativity detection can play an\nessential role in understanding the strategy of candidates and parties in their\ncampaigns. In this paper, we propose a hybrid model for detecting campaign\nnegativity consisting of a two-stage classifier that combines the strengths of\ntwo machine learning models. Here, we have collected Persian tweets from 50\npolitical users, including candidates and government officials. Then we\nannotated 5,100 of them that were published during the year before the 2021\npresidential election in Iran. In the proposed model, first, the required\ndatasets of two classifiers based on the cosine similarity of tweet embeddings\nwith axis embeddings (which are the average of embedding in positive and\nnegative classes of tweets) from the training set (85\\%) are made, and then\nthese datasets are considered the training set of the two classifiers in the\nhybrid model. Finally, our best model (RF-RF) was able to achieve 79\\% for the\nmacro F1 score and 82\\% for the weighted F1 score. By running the best model on\nthe rest of the tweets of 50 political users that were published one year\nbefore the election and with the help of statistical models, we find that the\npublication of a tweet by a candidate has nothing to do with the negativity of\nthat tweet, and the presence of the names of political persons and political\norganizations in the tweet is directly related to its negativity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajabi_F/0/1/0/all/0/1\">Fatemeh Rajabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohades_A/0/1/0/all/0/1\">Ali Mohades</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Longer Fixations, More Computation: Gaze-Guided Recurrent Neural Networks. (arXiv:2311.00159v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00159","description":"<p>Humans read texts at a varying pace, while machine learning models treat each\ntoken in the same way in terms of a computational process. Therefore, we ask,\ndoes it help to make models act more like humans? In this paper, we convert\nthis intuition into a set of novel models with fixation-guided parallel RNNs or\nlayers and conduct various experiments on language modeling and sentiment\nanalysis tasks to test their effectiveness, thus providing empirical validation\nfor this intuition. Our proposed models achieve good performance on the\nlanguage modeling task, considerably surpassing the baseline model. In\naddition, we find that, interestingly, the fixation duration predicted by\nneural networks bears some resemblance to humans' fixation. Without any\nexplicit guidance, the model makes similar choices to humans. We also\ninvestigate the reasons for the differences between them, which explain why\n\"model fixations\" are often more suitable than human fixations, when used to\nguide language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinting Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Jiajing Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kritikos_I/0/1/0/all/0/1\">Ioannis Kritikos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollenstein_N/0/1/0/all/0/1\">Nora Hollenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Denouncing Hate: Strategies for Countering Implied Biases and Stereotypes in Language. (arXiv:2311.00161v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00161","description":"<p>Counterspeech, i.e., responses to counteract potential harms of hateful\nspeech, has become an increasingly popular solution to address online hate\nspeech without censorship. However, properly countering hateful language\nrequires countering and dispelling the underlying inaccurate stereotypes\nimplied by such language. In this work, we draw from psychology and philosophy\nliterature to craft six psychologically inspired strategies to challenge the\nunderlying stereotypical implications of hateful language. We first examine the\nconvincingness of each of these strategies through a user study, and then\ncompare their usages in both human- and machine-generated counterspeech\ndatasets. Our results show that human-written counterspeech uses countering\nstrategies that are more specific to the implied stereotype (e.g., counter\nexamples to the stereotype, external factors about the stereotype's origins),\nwhereas machine-generated counterspeech uses less specific strategies (e.g.,\ngenerally denouncing the hatefulness of speech). Furthermore, machine-generated\ncounterspeech often employs strategies that humans deem less convincing\ncompared to human-produced counterspeech. Our findings point to the importance\nof accounting for the underlying stereotypical implications of speech when\ngenerating counterspeech and for better machine reasoning about\nanti-stereotypical examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mun_J/0/1/0/all/0/1\">Jimin Mun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allaway_E/0/1/0/all/0/1\">Emily Allaway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yerukola_A/0/1/0/all/0/1\">Akhila Yerukola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vianna_L/0/1/0/all/0/1\">Laura Vianna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leslie_S/0/1/0/all/0/1\">Sarah-Jane Leslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield. (arXiv:2311.00172v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00172","description":"<p>Large Language Models' safety remains a critical concern due to their\nvulnerability to adversarial attacks, which can prompt these systems to produce\nharmful responses. In the heart of these systems lies a safety classifier, a\ncomputational model trained to discern and mitigate potentially harmful,\noffensive, or unethical outputs. However, contemporary safety classifiers,\ndespite their potential, often fail when exposed to inputs infused with\nadversarial noise. In response, our study introduces the Adversarial Prompt\nShield (APS), a lightweight model that excels in detection accuracy and\ndemonstrates resilience against adversarial prompts. Additionally, we propose\nnovel strategies for autonomously generating adversarial training datasets,\nnamed Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are\ndesigned to fortify the safety classifier's robustness, and we investigate the\nconsequences of incorporating adversarial examples into the training process.\nThrough evaluations involving Large Language Models, we demonstrate that our\nclassifier has the potential to decrease the attack success rate resulting from\nadversarial attacks by up to 60%. This advancement paves the way for the next\ngeneration of more reliable and resilient conversational agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinhwa Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derakhshan_A/0/1/0/all/0/1\">Ali Derakhshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harris_I/0/1/0/all/0/1\">Ian G. Harris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChipNeMo: Domain-Adapted LLMs for Chip Design. (arXiv:2311.00176v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00176","description":"<p>ChipNeMo aims to explore the applications of large language models (LLMs) for\nindustrial chip design. Instead of directly deploying off-the-shelf commercial\nor open-source LLMs, we instead adopt the following domain adaptation\ntechniques: custom tokenizers, domain-adaptive continued pretraining,\nsupervised fine-tuning (SFT) with domain-specific instructions, and\ndomain-adapted retrieval models. We evaluate these methods on three selected\nLLM applications for chip design: an engineering assistant chatbot, EDA script\ngeneration, and bug summarization and analysis. Our results show that these\ndomain adaptation techniques enable significant LLM performance improvements\nover general-purpose base models across the three evaluated applications,\nenabling up to 5x model size reduction with similar or better performance on a\nrange of design tasks. Our findings also indicate that there's still room for\nimprovement between our current results and ideal outcomes. We believe that\nfurther investigation of domain-adapted LLM approaches will help close this gap\nin the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ene_T/0/1/0/all/0/1\">Teo Ene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirby_R/0/1/0/all/0/1\">Robert Kirby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Chris Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinckney_N/0/1/0/all/0/1\">Nathaniel Pinckney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1\">Rongjian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alben_J/0/1/0/all/0/1\">Jonah Alben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_H/0/1/0/all/0/1\">Himyanshu Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1\">Sanmitra Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayraktaroglu_I/0/1/0/all/0/1\">Ismet Bayraktaroglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhaskaran_B/0/1/0/all/0/1\">Bonita Bhaskaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_A/0/1/0/all/0/1\">Arjun Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clay_S/0/1/0/all/0/1\">Sharon Clay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dally_B/0/1/0/all/0/1\">Bill Dally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_L/0/1/0/all/0/1\">Laura Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_P/0/1/0/all/0/1\">Parikshit Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhodhi_S/0/1/0/all/0/1\">Siddhanth Dhodhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halepete_S/0/1/0/all/0/1\">Sameer Halepete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_E/0/1/0/all/0/1\">Eric Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jiashang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sumit Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khailany_B/0/1/0/all/0/1\">Brucek Khailany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunal_K/0/1/0/all/0/1\">Kishor Kunal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaowei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberman_S/0/1/0/all/0/1\">Stuart Oberman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omar_S/0/1/0/all/0/1\">Sujeet Omar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratty_S/0/1/0/all/0/1\">Sreedhar Pratty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Ambar Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhengjiang Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hanfei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suthar_P/0/1/0/all/0/1\">Pratik P Suthar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tej_V/0/1/0/all/0/1\">Varun Tej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaizhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Haoxing Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XAI-CLASS: Explanation-Enhanced Text Classification with Extremely Weak Supervision. (arXiv:2311.00189v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00189","description":"<p>Text classification aims to effectively categorize documents into pre-defined\ncategories. Traditional methods for text classification often rely on large\namounts of manually annotated training data, making the process time-consuming\nand labor-intensive. To address this issue, recent studies have focused on\nweakly-supervised and extremely weakly-supervised settings, which require\nminimal or no human annotation, respectively. In previous methods of weakly\nsupervised text classification, pseudo-training data is generated by assigning\npseudo-labels to documents based on their alignment (e.g., keyword matching)\nwith specific classes. However, these methods ignore the importance of\nincorporating the explanations of the generated pseudo-labels, or saliency of\nindividual words, as additional guidance during the text classification\ntraining process. To address this limitation, we propose XAI-CLASS, a novel\nexplanation-enhanced extremely weakly-supervised text classification method\nthat incorporates word saliency prediction as an auxiliary task. XAI-CLASS\nbegins by employing a multi-round question-answering process to generate\npseudo-training data that promotes the mutual enhancement of class labels and\ncorresponding explanation word generation. This pseudo-training data is then\nused to train a multi-task framework that simultaneously learns both text\nclassification and word saliency prediction. Extensive experiments on several\nweakly-supervised text classification datasets show that XAI-CLASS outperforms\nother weakly-supervised text classification methods significantly. Moreover,\nexperiments demonstrate that XAI-CLASS enhances both model performance and\nexplainability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hajialigol_D/0/1/0/all/0/1\">Daniel Hajialigol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous Training and Fine-tuning for Domain-Specific Language Models in Medical Question Answering. (arXiv:2311.00204v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00204","description":"<p>Large language models exhibit promising general capabilities but often lack\nspecialized knowledge for domain-specific tasks. Developing domain experts from\na base model enables a range of applications without prohibitive training\ncosts. This work demonstrates a method using continuous training and\ninstruction fine-tuning to rapidly adapt Llama 2 base models to the Chinese\nmedical domain. We first conduct continuous training on 1B tokens from Chinese\nmedical references to teach relevant vocabulary and knowledge. The models are\nthen fine-tuned on 54K examples sourced from the Chinese National Medical\nLicensing Examination. Experiments on Chinese medical data confirm the\neffectiveness of this approach, producing a model comparable to GPT-3.5-turbo\nwhile using way less computational resource. The resulting domain-specific\nmodel could be useful for various Chinese medical applications. More broadly,\nthis provides a template for domain-specific training of large language models\nin areas where pre-trained models lack the required expertise, such as law,\nscience, and engineering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yining Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers as Recognizers of Formal Languages: A Survey on Expressivity. (arXiv:2311.00208v1 [cs.LG])","link":"http://arxiv.org/abs/2311.00208","description":"<p>As transformers have gained prominence in natural language processing, some\nresearchers have investigated theoretically what problems they can and cannot\nsolve, by treating problems as formal languages. Exploring questions such as\nthis will help to compare transformers with other models, and transformer\nvariants with one another, for various tasks. Work in this subarea has made\nconsiderable progress in recent years. Here, we undertake a comprehensive\nsurvey of this work, documenting the diverse assumptions that underlie\ndifferent results and providing a unified framework for harmonizing seemingly\ncontradictory findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Strobl_L/0/1/0/all/0/1\">Lena Strobl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1\">Gail Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angluin_D/0/1/0/all/0/1\">Dana Angluin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is GPT Powerful Enough to Analyze the Emotions of Memes?. (arXiv:2311.00223v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00223","description":"<p>Large Language Models (LLMs), representing a significant achievement in\nartificial intelligence (AI) research, have demonstrated their ability in a\nmultitude of tasks. This project aims to explore the capabilities of GPT-3.5, a\nleading example of LLMs, in processing the sentiment analysis of Internet\nmemes. Memes, which include both verbal and visual aspects, act as a powerful\nyet complex tool for expressing ideas and sentiments, demanding an\nunderstanding of societal norms and cultural contexts. Notably, the detection\nand moderation of hateful memes pose a significant challenge due to their\nimplicit offensive nature. This project investigates GPT's proficiency in such\nsubjective tasks, revealing its strengths and potential limitations. The tasks\ninclude the classification of meme sentiment, determination of humor type, and\ndetection of implicit hate in memes. The performance evaluation, using datasets\nfrom SemEval-2020 Task 8 and Facebook hateful memes, offers a comparative\nunderstanding of GPT responses against human annotations. Despite GPT's\nremarkable progress, our findings underscore the challenges faced by these\nmodels in handling subjective tasks, which are rooted in their inherent\nlimitations including contextual understanding, interpretation of implicit\nmeanings, and data biases. This research contributes to the broader discourse\non the applicability of AI in handling complex, context-dependent tasks, and\noffers valuable insights for future advancements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingjing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Joshua Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Grace Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_A/0/1/0/all/0/1\">Allen Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Feng Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distort, Distract, Decode: Instruction-Tuned Model Can Refine its Response from Noisy Instructions. (arXiv:2311.00233v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00233","description":"<p>While instruction-tuned language models have demonstrated impressive\nzero-shot generalization, these models often struggle to generate accurate\nresponses when faced with instructions that fall outside their training set.\nThis paper presents Instructive Decoding (ID), a simple yet effective approach\nthat augments the efficacy of instruction-tuned models. Specifically, ID\nadjusts the logits for next-token prediction in a contrastive manner, utilizing\npredictions generated from a manipulated version of the original instruction,\nreferred to as a noisy instruction. This noisy instruction aims to elicit\nresponses that could diverge from the intended instruction yet remain\nplausible. We conduct experiments across a spectrum of such noisy instructions,\nranging from those that insert semantic noise via random words to others like\n'opposite' that elicit the deviated responses. Our approach achieves\nconsiderable performance gains across various instruction-tuned models and\ntasks without necessitating any additional parameter updates. Notably,\nutilizing 'opposite' as the noisy instruction in ID, which exhibits the maximum\ndivergence from the original instruction, consistently produces the most\nsignificant performance gains across multiple models and tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joonkee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gihun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Se-Young Yun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities. (arXiv:2311.00237v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00237","description":"<p>Understanding emergent abilities, such as in-context learning (ICL) and\nchain-of-thought (CoT) prompting in large language models (LLMs), is of utmost\nimportance. This importance stems not only from the better utilization of these\ncapabilities across various tasks, but also from the proactive identification\nand mitigation of potential risks, including concerns of truthfulness, bias,\nand toxicity, that may arise alongside these capabilities. In this paper, we\npresent a thorough survey on the interpretation and analysis of emergent\nabilities of LLMs. First, we provide a concise introduction to the background\nand definition of emergent abilities. Then, we give an overview of advancements\nfrom two perspectives: 1) a macro perspective, emphasizing studies on the\nmechanistic interpretability and delving into the mathematical foundations\nbehind emergent abilities; and 2) a micro-perspective, concerning studies that\nfocus on empirical interpretability by examining factors associated with these\nabilities. We conclude by highlighting the challenges encountered and\nsuggesting potential avenues for future research. We believe that our work\nestablishes the basis for further exploration into the interpretation of\nemergent abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuxiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiazheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yanzheng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hanqi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1\">Lin Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis. (arXiv:2311.00258v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00258","description":"<p>Recent advances in prompt engineering enable large language models (LLMs) to\nsolve multi-hop logical reasoning problems with impressive accuracy. However,\nthere is little existing work investigating the robustness of LLMs with\nfew-shot prompting techniques. Therefore, we introduce a systematic approach to\ntest the robustness of LLMs in multi-hop reasoning tasks via domain-agnostic\nperturbations. We include perturbations at multiple levels of abstractions\n(e.g. lexical perturbations such as typos, and semantic perturbations such as\nthe inclusion of intermediate reasoning steps in the questions) to conduct\nbehavioral analysis on the LLMs. Throughout our experiments, we find that\nmodels are more sensitive to certain perturbations such as replacing words with\ntheir synonyms. We also demonstrate that increasing the proportion of perturbed\nexemplars in the prompts improves the robustness of few-shot prompting methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hongyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saparov_A/0/1/0/all/0/1\">Abulhair Saparov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents. (arXiv:2311.00262v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00262","description":"<p>Proactive dialogues serve as a practical yet challenging dialogue problem in\nthe era of large language models (LLMs), where the dialogue policy planning is\nthe key to improving the proactivity of LLMs. Most existing studies enable the\ndialogue policy planning of LLMs using various prompting schemes or iteratively\nenhance this capability in handling the given case with verbal AI feedback.\nHowever, these approaches are either bounded by the policy planning capability\nof the frozen LLMs or hard to be transferred to new cases. In this work, we\nintroduce a new dialogue policy planning paradigm to strategize LLMs for\nproactive dialogue problems with a tunable language model plug-in as a\nplug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a\nnovel training framework to facilitate supervised fine-tuning over available\nhuman-annotated data as well as reinforcement learning from goal-oriented AI\nfeedback with dynamic interaction data collected by the LLM-based self-play\nsimulation. In this manner, the LLM-powered dialogue agent can not only be\ngeneralized to different cases after the training, but also be applicable to\ndifferent applications by just substituting the learned plug-in. In addition,\nwe propose to evaluate the policy planning capability of dialogue systems under\nthe interactive setting. Experimental results demonstrate that PPDPP\nconsistently and substantially outperforms existing approaches on three\ndifferent proactive dialogue applications, including negotiation, emotional\nsupport, and tutoring dialogues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1\">See-Kiong Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntactic Inductive Bias in Transformer Language Models: Especially Helpful for Low-Resource Languages?. (arXiv:2311.00268v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00268","description":"<p>A line of work on Transformer-based language models such as BERT has\nattempted to use syntactic inductive bias to enhance the pretraining process,\non the theory that building syntactic structure into the training process\nshould reduce the amount of data needed for training. But such methods are\noften tested for high-resource languages such as English. In this work, we\ninvestigate whether these methods can compensate for data sparseness in\nlow-resource languages, hypothesizing that they ought to be more effective for\nlow-resource languages. We experiment with five low-resource languages: Uyghur,\nWolof, Maltese, Coptic, and Ancient Greek. We find that these syntactic\ninductive bias methods produce uneven results in low-resource settings, and\nprovide surprisingly little benefit in most cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gessler_L/0/1/0/all/0/1\">Luke Gessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SoulChat: Improving LLMs' Empathy, Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations. (arXiv:2311.00273v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00273","description":"<p>Large language models (LLMs) have been widely applied in various fields due\nto their excellent capability for memorizing knowledge and chain of thought\n(CoT). When these language models are applied in the field of psychological\ncounseling, they often rush to provide universal advice. However, when users\nseek psychological support, they need to gain empathy, trust, understanding and\ncomfort, rather than just reasonable advice. To this end, we constructed a\nmulti-turn empathetic conversation dataset of more than 2 million samples, in\nwhich the input is the multi-turn conversation context, and the target is\nempathetic responses that cover expressions such as questioning, comfort,\nrecognition, listening, trust, emotional support, etc. Experiments have shown\nthat the empathy ability of LLMs can be significantly enhanced when finetuning\nby using multi-turn dialogue history and responses that are closer to the\nexpression of a psychological consultant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yirong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1\">Xiaofen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jingkai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huimin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiangmin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JADE: A Linguistic-based Safety Evaluation Platform for LLM. (arXiv:2311.00286v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00286","description":"<p>In this paper, we present \\textit{JADE}, a targeted linguistic fuzzing\nplatform which strengthens the linguistic complexity of seed questions to\nsimultaneously and consistently break a wide range of widely-used LLMs\ncategorized in three groups: eight open-sourced Chinese, six commercial Chinese\nand four commercial English LLMs. JADE generates three safety benchmarks for\nthe three groups of LLMs, which contain unsafe questions that are highly\nthreatening: the questions simultaneously trigger harmful generation of\nmultiple LLMs, with an average unsafe generation ratio of \\textbf{$70\\%$}\n(please see the table below), while are still natural questions, fluent and\npreserving the core unsafe semantics. We release the benchmark demos generated\nfor commercial English LLMs and open-sourced English LLMs in the following\nlink: https://github.com/whitzard-ai/jade-db. For readers who are interested in\nevaluating on more questions generated by JADE, please contact us.\n</p>\n<p>\\textit{JADE} is based on Noam Chomsky's seminal theory of\ntransformational-generative grammar. Given a seed question with unsafe\nintention, \\textit{JADE} invokes a sequence of generative and transformational\nrules to increment the complexity of the syntactic structure of the original\nquestion, until the safety guardrail is broken. Our key insight is: Due to the\ncomplexity of human language, most of the current best LLMs can hardly\nrecognize the invariant evil from the infinite number of different syntactic\nstructures which form an unbound example space that can never be fully covered.\nTechnically, the generative/transformative rules are constructed by native\nspeakers of the languages, and, once developed, can be used to automatically\ngrow and transform the parse tree of a given question, until the guardrail is\nbroken. For more evaluation results and demo, please check our website:\nhttps://whitzard-ai.github.io/jade.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xudong Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models. (arXiv:2311.00287v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00287","description":"<p>Clinical natural language processing requires methods that can address\ndomain-specific challenges, such as complex medical terminology and clinical\ncontexts. Recently, large language models (LLMs) have shown promise in this\ndomain. Yet, their direct deployment can lead to privacy issues and are\nconstrained by resources. To address this challenge, we delve into synthetic\nclinical text generation using LLMs for clinical NLP tasks. We propose an\ninnovative, resource-efficient approach, ClinGen, which infuses knowledge into\nthe process. Our model involves clinical knowledge extraction and\ncontext-informed LLM prompting. Both clinical topics and writing styles are\ndrawn from external domain-specific knowledge graphs and LLMs to guide data\ngeneration. Our extensive empirical study across 7 clinical NLP tasks and 16\ndatasets reveals that ClinGen consistently enhances performance across various\ntasks, effectively aligning the distribution of real datasets and significantly\nenriching the diversity of generated training instances. We will publish our\ncode and all the generated data in \\url{https://github.com/ritaranx/ClinGen}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_X/0/1/0/all/0/1\">Xuan Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Wenqi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yuchen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Wei Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Joyce Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks. (arXiv:2311.00288v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00288","description":"<p>Instruction tuning (IT) achieves impressive zero-shot generalization results\nby training large language models (LLMs) on a massive amount of diverse tasks\nwith instructions. However, how to select new tasks to improve the performance\nand generalizability of IT models remains an open question. Training on all\nexisting tasks is impractical due to prohibiting computation requirements, and\nrandomly selecting tasks can lead to suboptimal performance. In this work, we\npropose active instruction tuning based on prompt uncertainty, a novel\nframework to identify informative tasks, and then actively tune the models on\nthe selected tasks. We represent the informativeness of new tasks with the\ndisagreement of the current model outputs over perturbed prompts. Our\nexperiments on NIV2 and Self-Instruct datasets demonstrate that our method\nconsistently outperforms other baseline strategies for task selection,\nachieving better out-of-distribution generalization with fewer training tasks.\nAdditionally, we introduce a task map that categorizes and diagnoses tasks\nbased on prompt uncertainty and prediction probability. We discover that\ntraining on ambiguous (prompt-uncertain) tasks improves generalization while\ntraining on difficult (prompt-certain and low-probability) tasks offers no\nbenefit, underscoring the importance of task selection for instruction tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kung_P/0/1/0/all/0/1\">Po-Nien Kung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1\">Fan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IBADR: an Iterative Bias-Aware Dataset Refinement Framework for Debiasing NLU models. (arXiv:2311.00292v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00292","description":"<p>As commonly-used methods for debiasing natural language understanding (NLU)\nmodels, dataset refinement approaches heavily rely on manual data analysis, and\nthus maybe unable to cover all the potential biased features. In this paper, we\npropose IBADR, an Iterative Bias-Aware Dataset Refinement framework, which\ndebiases NLU models without predefining biased features. We maintain an\niteratively expanded sample pool. Specifically, at each iteration, we first\ntrain a shallow model to quantify the bias degree of samples in the pool. Then,\nwe pair each sample with a bias indicator representing its bias degree, and use\nthese extended samples to train a sample generator. In this way, this generator\ncan effectively learn the correspondence relationship between bias indicators\nand samples. Furthermore, we employ the generator to produce pseudo samples\nwith fewer biased features by feeding specific bias indicators. Finally, we\nincorporate the generated pseudo samples into the pool. Experimental results\nand in-depth analyses on two NLU tasks show that IBADR not only significantly\noutperforms existing dataset refinement approaches, achieving SOTA, but also is\ncompatible with model-centric methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaoxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Representation Learning of Scientific Literature based on Adaptive Feature and Graph Neural Network. (arXiv:2311.00296v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00296","description":"<p>Because most of the scientific literature data is unmarked, it makes semantic\nrepresentation learning based on unsupervised graph become crucial. At the same\ntime, in order to enrich the features of scientific literature, a learning\nmethod of semantic representation of scientific literature based on adaptive\nfeatures and graph neural network is proposed. By introducing the adaptive\nfeature method, the features of scientific literature are considered globally\nand locally. The graph attention mechanism is used to sum the features of\nscientific literature with citation relationship, and give each scientific\nliterature different feature weights, so as to better express the correlation\nbetween the features of different scientific literature. In addition, an\nunsupervised graph neural network semantic representation learning method is\nproposed. By comparing the mutual information between the positive and negative\nlocal semantic representation of scientific literature and the global graph\nsemantic representation in the potential space, the graph neural network can\ncapture the local and global information, thus improving the learning ability\nof the semantic representation of scientific literature. The experimental\nresults show that the proposed learning method of semantic representation of\nscientific literature based on adaptive feature and graph neural network is\ncompetitive on the basis of scientific literature classification, and has\nachieved good results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Hongrui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yawen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Meiyu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1\">Zeli Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zhe Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity Alignment Method of Science and Technology Patent based on Graph Convolution Network and Information Fusion. (arXiv:2311.00300v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00300","description":"<p>The entity alignment of science and technology patents aims to link the\nequivalent entities in the knowledge graph of different science and technology\npatent data sources. Most entity alignment methods only use graph neural\nnetwork to obtain the embedding of graph structure or use attribute text\ndescription to obtain semantic representation, ignoring the process of\nmulti-information fusion in science and technology patents. In order to make\nuse of the graphic structure and auxiliary information such as the name,\ndescription and attribute of the patent entity, this paper proposes an entity\nalignment method based on the graph convolution network for science and\ntechnology patent information fusion. Through the graph convolution network and\nBERT model, the structure information and entity attribute information of the\nscience and technology patent knowledge graph are embedded and represented to\nachieve multi-information fusion, thus improving the performance of entity\nalignment. Experiments on three benchmark data sets show that the proposed\nmethod Hit@K The evaluation indicators are better than the existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1\">Runze Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yawen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yingxia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1\">Zeli Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zhe Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Syllable-Level Pronunciation Stress with A Self-Attention Model. (arXiv:2311.00301v1 [cs.SD])","link":"http://arxiv.org/abs/2311.00301","description":"<p>One precondition of effective oral communication is that words should be\npronounced clearly, especially for non-native speakers. Word stress is the key\nto clear and correct English, and misplacement of syllable stress may lead to\nmisunderstandings. Thus, knowing the stress level is important for English\nspeakers and learners. This paper presents a self-attention model to identify\nthe stress level for each syllable of spoken English. Various prosodic and\ncategorical features, including the pitch level, intensity, duration and type\nof the syllable and its nuclei (the vowel of the syllable), are explored. These\nfeatures are input to the self-attention model, and syllable-level stresses are\npredicted. The simplest model yields an accuracy of over 88% and 93% on\ndifferent datasets, while more advanced models provide higher accuracy. Our\nstudy suggests that the self-attention model can be promising in stress-level\ndetection. These models could be applied to various scenarios, such as online\nmeetings and English learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weiying_W/0/1/0/all/0/1\">Wang Weiying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akinori_N/0/1/0/all/0/1\">Nakajima Akinori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Explicit and Implicit Gender Bias through LLM Conditional Text Generation. (arXiv:2311.00306v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00306","description":"<p>Large Language Models (LLMs) can generate biased and toxic responses. Yet\nmost prior work on LLM gender bias evaluation requires predefined\ngender-related phrases or gender stereotypes, which are challenging to be\ncomprehensively collected and are limited to explicit bias evaluation. In\naddition, we believe that instances devoid of gender-related language or\nexplicit stereotypes in inputs can still induce gender bias in LLMs. Thus, in\nthis work, we propose a conditional text generation mechanism without the need\nfor predefined gender phrases and stereotypes. This approach employs three\ntypes of inputs generated through three distinct strategies to probe LLMs,\naiming to show evidence of explicit and implicit gender biases in LLMs. We also\nutilize explicit and implicit evaluation metrics to evaluate gender bias in\nLLMs under different strategies. Our experiments demonstrate that an increased\nmodel size does not consistently lead to enhanced fairness and all tested LLMs\nexhibit explicit and/or implicit gender bias, even when explicit gender\nstereotypes are absent in the inputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiangjue Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caverlee_J/0/1/0/all/0/1\">James Caverlee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Lexical Simplification with Context Augmentation. (arXiv:2311.00310v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00310","description":"<p>We propose a new unsupervised lexical simplification method that uses only\nmonolingual data and pre-trained language models. Given a target word and its\ncontext, our method generates substitutes based on the target context and also\nadditional contexts sampled from monolingual data. We conduct experiments in\nEnglish, Portuguese, and Spanish on the TSAR-2022 shared task, and show that\nour model substantially outperforms other unsupervised systems across all\nlanguages. We also establish a new state-of-the-art by ensembling our model\nwith GPT-3.5. Lastly, we evaluate our model on the SWORDS lexical substitution\ndata set, achieving a state-of-the-art result.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wada_T/0/1/0/all/0/1\">Takashi Wada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation for Code Translation with Comparable Corpora and Multiple References. (arXiv:2311.00317v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00317","description":"<p>One major challenge of translating code between programming languages is that\nparallel training data is often limited. To overcome this challenge, we present\ntwo data augmentation techniques, one that builds comparable corpora (i.e.,\ncode pairs with similar functionality), and another that augments existing\nparallel data with multiple reference translations. Specifically, we build and\nanalyze multiple types of comparable corpora, including programs generated from\nnatural language documentation using a code generation model. Furthermore, to\nreduce overfitting to a single reference translation, we automatically generate\nadditional translation references for available parallel data and filter the\ntranslations by unit tests, which increases variation in target translations.\nExperiments show that our data augmentation techniques significantly improve\nCodeT5 for translation between Java, Python, and C++ by an average of 7.5%\nComputational Accuracy (CA@1), which verifies the correctness of translations\nby execution. The code is available at https://github.com/Veronicium/CMTrans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yiqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1\">Atharva Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rose_C/0/1/0/all/0/1\">Carolyn Rose</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning. (arXiv:2311.00321v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00321","description":"<p>With the proliferation of social media, accurate detection of hate speech has\nbecome critical to ensure safety online. To combat nuanced forms of hate\nspeech, it is important to identify and thoroughly explain hate speech to help\nusers understand its harmful effects. Recent benchmarks have attempted to\ntackle this issue by training generative models on free-text annotations of\nimplications in hateful text. However, we find significant reasoning gaps in\nthe existing annotations schemes, which may hinder the supervision of detection\nmodels. In this paper, we introduce a hate speech detection framework, HARE,\nwhich harnesses the reasoning capabilities of large language models (LLMs) to\nfill these gaps in explanations of hate speech, thus enabling effective\nsupervision of detection models. Experiments on SBIC and Implicit Hate\nbenchmarks show that our method, using model-generated data, consistently\noutperforms baselines, using existing free-text human annotations. Analysis\ndemonstrates that our method enhances the explanation quality of trained models\nand improves generalization to unseen datasets. Our code is available at\nhttps://github.com/joonkeekim/hare-hate-speech.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongjin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joonkee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yujin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Namgyu Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1\">James Thorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Se-young Yun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-based Logical Semantics Enhancement for Implicit Discourse Relation Recognition. (arXiv:2311.00367v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00367","description":"<p>Implicit Discourse Relation Recognition (IDRR), which infers discourse\nrelations without the help of explicit connectives, is still a crucial and\nchallenging task for discourse parsing. Recent works tend to exploit the\nhierarchical structure information from the annotated senses, which demonstrate\nenhanced discourse relation representations can be obtained by integrating\nsense hierarchy. Nevertheless, the performance and robustness for IDRR are\nsignificantly constrained by the availability of annotated data. Fortunately,\nthere is a wealth of unannotated utterances with explicit connectives, that can\nbe utilized to acquire enriched discourse relation features. In light of such\nmotivation, we propose a Prompt-based Logical Semantics Enhancement (PLSE)\nmethod for IDRR. Essentially, our method seamlessly injects knowledge relevant\nto discourse relation into pre-trained language models through prompt-based\nconnective prediction. Furthermore, considering the prompt-based connective\nprediction exhibits local dependencies due to the deficiency of masked language\nmodel (MLM) in capturing global semantics, we design a novel self-supervised\nlearning objective based on mutual information maximization to derive enhanced\nrepresentations of logical semantics for IDRR. Experimental results on PDTB 2.0\nand CoNLL16 datasets demonstrate that our method achieves outstanding and\nconsistent performance against the current state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenxu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_P/0/1/0/all/0/1\">Ping Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Mu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Knowledge Injection for Radiology Report Generation. (arXiv:2311.00399v1 [cs.CV])","link":"http://arxiv.org/abs/2311.00399","description":"<p>Automatic generation of radiology reports holds crucial clinical value, as it\ncan alleviate substantial workload on radiologists and remind less experienced\nones of potential anomalies. Despite the remarkable performance of various\nimage captioning methods in the natural image field, generating accurate\nreports for medical images still faces challenges, i.e., disparities in visual\nand textual data, and lack of accurate domain knowledge. To address these\nissues, we propose an enhanced knowledge injection framework, which utilizes\ntwo branches to extract different types of knowledge. The Weighted Concept\nKnowledge (WCK) branch is responsible for introducing clinical medical concepts\nweighted by TF-IDF scores. The Multimodal Retrieval Knowledge (MRK) branch\nextracts triplets from similar reports, emphasizing crucial clinical\ninformation related to entity positions and existence. By integrating this\nfiner-grained and well-structured knowledge with the current image, we are able\nto leverage the multi-source knowledge gain to ultimately facilitate more\naccurate report generation. Extensive experiments have been conducted on two\npublic benchmarks, demonstrating that our method achieves superior performance\nover other state-of-the-art methods. Ablation studies further validate the\neffectiveness of two extracted knowledge sources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jilan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1\">Runtian Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mohan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuejie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Rui Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaobo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaSent: Efficient Domain-Adapted Sentence Embeddings for Few-Shot Classification. (arXiv:2311.00408v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00408","description":"<p>Recent work has found that few-shot sentence classification based on\npre-trained Sentence Encoders (SEs) is efficient, robust, and effective. In\nthis work, we investigate strategies for domain-specialization in the context\nof few-shot sentence classification with SEs. We first establish that\nunsupervised Domain-Adaptive Pre-Training (DAPT) of a base Pre-trained Language\nModel (PLM) (i.e., not an SE) substantially improves the accuracy of few-shot\nsentence classification by up to 8.4 points. However, applying DAPT on SEs, on\nthe one hand, disrupts the effects of their (general-domain) Sentence Embedding\nPre-Training (SEPT). On the other hand, applying general-domain SEPT on top of\na domain-adapted base PLM (i.e., after DAPT) is effective but inefficient,\nsince the computationally expensive SEPT needs to be executed on top of a\nDAPT-ed PLM of each domain. As a solution, we propose AdaSent, which decouples\nSEPT from DAPT by training a SEPT adapter on the base PLM. The adapter can be\ninserted into DAPT-ed PLMs from any domain. We demonstrate AdaSent's\neffectiveness in extensive experiments on 17 different few-shot sentence\nclassification datasets. AdaSent matches or surpasses the performance of full\nSEPT on DAPT-ed PLM, while substantially reducing the training costs. The code\nfor AdaSent is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Sourav Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1\">Raj Nath Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Human-AI Coordination via Preparatory Language-based Convention. (arXiv:2311.00416v1 [cs.LG])","link":"http://arxiv.org/abs/2311.00416","description":"<p>Developing intelligent agents capable of seamless coordination with humans is\na critical step towards achieving artificial general intelligence. Existing\nmethods for human-AI coordination typically train an agent to coordinate with a\ndiverse set of policies or with human models fitted from real human data.\nHowever, the massively diverse styles of human behavior present obstacles for\nAI systems with constrained capacity, while high quality human data may not be\nreadily available in real-world scenarios. In this study, we observe that prior\nto coordination, humans engage in communication to establish conventions that\nspecify individual roles and actions, making their coordination proceed in an\norderly manner. Building upon this observation, we propose employing the large\nlanguage model (LLM) to develop an action plan (or equivalently, a convention)\nthat effectively guides both human and AI. By inputting task requirements,\nhuman preferences, the number of agents, and other pertinent information into\nthe LLM, it can generate a comprehensive convention that facilitates a clear\nunderstanding of tasks and responsibilities for all parties involved.\nFurthermore, we demonstrate that decomposing the convention formulation problem\ninto sub-problems with multiple new sessions being sequentially employed and\nhuman feedback, will yield a more efficient coordination convention.\nExperimental evaluations conducted in the Overcooked-AI environment, utilizing\na human proxy model, highlight the superior performance of our proposed method\ncompared to existing learning-based approaches. When coordinating with real\nhumans, our method achieves better alignment with human preferences and an\naverage performance improvement of 15% compared to the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cong Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lichao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chunpeng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yichen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lihe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yunjia Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yang Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling. (arXiv:2311.00430v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00430","description":"<p>As the size of pre-trained speech recognition models increases, running these\nlarge models in low-latency or resource-constrained environments becomes\nchallenging. In this work, we leverage pseudo-labelling to assemble a\nlarge-scale open-source dataset which we use to distill the Whisper model into\na smaller variant, called Distil-Whisper. Using a simple word error rate (WER)\nheuristic, we select only the highest quality pseudo-labels for training. The\ndistilled model is 5.8 times faster with 51% fewer parameters, while performing\nto within 1% WER on out-of-distribution test data in a zero-shot transfer\nsetting. Distil-Whisper maintains the robustness of the Whisper model to\ndifficult acoustic conditions, while being less prone to hallucination errors\non long-form audio. Distil-Whisper is designed to be paired with Whisper for\nspeculative decoding, yielding a 2 times speed-up while mathematically ensuring\nthe same outputs as the original model. To facilitate further research in this\ndomain, we make our training code, inference code and models publicly\naccessible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_S/0/1/0/all/0/1\">Sanchit Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Platen_P/0/1/0/all/0/1\">Patrick von Platen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models. (arXiv:2311.00445v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00445","description":"<p>A central component of rational behavior is logical inference: the process of\ndetermining which conclusions follow from a set of premises. Psychologists have\ndocumented several ways in which humans' inferences deviate from the rules of\nlogic. Do language models, which are trained on text generated by humans,\nreplicate these biases, or are they able to overcome them? Focusing on the case\nof syllogisms -- inferences from two simple premises, which have been studied\nextensively in psychology -- we show that larger models are more logical than\nsmaller ones, and also more logical than humans. At the same time, even the\nlargest models make systematic errors, some of which mirror human reasoning\nbiases such as ordering effects and logical fallacies. Overall, we find that\nlanguage models mimic the human biases included in their training data, but are\nable to overcome them in some cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eisape_T/0/1/0/all/0/1\">Tiwalayo Eisape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tessler_M/0/1/0/all/0/1\">MH Tessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_I/0/1/0/all/0/1\">Ishita Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1\">Fei Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steenkiste_S/0/1/0/all/0/1\">Sjoerd van Steenkiste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discourse Relations Classification and Cross-Framework Discourse Relation Classification Through the Lens of Cognitive Dimensions: An Empirical Investigation. (arXiv:2311.00451v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00451","description":"<p>Existing discourse formalisms use different taxonomies of discourse\nrelations, which require expert knowledge to understand, posing a challenge for\nannotation and automatic classification. We show that discourse relations can\nbe effectively captured by some simple cognitively inspired dimensions proposed\nby Sanders et al.(2018). Our experiments on cross-framework discourse relation\nclassification (PDTB &amp; RST) demonstrate that it is possible to transfer\nknowledge of discourse relations for one framework to another framework by\nmeans of these dimensions, in spite of differences in discourse segmentation of\nthe two frameworks. This manifests the effectiveness of these dimensions in\ncharacterizing discourse relations across frameworks. Ablation studies reveal\nthat different dimensions influence different types of discourse relations. The\npatterns can be explained by the role of dimensions in characterizing and\ndistinguishing different relations. We also report our experimental results on\nautomatic prediction of these dimensions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingxue Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style Locality for Controllable Generation with kNN Language Models. (arXiv:2311.00475v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00475","description":"<p>Recent language models have been improved by the addition of external memory.\nNearest neighbor language models retrieve similar contexts to assist in word\nprediction. The addition of locality levels allows a model to learn how to\nweight neighbors based on their relative location to the current text in source\ndocuments, and have been shown to further improve model performance. Nearest\nneighbor models have been explored for controllable generation but have not\nexamined the use of locality levels. We present a novel approach for this\npurpose and evaluate it using automatic and human evaluation on politeness,\nformality, supportiveness, and toxicity textual data. We find that our model is\nsuccessfully able to control style and provides a better fluency-style\ntrade-off than previous work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nawezi_G/0/1/0/all/0/1\">Gilles Nawezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flek_L/0/1/0/all/0/1\">Lucie Flek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welch_C/0/1/0/all/0/1\">Charles Welch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Optimization Targets for Contrast-Consistent Search. (arXiv:2311.00488v1 [cs.LG])","link":"http://arxiv.org/abs/2311.00488","description":"<p>We investigate the optimization target of Contrast-Consistent Search (CCS),\nwhich aims to recover the internal representations of truth of a large language\nmodel. We present a new loss function that we call the Midpoint-Displacement\n(MD) loss function. We demonstrate that for a certain hyper-parameter value\nthis MD loss function leads to a prober with very similar weights to CCS. We\nfurther show that this hyper-parameter is not optimal and that with a better\nhyper-parameter the MD loss function attains a higher test accuracy than CCS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fry_H/0/1/0/all/0/1\">Hugo Fry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallows_S/0/1/0/all/0/1\">Seamus Fallows</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_I/0/1/0/all/0/1\">Ian Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1\">Jamie Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoots_N/0/1/0/all/0/1\">Nandi Schoots</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient LLM Inference on CPUs. (arXiv:2311.00502v1 [cs.LG])","link":"http://arxiv.org/abs/2311.00502","description":"<p>Large language models (LLMs) have demonstrated remarkable performance and\ntremendous potential across a wide range of tasks. However, deploying these\nmodels has been challenging due to the astronomical amount of model parameters,\nwhich requires a demand for large memory capacity and high memory bandwidth. In\nthis paper, we propose an effective approach that can make the deployment of\nLLMs more efficiently. We support an automatic INT4 weight-only quantization\nflow and design a special LLM runtime with highly-optimized kernels to\naccelerate the LLM inference on CPUs. We demonstrate the general applicability\nof our approach on popular LLMs including Llama2, Llama, GPT-NeoX, and showcase\nthe extreme inference efficiency on CPUs. The code is publicly available at:\nhttps://github.com/intel/intel-extension-for-transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Haihao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hanwen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Hengyu Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness Tests for Automatic Machine Translation Metrics with Adversarial Attacks. (arXiv:2311.00508v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00508","description":"<p>We investigate MT evaluation metric performance on adversarially-synthesized\ntexts, to shed light on metric robustness. We experiment with word- and\ncharacter-level attacks on three popular machine translation metrics:\nBERTScore, BLEURT, and COMET. Our human experiments validate that automatic\nmetrics tend to overpenalize adversarially-degraded translations. We also\nidentify inconsistencies in BERTScore ratings, where it judges the original\nsentence and the adversarially-degraded one as similar, while judging the\ndegraded translation as notably worse than the original with respect to the\nreference. We identify patterns of brittleness that motivate more robust metric\ndevelopment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yichen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rule-Based Error Classification for Analyzing Differences in Frequent Errors. (arXiv:2311.00513v1 [cs.SE])","link":"http://arxiv.org/abs/2311.00513","description":"<p>Finding and fixing errors is a time-consuming task not only for novice\nprogrammers but also for expert programmers. Prior work has identified frequent\nerror patterns among various levels of programmers. However, the differences in\nthe tendencies between novices and experts have yet to be revealed. From the\nknowledge of the frequent errors in each level of programmers, instructors will\nbe able to provide helpful advice for each level of learners. In this paper, we\npropose a rule-based error classification tool to classify errors in code pairs\nconsisting of wrong and correct programs. We classify errors for 95,631 code\npairs and identify 3.47 errors on average, which are submitted by various\nlevels of programmers on an online judge system. The classified errors are used\nto analyze the differences in frequent errors between novice and expert\nprogrammers. The analyzed results show that, as for the same introductory\nproblems, errors made by novices are due to the lack of knowledge in\nprogramming, and the mistakes are considered an essential part of the learning\nprocess. On the other hand, errors made by experts are due to misunderstandings\ncaused by the carelessness of reading problems or the challenges of solving\nproblems differently than usual. The proposed tool can be used to create\nerror-labeled datasets and for further code-related educational research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shirafuji_A/0/1/0/all/0/1\">Atsushi Shirafuji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsumoto_T/0/1/0/all/0/1\">Taku Matsumoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1\">Md Faizul Ibne Amin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanobe_Y/0/1/0/all/0/1\">Yutaka Watanobe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Rendering Strategies for Pixel Language Models. (arXiv:2311.00522v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00522","description":"<p>Pixel-based language models process text rendered as images, which allows\nthem to handle any script, making them a promising approach to open vocabulary\nlanguage modelling. However, recent approaches use text renderers that produce\na large set of almost-equivalent input patches, which may prove sub-optimal for\ndownstream tasks, due to redundancy in the input representations. In this\npaper, we investigate four approaches to rendering text in the PIXEL model\n(Rust et al., 2023), and find that simple character bigram rendering brings\nimproved performance on sentence-level tasks without compromising performance\non token-level or multilingual tasks. This new rendering strategy also makes it\npossible to train a more compact model with only 22M parameters that performs\non par with the original 86M parameter model. Our analyses show that character\nbigram rendering leads to a consistently better model but with an anisotropic\npatch embedding space, driven by a patch frequency bias, highlighting the\nconnections between image patch- and tokenization-based language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lotz_J/0/1/0/all/0/1\">Jonas F. Lotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salesky_E/0/1/0/all/0/1\">Elizabeth Salesky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rust_P/0/1/0/all/0/1\">Phillip Rust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1\">Desmond Elliott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek. (arXiv:2311.00541v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00541","description":"<p>Word meanings change over time, and word senses evolve, emerge or die out in\nthe process. For ancient languages, where the corpora are often small, sparse\nand noisy, modelling such changes accurately proves challenging, and\nquantifying uncertainty in sense-change estimates consequently becomes\nimportant. GASC and DiSC are existing generative models that have been used to\nanalyse sense change for target words from an ancient Greek text corpus, using\nunsupervised learning without the help of any pre-training. These models\nrepresent the senses of a given target word such as \"kosmos\" (meaning\ndecoration, order or world) as distributions over context words, and sense\nprevalence as a distribution over senses. The models are fitted using MCMC\nmethods to measure temporal changes in these representations. In this paper, we\nintroduce EDiSC, an embedded version of DiSC, which combines word embeddings\nwith DiSC to provide superior model performance. We show empirically that EDiSC\noffers improved predictive accuracy, ground-truth recovery and uncertainty\nquantification, as well as better sampling efficiency and scalability\nproperties with MCMC methods. We also discuss the challenges of fitting these\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zafar_S/0/1/0/all/0/1\">Schyan Zafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicholls_G/0/1/0/all/0/1\">Geoff K. Nicholls</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing. (arXiv:2311.00571v1 [cs.CV])","link":"http://arxiv.org/abs/2311.00571","description":"<p>LLaVA-Interactive is a research prototype for multimodal human-AI\ninteraction. The system can have multi-turn dialogues with human users by\ntaking multimodal user inputs and generating multimodal responses. Importantly,\nLLaVA-Interactive goes beyond language prompt, where visual prompt is enabled\nto align human intents in the interaction. The development of LLaVA-Interactive\nis extremely cost-efficient as the system combines three multimodal skills of\npre-built AI models without additional model training: visual chat of LLaVA,\nimage segmentation from SEEM, as well as image generation and editing from\nGLIGEN. A diverse set of application scenarios is presented to demonstrate the\npromises of LLaVA-Interactive and to inspire future research in multimodal\ninteractive systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Ge Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiridonova_I/0/1/0/all/0/1\">Irina Spiridonova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crosslingual Retrieval Augmented In-context Learning for Bangla. (arXiv:2311.00587v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00587","description":"<p>The promise of Large Language Models (LLMs) in Natural Language Processing\nhas often been overshadowed by their limited performance in low-resource\nlanguages such as Bangla. To address this, our paper presents a pioneering\napproach that utilizes cross-lingual retrieval augmented in-context learning.\nBy strategically sourcing semantically similar prompts from high-resource\nlanguage, we enable multilingual pretrained language models (MPLMs), especially\nthe generative model BLOOMZ, to successfully boost performance on Bangla tasks.\nOur extensive evaluation highlights that the cross-lingual retrieval augmented\nprompts bring steady improvements to MPLMs over the zero-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_E/0/1/0/all/0/1\">Ercong Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Sheng Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Summarization with Normalizing Flows and Aggressive Training. (arXiv:2311.00588v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00588","description":"<p>This paper presents FlowSUM, a normalizing flows-based variational\nencoder-decoder framework for Transformer-based summarization. Our approach\ntackles two primary challenges in variational summarization: insufficient\nsemantic information in latent representations and posterior collapse during\ntraining. To address these challenges, we employ normalizing flows to enable\nflexible latent posterior modeling, and we propose a controlled alternate\naggressive training (CAAT) strategy with an improved gate mechanism.\nExperimental results show that FlowSUM significantly enhances the quality of\ngenerated summaries and unleashes the potential for knowledge distillation with\nminimal impact on inference time. Furthermore, we investigate the issue of\nposterior collapse in normalizing flows and analyze how the summary quality is\naffected by the training strategy, gate initialization, and the type and number\nof normalizing flows used, offering valuable insights for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaotong Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Formal Translation from Reversing Petri Nets to Coloured Petri Nets. (arXiv:2311.00629v1 [cs.LO])","link":"http://arxiv.org/abs/2311.00629","description":"<p>Reversible computation is an emerging computing paradigm that allows any\nsequence of operations to be executed in reverse order at any point during\ncomputation. Its appeal lies in its potential for lowpower computation and its\nrelevance to a wide array of applications such as chemical reactions, quantum\ncomputation, robotics, and distributed systems. Reversing Petri nets are a\nrecently-proposed extension of Petri nets that implements the three main forms\nof reversibility, namely, backtracking, causal reversing, and\nout-of-causal-order reversing. Their distinguishing feature is the use of named\ntokens that can be combined together to form bonds. Named tokens along with a\nhistory function, constitute the means of remembering past behaviour, thus,\nenabling reversal. In recent work, we have proposed a structural translation\nfrom a subclass of RPNs to the model of Coloured Petri Nets (CPNs), an\nextension of traditional Petri nets where tokens carry data values. In this\npaper, we extend the translation to handle RPNs with token multiplicity under\nthe individual-token interpretation, a model which allows multiple tokens of\nthe same type to exist in a system. To support the three types of\nreversibility, tokens are associated with their causal history and, while\ntokens of the same type are equally eligible to fire a transition when going\nforward, when going backwards they are able to reverse only the transitions\nthey have previously fired. The new translation, in addition to lifting the\nrestriction on token uniqueness, presents a refined approach for transforming\nRPNs to CPNs through a unifying approach that allows instantiating each of the\nthree types of reversibility. The paper also reports on a tool that implements\nthis translation, paving the way for automated translations and analysis of\nreversible systems using CPN Tools.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barylska_K/0/1/0/all/0/1\">Kamila Barylska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gogolinska_A/0/1/0/all/0/1\">Anna Gogolinska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikulski_L/0/1/0/all/0/1\">Lukasz Mikulski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philippou_A/0/1/0/all/0/1\">Anna Philippou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piatkowski_M/0/1/0/all/0/1\">Marcin Piatkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Psara_K/0/1/0/all/0/1\">Kyriaki Psara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explicit Morphological Knowledge Improves Pre-training of Language Models for Hebrew. (arXiv:2311.00658v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00658","description":"<p>Pre-trained language models (PLMs) have shown remarkable successes in\nacquiring a wide range of linguistic knowledge, relying solely on\nself-supervised training on text streams. Nevertheless, the effectiveness of\nthis language-agnostic approach has been frequently questioned for its\nsub-optimal performance when applied to morphologically-rich languages (MRLs).\nWe investigate the hypothesis that incorporating explicit morphological\nknowledge in the pre-training phase can improve the performance of PLMs for\nMRLs. We propose various morphologically driven tokenization methods enabling\nthe model to leverage morphological cues beyond raw text. We pre-train multiple\nlanguage models utilizing the different methods and evaluate them on Hebrew, a\nlanguage with complex and highly ambiguous morphology. Our experiments show\nthat morphologically driven tokenization demonstrates improved results compared\nto a standard language-agnostic tokenization, on a benchmark of both semantic\nand morphologic tasks. These findings suggest that incorporating morphological\nknowledge holds the potential for further improving PLMs for morphologically\nrich languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gueta_E/0/1/0/all/0/1\">Eylon Gueta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldman_O/0/1/0/all/0/1\">Omer Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion Detection for Misinformation: A Review. (arXiv:2311.00671v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00671","description":"<p>With the advent of social media, an increasing number of netizens are sharing\nand reading posts and news online. However, the huge volumes of misinformation\n(e.g., fake news and rumors) that flood the internet can adversely affect\npeople's lives, and have resulted in the emergence of rumor and fake news\ndetection as a hot research topic. The emotions and sentiments of netizens, as\nexpressed in social media posts and news, constitute important factors that can\nhelp to distinguish fake news from genuine news and to understand the spread of\nrumors. This article comprehensively reviews emotion-based methods for\nmisinformation detection. We begin by explaining the strong links between\nemotions and misinformation. We subsequently provide a detailed analysis of a\nrange of misinformation detection methods that employ a variety of emotion,\nsentiment and stance-based features, and describe their strengths and\nweaknesses. Finally, we discuss a number of ongoing challenges in emotion-based\nmisinformation detection based on large language models and suggest future\nresearch directions, including data collection (multi-platform, multilingual),\nannotation, benchmark, multimodality, and interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_P/0/1/0/all/0/1\">Paul Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zeping Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1\">Sophia Ananiadou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs. (arXiv:2311.00681v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00681","description":"<p>In recent years, Large Language Models (LLMs) have gained immense attention\ndue to their notable emergent capabilities, surpassing those seen in earlier\nlanguage models. A particularly intriguing application of LLMs is their role as\nevaluators for texts produced by various generative models.\n</p>\n<p>In this study, we delve into the potential of LLMs as reliable assessors of\nfactual consistency in summaries generated by text-generation models.\nInitially, we introduce an innovative approach for factuality assessment using\nLLMs. This entails employing a singular LLM for the entirety of the\nquestion-answering-based factuality scoring process. Following this, we examine\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\nagainst traditional measures and human annotations.\n</p>\n<p>Contrary to initial expectations, our results indicate a lack of significant\ncorrelations between factuality metrics and human evaluations, specifically for\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\ntwo factuality subcategories. These consistent findings across various factual\nerror categories suggest a fundamental limitation in the current LLMs'\ncapability to accurately gauge factuality.\n</p>\n<p>This version presents the information more concisely while maintaining the\nmain points and findings of the original text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xue-Yong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskar_M/0/1/0/all/0/1\">Md Tahmid Rahman Laskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+TN_S/0/1/0/all/0/1\">Shashi Bhushan TN</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation. (arXiv:2311.00684v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00684","description":"<p>An ideal length-extrapolatable Transformer language model can handle\nsequences longer than the training length without any long sequence\nfine-tuning. Such long-context utilization capability highly relies on a\nflexible positional embedding design. Upon investigating the flexibility of\nexisting large pre-trained Transformer language models, we find that the T5\nfamily deserves a closer look, as its positional embeddings capture rich and\nflexible attention patterns. However, T5 suffers from the dispersed attention\nissue: the longer the input sequence, the flatter the attention distribution.\nTo alleviate the issue, we propose two attention alignment strategies via\ntemperature scaling. Our findings improve the long-context utilization\ncapability of T5 on language modeling, retrieval, and multi-document question\nanswering without any fine-tuning, suggesting that a flexible positional\nembedding design and attention alignment go a long way toward Transformer\nlength\nextrapolation.\\footnote{\\url{https://github.com/chijames/Attention-Alignment-Transformer-Length-Extrapolation}}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_T/0/1/0/all/0/1\">Ta-Chung Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1\">Ting-Han Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudnicky_A/0/1/0/all/0/1\">Alexander I. Rudnicky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task. (arXiv:2311.00686v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00686","description":"<p>This paper describes and analyzes our participation in the 2023 Eval4NLP\nshared task, which focuses on assessing the effectiveness of prompt-based\ntechniques to empower Large Language Models to handle the task of quality\nestimation, particularly in the context of evaluating machine translations and\nsummaries. We conducted systematic experiments with various prompting\ntechniques, including standard prompting, prompts informed by annotator\ninstructions, and innovative chain-of-thought prompting. In addition, we\nintegrated these approaches with zero-shot and one-shot learning methods to\nmaximize the efficacy of our evaluation procedures. Our work reveals that\ncombining these approaches using a \"small\", open source model (orca_mini_v3_7B)\nyields competitive results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kotonya_N/0/1/0/all/0/1\">Neema Kotonya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnasamy_S/0/1/0/all/0/1\">Saran Krishnasamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tetreault_J/0/1/0/all/0/1\">Joel Tetreault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaimes_A/0/1/0/all/0/1\">Alejandro Jaimes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Interpersonal Communication by Simulating Audiences with Language Models. (arXiv:2311.00687v1 [cs.AI])","link":"http://arxiv.org/abs/2311.00687","description":"<p>How do we communicate with others to achieve our goals? We use our prior\nexperience or advice from others, or construct a candidate utterance by\npredicting how it will be received. However, our experiences are limited and\nbiased, and reasoning about potential outcomes can be difficult and cognitively\nchallenging. In this paper, we explore how we can leverage Large Language Model\n(LLM) simulations to help us communicate better. We propose the\nExplore-Generate-Simulate (EGS) framework, which takes as input any scenario\nwhere an individual is communicating to an audience with a goal they want to\nachieve. EGS (1) explores the solution space by producing a diverse set of\nadvice relevant to the scenario, (2) generates communication candidates\nconditioned on subsets of the advice, and (3) simulates the reactions from\nvarious audiences to determine both the best candidate and advice to use. We\nevaluate the framework on eight scenarios spanning the ten fundamental\nprocesses of interpersonal communication. For each scenario, we collect a\ndataset of human evaluations across candidates and baselines, and showcase that\nour framework's chosen candidate is preferred over popular generation\nmechanisms including Chain-of-Thought. We also find that audience simulations\nachieve reasonably high agreement with human raters across 5 of the 8\nscenarios. Finally, we demonstrate the generality of our framework by applying\nit to real-world scenarios described by users on web forums. Through\nevaluations and demonstrations, we show that EGS enhances the effectiveness and\noutcomes of goal-oriented communication across a variety of situations, thus\nopening up new possibilities for the application of large language models in\nrevolutionizing communication and decision-making processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ryan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_H/0/1/0/all/0/1\">Howard Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marjieh_R/0/1/0/all/0/1\">Raja Marjieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving. (arXiv:2311.00694v1 [cs.AI])","link":"http://arxiv.org/abs/2311.00694","description":"<p>Large Language Models (LLMs) have achieved tremendous progress, yet they\nstill often struggle with challenging reasoning problems. Current approaches\naddress this challenge by sampling or searching detailed and low-level\nreasoning chains. However, these methods are still limited in their exploration\ncapabilities, making it challenging for correct solutions to stand out in the\nhuge solution space. In this work, we unleash LLMs' creative potential for\nexploring multiple diverse problem solving strategies by framing an LLM as a\nhierarchical policy via in-context learning. This policy comprises of a\nvisionary leader that proposes multiple diverse high-level problem-solving\ntactics as hints, accompanied by a follower that executes detailed\nproblem-solving processes following each of the high-level instruction. The\nfollower uses each of the leader's directives as a guide and samples multiple\nreasoning chains to tackle the problem, generating a solution group for each\nleader proposal. Additionally, we propose an effective and efficient\ntournament-based approach to select among these explored solution groups to\nreach the final answer. Our approach produces meaningful and inspiring hints,\nenhances problem-solving strategy exploration, and improves the final answer\naccuracy on challenging problems in the MATH dataset. Code will be released at\nhttps://github.com/lz1oceani/LLM-As-Hierarchical-Policy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yunhao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1\">Tongzhou Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Mingu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pourreza_R/0/1/0/all/0/1\">Reza Pourreza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Memisevic_R/0/1/0/all/0/1\">Roland Memisevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Single-Channel Speaker-Turn Aware Conversational Speech Translation. (arXiv:2311.00697v1 [cs.CL])","link":"http://arxiv.org/abs/2311.00697","description":"<p>Conventional speech-to-text translation (ST) systems are trained on\nsingle-speaker utterances, and they may not generalize to real-life scenarios\nwhere the audio contains conversations by multiple speakers. In this paper, we\ntackle single-channel multi-speaker conversational ST with an end-to-end and\nmulti-task training model, named Speaker-Turn Aware Conversational Speech\nTranslation, that combines automatic speech recognition, speech translation and\nspeaker turn detection using special tokens in a serialized labeling format. We\nrun experiments on the Fisher-CALLHOME corpus, which we adapted by merging the\ntwo single-speaker channels into one multi-speaker channel, thus representing\nthe more realistic and challenging scenario with multi-speaker turns and\ncross-talk. Experimental results across single- and multi-speaker conditions\nand against conventional ST systems, show that our model outperforms the\nreference systems on the multi-speaker condition, while attaining comparable\nperformance on the single-speaker condition. We release scripts for data\nprocessing and model training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhaocheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paturi_R/0/1/0/all/0/1\">Rohit Paturi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Sundararajan Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_P/0/1/0/all/0/1\">Prashant Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_B/0/1/0/all/0/1\">Brian Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federico_M/0/1/0/all/0/1\">Marcello Federico</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SegAugment: Maximizing the Utility of Speech Translation Data with Segmentation-based Augmentations. (arXiv:2212.09699v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09699","description":"<p>End-to-end Speech Translation is hindered by a lack of available data\nresources. While most of them are based on documents, a sentence-level version\nis available, which is however single and static, potentially impeding the\nusefulness of the data. We propose a new data augmentation strategy,\nSegAugment, to address this issue by generating multiple alternative\nsentence-level versions of a dataset. Our method utilizes an Audio Segmentation\nsystem, which re-segments the speech of each document with different length\nconstraints, after which we obtain the target text via alignment methods.\nExperiments demonstrate consistent gains across eight language pairs in MuST-C,\nwith an average increase of 2.5 BLEU points, and up to 5 BLEU for low-resource\nscenarios in mTEDx. Furthermore, when combined with a strong system, SegAugment\nestablishes new state-of-the-art results in MuST-C. Finally, we show that the\nproposed method can also successfully augment sentence-level datasets, and that\nit enables Speech Translation models to close the gap between the manual and\nautomatic segmentation at inference time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsiamas_I/0/1/0/all/0/1\">Ioannis Tsiamas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonollosa_J/0/1/0/all/0/1\">Jos&#xe9; A. R. Fonollosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain. (arXiv:2301.13126v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.13126","description":"<p>Lately, propelled by the phenomenal advances around the transformer\narchitecture, the legal NLP field has enjoyed spectacular growth. To measure\nprogress, well curated and challenging benchmarks are crucial. However, most\nbenchmarks are English only and in legal NLP specifically there is no\nmultilingual benchmark available yet. Additionally, many benchmarks are\nsaturated, with the best models clearly outperforming the best humans and\nachieving near perfect scores. We survey the legal NLP literature and select 11\ndatasets covering 24 languages, creating LEXTREME. To provide a fair\ncomparison, we propose two aggregate scores, one based on the datasets and one\non the languages. The best baseline (XLM-R large) achieves both a dataset\naggregate score a language aggregate score of 61.3. This indicates that\nLEXTREME is still very challenging and leaves ample room for improvement. To\nmake it easy for researchers and practitioners to use, we release LEXTREME on\nhuggingface together with all the code required to evaluate models and a public\nWeights and Biases project with all the runs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niklaus_J/0/1/0/all/0/1\">Joel Niklaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matoshi_V/0/1/0/all/0/1\">Veton Matoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rani_P/0/1/0/all/0/1\">Pooja Rani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galassi_A/0/1/0/all/0/1\">Andrea Galassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sturmer_M/0/1/0/all/0/1\">Matthias St&#xfc;rmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Concept Algebra for (Score-Based) Text-Controlled Generative Models. (arXiv:2302.03693v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.03693","description":"<p>This paper concerns the structure of learned representations in text-guided\ngenerative models, focusing on score-based models. A key property of such\nmodels is that they can compose disparate concepts in a `disentangled' manner.\nThis suggests these models have internal representations that encode concepts\nin a `disentangled' manner. Here, we focus on the idea that concepts are\nencoded as subspaces of some representation space. We formalize what this\nmeans, show there's a natural choice for the representation, and develop a\nsimple method for identifying the part of the representation corresponding to a\ngiven concept. In particular, this allows us to manipulate the concepts\nexpressed by the model through algebraic manipulation of the representation. We\ndemonstrate the idea with examples using Stable Diffusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1\">Lin Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negrea_J/0/1/0/all/0/1\">Jeffrey Negrea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veitch_V/0/1/0/all/0/1\">Victor Veitch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KPEval: Towards Fine-grained Semantic-based Evaluation of Keyphrase Extraction and Generation Systems. (arXiv:2303.15422v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.15422","description":"<p>Despite the significant advancements in keyphrase extraction and keyphrase\ngeneration methods, the predominant approach for evaluation only relies on\nexact matching with human references and disregards reference-free attributes.\nThis scheme fails to recognize systems that generate keyphrases semantically\nequivalent to the references or diverse keyphrases that carry practical\nutility. To better assess the capability of keyphrase systems, we propose\nKPEval, a comprehensive evaluation framework consisting of four critical\ndimensions: saliency, faithfulness, diversity, and utility. For each dimension,\nwe design semantic-based metrics that align with the evaluation objectives.\nMeta-evaluation studies demonstrate that our evaluation strategy correlates\nbetter with human preferences compared to a range of previously used metrics.\nUsing this framework, we re-evaluate 20 keyphrase systems and further discover\nthat (1) the best model differs depending on the evaluation dimension; (2) the\nutility in downstream tasks does not always correlate with reference-based\nmetrics; and (3) large language models like GPT-3.5 exhibit a strong\nperformance under reference-free evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Da Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-step Jailbreaking Privacy Attacks on ChatGPT. (arXiv:2304.05197v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.05197","description":"<p>With the rapid progress of large language models (LLMs), many downstream NLP\ntasks can be well solved given appropriate prompts. Though model developers and\nresearchers work hard on dialog safety to avoid generating harmful content from\nLLMs, it is still challenging to steer AI-generated content (AIGC) for the\nhuman good. As powerful LLMs are devouring existing text data from various\ndomains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether\nthe private information is included in the training data and what privacy\nthreats can these LLMs and their downstream applications bring. In this paper,\nwe study the privacy threats from OpenAI's ChatGPT and the New Bing enhanced by\nChatGPT and show that application-integrated LLMs may cause new privacy\nthreats. To this end, we conduct extensive experiments to support our claims\nand discuss LLMs' privacy implications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Dadi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingshi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fanpu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCENE: Self-Labeled Counterfactuals for Extrapolating to Negative Examples. (arXiv:2305.07984v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.07984","description":"<p>Detecting negatives (such as non-entailment relationships, unanswerable\nquestions, and false claims) is an important and challenging aspect of many\nnatural language understanding tasks. Though manually collecting challenging\nnegative examples can help models detect them, it is both costly and\ndomain-specific. In this work, we propose Self-labeled Counterfactuals for\nExtrapolating to Negative Examples (SCENE), an automatic method for\nsynthesizing training data that greatly improves models' ability to detect\nchallenging negative examples. In contrast with standard data augmentation,\nwhich synthesizes new examples for existing labels, SCENE can synthesize\nnegative examples zero-shot from only positive ones. Given a positive example,\nSCENE perturbs it with a mask infilling model, then determines whether the\nresulting example is negative based on a self-training heuristic. With access\nto only answerable training examples, SCENE can close 69.6% of the performance\ngap on SQuAD 2.0, a dataset where half of the evaluation examples are\nunanswerable, compared to a model trained on SQuAD 2.0. Our method also extends\nto boolean question answering and recognizing textual entailment, and improves\ngeneralization from SQuAD to ACE-whQA, an out-of-domain extractive QA\nbenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1\">Deqing Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godbole_A/0/1/0/all/0/1\">Ameya Godbole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization. (arXiv:2305.08503v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08503","description":"<p>Pre-trained language models (PLMs) have achieved outstanding achievements in\nabstractive single-document summarization (SDS). However, such benefits may not\nfully extend to multi-document summarization (MDS), where the handling of\ncross-document information is more complex. Previous works either design new\nMDS architectures or apply PLMs bluntly with concatenated source documents as a\nreformulated SDS task. While the former does not utilize previous pre-training\nefforts and may not generalize well across different domains, the latter may\nnot sufficiently attend to the intricate cross-document relationships unique to\nMDS tasks. Instead, we enforce hierarchy on both the encoder and decoder to\nbetter utilize a PLM to facilitate multi-document interactions for the MDS\ntask. Across 10 MDS benchmarks from various domains, our method outperforms or\nis competitive with the previous best models, including those with additional\nMDS pre-training or with more parameters. It outperforms its corresponding PLM\nbackbone by up to 3 Rouge-L and is favored by humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chenhui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1\">Xuan-Phi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Increasing Probability Mass on Answer Choices Does Not Always Improve Accuracy. (arXiv:2305.14596v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14596","description":"<p>When pretrained language models (LMs) are applied to discriminative tasks\nsuch as multiple-choice questions, they place probability mass on vocabulary\ntokens that aren't among the given answer choices. Spreading probability mass\nacross multiple surface forms with identical meaning (such as \"bath\" and\n\"bathtub\") is thought to cause an underestimation of a model's true\nperformance, referred to as the \"surface form competition\" (SFC) hypothesis.\nThis has motivated the introduction of various probability normalization\nmethods. However, many core questions remain unanswered. How do we measure SFC?\nAre there direct ways of reducing it, and does doing so improve task\nperformance?\n</p>\n<p>We propose a mathematical formalism for SFC which allows us to quantify and\nbound its impact for the first time. We identify a simple method for reducing\nit -- namely, increasing probability mass on the given answer choices by a)\nincluding them in the prompt and b) using in-context learning with even just\none example. We show this method eliminates the impact of SFC in the majority\nof instances. Our experiments on three diverse datasets and six LMs reveal\nseveral additional surprising findings. For example, both normalization and\nprompting methods for reducing SFC can be ineffective or even detrimental to\ntask performance for some LMs. We conclude with practical insights for\neffectively prompting LMs for multiple-choice tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wiegreffe_S/0/1/0/all/0/1\">Sarah Wiegreffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finlayson_M/0/1/0/all/0/1\">Matthew Finlayson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tafjord_O/0/1/0/all/0/1\">Oyvind Tafjord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts. (arXiv:2306.04723v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.04723","description":"<p>Rapidly increasing quality of AI-generated content makes it difficult to\ndistinguish between human and AI-generated texts, which may lead to undesirable\nconsequences for society. Therefore, it becomes increasingly important to study\nthe properties of human texts that are invariant over different text domains\nand varying proficiency of human writers, can be easily calculated for any\nlanguage, and can robustly separate natural and AI-generated texts regardless\nof the generation model and sampling method. In this work, we propose such an\ninvariant for human-written texts, namely the intrinsic dimensionality of the\nmanifold underlying the set of embeddings for a given text sample. We show that\nthe average intrinsic dimensionality of fluent texts in a natural language is\nhovering around the value $9$ for several alphabet-based languages and around\n$7$ for Chinese, while the average intrinsic dimensionality of AI-generated\ntexts for each language is $\\approx 1.5$ lower, with a clear statistical\nseparation between human-generated and AI-generated distributions. This\nproperty allows us to build a score-based artificial text detector. The\nproposed detector's accuracy is stable over text domains, generator models, and\nhuman writer proficiency levels, outperforming SOTA detectors in model-agnostic\nand cross-domain scenarios by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tulchinskii_E/0/1/0/all/0/1\">Eduard Tulchinskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_K/0/1/0/all/0/1\">Kristian Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kushnareva_L/0/1/0/all/0/1\">Laida Kushnareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherniavskii_D/0/1/0/all/0/1\">Daniil Cherniavskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1\">Serguei Barannikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1\">Irina Piontkovskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolenko_S/0/1/0/all/0/1\">Sergey Nikolenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CREHate: A CRoss-cultural English Hate Speech Dataset. (arXiv:2308.16705v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.16705","description":"<p>Most NLP datasets neglect the cultural diversity among language speakers,\nresulting in a critical shortcoming in hate speech detection and other\nculturally sensitive tasks. To address this, we introduce CREHate, a\nCRoss-cultural English Hate speech dataset. To construct CREHate, we follow a\ntwo-step procedure: 1) culture-specific post collection and 2) cross-cultural\nannotation. We sample posts from the SBIC dataset, which predominantly\nrepresents North America, and collect posts from four geographically diverse\nEnglish-speaking countries using culture-specific hate speech keywords that we\nretrieve from our survey. Annotations are then collected from those four\nEnglish-speaking countries plus the US to establish representative labels for\neach country. Our analysis highlights statistically significant disparities in\ncross-cultural hate speech annotations. Only 56.2% of the posts in CREHate\nachieve consensus among all five countries, with a peak pairwise disagreement\nrate of 26%. The annotations show that label disagreements tend to come from\nthe inherent cultural context, subjectivity, and ambiguity of the posts.\nLastly, we develop cross-cultural hate speech classifiers that are more\naccurate at predicting each country's labels than the monocultural classifiers.\nThis confirms the utility of CREHate for constructing culturally sensitive hate\nspeech classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1\">Chani Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myung_J/0/1/0/all/0/1\">Junho Myung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jiho Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Juho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YaRN: Efficient Context Window Extension of Large Language Models. (arXiv:2309.00071v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.00071","description":"<p>Rotary Position Embeddings (RoPE) have been shown to effectively encode\npositional information in transformer-based language models. However, these\nmodels fail to generalize past the sequence length they were trained on. We\npresent YaRN (Yet another RoPE extensioN method), a compute-efficient method to\nextend the context window of such models, requiring 10x less tokens and 2.5x\nless training steps than previous methods. Using YaRN, we show that LLaMA\nmodels can effectively utilize and extrapolate to context lengths much longer\nthan their original pre-training would allow, while also surpassing previous\nthe state-of-the-art at context window extension. In addition, we demonstrate\nthat YaRN exhibits the capability to extrapolate beyond the limited context of\na fine-tuning dataset. The models fine-tuned using YaRN has been made available\nand reproduced online up to 128k context length at\nhttps://github.com/jquesnelle/yarn\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Bowen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quesnelle_J/0/1/0/all/0/1\">Jeffrey Quesnelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Honglu Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shippole_E/0/1/0/all/0/1\">Enrico Shippole</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised Learning and Large Language Model Benchmarks on Mental Health Datasets: Cognitive Distortions and Suicidal Risks in Chinese Social Media. (arXiv:2309.03564v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.03564","description":"<p>In the realm of social media, users frequently convey personal sentiments,\nwith some potentially indicating cognitive distortions or suicidal tendencies.\nTimely recognition of such signs is pivotal for effective interventions. In\nresponse, we introduce two novel annotated datasets from Chinese social media,\nfocused on cognitive distortions and suicidal risk classification. We propose a\ncomprehensive benchmark using both supervised learning and large language\nmodels, especially from the GPT series, to evaluate performance on these\ndatasets. To assess the capabilities of the large language models, we employed\nthree strategies: zero-shot, few-shot, and fine-tuning. Furthermore, we deeply\nexplored and analyzed the performance of these large language models from a\npsychological perspective, shedding light on their strengths and limitations in\nidentifying and understanding complex human emotions. Our evaluations\nunderscore a performance difference between the two approaches, with the models\noften challenged by subtle category distinctions. While GPT-4 consistently\ndelivered strong results, GPT-3.5 showed marked improvement in suicide risk\nclassification after fine-tuning. This research is groundbreaking in its\nevaluation of large language models for Chinese social media tasks,\naccentuating the models' potential in psychological contexts. All datasets and\ncode are made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Hongzhi Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Changwei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1\">Wei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1\">Dan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yi Jing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1\">Huijing Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bing Xiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1\">Guanghui Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code Soliloquies for Accurate Calculations in Large Language Models. (arXiv:2309.12161v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.12161","description":"<p>High-quality conversational datasets are crucial for the successful\ndevelopment of Intelligent Tutoring Systems (ITS) that utilize a Large Language\nModel (LLM) backend. Synthetic student-teacher dialogues, generated using\nadvanced GPT-4 models, are a common strategy for creating these datasets.\nHowever, subjects like physics that entail complex calculations pose a\nchallenge. While GPT-4 presents impressive language processing capabilities,\nits limitations in fundamental mathematical reasoning curtail its efficacy for\nsuch subjects. To tackle this limitation, we introduce in this paper an\ninnovative stateful prompt design. Our design orchestrates a mock conversation\nwhere both student and tutorbot roles are simulated by GPT-4. Each student\nresponse triggers an internal monologue, or `code soliloquy' in the\nGPT-tutorbot, which assesses whether its subsequent response would necessitate\ncalculations. If a calculation is deemed necessary, it scripts the relevant\nPython code and uses the Python output to construct a response to the student.\nOur approach notably enhances the quality of synthetic conversation datasets,\nespecially for subjects that are calculation-intensive. Our preliminary Subject\nMatter Expert evaluations reveal that our Higgs model, a fine-tuned LLaMA\nmodel, effectively uses Python for computations, which significantly enhances\nthe accuracy and computational reliability of Higgs' responses. Code, models,\nand datasets is available at https://github.com/luffycodes/Tutorbot-Spock-Phys.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sonkar_S/0/1/0/all/0/1\">Shashank Sonkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1\">MyCo Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinghe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Naiming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallick_D/0/1/0/all/0/1\">Debshila Basu Mallick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard G. Baraniuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Anonymization of Swiss Federal Supreme Court Rulings. (arXiv:2310.04632v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.04632","description":"<p>Releasing court decisions to the public relies on proper anonymization to\nprotect all involved parties, where necessary. The Swiss Federal Supreme Court\nrelies on an existing system that combines different traditional computational\nmethods with human experts. In this work, we enhance the existing anonymization\nsoftware using a large dataset annotated with entities to be anonymized. We\ncompared BERT-based models with models pre-trained on in-domain data. Our\nresults show that using in-domain data to pre-train the models further improves\nthe F1-score by more than 5\\% compared to existing models. Our work\ndemonstrates that combining existing anonymization methods, such as regular\nexpressions, with machine learning can further reduce manual labor and enhance\nautomatic suggestions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niklaus_J/0/1/0/all/0/1\">Joel Niklaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamie_R/0/1/0/all/0/1\">Robin Mami&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sturmer_M/0/1/0/all/0/1\">Matthias St&#xfc;rmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunner_D/0/1/0/all/0/1\">Daniel Brunner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gygli_M/0/1/0/all/0/1\">Marcel Gygli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. (arXiv:2310.05492v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.05492","description":"<p>Large language models (LLMs) with enormous pre-training tokens and parameter\namounts emerge abilities, including math reasoning, code generation, and\ninstruction following. These abilities are further enhanced by supervised\nfine-tuning (SFT). The open-source community has studied on ad-hoc SFT for each\nability, while proprietary LLMs are versatile for all abilities. It is\nimportant to investigate how to unlock them with multiple abilities via SFT. In\nthis study, we specifically focus on the data composition between mathematical\nreasoning, code generation, and general human-aligning abilities during SFT.\nFrom a scaling perspective, we investigate the relationship between model\nabilities and various factors including data amounts, data composition ratio,\nmodel parameters, and SFT strategies. Our experiments reveal that different\nabilities exhibit different scaling patterns, and larger models generally show\nsuperior performance with the same amount of data. Mathematical reasoning and\ncode generation improve as data amounts increase consistently, while the\ngeneral ability is enhanced with about a thousand samples and improves slowly.\nWe find data composition results in various abilities improvements with low\ndata amounts, while conflicts of abilities with high data amounts. Our\nexperiments further show that composition data amount impacts performance,\nwhile the influence of composition ratio is insignificant. Regarding the SFT\nstrategies, we evaluate sequential learning multiple abilities are prone to\ncatastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT)\nstrategy learns specialized abilities first and then learns general abilities\nwith a small amount of specialized data to prevent forgetting, offering a\npromising solution to learn multiple abilities with different scaling patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guanting Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongyi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Keming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1\">Mingfeng Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dayiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization. (arXiv:2310.05506v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.05506","description":"<p>In math reasoning with large language models (LLMs), fine-tuning data\naugmentation by query evolution and diverse reasoning paths is empirically\nverified effective, profoundly narrowing the gap between open-sourced LLMs and\ncutting-edge proprietary LLMs. In this paper, we conduct an investigation for\nsuch data augmentation in math reasoning and are intended to answer: (1) What\nstrategies of data augmentation are more effective; (2) What is the scaling\nrelationship between the amount of augmented data and model performance; and\n(3) Can data augmentation incentivize generalization to out-of-domain\nmathematical reasoning tasks? To this end, we create a new dataset, AugGSM8K,\nby complicating and diversifying the queries from GSM8K and sampling multiple\nreasoning paths. We obtained a series of LLMs called MuggleMath by fine-tuning\non subsets of AugGSM8K. MuggleMath substantially achieves new state-of-the-art\non GSM8K (from 54% to 68.4% at the scale of 7B, and from 63.9% to 74.0% at the\nscale of 13B). A log-linear relationship is presented between MuggleMath's\nperformance and the amount of augmented data. We also find that MuggleMath is\nweak in out-of-domain math reasoning generalization to MATH. This is attributed\nto the differences in query distribution between AugGSM8K and MATH which\nsuggest that augmentation on a single benchmark could not help with overall\nmath reasoning performance. Codes and AugGSM8K will be uploaded to\nhttps://github.com/OFA-Sys/gsm8k-ScRel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongyi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guanting Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Keming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiancan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Kelly is a Warm Person, Joseph is a Role Model\": Gender Biases in LLM-Generated Reference Letters. (arXiv:2310.09219v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.09219","description":"<p>Large Language Models (LLMs) have recently emerged as an effective tool to\nassist individuals in writing various types of content, including professional\ndocuments such as recommendation letters. Though bringing convenience, this\napplication also introduces unprecedented fairness concerns. Model-generated\nreference letters might be directly used by users in professional scenarios. If\nunderlying biases exist in these model-constructed letters, using them without\nscrutinization could lead to direct societal harms, such as sabotaging\napplication success rates for female applicants. In light of this pressing\nissue, it is imminent and necessary to comprehensively study fairness issues\nand associated harms in this real-world use case. In this paper, we critically\nexamine gender biases in LLM-generated reference letters. Drawing inspiration\nfrom social science findings, we design evaluation methods to manifest biases\nthrough 2 dimensions: (1) biases in language style and (2) biases in lexical\ncontent. We further investigate the extent of bias propagation by analyzing the\nhallucination bias of models, a term that we define to be bias exacerbation in\nmodel-hallucinated contents. Through benchmarking evaluation on 2 popular LLMs-\nChatGPT and Alpaca, we reveal significant gender biases in LLM-generated\nrecommendation letters. Our findings not only warn against using LLMs for this\napplication without scrutinization, but also illuminate the importance of\nthoroughly studying hidden biases and harms in LLM-generated professional\ndocuments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yixin Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1\">George Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garimella_A/0/1/0/all/0/1\">Aparna Garimella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sociotechnical Safety Evaluation of Generative AI Systems. (arXiv:2310.11986v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2310.11986","description":"<p>Generative AI systems produce a range of risks. To ensure the safety of\ngenerative AI systems, these risks must be evaluated. In this paper, we make\ntwo main contributions toward establishing such evaluations. First, we propose\na three-layered framework that takes a structured, sociotechnical approach to\nevaluating these risks. This framework encompasses capability evaluations,\nwhich are the main current approach to safety evaluation. It then reaches\nfurther by building on system safety principles, particularly the insight that\ncontext determines whether a given capability may cause harm. To account for\nrelevant context, our framework adds human interaction and systemic impacts as\nadditional layers of evaluation. Second, we survey the current state of safety\nevaluation of generative AI systems and create a repository of existing\nevaluations. Three salient evaluation gaps emerge from this analysis. We\npropose ways forward to closing these gaps, outlining practical steps as well\nas roles and responsibilities for different actors. Sociotechnical safety\nevaluation is a tractable approach to the robust and comprehensive safety\nevaluation of generative AI systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weidinger_L/0/1/0/all/0/1\">Laura Weidinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rauh_M/0/1/0/all/0/1\">Maribeth Rauh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchal_N/0/1/0/all/0/1\">Nahema Marchal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manzini_A/0/1/0/all/0/1\">Arianna Manzini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendricks_L/0/1/0/all/0/1\">Lisa Anne Hendricks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mateos_Garcia_J/0/1/0/all/0/1\">Juan Mateos-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergman_S/0/1/0/all/0/1\">Stevie Bergman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kay_J/0/1/0/all/0/1\">Jackie Kay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffin_C/0/1/0/all/0/1\">Conor Griffin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bariach_B/0/1/0/all/0/1\">Ben Bariach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabriel_I/0/1/0/all/0/1\">Iason Gabriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1\">Verena Rieser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isaac_W/0/1/0/all/0/1\">William Isaac</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dataset Bias Mitigation in Multiple-Choice Visual Question Answering and Beyond. (arXiv:2310.14670v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2310.14670","description":"<p>Vision-language (VL) understanding tasks evaluate models' comprehension of\ncomplex visual scenes through multiple-choice questions. However, we have\nidentified two dataset biases that models can exploit as shortcuts to resolve\nvarious VL tasks correctly without proper understanding. The first type of\ndataset bias is \\emph{Unbalanced Matching} bias, where the correct answer\noverlaps the question and image more than the incorrect answers. The second\ntype of dataset bias is \\emph{Distractor Similarity} bias, where incorrect\nanswers are overly dissimilar to the correct answer but significantly similar\nto other incorrect answers within the same sample. To address these dataset\nbiases, we first propose Adversarial Data Synthesis (ADS) to generate synthetic\ntraining and debiased evaluation data. We then introduce Intra-sample\nCounterfactual Training (ICT) to assist models in utilizing the synthesized\ntraining data, particularly the counterfactual data, via focusing on\nintra-sample differentiation. Extensive experiments demonstrate the\neffectiveness of ADS and ICT in consistently improving model performance across\ndifferent benchmarks, even in domain-shifted scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhecan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1\">Haoxuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Keyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yicheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Codella_N/0/1/0/all/0/1\">Noel Codella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3M-TRANSFORMER: A Multi-Stage Multi-Stream Multimodal Transformer for Embodied Turn-Taking Prediction. (arXiv:2310.14859v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2310.14859","description":"<p>Predicting turn-taking in multiparty conversations has many practical\napplications in human-computer/robot interaction. However, the complexity of\nhuman communication makes it a challenging task. Recent advances have shown\nthat synchronous multi-perspective egocentric data can significantly improve\nturn-taking prediction compared to asynchronous, single-perspective\ntranscriptions. Building on this research, we propose a new multimodal\ntransformer-based architecture for predicting turn-taking in embodied,\nsynchronized multi-perspective data. Our experimental results on the recently\nintroduced EgoCom dataset show a substantial performance improvement of up to\n14.01% on average compared to existing baselines and alternative\ntransformer-based approaches. The source code, and the pre-trained models of\nour 3M-Transformer will be available upon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fatan_M/0/1/0/all/0/1\">Mehdi Fatan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mincato_E/0/1/0/all/0/1\">Emanuele Mincato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pintzou_D/0/1/0/all/0/1\">Dimitra Pintzou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimiccoli_M/0/1/0/all/0/1\">Mariella Dimiccoli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations. (arXiv:2310.15431v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.15431","description":"<p>Moral or ethical judgments rely heavily on the specific contexts in which\nthey occur. Understanding varying shades of defeasible contextualizations\n(i.e., additional information that strengthens or attenuates the moral\nacceptability of an action) is critical to accurately represent the subtlety\nand intricacy of grounded human moral judgment in real-life scenarios.\n</p>\n<p>We introduce defeasible moral reasoning: a task to provide grounded contexts\nthat make an action more or less morally acceptable, along with commonsense\nrationales that justify the reasoning. To elicit high-quality task data, we\ntake an iterative self-distillation approach that starts from a small amount of\nunstructured seed knowledge from GPT-3 and then alternates between (1)\nself-distillation from student models; (2) targeted filtering with a critic\nmodel trained by human judgment (to boost validity) and NLI (to boost\ndiversity); (3) self-imitation learning (to amplify the desired data quality).\nThis process yields a student model that produces defeasible contexts with\nimproved validity, diversity, and defeasibility. From this model we distill a\nhigh-quality dataset, \\delta-Rules-of-Thumb, of 1.2M entries of\ncontextualizations and rationales for 115K defeasible moral actions rated\nhighly by human annotators 85.9% to 99.8% of the time. Using \\delta-RoT we\nobtain a final student model that wins over all intermediate student models by\na notable margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1\">Kavel Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1\">Valentina Pyatkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuling Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1\">Niket Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1\">Nouha Dziri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Translation for Nko: Tools, Corpora and Baseline Results. (arXiv:2310.15612v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.15612","description":"<p>Currently, there is no usable machine translation system for Nko\n\\footnote{Also spelled N'Ko, but speakers prefer the name Nko.}, a language\nspoken by tens of millions of people across multiple West African countries,\nwhich holds significant cultural and educational value.\n</p>\n<p>To address this issue, we present a set of tools, resources, and baseline\nresults aimed towards the development of usable machine translation systems for\nNko and other languages that do not currently have sufficiently large parallel\ntext corpora available.\n</p>\n<p>(1) Fria$\\parallel$el: A novel collaborative parallel text curation software\nthat incorporates quality control through copyedit-based workflows. (2)\nExpansion of the FLoRes-200 and NLLB-Seed corpora with 2,009 and 6,193\nhigh-quality Nko translations in parallel with 204 and 40 other languages. (3)\nnicolingua-0005: A collection of trilingual and bilingual corpora with 130,850\nparallel segments and monolingual corpora containing over 3 million Nko words.\n(4) Baseline bilingual and multilingual neural machine translation results with\nthe best model scoring 30.83 English-Nko chrF++ on FLoRes-devtest.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doumbouya_M/0/1/0/all/0/1\">Moussa Koulako Bala Doumbouya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diane_B/0/1/0/all/0/1\">Baba Mamadi Dian&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cisse_S/0/1/0/all/0/1\">Solo Farabado Ciss&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diane_D/0/1/0/all/0/1\">Djibrila Dian&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sow_A/0/1/0/all/0/1\">Abdoulaye Sow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doumbouya_S/0/1/0/all/0/1\">S&#xe9;r&#xe9; Moussa Doumbouya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bangoura_D/0/1/0/all/0/1\">Daouda Bangoura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayo_F/0/1/0/all/0/1\">Fod&#xe9; Moriba Bayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conde_I/0/1/0/all/0/1\">Ibrahima Sory 2. Cond&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diane_K/0/1/0/all/0/1\">Kalo Mory Dian&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piech_C/0/1/0/all/0/1\">Chris Piech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher Manning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language. (arXiv:2310.17306v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2310.17306","description":"<p>Formatting is an important property in tables for visualization,\npresentation, and analysis. Spreadsheet software allows users to automatically\nformat their tables by writing data-dependent conditional formatting (CF)\nrules. Writing such rules is often challenging for users as it requires them to\nunderstand and implement the underlying logic. We present FormaT5, a\ntransformer-based model that can generate a CF rule given the target table and\na natural language description of the desired formatting logic. We find that\nuser descriptions for these tasks are often under-specified or ambiguous,\nmaking it harder for code generation systems to accurately learn the desired\nrule in a single step. To tackle this problem of under-specification and\nminimise argument errors, FormaT5 learns to predict placeholders though an\nabstention objective. These placeholders can then be filled by a second model\nor, when examples of rows that should be formatted are available, by a\nprogramming-by-example system. To evaluate FormaT5 on diverse and real\nscenarios, we create an extensive benchmark of 1053 CF tasks, containing\nreal-world descriptions collected from four different sources. We release our\nbenchmarks to encourage research in this area. Abstention and filling allow\nFormaT5 to outperform 8 different neural approaches on our benchmarks, both\nwith and without examples. Our results illustrate the value of building\ndomain-specific learning systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mukul Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambronero_J/0/1/0/all/0/1\">Jos&#xe9; Cambronero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulwani_S/0/1/0/all/0/1\">Sumit Gulwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1\">Vu Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negreanu_C/0/1/0/all/0/1\">Carina Negreanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nouri_E/0/1/0/all/0/1\">Elnaz Nouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raza_M/0/1/0/all/0/1\">Mohammad Raza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verbruggen_G/0/1/0/all/0/1\">Gust Verbruggen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeFusion: A Pre-trained Diffusion Model for Code Generation. (arXiv:2310.17680v3 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2310.17680","description":"<p>Imagine a developer who can only change their last line of code, how often\nwould they have to start writing a function from scratch before it is correct?\nAuto-regressive models for code generation from natural language have a similar\nlimitation: they do not easily allow reconsidering earlier tokens generated. We\nintroduce CodeFusion, a pre-trained diffusion code generation model that\naddresses this limitation by iteratively denoising a complete program\nconditioned on the encoded natural language. We evaluate CodeFusion on the task\nof natural language to code generation for Bash, Python, and Microsoft Excel\nconditional formatting (CF) rules. Experiments show that CodeFusion (75M\nparameters) performs on par with state-of-the-art auto-regressive systems\n(350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and\ntop-5 accuracy due to its better balance in diversity versus quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mukul Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambronero_J/0/1/0/all/0/1\">Jos&#xe9; Cambronero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulwani_S/0/1/0/all/0/1\">Sumit Gulwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1\">Vu Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negreanu_C/0/1/0/all/0/1\">Carina Negreanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verbruggen_G/0/1/0/all/0/1\">Gust Verbruggen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare. (arXiv:2310.17956v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2310.17956","description":"<p>Large Language Models (LLMs) have introduced a new era of proficiency in\ncomprehending complex healthcare and biomedical topics. However, there is a\nnoticeable lack of models in languages other than English and models that can\ninterpret multi-modal input, which is crucial for global healthcare\naccessibility. In response, this study introduces Qilin-Med-VL, the first\nChinese large vision-language model designed to integrate the analysis of\ntextual and visual data. Qilin-Med-VL combines a pre-trained Vision Transformer\n(ViT) with a foundational LLM. It undergoes a thorough two-stage curriculum\ntraining process that includes feature alignment and instruction tuning. This\nmethod enhances the model's ability to generate medical captions and answer\ncomplex medical queries. We also release ChiMed-VL, a dataset consisting of\nmore than 1M image-text pairs. This dataset has been carefully curated to\nenable detailed and comprehensive interpretation of medical data using various\ntypes of images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qichen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_D/0/1/0/all/0/1\">Dading Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Peilin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yining Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CXR-LLaVA: Multimodal Large Language Model for Interpreting Chest X-ray Images. (arXiv:2310.18341v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.18341","description":"<p>Purpose: Recent advancements in large language models (LLMs) have expanded\ntheir capabilities in a multimodal fashion, potentially replicating the image\ninterpretation of human radiologists. This study aimed to develop open-source\nmultimodal large language model for interpreting chest X-ray images\n(CXR-LLaVA). We also examined the effect of prompt engineering and model\nparameters such as temperature and nucleus sampling.\n</p>\n<p>Materials and Methods: For training, we collected 659,287 publicly available\nCXRs: 417,336 CXRs had labels for certain radiographic abnormalities (dataset\n1); 241,951 CXRs provided free-text radiology reports (dataset 2). After\npre-training the Resnet50 as an image encoder, the contrastive language-image\npre-training was used to align CXRs and corresponding radiographic\nabnormalities. Then, the Large Language Model Meta AI-2 was fine-tuned using\ndataset 2, which were refined using GPT-4, with generating various question\nanswering scenarios. The code can be found at\nhttps://github.com/ECOFRI/CXR_LLaVA.\n</p>\n<p>Results: In the test set, we observed that the model's performance fluctuated\nbased on its parameters. On average, it achieved F1 score of 0.34 for five\npathologic findings (atelectasis, cardiomegaly, consolidation, edema, and\npleural effusion), which was improved to 0.46 through prompt engineering. In\nthe independent set, the model achieved an average F1 score of 0.30 for the\nsame pathologic findings. Notably, for the pediatric chest radiograph dataset,\nwhich was unseen during training, the model differentiated abnormal radiographs\nwith an F1 score ranging from 0.84 to 0.85.\n</p>\n<p>Conclusion: CXR-LLaVA demonstrates promising potential in CXR interpretation.\nBoth prompt engineering and model parameter adjustments can play pivotal roles\nin interpreting CXRs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seowoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Youn_J/0/1/0/all/0/1\">Jiwon Youn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Mansu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Soon Ho Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs. (arXiv:2310.19347v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.19347","description":"<p>Despite the recent progress in text summarization made by large language\nmodels (LLMs), they often generate summaries that are factually inconsistent\nwith original articles, known as \"hallucinations\" in text generation. Unlike\nprevious small models (e.g., BART, T5), current LLMs make fewer silly mistakes\nbut more sophisticated ones, such as imposing cause and effect, adding false\ndetails, and overgeneralizing, etc. These hallucinations are challenging to\ndetect through traditional methods, which poses great challenges for improving\nthe factual consistency of text summarization. In this paper, we propose an\nadversarially DEcoupling method to disentangle the Comprehension and\nEmbellishmeNT abilities of LLMs (DECENT). Furthermore, we adopt a probing-based\nparameter-efficient technique to cover the shortage of sensitivity for true and\nfalse in the training process of LLMs. In this way, LLMs are less confused\nabout embellishing and understanding, thus can execute the instructions more\naccurately and have enhanced abilities to distinguish hallucinations.\nExperimental results show that DECENT significantly improves the reliability of\ntext summarization based on LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Huawen Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting-En Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zekun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuchuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qianli Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InfoEntropy Loss to Mitigate Bias of Learning Difficulties for Generative Language Models. (arXiv:2310.19531v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.19531","description":"<p>Generative language models are usually pretrained on large text corpus via\npredicting the next token (i.e., sub-word/word/phrase) given the previous ones.\nRecent works have demonstrated the impressive performance of large generative\nlanguage models on downstream tasks. However, existing generative language\nmodels generally neglect an inherent challenge in text corpus during training,\ni.e., the imbalance between frequent tokens and infrequent ones. It can lead a\nlanguage model to be dominated by common and easy-to-learn tokens, thereby\noverlooking the infrequent and difficult-to-learn ones. To alleviate that, we\npropose an Information Entropy Loss (InfoEntropy Loss) function. During\ntraining, it can dynamically assess the learning difficulty of a to-be-learned\ntoken, according to the information entropy of the corresponding predicted\nprobability distribution over the vocabulary. Then it scales the training loss\nadaptively, trying to lead the model to focus more on the difficult-to-learn\ntokens. On the Pile dataset, we train generative language models at different\nscales of 436M, 1.1B, and 6.7B parameters. Experiments reveal that models\nincorporating the proposed InfoEntropy Loss can gain consistent performance\nimprovement on downstream benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhenpeng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xue Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zijia Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songlin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Language Models For Specialized Domains: A Colorful Approach. (arXiv:2310.19708v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.19708","description":"<p>General purpose language models (LMs) encounter difficulties when processing\ndomain-specific jargon and terminology, which are frequently utilized in\nspecialized fields such as medicine or industrial settings. Moreover, they\noften find it challenging to interpret mixed speech that blends general\nlanguage with specialized jargon. This poses a challenge for automatic speech\nrecognition systems operating within these specific domains. In this work, we\nintroduce a novel approach that integrates domain-specific or secondary LM into\ngeneral-purpose LM. This strategy involves labeling, or \"coloring\", each word\nto indicate its association with either the general or the domain-specific LM.\nWe develop an optimized algorithm that enhances the beam search algorithm to\neffectively handle inferences involving colored words. Our evaluations indicate\nthat this approach is highly effective in integrating jargon into language\ntasks. Notably, our method substantially lowers the error rate for\ndomain-specific words without compromising performance in the general domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eitan_D/0/1/0/all/0/1\">Daniel Eitan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirchi_M/0/1/0/all/0/1\">Menachem Pirchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glazer_N/0/1/0/all/0/1\">Neta Glazer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meital_S/0/1/0/all/0/1\">Shai Meital</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayach_G/0/1/0/all/0/1\">Gil Ayach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krendel_G/0/1/0/all/0/1\">Gidon Krendel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsian_A/0/1/0/all/0/1\">Aviv Shamsian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navon_A/0/1/0/all/0/1\">Aviv Navon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hetz_G/0/1/0/all/0/1\">Gil Hetz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keshet_J/0/1/0/all/0/1\">Joseph Keshet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations. (arXiv:2310.20246v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.20246","description":"<p>Existing research predominantly focuses on developing powerful language\nlearning models (LLMs) for mathematical reasoning within monolingual languages,\nwith few explorations in preserving efficacy in a multilingual context. To\nbridge this gap, this paper pioneers exploring and training powerful\nMultilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we\nconstruct the first multilingual math reasoning instruction dataset,\nMGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue\nof training data scarcity in xMR tasks. Based on the collected dataset, we\npropose different training strategies to build powerful xMR LLMs, named\nMathOctopus, notably outperform conventional open-source LLMs and exhibit\nsuperiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B\nreaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond\nremarkable results, we unearth several pivotal observations and insights from\nextensive experiments: (1) When extending the rejection sampling strategy to\nthe multilingual context, it proves effective for model performances, albeit\nlimited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT)\nacross multiple languages not only significantly enhances model performance\nmultilingually but also elevates their monolingual performance. This indicates\nthat crafting multilingual corpora can be regarded as a vital strategy for\nenhancing model performance in a specific language, especially in mathematical\nreasoning tasks. For instance, MathOctopus-7B improves its counterparts that\ntrained on English from 42.2% to 50.8% on GSM8K testset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zinan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1\">Ning Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_L/0/1/0/all/0/1\">Linjun Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Ming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-11-01T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}