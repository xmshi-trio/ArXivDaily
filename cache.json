{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-05-03T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis. (arXiv:2305.00976v1 [cs.CV])","link":"http://arxiv.org/abs/2305.00976","description":"<p>In this paper, we present TMR, a simple yet effective approach for text to 3D\nhuman motion retrieval. While previous work has only treated retrieval as a\nproxy evaluation metric, we tackle it as a standalone task. Our method extends\nthe state-of-the-art text-to-motion synthesis model TEMOS, and incorporates a\ncontrastive loss to better structure the cross-modal latent space. We show that\nmaintaining the motion generation loss, along with the contrastive training, is\ncrucial to obtain good performance. We introduce a benchmark for evaluation and\nprovide an in-depth analysis by reporting results on several protocols. Our\nextensive experiments on the KIT-ML and HumanML3D datasets show that TMR\noutperforms the prior work by a significant margin, for example reducing the\nmedian rank from 54 to 19. Finally, we showcase the potential of our approach\non moment retrieval. Our code and models are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petrovich_M/0/1/0/all/0/1\">Mathis Petrovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_G/0/1/0/all/0/1\">G&#xfc;l Varol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deception Detection with Feature-Augmentation by soft Domain Transfer. (arXiv:2305.01011v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01011","description":"<p>In this era of information explosion, deceivers use different domains or\nmediums of information to exploit the users, such as News, Emails, and Tweets.\nAlthough numerous research has been done to detect deception in all these\ndomains, information shortage in a new event necessitates these domains to\nassociate with each other to battle deception. To form this association, we\npropose a feature augmentation method by harnessing the intermediate layer\nrepresentation of neural models. Our approaches provide an improvement over the\nself-domain baseline models by up to 6.60%. We find Tweets to be the most\nhelpful information provider for Fake News and Phishing Email detection,\nwhereas News helps most in Tweet Rumor detection. Our analysis provides a\nuseful insight for domain knowledge transfer which can help build a stronger\ndeception detection system than the existing literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1\">Sadat Shahriar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Arjun Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gnawali_O/0/1/0/all/0/1\">Omprakash Gnawali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating statistical language models as pragmatic reasoners. (arXiv:2305.01020v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01020","description":"<p>The relationship between communicated language and intended meaning is often\nprobabilistic and sensitive to context. Numerous strategies attempt to estimate\nsuch a mapping, often leveraging recursive Bayesian models of communication. In\nparallel, large language models (LLMs) have been increasingly applied to\nsemantic parsing applications, tasked with inferring logical representations\nfrom natural language. While existing LLM explorations have been largely\nrestricted to literal language use, in this work, we evaluate the capacity of\nLLMs to infer the meanings of pragmatic utterances. Specifically, we explore\nthe case of threshold estimation on the gradable adjective ``strong'',\ncontextually conditioned on a strength prior, then extended to composition with\nqualification, negation, polarity inversion, and class comparison. We find that\nLLMs can derive context-grounded, human-like distributions over the\ninterpretations of several complex pragmatic utterances, yet struggle composing\nwith negation. These results inform the inferential capacity of statistical\nlanguage models, and their use in pragmatic and semantic parsing applications.\nAll corresponding code is made publicly available\n(https://github.com/benlipkin/probsem/tree/CogSci2023).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lipkin_B/0/1/0/all/0/1\">Benjamin Lipkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_L/0/1/0/all/0/1\">Lionel Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grand_G/0/1/0/all/0/1\">Gabriel Grand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B Tenenbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Company classification using zero-shot learning. (arXiv:2305.01028v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01028","description":"<p>In recent years, natural language processing (NLP) has become increasingly\nimportant in a variety of business applications, including sentiment analysis,\ntext classification, and named entity recognition. In this paper, we propose an\napproach for company classification using NLP and zero-shot learning. Our\nmethod utilizes pre-trained transformer models to extract features from company\ndescriptions, and then applies zero-shot learning to classify companies into\nrelevant categories without the need for specific training data for each\ncategory. We evaluate our approach on publicly available datasets of textual\ndescriptions of companies, and demonstrate that it can streamline the process\nof company classification, thereby reducing the time and resources required in\ntraditional approaches such as the Global Industry Classification Standard\n(GICS). The results show that this method has potential for automation of\ncompany classification, making it a promising avenue for future research in\nthis area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rizinski_M/0/1/0/all/0/1\">Maryan Rizinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jankov_A/0/1/0/all/0/1\">Andrej Jankov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaradas_V/0/1/0/all/0/1\">Vignesh Sankaradas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinsky_E/0/1/0/all/0/1\">Eugene Pinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miskovski_I/0/1/0/all/0/1\">Igor Miskovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trajanov_D/0/1/0/all/0/1\">Dimitar Trajanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SafeWebUH at SemEval-2023 Task 11: Learning Annotator Disagreement in Derogatory Text: Comparison of Direct Training vs Aggregation. (arXiv:2305.01050v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01050","description":"<p>Subjectivity and difference of opinion are key social phenomena, and it is\ncrucial to take these into account in the annotation and detection process of\nderogatory textual content. In this paper, we use four datasets provided by\nSemEval-2023 Task 11 and fine-tune a BERT model to capture the disagreement in\nthe annotation. We find individual annotator modeling and aggregation lowers\nthe Cross-Entropy score by an average of 0.21, compared to the direct training\non the soft labels. Our findings further demonstrate that annotator metadata\ncontributes to the average 0.029 reduction in the Cross-Entropy score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1\">Sadat Shahriar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1\">Thamar Solorio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Multilingual Spellchecker for User Queries. (arXiv:2305.01082v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01082","description":"<p>Spellchecking is one of the most fundamental and widely used search features.\nCorrecting incorrectly spelled user queries not only enhances the user\nexperience but is expected by the user. However, most widely available\nspellchecking solutions are either lower accuracy than state-of-the-art\nsolutions or too slow to be used for search use cases where latency is a key\nrequirement. Furthermore, most innovative recent architectures focus on English\nand are not trained in a multilingual fashion and are trained for spell\ncorrection in longer text, which is a different paradigm from spell correction\nfor user queries, where context is sparse (most queries are 1-2 words long).\nFinally, since most enterprises have unique vocabularies such as product names,\noff-the-shelf spelling solutions fall short of users' needs. In this work, we\nbuild a multilingual spellchecker that is extremely fast and scalable and that\nadapts its vocabulary and hence speller output based on a specific product's\nneeds. Furthermore, our speller out-performs general purpose spellers by a wide\nmargin on in-domain datasets. Our multilingual speller is used in search in\nAdobe products, powering autocomplete in various applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Sanat Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valls_Vargas_J/0/1/0/all/0/1\">Josep Valls-Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_T/0/1/0/all/0/1\">Tracy Holloway King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1\">Francois Guerin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1\">Chirag Arora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logion: Machine Learning for Greek Philology. (arXiv:2305.01099v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01099","description":"<p>This paper presents machine-learning methods to address various problems in\nGreek philology. After training a BERT model on the largest premodern Greek\ndataset used for this purpose to date, we identify and correct previously\nundetected errors made by scribes in the process of textual transmission, in\nwhat is, to our knowledge, the first successful identification of such errors\nvia machine learning. Additionally, we demonstrate the model's capacity to fill\ngaps caused by material deterioration of premodern manuscripts and compare the\nmodel's performance to that of a domain expert. We find that best performance\nis achieved when the domain expert is provided with model suggestions for\ninspiration. With such human-computer collaborations in mind, we explore the\nmodel's interpretability and find that certain attention heads appear to encode\nselect grammatical features of premodern Greek.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cowen_Breen_C/0/1/0/all/0/1\">Charlie Cowen-Breen</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Brooks_C/0/1/0/all/0/1\">Creston Brooks</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Haubold_J/0/1/0/all/0/1\">Johannes Haubold</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Graziosi_B/0/1/0/all/0/1\">Barbara Graziosi</a> (2) ((1) University of Cambridge, (2) Princeton University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADVISE: AI-accelerated Design of Evidence Synthesis for Global Development. (arXiv:2305.01145v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01145","description":"<p>When designing evidence-based policies and programs, decision-makers must\ndistill key information from a vast and rapidly growing literature base.\nIdentifying relevant literature from raw search results is time and resource\nintensive, and is often done by manual screening. In this study, we develop an\nAI agent based on a bidirectional encoder representations from transformers\n(BERT) model and incorporate it into a human team designing an evidence\nsynthesis product for global development. We explore the effectiveness of the\nhuman-AI hybrid team in accelerating the evidence synthesis process. To further\nimprove team efficiency, we enhance the human-AI hybrid team through active\nlearning (AL). Specifically, we explore different sampling strategies,\nincluding random sampling, least confidence (LC) sampling, and highest priority\n(HP) sampling, to study their influence on the collaborative screening process.\nResults show that incorporating the BERT-based AI agent into the human team can\nreduce the human screening effort by 68.5% compared to the case of no AI\nassistance and by 16.8% compared to the case of using a support vector machine\n(SVM)-based AI agent for identifying 80% of all relevant documents. When we\napply the HP sampling strategy for AL, the human screening effort can be\nreduced even more: by 78.3% for identifying 80% of all relevant documents\ncompared to no AI assistance. We apply the AL-enhanced human-AI hybrid teaming\nworkflow in the design process of three evidence gap maps (EGMs) for USAID and\nfind it to be highly effective. These findings demonstrate how AI can\naccelerate the development of evidence synthesis products and promote timely\nevidence-based decision making in global development in a human-AI hybrid\nteaming context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Edwards_K/0/1/0/all/0/1\">Kristen M. Edwards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1\">Binyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porciello_J/0/1/0/all/0/1\">Jaron Porciello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelbert_M/0/1/0/all/0/1\">Mark Engelbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Carolyn Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1\">Faez Ahmed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models. (arXiv:2305.01146v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01146","description":"<p>We systematically investigate lightweight strategies to adapt large language\nmodels (LLMs) for the task of radiology report summarization (RRS).\nSpecifically, we focus on domain adaptation via pretraining (on natural\nlanguage, biomedical text, and clinical text) and via prompting (zero-shot,\nin-context learning) or parameter-efficient fine-tuning (prefix tuning, LoRA).\nOur results on the MIMIC-III dataset consistently demonstrate best performance\nby maximally adapting to the task via pretraining on clinical text and\nparameter-efficient fine-tuning on RRS examples. Importantly, this method\nfine-tunes a mere 0.32% of parameters throughout the model, in contrast to\nend-to-end fine-tuning (100% of parameters). Additionally, we study the effect\nof in-context examples and out-of-distribution (OOD) training before concluding\nwith a radiologist reader study and qualitative analysis. Our findings\nhighlight the importance of domain adaptation in RRS and provide valuable\ninsights toward developing effective natural language processing solutions for\nclinical tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Veen_D/0/1/0/all/0/1\">Dave Van Veen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uden_C/0/1/0/all/0/1\">Cara Van Uden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attias_M/0/1/0/all/0/1\">Maayane Attias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pareek_A/0/1/0/all/0/1\">Anuj Pareek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bluethgen_C/0/1/0/all/0/1\">Christian Bluethgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polacin_M/0/1/0/all/0/1\">Malgorzata Polacin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1\">Wah Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delbrouck_J/0/1/0/all/0/1\">Jean-Benoit Delbrouck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaves_J/0/1/0/all/0/1\">Juan Manuel Zambrano Chaves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhari_A/0/1/0/all/0/1\">Akshay S. Chaudhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauly_J/0/1/0/all/0/1\">John Pauly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lessons Learned in ATCO2: 5000 hours of Air Traffic Control Communications for Robust Automatic Speech Recognition and Understanding. (arXiv:2305.01155v1 [eess.AS])","link":"http://arxiv.org/abs/2305.01155","description":"<p>Voice communication between air traffic controllers (ATCos) and pilots is\ncritical for ensuring safe and efficient air traffic control (ATC). This task\nrequires high levels of awareness from ATCos and can be tedious and\nerror-prone. Recent attempts have been made to integrate artificial\nintelligence (AI) into ATC in order to reduce the workload of ATCos. However,\nthe development of data-driven AI systems for ATC demands large-scale annotated\ndatasets, which are currently lacking in the field. This paper explores the\nlessons learned from the ATCO2 project, a project that aimed to develop a\nunique platform to collect and preprocess large amounts of ATC data from\nairspace in real time. Audio and surveillance data were collected from publicly\naccessible radio frequency channels with VHF receivers owned by a community of\nvolunteers and later uploaded to Opensky Network servers, which can be\nconsidered an \"unlimited source\" of data. In addition, this paper reviews\nprevious work from ATCO2 partners, including (i) robust automatic speech\nrecognition, (ii) natural language processing, (iii) English language\nidentification of ATC communications, and (iv) the integration of surveillance\ndata such as ADS-B. We believe that the pipeline developed during the ATCO2\nproject, along with the open-sourcing of its data, will encourage research in\nthe ATC field. A sample of the ATCO2 corpus is available on the following\nwebsite: https://www.atco2.org/data, while the full corpus can be purchased\nthrough ELDA at <a href=\"http://catalog.elra.info/en-us/repository/browse/ELRA-S0484.\">this http URL</a> We\ndemonstrated that ATCO2 is an appropriate dataset to develop ASR engines when\nlittle or near to no ATC in-domain data is available. For instance, with the\nCNN-TDNNf kaldi model, we reached the performance of as low as 17.9% and 24.9%\nWER on public ATC datasets which is 6.6/7.6% better than \"out-of-domain\" but\nsupervised CNN-TDNNf model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nigmatulina_I/0/1/0/all/0/1\">Iuliia Nigmatulina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasad_A/0/1/0/all/0/1\">Amrutha Prasad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khalil_D/0/1/0/all/0/1\">Driss Khalil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Madikeri_S/0/1/0/all/0/1\">Srikanth Madikeri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tart_A/0/1/0/all/0/1\">Allan Tart</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szoke_I/0/1/0/all/0/1\">Igor Szoke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lenders_V/0/1/0/all/0/1\">Vincent Lenders</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rigault_M/0/1/0/all/0/1\">Mickael Rigault</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Choukri_K/0/1/0/all/0/1\">Khalid Choukri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"New Trends in Machine Translation using Large Language Models: Case Examples with ChatGPT. (arXiv:2305.01181v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01181","description":"<p>Machine Translation (MT) has made significant progress in recent years using\ndeep learning, especially after the emergence of large language models (LLMs)\nsuch as GPT-3 and ChatGPT. This brings new challenges and opportunities for MT\nusing LLMs. In this paper, we brainstorm some interesting directions for MT\nusing LLMs, including stylized MT, interactive MT, and Translation Memory-based\nMT, as well as a new evaluation paradigm using LLMs. We also discuss the\nprivacy concerns in MT using LLMs and a basic privacy-preserving method to\nmitigate such risks. To illustrate the potential of our proposed directions, we\npresent several examples for the new directions mentioned above, demonstrating\nthe feasibility of the proposed directions and highlight the opportunities and\nchallenges for future research in MT using LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chenyang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jitao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Pipeline System of ASR and NLU with MLM-based Data Augmentation toward STOP Low-resource Challenge. (arXiv:2305.01194v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01194","description":"<p>This paper describes our system for the low-resource domain adaptation track\n(Track 3) in Spoken Language Understanding Grand Challenge, which is a part of\nICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a\npipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain\nwith upsampling. For NLU, we fine-tune BART on all the Track3 data and then on\nlow-resource domain data. We apply masked LM (MLM) -based data augmentation,\nwhere some of input tokens and corresponding target labels are replaced using\nMLM. We also apply a retrieval-based approach, where model input is augmented\nwith similar training samples. As a result, we achieved exact match (EM)\naccuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the\n1st place at the challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Futami_H/0/1/0/all/0/1\">Hayato Futami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_J/0/1/0/all/0/1\">Jessica Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Siddhant Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shih-Lun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashiwagi_Y/0/1/0/all/0/1\">Yosuke Kashiwagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsunoo_E/0/1/0/all/0/1\">Emiru Tsunoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Shift Detection in Chinese Dialogues: Corpus and Benchmark. (arXiv:2305.01195v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01195","description":"<p>Dialogue topic shift detection is to detect whether an ongoing topic has\nshifted or should shift in a dialogue, which can be divided into two\ncategories, i.e., response-known task and response-unknown task. Currently,\nonly a few investigated the latter, because it is still a challenge to predict\nthe topic shift without the response information. In this paper, we first\nannotate a Chinese Natural Topic Dialogue (CNTD) corpus consisting of 1308\ndialogues to fill the gap in the Chinese natural conversation topic corpus. And\nthen we focus on the response-unknown task and propose a teacher-student\nframework based on hierarchical contrastive learning to predict the topic shift\nwithout the response. Specifically, the response at high-level teacher-student\nis introduced to build the contrastive learning between the response and the\ncontext, while the label contrastive learning is constructed at low-level\nstudent. The experimental results on our Chinese CNTD and English TIAGE show\nthe effectiveness of our proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiangyi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yaxin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1\">Feng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaomin Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peifeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. (arXiv:2305.01210v1 [cs.SE])","link":"http://arxiv.org/abs/2305.01210","description":"<p>Program synthesis has been long studied with recent approaches focused on\ndirectly using the power of Large Language Models (LLMs) to generate code\naccording to user intent written in natural language. Code evaluation datasets,\ncontaining curated synthesis problems with input/output test-cases, are used to\nmeasure the performance of various LLMs on code synthesis. However, test-cases\nin these datasets can be limited in both quantity and quality for fully\nassessing the functional correctness of the generated code. Such limitation in\nthe existing benchmarks begs the following question: In the era of LLMs, is the\ncode generated really correct? To answer this, we propose EvalPlus -- a code\nsynthesis benchmarking framework to rigorously evaluate the functional\ncorrectness of LLM-synthesized code. In short, EvalPlus takes in the base\nevaluation dataset and uses an automatic input generation step to produce and\ndiversify large amounts of new test inputs using both LLM-based and\nmutation-based input generators to further validate the synthesized code. We\nextend the popular HUMANEVAL benchmark and build HUMANEVAL+ with 81x\nadditionally generated tests. Our extensive evaluation across 14 popular LLMs\ndemonstrates that HUMANEVAL+ is able to catch significant amounts of previously\nundetected wrong code synthesized by LLMs, reducing the pass@k by 15.1% on\naverage! Moreover, we even found several incorrect ground-truth implementations\nin HUMANEVAL. Our work not only indicates that prior popular code synthesis\nevaluation results do not accurately reflect the true performance of LLMs for\ncode synthesis but also opens up a new direction to improve programming\nbenchmarks through automated test input generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Chunqiu Steven Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lingming Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiLegalSBD: A Multilingual Legal Sentence Boundary Detection Dataset. (arXiv:2305.01211v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01211","description":"<p>Sentence Boundary Detection (SBD) is one of the foundational building blocks\nof Natural Language Processing (NLP), with incorrectly split sentences heavily\ninfluencing the output quality of downstream tasks. It is a challenging task\nfor algorithms, especially in the legal domain, considering the complex and\ndifferent sentence structures used. In this work, we curated a diverse\nmultilingual legal dataset consisting of over 130'000 annotated sentences in 6\nlanguages. Our experimental results indicate that the performance of existing\nSBD models is subpar on multilingual legal data. We trained and tested\nmonolingual and multilingual models based on CRF, BiLSTM-CRF, and transformers,\ndemonstrating state-of-the-art performance. We also show that our multilingual\nmodels outperform all baselines in the zero-shot setting on a Portuguese test\nset. To encourage further research and development by the community, we have\nmade our dataset, models, and code publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brugger_T/0/1/0/all/0/1\">Tobias Brugger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sturmer_M/0/1/0/all/0/1\">Matthias St&#xfc;rmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niklaus_J/0/1/0/all/0/1\">Joel Niklaus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. (arXiv:2305.01219v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01219","description":"<p>The prompt-based learning paradigm, which bridges the gap between\npre-training and fine-tuning, achieves state-of-the-art performance on several\nNLP tasks, particularly in few-shot settings. Despite being widely applied,\nprompt-based learning is vulnerable to backdoor attacks. Textual backdoor\nattacks are designed to introduce targeted vulnerabilities into models by\npoisoning a subset of training samples through trigger injection and label\nmodification. However, they suffer from flaws such as abnormal natural language\nexpressions resulting from the trigger and incorrect labeling of poisoned\nsamples. In this study, we propose {\\bf ProAttack}, a novel and efficient\nmethod for performing clean-label backdoor attacks based on the prompt, which\nuses the prompt itself as a trigger. Our method does not require external\ntriggers and ensures correct labeling of poisoned samples, improving the\nstealthy nature of the backdoor attack. With extensive experiments on\nrich-resource and few-shot text classification tasks, we empirically validate\nProAttack's competitive performance in textual backdoor attacks. Notably, in\nthe rich-resource setting, ProAttack achieves state-of-the-art attack success\nrates in the clean-label backdoor attack benchmark without external triggers.\nAll data and code used in our models are publically\navailable\\footnote{\\url{https://github.com/shuaizhao95/Prompt_attack}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jinming Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1\">Luu Anh Tuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Role of Summarization in Generative Agents: A Preliminary Perspective. (arXiv:2305.01253v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01253","description":"<p>Generative agents that simulate human society show tremendous potential for\nfurther research and practical applications. Specifically, the generative agent\narchitecture comprising several meticulously designed modules constitutes the\nmost critical component. To facilitate progress in this research, this report\npresents our integrated perspective on comprehending generative agents through\nsummarization, since we believe summarization is the most fundamental and\nindispensable capacity of generative agents manifested across diverse\nscenarios. We hope this report can provide insight into understanding the\nimportance of summarization capacity in generative agents and motivate future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiachong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaocheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Visual Prompt Generator across LLMs. (arXiv:2305.01278v1 [cs.CV])","link":"http://arxiv.org/abs/2305.01278","description":"<p>While developing a new vision-language LLM (VL-LLM) by pre-training on\ntremendous image-text pairs from scratch can be exceedingly resource-consuming,\nconnecting an existing LLM with a comparatively lightweight visual prompt\ngenerator (VPG) becomes a feasible paradigm. However, further tuning the VPG\npart of the VL-LLM still suffers from indispensable computational costs, i.e.,\nrequiring thousands of GPU hours and millions of training data. One alternative\nsolution is to transfer an existing VPG from any existing VL-LLMs for the\ntarget VL-LLM.\n</p>\n<p>In this work, we for the first time investigate the VPG transferability\nacross LLMs, and explore a solution to reduce the cost of VPG transfer. We\nfirst study the VPG transfer across different LLM sizes (e.g., small-to-large),\nand across different LLM types, through which we diagnose the key factors to\nmaximize the transfer efficiency. Based on our observation, we design a\ntwo-stage transfer framework named VPGTrans, which is simple yet highly\neffective. Through extensive experiments, we demonstrate that VPGTrans helps\nsignificantly speed up the transfer learning process without compromising\nperformance. Remarkably, it helps achieve the VPG transfer from BLIP-2\nOPT$_\\text{2.7B}$ to BLIP-2 OPT$_\\text{6.7B}$ with over 10 times speed-up and\n10.7% training data compared with connecting a VPG to OPT$_\\text{6.7B}$ from\nscratch. Further, a series of intriguing findings and potential rationales\nbehind them are provided and discussed. Finally, we showcase the practical\nvalue of our VPGTrans approach, by customizing two novel VL-LLMs, including\nVL-LLaMA and VL-Vicuna, with recently released LLaMA and Vicuna LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hao Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Turning Flowchart into Dialog: Plan-based Data Augmentation for Low-Resource Flowchart-grounded Troubleshooting Dialogs. (arXiv:2305.01323v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01323","description":"<p>Flowchart-grounded troubleshooting dialogue (FTD) systems, which follow the\ninstructions of a flowchart to diagnose users' problems in specific domains\n(eg., vehicle, laptop), have been gaining research interest in recent years.\nHowever, collecting sufficient dialogues that are naturally grounded on\nflowcharts is costly, thus FTD systems are impeded by scarce training data. To\nmitigate the data sparsity issue, we propose a plan-based data augmentation\n(PlanDA) approach that generates diverse synthetic dialog data at scale by\ntransforming concise flowchart into dialogues. Specifically, its generative\nmodel employs a variational-base framework with a hierarchical planning\nstrategy that includes global and local latent planning variables. Experiments\non the FloDial dataset show that synthetic dialogue produced by PlanDA improves\nthe performance of downstream tasks, including flowchart path retrieval and\nresponse generation, in particular on the Out-of-Flowchart settings. In\naddition, further analysis demonstrate the quality of synthetic data generated\nby PlanDA in paths that are covered by current sample dialogues and paths that\nare not covered.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1\">Haolan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maruf_S/0/1/0/all/0/1\">Sameen Maruf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zukerman_I/0/1/0/all/0/1\">Ingrid Zukerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class based Influence Functions for Error Detection. (arXiv:2305.01384v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01384","description":"<p>Influence functions (IFs) are a powerful tool for detecting anomalous\nexamples in large scale datasets. However, they are unstable when applied to\ndeep networks. In this paper, we provide an explanation for the instability of\nIFs and develop a solution to this problem. We show that IFs are unreliable\nwhen the two data points belong to two different classes. Our solution\nleverages class information to improve the stability of IFs. Extensive\nexperiments show that our modification significantly improves the performance\nand stability of IFs while incurring no additional computational cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Duc_T/0/1/0/all/0/1\">Thang Nguyen-Duc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thanh_Tung_H/0/1/0/all/0/1\">Hoang Thanh-Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quan Hung Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huu_Tien_D/0/1/0/all/0/1\">Dang Huu-Tien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hieu Ngoc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dau_A/0/1/0/all/0/1\">Anh T. V. Dau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_N/0/1/0/all/0/1\">Nghi D. Q. Bui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Local to Global: Navigating Linguistic Diversity in the African Context. (arXiv:2305.01427v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01427","description":"<p>The focus is on critical problems in NLP related to linguistic diversity and\nvariation across the African continent, specifically with regards to African\nlocal dialects and Arabic dialects that have received little attention. We\nevaluated our various approaches, demonstrating their effectiveness while\nhighlighting the potential impact of the proposed approach on businesses\nseeking to improve customer experience and product development in African local\ndialects. The idea of using the model as a teaching tool for product-based\ninstruction is interesting, as it could potentially stimulate interest in\nlearners and trigger techno entrepreneurship. Overall, our modified approach\noffers a promising analysis of the challenges of dealing with African local\ndialects. Particularly Arabic dialects, which could have a significant impact\non businesses seeking to improve customer experience and product development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Margani_R/0/1/0/all/0/1\">Rashmi Margani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ndugu_N/0/1/0/all/0/1\">Nelson Ndugu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment Perception Adversarial Attacks on Neural Machine Translation Systems. (arXiv:2305.01437v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01437","description":"<p>With the advent of deep learning methods, Neural Machine Translation (NMT)\nsystems have become increasingly powerful. However, deep learning based systems\nare susceptible to adversarial attacks, where imperceptible changes to the\ninput can cause undesirable changes at the output of the system. To date there\nhas been little work investigating adversarial attacks on sequence-to-sequence\nsystems, such as NMT models. Previous work in NMT has examined attacks with the\naim of introducing target phrases in the output sequence. In this work,\nadversarial attacks for NMT systems are explored from an output perception\nperspective. Thus the aim of an attack is to change the perception of the\noutput sequence, without altering the perception of the input sequence. For\nexample, an adversary may distort the sentiment of translated reviews to have\nan exaggerated positive sentiment. In practice it is challenging to run\nextensive human perception experiments, so a proxy deep-learning classifier\napplied to the NMT output is used to measure perception changes. Experiments\ndemonstrate that the sentiment perception of NMT systems' output sequences can\nbe changed significantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vyas Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark Gales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Neural Databases. (arXiv:2305.01447v1 [cs.MM])","link":"http://arxiv.org/abs/2305.01447","description":"<p>The rise in loosely-structured data available through text, images, and other\nmodalities has called for new ways of querying them. Multimedia Information\nRetrieval has filled this gap and has witnessed exciting progress in recent\nyears. Tasks such as search and retrieval of extensive multimedia archives have\nundergone massive performance improvements, driven to a large extent by recent\ndevelopments in multimodal deep learning. However, methods in this field remain\nlimited in the kinds of queries they support and, in particular, their\ninability to answer database-like queries. For this reason, inspired by recent\nwork on neural databases, we propose a new framework, which we name Multimodal\nNeural Databases (MMNDBs). MMNDBs can answer complex database-like queries that\ninvolve reasoning over different input modalities, such as text and images, at\nscale. In this paper, we present the first architecture able to fulfill this\nset of requirements and test it with several baselines, showing the limitations\nof currently available models. The results show the potential of these new\ntechniques to process unstructured data coming from different modalities,\npaving the way for future research in the area. Code to replicate the\nexperiments will be released at\nhttps://github.com/GiovanniTRA/MultimodalNeuralDatabases\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trappolini_G/0/1/0/all/0/1\">Giovanni Trappolini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santilli_A/0/1/0/all/0/1\">Andrea Santilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1\">Emanuele Rodol&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halevy_A/0/1/0/all/0/1\">Alon Halevy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1\">Fabrizio Silvestri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Summarizing Multiple Documents with Hierarchical Relationships. (arXiv:2305.01498v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01498","description":"<p>Most existing multi-document summarization (MDS) datasets lack\nhuman-generated and genuine (i.e., not synthetic) summaries or source documents\nwith explicit inter-document relationships that a summary must capture. To\nenhance the capabilities of MDS systems we present PeerSum, a novel dataset for\ngenerating meta-reviews of scientific papers, where the meta-reviews are highly\nabstractive and genuine summaries of reviews and corresponding discussions.\nThese source documents have rich inter-document relationships of an explicit\nhierarchical structure with cross-references and often feature conflicts. As\nthere is a scarcity of research that incorporates hierarchical relationships\ninto MDS systems through attention manipulation on pre-trained language models,\nwe additionally present Rammer (Relationship-aware Multi-task Meta-review\nGenerator), a meta-review generation model that uses sparse attention based on\nthe hierarchical relationships and a multi-task objective that predicts several\nmetadata features in addition to the standard text generation objective. Our\nexperimental results show that PeerSum is a challenging dataset, and Rammer\noutperforms other strong baseline MDS models under various evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Miao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NewsPanda: Media Monitoring for Timely Conservation Action. (arXiv:2305.01503v1 [cs.IR])","link":"http://arxiv.org/abs/2305.01503","description":"<p>Non-governmental organizations for environmental conservation have a\nsignificant interest in monitoring conservation-related media and getting\ntimely updates about infrastructure construction projects as they may cause\nmassive impact to key conservation areas. Such monitoring, however, is\ndifficult and time-consuming. We introduce NewsPanda, a toolkit which\nautomatically detects and analyzes online articles related to environmental\nconservation and infrastructure construction. We fine-tune a BERT-based model\nusing active learning methods and noise correction algorithms to identify\narticles that are relevant to conservation and infrastructure construction. For\nthe identified articles, we perform further analysis, extracting keywords and\nfinding potentially related sources. NewsPanda has been successfully deployed\nby the World Wide Fund for Nature teams in the UK, India, and Nepal since\nFebruary 2022. It currently monitors over 80,000 websites and 1,074\nconservation sites across India and Nepal, saving more than 30 hours of human\nefforts weekly. We have now scaled it up to cover 60,000 conservation sites\nglobally.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keh_S/0/1/0/all/0/1\">Sedrick Scott Keh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zheyuan Ryan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patterson_D/0/1/0/all/0/1\">David J. Patterson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagabati_N/0/1/0/all/0/1\">Nirmal Bhagabati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dewan_K/0/1/0/all/0/1\">Karun Dewan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopala_A/0/1/0/all/0/1\">Areendran Gopala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izquierdo_P/0/1/0/all/0/1\">Pablo Izquierdo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallick_D/0/1/0/all/0/1\">Debojyoti Mallick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Ambika Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrestha_P/0/1/0/all/0/1\">Pooja Shrestha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1\">Fei Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Classification: Financial Reasoning in State-of-the-Art Language Models. (arXiv:2305.01505v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01505","description":"<p>Large Language Models (LLMs), consisting of 100 billion or more parameters,\nhave demonstrated remarkable ability in complex multi-step reasoning tasks.\nHowever, the application of such generic advancements has been limited to a few\nfields, such as clinical or legal, with the field of financial reasoning\nremaining largely unexplored. To the best of our knowledge, the ability of LLMs\nto solve financial reasoning problems has never been dealt with, and whether it\ncan be performed at any scale remains unknown. To address this knowledge gap,\nthis research presents a comprehensive investigation into the potential\napplication of LLMs in the financial domain. The investigation includes a\ndetailed exploration of a range of subjects, including task formulation,\nsynthetic data generation, prompting methods, and evaluation capability.\nFurthermore, the study benchmarks various GPT variants with parameter scales\nranging from 2.8B to 13B, with and without instruction tuning, on diverse\ndataset sizes. By analyzing the results, we reveal that the ability to generate\ncoherent financial reasoning first emerges at 6B parameters, and continues to\nimprove with better instruction-tuning or larger datasets. Additionally, the\nstudy provides a publicly accessible dataset named sFIOG (Synthetic-Financial\nInvestment Opinion Generation), consisting of 11,802 synthetic investment\nthesis samples, to support further research in the field of financial\nreasoning. Overall, this research seeks to contribute to the understanding of\nthe efficacy of language models in the field of finance, with a particular\nemphasis on their ability to engage in sophisticated reasoning and analysis\nwithin the context of investment decision-making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Son_G/0/1/0/all/0/1\">Guijin Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">Hanearl Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahm_M/0/1/0/all/0/1\">Moonjeong Hahm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_K/0/1/0/all/0/1\">Keonju Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Sol Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Huatuo-26M, a Large-scale Chinese Medical QA Dataset. (arXiv:2305.01526v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01526","description":"<p>In this paper, we release a largest ever medical Question Answering (QA)\ndataset with 26 million QA pairs. We benchmark many existing approaches in our\ndataset in terms of both retrieval and generation. Experimental results show\nthat the existing models perform far lower than expected and the released\ndataset is still challenging in the pre-trained language model era. Moreover,\nwe also experimentally show the benefit of the proposed dataset in many\naspects: (i) trained models for other QA datasets in a zero-shot fashion; and\n(ii) as external knowledge for retrieval-augmented generation (RAG); and (iii)\nimproving existing pre-trained language models by using the QA pairs as a\npre-training corpus in continued training manner. We believe that this dataset\nwill not only contribute to medical research but also facilitate both the\npatients and clinical doctors. See\n\\url{https://github.com/FreedomIntelligence/Huatuo-26M}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianquan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiangbo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaolong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_P/0/1/0/all/0/1\">Prayag Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benyou Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information. (arXiv:2305.01528v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01528","description":"<p>Dungeons &amp; Dragons (D&amp;D) is a tabletop roleplaying game with complex natural\nlanguage interactions between players and hidden state information. Recent work\nhas shown that large language models (LLMs) that have access to state\ninformation can generate higher quality game turns than LLMs that use dialog\nhistory alone. However, previous work used game state information that was\nheuristically created and was not a true gold standard game state. We present\nFIREBALL, a large dataset containing nearly 25,000 unique sessions from real\nD\\&amp;D gameplay on Discord with true game state info. We recorded game play\nsessions of players who used the Avrae bot, which was developed to aid people\nin playing D&amp;D online, capturing language, game commands and underlying game\nstate information. We demonstrate that FIREBALL can improve natural language\ngeneration (NLG) by using Avrae state information, improving both automated\nmetrics and human judgments of quality. Additionally, we show that LLMs can\ngenerate executable Avrae commands, particularly after finetuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_A/0/1/0/all/0/1\">Andrew Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_K/0/1/0/all/0/1\">Karmanya Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_A/0/1/0/all/0/1\">Alexander Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1\">Lara J. Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Approximate Memorization in Language Models via Dissimilarity Learned Policy. (arXiv:2305.01550v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01550","description":"<p>Large Language models (LLMs) are trained on large amounts of data, which can\ninclude sensitive information that may compromise personal privacy. LLMs showed\nto memorize parts of the training data and emit those data verbatim when an\nadversary prompts appropriately. Previous research has primarily focused on\ndata preprocessing and differential privacy techniques to address memorization\nor prevent verbatim memorization exclusively, which can give a false sense of\nprivacy. However, these methods rely on explicit and implicit assumptions about\nthe structure of the data to be protected, which often results in an incomplete\nsolution to the problem. To address this, we propose a novel framework that\nutilizes a reinforcement learning approach (PPO) to fine-tune LLMs to mitigate\napproximate memorization. Our approach utilizes a negative similarity score,\nsuch as BERTScore or SacreBLEU, as a reward signal to learn a dissimilarity\npolicy. Our results demonstrate that this framework effectively mitigates\napproximate memorization while maintaining high levels of coherence and fluency\nin the generated samples. Furthermore, our framework is robust in mitigating\napproximate memorization across various circumstances, including longer\ncontext, which is known to increase memorization in LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kassem_A/0/1/0/all/0/1\">Aly M. Kassem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01555","description":"<p>Scaling language models have revolutionized widespread NLP tasks, yet little\ncomprehensively explored few-shot relation extraction with large language\nmodels. In this paper, we investigate principal methodologies, in-context\nlearning and data generation, for few-shot relation extraction via GPT-3.5\nthrough exhaustive experiments. To enhance few-shot performance, we further\npropose task-related instructions and schema-constrained data generation. We\nobserve that in-context learning can achieve performance on par with previous\nprompt learning approaches, and data generation with the large language model\ncan boost previous solutions to obtain new state-of-the-art few-shot results on\nfour widely-studied relation extraction datasets. We hope our work can inspire\nfuture research for the capabilities of large language models in few-shot\nrelation extraction. Code is available in\n\\url{https://github.com/zjunlp/DeepKE/tree/main/example/llm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Type-enhanced Ensemble Triple Representation via Triple-aware Attention for Cross-lingual Entity Alignment. (arXiv:2305.01556v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01556","description":"<p>Entity alignment(EA) is a crucial task for integrating cross-lingual and\ncross-domain knowledge graphs(KGs), which aims to discover entities referring\nto the same real-world object from different KGs. Most existing methods\ngenerate aligning entity representation by mining the relevance of triple\nelements via embedding-based methods, paying little attention to triple\nindivisibility and entity role diversity. In this paper, a novel framework\nnamed TTEA -- Type-enhanced Ensemble Triple Representation via Triple-aware\nAttention for Cross-lingual Entity Alignment is proposed to overcome the above\nissues considering ensemble triple specificity and entity role features.\nSpecifically, the ensemble triple representation is derived by regarding\nrelation as information carrier between semantic space and type space, and\nhence the noise influence during spatial transformation and information\npropagation can be smoothly controlled via specificity-aware triple attention.\nMoreover, our framework uses triple-ware entity enhancement to model the role\ndiversity of triple elements. Extensive experiments on three real-world\ncross-lingual datasets demonstrate that our framework outperforms\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhishuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chengxiang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haihang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xueyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OTIEA:Ontology-enhanced Triple Intrinsic-Correlation for Cross-lingual Entity Alignment. (arXiv:2305.01561v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01561","description":"<p>Cross-lingual and cross-domain knowledge alignment without sufficient\nexternal resources is a fundamental and crucial task for fusing irregular data.\nAs the element-wise fusion process aiming to discover equivalent objects from\ndifferent knowledge graphs (KGs), entity alignment (EA) has been attracting\ngreat interest from industry and academic research recent years. Most of\nexisting EA methods usually explore the correlation between entities and\nrelations through neighbor nodes, structural information and external\nresources. However, the complex intrinsic interactions among triple elements\nand role information are rarely modeled in these methods, which may lead to the\ninadequate illustration for triple. In addition, external resources are usually\nunavailable in some scenarios especially cross-lingual and cross-domain\napplications, which reflects the little scalability of these methods. To tackle\nthe above insufficiency, a novel universal EA framework (OTIEA) based on\nontology pair and role enhancement mechanism via triple-aware attention is\nproposed in this paper without introducing external resources. Specifically, an\nontology-enhanced triple encoder is designed via mining intrinsic correlations\nand ontology pair information instead of independent elements. In addition, the\nEA-oriented representations can be obtained in triple-aware entity decoder by\nfusing role diversity. Finally, a bidirectional iterative alignment strategy is\ndeployed to expand seed entity pairs. The experimental results on three\nreal-world datasets show that our framework achieves a competitive performance\ncompared with baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhishuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chengxiang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xueyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chaoqun Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised learning for infant cry analysis. (arXiv:2305.01578v1 [cs.SD])","link":"http://arxiv.org/abs/2305.01578","description":"<p>In this paper, we explore self-supervised learning (SSL) for analyzing a\nfirst-of-its-kind database of cry recordings containing clinical indications of\nmore than a thousand newborns. Specifically, we target cry-based detection of\nneurological injury as well as identification of cry triggers such as pain,\nhunger, and discomfort. Annotating a large database in the medical setting is\nexpensive and time-consuming, typically requiring the collaboration of several\nexperts over years. Leveraging large amounts of unlabeled audio data to learn\nuseful representations can lower the cost of building robust models and,\nultimately, clinical solutions. In this work, we experiment with\nself-supervised pre-training of a convolutional neural network on large audio\ndatasets. We show that pre-training with SSL contrastive loss (SimCLR) performs\nsignificantly better than supervised pre-training for both neuro injury and cry\ntriggers. In addition, we demonstrate further performance gains through\nSSL-based domain adaptation using unlabeled infant cries. We also show that\nusing such SSL-based pre-training for adaptation to cry sounds decreases the\nneed for labeled data of the overall system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gorin_A/0/1/0/all/0/1\">Arsenii Gorin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subakan_C/0/1/0/all/0/1\">Cem Subakan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdoli_S/0/1/0/all/0/1\">Sajjad Abdoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Latremouille_S/0/1/0/all/0/1\">Samantha Latremouille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onu_C/0/1/0/all/0/1\">Charles Onu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discern and Answer: Mitigating the Impact of Misinformation in Retrieval-Augmented Models with Discriminators. (arXiv:2305.01579v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01579","description":"<p>Most existing retrieval-augmented language models (LMs) for question\nanswering assume all retrieved information is factually correct. In this work,\nwe study a more realistic scenario in which retrieved documents may contain\nmisinformation, causing conflicts among them. We observe that the existing\nmodels are highly brittle to such information in both fine-tuning and\nin-context few-shot learning settings. We propose approaches to make\nretrieval-augmented LMs robust to misinformation by explicitly fine-tuning a\ndiscriminator or prompting to elicit discrimination capability in GPT-3. Our\nempirical results on open-domain question answering show that these approaches\nsignificantly improve LMs' robustness to knowledge conflicts. We also provide\nour findings on interleaving the fine-tuned model's decision with the\nin-context learning process, paving a new path to leverage the best of both\nworlds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_G/0/1/0/all/0/1\">Giwon Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jeonghwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Junmo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myaeng_S/0/1/0/all/0/1\">Sung-Hyon Myaeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whang_J/0/1/0/all/0/1\">Joyce Jiyoung Whang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FreeLM: Fine-Tuning-Free Language Model. (arXiv:2305.01616v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01616","description":"<p>Pre-trained language models (PLMs) have achieved remarkable success in NLP\ntasks. Despite the great success, mainstream solutions largely follow the\npre-training then finetuning paradigm, which brings in both high deployment\ncosts and low training efficiency. Nevertheless, fine-tuning on a specific task\nis essential because PLMs are only pre-trained with language signal from large\nraw data. In this paper, we propose a novel fine-tuning-free strategy for\nlanguage models, to consider both language signal and teacher signal. Teacher\nsignal is an abstraction of a battery of downstream tasks, provided in a\nunified proposition format. Trained with both language and strong task-aware\nteacher signals in an interactive manner, our FreeLM model demonstrates strong\ngeneralization and robustness. FreeLM outperforms large models e.g., GPT-3 and\nInstructGPT, on a range of language understanding tasks in experiments. FreeLM\nis much smaller with 0.3B parameters, compared to 175B in these models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xuying Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yequan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study on the Integration of Pipeline and E2E SLU systems for Spoken Semantic Parsing toward STOP Quality Challenge. (arXiv:2305.01620v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01620","description":"<p>Recently there have been efforts to introduce new benchmark tasks for spoken\nlanguage understanding (SLU), like semantic parsing. In this paper, we describe\nour proposed spoken semantic parsing system for the quality track (Track 1) in\nSpoken Language Understanding Grand Challenge which is part of ICASSP Signal\nProcessing Grand Challenge 2023. We experiment with both end-to-end and\npipeline systems for this task. Strong automatic speech recognition (ASR)\nmodels like Whisper and pretrained Language models (LM) like BART are utilized\ninside our SLU framework to boost performance. We also investigate the output\nlevel combination of various models to get an exact match accuracy of 80.8,\nwhich won the 1st place at the challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Siddhant Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Futami_H/0/1/0/all/0/1\">Hayato Futami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shih-Lun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_J/0/1/0/all/0/1\">Jessica Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashiwagi_Y/0/1/0/all/0/1\">Yosuke Kashiwagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsunoo_E/0/1/0/all/0/1\">Emiru Tsunoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNTER: A Unified Knowledge Interface for Enhancing Pre-trained Language Models. (arXiv:2305.01624v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01624","description":"<p>Recent research demonstrates that external knowledge injection can advance\npre-trained language models (PLMs) in a variety of downstream NLP tasks.\nHowever, existing knowledge injection methods are either applicable to\nstructured knowledge or unstructured knowledge, lacking a unified usage. In\nthis paper, we propose a UNified knowledge inTERface, UNTER, to provide a\nunified perspective to exploit both structured knowledge and unstructured\nknowledge. In UNTER, we adopt the decoder as a unified knowledge interface,\naligning span representations obtained from the encoder with their\ncorresponding knowledge. This approach enables the encoder to uniformly invoke\nspan-related knowledge from its parameters for downstream applications.\nExperimental results show that, with both forms of knowledge injected, UNTER\ngains continuous improvements on a series of knowledge-driven NLP tasks,\nincluding entity typing, named entity recognition and relation extraction,\nespecially in low-resource scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1\">Deming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unlimiformer: Long-Range Transformers with Unlimited Length Input. (arXiv:2305.01625v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01625","description":"<p>Transformer-based models typically have a predefined bound to their input\nlength, because of their need to potentially attend to every token in the\ninput. In this work, we propose Unlimiformer: a general approach that can wrap\nany existing pretrained encoder-decoder transformer, and offload the attention\ncomputation across all layers to a single $k$-nearest-neighbor index; this\nindex can be kept on either the GPU or CPU memory and queried in sub-linear\ntime. This way, we can index extremely long input sequences, while every\nattention head in every decoder layer retrieves its top-$k$ keys, instead of\nattending to every key. We demonstrate Unlimiformers's efficacy on several\nlong-document and multi-document summarization benchmarks, showing that it can\nsummarize even 350k token-long inputs from the BookSum dataset, without any\ninput truncation at test time. Unlimiformer improves pretrained models such as\nBART and Longformer by extending them to unlimited inputs without additional\nlearned weights and without modifying their code. We make our code and models\npublicly available at https://github.com/abertsch72/unlimiformer .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bertsch_A/0/1/0/all/0/1\">Amanda Bertsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1\">Uri Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gormley_M/0/1/0/all/0/1\">Matthew R. Gormley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Basic syntax from speech: Spontaneous concatenation in unsupervised deep neural networks. (arXiv:2305.01626v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01626","description":"<p>Computational models of syntax are predominantly text-based. Here we propose\nthat basic syntax can be modeled directly from raw speech in a fully\nunsupervised way. We focus on one of the most ubiquitous and basic properties\nof syntax -- concatenation. We introduce spontaneous concatenation: a\nphenomenon where convolutional neural networks (CNNs) trained on acoustic\nrecordings of individual words start generating outputs with two or even three\nwords concatenated without ever accessing data with multiple words in the\ninput. Additionally, networks trained on two words learn to embed words into\nnovel unobserved word combinations. To our knowledge, this is a previously\nunreported property of CNNs trained on raw speech in the Generative Adversarial\nNetwork setting and has implications both for our understanding of how these\narchitectures learn as well as for modeling syntax and its evolution from raw\nacoustic inputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Thomas Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zili Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers. (arXiv:2305.01628v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01628","description":"<p>Applying language models to natural language processing tasks typically\nrelies on the representations in the final model layer, as intermediate hidden\nlayer representations are presumed to be less informative. In this work, we\nargue that due to the gradual improvement across model layers, additional\ninformation can be gleaned from the contrast between higher and lower layers\nduring inference. Specifically, in choosing between the probable next token\npredictions of a generative model, the predictions of lower layers can be used\nto highlight which candidates are best avoided. We propose a novel approach\nthat utilizes the contrast between layers to improve text generation outputs,\nand show that it mitigates degenerative behaviors of the model in open-ended\ngeneration, significantly improving the quality of generated texts.\nFurthermore, our results indicate that contrasting between model layers at\ninference time can yield substantial benefits to certain aspects of general\nlanguage model capabilities, more effectively extracting knowledge during\ninference from a given set of model parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gera_A/0/1/0/all/0/1\">Ariel Gera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_R/0/1/0/all/0/1\">Roni Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arviv_O/0/1/0/all/0/1\">Ofir Arviv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunasekara_C/0/1/0/all/0/1\">Chulaka Gunasekara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sznajder_B/0/1/0/all/0/1\">Benjamin Sznajder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shnarch_E/0/1/0/all/0/1\">Eyal Shnarch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Missing Information, Unresponsive Authors, Experimental Flaws: The Impossibility of Assessing the Reproducibility of Previous Human Evaluations in NLP. (arXiv:2305.01633v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01633","description":"<p>We report our efforts in identifying a set of previous human evaluations in\nNLP that would be suitable for a coordinated study examining what makes human\nevaluations in NLP more/less reproducible. We present our results and findings,\nwhich include that just 13\\% of papers had (i) sufficiently low barriers to\nreproduction, and (ii) enough obtainable information, to be considered for\nreproduction, and that all but one of the experiments we selected for\nreproduction was discovered to have flaws that made the meaningfulness of\nconducting a reproduction questionable. As a result, we had to change our\ncoordinated study design from a reproduce approach to a\nstandardise-then-reproduce-twice approach. Our overall (negative) finding that\nthe great majority of human evaluations in NLP is not repeatable and/or not\nreproducible and/or too flawed to justify reproduction, paints a dire picture,\nbut presents an opportunity for a rethink about how to design and report human\nevaluations in NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belz_A/0/1/0/all/0/1\">Anya Belz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomson_C/0/1/0/all/0/1\">Craig Thomson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiter_E/0/1/0/all/0/1\">Ehud Reiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abercrombie_G/0/1/0/all/0/1\">Gavin Abercrombie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_Moral_J/0/1/0/all/0/1\">Jose M. Alonso-Moral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arvan_M/0/1/0/all/0/1\">Mohammad Arvan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cieliebak_M/0/1/0/all/0/1\">Mark Cieliebak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_E/0/1/0/all/0/1\">Elizabeth Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deemter_K/0/1/0/all/0/1\">Kees van Deemter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinkar_T/0/1/0/all/0/1\">Tanvi Dinkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1\">Ond&#x159;ej Du&#x161;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qixiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkatzia_D/0/1/0/all/0/1\">Dimitra Gkatzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Corbelle_J/0/1/0/all/0/1\">Javier Gonz&#xe1;lez-Corbelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hurlimann_M/0/1/0/all/0/1\">Manuela H&#xfc;rlimann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ito_T/0/1/0/all/0/1\">Takumi Ito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelleher_J/0/1/0/all/0/1\">John D. Kelleher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klubicka_F/0/1/0/all/0/1\">Filip Klubicka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1\">Huiyuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chris van der Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miltenburg_E/0/1/0/all/0/1\">Emiel van Miltenburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiru Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahamood_S/0/1/0/all/0/1\">Saad Mahamood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mieskes_M/0/1/0/all/0/1\">Margot Mieskes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nissim_M/0/1/0/all/0/1\">Malvina Nissim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parde_N/0/1/0/all/0/1\">Natalie Parde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Platek_O/0/1/0/all/0/1\">Ond&#x159;ej Pl&#xe1;tek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1\">Verena Rieser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_P/0/1/0/all/0/1\">Pablo Mosteiro Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tetreault_J/0/1/0/all/0/1\">Joel Tetreault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toral_A/0/1/0/all/0/1\">Antonio Toral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wanner_L/0/1/0/all/0/1\">Leo Wanner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watson_L/0/1/0/all/0/1\">Lewis Watson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models. (arXiv:2305.01645v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01645","description":"<p>Fine-tuning large models is highly effective, however, inference using these\nmodels can be expensive and produces carbon emissions. Knowledge distillation\nhas been shown to be a practical solution to reduce inference costs, but the\ndistillation process itself requires significant computational resources.\nRather than buying or renting GPUs to fine-tune, then distill a large model, an\nNLP practitioner who needs a compact model might also choose to simply allocate\nan available budget to hire annotators and manually label additional\nfine-tuning data. In this paper, we investigate how to most efficiently use a\nfixed budget to build a compact model. Through our extensive experiments on six\ndiverse NLP tasks, we find that distilling from T5-XXL (11B) to T5-Small (60M)\nleads to almost always a cost-efficient option compared to annotating more data\nto directly train a compact model (T5-Small (60M)). We further demonstrate that\nthe optimal amount of distillation that maximizes utility varies across\ndifferent budgetary scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Junmo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge. (arXiv:2305.01651v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01651","description":"<p>Pre-trained language models (LMs) are used for knowledge intensive tasks like\nquestion answering, but their knowledge gets continuously outdated as the world\nchanges. Prior work has studied targeted updates to LMs, injecting individual\nfacts and evaluating whether the model learns these facts while not changing\npredictions on other contexts. We take a step forward and study LMs' abilities\nto make inferences based on injected facts (or propagate those facts): for\nexample, after learning that something is a TV show, does an LM predict that\nyou can watch it? We study this with two cloze-style tasks: an existing dataset\nof real-world sentences about novel entities (ECBD) as well as a new controlled\nbenchmark with manually designed templates requiring varying levels of\ninference about injected knowledge. Surprisingly, we find that existing methods\nfor updating knowledge (gradient-based fine-tuning and modifications of this\napproach) show little propagation of injected knowledge. These methods improve\nperformance on cloze instances only when there is lexical overlap between\ninjected facts and target inferences. Yet, prepending entity definitions in an\nLM's context improves performance across all settings, suggesting that there is\nsubstantial headroom for parameter-updating approaches for knowledge injection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Onoe_Y/0/1/0/all/0/1\">Yasumasa Onoe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Michael J.Q. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmanabhan_S/0/1/0/all/0/1\">Shankar Padmanabhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Embeddings: A Survey. (arXiv:1901.09069v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1901.09069","description":"<p>This work lists and describes the main recent strategies for building\nfixed-length, dense and distributed representations for words, based on the\ndistributional hypothesis. These representations are now commonly called word\nembeddings and, in addition to encoding surprisingly good syntactic and\nsemantic information, have been proven useful as extra features in many\ndownstream NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Almeida_F/0/1/0/all/0/1\">Felipe Almeida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xexeo_G/0/1/0/all/0/1\">Geraldo Xex&#xe9;o</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Learning to Speak and Hear Through Multi-Agent Communication over a Continuous Acoustic Channel. (arXiv:2111.02827v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.02827","description":"<p>Multi-agent reinforcement learning has been used as an effective means to\nstudy emergent communication between agents, yet little focus has been given to\ncontinuous acoustic communication. This would be more akin to human language\nacquisition; human infants acquire language in large part through continuous\nsignalling with their caregivers. We therefore ask: Are we able to observe\nemergent language between agents with a continuous communication channel? Our\ngoal is to provide a platform to begin bridging the gap between human and agent\ncommunication, allowing us to analyse continuous signals, how they emerge,\ntheir characteristics, and how they relate to human language acquisition. We\npropose a messaging environment where a Speaker agent needs to convey a set of\nattributes to a Listener over a noisy acoustic channel. Using DQN to train our\nagents, we show that: (1) unlike the discrete case, the acoustic Speaker learns\nredundancy to improve Listener coherency, (2) the acoustic Speaker develops\nmore compositional communication protocols which implicitly compensates for\ntransmission errors over a noisy channel, and (3) DQN has significant\nperformance gains and increased compositionality when compared to previous\nmethods optimised using REINFORCE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eloff_K/0/1/0/all/0/1\">Kevin Eloff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelbrecht_H/0/1/0/all/0/1\">Herman A. Engelbrecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pretorius_A/0/1/0/all/0/1\">Arnu Pretorius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1\">Herman Kamper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Training for Back-Translation with Categorical Reparameterization Trick. (arXiv:2202.08465v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.08465","description":"<p>Back-translation is an effective semi-supervised learning framework in neural\nmachine translation (NMT). A pre-trained NMT model translates monolingual\nsentences and makes synthetic bilingual sentence pairs for the training of the\nother NMT model, and vice versa. Understanding the two NMT models as inference\nand generation models, respectively, previous works applied the training\nframework of variational auto-encoder (VAE). However, the discrete property of\ntranslated sentences prevents gradient information from flowing between the two\nNMT models. In this paper, we propose a categorical reparameterization trick\nthat makes NMT models generate differentiable sentences so that the VAE's\ntraining framework can work in the end-to-end fashion. Our experiments\ndemonstrate that our method effectively trains the NMT models and achieves\nbetter BLEU scores than the previous baseline on the datasets of the WMT\ntranslation task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heo_D/0/1/0/all/0/1\">DongNyeong Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Heeyoul Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Know your audience: specializing grounded language models with listener subtraction. (arXiv:2206.08349v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.08349","description":"<p>Effective communication requires adapting to the idiosyncrasies of each\ncommunicative context--such as the common ground shared with each partner.\nHumans demonstrate this ability to specialize to their audience in many\ncontexts, such as the popular game Dixit. We take inspiration from Dixit to\nformulate a multi-agent image reference game where a (trained) speaker model is\nrewarded for describing a target image such that one (pretrained) listener\nmodel can correctly identify it among distractors, but another listener cannot.\nTo adapt, the speaker must exploit differences in the knowledge it shares with\nthe different listeners. We show that finetuning an attention-based adapter\nbetween a CLIP vision encoder and a large language model in this contrastive,\nmulti-agent setting gives rise to context-dependent natural language\nspecialization from rewards only, without direct supervision. Through\ncontrolled experiments, we show that training a speaker with two listeners that\nperceive differently, using our method, allows the speaker to adapt to the\nidiosyncracies of the listeners. Furthermore, we show zero-shot transfer of the\nspecialization to real-world data. Our experiments demonstrate a method for\nspecializing grounded language models without direct supervision and highlight\nthe interesting research challenges posed by complex multi-agent communication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aaditya K. Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">David Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxe_A/0/1/0/all/0/1\">Andrew Saxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1\">Andrew K. Lampinen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large scale analysis of gender bias and sexism in song lyrics. (arXiv:2208.02052v5 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2208.02052","description":"<p>We employ Natural Language Processing techniques to analyse 377808 English\nsong lyrics from the \"Two Million Song Database\" corpus, focusing on the\nexpression of sexism across five decades (1960-2010) and the measurement of\ngender biases. Using a sexism classifier, we identify sexist lyrics at a larger\nscale than previous studies using small samples of manually annotated popular\nsongs. Furthermore, we reveal gender biases by measuring associations in word\nembeddings learned on song lyrics. We find sexist content to increase across\ntime, especially from male artists and for popular songs appearing in Billboard\ncharts. Songs are also shown to contain different language biases depending on\nthe gender of the performer, with male solo artist songs containing more and\nstronger biases. This is the first large scale analysis of this type, giving\ninsights into language usage in such an influential part of popular culture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Betti_L/0/1/0/all/0/1\">Lorenzo Betti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abrate_C/0/1/0/all/0/1\">Carlo Abrate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaltenbrunner_A/0/1/0/all/0/1\">Andreas Kaltenbrunner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Executable Action Plans with Environmentally-Aware Language Models. (arXiv:2210.04964v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2210.04964","description":"<p>Large Language Models (LLMs) trained using massive text datasets have\nrecently shown promise in generating action plans for robotic agents from high\nlevel text queries. However, these models typically do not consider the robot's\nenvironment, resulting in generated plans that may not actually be executable,\ndue to ambiguities in the planned actions or environmental constraints. In this\npaper, we propose an approach to generate environmentally-aware action plans\nthat agents are better able to execute. Our approach involves integrating\nenvironmental objects and object relations as additional inputs into LLM action\nplan generation to provide the system with an awareness of its surroundings,\nresulting in plans where each generated action is mapped to objects present in\nthe scene. We also design a novel scoring function that, along with generating\nthe action steps and associating them with objects, helps the system\ndisambiguate among object instances and take into account their states. We\nevaluated our approach using the VirtualHome simulator and the ActivityPrograms\nknowledge base and found that action plans generated from our system had a 310%\nimprovement in executability and a 147% improvement in correctness over prior\nwork. The complete code and a demo of our method is publicly available at\nhttps://github.com/hri-ironlab/scene_aware_language_planner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gramopadhye_M/0/1/0/all/0/1\">Maitrey Gramopadhye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szafir_D/0/1/0/all/0/1\">Daniel Szafir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Long-tail Generalization with Likelihood Splits. (arXiv:2210.06799v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06799","description":"<p>In order to reliably process natural language, NLP systems must generalize to\nthe long tail of rare utterances. We propose a method to create challenging\nbenchmarks that require generalizing to the tail of the distribution by\nre-splitting existing datasets. We create 'Likelihood Splits' where examples\nthat are assigned lower likelihood by a pre-trained language model (LM) are\nplaced in the test set, and more likely examples are in the training set. This\nsimple approach can be customized to construct meaningful train-test splits for\na wide range of tasks. Likelihood Splits surface more challenges than random\nsplits: relative error rates of state-of-the-art models increase by 59% for\nsemantic parsing on Spider, 93% for natural language inference on SNLI, and 33%\nfor yes/no question answering on BoolQ, on our splits compared with the\ncorresponding random splits. Moreover, Likelihood Splits create fairer\nbenchmarks than adversarial filtering; when the LM used to create the splits is\nalso employed as the task model, our splits do not unfairly penalize the LM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Godbole_A/0/1/0/all/0/1\">Ameya Godbole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing Verbatim Short-Term Memory in Neural Language Models. (arXiv:2210.13569v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13569","description":"<p>When a language model is trained to predict natural language sequences, its\nprediction at each moment depends on a representation of prior context. What\nkind of information about the prior context can language models retrieve? We\ntested whether language models could retrieve the exact words that occurred\npreviously in a text. In our paradigm, language models (transformers and an\nLSTM) processed English text in which a list of nouns occurred twice. We\noperationalized retrieval as the reduction in surprisal from the first to the\nsecond list. We found that the transformers retrieved both the identity and\nordering of nouns from the first list. Further, the transformers' retrieval was\nmarkedly enhanced when they were trained on a larger corpus and with greater\nmodel depth. Lastly, their ability to index prior tokens was dependent on\nlearned attention patterns. In contrast, the LSTM exhibited less precise\nretrieval, which was limited to list-initial tokens and to short intervening\ntexts. The LSTM's retrieval was not sensitive to the order of nouns and it\nimproved when the list was semantically coherent. We conclude that transformers\nimplemented something akin to a working memory system that could flexibly\nretrieve individual token representations across arbitrary delays; conversely,\nthe LSTM maintained a coarser and more rapidly-decaying semantic gist of prior\ntokens, weighted toward the earliest items.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Armeni_K/0/1/0/all/0/1\">Kristijan Armeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honey_C/0/1/0/all/0/1\">Christopher Honey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frustratingly Easy Label Projection for Cross-lingual Transfer. (arXiv:2211.15613v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.15613","description":"<p>Translating training data into many languages has emerged as a practical\nsolution for improving cross-lingual transfer. For tasks that involve\nspan-level annotations, such as information extraction or question answering,\nan additional label projection step is required to map annotated spans onto the\ntranslated texts. Recently, a few efforts have utilized a simple\nmark-then-translate method to jointly perform translation and projection by\ninserting special markers around the labeled spans in the original sentence.\nHowever, as far as we are aware, no empirical analysis has been conducted on\nhow this approach compares to traditional annotation projection based on word\nalignment. In this paper, we present an extensive empirical study across 57\nlanguages and three tasks (QA, NER, and Event Extraction) to evaluate the\neffectiveness and limitations of both methods, filling an important gap in the\nliterature. Experimental results show that our optimized version of\nmark-then-translate, which we call EasyProject, is easily applied to many\nlanguages and works surprisingly well, outperforming the more complex word\nalignment-based methods. We analyze several key factors that affect the\nend-task performance, and show EasyProject works well because it can accurately\npreserve label span boundaries after translation. We will publicly release all\nour code and data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoRRPUS: Codex-Leveraged Structured Representations for Neurosymbolic Story Understanding. (arXiv:2212.10754v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10754","description":"<p>Story generation and understanding -- as with all NLG/NLU tasks -- has seen a\nsurge in neurosymbolic work. Researchers have recognized that, while large\nlanguage models (LLMs) have tremendous utility, they can be augmented with\nsymbolic means to be even better and to make up for any flaws that the neural\nnetworks might have. However, symbolic methods are extremely costly in terms of\nthe amount of time and expertise needed to create them. In this work, we\ncapitalize on state-of-the-art Code-LLMs, such as Codex, to bootstrap the use\nof symbolic methods for tracking the state of stories and aiding in story\nunderstanding. We show that our CoRRPUS system and abstracted prompting\nprocedures can beat current state-of-the-art structured LLM techniques on\npre-existing story understanding tasks (bAbI task 2 and Re^3) with minimal hand\nengineering. We hope that this work can help highlight the importance of\nsymbolic representations and specialized prompting for LLMs as these models\nrequire some guidance for performing reasoning tasks properly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yijiang River Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1\">Lara J. Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model Analysis for Ontology Subsumption Inference. (arXiv:2302.06761v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.06761","description":"<p>Investigating whether pre-trained language models (LMs) can function as\nknowledge bases (KBs) has raised wide research interests recently. However,\nexisting works focus on simple, triple-based, relational KBs, but omit more\nsophisticated, logic-based, conceptualised KBs such as OWL ontologies. To\ninvestigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of\ninference-based probing tasks and datasets from ontology subsumption axioms\ninvolving both atomic and complex concepts. We conduct extensive experiments on\nontologies of different domains and scales, and our results demonstrate that\nLMs encode relatively less background knowledge of Subsumption Inference (SI)\nthan traditional Natural Language Inference (NLI) but can improve on SI\nsignificantly when a small number of samples are given. We will open-source our\ncode and datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jimenez_Ruiz_E/0/1/0/all/0/1\">Ernesto Jim&#xe9;nez-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horrocks_I/0/1/0/all/0/1\">Ian Horrocks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AUTODIAL: Efficient Asynchronous Task-Oriented Dialogue Model. (arXiv:2303.06245v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.06245","description":"<p>As large dialogue models become commonplace in practice, the problems\nsurrounding high compute requirements for training, inference and larger memory\nfootprint still persists. In this work, we present AUTODIAL, a multi-task\ndialogue model that addresses the challenges of deploying dialogue model.\nAUTODIAL utilizes parallel decoders to perform tasks such as dialogue act\nprediction, domain prediction, intent prediction, and dialogue state tracking.\nUsing classification decoders over generative decoders allows AUTODIAL to\nsignificantly reduce memory footprint and achieve faster inference times\ncompared to existing generative approach namely SimpleTOD. We demonstrate that\nAUTODIAL provides 3-6x speedups during inference while having 11x fewer\nparameters on three dialogue tasks compared to SimpleTOD. Our results show that\nextending current dialogue models to have parallel decoders can be a viable\nalternative for deploying them in resource-constrained environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhargava_P/0/1/0/all/0/1\">Prajjwal Bhargava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_P/0/1/0/all/0/1\">Pooyan Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shayandeh_S/0/1/0/all/0/1\">Shahin Shayandeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1\">Chinnadhurai Sankar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning. (arXiv:2303.10475v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.10475","description":"<p>Task semantics can be expressed by a set of input-to-output examples or a\npiece of textual instruction. Conventional machine learning approaches for\nnatural language processing (NLP) mainly rely on the availability of\nlarge-scale sets of task-specific examples. Two issues arise: first, collecting\ntask-specific labeled examples does not apply to scenarios where tasks may be\ntoo complicated or costly to annotate, or the system is required to handle a\nnew task immediately; second, this is not user-friendly since end-users are\nprobably more willing to provide task description rather than a set of examples\nbefore using the system. Therefore, the community is paying increasing interest\nin a new supervision-seeking paradigm for NLP: learning from task instructions.\nDespite its impressive progress, there are some common issues that the\ncommunity struggles with. This survey paper tries to summarize the current\nresearch on instruction learning, particularly, by answering the following\nquestions: (i) what is task instruction, and what instruction types exist? (ii)\nhow to model instructions? (iii) what factors influence and explain the\ninstructions' performance? (iv) what challenges remain in instruction learning?\nTo our knowledge, this is the first comprehensive survey about textual\ninstructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_R/0/1/0/all/0/1\">Renze Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenpeng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. (arXiv:2304.03279v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2304.03279","description":"<p>Artificial agents have traditionally been trained to maximize reward, which\nmay incentivize power-seeking and deception, analogous to how next-token\nprediction in language models (LMs) may incentivize toxicity. So do agents\nnaturally learn to be Machiavellian? And how do we measure these behaviors in\ngeneral-purpose models such as GPT-4? Towards answering these questions, we\nintroduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games\ncontaining over half a million rich, diverse scenarios that center on social\ndecision-making. Scenario labeling is automated with LMs, which are more\nperformant than human annotators. We mathematize dozens of harmful behaviors\nand use our annotations to evaluate agents' tendencies to be power-seeking,\ncause disutility, and commit ethical violations. We observe some tension\nbetween maximizing reward and behaving ethically. To improve this trade-off, we\ninvestigate LM-based methods to steer agents' towards less harmful behaviors.\nOur results show that agents can both act competently and morally, so concrete\nprogress can currently be made in machine ethics--designing agents that are\nPareto improvements in both safety and capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_A/0/1/0/all/0/1\">Alexander Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shern_C/0/1/0/all/0/1\">Chan Jun Shern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1\">Andy Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nathaniel Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodside_T/0/1/0/all/0/1\">Thomas Woodside</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_J/0/1/0/all/0/1\">Jonathan Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emmons_S/0/1/0/all/0/1\">Scott Emmons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis. (arXiv:2304.04675v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.04675","description":"<p>Large language models (LLMs) have demonstrated remarkable potential in\nhandling multilingual machine translation (MMT). In this paper, we\nsystematically investigate the advantages and challenges of LLMs for MMT by\nanswering two questions: 1) How well do LLMs perform in translating a massive\nnumber of languages? 2) Which factors affect LLMs' performance in translation?\nWe evaluate popular LLMs, including XGLM, OPT, BLOOMZ, and ChatGPT, on 102\nlanguages. Our empirical results show that even the best model ChatGPT still\nlags behind the supervised baseline NLLB in 83.33% of translation directions.\nThrough further analysis, we discover that LLMs exhibit new working patterns\nwhen used for MMT. First, prompt semantics can surprisingly be ignored when\ngiven in-context exemplars, where LLMs still show strong performance even with\nunreasonable prompts. Second, cross-lingual exemplars can provide better task\ninstruction for low-resource translation than exemplars in the same language\npairs. Third, we observe the overestimated performance of BLOOMZ on dataset\nFlores-101, indicating the potential risk when using public datasets for\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenhao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qingxiu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sebis at SemEval-2023 Task 7: A Joint System for Natural Language Inference and Evidence Retrieval from Clinical Trial Reports. (arXiv:2304.13180v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.13180","description":"<p>With the increasing number of clinical trial reports generated every day, it\nis becoming hard to keep up with novel discoveries that inform evidence-based\nhealthcare recommendations. To help automate this process and assist medical\nexperts, NLP solutions are being developed. This motivated the SemEval-2023\nTask 7, where the goal was to develop an NLP system for two tasks: evidence\nretrieval and natural language inference from clinical trial data. In this\npaper, we describe our two developed systems. The first one is a pipeline\nsystem that models the two tasks separately, while the second one is a joint\nsystem that learns the two tasks simultaneously with a shared representation\nand a multi-task learning approach. The final system combines their outputs in\nan ensemble system. We formalize the models, present their characteristics and\nchallenges, and provide an analysis of achieved results. Our system ranked 3rd\nout of 40 participants with a final submission.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vladika_J/0/1/0/all/0/1\">Juraj Vladika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthes_F/0/1/0/all/0/1\">Florian Matthes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards autonomous system: flexible modular production system enhanced with large language model agents. (arXiv:2304.14721v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2304.14721","description":"<p>In this paper, we present a novel framework that combines large language\nmodels (LLMs), digital twins and industrial automation system to enable\nintelligent planning and control of production processes. We retrofit the\nautomation system for a modular production facility and create executable\ncontrol interfaces of fine-granular functionalities and coarse-granular skills.\nLow-level functionalities are executed by automation components, and high-level\nskills are performed by automation modules. Subsequently, a digital twin system\nis developed, registering these interfaces and containing additional\ndescriptive information about the production system. Based on the retrofitted\nautomation system and the created digital twins, LLM-agents are designed to\ninterpret descriptive information in the digital twins and control the physical\nsystem through service interfaces. These LLM-agents serve as intelligent agents\non different levels within an automation system, enabling autonomous planning\nand control of flexible production. Given a task instruction as input, the\nLLM-agents orchestrate a sequence of atomic functionalities and skills to\naccomplish the task. We demonstrate how our implemented prototype can handle\nun-predefined tasks, plan a production process, and execute the operations.\nThis research highlights the potential of integrating LLMs into industrial\nautomation systems in the context of smart factory for more agile, flexible,\nand adaptive production processes, while it also underscores the critical\ninsights and limitations for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yuchen Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_M/0/1/0/all/0/1\">Manthan Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jazdi_N/0/1/0/all/0/1\">Nasser Jazdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weyrich_M/0/1/0/all/0/1\">Michael Weyrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Still no evidence for an effect of the proportion of non-native speakers on language complexity -- A response to Kauhanen, Einhaus & Walkden (2023). (arXiv:2305.00217v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.00217","description":"<p>In a recent paper published in the Journal of Language Evolution, Kauhanen,\nEinhaus &amp; Walkden (https://doi.org/10.1093/jole/lzad005, KEW) challenge the\nresults presented in one of my papers (Koplenig, Royal Society Open Science, 6,\n181274 (2019), https://doi.org/10.1098/rsos.181274), in which I tried to show\nthrough a series of statistical analyses that large numbers of L2 (second\nlanguage) speakers do not seem to affect the (grammatical or statistical)\ncomplexity of a language. To this end, I focus on the way in which the\nEthnologue assesses language status: a language is characterised as vehicular\nif, in addition to being used by L1 (first language) speakers, it should also\nhave a significant number of L2 users. KEW criticise both the use of\nvehicularity as a (binary) indicator of whether a language has a significant\nnumber of L2 users and the idea of imputing a zero proportion of L2 speakers to\nnon-vehicular languages whenever a direct estimate of that proportion is\nunavailable. While I recognise the importance of post-publication commentary on\npublished research, I show in this rejoinder that both points of criticism are\nexplicitly mentioned and analysed in my paper. In addition, I also comment on\nother points raised by KEW and demonstrate that both alternative analyses\noffered by KEW do not stand up to closer scrutiny.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koplenig_A/0/1/0/all/0/1\">Alexander Koplenig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding. (arXiv:2305.00633v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.00633","description":"<p>We endow Large Language Models (LLMs) with fine-grained self-evaluation to\nrefine multi-step reasoning inference. We propose an effective prompting\napproach that integrates self-evaluation guidance through stochastic beam\nsearch. Our approach explores the reasoning search space using a\nwell-calibrated automatic criterion. This enables an efficient search to\nproduce higher-quality final predictions. With the self-evaluation guided\nstochastic beam search, we also balance the quality-diversity trade-off in the\ngeneration of reasoning chains. This allows our approach to adapt well with\nmajority voting and surpass the corresponding Codex-backboned baselines by\n$6.34\\%$, $9.56\\%$, and $5.46\\%$ on the GSM8K, AQuA, and StrategyQA benchmarks,\nrespectively, in few-shot accuracy. Analysis of our decompositional reasoning\nfinds it pinpoints logic failures and leads to higher consistency and\nrobustness. Our code is publicly available at\nhttps://github.com/YuxiXie/SelfEval-Guided-Decoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yiran Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Min-Yen Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qizhe Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-05-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}