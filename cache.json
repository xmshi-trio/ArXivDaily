{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-09-27T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM. (arXiv:2309.14348v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14348","description":"<p>Recently, Large Language Models (LLMs) have made significant advancements and\nare now widely used across various domains. Unfortunately, there has been a\nrising concern that LLMs can be misused to generate harmful or malicious\ncontent. Though a line of research has focused on aligning LLMs with human\nvalues and preventing them from producing inappropriate content, such\nalignments are usually vulnerable and can be bypassed by alignment-breaking\nattacks via adversarially optimized or handcrafted jailbreaking prompts. In\nthis work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against\npotential alignment-breaking attacks. RA-LLM can be directly constructed upon\nan existing aligned LLM with a robust alignment checking function, without\nrequiring any expensive retraining or fine-tuning process of the original LLM.\nFurthermore, we also provide a theoretical analysis for RA-LLM to verify its\neffectiveness in defending against alignment-breaking attacks. Through\nreal-world experiments on open-source large language models, we demonstrate\nthat RA-LLM can successfully defend against both state-of-the-art adversarial\nprompts and popular handcrafted jailbreaking prompts by reducing their attack\nsuccess rates from nearly 100\\% to around 10\\% or less.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Bochuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuanpu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinghui Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PopBERT. Detecting populism and its host ideologies in the German Bundestag. (arXiv:2309.14355v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14355","description":"<p>The rise of populism concerns many political scientists and practitioners,\nyet the detection of its underlying language remains fragmentary. This paper\naims to provide a reliable, valid, and scalable approach to measure populist\nstances. For that purpose, we created an annotated dataset based on\nparliamentary speeches of the German Bundestag (2013 to 2021). Following the\nideational definition of populism, we label moralizing references to the\nvirtuous people or the corrupt elite as core dimensions of populist language.\nTo identify, in addition, how the thin ideology of populism is thickened, we\nannotate how populist statements are attached to left-wing or right-wing host\nideologies. We then train a transformer-based model (PopBERT) as a multilabel\nclassifier to detect and quantify each dimension. A battery of validation\nchecks reveals that the model has a strong predictive accuracy, provides high\nqualitative face validity, matches party rankings of expert surveys, and\ndetects out-of-sample text snippets correctly. PopBERT enables dynamic analyses\nof how German-speaking politicians and parties use populist language as a\nstrategic device. Furthermore, the annotator-level data may also be applied in\ncross-domain applications or to develop related classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Erhard_L/0/1/0/all/0/1\">L. Erhard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanke_S/0/1/0/all/0/1\">S. Hanke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remer_U/0/1/0/all/0/1\">U. Remer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falenska_A/0/1/0/all/0/1\">A. Falenska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heiberger_R/0/1/0/all/0/1\">R. Heiberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs. (arXiv:2309.14356v1 [cs.LG])","link":"http://arxiv.org/abs/2309.14356","description":"<p>Counterfactual examples have proven to be valuable in the field of natural\nlanguage processing (NLP) for both evaluating and improving the robustness of\nlanguage models to spurious correlations in datasets. Despite their\ndemonstrated utility for NLP, multimodal counterfactual examples have been\nrelatively unexplored due to the difficulty of creating paired image-text data\nwith minimal counterfactual changes. To address this challenge, we introduce a\nscalable framework for automatic generation of counterfactual examples using\ntext-to-image diffusion models. We use our framework to create\nCOCO-Counterfactuals, a multimodal counterfactual dataset of paired image and\ntext captions based on the MS-COCO dataset. We validate the quality of\nCOCO-Counterfactuals through human evaluations and show that existing\nmultimodal models are challenged by our counterfactual image-text pairs.\nAdditionally, we demonstrate the usefulness of COCO-Counterfactuals for\nimproving out-of-domain generalization of multimodal vision-language models via\ntraining data augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Tiep Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1\">Vasudev Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howard_P/0/1/0/all/0/1\">Phillip Howard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diversifying Question Generation over Knowledge Base via External Natural Questions. (arXiv:2309.14362v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14362","description":"<p>Previous methods on knowledge base question generation (KBQG) primarily focus\non enhancing the quality of a single generated question. Recognizing the\nremarkable paraphrasing ability of humans, we contend that diverse texts should\nconvey the same semantics through varied expressions. The above insights make\ndiversifying question generation an intriguing task, where the first challenge\nis evaluation metrics for diversity. Current metrics inadequately assess the\nabove diversity since they calculate the ratio of unique n-grams in the\ngenerated question itself, which leans more towards measuring duplication\nrather than true diversity. Accordingly, we devise a new diversity evaluation\nmetric, which measures the diversity among top-k generated questions for each\ninstance while ensuring their relevance to the ground truth. Clearly, the\nsecond challenge is how to enhance diversifying question generation. To address\nthis challenge, we introduce a dual model framework interwoven by two selection\nstrategies to generate diverse questions leveraging external natural questions.\nThe main idea of our dual framework is to extract more diverse expressions and\nintegrate them into the generation model to enhance diversifying question\ngeneration. Extensive experiments on widely used benchmarks for KBQG\ndemonstrate that our proposed approach generates highly diverse questions and\nimproves the performance of question answering tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shasha Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_X/0/1/0/all/0/1\">Xirui Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cuiping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An In-depth Survey of Large Language Model-based Artificial Intelligence Agents. (arXiv:2309.14365v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14365","description":"<p>Due to the powerful capabilities demonstrated by large language model (LLM),\nthere has been a recent surge in efforts to integrate them with AI agents to\nenhance their performance. In this paper, we have explored the core differences\nand characteristics between LLM-based AI agents and traditional AI agents.\nSpecifically, we first compare the fundamental characteristics of these two\ntypes of agents, clarifying the significant advantages of LLM-based agents in\nhandling natural language, knowledge storage, and reasoning capabilities.\nSubsequently, we conducted an in-depth analysis of the key components of AI\nagents, including planning, memory, and tool use. Particularly, for the crucial\ncomponent of memory, this paper introduced an innovative classification scheme,\nnot only departing from traditional classification methods but also providing a\nfresh perspective on the design of an AI agent's memory system. We firmly\nbelieve that in-depth research and understanding of these core components will\nlay a solid foundation for the future advancement of AI agent technology. At\nthe end of the paper, we provide directional suggestions for further research\nin this field, with the hope of offering valuable insights to scholars and\nresearchers in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pengyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zijian Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Ning Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Transcription Quality Improvement. (arXiv:2309.14372v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14372","description":"<p>High quality transcription data is crucial for training automatic speech\nrecognition (ASR) systems. However, the existing industry-level data collection\npipelines are expensive to researchers, while the quality of crowdsourced\ntranscription is low. In this paper, we propose a reliable method to collect\nspeech transcriptions. We introduce two mechanisms to improve transcription\nquality: confidence estimation based reprocessing at labeling stage, and\nautomatic word error correction at post-labeling stage. We collect and release\nLibriCrowd - a large-scale crowdsourced dataset of audio transcriptions on 100\nhours of English speech. Experiment shows the Transcription WER is reduced by\nover 50%. We further investigate the impact of transcription error on ASR model\nperformance and found a strong correlation. The transcription quality\nimprovement provides over 10% relative WER reduction for ASR models. We release\nthe dataset and code to benefit the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jian Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hanbo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Cheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zheng Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Text Classification-Based Approach for Evaluating and Enhancing the Machine Interpretability of Building Codes. (arXiv:2309.14374v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14374","description":"<p>Interpreting regulatory documents or building codes into computer-processable\nformats is essential for the intelligent design and construction of buildings\nand infrastructures. Although automated rule interpretation (ARI) methods have\nbeen investigated for years, most of them highly depend on the early and manual\nfiltering of interpretable clauses from a building code. While few of them\nconsidered machine interpretability, which represents the potential to be\ntransformed into a computer-processable format, from both clause- and\ndocument-level. Therefore, this research aims to propose a novel approach to\nautomatically evaluate and enhance the machine interpretability of single\nclause and building codes. First, a few categories are introduced to classify\neach clause in a building code considering the requirements for rule\ninterpretation, and a dataset is developed for model training. Then, an\nefficient text classification model is developed based on a pretrained\ndomain-specific language model and transfer learning techniques. Finally, a\nquantitative evaluation method is proposed to assess the overall\ninterpretability of building codes. Experiments show that the proposed text\nclassification algorithm outperforms the existing CNN- or RNN-based methods,\nimproving the F1-score from 72.16% to 93.60%. It is also illustrated that the\nproposed classification method can enhance downstream ARI methods with an\nimprovement of 4%. Furthermore, analyzing the results of more than 150 building\ncodes in China showed that their average interpretability is 34.40%, which\nimplies that it is still hard to fully transform the entire regulatory document\ninto computer-processable formats. It is also argued that the interpretability\nof building codes should be further improved both from the human side and the\nmachine side.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu-Cheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Ke-Yin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xin-Zheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Z/0/1/0/all/0/1\">Zhong-Tian She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jia-Rui Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine-assisted mixed methods: augmenting humanities and social sciences with artificial intelligence. (arXiv:2309.14379v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14379","description":"<p>The increasing capacities of large language models (LLMs) present an\nunprecedented opportunity to scale up data analytics in the humanities and\nsocial sciences, augmenting and automating qualitative analytic tasks\npreviously typically allocated to human labor. This contribution proposes a\nsystematic mixed methods framework to harness qualitative analytic expertise,\nmachine scalability, and rigorous quantification, with attention to\ntransparency and replicability. 16 machine-assisted case studies are showcased\nas proof of concept. Tasks include linguistic and discourse analysis, lexical\nsemantic change detection, interview analysis, historical event cause inference\nand text mining, detection of political stance, text and idea reuse, genre\ncomposition in literature and film; social network inference, automated\nlexicography, missing metadata augmentation, and multimodal visual cultural\nanalytics. In contrast to the focus on English in the emerging LLM\napplicability literature, many examples here deal with scenarios involving\nsmaller languages and historical texts prone to digitization distortions. In\nall but the most difficult tasks requiring expert knowledge, generative LLMs\ncan demonstrably serve as viable research instruments. LLM (and human)\nannotations may contain errors and variation, but the agreement rate can and\nshould be accounted for in subsequent statistical modeling; a bootstrapping\napproach is discussed. The replications among the case studies illustrate how\ntasks previously requiring potentially months of team effort and complex\ncomputational pipelines, can now be accomplished by an LLM-assisted scholar in\na fraction of the time. Importantly, this approach is not intended to replace,\nbut to augment researcher knowledge and skills. With these opportunities in\nsight, qualitative expertise and the ability to pose insightful questions have\narguably never been more critical.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karjus_A/0/1/0/all/0/1\">Andres Karjus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey of Social Bias in Vision-Language Models. (arXiv:2309.14381v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14381","description":"<p>In recent years, the rapid advancement of machine learning (ML) models,\nparticularly transformer-based pre-trained models, has revolutionized Natural\nLanguage Processing (NLP) and Computer Vision (CV) fields. However, researchers\nhave discovered that these models can inadvertently capture and reinforce\nsocial biases present in their training datasets, leading to potential social\nharms, such as uneven resource allocation and unfair representation of specific\nsocial groups. Addressing these biases and ensuring fairness in artificial\nintelligence (AI) systems has become a critical concern in the ML community.\n</p>\n<p>The recent introduction of pre-trained vision-and-language (VL) models in the\nemerging multimodal field demands attention to the potential social biases\npresent in these models as well. Although VL models are susceptible to social\nbias, there is a limited understanding compared to the extensive discussions on\nbias in NLP and CV. This survey aims to provide researchers with a high-level\ninsight into the similarities and differences of social bias studies in\npre-trained models across NLP, CV, and VL. By examining these perspectives, the\nsurvey aims to offer valuable guidelines on how to approach and mitigate social\nbias in both unimodal and multimodal settings. The findings and recommendations\npresented here can benefit the ML community, fostering the development of\nfairer and non-biased AI models in various applications and research endeavors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bang_Y/0/1/0/all/0/1\">Yejin Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Agree To Disagree. (arXiv:2309.14382v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14382","description":"<p>How frequently do individuals thoroughly review terms and conditions before\nproceeding to register for a service, install software, or access a website?\nThe majority of internet users do not engage in this practice. This trend is\nnot surprising, given that terms and conditions typically consist of lengthy\ndocuments replete with intricate legal terminology and convoluted sentences. In\nthis paper, we introduce a Machine Learning-powered approach designed to\nautomatically parse and summarize critical information in a user-friendly\nmanner. This technology focuses on distilling the pertinent details that users\nshould contemplate before committing to an agreement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raghuvanshi_A/0/1/0/all/0/1\">Abhinav Raghuvanshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pawar_S/0/1/0/all/0/1\">Siddhesh Pawar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Anirudh Mittal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An AI Chatbot for Explaining Deep Reinforcement Learning Decisions of Service-oriented Systems. (arXiv:2309.14391v1 [cs.LG])","link":"http://arxiv.org/abs/2309.14391","description":"<p>Deep Reinforcement Learning (Deep RL) is increasingly used to cope with the\nopen-world assumption in service-oriented systems. Deep RL was successfully\napplied to problems such as dynamic service composition, job scheduling, and\noffloading, as well as service adaptation. While Deep RL offers many benefits,\nunderstanding the decision-making of Deep RL is challenging because its learned\ndecision-making policy essentially appears as a black box. Yet, understanding\nthe decision-making of Deep RL is key to help service developers perform\ndebugging, support service providers to comply with relevant legal frameworks,\nand facilitate service users to build trust. We introduce Chat4XAI to\nfacilitate the understanding of the decision-making of Deep RL by providing\nnatural-language explanations. Compared with visual explanations, the reported\nbenefits of natural-language explanations include better understandability for\nnon-technical users, increased user acceptance and trust, as well as more\nefficient explanations. Chat4XAI leverages modern AI chatbot technology and\ndedicated prompt engineering. Compared to earlier work on natural-language\nexplanations using classical software-based dialogue systems, using an AI\nchatbot eliminates the need for eliciting and defining potential questions and\nanswers up-front. We prototypically realize Chat4XAI using OpenAI's ChatGPT API\nand evaluate the fidelity and stability of its explanations using an adaptive\nservice exemplar.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Metzger_A/0/1/0/all/0/1\">Andreas Metzger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartel_J/0/1/0/all/0/1\">Jone Bartel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laufer_J/0/1/0/all/0/1\">Jan Laufer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models. (arXiv:2309.14393v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14393","description":"<p>The carbon footprint associated with large language models (LLMs) is a\nsignificant concern, encompassing emissions from their training, inference,\nexperimentation, and storage processes, including operational and embodied\ncarbon emissions. An essential aspect is accurately estimating the carbon\nimpact of emerging LLMs even before their training, which heavily relies on GPU\nusage. Existing studies have reported the carbon footprint of LLM training, but\nonly one tool, mlco2, can predict the carbon footprint of new neural networks\nprior to physical training. However, mlco2 has several serious limitations. It\ncannot extend its estimation to dense or mixture-of-experts (MoE) LLMs,\ndisregards critical architectural parameters, focuses solely on GPUs, and\ncannot model embodied carbon footprints. Addressing these gaps, we introduce\n\\textit{LLMCarbon}, an end-to-end carbon footprint projection model designed\nfor both dense and MoE LLMs. Compared to mlco2, LLMCarbon significantly\nenhances the accuracy of carbon footprint estimations for various LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faiz_A/0/1/0/all/0/1\">Ahmad Faiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaneda_S/0/1/0/all/0/1\">Sotaro Kaneda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osi_R/0/1/0/all/0/1\">Rita Osi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Parteek Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lei Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation. (arXiv:2309.14394v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14394","description":"<p>Domain-to-domain translation involves generating a target domain sample given\na condition in the source domain. Most existing methods focus on fixed input\nand output domains, i.e. they only work for specific configurations (i.e. for\ntwo domains, either $D_1\\rightarrow{}D_2$ or $D_2\\rightarrow{}D_1$). This paper\nproposes Multi-Domain Diffusion (MDD), a conditional diffusion framework for\nmulti-domain translation in a semi-supervised context. Unlike previous methods,\nMDD does not require defining input and output domains, allowing translation\nbetween any partition of domains within a set (such as $(D_1,\nD_2)\\rightarrow{}D_3$, $D_2\\rightarrow{}(D_1, D_3)$, $D_3\\rightarrow{}D_1$,\netc. for 3 domains), without the need to train separate models for each domain\nconfiguration. The key idea behind MDD is to leverage the noise formulation of\ndiffusion models by incorporating one noise level per domain, which allows\nmissing domains to be modeled with noise in a natural way. This transforms the\ntraining task from a simple reconstruction task to a domain translation task,\nwhere the model relies on less noisy domains to reconstruct more noisy domains.\nWe present results on a multi-domain (with more than two domains) synthetic\nimage translation dataset with challenging semantic domain inversion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mayet_T/0/1/0/all/0/1\">Tsiry Mayet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernard_S/0/1/0/all/0/1\">Simon Bernard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatelain_C/0/1/0/all/0/1\">Clement Chatelain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herault_R/0/1/0/all/0/1\">Romain Herault</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeing and hearing what has not been said; A multimodal client behavior classifier in Motivational Interviewing with interpretable fusion. (arXiv:2309.14398v1 [cs.LG])","link":"http://arxiv.org/abs/2309.14398","description":"<p>Motivational Interviewing (MI) is an approach to therapy that emphasizes\ncollaboration and encourages behavioral change. To evaluate the quality of an\nMI conversation, client utterances can be classified using the MISC code as\neither change talk, sustain talk, or follow/neutral talk. The proportion of\nchange talk in a MI conversation is positively correlated with therapy\noutcomes, making accurate classification of client utterances essential. In\nthis paper, we present a classifier that accurately distinguishes between the\nthree MISC classes (change talk, sustain talk, and follow/neutral talk)\nleveraging multimodal features such as text, prosody, facial expressivity, and\nbody expressivity. To train our model, we perform annotations on the publicly\navailable AnnoMI dataset to collect multimodal information, including text,\naudio, facial expressivity, and body expressivity. Furthermore, we identify the\nmost important modalities in the decision-making process, providing valuable\ninsights into the interplay of different modalities during a MI conversation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galland_L/0/1/0/all/0/1\">Lucie Galland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelachaud_C/0/1/0/all/0/1\">Catherine Pelachaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pecune_F/0/1/0/all/0/1\">Florian Pecune</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physics of Language Models: Part 3.2, Knowledge Manipulation. (arXiv:2309.14402v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14402","description":"<p>Language models can store vast amounts of factual knowledge, but their\nability to use this knowledge for logical reasoning remains questionable. This\npaper explores a language model's ability to manipulate its stored knowledge\nduring inference. We focus on four manipulation types: retrieval (e.g., \"What\nis person A's attribute X\"), classification (e.g., \"Is A's attribute X even or\nodd?\"), comparison (e.g., \"Is A greater than B in attribute X?\") and inverse\nsearch (e.g., \"Which person's attribute X equals T?\")\n</p>\n<p>We observe that pre-trained language models like GPT2/3/4 excel in knowledge\nretrieval but struggle with simple classification or comparison tasks unless\nChain of Thoughts (CoTs) are employed during both training and inference. They\nalso perform poorly in inverse knowledge search, irrespective of the prompts.\nOur primary contribution is a synthetic dataset for a controlled experiment\nthat confirms these inherent weaknesses: a language model cannot efficiently\nmanipulate knowledge from pre-training data, even when such knowledge is\nperfectly stored and fully extractable in the models, and despite adequate\ninstruct fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1\">Zeyuan Allen-Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Active Learning For Sound Event Detection. (arXiv:2309.14460v1 [eess.AS])","link":"http://arxiv.org/abs/2309.14460","description":"<p>Data collection and annotation is a laborious, time-consuming prerequisite\nfor supervised machine learning tasks. Online Active Learning (OAL) is a\nparadigm that addresses this issue by simultaneously minimizing the amount of\nannotation required to train a classifier and adapting to changes in the data\nover the duration of the data collection process. Prior work has indicated that\nfluctuating class distributions and data drift are still common problems for\nOAL. This work presents new loss functions that address these challenges when\nOAL is applied to Sound Event Detection (SED). Experimental results from the\nSONYC dataset and two Voice-Type Discrimination (VTD) corpora indicate that OAL\ncan reduce the time and effort required to train SED classifiers by a factor of\n5 for SONYC, and that the new methods presented here successfully resolve\nissues present in existing OAL methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lindsey_M/0/1/0/all/0/1\">Mark Lindsey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shah_A/0/1/0/all/0/1\">Ankit Shah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kubala_F/0/1/0/all/0/1\">Francis Kubala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stern_R/0/1/0/all/0/1\">Richard M. Stern</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable and Accurate Natural Language Understanding for Voice Assistants and Beyond. (arXiv:2309.14485v1 [cs.LG])","link":"http://arxiv.org/abs/2309.14485","description":"<p>Joint intent detection and slot filling, which is also termed as joint NLU\n(Natural Language Understanding) is invaluable for smart voice assistants.\nRecent advancements in this area have been heavily focusing on improving\naccuracy using various techniques. Explainability is undoubtedly an important\naspect for deep learning-based models including joint NLU models. Without\nexplainability, their decisions are opaque to the outside world and hence, have\ntendency to lack user trust. Therefore to bridge this gap, we transform the\nfull joint NLU model to be `inherently' explainable at granular levels without\ncompromising on accuracy. Further, as we enable the full joint NLU model\nexplainable, we show that our extension can be successfully used in other\ngeneral classification tasks. We demonstrate this using sentiment analysis and\nnamed entity recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gunaratna_K/0/1/0/all/0/1\">Kalpa Gunaratna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_V/0/1/0/all/0/1\">Vijay Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hongxia Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Automated Assessment Meets Automated Content Generation: Examining Text Quality in the Era of GPTs. (arXiv:2309.14488v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14488","description":"<p>The use of machine learning (ML) models to assess and score textual data has\nbecome increasingly pervasive in an array of contexts including natural\nlanguage processing, information retrieval, search and recommendation, and\ncredibility assessment of online content. A significant disruption at the\nintersection of ML and text are text-generating large-language models such as\ngenerative pre-trained transformers (GPTs). We empirically assess the\ndifferences in how ML-based scoring models trained on human content assess the\nquality of content generated by humans versus GPTs. To do so, we propose an\nanalysis framework that encompasses essay scoring ML-models, human and\nML-generated essays, and a statistical model that parsimoniously considers the\nimpact of type of respondent, prompt genre, and the ML model used for\nassessment model. A rich testbed is utilized that encompasses 18,460\nhuman-generated and GPT-based essays. Results of our benchmark analysis reveal\nthat transformer pretrained language models (PLMs) more accurately score human\nessay quality as compared to CNN/RNN and feature-based ML methods.\nInterestingly, we find that the transformer PLMs tend to score GPT-generated\ntext 10-15\\% higher on average, relative to human-authored documents.\nConversely, traditional deep learning and feature-based ML models score human\ntext considerably higher. Further analysis reveals that although the\ntransformer PLMs are exclusively fine-tuned on human text, they more\nprominently attend to certain tokens appearing only in GPT-generated text,\npossibly due to familiarity/overlap in pre-training. Our framework and results\nhave implications for text classification settings where automated scoring of\ntext is likely to be disrupted by generative AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bevilacqua_M/0/1/0/all/0/1\">Marialena Bevilacqua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oketch_K/0/1/0/all/0/1\">Kezia Oketch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Ruiyang Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamey_W/0/1/0/all/0/1\">Will Stamey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1\">Yi Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbasi_A/0/1/0/all/0/1\">Ahmed Abbasi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classifying token frequencies using angular Minkowski $p$-distance. (arXiv:2309.14495v1 [cs.LG])","link":"http://arxiv.org/abs/2309.14495","description":"<p>Angular Minkowski $p$-distance is a dissimilarity measure that is obtained by\nreplacing Euclidean distance in the definition of cosine dissimilarity with\nother Minkowski $p$-distances. Cosine dissimilarity is frequently used with\ndatasets containing token frequencies, and angular Minkowski $p$-distance may\npotentially be an even better choice for certain tasks. In a case study based\non the 20-newsgroups dataset, we evaluate clasification performance for\nclassical weighted nearest neighbours, as well as fuzzy rough nearest\nneighbours. In addition, we analyse the relationship between the hyperparameter\n$p$, the dimensionality $m$ of the dataset, the number of neighbours $k$, the\nchoice of weights and the choice of classifier. We conclude that it is possible\nto obtain substantially higher classification performance with angular\nMinkowski $p$-distance with suitable values for $p$ than with classical cosine\ndissimilarity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lenz_O/0/1/0/all/0/1\">Oliver Urs Lenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornelis_C/0/1/0/all/0/1\">Chris Cornelis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models. (arXiv:2309.14509v1 [cs.LG])","link":"http://arxiv.org/abs/2309.14509","description":"<p>Computation in a typical Transformer-based large language model (LLM) can be\ncharacterized by batch size, hidden dimension, number of layers, and sequence\nlength. Until now, system works for accelerating LLM training have focused on\nthe first three dimensions: data parallelism for batch size, tensor parallelism\nfor hidden size and pipeline parallelism for model depth or layers. These\nwidely studied forms of parallelism are not targeted or optimized for long\nsequence Transformer models. Given practical application needs for long\nsequence LLM, renewed attentions are being drawn to sequence parallelism.\nHowever, existing works in sequence parallelism are constrained by\nmemory-communication inefficiency, limiting their scalability to long sequence\nlarge models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable\nand effective methodology for enabling highly efficient and scalable LLM\ntraining with extremely long sequence length. DeepSpeed-Ulysses at its core\npartitions input data along the sequence dimension and employs an efficient\nall-to-all collective communication for attention computation. Theoretical\ncommunication analysis shows that whereas other methods incur communication\noverhead as sequence length increases, DeepSpeed-Ulysses maintains constant\ncommunication volume when sequence length and compute devices are increased\nproportionally. Furthermore, experimental evaluations show that\nDeepSpeed-Ulysses trains 2.5X faster with 4X longer sequence length than the\nexisting method SOTA baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_S/0/1/0/all/0/1\">Sam Ade Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_M/0/1/0/all/0/1\">Masahiro Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minjia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Leon Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajbhandari_S/0/1/0/all/0/1\">Samyam Rajbhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxiong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Watch Your Language: Large Language Models and Content Moderation. (arXiv:2309.14517v1 [cs.HC])","link":"http://arxiv.org/abs/2309.14517","description":"<p>Large language models (LLMs) have exploded in popularity due to their ability\nto perform a wide array of natural language tasks. Text-based content\nmoderation is one LLM use case that has received recent enthusiasm, however,\nthere is little research investigating how LLMs perform in content moderation\nsettings. In this work, we evaluate a suite of modern, commercial LLMs (GPT-3,\nGPT-3.5, GPT-4) on two common content moderation tasks: rule-based community\nmoderation and toxic content detection. For rule-based community moderation, we\nconstruct 95 LLM moderation-engines prompted with rules from 95 Reddit\nsubcommunities and find that LLMs can be effective at rule-based moderation for\nmany communities, achieving a median accuracy of 64% and a median precision of\n83%. For toxicity detection, we find that LLMs significantly outperform\nexisting commercially available toxicity classifiers. However, we also find\nthat recent increases in model size add only marginal benefit to toxicity\ndetection, suggesting a potential performance plateau for LLMs on toxicity\ndetection tasks. We conclude by outlining avenues for future work in studying\nLLMs and content moderation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1\">Deepak Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AbuHashem_Y/0/1/0/all/0/1\">Yousef AbuHashem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durumeric_Z/0/1/0/all/0/1\">Zakir Durumeric</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT Performance on Standardized Testing Exam -- A Proposed Strategy for Learners. (arXiv:2309.14519v1 [cs.CY])","link":"http://arxiv.org/abs/2309.14519","description":"<p>This study explores the problem solving capabilities of ChatGPT and its\nprospective applications in standardized test preparation, focusing on the GRE\nquantitative exam. Prior research has shown great potential for the utilization\nof ChatGPT for academic purposes in revolutionizing the approach to studying\nacross various disciplines. We investigate how ChatGPT performs across various\nquestion types in the GRE quantitative domain, and how modifying question\nprompts impacts its accuracy. More specifically this study addressed two\nresearch questions: 1. How does ChatGPT perform in answering GRE-based\nquantitative questions across various content areas? 2. How does the accuracy\nof ChatGPT vary with modifying the question prompts? The dataset consisting of\n100 randomly selected GRE quantitative questions was collected from the ETS\nofficial guide to GRE test preparation. We used quantitative evaluation to\nanswer our first research question, and t-test to examine the statistical\nassociation between prompt modification and ChatGPT's accuracy. Results show a\nstatistical improvement in the ChatGPT's accuracy after applying instruction\npriming and contextual prompts to the original questions. ChatGPT showed 84%\naccuracy with the modified prompts compared to 69% with the original data. The\nstudy discusses the areas where ChatGPT struggled with certain questions and\nhow modifications can be helpful for preparing for standardized tests like GRE\nand provides future directions for prompt modifications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farooq_U/0/1/0/all/0/1\">Umer Farooq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saira Anwar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aligning Large Multimodal Models with Factually Augmented RLHF. (arXiv:2309.14525v1 [cs.CV])","link":"http://arxiv.org/abs/2309.14525","description":"<p>Large Multimodal Models (LMM) are built across modalities and the\nmisalignment between two modalities can result in \"hallucination\", generating\ntextual outputs that are not grounded by the multimodal information in context.\nTo address the multimodal misalignment issue, we adapt the Reinforcement\nLearning from Human Feedback (RLHF) from the text domain to the task of\nvision-language alignment, where human annotators are asked to compare two\nresponses and pinpoint the more hallucinated one, and the vision-language model\nis trained to maximize the simulated human rewards. We propose a new alignment\nalgorithm called Factually Augmented RLHF that augments the reward model with\nadditional factual information such as image captions and ground-truth\nmulti-choice options, which alleviates the reward hacking phenomenon in RLHF\nand further improves the performance. We also enhance the GPT-4-generated\ntraining data (for vision instruction tuning) with previously available\nhuman-written image-text pairs to improve the general capabilities of our\nmodel. To evaluate the proposed approach in real-world scenarios, we develop a\nnew evaluation benchmark MMHAL-BENCH with a special focus on penalizing\nhallucinations. As the first LMM trained with RLHF, our approach achieves\nremarkable improvement on the LLaVA-Bench dataset with the 94% performance\nlevel of the text-only GPT-4 (while previous best methods can only achieve the\n87% level), and an improvement by 60% on MMHAL-BENCH over other baselines. We\nopensource our code, model, data at https://llava-rlhf.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhiqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shengcao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haotian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yikang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1\">Liang-Yan Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Art or Artifice? Large Language Models and the False Promise of Creativity. (arXiv:2309.14556v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14556","description":"<p>Researchers have argued that large language models (LLMs) exhibit\nhigh-quality writing capabilities from blogs to stories. However, evaluating\nobjectively the creativity of a piece of writing is challenging. Inspired by\nthe Torrance Test of Creative Thinking (TTCT), which measures creativity as a\nprocess, we use the Consensual Assessment Technique [3] and propose the\nTorrance Test of Creative Writing (TTCW) to evaluate creativity as a product.\nTTCW consists of 14 binary tests organized into the original dimensions of\nFluency, Flexibility, Originality, and Elaboration. We recruit 10 creative\nwriters and implement a human assessment of 48 stories written either by\nprofessional authors or LLMs using TTCW. Our analysis shows that LLM-generated\nstories pass 3-10X less TTCW tests than stories written by professionals. In\naddition, we explore the use of LLMs as assessors to automate the TTCW\nevaluation, revealing that none of the LLMs positively correlate with the\nexpert assessments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1\">Philippe Laban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1\">Divyansh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Introducing DictaLM -- A Large Generative Language Model for Modern Hebrew. (arXiv:2309.14568v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14568","description":"<p>We present DictaLM, a large-scale language model tailored for Modern Hebrew.\nBoasting 7B parameters, this model is predominantly trained on Hebrew-centric\ndata. As a commitment to promoting research and development in the Hebrew\nlanguage, we release both the foundation model and the instruct-tuned model\nunder a Creative Commons license. Concurrently, we introduce DictaLM-Rab,\nanother foundation model geared towards Rabbinic/Historical Hebrew. These\nfoundation models serve as ideal starting points for fine-tuning various\nHebrew-specific tasks, such as instruction, Q&amp;A, sentiment analysis, and more.\nThis release represents a preliminary step, offering an initial Hebrew LLM\nmodel for the Hebrew NLP community to experiment with.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shmidman_S/0/1/0/all/0/1\">Shaltiel Shmidman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmidman_A/0/1/0/all/0/1\">Avi Shmidman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1\">Amir David Nissan Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppel_M/0/1/0/all/0/1\">Moshe Koppel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Post-training Quantization with FP8 Formats. (arXiv:2309.14592v1 [cs.LG])","link":"http://arxiv.org/abs/2309.14592","description":"<p>Recent advances in deep learning methods such as LLMs and Diffusion models\nhave created a need for improved quantization methods that can meet the\ncomputational demands of these modern architectures while maintaining accuracy.\nTowards this goal, we study the advantages of FP8 data formats for\npost-training quantization across 75 unique network architectures covering a\nwide range of tasks, including machine translation, language modeling, text\ngeneration, image classification, generation, and segmentation. We examine\nthree different FP8 representations (E5M2, E4M3, and E3M4) to study the effects\nof varying degrees of trade-off between dynamic range and precision on model\naccuracy. Based on our extensive study, we developed a quantization workflow\nthat generalizes across different network architectures. Our empirical results\nshow that FP8 formats outperform INT8 in multiple aspects, including workload\ncoverage (92.64% vs. 65.87%), model accuracy and suitability for a broader\nrange of operations. Furthermore, our findings suggest that E4M3 is better\nsuited for NLP models, whereas E3M4 performs marginally better than E4M3 on\ncomputer vision tasks. The code is publicly available on Intel Neural\nCompressor: https://github.com/intel/neural-compressor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Haihao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mellempudi_N/0/1/0/all/0/1\">Naveen Mellempudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengni Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Text to Video Model via Transformer. (arXiv:2309.14683v1 [cs.CV])","link":"http://arxiv.org/abs/2309.14683","description":"<p>We present a general and simple text to video model based on Transformer.\nSince both text and video are sequential data, we encode both texts and images\ninto the same hidden space, which are further fed into Transformer to capture\nthe temporal consistency and then decoder to generate either text or images.\nConsidering the image signal may become weak in the long sequence, we introduce\nthe U-Net to reconstruct image from its noised version. Specifically, we\nincrease the noise level to the original image in the long sequence, then use\nthe $down$ module from U-Net to encode noised images, which are further input\nto transformer to predict next clear images. We also add a constraint to\npromote motion between any generated image pair in the video. We use GPT2 and\ntest our approach on UCF101 dataset and show it can generate promising videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Gang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PLMM: Personal Large Models on Mobile Devices. (arXiv:2309.14726v1 [cs.CV])","link":"http://arxiv.org/abs/2309.14726","description":"<p>Inspired by Federated Learning, in this paper, we propose personal large\nmodels that are distilled from traditional large language models but more\nadaptive to local users' personal information such as education background and\nhobbies. We classify the large language models into three levels: the personal\nlevel, expert level and traditional level. The personal level models are\nadaptive to users' personal information. They encrypt the users' input and\nprotect their privacy. The expert level models focus on merging specific\nknowledge such as finance, IT and art. The traditional models focus on the\nuniversal knowledge discovery and upgrading the expert models. In such\nclassifications, the personal models directly interact with the user. For the\nwhole system, the personal models have users' (encrypted) personal information.\nMoreover, such models must be small enough to be performed on personal\ncomputers or mobile devices. Finally, they also have to response in real-time\nfor better user experience and produce high quality results. The proposed\npersonal large models can be applied in a wide range of applications such as\nlanguage and vision tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yuanhao Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparative Analysis of Artificial Intelligence for Indian Legal Question Answering (AILQA) Using Different Retrieval and QA Models. (arXiv:2309.14735v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14735","description":"<p>Legal question-answering (QA) systems have the potential to revolutionize the\nway legal professionals interact with case law documents. This paper conducts a\ncomparative analysis of existing artificial intelligence models for their\nutility in answering legal questions within the Indian legal system,\nspecifically focusing on Indian Legal Question Answering (AILQA) and our study\ninvestigates the efficacy of different retrieval and QA algorithms currently\navailable. Utilizing the OpenAI GPT model as a benchmark, along with query\nprompts, our investigation shows that existing AILQA systems can automatically\ninterpret natural language queries from users and generate highly accurate\nresponses. This research is particularly focused on applications within the\nIndian criminal justice domain, which has its own set of challenges due to its\ncomplexity and resource constraints. In order to rigorously assess the\nperformance of these models, empirical evaluations are complemented by feedback\nfrom practicing legal professionals, thereby offering a multifaceted view on\nthe capabilities and limitations of AI in the context of Indian legal\nquestion-answering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nigam_S/0/1/0/all/0/1\">Shubham Kumar Nigam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shubham Kumar Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1\">Ayush Kumar Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shallum_N/0/1/0/all/0/1\">Noel Shallum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Arnab Bhattacharya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Program Repair with Minimal Edits Using CodeT5. (arXiv:2309.14760v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14760","description":"<p>Programmers often struggle to identify and fix bugs in their programs. In\nrecent years, many language models (LMs) have been proposed to fix erroneous\nprograms and support error recovery. However, the LMs tend to generate\nsolutions that differ from the original input programs. This leads to potential\ncomprehension difficulties for users. In this paper, we propose an approach to\nsuggest a correct program with minimal repair edits using CodeT5. We fine-tune\na pre-trained CodeT5 on code pairs of wrong and correct programs and evaluate\nits performance with several baseline models. The experimental results show\nthat the fine-tuned CodeT5 achieves a pass@100 of 91.95% and an average edit\ndistance of the most similar correct program of 6.84, which indicates that at\nleast one correct program can be suggested by generating 100 candidate\nprograms. We demonstrate the effectiveness of LMs in suggesting program repair\nwith minimal edits for solving introductory programming problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shirafuji_A/0/1/0/all/0/1\">Atsushi Shirafuji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md. Mostafizer Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1\">Md Faizul Ibne Amin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanobe_Y/0/1/0/all/0/1\">Yutaka Watanobe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConPET: Continual Parameter-Efficient Tuning for Large Language Models. (arXiv:2309.14763v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14763","description":"<p>Continual learning necessitates the continual adaptation of models to newly\nemerging tasks while minimizing the catastrophic forgetting of old ones. This\nis extremely challenging for large language models (LLMs) with vanilla\nfull-parameter tuning due to high computation costs, memory consumption, and\nforgetting issue. Inspired by the success of parameter-efficient tuning (PET),\nwe propose Continual Parameter-Efficient Tuning (ConPET), a generalizable\nparadigm for continual task adaptation of LLMs with task-number-independent\ntraining complexity. ConPET includes two versions with different application\nscenarios. First, Static ConPET can adapt former continual learning methods\noriginally designed for relatively smaller models to LLMs through PET and a\ndynamic replay strategy, which largely reduces the tuning costs and alleviates\nthe over-fitting and forgetting issue. Furthermore, to maintain scalability,\nDynamic ConPET adopts separate PET modules for different tasks and a PET module\nselector for dynamic optimal selection. In our extensive experiments, the\nadaptation of Static ConPET helps multiple former methods reduce the scale of\ntunable parameters by over 3,000 times and surpass the PET-only baseline by at\nleast 5 points on five smaller benchmarks, while Dynamic ConPET gains its\nadvantage on the largest dataset. The codes and datasets are available at\nhttps://github.com/Raincleared-Song/ConPET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chenyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zheni Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KERMIT: Knowledge Graph Completion of Enhanced Relation Modeling with Inverse Transformation. (arXiv:2309.14770v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14770","description":"<p>Knowledge graph completion is a task that revolves around filling in missing\ntriples based on the information available in a knowledge graph. Among the\ncurrent studies, text-based methods complete the task by utilizing textual\ndescriptions of triples. However, this modeling approach may encounter\nlimitations, particularly when the description fails to accurately and\nadequately express the intended meaning. To overcome these challenges, we\npropose the augmentation of data through two additional mechanisms. Firstly, we\nemploy ChatGPT as an external knowledge base to generate coherent descriptions\nto bridge the semantic gap between the queries and answers. Secondly, we\nleverage inverse relations to create a symmetric graph, thereby creating extra\nlabeling and providing supplementary information for link prediction. This\napproach offers additional insights into the relationships between entities.\nThrough these efforts, we have observed significant improvements in knowledge\ngraph completion, as these mechanisms enhance the richness and diversity of the\navailable data, leading to more accurate results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haotian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yuliang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Richard Yi Da Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bailing Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting In-Context Learning with Factual Knowledge. (arXiv:2309.14771v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14771","description":"<p>In-Context Learning (ICL) over Large language models (LLMs) aims at solving\npreviously unseen tasks by conditioning on a few training examples, eliminating\nthe need for parameter updates and achieving competitive performance. In this\npaper, we demonstrate that factual knowledge is imperative for the performance\nof ICL in three core facets, i.e., the inherent knowledge learned in LLMs, the\nfactual knowledge derived from the selected in-context examples, and the\nknowledge biases in LLMs for output generation. To unleash the power of LLMs in\nfew-shot learning scenarios, we introduce a novel Knowledgeable In-Context\nTuning (KICT) framework to further improve the performance of ICL: 1) injecting\nfactual knowledge to LLMs during continual self-supervised pre-training, 2)\njudiciously selecting the examples with high knowledge relevance, and 3)\ncalibrating the prediction results based on prior knowledge. We evaluate the\nproposed approaches on auto-regressive LLMs (e.g., GPT-style models) over\nmultiple text classification and question answering tasks. Experimental results\ndemonstrate that KICT substantially outperforms strong baselines, and improves\nby more than 13% and 7% of accuracy on text classification and question\nanswering tasks, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Ming Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BLIP-Adapter: Parameter-Efficient Transfer Learning for Mobile Screenshot Captioning. (arXiv:2309.14774v1 [cs.LG])","link":"http://arxiv.org/abs/2309.14774","description":"<p>This study aims to explore efficient tuning methods for the screenshot\ncaptioning task. Recently, image captioning has seen significant advancements,\nbut research in captioning tasks for mobile screens remains relatively scarce.\nCurrent datasets and use cases describing user behaviors within product\nscreenshots are notably limited. Consequently, we sought to fine-tune\npre-existing models for the screenshot captioning task. However, fine-tuning\nlarge pre-trained models can be resource-intensive, requiring considerable\ntime, computational power, and storage due to the vast number of parameters in\nimage captioning models. To tackle this challenge, this study proposes a\ncombination of adapter methods, which necessitates tuning only the additional\nmodules on the model. These methods are originally designed for vision or\nlanguage tasks, and our intention is to apply them to address similar\nchallenges in screenshot captioning. By freezing the parameters of the image\ncaption models and training only the weights associated with the methods,\nperformance comparable to fine-tuning the entire model can be achieved, while\nsignificantly reducing the number of parameters. This study represents the\nfirst comprehensive investigation into the effectiveness of combining adapters\nwithin the context of the screenshot captioning task. Through our experiments\nand analyses, this study aims to provide valuable insights into the application\nof adapters in vision-language models and contribute to the development of\nefficient tuning techniques for the screenshot captioning task. Our study is\navailable at https://github.com/RainYuGG/BLIP-Adapter\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_C/0/1/0/all/0/1\">Ching-Yu Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_I/0/1/0/all/0/1\">I-Hua Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shih-Wei Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Small Language Models with Prompt-Learning Paradigm for Efficient Domain-Specific Text Classification. (arXiv:2309.14779v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14779","description":"<p>Domain-specific text classification faces the challenge of scarce labeled\ndata due to the high cost of manual labeling. Prompt-learning, known for its\nefficiency in few-shot scenarios, is proposed as an alternative to traditional\nfine-tuning methods. And besides, although large language models (LLMs) have\ngained prominence, small language models (SLMs, with under 1B parameters) offer\nsignificant customizability, adaptability, and cost-effectiveness for\ndomain-specific tasks, given industry constraints. In this study, we\ninvestigate the potential of SLMs combined with prompt-learning paradigm for\ndomain-specific text classification, specifically within customer-agent\ninteractions in retail. Our evaluations show that, in few-shot settings when\nprompt-based model fine-tuning is possible, T5-base, a typical SLM with 220M\nparameters, achieve approximately 75% accuracy with limited labeled data (up to\n15% of full data), which shows great potentials of SLMs with prompt-learning.\nBased on this, We further validate the effectiveness of active few-shot\nsampling and the ensemble strategy in the prompt-learning pipeline that\ncontribute to a remarkable performance gain. Besides, in zero-shot settings\nwith a fixed model, we underscore a pivotal observation that, although the\nGPT-3.5-turbo equipped with around 154B parameters garners an accuracy of\n55.16%, the power of well designed prompts becomes evident when the\nFLAN-T5-large, a model with a mere 0.5% of GPT-3.5-turbo's parameters, achieves\nan accuracy exceeding 31% with the optimized prompt, a leap from its sub-18%\nperformance with an unoptimized one. Our findings underscore the promise of\nprompt-learning in classification tasks with SLMs, emphasizing the benefits of\nactive few-shot sampling, and ensemble strategies in few-shot settings, and the\nimportance of prompt engineering in zero-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hengyu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esping_S/0/1/0/all/0/1\">Stefan Esping</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning and aligning question answering models for complex information extraction tasks. (arXiv:2309.14805v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14805","description":"<p>The emergence of Large Language Models (LLMs) has boosted performance and\npossibilities in various NLP tasks. While the usage of generative AI models\nlike ChatGPT opens up new opportunities for several business use cases, their\ncurrent tendency to hallucinate fake content strongly limits their\napplicability to document analysis, such as information retrieval from\ndocuments. In contrast, extractive language models like question answering (QA)\nor passage retrieval models guarantee query results to be found within the\nboundaries of an according context document, which makes them candidates for\nmore reliable information extraction in productive environments of companies.\nIn this work we propose an approach that uses and integrates extractive QA\nmodels for improved feature extraction of German business documents such as\ninsurance reports or medical leaflets into a document analysis solution. We\nfurther show that fine-tuning existing German QA models boosts performance for\ntailored extraction tasks of complex linguistic features like damage cause\nexplanations or descriptions of medication appearance, even with using only a\nsmall set of annotated data. Finally, we discuss the relevance of scoring\nmetrics for evaluating information extraction tasks and deduce a combined\nmetric from Levenshtein distance, F1-Score, Exact Match and ROUGE-L to mimic\nthe assessment criteria from human experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Engelbach_M/0/1/0/all/0/1\">Matthias Engelbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klau_D/0/1/0/all/0/1\">Dennis Klau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheerer_F/0/1/0/all/0/1\">Felix Scheerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drawehn_J/0/1/0/all/0/1\">Jens Drawehn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kintz_M/0/1/0/all/0/1\">Maximilien Kintz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation-Free Streaming Machine Translation. (arXiv:2309.14823v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14823","description":"<p>Streaming Machine Translation (MT) is the task of translating an unbounded\ninput text stream in real-time. The traditional cascade approach, which\ncombines an Automatic Speech Recognition (ASR) and an MT system, relies on an\nintermediate segmentation step which splits the transcription stream into\nsentence-like units. However, the incorporation of a hard segmentation\nconstrains the MT system and is a source of errors. This paper proposes a\nSegmentation-Free framework that enables the model to translate an unsegmented\nsource stream by delaying the segmentation decision until the translation has\nbeen generated. Extensive experiments show how the proposed Segmentation-Free\nframework has better quality-latency trade-off than competing approaches that\nuse an independent segmentation model. Software, data and models will be\nreleased upon paper acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iranzo_Sanchez_J/0/1/0/all/0/1\">Javier Iranzo-S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iranzo_Sanchez_J/0/1/0/all/0/1\">Jorge Iranzo-S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimenez_A/0/1/0/all/0/1\">Adri&#xe0; Gim&#xe9;nez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1\">Jorge Civera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juan_A/0/1/0/all/0/1\">Alfons Juan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness of the Random Language Model. (arXiv:2309.14913v1 [cond-mat.dis-nn])","link":"http://arxiv.org/abs/2309.14913","description":"<p>The Random Language Model (De Giuli 2019) is an ensemble of stochastic\ncontext-free grammars, quantifying the syntax of human and computer languages.\nThe model suggests a simple picture of first language learning as a type of\nannealing in the vast space of potential languages. In its simplest\nformulation, it implies a single continuous transition to grammatical syntax,\nat which the symmetry among potential words and categories is spontaneously\nbroken. Here this picture is scrutinized by considering its robustness against\nexplicit symmetry breaking, an inevitable component of learning in the real\nworld. It is shown that the scenario is robust to such symmetry breaking.\nComparison with human data on the clustering coefficient of syntax networks\nsuggests that the observed transition is equivalent to that normally\nexperienced by children at age 24 months.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Lalegani_F/0/1/0/all/0/1\">Fatemeh Lalegani</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Giuli_E/0/1/0/all/0/1\">Eric De Giuli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactively Learning Social Media Representations Improves News Source Factuality Detection. (arXiv:2309.14966v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14966","description":"<p>The rise of social media has enabled the widespread propagation of fake news,\ntext that is published with an intent to spread misinformation and sway\nbeliefs. Rapidly detecting fake news, especially as new events arise, is\nimportant to prevent misinformation.\n</p>\n<p>While prior works have tackled this problem using supervised learning\nsystems, automatedly modeling the complexities of the social media landscape\nthat enables the spread of fake news is challenging. On the contrary, having\nhumans fact check all news is not scalable. Thus, in this paper, we propose to\napproach this problem interactively, where humans can interact to help an\nautomated system learn a better social media representation quality. On real\nworld events, our experiments show performance improvements in detecting\nfactuality of news sources, even after few human interactions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_N/0/1/0/all/0/1\">Nikhil Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Sexual Content at the Sentence Level in First Millennium Latin Texts. (arXiv:2309.14974v1 [cs.CL])","link":"http://arxiv.org/abs/2309.14974","description":"<p>In this study, we propose to evaluate the use of deep learning methods for\nsemantic classification at the sentence level to accelerate the process of\ncorpus building in the field of humanities and linguistics, a traditional and\ntime-consuming task. We introduce a novel corpus comprising around 2500\nsentences spanning from 300 BCE to 900 CE including sexual semantics (medical,\nerotica, etc.). We evaluate various sentence classification approaches and\ndifferent input embedding layers, and show that all consistently outperform\nsimple token-based searches. We explore the integration of idiolectal and\nsociolectal metadata embeddings (centuries, author, type of writing), but find\nthat it leads to overfitting. Our results demonstrate the effectiveness of this\napproach, achieving high precision and true positive rates (TPR) of\nrespectively 70.60% and 86.33% using HAN. We evaluate the impact of the dataset\nsize on the model performances (420 instead of 2013), and show that, while our\nmodels perform worse, they still offer a high enough precision and TPR, even\nwithout MLM, respectively 69% and 51%. Given the result, we provide an analysis\nof the attention mechanism as a supporting added value for humanists in order\nto produce more data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clerice_T/0/1/0/all/0/1\">Thibault Cl&#xe9;rice</a> (ALMAnaCH, CJM)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automating question generation from educational text. (arXiv:2309.15004v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15004","description":"<p>The use of question-based activities (QBAs) is wide-spread in education,\ntraditionally forming an integral part of the learning and assessment process.\nIn this paper, we design and evaluate an automated question generation tool for\nformative and summative assessment in schools. We present an expert survey of\none hundred and four teachers, demonstrating the need for automated generation\nof QBAs, as a tool that can significantly reduce the workload of teachers and\nfacilitate personalized learning experiences. Leveraging the recent\nadvancements in generative AI, we then present a modular framework employing\ntransformer based language models for automatic generation of multiple-choice\nquestions (MCQs) from textual content. The presented solution, with distinct\nmodules for question generation, correct answer prediction, and distractor\nformulation, enables us to evaluate different language models and generation\ntechniques. Finally, we perform an extensive quantitative and qualitative\nevaluation, demonstrating trade-offs in the use of different techniques and\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhowmick_A/0/1/0/all/0/1\">Ayan Kumar Bhowmick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagmohan_A/0/1/0/all/0/1\">Ashish Jagmohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vempaty_A/0/1/0/all/0/1\">Aditya Vempaty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_P/0/1/0/all/0/1\">Prasenjit Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_L/0/1/0/all/0/1\">Leigh Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartman_J/0/1/0/all/0/1\">Jeremy Hartman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kokku_R/0/1/0/all/0/1\">Ravi Kokku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_H/0/1/0/all/0/1\">Hema Maheshwari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Updated Corpora and Benchmarks for Long-Form Speech Recognition. (arXiv:2309.15013v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15013","description":"<p>The vast majority of ASR research uses corpora in which both the training and\ntest data have been pre-segmented into utterances. In most real-word ASR\nuse-cases, however, test audio is not segmented, leading to a mismatch between\ninference-time conditions and models trained on segmented utterances. In this\npaper, we re-release three standard ASR corpora - TED-LIUM 3, Gigapeech, and\nVoxPopuli-en - with updated transcription and alignments to enable their use\nfor long-form ASR research. We use these reconstituted corpora to study the\ntrain-test mismatch problem for transducers and attention-based\nencoder-decoders (AEDs), confirming that AEDs are more susceptible to this\nissue. Finally, we benchmark a simple long-form training for these models,\nshowing its efficacy for model robustness under this domain shift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fox_J/0/1/0/all/0/1\">Jennifer Drexler Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_D/0/1/0/all/0/1\">Desh Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delworth_N/0/1/0/all/0/1\">Natalie Delworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McNamara_Q/0/1/0/all/0/1\">Quinn McNamara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_C/0/1/0/all/0/1\">Corey Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jette_M/0/1/0/all/0/1\">Mig&#xfc;el Jett&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Question-Answering Approach to Evaluate Legal Summaries. (arXiv:2309.15016v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15016","description":"<p>Traditional evaluation metrics like ROUGE compare lexical overlap between the\nreference and generated summaries without taking argumentative structure into\naccount, which is important for legal summaries. In this paper, we propose a\nnovel legal summarization evaluation framework that utilizes GPT-4 to generate\na set of question-answer pairs that cover main points and information in the\nreference summary. GPT-4 is then used to generate answers based on the\ngenerated summary for the questions from the reference summary. Finally, GPT-4\ngrades the answers from the reference summary and the generated summary. We\nexamined the correlation between GPT-4 grading with human grading. The results\nsuggest that this question-answering approach with GPT-4 can be a useful tool\nfor gauging the quality of the summary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huihui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashley_K/0/1/0/all/0/1\">Kevin Ashley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Model Alignment: A Survey. (arXiv:2309.15025v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15025","description":"<p>Recent years have witnessed remarkable progress made in large language models\n(LLMs). Such advancements, while garnering significant attention, have\nconcurrently elicited various concerns. The potential of these models is\nundeniably vast; however, they may yield texts that are imprecise, misleading,\nor even detrimental. Consequently, it becomes paramount to employ alignment\ntechniques to ensure these models to exhibit behaviors consistent with human\nvalues.\n</p>\n<p>This survey endeavors to furnish an extensive exploration of alignment\nmethodologies designed for LLMs, in conjunction with the extant capability\nresearch in this domain. Adopting the lens of AI alignment, we categorize the\nprevailing methods and emergent proposals for the alignment of LLMs into outer\nand inner alignment. We also probe into salient issues including the models'\ninterpretability, and potential vulnerabilities to adversarial attacks. To\nassess LLM alignment, we present a wide variety of benchmarks and evaluation\nmethodologies. After discussing the state of alignment research for LLMs, we\nfinally cast a vision toward the future, contemplating the promising avenues of\nresearch that lie ahead.\n</p>\n<p>Our aspiration for this survey extends beyond merely spurring research\ninterests in this realm. We also envision bridging the gap between the AI\nalignment research community and the researchers engrossed in the capability\nexploration of LLMs for both capable and safe LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tianhao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Renren Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Weilong Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zishan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making PPO even better: Value-Guided Monte-Carlo Tree Search decoding. (arXiv:2309.15028v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15028","description":"<p>Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may\nseem unnecessary when generating natural language text based on\nstate-of-the-art reinforcement learning such as Proximal Policy Optimization\n(PPO). In this paper, we demonstrate that it is possible to get extra mileage\nout of PPO by integrating MCTS on top. The key idea is not to throw out the\nvalue network, a byproduct of PPO training for evaluating partial output\nsequences, when decoding text out of the policy network. More concretely, we\npresent a novel value-guided decoding algorithm called PPO-MCTS, which can\nintegrate the value network from PPO to work closely with the policy network\nduring inference-time generation. Compared to prior approaches based on MCTS\nfor controlled text generation, the key strength of our approach is to reduce\nthe fundamental mismatch of the scoring mechanisms of the partial outputs\nbetween training and test. Evaluation on four text generation tasks demonstrate\nthat PPO-MCTS greatly improves the preferability of generated text compared to\nthe standard practice of using only the PPO policy. Our results demonstrate the\npromise of search algorithms even on top of the aligned language models from\nPPO, and the under-explored benefit of the value network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiacheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1\">Andrew Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ramakanth Pasunuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language based Context Modeling and Reasoning with LLMs: A Tutorial. (arXiv:2309.15074v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15074","description":"<p>Large language models (LLMs) have become phenomenally surging, since\n2018--two decades after introducing context-awareness into computing systems.\nThrough taking into account the situations of ubiquitous devices, users and the\nsocieties, context-aware computing has enabled a wide spectrum of innovative\napplications, such as assisted living, location-based social network services\nand so on. To recognize contexts and make decisions for actions accordingly,\nvarious artificial intelligence technologies, such as Ontology and OWL, have\nbeen adopted as representations for context modeling and reasoning. Recently,\nwith the rise of LLMs and their improved natural language understanding and\nreasoning capabilities, it has become feasible to model contexts using natural\nlanguage and perform context reasoning by interacting with LLMs such as ChatGPT\nand GPT-4. In this tutorial, we demonstrate the use of texts, prompts, and\nautonomous agents (AutoAgents) that enable LLMs to perform context modeling and\nreasoning without requiring fine-tuning of the model. We organize and introduce\nworks in the related field, and name this computing paradigm as the LLM-driven\nContext-aware Computing (LCaC). In the LCaC paradigm, users' requests, sensors\nreading data, and the command to actuators are supposed to be represented as\ntexts. Given the text of users' request and sensor data, the AutoAgent models\nthe context by prompting and sends to the LLM for context reasoning. LLM\ngenerates a plan of actions and responds to the AutoAgent, which later follows\nthe action plan to foster context-awareness. To prove the concepts, we use two\nshowcases--(1) operating a mobile z-arm in an apartment for assisted living,\nand (2) planning a trip and scheduling the itinerary in a context-aware and\npersonalized manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sijia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaofei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Linghe Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Daqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models. (arXiv:2309.15088v1 [cs.IR])","link":"http://arxiv.org/abs/2309.15088","description":"<p>Researchers have successfully applied large language models (LLMs) such as\nChatGPT to reranking in an information retrieval context, but to date, such\nwork has mostly been built on proprietary models hidden behind opaque API\nendpoints. This approach yields experimental results that are not reproducible\nand non-deterministic, threatening the veracity of outcomes that build on such\nshaky foundations. To address this significant shortcoming, we present\nRankVicuna, the first fully open-source LLM capable of performing high-quality\nlistwise reranking in a zero-shot setting. Experimental results on the TREC\n2019 and 2020 Deep Learning Tracks show that we can achieve effectiveness\ncomparable to zero-shot reranking with GPT-3.5 with a much smaller 7B parameter\nmodel, although our effectiveness remains slightly behind reranking with GPT-4.\nWe hope our work provides the foundation for future research on reranking with\nmodern LLMs. All the code necessary to reproduce our results is available at\nhttps://github.com/castorini/rank_llm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pradeep_R/0/1/0/all/0/1\">Ronak Pradeep</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharifymoghaddam_S/0/1/0/all/0/1\">Sahel Sharifymoghaddam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning. (arXiv:2309.15091v1 [cs.CV])","link":"http://arxiv.org/abs/2309.15091","description":"<p>Although recent text-to-video (T2V) generation methods have seen significant\nadvancements, most of these works focus on producing short video clips of a\nsingle event with a single background (i.e., single-scene videos). Meanwhile,\nrecent large language models (LLMs) have demonstrated their capability in\ngenerating layouts and programs to control downstream visual modules such as\nimage generation models. This raises an important question: can we leverage the\nknowledge embedded in these LLMs for temporally consistent long video\ngeneration? In this paper, we propose VideoDirectorGPT, a novel framework for\nconsistent multi-scene video generation that uses the knowledge of LLMs for\nvideo content planning and grounded video generation. Specifically, given a\nsingle text prompt, we first ask our video planner LLM (GPT-4) to expand it\ninto a 'video plan', which involves generating the scene descriptions, the\nentities with their respective layouts, the background for each scene, and\nconsistency groupings of the entities and backgrounds. Next, guided by this\noutput from the video planner, our video generator, Layout2Vid, has explicit\ncontrol over spatial layouts and can maintain temporal consistency of\nentities/backgrounds across scenes, while only trained with image-level\nannotations. Our experiments demonstrate that VideoDirectorGPT framework\nsubstantially improves layout and movement control in both single- and\nmulti-scene video generation and can generate multi-scene videos with visual\nconsistency across scenes, while achieving competitive performance with SOTAs\nin open-domain single-scene T2V generation. We also demonstrate that our\nframework can dynamically control the strength for layout guidance and can also\ngenerate videos with user-provided images. We hope our framework can inspire\nfuture work on better integrating the planning ability of LLMs into consistent\nlong video generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Han Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zala_A/0/1/0/all/0/1\">Abhay Zala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models. (arXiv:2309.15098v1 [cs.CL])","link":"http://arxiv.org/abs/2309.15098","description":"<p>We investigate the internal behavior of Transformer-based Large Language\nModels (LLMs) when they generate factually incorrect text. We propose modeling\nfactual queries as Constraint Satisfaction Problems and use this framework to\ninvestigate how the model interacts internally with factual constraints.\nSpecifically, we discover a strong positive relation between the model's\nattention to constraint tokens and the factual accuracy of its responses. In\nour curated suite of 11 datasets with over 40,000 prompts, we study the task of\npredicting factual errors with the Llama-2 family across all scales (7B, 13B,\n70B). We propose SAT Probe, a method probing self-attention patterns, that can\npredict constraint satisfaction and factual errors, and allows early error\nidentification. The approach and findings demonstrate how using the mechanistic\nunderstanding of factuality in LLMs can enhance reliability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuksekgonul_M/0/1/0/all/0/1\">Mert Yuksekgonul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_V/0/1/0/all/0/1\">Varun Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_E/0/1/0/all/0/1\">Erik Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunasekar_S/0/1/0/all/0/1\">Suriya Gunasekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_R/0/1/0/all/0/1\">Ranjita Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1\">Hamid Palangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamar_E/0/1/0/all/0/1\">Ece Kamar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nushi_B/0/1/0/all/0/1\">Besmira Nushi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Permutation invariant matrix statistics and computational language tasks. (arXiv:2202.06829v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.06829","description":"<p>The Linguistic Matrix Theory programme introduced by Kartsaklis, Ramgoolam\nand Sadrzadeh is an approach to the statistics of matrices that are generated\nin type-driven distributional semantics, based on permutation invariant\npolynomial functions which are regarded as the key observables encoding the\nsignificant statistics. In this paper we generalize the previous results on the\napproximate Gaussianity of matrix distributions arising from compositional\ndistributional semantics. We also introduce a geometry of observable vectors\nfor words, defined by exploiting the graph-theoretic basis for the permutation\ninvariants and the statistical characteristics of the ensemble of matrices\nassociated with the words. We describe successful applications of this unified\nframework to a number of tasks in computational linguistics, associated with\nthe distinctions between synonyms, antonyms, hypernyms and hyponyms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1\">Manuel Accettulli Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correia_A/0/1/0/all/0/1\">Adriana Correia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramgoolam_S/0/1/0/all/0/1\">Sanjaye Ramgoolam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadrzadeh_M/0/1/0/all/0/1\">Mehrnoosh Sadrzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Prosody Representations with Unsupervised Speech Reconstruction. (arXiv:2212.06972v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2212.06972","description":"<p>Human speech can be characterized by different components, including semantic\ncontent, speaker identity and prosodic information. Significant progress has\nbeen made in disentangling representations for semantic content and speaker\nidentity in Automatic Speech Recognition (ASR) and speaker verification tasks\nrespectively. However, it is still an open challenging research question to\nextract prosodic information because of the intrinsic association of different\nattributes, such as timbre and rhythm, and because of the need for supervised\ntraining schemes to achieve robust large-scale and speaker-independent ASR. The\naim of this paper is to address the disentanglement of emotional prosody from\nspeech based on unsupervised reconstruction. Specifically, we identify, design,\nimplement and integrate three crucial components in our proposed speech\nreconstruction model Prosody2Vec: (1) a unit encoder that transforms speech\nsignals into discrete units for semantic content, (2) a pretrained speaker\nverification model to generate speaker identity embeddings, and (3) a trainable\nprosody encoder to learn prosody representations. We first pretrain the\nProsody2Vec representations on unlabelled emotional speech corpora, then\nfine-tune the model on specific datasets to perform Speech Emotion Recognition\n(SER) and Emotional Voice Conversion (EVC) tasks. Both objective (weighted and\nunweighted accuracies) and subjective (mean opinion score) evaluations on the\nEVC task suggest that Prosody2Vec effectively captures general prosodic\nfeatures that can be smoothly transferred to other emotional speech. In\naddition, our SER experiments on the IEMOCAP dataset reveal that the prosody\nfeatures learned by Prosody2Vec are complementary and beneficial for the\nperformance of widely used speech pretraining models and surpass the\nstate-of-the-art methods when combining Prosody2Vec with HuBERT\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Leyuan Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Taihao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pekarek_Rosin_T/0/1/0/all/0/1\">Theresa Pekarek-Rosin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_F/0/1/0/all/0/1\">Fuji Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v6 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2305.06569","description":"<p>Recommendation foundation model utilizes large language models (LLM) for\nrecommendation by converting recommendation tasks into natural language tasks.\nIt enables generative recommendation which directly generates the item(s) to\nrecommend rather than calculating a ranking score for each and every candidate\nitem as in traditional recommendation models, simplifying the recommendation\npipeline from multi-stage filtering to single-stage filtering. To avoid\ngenerating excessively long text and hallucinated recommendations when deciding\nwhich item(s) to recommend, creating LLM-compatible item IDs to uniquely\nidentify each item is essential for recommendation foundation models. In this\nstudy, we systematically examine the item ID creation and indexing problem for\nrecommendation foundation models, using P5 as an example of the backbone LLM.\nTo emphasize the importance of item indexing, we first discuss the issues of\nseveral trivial item indexing methods, such as random indexing, title indexing,\nand independent indexing. We then propose four simple yet effective solutions,\nincluding sequential indexing, collaborative indexing, semantic (content-based)\nindexing, and hybrid indexing. Our study highlights the significant influence\nof item indexing methods on the performance of LLM-based recommendation, and\nour results on real-world datasets validate the effectiveness of our proposed\nsolutions. The research also demonstrates how recent advances on language\nmodeling and traditional IR principles such as indexing can help each other for\nbetter learning and inference. Source code and data are available at\nhttps://github.com/Wenyueh/LLM-RecSys-ID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wenyue Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-Supervised Visual-Textual Grounding with Semantic Prior Refinement. (arXiv:2305.10913v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.10913","description":"<p>Using only image-sentence pairs, weakly-supervised visual-textual grounding\naims to learn region-phrase correspondences of the respective entity mentions.\nCompared to the supervised approach, learning is more difficult since bounding\nboxes and textual phrases correspondences are unavailable. In light of this, we\npropose the Semantic Prior Refinement Model (SPRM), whose predictions are\nobtained by combining the output of two main modules. The first untrained\nmodule aims to return a rough alignment between textual phrases and bounding\nboxes. The second trained module is composed of two sub-components that refine\nthe rough alignment to improve the accuracy of the final phrase-bounding box\nalignments. The model is trained to maximize the multimodal similarity between\nan image and a sentence, while minimizing the multimodal similarity of the same\nsentence and a new unrelated image, carefully selected to help the most during\ntraining. Our approach shows state-of-the-art results on two popular datasets,\nFlickr30k Entities and ReferIt, shining especially on ReferIt with a 9.6%\nabsolute improvement. Moreover, thanks to the untrained component, it reaches\ncompetitive performances just using a small fraction of training examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rigoni_D/0/1/0/all/0/1\">Davide Rigoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parolari_L/0/1/0/all/0/1\">Luca Parolari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serafini_L/0/1/0/all/0/1\">Luciano Serafini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sperduti_A/0/1/0/all/0/1\">Alessandro Sperduti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballan_L/0/1/0/all/0/1\">Lamberto Ballan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Med-UniC: Unifying Cross-Lingual Medical Vision-Language Pre-Training by Diminishing Bias. (arXiv:2305.19894v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.19894","description":"<p>The scarcity of data presents a critical obstacle to the efficacy of medical\nvisionlanguage pre-training (VLP). A potential solution lies in the combination\nof datasets from various language communities. Nevertheless, the main challenge\nstems from the complexity of integrating diverse syntax and semantics,\nlanguage-specific medical terminology, and culture-specific implicit knowledge.\nTherefore, one crucial aspect to consider is the presence of community bias\ncaused by different languages. This paper presents a novel framework named\nUnifying Cross-Lingual Medical Vision-Language Pre-Training (Med-UniC),\ndesigned to integrate multimodal medical data from the two most prevalent\nlanguages, English and Spanish. Specifically, we propose Cross-lingual Text\nAlignment Regularization (CTR) to explicitly unify cross-lingual semantic\nrepresentations of medical reports originating from diverse language\ncommunities. CTR is optimized through latent language disentanglement,\nrendering our optimization objective to not depend on negative samples, thereby\nsignificantly mitigating the bias from determining positive-negative sample\npairs within analogous medical reports. Furthermore, it ensures that the\ncross-lingual representation is not biased toward any specific language\ncommunity. Med-UniC reaches superior performance across 5 medical image tasks\nand 10 datasets encompassing over 30 diseases, offering a versatile framework\nfor unifying multi-modal medical data within diverse linguistic communities.\nThe experimental outcomes highlight the presence of community bias in\ncross-lingual VLP. Reducing this bias enhances the performance not only in\nvision-language tasks but also in uni-modal visual tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zhongwei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Che Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benyou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Sibo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quilodran_Casas_C/0/1/0/all/0/1\">C&#xe9;sar Quilodr&#xe1;n-Casas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arcucci_R/0/1/0/all/0/1\">Rossella Arcucci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MO-VLN: A Multi-Task Benchmark for Open-set Zero-Shot Vision-and-Language Navigation. (arXiv:2306.10322v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2306.10322","description":"<p>Given a natural language, a general robot has to comprehend the instruction\nand find the target object or location based on visual observations even in\nunexplored environments. Most agents rely on massive diverse training data to\nachieve better generalization, which requires expensive labor. These agents\noften focus on common objects and fewer tasks, thus are not intelligent enough\nto handle different types of instructions. To facilitate research in open-set\nvision-and-language navigation, we propose a benchmark named MO-VLN, aiming at\ntesting the effectiveness and generalization of the agent in the multi-task\nsetting. First, we develop a 3D simulator rendered by realistic scenarios using\nUnreal Engine 5, containing more realistic lights and details. The simulator\ncontains three scenes, i.e., cafe, restaurant, and nursing house, of high value\nin the industry. Besides, our simulator involves multiple uncommon objects,\nsuch as takeaway cup and medical adhesive tape, which are more complicated\ncompared with existing environments. Inspired by the recent success of large\nlanguage models (e.g., ChatGPT, Vicuna), we construct diverse high-quality data\nof instruction type without human annotation. Our benchmark MO-VLN provides\nfour tasks: 1) goal-conditioned navigation given a specific object category\n(e.g., \"fork\"); 2) goal-conditioned navigation given simple instructions (e.g.,\n\"Search for and move towards a tennis ball\"); 3) step-by-step instruction\nfollowing; 4) finding abstract object based on high-level instruction (e.g., \"I\nam thirsty\").\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiwen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Liang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shanshan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jianhua Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shikui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Going Beyond Local: Global Graph-Enhanced Personalized News Recommendations. (arXiv:2307.06576v5 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2307.06576","description":"<p>Precisely recommending candidate news articles to users has always been a\ncore challenge for personalized news recommendation systems. Most recent works\nprimarily focus on using advanced natural language processing techniques to\nextract semantic information from rich textual data, employing content-based\nmethods derived from local historical news. However, this approach lacks a\nglobal perspective, failing to account for users' hidden motivations and\nbehaviors beyond semantic information. To address this challenge, we propose a\nnovel model called GLORY (Global-LOcal news Recommendation sYstem), which\ncombines global representations learned from other users with local\nrepresentations to enhance personalized recommendation systems. We accomplish\nthis by constructing a Global-aware Historical News Encoder, which includes a\nglobal news graph and employs gated graph neural networks to enrich news\nrepresentations, thereby fusing historical news representations by a historical\nnews aggregator. Similarly, we extend this approach to a Global Candidate News\nEncoder, utilizing a global entity graph and a candidate news aggregator to\nenhance candidate news representation. Evaluation results on two public news\ndatasets demonstrate that our method outperforms existing approaches.\nFurthermore, our model offers more diverse recommendations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Boming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dairui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzumura_T/0/1/0/all/0/1\">Toyotaro Suzumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Ruihai Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Catastrophic Forgetting in Multimodal Large Language Models. (arXiv:2309.10313v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.10313","description":"<p>Following the success of GPT4, there has been a surge in interest in\nmultimodal large language model (MLLM) research. This line of research focuses\non developing general-purpose LLMs through fine-tuning pre-trained LLMs and\nvision models. However, catastrophic forgetting, a notorious phenomenon where\nthe fine-tuned model fails to retain similar performance compared to the\npre-trained model, still remains an inherent problem in multimodal LLMs (MLLM).\nIn this paper, we introduce EMT: Evaluating MulTimodality for evaluating the\ncatastrophic forgetting in MLLMs, by treating each MLLM as an image classifier.\nWe first apply EMT to evaluate several open-source fine-tuned MLLMs and we\ndiscover that almost all evaluated MLLMs fail to retain the same performance\nlevels as their vision encoders on standard image classification tasks.\nMoreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess\nperformance throughout the fine-tuning. Interestingly, our results suggest that\nearly-stage fine-tuning on an image dataset improves performance across other\nimage datasets, by enhancing the alignment of text and visual features.\nHowever, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in\na significant loss of generalizability, even when the image encoder remains\nfrozen. Our results suggest that MLLMs have yet to demonstrate performance on\npar with their vision models on standard image classification tasks and the\ncurrent MLLM fine-tuning procedure still has room for improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1\">Yuexiang Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1\">Shengbang Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Mu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Q/0/1/0/all/0/1\">Qing Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong Jae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods. (arXiv:2309.10966v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.10966","description":"<p>Recent research in decoding methods for Natural Language Generation (NLG)\ntasks has shown that MAP decoding is not optimal, because model probabilities\ndo not always align with human preferences. Stronger decoding methods,\nincluding Quality Estimation (QE) reranking and Minimum Bayes' Risk (MBR)\ndecoding, have since been proposed to mitigate the model-perplexity-vs-quality\nmismatch. While these decoding methods achieve state-of-the-art performance,\nthey are prohibitively expensive to compute. In this work, we propose MBR\nfinetuning and QE finetuning which distill the quality gains from these\ndecoding methods at training time, while using an efficient decoding algorithm\nat inference time. Using the canonical NLG task of Neural Machine Translation\n(NMT), we show that even with self-training, these finetuning methods\nsignificantly outperform the base model. Moreover, when using an external LLM\nas a teacher model, these finetuning methods outperform finetuning on\nhuman-generated references. These findings suggest new ways to leverage\nmonolingual data to achieve improvements in model quality that are on par with,\nor even exceed, improvements from human-curated data, while maintaining maximum\nefficiency during decoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Finkelstein_M/0/1/0/all/0/1\">Mara Finkelstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Deep Learning for Scientific Imaging Interpretation. (arXiv:2309.12460v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2309.12460","description":"<p>In the domain of scientific imaging, interpreting visual data often demands\nan intricate combination of human expertise and deep comprehension of the\nsubject materials. This study presents a novel methodology to linguistically\nemulate and subsequently evaluate human-like interactions with Scanning\nElectron Microscopy (SEM) images, specifically of glass materials. Leveraging a\nmultimodal deep learning framework, our approach distills insights from both\ntextual and visual data harvested from peer-reviewed articles, further\naugmented by the capabilities of GPT-4 for refined data synthesis and\nevaluation. Despite inherent challenges--such as nuanced interpretations and\nthe limited availability of specialized datasets--our model (GlassLLaVA) excels\nin crafting accurate interpretations, identifying key features, and detecting\ndefects in previously unseen SEM images. Moreover, we introduce versatile\nevaluation metrics, suitable for an array of scientific imaging applications,\nwhich allows for benchmarking against research-grounded answers. Benefiting\nfrom the robustness of contemporary Large Language Models, our model adeptly\naligns with insights from research papers. This advancement not only\nunderscores considerable progress in bridging the gap between human and machine\ninterpretation in scientific imaging, but also hints at expansive avenues for\nfuture research and broader application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alshehri_A/0/1/0/all/0/1\">Abdulelah S. Alshehri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_F/0/1/0/all/0/1\">Franklin L. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shihu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MiChao-HuaFen 1.0: A Specialized Pre-trained Corpus Dataset for Domain-specific Large Models. (arXiv:2309.13079v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.13079","description":"<p>With the advancement of deep learning technologies, general-purpose large\nmodels such as GPT-4 have demonstrated exceptional capabilities across various\ndomains. Nevertheless, there remains a demand for high-quality, domain-specific\noutputs in areas like healthcare, law, and finance. This paper first evaluates\nthe existing large models for specialized domains and discusses their\nlimitations. To cater to the specific needs of certain domains, we introduce\nthe ``MiChao-HuaFen 1.0'' pre-trained corpus dataset, tailored for the news and\ngovernmental sectors. The dataset, sourced from publicly available internet\ndata from 2022, underwent multiple rounds of cleansing and processing to ensure\nhigh quality and reliable origins, with provisions for consistent and stable\nupdates. This dataset not only supports the pre-training of large models for\nChinese vertical domains but also aids in propelling deep learning research and\napplications in related fields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yidong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1\">FuKai Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Rui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Conghui He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Connecting Speech Encoder and Large Language Model for ASR. (arXiv:2309.13963v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2309.13963","description":"<p>The impressive capability and versatility of large language models (LLMs)\nhave aroused increasing attention in automatic speech recognition (ASR), with\nseveral pioneering studies attempting to build integrated ASR models by\nconnecting a speech encoder with an LLM. This paper presents a comparative\nstudy of three commonly used structures as connectors, including fully\nconnected layers, multi-head cross-attention, and Q-Former. Speech encoders\nfrom the Whisper model series as well as LLMs from the Vicuna model series with\ndifferent model sizes were studied. Experiments were performed on the commonly\nused LibriSpeech, Common Voice, and GigaSpeech datasets, where the LLMs with\nQ-Formers demonstrated consistent and considerable word error rate (WER)\nreductions over LLMs with other connector structures. Q-Former-based LLMs can\ngeneralise well to out-of-domain datasets, where 12% relative WER reductions\nover the Whisper baseline ASR model were achieved on the Eval2000 test set\nwithout using any in-domain training data from Switchboard. Moreover, a novel\nsegment-level Q-Former is proposed to enable LLMs to recognise speech segments\nwith a duration exceeding the limitation of the encoders, which results in 17%\nrelative WER reductions over other connector structures on 90-second-long\nspeech data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yu_W/0/1/0/all/0/1\">Wenyi Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_C/0/1/0/all/0/1\">Changli Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_G/0/1/0/all/0/1\">Guangzhi Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xianzhao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_T/0/1/0/all/0/1\">Tian Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1\">Lu Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_Z/0/1/0/all/0/1\">Zejun Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InvestLM: A Large Language Model for Investment using Financial Domain Instruction Tuning. (arXiv:2309.13064v1 [q-fin.GN] CROSS LISTED)","link":"http://arxiv.org/abs/2309.13064","description":"<p>We present a new financial domain large language model, InvestLM, tuned on\nLLaMA-65B (Touvron et al., 2023), using a carefully curated instruction dataset\nrelated to financial investment. Inspired by less-is-more-for-alignment (Zhou\net al., 2023), we manually curate a small yet diverse instruction dataset,\ncovering a wide range of financial related topics, from Chartered Financial\nAnalyst (CFA) exam questions to SEC filings to Stackexchange quantitative\nfinance discussions. InvestLM shows strong capabilities in understanding\nfinancial text and provides helpful responses to investment related questions.\nFinancial experts, including hedge fund managers and research analysts, rate\nInvestLM's response as comparable to those of state-of-the-art commercial\nmodels (GPT-3.5, GPT-4 and Claude-2). Zero-shot evaluation on a set of\nfinancial NLP benchmarks demonstrates strong generalizability. From a research\nperspective, this work suggests that a high-quality domain specific LLM can be\ntuned using a small set of carefully curated instructions on a well-trained\nfoundation model, which is consistent with the Superficial Alignment Hypothesis\n(Zhou et al., 2023). From a practical perspective, this work develops a\nstate-of-the-art financial domain LLM with superior capability in understanding\nfinancial texts and providing helpful investment advice, potentially enhancing\nthe work efficiency of financial professionals. We release the model parameters\nto the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Tang_Y/0/1/0/all/0/1\">Yixuan Tang</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Tam_K/0/1/0/all/0/1\">Kar Yan Tam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-09-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}