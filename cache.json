{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-06-16T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Utilizing Social Media Attributes for Enhanced Keyword Detection: An IDF-LDA Model Applied to Sina Weibo. (arXiv:2306.07978v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07978","description":"<p>With the rapid development of social media such as Twitter and Weibo,\ndetecting keywords from a huge volume of text data streams in real-time has\nbecome a critical problem. The keyword detection problem aims at searching\nimportant information from massive text data to reflect the most important\nevents or topics. However, social media data usually has unique features: the\ndocuments are usually short, the language is colloquial, and the data is likely\nto have significant temporal patterns. Therefore, it could be challenging to\ndiscover critical information from these text streams. In this paper, we\npropose a novel method to address the keyword detection problem in social\nmedia. Our model combines the Inverse Document Frequency (IDF) and Latent\nDirichlet Allocation (LDA) models to better cope with the distinct attributes\nof social media data, such as the number of likes, comments, and retweets. By\nweighting the importance of each document based on these attributes, our method\ncan effectively detect more representative keywords over time. Comprehensive\nexperiments conducted under various conditions on Weibo data illustrate that\nour approach outperforms the baselines in various evaluation metrics, including\nprecision and recall for multiple problem settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yifei Yue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSSRNet: Manipulating Sequential Style Representation for Unsupervised Text Style Transfer. (arXiv:2306.07994v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07994","description":"<p>Unsupervised text style transfer task aims to rewrite a text into target\nstyle while preserving its main content. Traditional methods rely on the use of\na fixed-sized vector to regulate text style, which is difficult to accurately\nconvey the style strength for each individual token. In fact, each token of a\ntext contains different style intensity and makes different contribution to the\noverall style. Our proposed method addresses this issue by assigning individual\nstyle vector to each token in a text, allowing for fine-grained control and\nmanipulation of the style strength. Additionally, an adversarial training\nframework integrated with teacher-student learning is introduced to enhance\ntraining stability and reduce the complexity of high-dimensional optimization.\nThe results of our experiments demonstrate the efficacy of our method in terms\nof clearly improved style transfer accuracy and content preservation in both\ntwo-style transfer and multi-style transfer settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yazheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Zero-Shot Detection of Low Prevalence Chest Pathologies using Domain Pre-trained Language Models. (arXiv:2306.08000v1 [physics.med-ph])","link":"http://arxiv.org/abs/2306.08000","description":"<p>Recent advances in zero-shot learning have enabled the use of paired\nimage-text data to replace structured labels, replacing the need for expert\nannotated datasets. Models such as CLIP-based CheXzero utilize these\nadvancements in the domain of chest X-ray interpretation. We hypothesize that\ndomain pre-trained models such as CXR-BERT, BlueBERT, and ClinicalBERT offer\nthe potential to improve the performance of CLIP-like models with specific\ndomain knowledge by replacing BERT weights at the cost of breaking the original\nmodel's alignment. We evaluate the performance of zero-shot classification\nmodels with domain-specific pre-training for detecting low-prevalence\npathologies. Even though replacing the weights of the original CLIP-BERT\ndegrades model performance on commonly found pathologies, we show that\npre-trained text towers perform exceptionally better on low-prevalence\ndiseases. This motivates future ensemble models with a combination of\ndifferently trained language models for maximal performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Mishra_A/0/1/0/all/0/1\">Aakash Mishra</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mittal_R/0/1/0/all/0/1\">Rajat Mittal</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jestin_C/0/1/0/all/0/1\">Christy Jestin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tingos_K/0/1/0/all/0/1\">Kostas Tingos</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Scheme to classify Read and Spontaneous Speech. (arXiv:2306.08012v1 [cs.SD])","link":"http://arxiv.org/abs/2306.08012","description":"<p>The COVID-19 pandemic has led to an increased use of remote telephonic\ninterviews, making it important to distinguish between scripted and spontaneous\nspeech in audio recordings. In this paper, we propose a novel scheme for\nidentifying read and spontaneous speech. Our approach uses a pre-trained\nDeepSpeech audio-to-alphabet recognition engine to generate a sequence of\nalphabets from the audio. From these alphabets, we derive features that allow\nus to discriminate between read and spontaneous speech. Our experimental\nresults show that even a small set of self-explanatory features can effectively\nclassify the two types of speech very effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kopparapu_S/0/1/0/all/0/1\">Sunil Kumar Kopparapu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])","link":"http://arxiv.org/abs/2306.08018","description":"<p>Large Language Models (LLMs), with their remarkable task-handling\ncapabilities and innovative outputs, have catalyzed significant advancements\nacross a spectrum of fields. However, their proficiency within specialized\ndomains such as biomolecular studies remains limited. To address this\nchallenge, we introduce Mol-Instructions, a meticulously curated, comprehensive\ninstruction dataset expressly designed for the biomolecular realm.\nMol-Instructions is composed of three pivotal components: molecule-oriented\ninstructions, protein-oriented instructions, and biomolecular text\ninstructions, each curated to enhance the understanding and prediction\ncapabilities of LLMs concerning biomolecular features and behaviors. Through\nextensive instruction tuning experiments on the representative LLM, we\nunderscore the potency of Mol-Instructions to enhance the adaptability and\ncognitive acuity of large models within the complex sphere of biomolecular\nstudies, thereby promoting advancements in the biomolecular research community.\nMol-Instructions is made publicly accessible for future research endeavors and\nwill be subjected to continual updates for enhanced applicability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Fang_Y/0/1/0/all/0/1\">Yin Fang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liu_K/0/1/0/all/0/1\">Kangwei Liu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Huang_R/0/1/0/all/0/1\">Rui Huang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Fan_X/0/1/0/all/0/1\">Xiaohui Fan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curatr: A Platform for Semantic Analysis and Curation of Historical Literary Texts. (arXiv:2306.08020v1 [cs.CL])","link":"http://arxiv.org/abs/2306.08020","description":"<p>The increasing availability of digital collections of historical and\ncontemporary literature presents a wealth of possibilities for new research in\nthe humanities. The scale and diversity of such collections however, presents\nparticular challenges in identifying and extracting relevant content. This\npaper presents Curatr, an online platform for the exploration and curation of\nliterature with machine learning-supported semantic search, designed within the\ncontext of digital humanities scholarship. The platform provides a text mining\nworkflow that combines neural word embeddings with expert domain knowledge to\nenable the generation of thematic lexicons, allowing researches to curate\nrelevant sub-corpora from a large corpus of 18th and 19th century digitised\ntexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leavy_S/0/1/0/all/0/1\">Susan Leavy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meaney_G/0/1/0/all/0/1\">Gerardine Meaney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wade_K/0/1/0/all/0/1\">Karen Wade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greene_D/0/1/0/all/0/1\">Derek Greene</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FLamE: Few-shot Learning from Natural Language Explanations. (arXiv:2306.08042v1 [cs.CL])","link":"http://arxiv.org/abs/2306.08042","description":"<p>Natural language explanations have the potential to provide rich information\nthat in principle guides model reasoning. Yet, recent work by Lampinen et al.\n(2022) has shown limited utility of natural language explanations in improving\nclassification. To effectively learn from explanations, we present FLamE, a\ntwo-stage few-shot learning framework that first generates explanations using\nGPT-3, and then finetunes a smaller model (e.g., RoBERTa) with generated\nexplanations. Our experiments on natural language inference demonstrate\neffectiveness over strong baselines, increasing accuracy by 17.6% over GPT-3\nBabbage and 5.7% over GPT-3 Davinci in e-SNLI. Despite improving classification\nperformance, human evaluation surprisingly reveals that the majority of\ngenerated explanations does not adequately justify classification decisions.\nAdditional analyses point to the important role of label-specific cues (e.g.,\n\"not know\" for the neutral label) in generated explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yangqiaoyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks. (arXiv:2306.08107v1 [cs.LG])","link":"http://arxiv.org/abs/2306.08107","description":"<p>The fields of both Natural Language Processing (NLP) and Automated Machine\nLearning (AutoML) have achieved remarkable results over the past years. In NLP,\nespecially Large Language Models (LLMs) have experienced a rapid series of\nbreakthroughs very recently. We envision that the two fields can radically push\nthe boundaries of each other through tight integration. To showcase this\nvision, we explore the potential of a symbiotic relationship between AutoML and\nLLMs, shedding light on how they can benefit each other. In particular, we\ninvestigate both the opportunities to enhance AutoML approaches with LLMs from\ndifferent perspectives and the challenges of leveraging AutoML to further\nimprove LLMs. To this end, we survey existing work, and we critically assess\nrisks. We strongly believe that the integration of the two fields has the\npotential to disrupt both fields, NLP and AutoML. By highlighting conceivable\nsynergies, but also risks, we aim to foster further exploration at the\nintersection of AutoML and LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tornede_A/0/1/0/all/0/1\">Alexander Tornede</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1\">Difan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eimer_T/0/1/0/all/0/1\">Theresa Eimer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giovanelli_J/0/1/0/all/0/1\">Joseph Giovanelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_A/0/1/0/all/0/1\">Aditya Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruhkopf_T/0/1/0/all/0/1\">Tim Ruhkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segel_S/0/1/0/all/0/1\">Sarah Segel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theodorakopoulos_D/0/1/0/all/0/1\">Daphne Theodorakopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tornede_T/0/1/0/all/0/1\">Tanja Tornede</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachsmuth_H/0/1/0/all/0/1\">Henning Wachsmuth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1\">Marius Lindauer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CipherSniffer: Classifying Cipher Types. (arXiv:2306.08116v1 [cs.CL])","link":"http://arxiv.org/abs/2306.08116","description":"<p>Ciphers are a powerful tool for encrypting communication. There are many\ndifferent cipher types, which makes it computationally expensive to solve a\ncipher using brute force. In this paper, we frame the decryption task as a\nclassification problem. We first create a dataset of transpositions,\nsubstitutions, text reversals, word reversals, sentence shifts, and unencrypted\ntext. Then, we evaluate the performance of various tokenizer-model combinations\non this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Artley_B/0/1/0/all/0/1\">Brendan Artley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdiyev_G/0/1/0/all/0/1\">Greg Mehdiyev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Black Box AI-Generated Plagiarism Detection: From Sentence to Document Level. (arXiv:2306.08122v1 [cs.CL])","link":"http://arxiv.org/abs/2306.08122","description":"<p>The increasing reliance on large language models (LLMs) in academic writing\nhas led to a rise in plagiarism. Existing AI-generated text classifiers have\nlimited accuracy and often produce false positives. We propose a novel approach\nusing natural language processing (NLP) techniques, offering quantifiable\nmetrics at both sentence and document levels for easier interpretation by human\nevaluators. Our method employs a multi-faceted approach, generating multiple\nparaphrased versions of a given question and inputting them into the LLM to\ngenerate answers. By using a contrastive loss function based on cosine\nsimilarity, we match generated sentences with those from the student's\nresponse. Our approach achieves up to 94% accuracy in classifying human and AI\ntext, providing a robust and adaptable solution for plagiarism detection in\nacademic settings. This method improves with LLM advancements, reducing the\nneed for new model training or reconfiguration, and offers a more transparent\nway of evaluating and detecting AI-generated text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quidwai_M/0/1/0/all/0/1\">Mujahid Ali Quidwai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunhui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dube_P/0/1/0/all/0/1\">Parijat Dube</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PersonaPKT: Building Personalized Dialogue Agents via Parameter-efficient Knowledge Transfer. (arXiv:2306.08126v1 [cs.CL])","link":"http://arxiv.org/abs/2306.08126","description":"<p>Personalized dialogue agents (DAs) powered by large pre-trained language\nmodels (PLMs) often rely on explicit persona descriptions to maintain\npersonality consistency. However, such descriptions may not always be available\nor may pose privacy concerns. To tackle this bottleneck, we introduce\nPersonaPKT, a lightweight transfer learning approach that can build\npersona-consistent dialogue models without explicit persona descriptions. By\nrepresenting each persona as a continuous vector, PersonaPKT learns implicit\npersona-specific features directly from a small number of dialogue samples\nproduced by the same persona, adding less than 0.1% trainable parameters for\neach persona on top of the PLM backbone. Empirical results demonstrate that\nPersonaPKT effectively builds personalized DAs with high storage efficiency,\noutperforming various baselines in terms of persona consistency while\nmaintaining good response generation quality. In addition, it enhances privacy\nprotection by avoiding explicit persona descriptions. Overall, PersonaPKT is an\neffective solution for creating personalized DAs that respect user privacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Bin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1\">Yoon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Benjamin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaohu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chenlei Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AVIS: Autonomous Visual Information Seeking with Large Language Models. (arXiv:2306.08129v1 [cs.CV])","link":"http://arxiv.org/abs/2306.08129","description":"<p>In this paper, we propose an autonomous information seeking visual question\nanswering framework, AVIS. Our method leverages a Large Language Model (LLM) to\ndynamically strategize the utilization of external tools and to investigate\ntheir outputs, thereby acquiring the indispensable knowledge needed to provide\nanswers to the posed questions. Responding to visual questions that necessitate\nexternal knowledge, such as \"What event is commemorated by the building\ndepicted in this image?\", is a complex task. This task presents a combinatorial\nsearch space that demands a sequence of actions, including invoking APIs,\nanalyzing their responses, and making informed decisions. We conduct a user\nstudy to collect a variety of instances of human decision-making when faced\nwith this task. This data is then used to design a system comprised of three\ncomponents: an LLM-powered planner that dynamically determines which tool to\nuse next, an LLM-powered reasoner that analyzes and extracts key information\nfrom the tool outputs, and a working memory component that retains the acquired\ninformation throughout the process. The collected user behavior serves as a\nguide for our system in two key ways. First, we create a transition graph by\nanalyzing the sequence of decisions made by users. This graph delineates\ndistinct states and confines the set of actions available at each state.\nSecond, we use examples of user decision-making to provide our LLM-powered\nplanner and reasoner with relevant contextual instances, enhancing their\ncapacity to make informed decisions. We show that AVIS achieves\nstate-of-the-art results on knowledge-intensive visual question answering\nbenchmarks such as Infoseek and OK-VQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Ziniu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iscen_A/0/1/0/all/0/1\">Ahmet Iscen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1\">David A Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathi_A/0/1/0/all/0/1\">Alireza Fathi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-scale Language Model Rescoring on Long-form Data. (arXiv:2306.08133v1 [eess.AS])","link":"http://arxiv.org/abs/2306.08133","description":"<p>In this work, we study the impact of Large-scale Language Models (LLM) on\nAutomated Speech Recognition (ASR) of YouTube videos, which we use as a source\nfor long-form ASR. We demonstrate up to 8\\% relative reduction in Word Error\nEate (WER) on US English (en-us) and code-switched Indian English (en-in)\nlong-form ASR test sets and a reduction of up to 30\\% relative on Salient Term\nError Rate (STER) over a strong first-pass baseline that uses a maximum-entropy\nbased language model. Improved lattice processing that results in a lattice\nwith a proper (non-tree) digraph topology and carrying context from the 1-best\nhypothesis of the previous segment(s) results in significant wins in rescoring\nwith LLMs. We also find that the gains in performance from the combination of\nLLMs trained on vast quantities of available data (such as C4) and conventional\nneural LMs is additive and significantly outperforms a strong first-pass\nbaseline with a maximum entropy LM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_T/0/1/0/all/0/1\">Tongzhou Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Allauzen_C/0/1/0/all/0/1\">Cyril Allauzen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yinghui Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_D/0/1/0/all/0/1\">Daniel Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rybach_D/0/1/0/all/0/1\">David Rybach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1\">W. Ronny Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cabrera_R/0/1/0/all/0/1\">Rodrigo Cabrera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Audhkhasi_K/0/1/0/all/0/1\">Kartik Audhkhasi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramabhadran_B/0/1/0/all/0/1\">Bhuvana Ramabhadran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moreno_P/0/1/0/all/0/1\">Pedro J. Moreno</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riley_M/0/1/0/all/0/1\">Michael Riley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])","link":"http://arxiv.org/abs/2306.08158","description":"<p>Deep neural networks often learn unintended biases during training, which\nmight have harmful effects when deployed in real-world settings. This paper\nsurveys 209 papers on bias in NLP models, most of which address\nsociodemographic bias. To better understand the distinction between bias and\nreal-world harm, we turn to ideas from psychology and behavioral economics to\npropose a definition for sociodemographic bias. We identify three main\ncategories of NLP bias research: types of bias, quantifying bias, and\ndebiasing. We conclude that current approaches on quantifying bias face\nreliability issues, that many of the bias metrics do not relate to real-world\nbiases, and that current debiasing techniques are superficial and hide bias\nrather than removing it. Finally, we provide recommendations for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vipul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkit_P/0/1/0/all/0/1\">Pranav Narayanan Venkit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1\">Shomir Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passonneau_R/0/1/0/all/0/1\">Rebecca J. Passonneau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"h2oGPT: Democratizing Large Language Models. (arXiv:2306.08161v1 [cs.CL])","link":"http://arxiv.org/abs/2306.08161","description":"<p>Foundation Large Language Models (LLMs) such as GPT-4 represent a revolution\nin AI due to their real-world applications though natural language processing.\nHowever, they also pose many significant risks such as the presence of biased,\nprivate, or harmful text, and the unauthorized inclusion of copyrighted\nmaterial.\n</p>\n<p>We introduce h2oGPT, a suite of open-source code repositories for the\ncreation and use of Large Language Models (LLMs) based on Generative Pretrained\nTransformers (GPTs). The goal of this project is to create the world's best\ntruly open-source alternative to closed-source GPTs. In collaboration with and\nas part of the incredible and unstoppable open-source community, we open-source\nseveral fine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for\ncommercial use under fully permissive Apache 2.0 licenses. Included in our\nrelease is 100% private document search using natural language.\n</p>\n<p>Open-source language models help boost AI development and make it more\naccessible and trustworthy. They lower entry hurdles, allowing people and\ngroups to tailor these models to their needs. This openness increases\ninnovation, transparency, and fairness. An open-source strategy is needed to\nshare AI benefits fairly, and H2O.ai will continue to democratize AI and LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Candel_A/0/1/0/all/0/1\">Arno Candel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKinney_J/0/1/0/all/0/1\">Jon McKinney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singer_P/0/1/0/all/0/1\">Philipp Singer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_P/0/1/0/all/0/1\">Pascal Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeblick_M/0/1/0/all/0/1\">Maximilian Jeblick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhu_P/0/1/0/all/0/1\">Prithvi Prabhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gambera_J/0/1/0/all/0/1\">Jeff Gambera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landry_M/0/1/0/all/0/1\">Mark Landry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1\">Shivam Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chesler_R/0/1/0/all/0/1\">Ryan Chesler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chun Ming Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1\">Marcos V. Conde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stetsenko_P/0/1/0/all/0/1\">Pasha Stetsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grellier_O/0/1/0/all/0/1\">Olivier Grellier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambati_S/0/1/0/all/0/1\">SriSatish Ambati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error Correction through Low-Rank Adaptation. (arXiv:2306.08162v1 [cs.CL])","link":"http://arxiv.org/abs/2306.08162","description":"<p>We introduce a method that dramatically reduces fine-tuning VRAM requirements\nand rectifies quantization errors in quantized Large Language Models. First, we\ndevelop an extremely memory-efficient fine-tuning (EMEF) method for quantized\nmodels using Low-Rank Adaptation (LoRA), and drawing upon it, we construct an\nerror-correcting algorithm designed to minimize errors induced by the\nquantization process. Our method reduces the memory requirements by up to 5.6\ntimes, which enables fine-tuning a 7 billion parameter Large Language Model\n(LLM) on consumer laptops. At the same time, we propose a Low-Rank Error\nCorrection (LREC) method that exploits the added LoRA layers to ameliorate the\ngap between the quantized model and its float point counterpart. Our error\ncorrection framework leads to a fully functional INT2 quantized LLM with the\ncapacity to generate coherent English text. To the best of our knowledge, this\nis the first INT2 Large Language Model that has been able to reach such a\nperformance. The overhead of our method is merely a 1.05 times increase in\nmodel size, which translates to an effective precision of INT2.1. Also, our\nmethod readily generalizes to other quantization standards, such as INT3, INT4,\nand INT8, restoring their lost performance, which marks a significant milestone\nin the field of model quantization. The strategies delineated in this paper\nhold promising implications for the future development and optimization of\nquantized models, marking a pivotal shift in the landscape of low-resource\nmachine learning computations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1\">Yuji Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkountouras_J/0/1/0/all/0/1\">John Gkountouras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_G/0/1/0/all/0/1\">Glenn G. Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brooks_D/0/1/0/all/0/1\">David Brooks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">Gu-Yeon Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language models are not naysayers: An analysis of language models on negation benchmarks. (arXiv:2306.08189v1 [cs.CL])","link":"http://arxiv.org/abs/2306.08189","description":"<p>Negation has been shown to be a major bottleneck for masked language models,\nsuch as BERT. However, whether this finding still holds for larger-sized\nauto-regressive language models (``LLMs'') has not been studied\ncomprehensively. With the ever-increasing volume of research and applications\nof LLMs, we take a step back to evaluate the ability of current-generation LLMs\nto handle negation, a fundamental linguistic phenomenon that is central to\nlanguage understanding. We evaluate different LLMs -- including the open-source\nGPT-neo, GPT-3, and InstructGPT -- against a wide range of negation benchmarks.\nThrough systematic experimentation with varying model sizes and prompts, we\nshow that LLMs have several limitations including insensitivity to the presence\nof negation, an inability to capture the lexical semantics of negation, and a\nfailure to reason under negation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Thinh Hung Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verspoor_K/0/1/0/all/0/1\">Karin Verspoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing the Effectiveness of GPT-3 in Detecting False Political Statements: A Case Study on the LIAR Dataset. (arXiv:2306.08190v1 [cs.CL])","link":"http://arxiv.org/abs/2306.08190","description":"<p>The detection of political fake statements is crucial for maintaining\ninformation integrity and preventing the spread of misinformation in society.\nHistorically, state-of-the-art machine learning models employed various methods\nfor detecting deceptive statements. These methods include the use of metadata\n(W. Wang et al., 2018), n-grams analysis (Singh et al., 2021), and linguistic\n(Wu et al., 2022) and stylometric (Islam et al., 2020) features. Recent\nadvancements in large language models, such as GPT-3 (Brown et al., 2020) have\nachieved state-of-the-art performance on a wide range of tasks. In this study,\nwe conducted experiments with GPT-3 on the LIAR dataset (W. Wang et al., 2018)\nand achieved higher accuracy than state-of-the-art models without using any\nadditional meta or linguistic features. Additionally, we experimented with\nzero-shot learning using a carefully designed prompt and achieved near\nstate-of-the-art performance. An advantage of this approach is that the model\nprovided evidence for its decision, which adds transparency to the model's\ndecision-making and offers a chance for users to verify the validity of the\nevidence provided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buchholz_M/0/1/0/all/0/1\">Mars Gokturk Buchholz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Operationalising Representation in Natural Language Processing. (arXiv:2306.08193v1 [cs.CL])","link":"http://arxiv.org/abs/2306.08193","description":"<p>Despite its centrality in the philosophy of cognitive science, there has been\nlittle prior philosophical work engaging with the notion of representation in\ncontemporary NLP practice. This paper attempts to fill that lacuna: drawing on\nideas from cognitive science, I introduce a framework for evaluating the\nrepresentational claims made about components of neural NLP models, proposing\nthree criteria with which to evaluate whether a component of a model represents\na property and operationalising these criteria using probing classifiers, a\npopular analysis technique in NLP (and deep learning more broadly).\n</p>\n<p>The project of operationalising a philosophically-informed notion of\nrepresentation should be of interest to both philosophers of science and NLP\npractitioners. It affords philosophers a novel testing-ground for claims about\nthe nature of representation, and helps NLPers organise the large literature on\nprobing experiments, suggesting novel avenues for empirical research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harding_J/0/1/0/all/0/1\">Jacqueline Harding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Loss is All You Need to Recover Analogies as Parallel Lines. (arXiv:2306.08221v1 [cs.CL])","link":"http://arxiv.org/abs/2306.08221","description":"<p>While static word embedding models are known to represent linguistic\nanalogies as parallel lines in high-dimensional space, the underlying mechanism\nas to why they result in such geometric structures remains obscure. We find\nthat an elementary contrastive-style method employed over distributional\ninformation performs competitively with popular word embedding models on\nanalogy recovery tasks, while achieving dramatic speedups in training time.\nFurther, we demonstrate that a contrastive loss is sufficient to create these\nparallel structures in word embeddings, and establish a precise relationship\nbetween the co-occurrence statistics and the geometric structure of the\nresulting word embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ri_N/0/1/0/all/0/1\">Narutatsu Ri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_F/0/1/0/all/0/1\">Fei-Tzin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_N/0/1/0/all/0/1\">Nakul Verma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WARM: A Weakly (+Semi) Supervised Model for Solving Math word Problems. (arXiv:2104.06722v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06722","description":"<p>Solving math word problems (MWPs) is an important and challenging problem in\nnatural language processing. Existing approaches to solve MWPs require full\nsupervision in the form of intermediate equations. However, labeling every MWP\nwith its corresponding equations is a time-consuming and expensive task. In\norder to address this challenge of equation annotation, we propose a weakly\nsupervised model for solving MWPs by requiring only the final answer as\nsupervision. We approach this problem by first learning to generate the\nequation using the problem description and the final answer, which we\nsubsequently use to train a supervised MWP solver. We propose and compare\nvarious weakly supervised techniques to learn to generate equations directly\nfrom the problem description and answer. Through extensive experiments, we\ndemonstrate that without using equations for supervision, our approach achieves\naccuracy gains of 4.5% and 32% over the state-of-the-art weakly supervised\napproach, on the standard Math23K and AllArith datasets respectively.\nAdditionally, we curate and release new datasets of roughly 10k MWPs each in\nEnglish and in Hindi (a low resource language).These datasets are suitable for\ntraining weakly supervised models. We also present an extension of WARMM to\nsemi-supervised learning and present further improvements on results, along\nwith insights.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_O/0/1/0/all/0/1\">Oishik Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_I/0/1/0/all/0/1\">Isha Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waikar_A/0/1/0/all/0/1\">Aashish Waikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vishwajeet Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-conditioning pre-trained language models. (arXiv:2110.02802v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02802","description":"<p>In this paper we aim to investigate the mechanisms that guide text generation\nwith pre-trained Transformer-based Language Models (TLMs). Grounded on the\nProduct of Experts formulation by Hinton (1999), we describe a generative\nmechanism that exploits expert units which naturally exist in TLMs. Such units\nare responsible for detecting concepts in the input and conditioning text\ngeneration on such concepts. We describe how to identify expert units and how\nto activate them during inference in order to induce any desired concept in the\ngenerated output. We find that the activation of a surprisingly small amount of\nunits is sufficient to steer text generation (as little as 3 units in a model\nwith 345M parameters). While the objective of this work is to learn more about\nhow TLMs work, we show that our method is effective for conditioning without\nfine-tuning or using extra parameters, even on fine-grained homograph concepts.\nAdditionally, we show that our method can be used to correct gender bias\npresent in the output of TLMs and achieves gender parity for all evaluated\ncontexts. We compare our method with FUDGE and PPLM-BoW, and show that our\napproach is able to achieve gender parity at a lower perplexity. The proposed\nmethod is accessible to a wide audience thanks to its simplicity and minimal\ncompute needs. The findings in this paper are a step forward in understanding\nthe generative mechanisms of TLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suau_X/0/1/0/all/0/1\">Xavier Suau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zappella_L/0/1/0/all/0/1\">Luca Zappella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apostoloff_N/0/1/0/all/0/1\">Nicholas Apostoloff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focusing on Potential Named Entities During Active Label Acquisition. (arXiv:2111.03837v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.03837","description":"<p>Named entity recognition (NER) aims to identify mentions of named entities in\nan unstructured text and classify them into predefined named entity classes.\nWhile deep learning-based pre-trained language models help to achieve good\npredictive performances in NER, many domain-specific NER applications still\ncall for a substantial amount of labeled data. Active learning (AL), a general\nframework for the label acquisition problem, has been used for NER tasks to\nminimize the annotation cost without sacrificing model performance. However,\nthe heavily imbalanced class distribution of tokens introduces challenges in\ndesigning effective AL querying methods for NER. We propose several AL sentence\nquery evaluation functions that pay more attention to potential positive\ntokens, and evaluate these proposed functions with both sentence-based and\ntoken-based cost evaluation strategies. We also propose a better data-driven\nnormalization approach to penalize sentences that are too long or too short.\nOur experiments on three datasets from different domains reveal that the\nproposed approach reduces the number of annotated tokens while achieving better\nor comparable prediction performance with conventional methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sapci_A/0/1/0/all/0/1\">Ali Osman Berk Sapci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tastan_O/0/1/0/all/0/1\">Oznur Tastan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeniterzi_R/0/1/0/all/0/1\">Reyyan Yeniterzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of sepsis during emergency department triage using machine learning. (arXiv:2204.07657v6 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.07657","description":"<p>Sepsis is a life-threatening condition with organ dysfunction and is a\nleading cause of death and critical illness worldwide. Even a few hours of\ndelay in the treatment of sepsis results in increased mortality. Early\ndetection of sepsis during emergency department triage would allow early\ninitiation of lab analysis, antibiotic administration, and other sepsis\ntreatment protocols. The purpose of this study was to compare sepsis detection\nperformance at ED triage (prior to the use of laboratory diagnostics) of the\nstandard sepsis screening algorithm (SIRS with source of infection) and a\nmachine learning algorithm trained on EHR triage data. A machine learning model\n(KATE Sepsis) was developed using patient encounters with triage data from\n16participating hospitals. KATE Sepsis and standard screening were\nretrospectively evaluated on the adult population of 512,949 medical records.\nKATE Sepsis demonstrates an AUC of 0.9423 (0.9401 - 0.9441) with sensitivity of\n71.09% (70.12% - 71.98%) and specificity of 94.81% (94.75% - 94.87%). Standard\nscreening demonstrates an AUC of 0.6826 (0.6774 - 0.6878) with sensitivity of\n40.8% (39.71% - 41.86%) and specificity of 95.72% (95.68% - 95.78%). The KATE\nSepsis model trained to detect sepsis demonstrates 77.67% (75.78% -79.42%)\nsensitivity in detecting severe sepsis and 86.95% (84.2% - 88.81%) sensitivity\nin detecting septic shock. The standard screening protocol demonstrates 43.06%\n(41% - 45.87%) sensitivity in detecting severe sepsis and40% (36.55% - 43.26%)\nsensitivity in detecting septic shock. Future research should focus on the\nprospective impact of KATE Sepsis on administration of antibiotics, readmission\nrate, morbidity and mortality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivanov_O/0/1/0/all/0/1\">Oleksandr Ivanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molander_K/0/1/0/all/0/1\">Karin Molander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunne_R/0/1/0/all/0/1\">Robert Dunne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Stephen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brecher_D/0/1/0/all/0/1\">Deena Brecher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masek_K/0/1/0/all/0/1\">Kevin Masek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_E/0/1/0/all/0/1\">Erica Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lisa Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Travers_D/0/1/0/all/0/1\">Debbie Travers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delaney_D/0/1/0/all/0/1\">Deb Delaney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montgomery_K/0/1/0/all/0/1\">Kyla Montgomery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reilly_C/0/1/0/all/0/1\">Christian Reilly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Taxonomy of Prompt Modifiers for Text-To-Image Generation. (arXiv:2204.13988v3 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2204.13988","description":"<p>Text-to-image generation has seen an explosion of interest since 2021. Today,\nbeautiful and intriguing digital images and artworks can be synthesized from\ntextual inputs (\"prompts\") with deep generative models. Online communities\naround text-to-image generation and AI generated art have quickly emerged. This\npaper identifies six types of prompt modifiers used by practitioners in the\nonline community based on a 3-month ethnographic study. The novel taxonomy of\nprompt modifiers provides researchers a conceptual starting point for\ninvestigating the practice of text-to-image generation, but may also help\npractitioners of AI generated art improve their images. We further outline how\nprompt modifiers are applied in the practice of \"prompt engineering.\" We\ndiscuss research opportunities of this novel creative practice in the field of\nHuman-Computer Interaction (HCI). The paper concludes with a discussion of\nbroader implications of prompt engineering from the perspective of Human-AI\nInteraction (HAI) in future applications beyond the use case of text-to-image\ngeneration and AI generated art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oppenlaender_J/0/1/0/all/0/1\">Jonas Oppenlaender</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PLAtE: A Large-scale Dataset for List Page Web Extraction. (arXiv:2205.12386v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12386","description":"<p>Recently, neural models have been leveraged to significantly improve the\nperformance of information extraction from semi-structured websites. However, a\nbarrier for continued progress is the small number of datasets large enough to\ntrain these models. In this work, we introduce the PLAtE (Pages of Lists\nAttribute Extraction) benchmark dataset as a challenging new web extraction\ntask. PLAtE focuses on shopping data, specifically extractions from product\nreview pages with multiple items encompassing the tasks of: (1) finding\nproduct-list segmentation boundaries and (2) extracting attributes for each\nproduct. PLAtE is composed of 52, 898 items collected from 6, 694 pages and\n156, 014 attributes, making it the first largescale list page web extraction\ndataset. We use a multi-stage approach to collect and annotate the dataset and\nadapt three state-of-the-art web extraction models to the two tasks comparing\ntheir strengths and weaknesses both quantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+San_A/0/1/0/all/0/1\">Aidan San</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yuan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakus_J/0/1/0/all/0/1\">Jan Bakus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lockard_C/0/1/0/all/0/1\">Colin Lockard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciemiewicz_D/0/1/0/all/0/1\">David Ciemiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atluri_S/0/1/0/all/0/1\">Sandeep Atluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Small_K/0/1/0/all/0/1\">Kevin Small</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elfardy_H/0/1/0/all/0/1\">Heba Elfardy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Dynamic Contextualised Word Embeddings via Template-based Temporal Adaptation. (arXiv:2208.10734v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.10734","description":"<p>Dynamic contextualised word embeddings (DCWEs) represent the temporal\nsemantic variations of words. We propose a method for learning DCWEs by\ntime-adapting a pretrained Masked Language Model (MLM) using time-sensitive\ntemplates. Given two snapshots $C_1$ and $C_2$ of a corpus taken respectively\nat two distinct timestamps $T_1$ and $T_2$, we first propose an unsupervised\nmethod to select (a) \\emph{pivot} terms related to both $C_1$ and $C_2$, and\n(b) \\emph{anchor} terms that are associated with a specific pivot term in each\nindividual snapshot. We then generate prompts by filling manually compiled\ntemplates using the extracted pivot and anchor terms. Moreover, we propose an\nautomatic method to learn time-sensitive templates from $C_1$ and $C_2$,\nwithout requiring any human supervision. Next, we use the generated prompts to\nadapt a pretrained MLM to $T_2$ by fine-tuning using those prompts. Multiple\nexperiments show that our proposed method reduces the perplexity of test\nsentences in $C_2$, outperforming the current state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaohang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data. (arXiv:2209.15329v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.15329","description":"<p>How to boost speech pre-training with textual data is an unsolved problem due\nto the fact that speech and text are very different modalities with distinct\ncharacteristics. In this paper, we propose a cross-modal Speech and Language\nModel (SpeechLM) to explicitly align speech and text pre-training with a\npre-defined unified discrete representation. Specifically, we introduce two\nalternative discrete tokenizers to bridge the speech and text modalities,\nincluding phoneme-unit and hidden-unit tokenizers, which can be trained using a\nsmall amount of paired speech-text data. Based on the trained tokenizers, we\nconvert the unlabeled speech and text data into tokens of phoneme units or\nhidden units. The pre-training objective is designed to unify the speech and\nthe text into the same discrete semantic space with a unified Transformer\nnetwork. We evaluate SpeechLM on various spoken language processing tasks\nincluding speech recognition, speech translation, and universal representation\nevaluation framework SUPERB, demonstrating significant improvements on\ncontent-related tasks. Code and models are available at\nhttps://aka.ms/SpeechLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sanyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuo Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhuoyuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1\">Lirong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language Representation Learning. (arXiv:2210.04183v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.04183","description":"<p>Multimodal representation learning has shown promising improvements on\nvarious vision-language tasks. Most existing methods excel at building\nglobal-level alignment between vision and language while lacking effective\nfine-grained image-text interaction. In this paper, we propose a jointly masked\nmultimodal modeling method to learn fine-grained multimodal representations.\nOur method performs joint masking on image-text input and integrates both\nimplicit and explicit targets for the masked signals to recover. The implicit\ntarget provides a unified and debiased objective for vision and language, where\nthe model predicts latent multimodal representations of the unmasked input. The\nexplicit target further enriches the multimodal representations by recovering\nhigh-level and semantically meaningful information: momentum visual features of\nimage patches and concepts of word tokens. Through such a masked modeling\nprocess, our model not only learns fine-grained multimodal interaction, but\nalso avoids the semantic gap between high-level representations and low- or\nmid-level prediction targets (e.g. image pixels), thus producing semantically\nrich multimodal representations that perform well on both zero-shot and\nfine-tuned settings. Our pre-trained model (named MAMO) achieves\nstate-of-the-art performance on various downstream vision-language tasks,\nincluding image-text retrieval, visual question answering, visual reasoning,\nand weakly-supervised visual grounding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zijia Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Longteng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xingjian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1\">Shuai Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting. (arXiv:2210.08964v3 [stat.ME] UPDATED)","link":"http://arxiv.org/abs/2210.08964","description":"<p>This paper presents a new perspective on time series forecasting. In existing\ntime series forecasting methods, the models take a sequence of numerical values\nas input and yield numerical values as output. The existing SOTA models are\nlargely based on the Transformer architecture, modified with multiple encoding\nmechanisms to incorporate the context and semantics around the historical data.\nInspired by the successes of pre-trained language foundation models, we pose a\nquestion about whether these models can also be adapted to solve time-series\nforecasting. Thus, we propose a new forecasting paradigm: prompt-based time\nseries forecasting (PromptCast). In this novel task, the numerical input and\noutput are transformed into prompts and the forecasting task is framed in a\nsentence-to-sentence manner, making it possible to directly apply language\nmodels for forecasting purposes. To support and facilitate the research of this\ntask, we also present a large-scale dataset (PISA) that includes three\nreal-world forecasting scenarios. We evaluate different SOTA numerical-based\nforecasting methods and language generation models. The benchmark results with\nvarious forecasting settings demonstrate the proposed PromptCast with language\ngeneration models is a promising research direction. Additionally, in\ncomparison to conventional numerical-based forecasting, PromptCast shows a much\nbetter generalization ability under the zero-shot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Xue_H/0/1/0/all/0/1\">Hao Xue</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Salim_F/0/1/0/all/0/1\">Flora D.Salim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ATCO2 corpus: A Large-Scale Dataset for Research on Automatic Speech Recognition and Natural Language Understanding of Air Traffic Control Communications. (arXiv:2211.04054v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.04054","description":"<p>Personal assistants, automatic speech recognizers and dialogue understanding\nsystems are becoming more critical in our interconnected digital world. A clear\nexample is air traffic control (ATC) communications. ATC aims at guiding\naircraft and controlling the airspace in a safe and optimal manner. These\nvoice-based dialogues are carried between an air traffic controller (ATCO) and\npilots via very-high frequency radio channels. In order to incorporate these\nnovel technologies into ATC (low-resource domain), large-scale annotated\ndatasets are required to develop the data-driven AI systems. Two examples are\nautomatic speech recognition (ASR) and natural language understanding (NLU). In\nthis paper, we introduce the ATCO2 corpus, a dataset that aims at fostering\nresearch on the challenging ATC field, which has lagged behind due to lack of\nannotated data. The ATCO2 corpus covers 1) data collection and pre-processing,\n2) pseudo-annotations of speech data, and 3) extraction of ATC-related named\nentities. The ATCO2 corpus is split into three subsets. 1) ATCO2-test-set\ncorpus contains 4 hours of ATC speech with manual transcripts and a subset with\ngold annotations for named-entity recognition (callsign, command, value). 2)\nThe ATCO2-PL-set corpus consists of 5281 hours of unlabeled ATC data enriched\nwith automatic transcripts from an in-domain speech recognizer, contextual\ninformation, speaker turn information, signal-to-noise ratio estimate and\nEnglish language detection score per sample. Both available for purchase\nthrough ELDA at <a href=\"http://catalog.elra.info/en-us/repository/browse/ELRA-S0484.\">this http URL</a> 3)\nThe ATCO2-test-set-1h corpus is a one-hour subset from the original test set\ncorpus, that we are offering for free at https://www.atco2.org/data. We expect\nthe ATCO2 corpus will foster research on robust ASR and NLU not only in the\nfield of ATC communications but also in the general research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vesely_K/0/1/0/all/0/1\">Karel Vesel&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szoke_I/0/1/0/all/0/1\">Igor Sz&#xf6;ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blatt_A/0/1/0/all/0/1\">Alexander Blatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocour_M/0/1/0/all/0/1\">Martin Kocour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigault_M/0/1/0/all/0/1\">Mickael Rigault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choukri_K/0/1/0/all/0/1\">Khalid Choukri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1\">Amrutha Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarfjoo_S/0/1/0/all/0/1\">Seyyed Saeed Sarfjoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nigmatulina_I/0/1/0/all/0/1\">Iuliia Nigmatulina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cevenini_C/0/1/0/all/0/1\">Claudia Cevenini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolcarek_P/0/1/0/all/0/1\">Pavel Kol&#x10d;&#xe1;rek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tart_A/0/1/0/all/0/1\">Allan Tart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cernocky_J/0/1/0/all/0/1\">Jan &#x10c;ernock&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing Hallucinations in Neural Machine Translation with Feature Attribution. (arXiv:2211.09878v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.09878","description":"<p>Neural conditional language generation models achieve the state-of-the-art in\nNeural Machine Translation (NMT) but are highly dependent on the quality of\nparallel training dataset. When trained on low-quality datasets, these models\nare prone to various error types, including hallucinations, i.e. outputs that\nare fluent, but unrelated to the source sentences. These errors are\nparticularly dangerous, because on the surface the translation can be perceived\nas a correct output, especially if the reader does not understand the source\nlanguage. We present a case study focusing on model understanding and\nregularisation to reduce hallucinations in NMT. We first use feature\nattribution methods to study the behaviour of an NMT model that produces\nhallucinations. We then leverage these methods to propose a novel loss function\nthat substantially helps reduce hallucinations and does not require retraining\nthe model from scratch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jo&#xeb;l Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fomicheva_M/0/1/0/all/0/1\">Marina Fomicheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inconsistency Ranking-based Noisy Label Detection for High-quality Data. (arXiv:2212.00239v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.00239","description":"<p>The success of deep learning requires high-quality annotated and massive\ndata. However, the size and the quality of a dataset are usually a trade-off in\npractice, as data collection and cleaning are expensive and time-consuming. In\nreal-world applications, especially those using crowdsourcing datasets, it is\nimportant to exclude noisy labels. To address this, this paper proposes an\nautomatic noisy label detection (NLD) technique with inconsistency ranking for\nhigh-quality data. We apply this technique to the automatic speaker\nverification (ASV) task as a proof of concept. We investigate both inter-class\nand intra-class inconsistency ranking and compare several metric learning loss\nfunctions under different noise settings. Experimental results confirm that the\nproposed solution could increase both the efficient and effective cleaning of\nlarge-scale speaker recognition datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1\">Ruibin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hanzhi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yifan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yushi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhizheng Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language. (arXiv:2212.07525v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2212.07525","description":"<p>Current self-supervised learning algorithms are often modality-specific and\nrequire large amounts of computational resources. To address these issues, we\nincrease the training efficiency of data2vec, a learning objective that\ngeneralizes across several modalities. We do not encode masked tokens, use a\nfast convolutional decoder and amortize the effort to build teacher\nrepresentations. data2vec 2.0 benefits from the rich contextualized target\nrepresentations introduced in data2vec which enable a fast self-supervised\nlearner. Experiments on ImageNet-1K image classification show that data2vec 2.0\nmatches the accuracy of Masked Autoencoders in 16.4x lower pre-training time,\non Librispeech speech recognition it performs as well as wav2vec 2.0 in 10.6x\nless time, and on GLUE natural language understanding it matches a retrained\nRoBERTa model in half the time. Trading some speed for accuracy results in\nImageNet-1K top-1 accuracy of 86.8\\% with a ViT-L model trained for 150 epochs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1\">Arun Babu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DOC: Improving Long Story Coherence With Detailed Outline Control. (arXiv:2212.10077v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10077","description":"<p>We propose the Detailed Outline Control (DOC) framework for improving\nlong-range plot coherence when automatically generating\nseveral-thousand-word-long stories. DOC consists of two complementary\ncomponents: a detailed outliner and a detailed controller. The detailed\noutliner creates a more detailed, hierarchically structured outline, shifting\ncreative burden from the main drafting procedure to the planning stage. The\ndetailed controller ensures the more detailed outline is still respected during\ngeneration by controlling story passages to align with outline details. In\nhuman evaluations of automatically generated stories, DOC substantially\noutperforms a strong Re3 baseline (Yang et al., 2022) on plot coherence (22.5%\nabsolute gain), outline relevance (28.2%), and interestingness (20.7%). Humans\nalso judged DOC to be much more controllable in an interactive generation\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization. (arXiv:2212.10397v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10397","description":"<p>To prevent the costly and inefficient use of resources on low-quality\nannotations, we want a method for creating a pool of dependable annotators who\ncan effectively complete difficult tasks, such as evaluating automatic\nsummarization. Thus, we investigate the recruitment of high-quality Amazon\nMechanical Turk workers via a two-step pipeline. We show that we can\nsuccessfully filter out subpar workers before they carry out the evaluations\nand obtain high-agreement annotations with similar constraints on resources.\nAlthough our workers demonstrate a strong consensus among themselves and\nCloudResearch workers, their alignment with expert judgments on a subset of the\ndata is not as expected and needs further training in correctness. This paper\nstill serves as a best practice for the recruitment of qualified annotators in\nother challenging annotation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lining Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mille_S/0/1/0/all/0/1\">Simon Mille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yufang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deutsch_D/0/1/0/all/0/1\">Daniel Deutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_E/0/1/0/all/0/1\">Elizabeth Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahamood_S/0/1/0/all/0/1\">Saad Mahamood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clinciu_M/0/1/0/all/0/1\">Miruna Clinciu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1\">Khyathi Chandu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is GPT-3 a Good Data Annotator?. (arXiv:2212.10450v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10450","description":"<p>Data annotation is the process of labeling data that could be used to train\nmachine learning models. Having high-quality annotation is crucial, as it\nallows the model to learn the relationship between the input data and the\ndesired output. GPT-3, a large-scale language model developed by OpenAI, has\ndemonstrated impressive zero- and few-shot performance on a wide range of NLP\ntasks. It is therefore natural to wonder whether it can be used to effectively\nannotate data for NLP tasks. In this paper, we evaluate the performance of\nGPT-3 as a data annotator by comparing it with traditional data annotation\nmethods and analyzing its output on a range of tasks. Through this analysis, we\naim to provide insight into the potential of GPT-3 as a general-purpose data\nannotator in NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bosheng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chengwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chia_Y/0/1/0/all/0/1\">Yew Ken Chia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-Augmented Linear Probing: Scaling beyond the Limit of Few-shot In-Context Learners. (arXiv:2212.10873v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10873","description":"<p>Through in-context learning (ICL), large-scale language models are effective\nfew-shot learners without additional model fine-tuning. However, the ICL\nperformance does not scale well with the number of available training samples\nas it is limited by the inherent input length constraint of the underlying\nlanguage model. Meanwhile, many studies have revealed that language models are\nalso powerful feature extractors, allowing them to be utilized in a black-box\nmanner and enabling the linear probing paradigm, where lightweight\ndiscriminators are trained on top of the pre-extracted input representations.\nThis paper proposes prompt-augmented linear probing (PALP), a hybrid of linear\nprobing and ICL, which leverages the best of both worlds. PALP inherits the\nscalability of linear probing and the capability of enforcing language models\nto derive more meaningful representations via tailoring input into a more\nconceivable form. Throughout in-depth investigations on various datasets, we\nverified that PALP significantly enhances the input representations closing the\ngap between ICL in the data-hungry scenario and fine-tuning in the\ndata-abundant scenario with little training overhead, potentially making PALP a\nstrong alternative in a black-box scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyunsoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyuhng Joon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junyeob Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-goo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taeuk Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient Transfer Learning. (arXiv:2301.11660v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.11660","description":"<p>As the size of the pre-trained language model (PLM) continues to increase,\nnumerous parameter-efficient transfer learning methods have been proposed\nrecently to compensate for the tremendous cost of fine-tuning. Despite the\nimpressive results achieved by large pre-trained language models (PLMs) and\nvarious parameter-efficient transfer learning (PETL) methods on sundry\nbenchmarks, it remains unclear if they can handle inputs that have been\ndistributionally shifted effectively. In this study, we systematically explore\nhow the ability to detect out-of-distribution (OOD) changes as the size of the\nPLM grows or the transfer methods are altered. Specifically, we evaluated\nvarious PETL techniques, including fine-tuning, Adapter, LoRA, and\nprefix-tuning, on three different intention classification tasks, each\nutilizing various language models with different scales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyunsoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Choonghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junyeop Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyuhng Joon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-goo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding Language Models to Images for Multimodal Inputs and Outputs. (arXiv:2301.13823v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.13823","description":"<p>We propose an efficient method to ground pretrained text-only language models\nto the visual domain, enabling them to process arbitrarily interleaved\nimage-and-text data, and generate text interleaved with retrieved images. Our\nmethod leverages the abilities of language models learnt from large scale\ntext-only pretraining, such as in-context learning and free-form text\ngeneration. We keep the language model frozen, and finetune input and output\nlinear layers to enable cross-modality interactions. This allows our model to\nprocess arbitrarily interleaved image-and-text inputs, and generate free-form\ntext interleaved with retrieved images. We achieve strong zero-shot performance\non grounded tasks such as contextual image retrieval and multimodal dialogue,\nand showcase compelling interactive abilities. Our approach works with any\noff-the-shelf language model and paves the way towards an effective, general\nsolution for leveraging pretrained language models in visually grounded\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1\">Jing Yu Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large language models predict human sensory judgments across six modalities. (arXiv:2302.01308v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.01308","description":"<p>Determining the extent to which the perceptual world can be recovered from\nlanguage is a longstanding problem in philosophy and cognitive science. We show\nthat state-of-the-art large language models can unlock new insights into this\nproblem by providing a lower bound on the amount of perceptual information that\ncan be extracted from language. Specifically, we elicit pairwise similarity\njudgments from GPT models across six psychophysical datasets. We show that the\njudgments are significantly correlated with human data across all domains,\nrecovering well-known representations like the color wheel and pitch spiral.\nSurprisingly, we find that a model (GPT-4) co-trained on vision and language\ndoes not necessarily lead to improvements specific to the visual modality. To\nstudy the influence of specific languages on perception, we also apply the\nmodels to a multilingual color-naming task. We find that GPT-4 replicates\ncross-linguistic variation in English and Russian illuminating the interaction\nof language and perception.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marjieh_R/0/1/0/all/0/1\">Raja Marjieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sucholutsky_I/0/1/0/all/0/1\">Ilia Sucholutsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijn_P/0/1/0/all/0/1\">Pol van Rijn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1\">Nori Jacoby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pretraining Language Models with Human Preferences. (arXiv:2302.08582v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.08582","description":"<p>Language models (LMs) are pretrained to imitate internet text, including\ncontent that would violate human preferences if generated by an LM: falsehoods,\noffensive comments, personally identifiable information, low-quality or buggy\ncode, and more. Here, we explore alternative objectives for pretraining LMs in\na way that also guides them to generate text aligned with human preferences. We\nbenchmark five objectives for pretraining with human feedback across three\ntasks and study how they affect the trade-off between alignment and\ncapabilities of pretrained LMs. We find a Pareto-optimal and simple approach\namong those we explored: conditional training, or learning distribution over\ntokens conditional on their human preference scores given by a reward model.\nConditional training reduces the rate of undesirable content by up to an order\nof magnitude, both when generating without a prompt and with an\nadversarially-chosen prompt. Moreover, conditional training maintains the\ndownstream task performance of standard LM pretraining, both before and after\ntask-specific finetuning. Pretraining with human feedback results in much\nbetter preference satisfaction than standard LM pretraining followed by\nfinetuning with feedback, i.e., learning and then unlearning undesirable\nbehavior. Our results suggest that we should move beyond imitation learning\nwhen pretraining LMs and incorporate human preferences from the start of\ntraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korbak_T/0/1/0/all/0/1\">Tomasz Korbak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Kejian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Angelica Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhalerao_R/0/1/0/all/0/1\">Rasika Bhalerao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buckley_C/0/1/0/all/0/1\">Christopher L. Buckley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phang_J/0/1/0/all/0/1\">Jason Phang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Transductions and Alignments with RNN Seq2seq Models. (arXiv:2303.06841v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.06841","description":"<p>The paper studies the capabilities of Recurrent-Neural-Network sequence to\nsequence (RNN seq2seq) models in learning four transduction tasks: identity,\nreversal, total reduplication, and quadratic copying. These transductions are\ntraditionally well studied under finite state transducers and attributed with\nincreasing complexity. We find that RNN seq2seq models are only able to\napproximate a mapping that fits the training or in-distribution data, instead\nof learning the underlying functions. Although attention makes learning more\nefficient and robust, it does not overcome the out-of-distribution\ngeneralization limitation. We establish a novel complexity hierarchy for\nlearning the four tasks for attention-less RNN seq2seq models, which may be\nunderstood in terms of the complexity hierarchy of formal languages, instead of\nstring transductions. RNN variants also play a role in the results. In\nparticular, we show that Simple RNN seq2seq models cannot count the input\nlength.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhengxiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. (arXiv:2303.16199v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2303.16199","description":"<p>We present LLaMA-Adapter, a lightweight adaption method to efficiently\nfine-tune LLaMA into an instruction-following model. Using 52K self-instruct\ndemonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon\nthe frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8\nA100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and\nprepend them to the word tokens at higher transformer layers. Then, a\nzero-initialized attention mechanism with zero gating is proposed, which\nadaptively injects the new instructional cues into LLaMA, while effectively\npreserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter\ncan generate high-quality responses, comparable to Alpaca with fully fine-tuned\n7B parameters. Besides language commands, our approach can be simply extended\nto multi-modal instructions for learning image-conditioned LLaMA model, which\nachieves superior reasoning performance on ScienceQA and COCO Caption\nbenchmarks. Furthermore, we also evaluate the zero-initialized attention\nmechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on\ntraditional vision and language tasks, demonstrating the superior\ngeneralization capacity of our approach. Code is released at\nhttps://github.com/OpenGVLab/LLaMA-Adapter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiaming Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chris Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aojun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiangfei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shilin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2304.04370","description":"<p>Human intelligence has the remarkable ability to assemble basic skills into\ncomplex ones so as to solve complex tasks. This ability is equally important\nfor Artificial Intelligence (AI), and thus, we assert that in addition to the\ndevelopment of large, comprehensive intelligent models, it is equally crucial\nto equip such models with the capability to harness various domain-specific\nexpert models for complex task-solving in the pursuit of Artificial General\nIntelligence (AGI). Recent developments in Large Language Models (LLMs) have\ndemonstrated remarkable learning and reasoning abilities, making them promising\nas a controller to select, synthesize, and execute external models to solve\ncomplex tasks. In this project, we develop OpenAGI, an open-source AGI research\nplatform, specifically designed to offer complex, multi-step tasks and\naccompanied by task-specific datasets, evaluation metrics, and a diverse range\nof extensible models. OpenAGI formulates complex tasks as natural language\nqueries, serving as input to the LLM. The LLM subsequently selects,\nsynthesizes, and executes models provided by OpenAGI to address the task.\nFurthermore, we propose a Reinforcement Learning from Task Feedback (RLTF)\nmechanism, which uses the task-solving result as feedback to improve the LLM's\ntask-solving ability. Thus, the LLM is responsible for synthesizing various\nexternal models for solving complex tasks, while RLTF provides feedback to\nimprove its task-solving ability, enabling a feedback loop for self-improving\nAI. We believe that the paradigm of LLMs operating various expert models for\ncomplex task-solving is a promising approach towards AGI. To facilitate the\ncommunity's long-term improvement and evaluation of AGI's ability, we\nopen-source the code, benchmark, and evaluation methods of the OpenAGI project\nat https://github.com/agiresearch/OpenAGI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wenyue Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_K/0/1/0/all/0/1\">Kai Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jianchao Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Juntao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zelong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca. (arXiv:2304.08177v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.08177","description":"<p>Large Language Models (LLMs), such as ChatGPT and GPT-4, have dramatically\ntransformed natural language processing research and shown promising strides\ntowards Artificial General Intelligence (AGI). Nonetheless, the high costs\nassociated with training and deploying LLMs present substantial obstacles to\ntransparent, accessible academic research. While several large language models,\nsuch as LLaMA, have been open-sourced by the community, these predominantly\nfocus on English corpora, limiting their usefulness for other languages. In\nthis paper, we propose a method to augment LLaMA with capabilities for\nunderstanding and generating Chinese text and its ability to follow\ninstructions. We achieve this by extending LLaMA's existing vocabulary with an\nadditional 20,000 Chinese tokens, thereby improving its encoding efficiency and\nsemantic understanding of Chinese. We further incorporate secondary\npre-training using Chinese data and fine-tune the model with Chinese\ninstruction datasets, significantly enhancing the model's ability to comprehend\nand execute instructions. Our experimental results indicate that the newly\nproposed model markedly enhances the original LLaMA's proficiency in\nunderstanding and generating Chinese content. Additionally, the results on the\nC-Eval dataset yield competitive performance among the models with several\ntimes the size of ours. We have made our pre-trained models, training scripts,\nand other resources available through GitHub, fostering open research for our\ncommunity. GitHub repository: https://github.com/ymcui/Chinese-LLaMA-Alpaca\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yiming Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xin Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tool Learning with Foundation Models. (arXiv:2304.08354v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.08354","description":"<p>Humans possess an extraordinary ability to create and utilize tools, allowing\nthem to overcome physical limitations and explore new frontiers. With the\nadvent of foundation models, AI systems have the potential to be equally adept\nin tool use as humans. This paradigm, i.e., tool learning with foundation\nmodels, combines the strengths of specialized tools and foundation models to\nachieve enhanced accuracy, efficiency, and automation in problem-solving.\nDespite its immense potential, there is still a lack of a comprehensive\nunderstanding of key challenges, opportunities, and future endeavors in this\nfield. To this end, we present a systematic investigation of tool learning in\nthis paper. We first introduce the background of tool learning, including its\ncognitive origins, the paradigm shift of foundation models, and the\ncomplementary roles of tools and models. Then we recapitulate existing tool\nlearning research into tool-augmented and tool-oriented learning. We formulate\na general tool learning framework: starting from understanding the user\ninstruction, models should learn to decompose a complex task into several\nsubtasks, dynamically adjust their plan through reasoning, and effectively\nconquer each sub-task by selecting appropriate tools. We also discuss how to\ntrain models for improved tool-use capabilities and facilitate the\ngeneralization in tool learning. Considering the lack of a systematic tool\nlearning evaluation in prior works, we experiment with 18 representative tools\nand show the potential of current foundation models in skillfully utilizing\ntools. Finally, we discuss several open problems that require further\ninvestigation for tool learning. Overall, we hope this paper could inspire\nfuture research in integrating tools with foundation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengding Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weize Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1\">Ganqu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zheni Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaojun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_Y/0/1/0/all/0/1\">Yi Ren Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yusheng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huadong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Cheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_R/0/1/0/all/0/1\">Runchu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kunlun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shihao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xingyu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bokai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yining Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Ziwei Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jing Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuzhang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zhenning Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Lan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1\">Xin Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yaxi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuxiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junxi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dahai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phang_J/0/1/0/all/0/1\">Jason Phang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Multilingual Spellchecker for User Queries. (arXiv:2305.01082v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01082","description":"<p>Spellchecking is one of the most fundamental and widely used search features.\nCorrecting incorrectly spelled user queries not only enhances the user\nexperience but is expected by the user. However, most widely available\nspellchecking solutions are either lower accuracy than state-of-the-art\nsolutions or too slow to be used for search use cases where latency is a key\nrequirement. Furthermore, most innovative recent architectures focus on English\nand are not trained in a multilingual fashion and are trained for spell\ncorrection in longer text, which is a different paradigm from spell correction\nfor user queries, where context is sparse (most queries are 1-2 words long).\nFinally, since most enterprises have unique vocabularies such as product names,\noff-the-shelf spelling solutions fall short of users' needs. In this work, we\nbuild a multilingual spellchecker that is extremely fast and scalable and that\nadapts its vocabulary and hence speller output based on a specific product's\nneeds. Furthermore, our speller out-performs general purpose spellers by a wide\nmargin on in-domain datasets. Our multilingual speller is used in search in\nAdobe products, powering autocomplete in various applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Sanat Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valls_Vargas_J/0/1/0/all/0/1\">Josep Valls-Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_T/0/1/0/all/0/1\">Tracy Holloway King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1\">Francois Guerin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1\">Chirag Arora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPTutor: a ChatGPT-powered programming tool for code explanation. (arXiv:2305.01863v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2305.01863","description":"<p>Learning new programming skills requires tailored guidance. With the\nemergence of advanced Natural Language Generation models like the ChatGPT API,\nthere is now a possibility of creating a convenient and personalized tutoring\nsystem with AI for computer science education. This paper presents GPTutor, a\nChatGPT-powered programming tool, which is a Visual Studio Code extension using\nthe ChatGPT API to provide programming code explanations. By integrating Visual\nStudio Code API, GPTutor can comprehensively analyze the provided code by\nreferencing the relevant source codes. As a result, GPTutor can use designed\nprompts to explain the selected code with a pop-up message. GPTutor is now\npublished at the Visual Studio Code Extension Marketplace, and its source code\nis openly accessible on GitHub. Preliminary evaluation indicates that GPTutor\ndelivers the most concise and accurate explanations compared to vanilla ChatGPT\nand GitHub Copilot. Moreover, the feedback from students and teachers indicated\nthat GPTutor is user-friendly and can explain given codes satisfactorily.\nFinally, we discuss possible future research directions for GPTutor. This\nincludes enhancing its performance and personalization via further prompt\nprogramming, as well as evaluating the effectiveness of GPTutor with real\nusers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Eason Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Ray Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Han-Shin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_Y/0/1/0/all/0/1\">Yuen-Hsien Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang-Yi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Influence of Language on Time-Reward Perceptions in Large Language Models: A Study Using GPT-3.5. (arXiv:2305.02531v3 [econ.GN] UPDATED)","link":"http://arxiv.org/abs/2305.02531","description":"<p>Language has a strong influence on our perceptions of time and rewards. This\nraises the question of whether large language models, when asked the same\nquestion in different languages, show different preferences for rewards over\ntime and if their choices are similar to those of humans. In this study, we\nanalyze the responses of GPT-3.5 (hereafter referred to as GPT) to prompts in\nmultiple languages, exploring preferences between smaller, sooner rewards and\nlarger, later rewards. Our results show that GPT displays greater patience when\nprompted in languages with weak future tense references (FTR), such as German\nand Mandarin, compared to languages with strong FTR, like English and French.\nThese findings are consistent with the existing literature and suggest a\ncorrelation between GPT's choices and the preferences of speakers of these\nlanguages. However, further analysis reveals that the preference for earlier or\nlater rewards does not systematically change with reward gaps, indicating a\nlexicographic preference for earlier payments. While GPT may capture intriguing\nvariations across languages, our findings indicate that the choices made by\nthese models do not correspond to those of human decision-makers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/econ/1/au:+Goli_A/0/1/0/all/0/1\">Ali Goli</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Singh_A/0/1/0/all/0/1\">Amandeep Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LMs stand their Ground: Investigating the Effect of Embodiment in Figurative Language Interpretation by Language Models. (arXiv:2305.03445v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03445","description":"<p>Figurative language is a challenge for language models since its\ninterpretation is based on the use of words in a way that deviates from their\nconventional order and meaning. Yet, humans can easily understand and interpret\nmetaphors, similes or idioms as they can be derived from embodied metaphors.\nLanguage is a proxy for embodiment and if a metaphor is conventional and\nlexicalised, it becomes easier for a system without a body to make sense of\nembodied concepts. Yet, the intricate relation between embodiment and features\nsuch as concreteness or age of acquisition has not been studied in the context\nof figurative language interpretation concerning language models. Hence, the\npresented study shows how larger language models perform better at interpreting\nmetaphoric sentences when the action of the metaphorical sentence is more\nembodied. The analysis rules out multicollinearity with other features (e.g.\nword length or concreteness) and provides initial evidence that larger language\nmodels conceptualise embodied concepts to a degree that facilitates figurative\nlanguage understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wicke_P/0/1/0/all/0/1\">Philipp Wicke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Text-to-SQL Generation via Editable Step-by-Step Explanations. (arXiv:2305.07372v2 [cs.DB] UPDATED)","link":"http://arxiv.org/abs/2305.07372","description":"<p>Relational databases play an important role in this Big Data era. However, it\nis challenging for non-experts to fully unleash the analytical power of\nrelational databases, since they are not familiar with database languages such\nas SQL. Many techniques have been proposed to automatically generate SQL from\nnatural language, but they suffer from two issues: (1) they still make many\nmistakes, particularly for complex queries, and (2) they do not provide a\nflexible way for non-expert users to validate and refine the incorrect queries.\nTo address these issues, we introduce a new interaction mechanism that allows\nusers directly edit a step-by-step explanation of an incorrect SQL to fix SQL\nerrors. Experiments on the Spider benchmark show that our approach outperforms\nthree SOTA approaches by at least 31.6% in terms of execution accuracy. A user\nstudy with 24 participants further shows that our approach helped users solve\nsignificantly more SQL tasks with less time and higher confidence,\ndemonstrating its potential to expand access to databases, particularly for\nnon-experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Z/0/1/0/all/0/1\">Zheng Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Toby Jia-Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kummerfeld_J/0/1/0/all/0/1\">Jonathan K. Kummerfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM-Pruner: On the Structural Pruning of Large Language Models. (arXiv:2305.11627v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11627","description":"<p>Large language models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. However, such impressive capability typically\ncomes with a substantial model size, which presents significant challenges in\nboth the deployment, inference, and training stages. With LLM being a\ngeneral-purpose task solver, we explore its compression in a task-agnostic\nmanner, which aims to preserve the multi-task solving and language generation\nability of the original LLM. One challenge to achieving this is the enormous\nsize of the training corpus of LLM, which makes both data transfer and model\npost-training over-burdensome. Thus, we tackle the compression of LLMs within\nthe bound of two constraints: being task-agnostic and minimizing the reliance\non the original training dataset. Our method, named LLM-Pruner, adopts\nstructural pruning that selectively removes non-critical coupled structures\nbased on gradient information, maximally preserving the majority of the LLM's\nfunctionality. To this end, the performance of pruned models can be efficiently\nrecovered through tuning techniques, LoRA, in merely 3 hours, requiring only\n50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna,\nand ChatGLM, and demonstrate that the compressed models still exhibit\nsatisfactory capabilities in zero-shot classification and generation. The code\nis available at: https://github.com/horseee/LLM-Pruner\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinyin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_G/0/1/0/all/0/1\">Gongfan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebIE: Faithful and Robust Information Extraction on the Web. (arXiv:2305.14293v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14293","description":"<p>Extracting structured and grounded fact triples from raw text is a\nfundamental task in Information Extraction (IE). Existing IE datasets are\ntypically collected from Wikipedia articles, using hyperlinks to link entities\nto the Wikidata knowledge base. However, models trained only on Wikipedia have\nlimitations when applied to web domains, which often contain noisy text or text\nthat does not have any factual information. We present WebIE, the first\nlarge-scale, entity-linked closed IE dataset consisting of 1.6M sentences\nautomatically collected from the English Common Crawl corpus. WebIE also\nincludes negative examples, i.e. sentences without fact triples, to better\nreflect the data on the web. We annotate ~21K triples from WebIE through\ncrowdsourcing and introduce mWebIE, a translation of the annotated set in four\nother languages: French, Spanish, Portuguese, and Hindi. We evaluate the\nin-domain, out-of-domain, and zero-shot cross-lingual performance of generative\nIE models and find models trained on WebIE show better generalisability. We\nalso propose three training strategies that use entity linking as an auxiliary\ntask. Our experiments show that adding Entity-Linking objectives improves the\nfaithfulness of our generative IE models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Whitehouse_C/0/1/0/all/0/1\">Chenxi Whitehouse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vania_C/0/1/0/all/0/1\">Clara Vania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christodoulopoulos_C/0/1/0/all/0/1\">Christos Christodoulopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierleoni_A/0/1/0/all/0/1\">Andrea Pierleoni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Diffusion Models in Natural Language Processing. (arXiv:2305.14671v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14671","description":"<p>This survey paper provides a comprehensive review of the use of diffusion\nmodels in natural language processing (NLP). Diffusion models are a class of\nmathematical models that aim to capture the diffusion of information or signals\nacross a network or manifold. In NLP, diffusion models have been used in a\nvariety of applications, such as natural language generation, sentiment\nanalysis, topic modeling, and machine translation. This paper discusses the\ndifferent formulations of diffusion models used in NLP, their strengths and\nlimitations, and their applications. We also perform a thorough comparison\nbetween diffusion models and alternative generative models, specifically\nhighlighting the autoregressive (AR) models, while also examining how diverse\narchitectures incorporate the Transformer in conjunction with diffusion models.\nCompared to AR models, diffusion models have significant advantages for\nparallel generation, text interpolation, token-level controls such as syntactic\nstructures and semantic contents, and robustness. Exploring further\npermutations of integrating Transformers into diffusion models would be a\nvaluable pursuit. Also, the development of multimodal diffusion models and\nlarge-scale diffusion language models with notable capabilities for few-shot\nlearning would be important directions for the future advance of diffusion\nmodels in NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1\">Hao Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Z/0/1/0/all/0/1\">Zae Myung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bhasha-Abhijnaanam: Native-script and romanized Language Identification for 22 Indic languages. (arXiv:2305.15814v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.15814","description":"<p>We create publicly available language identification (LID) datasets and\nmodels in all 22 Indian languages listed in the Indian constitution in both\nnative-script and romanized text. First, we create Bhasha-Abhijnaanam, a\nlanguage identification test set for native-script as well as romanized text\nwhich spans all 22 Indic languages. We also train IndicLID, a language\nidentifier for all the above-mentioned languages in both native and romanized\nscript. For native-script text, it has better language coverage than existing\nLIDs and is competitive or better than other LIDs. IndicLID is the first LID\nfor romanized text in Indian languages. Two major challenges for romanized text\nLID are the lack of training data and low-LID performance when languages are\nsimilar. We provide simple and effective solutions to these problems. In\ngeneral, there has been limited work on romanized text in any language, and our\nfindings are relevant to other languages that need romanized language\nidentification. Our models are publicly available at\nhttps://ai4bharat.iitm.ac.in/indiclid under open-source licenses. Our training\nand test sets are also publicly available at\nhttps://ai4bharat.iitm.ac.in/bhasha-abhijnaanam under open-source licenses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madhani_Y/0/1/0/all/0/1\">Yash Madhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Data-Constrained Language Models. (arXiv:2305.16264v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16264","description":"<p>The current trend of scaling language models involves increasing both\nparameter count and training dataset size. Extrapolating this trend suggests\nthat training dataset size may soon be limited by the amount of text data\navailable on the internet. Motivated by this limit, we investigate scaling\nlanguage models in data-constrained regimes. Specifically, we run a large set\nof experiments varying the extent of data repetition and compute budget,\nranging up to 900 billion training tokens and 9 billion parameter models. We\nfind that with constrained data for a fixed compute budget, training with up to\n4 epochs of repeated data yields negligible changes to loss compared to having\nunique data. However, with more repetition, the value of adding compute\neventually decays to zero. We propose and empirically validate a scaling law\nfor compute optimality that accounts for the decreasing value of repeated\ntokens and excess parameters. Finally, we experiment with approaches mitigating\ndata scarcity, including augmenting the training dataset with code data or\nremoving commonly used filters. Models and datasets from our 400 training runs\nare freely available at https://github.com/huggingface/datablations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1\">Niklas Muennighoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barak_B/0/1/0/all/0/1\">Boaz Barak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scao_T/0/1/0/all/0/1\">Teven Le Scao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piktus_A/0/1/0/all/0/1\">Aleksandra Piktus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tazi_N/0/1/0/all/0/1\">Nouamane Tazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyysalo_S/0/1/0/all/0/1\">Sampo Pyysalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_T/0/1/0/all/0/1\">Thomas Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GDA: Generative Data Augmentation Techniques for Relation Extraction Tasks. (arXiv:2305.16663v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16663","description":"<p>Relation extraction (RE) tasks show promising performance in extracting\nrelations from two entities mentioned in sentences, given sufficient\nannotations available during training. Such annotations would be\nlabor-intensive to obtain in practice. Existing work adopts data augmentation\ntechniques to generate pseudo-annotated sentences beyond limited annotations.\nThese techniques neither preserve the semantic consistency of the original\nsentences when rule-based augmentations are adopted, nor preserve the syntax\nstructure of sentences when expressing relations using seq2seq models,\nresulting in less diverse augmentations. In this work, we propose a dedicated\naugmentation technique for relational texts, named GDA, which uses two\ncomplementary modules to preserve both semantic consistency and syntax\nstructures. We adopt a generative formulation and design a multi-tasking\nsolution to achieve synergies. Furthermore, GDA adopts entity hints as the\nprior knowledge of the generative model to augment diverse sentences.\nExperimental results in three datasets under a low-resource setting showed that\nGDA could bring {\\em 2.0\\%} F1 improvements compared with no augmentation\ntechnique. Source code and data are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zeqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Imagine: Visually-Augmented Natural Language Generation. (arXiv:2305.16944v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16944","description":"<p>People often imagine relevant scenes to aid in the writing process. In this\nwork, we aim to utilize visual information for composition in the same manner\nas humans. We propose a method, LIVE, that makes pre-trained language models\n(PLMs) Learn to Imagine for Visuallyaugmented natural language gEneration.\nFirst, we imagine the scene based on the text: we use a diffusion model to\nsynthesize high-quality images conditioned on the input texts. Second, we use\nCLIP to determine whether the text can evoke the imagination in a posterior\nway. Finally, our imagination is dynamic, and we conduct synthesis for each\nsentence rather than generate only one image for an entire paragraph.\nTechnically, we propose a novel plug-and-play fusion layer to obtain\nvisually-augmented representations for each text. Our vision-text fusion layer\nis compatible with Transformerbased architecture. We have conducted extensive\nexperiments on four generation tasks using BART and T5, and the automatic\nresults and human evaluation demonstrate the effectiveness of our proposed\nmethod. We will release the code, model, and data at the link:\nhttps://github.com/RUCAIBox/LIVE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yushuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yifan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Images with Multimodal Language Models. (arXiv:2305.17216v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17216","description":"<p>We propose a method to fuse frozen text-only large language models (LLMs)\nwith pre-trained image encoder and decoder models, by mapping between their\nembedding spaces. Our model demonstrates a wide suite of multimodal\ncapabilities: image retrieval, novel image generation, and multimodal dialogue.\nOurs is the first approach capable of conditioning on arbitrarily interleaved\nimage and text inputs to generate coherent image (and text) outputs. To achieve\nstrong performance on image generation, we propose an efficient mapping network\nto ground the LLM to an off-the-shelf text-to-image generation model. This\nmapping network translates hidden representations of text into the embedding\nspace of the visual models, enabling us to leverage the strong text\nrepresentations of the LLM for visual outputs. Our approach outperforms\nbaseline generation models on tasks with longer and more complex language. In\naddition to novel image generation, our model is also capable of image\nretrieval from a prespecified dataset, and decides whether to retrieve or\ngenerate at inference time. This is done with a learnt decision module which\nconditions on the hidden representations of the LLM. Our model exhibits a wider\nrange of capabilities compared to prior multimodal language models. It can\nprocess image-and-text inputs, and produce retrieved images, generated images,\nand generated text -- outperforming non-LLM based generation models across\nseveral text-to-image tasks that measure context dependence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1\">Jing Yu Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallel Neurosymbolic Integration with Concordia. (arXiv:2306.00480v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2306.00480","description":"<p>Parallel neurosymbolic architectures have been applied effectively in NLP by\ndistilling knowledge from a logic theory into a deep model.However, prior art\nfaces several limitations including supporting restricted forms of logic\ntheories and relying on the assumption of independence between the logic and\nthe deep network. We present Concordia, a framework overcoming the limitations\nof prior art. Concordia is agnostic both to the deep network and the logic\ntheory offering support for a wide range of probabilistic theories. Our\nframework can support supervised training of both components and unsupervised\ntraining of the neural component. Concordia has been successfully applied to\ntasks beyond NLP and data classification, improving the accuracy of\nstate-of-the-art on collective activity detection, entity linking and\nrecommendation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feldstein_J/0/1/0/all/0/1\">Jonathan Feldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurcius_M/0/1/0/all/0/1\">Modestas Jur&#x10d;ius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsamoura_E/0/1/0/all/0/1\">Efthymia Tsamoura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Hate Speech Benchmarks: From Data Curation to System Deployment. (arXiv:2306.01105v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.01105","description":"<p>Social media is awash with hateful content, much of which is often veiled\nwith linguistic and topical diversity. The benchmark datasets used for hate\nspeech detection do not account for such divagation as they are predominantly\ncompiled using hate lexicons. However, capturing hate signals becomes\nchallenging in neutrally-seeded malicious content. Thus, designing models and\ndatasets that mimic the real-world variability of hate warrants further\ninvestigation.\n</p>\n<p>To this end, we present GOTHate, a large-scale code-mixed crowdsourced\ndataset of around 51k posts for hate speech detection from Twitter. GOTHate is\nneutrally seeded, encompassing different languages and topics. We conduct\ndetailed comparisons of GOTHate with the existing hate speech datasets,\nhighlighting its novelty. We benchmark it with 10 recent baselines. Our\nextensive empirical and benchmarking experiments suggest that GOTHate is hard\nto classify in a text-only setup. Thus, we investigate how adding endogenous\nsignals enhances the hate speech detection task. We augment GOTHate with the\nuser's timeline information and ego network, bringing the overall data source\ncloser to the real-world setup for understanding hateful content. Our proposed\nsolution HEN-mBERT is a modular, multilingual, mixture-of-experts model that\nenriches the linguistic subspace with latent endogenous signals from history,\ntopology, and exemplars. HEN-mBERT transcends the best baseline by 2.5% and 5%\nin overall macro-F1 and hate class F1, respectively. Inspired by our\nexperiments, in partnership with Wipro AI, we are developing a semi-automated\npipeline to detect hateful content as a part of their mission to tackle online\nharm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Atharva Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masud_S/0/1/0/all/0/1\">Sarah Masud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1\">Vikram Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?. (arXiv:2306.01248v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.01248","description":"<p>Automatic summarization of legal case judgements has traditionally been\nattempted by using extractive summarization methods. However, in recent years,\nabstractive summarization models are gaining popularity since they can generate\nmore natural and coherent summaries. Legal domain-specific pre-trained\nabstractive summarization models are now available. Moreover, general-domain\npre-trained Large Language Models (LLMs), such as ChatGPT, are known to\ngenerate high-quality text and have the capacity for text summarization. Hence\nit is natural to ask if these models are ready for off-the-shelf application to\nautomatically generate abstractive summaries for case judgements. To explore\nthis question, we apply several state-of-the-art domain-specific abstractive\nsummarization models and general-domain LLMs on Indian court case judgements,\nand check the quality of the generated summaries. In addition to standard\nmetrics for summary quality, we check for inconsistencies and hallucinations in\nthe summaries. We see that abstractive summarization models generally achieve\nslightly higher scores than extractive models in terms of standard summary\nevaluation metrics such as ROUGE and BLEU. However, we often find inconsistent\nor hallucinated information in the generated abstractive summaries. Overall,\nour investigation indicates that the pre-trained abstractive summarization\nmodels and LLMs are not yet ready for fully automatic deployment for case\njudgement summarization; rather a human-in-the-loop approach including manual\nchecks for inconsistencies is more suitable at present.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deroy_A/0/1/0/all/0/1\">Aniket Deroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_K/0/1/0/all/0/1\">Kripabandhu Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Saptarshi Ghosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PassGPT: Password Modeling and (Guided) Generation with Large Language Models. (arXiv:2306.01545v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.01545","description":"<p>Large language models (LLMs) successfully model natural language from vast\namounts of text without the need for explicit supervision. In this paper, we\ninvestigate the efficacy of LLMs in modeling passwords. We present PassGPT, a\nLLM trained on password leaks for password generation. PassGPT outperforms\nexisting methods based on generative adversarial networks (GAN) by guessing\ntwice as many previously unseen passwords. Furthermore, we introduce the\nconcept of guided password generation, where we leverage PassGPT sampling\nprocedure to generate passwords matching arbitrary constraints, a feat lacking\nin current GAN-based strategies. Lastly, we conduct an in-depth analysis of the\nentropy and probability distribution that PassGPT defines over passwords and\ndiscuss their use in enhancing existing password strength estimators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rando_J/0/1/0/all/0/1\">Javier Rando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Cruz_F/0/1/0/all/0/1\">Fernando Perez-Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hitaj_B/0/1/0/all/0/1\">Briland Hitaj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Partially Annotated Data: Example-aware Creation of Gap-filling Exercises for Language Learning. (arXiv:2306.01584v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.01584","description":"<p>Since performing exercises (including, e.g., practice tests) forms a crucial\ncomponent of learning, and creating such exercises requires non-trivial effort\nfrom the teacher, there is a great value in automatic exercise generation in\ndigital tools in education. In this paper, we particularly focus on automatic\ncreation of gapfilling exercises for language learning, specifically grammar\nexercises. Since providing any annotation in this domain requires human expert\neffort, we aim to avoid it entirely and explore the task of converting existing\ntexts into new gap-filling exercises, purely based on an example exercise,\nwithout explicit instruction or detailed annotation of the intended grammar\ntopics. We contribute (i) a novel neural network architecture specifically\ndesigned for aforementioned gap-filling exercise generation task, and (ii) a\nreal-world benchmark dataset for French grammar. We show that our model for\nthis French grammar gap-filling exercise generation outperforms a competitive\nbaseline classifier by 8% in F1 percentage points, achieving an average F1\nscore of 82%. Our model implementation and the dataset are made publicly\navailable to foster future research, thus offering a standardized evaluation\nand baseline solution of the proposed partially annotated data prediction task\nin grammar exercise creation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bitew_S/0/1/0/all/0/1\">Semere Kiros Bitew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deleu_J/0/1/0/all/0/1\">Johannes Deleu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dogruoz_A/0/1/0/all/0/1\">A. Seza Do&#x11f;ru&#xf6;z</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Develder_C/0/1/0/all/0/1\">Chris Develder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1\">Thomas Demeester</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MidMed: Towards Mixed-Type Dialogues for Medical Consultation. (arXiv:2306.02923v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.02923","description":"<p>Most medical dialogue systems assume that patients have clear goals (medicine\nquerying, surgical operation querying, etc.) before medical consultation.\nHowever, in many real scenarios, due to the lack of medical knowledge, it is\nusually difficult for patients to determine clear goals with all necessary\nslots. In this paper, we identify this challenge as how to construct medical\nconsultation dialogue systems to help patients clarify their goals. To mitigate\nthis challenge, we propose a novel task and create a human-to-human mixed-type\nmedical consultation dialogue corpus, termed MidMed, covering five dialogue\ntypes: task-oriented dialogue for diagnosis, recommendation, knowledge-grounded\ndialogue, QA, and chitchat. MidMed covers four departments\n(otorhinolaryngology, ophthalmology, skin, and digestive system), with 8,175\ndialogues. Furthermore, we build baselines on MidMed and propose an\ninstruction-guiding medical dialogue generation framework, termed InsMed, to\naddress this task. Experimental results show the effectiveness of InsMed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaoming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zeming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_H/0/1/0/all/0/1\">Haitao Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_K/0/1/0/all/0/1\">Kui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaofan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems. (arXiv:2306.04357v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.04357","description":"<p>Dialogue response selection aims to select an appropriate response from\nseveral candidates based on a given user and system utterance history. Recent\nstudies have been improving the accuracy of dialogue response selection through\npost-training, mostly relying on naive masked language modeling methods.\nHowever, the recently developed generative methods have shown promising text\nrepresentation capabilities in IR community, which could potentially lead to\nbetter dialogue semantics modeling. Thus, in this paper, we propose Dial-MAE\n(Dialogue Contextual Masking Auto-encoder), a straightforward yet effective\npost-training technique tailored for dialogue response selection. Dial-MAE uses\nan asymmetric encoder-decoder architecture that learns to better compress the\nsemantics of the dialogue into dialogue-dense vectors. The process of Dial-MAE\ninvolves a deep encoder creating a dialogue embedding with the masked dialogue\ncontext, followed by a shallow decoder that uses this embedding along with the\nhighly masked response to restore the original response. Our experiments have\ndemonstrated that Dial-MAE is highly effective, achieving state-of-the-art\nperformance on two commonly evaluated benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhenpeng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1\">Guangyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songlin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zambezi Voice: A Multilingual Speech Corpus for Zambian Languages. (arXiv:2306.04428v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.04428","description":"<p>This work introduces Zambezi Voice, an open-source multilingual speech\nresource for Zambian languages. It contains two collections of datasets:\nunlabelled audio recordings of radio news and talk shows programs (160 hours)\nand labelled data (over 80 hours) consisting of read speech recorded from text\nsourced from publicly available literature books. The dataset is created for\nspeech recognition but can be extended to multilingual speech processing\nresearch for both supervised and unsupervised learning approaches. To our\nknowledge, this is the first multilingual speech dataset created for Zambian\nlanguages. We exploit pretraining and cross-lingual transfer learning by\nfinetuning the Wav2Vec2.0 large-scale multilingual pre-trained model to build\nend-to-end (E2E) speech recognition models for our baseline models. The dataset\nis released publicly under a Creative Commons BY-NC-ND 4.0 license and can be\naccessed via https://github.com/unza-speech-lab/zambezi-voice .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sikasote_C/0/1/0/all/0/1\">Claytone Sikasote</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siaminwe_K/0/1/0/all/0/1\">Kalinda Siaminwe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mwape_S/0/1/0/all/0/1\">Stanly Mwape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zulu_B/0/1/0/all/0/1\">Bangiwe Zulu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phiri_M/0/1/0/all/0/1\">Mofya Phiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phiri_M/0/1/0/all/0/1\">Martin Phiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zulu_D/0/1/0/all/0/1\">David Zulu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyirenda_M/0/1/0/all/0/1\">Mayumbo Nyirenda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models. (arXiv:2306.04757v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.04757","description":"<p>Instruction-tuned large language models have revolutionized natural language\nprocessing and have shown great potential in applications such as\nconversational agents. These models, such as GPT-4, can not only master\nlanguage but also solve complex tasks in areas like mathematics, coding,\nmedicine, and law. Despite their impressive capabilities, there is still a lack\nof comprehensive understanding regarding their full potential, primarily due to\nthe black-box nature of many models and the absence of holistic evaluation\nstudies. To address these challenges, we present INSTRUCTEVAL, a more\ncomprehensive evaluation suite designed specifically for instruction-tuned\nlarge language models. Unlike previous works, our evaluation involves a\nrigorous assessment of models based on problem-solving, writing ability, and\nalignment to human values. We take a holistic approach to analyze various\nfactors affecting model performance, including the pretraining foundation,\ninstruction-tuning data, and training methods. Our findings reveal that the\nquality of instruction data is the most crucial factor in scaling model\nperformance. While open-source models demonstrate impressive writing abilities,\nthere is substantial room for improvement in problem-solving and alignment. We\nare encouraged by the rapid development of models by the open-source community,\nbut we also highlight the need for rigorous evaluation to support claims made\nabout these models. Through INSTRUCTEVAL, we aim to foster a deeper\nunderstanding of instruction-tuned models and advancements in their\ncapabilities. INSTRUCTEVAL is publicly available at\nhttps://github.com/declare-lab/instruct-eval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chia_Y/0/1/0/all/0/1\">Yew Ken Chia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1\">Pengfei Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KIT's Multilingual Speech Translation System for IWSLT 2023. (arXiv:2306.05320v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.05320","description":"<p>Many existing speech translation benchmarks focus on native-English speech in\nhigh-quality recording conditions, which often do not match the conditions in\nreal-life use-cases. In this paper, we describe our speech translation system\nfor the multilingual track of IWSLT 2023, which evaluates translation quality\non scientific conference talks. The test condition features accented input\nspeech and terminology-dense contents. The task requires translation into 10\nlanguages of varying amounts of resources. In absence of training data from the\ntarget domain, we use a retrieval-based approach (kNN-MT) for effective\nadaptation (+0.8 BLEU for speech translation). We also use adapters to easily\nintegrate incremental training data from data augmentation, and show that it\nmatches the performance of re-training. We observe that cascaded systems are\nmore easily adaptable towards specific target domains, due to their separate\nmodules. Our cascaded speech system substantially outperforms its end-to-end\ncounterpart on scientific talk translation, although their performance remains\nsimilar on TED talks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Danni Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thai Binh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koneru_S/0/1/0/all/0/1\">Sai Koneru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ugan_E/0/1/0/all/0/1\">Enes Yavuz Ugan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1\">Ngoc-Quan Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan-Nam Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1\">Tu Anh Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullov_C/0/1/0/all/0/1\">Carlos Mullov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alexander Waibel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1\">Jan Niehues</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models. (arXiv:2306.05659v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.05659","description":"<p>Prompt-based learning has been proved to be an effective way in pre-trained\nlanguage models (PLMs), especially in low-resource scenarios like few-shot\nsettings. However, the trustworthiness of PLMs is of paramount significance and\npotential vulnerabilities have been shown in prompt-based templates that could\nmislead the predictions of language models, causing serious security concerns.\nIn this paper, we will shed light on some vulnerabilities of PLMs, by proposing\na prompt-based adversarial attack on manual templates in black box scenarios.\nFirst of all, we design character-level and word-level heuristic approaches to\nbreak manual templates separately. Then we present a greedy algorithm for the\nattack based on the above heuristic destructive approaches. Finally, we\nevaluate our approach with the classification tasks on three variants of BERT\nseries models and eight datasets. And comprehensive experimental results\njustify the effectiveness of our approach in terms of attack success rate and\nattack speed. Further experimental studies indicate that our proposed method\nalso displays good capabilities in scenarios with varying shot counts, template\nlengths and query counts, exhibiting good generalizability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zihao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenbin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongjian Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation. (arXiv:2306.05783v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.05783","description":"<p>New Natural Langauge Process~(NLP) benchmarks are urgently needed to align\nwith the rapid development of large language models (LLMs). We present Xiezhi,\nthe most comprehensive evaluation suite designed to assess holistic domain\nknowledge. Xiezhi comprises multiple-choice questions across 516 diverse\ndisciplines ranging from 13 different subjects with 249,587 questions and\naccompanied by Xiezhi-Specialty and Xiezhi-Interdiscipline, both with 15k\nquestions. We conduct evaluation of the 47 cutting-edge LLMs on Xiezhi. Results\nindicate that LLMs exceed average performance of humans in science,\nengineering, agronomy, medicine, and art, but fall short in economics,\njurisprudence, pedagogy, literature, history, and management. We anticipate\nXiezhi will help analyze important strengths and shortcomings of LLMs, and the\nbenchmark is released in~\\url{https://github.com/MikeGu721/XiezhiBenchmark}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zhouhong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoxuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Haoning Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianchen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Sihang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhuozhi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zihan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qianyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Rui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shusen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Weiguo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Hongwei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mind2Web: Towards a Generalist Agent for the Web. (arXiv:2306.06070v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.06070","description":"<p>We introduce Mind2Web, the first dataset for developing and evaluating\ngeneralist agents for the web that can follow language instructions to complete\ncomplex tasks on any website. Existing datasets for web agents either use\nsimulated websites or only cover a limited set of websites and tasks, thus not\nsuitable for generalist web agents. With over 2,000 open-ended tasks collected\nfrom 137 websites spanning 31 domains and crowdsourced action sequences for the\ntasks, Mind2Web provides three necessary ingredients for building generalist\nweb agents: 1) diverse domains, websites, and tasks, 2) use of real-world\nwebsites instead of simulated and simplified ones, and 3) a broad spectrum of\nuser interaction patterns. Based on Mind2Web, we conduct an initial exploration\nof using large language models (LLMs) for building generalist web agents. While\nthe raw HTML of real-world websites are often too large to be fed to LLMs, we\nshow that first filtering it with a small LM significantly improves the\neffectiveness and efficiency of LLMs. Our solution demonstrates a decent level\nof performance, even on websites or entire domains the model has never seen\nbefore, but there is still a substantial room to improve towards truly\ngeneralizable agents. We open-source our dataset, model implementation, and\ntrained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further\nresearch on building a generalist agent for the web.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Boyuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shijie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevens_S/0/1/0/all/0/1\">Samuel Stevens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boshi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Devil is in the Details: On the Pitfalls of Event Extraction Evaluation. (arXiv:2306.06918v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.06918","description":"<p>Event extraction (EE) is a crucial task aiming at extracting events from\ntexts, which includes two subtasks: event detection (ED) and event argument\nextraction (EAE). In this paper, we check the reliability of EE evaluations and\nidentify three major pitfalls: (1) The data preprocessing discrepancy makes the\nevaluation results on the same dataset not directly comparable, but the data\npreprocessing details are not widely noted and specified in papers. (2) The\noutput space discrepancy of different model paradigms makes different-paradigm\nEE models lack grounds for comparison and also leads to unclear mapping issues\nbetween predictions and annotations. (3) The absence of pipeline evaluation of\nmany EAE-only works makes them hard to be directly compared with EE works and\nmay not well reflect the model performance in real-world pipeline scenarios. We\ndemonstrate the significant influence of these pitfalls through comprehensive\nmeta-analyses of recent papers and empirical experiments. To avoid these\npitfalls, we suggest a series of remedies, including specifying data\npreprocessing, standardizing outputs, and providing pipeline evaluation\nresults. To help implement these remedies, we develop a consistent evaluation\nframework OMNIEVENT, which can be obtained from\nhttps://github.com/THU-KEG/OmniEvent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_F/0/1/0/all/0/1\">Feng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_K/0/1/0/all/0/1\">Kaisheng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Weixing Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models. (arXiv:2306.06815v1 [cs.CR] CROSS LISTED)","link":"http://arxiv.org/abs/2306.06815","description":"<p>Prompt learning has been proven to be highly effective in improving\npre-trained language model (PLM) adaptability, surpassing conventional\nfine-tuning paradigms, and showing exceptional promise in an ever-growing\nlandscape of applications and APIs tailored for few-shot learning scenarios.\nDespite the growing prominence of prompt learning-based APIs, their security\nconcerns remain underexplored. In this paper, we undertake a pioneering study\non the Trojan susceptibility of prompt-learning PLM APIs. We identified several\nkey challenges, including discrete-prompt, few-shot, and black-box settings,\nwhich limit the applicability of existing backdoor attacks. To address these\nchallenges, we propose TrojPrompt, an automatic and black-box framework to\neffectively generate universal and stealthy triggers and insert Trojans into\nhard prompts. Specifically, we propose a universal API-driven trigger discovery\nalgorithm for generating universal triggers for various inputs by querying\nvictim PLM APIs using few-shot data samples. Furthermore, we introduce a novel\nprogressive trojan poisoning algorithm designed to generate poisoned prompts\nthat retain efficacy and transferability across a diverse range of models. Our\nexperiments and results demonstrate TrojPrompt's capacity to effectively insert\nTrojans into text prompts in real-world black-box PLM APIs, while maintaining\nexceptional performance on clean test sets and significantly outperforming\nbaseline models. Our work sheds light on the potential security risks in\ncurrent models and offers a potential defensive approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jiaqi Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yepeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Mengxin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_T/0/1/0/all/0/1\">Ting Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yilin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boloni_L/0/1/0/all/0/1\">Ladislau Boloni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Q/0/1/0/all/0/1\">Qian Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-06-15T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}