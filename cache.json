{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-12-07T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Cross-Domain Few-Shot Relation Extraction via Representation Learning and Domain Adaptation. (arXiv:2212.02560v1 [cs.CL])","link":"http://arxiv.org/abs/2212.02560","description":"<p>Cross-domain few-shot relation extraction poses a great challenge for the\nexisting few-shot learning methods and domain adaptation methods when the\nsource domain and target domain have large discrepancies. This paper proposes a\nmethod by combining the idea of few-shot learning and domain adaptation to deal\nwith this problem. In the proposed method, an encoder, learned by optimizing a\nrepresentation loss and an adversarial loss, is used to extract the relation of\nsentences in the source and target domain. The representation loss, including a\ncross-entropy loss and a contrastive loss, makes the encoder extract the\nrelation of the source domain and keep the geometric structure of the classes\nin the source domain. And the adversarial loss is used to merge the source\ndomain and target domain. The experimental results on the benchmark FewRel\ndataset demonstrate that the proposed method can outperform some\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhongju Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenkun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Genghui Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"INCLUSIFY: A benchmark and a model for gender-inclusive German. (arXiv:2212.02564v1 [cs.CL])","link":"http://arxiv.org/abs/2212.02564","description":"<p>Gender-inclusive language is important for achieving gender equality in\nlanguages with gender inflections, such as German. While stirring some\ncontroversy, it is increasingly adopted by companies and political\ninstitutions. A handful of tools have been developed to help people use\ngender-inclusive language by identifying instances of the generic masculine and\nproviding suggestions for more inclusive reformulations. In this report, we\ndefine the underlying tasks in terms of natural language processing, and\npresent a dataset and measures for benchmarking them. We also present a model\nthat implements these tasks, by combining an inclusive language database with\nan elaborate sequence of processing steps via standard pre-trained models. Our\nmodel achieves a recall of 0.89 and a precision of 0.82 in our benchmark for\nidentifying exclusive language; and one of its top five suggestions is chosen\nin real-world texts in 44% of cases. We sketch how the area could be further\nadvanced by training end-to-end models and using large language models; and we\nurge the community to include more gender-inclusive texts in their training\ndata in order to not present an obstacle to the adoption of gender-inclusive\nlanguage. Through these efforts, we hope to contribute to restoring justice in\nlanguage and, to a small extent, in reality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pomerenke_D/0/1/0/all/0/1\">David Pomerenke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning a Subtle Parsing Distinction Using a Probabilistic Decision Tree: the Case of Postnominal \"that\" in Noun Complement Clauses vs. Relative Clauses. (arXiv:2212.02591v1 [cs.CL])","link":"http://arxiv.org/abs/2212.02591","description":"<p>In this paper we investigated two different methods to parse relative and\nnoun complement clauses in English and resorted to distinct tags for their\ncorresponding that as a relative pronoun and as a complementizer. We used an\nalgorithm to relabel a corpus parsed with the GUM Treebank using Universal\nDependency. Our second experiment consisted in using TreeTagger, a\nProbabilistic Decision Tree, to learn the distinction between the two\ncomplement and relative uses of postnominal \"that\". We investigated the effect\nof the training set size on TreeTagger accuracy and how representative the GUM\nTreebank files are for the two structures under scrutiny. We discussed some of\nthe linguistic and structural tenets of the learnability of this distinction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tighidet_Z/0/1/0/all/0/1\">Zineddine Tighidet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballier_N/0/1/0/all/0/1\">Nicolas Ballier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unifying Vision, Text, and Layout for Universal Document Processing. (arXiv:2212.02623v1 [cs.CV])","link":"http://arxiv.org/abs/2212.02623","description":"<p>We propose Universal Document Processing (UDOP), a foundation Document AI\nmodel which unifies text, image, and layout modalities together with varied\ntask formats, including document understanding and generation. UDOP leverages\nthe spatial correlation between textual content and document image to model\nimage, text, and layout modalities with one uniform representation. With a\nnovel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain\ndownstream tasks into a prompt-based sequence generation scheme. UDOP is\npretrained on both large-scale unlabeled document corpora using innovative\nself-supervised objectives and diverse labeled data. UDOP also learns to\ngenerate document images from text and layout modalities via masked image\nreconstruction. To the best of our knowledge, this is the first time in the\nfield of document AI that one model simultaneously achieves high-quality neural\ndocument editing and content customization. Our method sets the\nstate-of-the-art on 9 Document AI tasks, e.g., document understanding and QA,\nacross diverse data domains like finance reports, academic papers, and\nwebsites. UDOP ranks first on the leaderboard of the Document Understanding\nBenchmark (DUE).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zineng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cha Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"POQue: Asking Participant-specific Outcome Questions for a Deeper Understanding of Complex Events. (arXiv:2212.02629v1 [cs.CL])","link":"http://arxiv.org/abs/2212.02629","description":"<p>Knowledge about outcomes is critical for complex event understanding but is\nhard to acquire. We show that by pre-identifying a participant in a complex\nevent, crowd workers are able to (1) infer the collective impact of salient\nevents that make up the situation, (2) annotate the volitional engagement of\nparticipants in causing the situation, and (3) ground the outcome of the\nsituation in state changes of the participants. By creating a multi-step\ninterface and a careful quality control strategy, we collect a high quality\nannotated dataset of 8K short newswire narratives and ROCStories with high\ninter-annotator agreement (0.74-0.96 weighted Fleiss Kappa). Our dataset, POQue\n(Participant Outcome Questions), enables the exploration and development of\nmodels that address multiple aspects of semantic understanding. Experimentally,\nwe show that current language models lag behind human performance in subtle\nways through our task formulations that target abstract and specific\ncomprehension of a complex event, its outcome, and a participant's influence\nover the event culmination.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vallurupalli_S/0/1/0/all/0/1\">Sai Vallurupalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sayontan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erk_K/0/1/0/all/0/1\">Katrin Erk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraro_F/0/1/0/all/0/1\">Francis Ferraro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training. (arXiv:2212.02691v1 [cs.CL])","link":"http://arxiv.org/abs/2212.02691","description":"<p>Transformers are widely used in NLP tasks. However, current approaches to\nleveraging transformers to understand language expose one weak spot: Number\nunderstanding. In some scenarios, numbers frequently occur, especially in\nsemi-structured data like tables. But current approaches to rich-number tasks\nwith transformer-based language models abandon or lose some of the numeracy\ninformation - e.g., breaking numbers into sub-word tokens - which leads to many\nnumber-related errors. In this paper, we propose the LUNA framework which\nimproves the numerical reasoning and calculation capabilities of\ntransformer-based language models. With the number plugin of NumTok and NumBed,\nLUNA represents each number as a whole to model input. With number\npre-training, including regression loss and model distillation, LUNA bridges\nthe gap between number and vocabulary embeddings. To the best of our knowledge,\nthis is the first work that explicitly injects numeracy capability into\nlanguage models using Number Plugins. Besides evaluating toy models on toy\ntasks, we evaluate LUNA on three large-scale transformer models (RoBERTa, BERT,\nTabBERT) over three different downstream tasks (TATQA, TabFact, CrediTrans),\nand observe the performances of language models are constantly improved by\nLUNA. The augmented models also improve the official baseline of TAT-QA (EM:\n50.15 -&gt; 59.58) and achieve SOTA performance on CrediTrans (F1 = 86.17).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Hongwei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jialiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mengyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yijia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Beam Search for Hallucination Mitigation in Abstractive Summarization. (arXiv:2212.02712v1 [cs.CL])","link":"http://arxiv.org/abs/2212.02712","description":"<p>Advancement in large pretrained language models has significantly improved\ntheir performance for conditional language generation tasks including\nsummarization albeit with hallucinations. To reduce hallucinations,\nconventional methods proposed improving beam search or using a fact checker as\na postprocessing step. In this paper, we investigate the use of the Natural\nLanguage Inference (NLI) entailment metric to detect and prevent hallucinations\nin summary generation. We propose an NLI-assisted beam re-ranking mechanism by\ncomputing entailment probability scores between the input context and\nsummarization model-generated beams during saliency-enhanced greedy decoding.\nMoreover, a diversity metric is introduced to compare its effectiveness against\nvanilla beam search. Our proposed algorithm significantly outperforms vanilla\nbeam decoding on XSum and CNN/DM datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1\">Arvind Krishna Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Visser_E/0/1/0/all/0/1\">Erik Visser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sources of Noise in Dialogue and How to Deal with Them. (arXiv:2212.02745v1 [cs.CL])","link":"http://arxiv.org/abs/2212.02745","description":"<p>Training dialogue systems often entails dealing with noisy training examples\nand unexpected user inputs. Despite their prevalence, there currently lacks an\naccurate survey of dialogue noise, nor is there a clear sense of the impact of\neach noise type on task performance. This paper addresses this gap by first\nconstructing a taxonomy of noise encountered by dialogue systems. In addition,\nwe run a series of experiments to show how different models behave when\nsubjected to varying levels of noise and types of noise. Our results reveal\nthat models are quite robust to label errors commonly tackled by existing\ndenoising algorithms, but that performance suffers from dialogue-specific\nnoise. Driven by these observations, we design a data cleaning algorithm\nspecialized for conversational settings and apply it as a proof-of-concept for\ntargeted dialogue denoising.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Derek Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Identification of Eviction Status from Electronic Health Record Notes. (arXiv:2212.02762v1 [cs.CL])","link":"http://arxiv.org/abs/2212.02762","description":"<p>Objective: Evictions are involved in a cascade of negative events that can\nlead to unemployment, homelessness, long-term poverty, and mental health\nproblems. In this study, we developed a natural language processing system to\nautomatically detect eviction incidences and their attributes from electronic\nhealth record (EHR) notes.\n</p>\n<p>Materials and Methods: We annotated eviction status in 5000 EHR notes from\nthe Veterans Health Administration. We developed a novel model, called\nKnowledge Injection based on Ripple Effects of Social and Behavioral\nDeterminants of Health (KIRESH), that has shown to substantially outperform\nother state-of-the-art models such as fine-tuning pre-trained language models\nlike BioBERT and Bio_ClinicalBERT. Moreover, we designed a prompt to further\nimprove the model performance by using the intrinsic connection between the two\nsub-tasks of eviction presence and period prediction. Finally, we used the\nTemperature Scaling-based Calibration on our KIRESH-Prompt method to avoid\nover-confidence issues arising from the imbalance dataset.\n</p>\n<p>Results: KIRESH-Prompt achieved a Macro-F1 of 0.6273 (presence) and 0.7115\n(period), which was significantly higher than 0.5382 (presence) and 0.67167\n(period) for just fine-tuning Bio_ClinicalBERT model.\n</p>\n<p>Conclusion and Future Work: KIRESH-Prompt has substantially improved eviction\nstatus classification. In future work, we will evaluate the generalizability of\nthe model framework to other applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zonghai Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_J/0/1/0/all/0/1\">Jack Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weisong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_D/0/1/0/all/0/1\">David A. Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Druhl_E/0/1/0/all/0/1\">Emily Druhl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reisman_J/0/1/0/all/0/1\">Joel I Reisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Life-long Learning for Multilingual Neural Machine Translation with Knowledge Distillation. (arXiv:2212.02800v1 [cs.CL])","link":"http://arxiv.org/abs/2212.02800","description":"<p>A common scenario of Multilingual Neural Machine Translation (MNMT) is that\neach translation task arrives in a sequential manner, and the training data of\nprevious tasks is unavailable. In this scenario, the current methods suffer\nheavily from catastrophic forgetting (CF). To alleviate the CF, we investigate\nknowledge distillation based life-long learning methods. Specifically, in\none-tomany scenario, we propose a multilingual distillation method to make the\nnew model (student) jointly learn multilingual output from old model (teacher)\nand new task. In many-to one scenario, we find that direct distillation faces\nthe extreme partial distillation problem, and we propose two different methods\nto address it: pseudo input distillation and reverse teacher distillation. The\nexperimental results on twelve translation tasks show that the proposed methods\ncan better consolidate the previous knowledge and sharply alleviate the CF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Junnan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1\">Lu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_F/0/1/0/all/0/1\">Feifei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_C/0/1/0/all/0/1\">Chengqing Zong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiSTRICT: Dialogue State Tracking with Retriever Driven In-Context Tuning. (arXiv:2212.02851v1 [cs.CL])","link":"http://arxiv.org/abs/2212.02851","description":"<p>Dialogue State Tracking (DST), a key component of task-oriented conversation\nsystems, represents user intentions by determining the values of pre-defined\nslots in an ongoing dialogue. Existing approaches use hand-crafted templates\nand additional slot information to fine-tune and prompt large pre-trained\nlanguage models and elicit slot values from the dialogue context. Significant\nmanual effort and domain knowledge is required to design effective prompts,\nlimiting the generalizability of these approaches to new domains and tasks. In\nthis work, we propose DiSTRICT, a generalizable in-context tuning approach for\nDST that retrieves highly relevant training examples for a given dialogue to\nfine-tune the model without any hand-crafted templates. Experiments with the\nMultiWOZ benchmark datasets show that DiSTRICT outperforms existing approaches\nin various zero-shot and few-shot settings using a much smaller model, thereby\nproviding an important advantage for real-world deployments that often have\nlimited resource availability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venkateswaran_P/0/1/0/all/0/1\">Praveen Venkateswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duesterwald_E/0/1/0/all/0/1\">Evelyn Duesterwald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isahagian_V/0/1/0/all/0/1\">Vatche Isahagian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Template-based Recruitment Email Generation For Job Recommendation. (arXiv:2212.02885v1 [cs.CL])","link":"http://arxiv.org/abs/2212.02885","description":"<p>Text generation has long been a popular research topic in NLP. However, the\ntask of generating recruitment emails from recruiters to candidates in the job\nrecommendation scenario has received little attention by the research\ncommunity. This work aims at defining the topic of automatic email generation\nfor job recommendation, identifying the challenges, and providing a baseline\ntemplate-based solution for Danish jobs. Evaluation by human experts shows that\nour method is effective. We wrap up by discussing the future research\ndirections for better solving this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiuchi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lioma_C/0/1/0/all/0/1\">Christina Lioma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion Conditioned Creative Dialog Generation. (arXiv:2212.02907v1 [cs.CL])","link":"http://arxiv.org/abs/2212.02907","description":"<p>We present a DialGPT based model for generating creative dialog responses\nthat are conditioned based on one of the following emotions: anger, disgust,\nfear, happiness, pain, sadness and surprise. Our model is capable of producing\na contextually apt response given an input sentence and a desired emotion\nlabel. Our model is capable of expressing the desired emotion with an accuracy\nof 0.6. The best performing emotions are neutral, fear and disgust. When\nmeasuring the strength of the expressed emotion, we find that anger, fear and\ndisgust are expressed in the most strong fashion by the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alnajjar_K/0/1/0/all/0/1\">Khalid Alnajjar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamalainen_M/0/1/0/all/0/1\">Mika H&#xe4;m&#xe4;l&#xe4;inen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards human-compatible autonomous car: A study of non-verbal Turing test in automated driving with affective transition modelling. (arXiv:2212.02908v1 [cs.HC])","link":"http://arxiv.org/abs/2212.02908","description":"<p>Autonomous cars are indispensable when humans go further down the hands-free\nroute. Although existing literature highlights that the acceptance of the\nautonomous car will increase if it drives in a human-like manner, sparse\nresearch offers the naturalistic experience from a passenger's seat perspective\nto examine the human likeness of current autonomous cars. The present study\ntested whether the AI driver could create a human-like ride experience for\npassengers based on 69 participants' feedback in a real-road scenario. We\ndesigned a ride experience-based version of the non-verbal Turing test for\nautomated driving. Participants rode in autonomous cars (driven by either human\nor AI drivers) as a passenger and judged whether the driver was human or AI.\nThe AI driver failed to pass our test because passengers detected the AI driver\nabove chance. In contrast, when the human driver drove the car, the passengers'\njudgement was around chance. We further investigated how human passengers\nascribe humanness in our test. Based on Lewin's field theory, we advanced a\ncomputational model combining signal detection theory with pre-trained language\nmodels to predict passengers' humanness rating behaviour. We employed affective\ntransition between pre-study baseline emotions and corresponding post-stage\nemotions as the signal strength of our model. Results showed that the\npassengers' ascription of humanness would increase with the greater affective\ntransition. Our study suggested an important role of affective transition in\npassengers' ascription of humanness, which might become a future direction for\nautonomous driving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaoning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qiaoli Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhengming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haiyan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Miner Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_Y/0/1/0/all/0/1\">Yixuan Ku</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modern French Poetry Generation with RoBERTa and GPT-2. (arXiv:2212.02911v1 [cs.CL])","link":"http://arxiv.org/abs/2212.02911","description":"<p>We present a novel neural model for modern poetry generation in French. The\nmodel consists of two pretrained neural models that are fine-tuned for the poem\ngeneration task. The encoder of the model is a RoBERTa based one while the\ndecoder is based on GPT-2. This way the model can benefit from the superior\nnatural language understanding performance of RoBERTa and the good natural\nlanguage generation performance of GPT-2. Our evaluation shows that the model\ncan create French poetry successfully. On a 5 point scale, the lowest score of\n3.57 was given by human judges to typicality and emotionality of the output\npoetry while the best score of 3.79 was given to understandability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamalainen_M/0/1/0/all/0/1\">Mika H&#xe4;m&#xe4;l&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alnajjar_K/0/1/0/all/0/1\">Khalid Alnajjar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poibeau_T/0/1/0/all/0/1\">Thierry Poibeau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlled Text Generation using T5 based Encoder-Decoder Soft Prompt Tuning and Analysis of the Utility of Generated Text in AI. (arXiv:2212.02924v1 [cs.CL])","link":"http://arxiv.org/abs/2212.02924","description":"<p>Controlled text generation is a very important task in the arena of natural\nlanguage processing due to its promising applications. In order to achieve this\ntask we mainly introduce the novel soft prompt tuning method of using soft\nprompts at both encoder and decoder levels together in a T5 model and\ninvestigate the performance as the behaviour of an additional soft prompt\nrelated to the decoder of a T5 model in controlled text generation remained\nunexplored. Then we also investigate the feasibility of steering the output of\nthis extended soft prompted T5 model at decoder level and finally analyse the\nutility of generated text to be used in AI related tasks such as training AI\nmodels with an interpretability analysis of the classifier trained with\nsynthetic text, as there is a lack of proper analysis of methodologies in\ngenerating properly labelled data to be utilized in AI tasks. Through the\nperformed in-depth intrinsic and extrinsic evaluations of this generation model\nalong with the artificially generated data, we found that this model produced\nbetter results compared to the T5 model with a single soft prompt at encoder\nlevel and the sentiment classifier trained using this artificially generated\ndata can produce comparable classification results to the results of a\nclassifier trained with real labelled data and also the classifier decision is\ninterpretable with respect to the input text content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Senadeera_D/0/1/0/all/0/1\">Damith Chamalke Senadeera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ive_J/0/1/0/all/0/1\">Julia Ive</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CySecBERT: A Domain-Adapted Language Model for the Cybersecurity Domain. (arXiv:2212.02974v1 [cs.CR])","link":"http://arxiv.org/abs/2212.02974","description":"<p>The field of cybersecurity is evolving fast. Experts need to be informed\nabout past, current and - in the best case - upcoming threats, because attacks\nare becoming more advanced, targets bigger and systems more complex. As this\ncannot be addressed manually, cybersecurity experts need to rely on machine\nlearning techniques. In the texutual domain, pre-trained language models like\nBERT have shown to be helpful, by providing a good baseline for further\nfine-tuning. However, due to the domain-knowledge and many technical terms in\ncybersecurity general language models might miss the gist of textual\ninformation, hence doing more harm than good. For this reason, we create a\nhigh-quality dataset and present a language model specifically tailored to the\ncybersecurity domain, which can serve as a basic building block for\ncybersecurity systems that deal with natural language. The model is compared\nwith other models based on 15 different domain-dependent extrinsic and\nintrinsic tasks as well as general tasks from the SuperGLUE benchmark. On the\none hand, the results of the intrinsic tasks show that our model improves the\ninternal representation space of words compared to the other models. On the\nother hand, the extrinsic, domain-dependent tasks, consisting of sequence\ntagging and classification, show that the model is best in specific application\nscenarios, in contrast to the others. Furthermore, we show that our approach\nagainst catastrophic forgetting works, as the model is able to retrieve the\npreviously trained domain-independent knowledge. The used dataset and trained\nmodel are made publicly available\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bayer_M/0/1/0/all/0/1\">Markus Bayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehn_P/0/1/0/all/0/1\">Philipp Kuehn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanehsaz_R/0/1/0/all/0/1\">Ramin Shanehsaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reuter_C/0/1/0/all/0/1\">Christian Reuter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Bridged Causal Interaction Network for Causal Emotion Entailment. (arXiv:2212.02995v1 [cs.CL])","link":"http://arxiv.org/abs/2212.02995","description":"<p>Causal Emotion Entailment aims to identify causal utterances that are\nresponsible for the target utterance with a non-neutral emotion in\nconversations. Previous works are limited in thorough understanding of the\nconversational context and accurate reasoning of the emotion cause. To this\nend, we propose Knowledge-Bridged Causal Interaction Network (KBCIN) with\ncommonsense knowledge (CSK) leveraged as three bridges. Specifically, we\nconstruct a conversational graph for each conversation and leverage the\nevent-centered CSK as the semantics-level bridge (S-bridge) to capture the deep\ninter-utterance dependencies in the conversational context via the CSK-Enhanced\nGraph Attention module. Moreover, social-interaction CSK serves as\nemotion-level bridge (E-bridge) and action-level bridge (A-bridge) to connect\ncandidate utterances with the target one, which provides explicit causal clues\nfor the Emotional Interaction module and Actional Interaction module to reason\nthe target emotion. Experimental results show that our model achieves better\nperformance over most baseline models. Our source code is publicly available at\nhttps://github.com/circle-hit/KBCIN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weixiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuojun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SODA: A Natural Language Processing Package to Extract Social Determinants of Health for Cancer Studies. (arXiv:2212.03000v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03000","description":"<p>Objective: We aim to develop an open-source natural language processing (NLP)\npackage, SODA (i.e., SOcial DeterminAnts), with pre-trained transformer models\nto extract social determinants of health (SDoH) for cancer patients, examine\nthe generalizability of SODA to a new disease domain (i.e., opioid use), and\nevaluate the extraction rate of SDoH using cancer populations.\n</p>\n<p>Methods: We identified SDoH categories and attributes and developed an SDoH\ncorpus using clinical notes from a general cancer cohort. We compared four\ntransformer-based NLP models to extract SDoH, examined the generalizability of\nNLP models to a cohort of patients prescribed with opioids, and explored\ncustomization strategies to improve performance. We applied the best NLP model\nto extract 19 categories of SDoH from the breast (n=7,971), lung (n=11,804),\nand colorectal cancer (n=6,240) cohorts.\n</p>\n<p>Results and Conclusion: We developed a corpus of 629 cancer patients notes\nwith annotations of 13,193 SDoH concepts/attributes from 19 categories of SDoH.\nThe Bidirectional Encoder Representations from Transformers (BERT) model\nachieved the best strict/lenient F1 scores of 0.9216 and 0.9441 for SDoH\nconcept extraction, 0.9617 and 0.9626 for linking attributes to SDoH concepts.\nFine-tuning the NLP models using new annotations from opioid use patients\nimproved the strict/lenient F1 scores from 0.8172/0.8502 to 0.8312/0.8679. The\nextraction rates among 19 categories of SDoH varied greatly, where 10 SDoH\ncould be extracted from &gt;70% of cancer patients, but 9 SDoH had a low\nextraction rate (&lt;70% of cancer patients). The SODA package with pre-trained\ntransformer models is publicly available at\nhttps://github.com/uf-hobiinformatics-lab/SDoH_SODA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zehao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_C/0/1/0/all/0/1\">Chong Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adekkanattu_P/0/1/0/all/0/1\">Prakash Adekkanattu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patra_B/0/1/0/all/0/1\">Braja Gopal Patra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_J/0/1/0/all/0/1\">Jyotishman Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1\">Debbie L. Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Ching-Yuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_Ciganic_W/0/1/0/all/0/1\">Wei-Hsuan Lo-Ciganic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+George_T/0/1/0/all/0/1\">Thomas J. George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hogan_W/0/1/0/all/0/1\">William R. Hogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Document-Level Abstractive Summarization. (arXiv:2212.03013v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03013","description":"<p>The task of automatic text summarization produces a concise and fluent text\nsummary while preserving key information and overall meaning. Recent approaches\nto document-level summarization have seen significant improvements in recent\nyears by using models based on the Transformer architecture. However, the\nquadratic memory and time complexities with respect to the sequence length make\nthem very expensive to use, especially with long sequences, as required by\ndocument-level summarization. Our work addresses the problem of document-level\nsummarization by studying how efficient Transformer techniques can be used to\nimprove the automatic summarization of very long texts. In particular, we will\nuse the arXiv dataset, consisting of several scientific papers and the\ncorresponding abstracts, as baselines for this work. Then, we propose a novel\nretrieval-enhanced approach based on the architecture which reduces the cost of\ngenerating a summary of the entire document by processing smaller chunks. The\nresults were below the baselines but suggest a more efficient memory a\nconsumption and truthfulness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raposo_G/0/1/0/all/0/1\">Gon&#xe7;alo Raposo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raposo_A/0/1/0/all/0/1\">Afonso Raposo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmo_A/0/1/0/all/0/1\">Ana Sofia Carmo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style transfer and classification in hebrew news items. (arXiv:2212.03019v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03019","description":"<p>Hebrew is a Morphological rich language, making its modeling harder than\nsimpler language. Recent developments such as Transformers in general and Bert\nin particular opened a path for Hebrew models that reach SOTA results, not\nfalling short from other non-MRL languages. We explore the cutting edge in this\nfield performing style transfer, text generation and classification over news\narticles collected from online archives. Furthermore, the news portals that\nfeed our collective consciousness are an interesting corpus to study, as their\nanalysis and tracing might reveal insights about our society and discourse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weingarten_N/0/1/0/all/0/1\">Nir Weingarten</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZeroKBC: A Comprehensive Benchmark for Zero-Shot Knowledge Base Completion. (arXiv:2212.03091v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03091","description":"<p>Knowledge base completion (KBC) aims to predict the missing links in\nknowledge graphs. Previous KBC tasks and approaches mainly focus on the setting\nwhere all test entities and relations have appeared in the training set.\nHowever, there has been limited research on the zero-shot KBC settings, where\nwe need to deal with unseen entities and relations that emerge in a constantly\ngrowing knowledge base. In this work, we systematically examine different\npossible scenarios of zero-shot KBC and develop a comprehensive benchmark,\nZeroKBC, that covers these scenarios with diverse types of knowledge sources.\nOur systematic analysis reveals several missing yet important zero-shot KBC\nsettings. Experimental results show that canonical and state-of-the-art KBC\nsystems cannot achieve satisfactory performance on this challenging benchmark.\nBy analyzing the strength and weaknesses of these systems on solving ZeroKBC,\nwe further present several important observations and promising future\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wenlin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiaoman Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianshu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-Conditional Diffusion Networks for Image Captioning. (arXiv:2212.03099v1 [cs.CV])","link":"http://arxiv.org/abs/2212.03099","description":"<p>Recent advances on text-to-image generation have witnessed the rise of\ndiffusion models which act as powerful generative models. Nevertheless, it is\nnot trivial to exploit such latent variable models to capture the dependency\namong discrete words and meanwhile pursue complex visual-language alignment in\nimage captioning. In this paper, we break the deeply rooted conventions in\nlearning Transformer-based encoder-decoder, and propose a new diffusion model\nbased paradigm tailored for image captioning, namely Semantic-Conditional\nDiffusion Networks (SCD-Net). Technically, for each input image, we first\nsearch the semantically relevant sentences via cross-modal retrieval model to\nconvey the comprehensive semantic information. The rich semantics are further\nregarded as semantic prior to trigger the learning of Diffusion Transformer,\nwhich produces the output sentence in a diffusion process. In SCD-Net, multiple\nDiffusion Transformer structures are stacked to progressively strengthen the\noutput sentence with better visional-language alignment and linguistical\ncoherence in a cascaded manner. Furthermore, to stabilize the diffusion\nprocess, a new self-critical sequence training strategy is designed to guide\nthe learning of SCD-Net with the knowledge of a standard autoregressive\nTransformer model. Extensive experiments on COCO dataset demonstrate the\npromising potential of using diffusion models in the challenging image\ncaptioning task. Source code is available at\n\\url{https://github.com/YehLi/xmodaler/tree/master/configs/image_caption/scdnet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jianjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yehao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianlin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1\">Hongyang Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Machine Translation with Contrastive Translation Memories. (arXiv:2212.03140v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03140","description":"<p>Retrieval-augmented Neural Machine Translation models have been successful in\nmany translation scenarios. Different from previous works that make use of\nmutually similar but redundant translation memories~(TMs), we propose a new\nretrieval-augmented NMT to model contrastively retrieved translation memories\nthat are holistically similar to the source sentence while individually\ncontrastive to each other providing maximal information gains in three phases.\nFirst, in TM retrieval phase, we adopt a contrastive retrieval algorithm to\navoid redundancy and uninformativeness of similar translation pieces. Second,\nin memory encoding stage, given a set of TMs we propose a novel Hierarchical\nGroup Attention module to gather both local context of each TM and global\ncontext of the whole TM set. Finally, in training phase, a Multi-TM contrastive\nlearning objective is introduced to learn salient feature of each TM with\nrespect to target sentence. Experimental results show that our framework\nobtains improvements over strong baselines on the benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LawngNLI: A Long-Premise Benchmark for In-Domain Generalization from Short to Long Contexts and for Implication-Based Retrieval. (arXiv:2212.03222v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03222","description":"<p>Natural language inference has trended toward studying contexts beyond the\nsentence level. An important application area is law: past cases often do not\nforetell how they apply to new situations and implications must be inferred.\nThis paper introduces LawngNLI, constructed from U.S. legal opinions with\nautomatic labels with high human-validated accuracy. Premises are long and\nmultigranular. Experiments show two use cases. First, LawngNLI can benchmark\nfor in-domain generalization from short to long contexts. It has remained\nunclear if large-scale long-premise NLI datasets actually need to be\nconstructed: near-top performance on long premises could be achievable by\nfine-tuning using short premises. Without multigranularity, benchmarks cannot\ndistinguish lack of fine-tuning on long premises versus domain shift between\nshort and long datasets. In contrast, our long and short premises share the\nsame examples and domain. Models fine-tuned using several past NLI datasets\nand/or our short premises fall short of top performance on our long premises.\nSo for at least certain domains (such as ours), large-scale long-premise\ndatasets are needed. Second, LawngNLI can benchmark for implication-based\nretrieval. Queries are entailed or contradicted by target documents, allowing\nusers to move between arguments and evidence. Leading retrieval models perform\nreasonably zero shot on a LawngNLI-derived retrieval task. We compare different\nsystems for re-ranking, including lexical overlap and cross-encoders fine-tuned\nusing a modified LawngNLI or past NLI datasets. LawngNLI can train and test\nsystems for implication-based case retrieval and argumentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bruno_W/0/1/0/all/0/1\">William Bruno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Switching to Discriminative Image Captioning by Relieving a Bottleneck of Reinforcement Learning. (arXiv:2212.03230v1 [cs.CV])","link":"http://arxiv.org/abs/2212.03230","description":"<p>Discriminativeness is a desirable feature of image captions: captions should\ndescribe the characteristic details of input images. However, recent\nhigh-performing captioning models, which are trained with reinforcement\nlearning (RL), tend to generate overly generic captions despite their high\nperformance in various other criteria. First, we investigate the cause of the\nunexpectedly low discriminativeness and show that RL has a deeply rooted side\neffect of limiting the output words to high-frequency words. The limited\nvocabulary is a severe bottleneck for discriminativeness as it is difficult for\na model to describe the details beyond its vocabulary. Then, based on this\nidentification of the bottleneck, we drastically recast discriminative image\ncaptioning as a much simpler task of encouraging low-frequency word generation.\nHinted by long-tail classification and debiasing methods, we propose methods\nthat easily switch off-the-shelf RL models to discriminativeness-aware models\nwith only a single-epoch fine-tuning on the part of the parameters. Extensive\nexperiments demonstrate that our methods significantly enhance the\ndiscriminativeness of off-the-shelf RL models and even outperform previous\ndiscriminativeness-aware methods with much smaller computational costs.\nDetailed analysis and human evaluation also verify that our methods boost the\ndiscriminativeness without sacrificing the overall quality of captions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Honda_U/0/1/0/all/0/1\">Ukyo Honda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1\">Taro Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsumoto_Y/0/1/0/all/0/1\">Yuji Matsumoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sharpness-Aware Minimization with Dynamic Reweighting. (arXiv:2112.08772v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.08772","description":"<p>Deep neural networks are often overparameterized and may not easily achieve\nmodel generalization. Adversarial training has shown effectiveness in improving\ngeneralization by regularizing the change of loss on top of adversarially\nchosen perturbations. The recently proposed sharpness-aware minimization (SAM)\nalgorithm conducts adversarial weight perturbation, encouraging the model to\nconverge to a flat minima. SAM finds a common adversarial weight perturbation\nper-batch. Although per-instance adversarial weight perturbations are stronger\nadversaries and can potentially lead to better generalization performance,\ntheir computational cost is very high and thus it is impossible to use\nper-instance perturbations efficiently in SAM. In this paper, we tackle this\nefficiency bottleneck and propose sharpness-aware minimization with dynamic\nreweighting (delta-SAM). Our theoretical analysis motivates that it is possible\nto approach the stronger, per-instance adversarial weight perturbations using\nreweighted per-batch weight perturbations. delta-SAM dynamically reweights\nperturbation within each batch according to the theoretically principled\nweighting factors, serving as a good approximation to per-instance\nperturbation. Experiments on various natural language understanding tasks\ndemonstrate the effectiveness of delta-SAM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensemble Transformer for Efficient and Accurate Ranking Tasks: an Application to Question Answering Systems. (arXiv:2201.05767v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05767","description":"<p>Large transformer models can highly improve Answer Sentence Selection (AS2)\ntasks, but their high computational costs prevent their use in many real-world\napplications. In this paper, we explore the following research question: How\ncan we make the AS2 models more accurate without significantly increasing their\nmodel complexity? To address the question, we propose a Multiple Heads Student\narchitecture (named CERBERUS), an efficient neural network designed to distill\nan ensemble of large transformers into a single smaller model. CERBERUS\nconsists of two components: a stack of transformer layers that is used to\nencode inputs, and a set of ranking heads; unlike traditional distillation\ntechnique, each of them is trained by distilling a different large transformer\narchitecture in a way that preserves the diversity of the ensemble members. The\nresulting model captures the knowledge of heterogeneous transformer models by\nusing just a few extra parameters. We show the effectiveness of CERBERUS on\nthree English datasets for AS2; our proposed approach outperforms all\nsingle-model distillations we consider, rivaling the state-of-the-art large AS2\nmodels that have 2.7x more parameters and run 2.5x slower. Code for our model\nis available at https://github.com/amazon-research/wqa-cerberus\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsubara_Y/0/1/0/all/0/1\">Yoshitomo Matsubara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldaini_L/0/1/0/all/0/1\">Luca Soldaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lind_E/0/1/0/all/0/1\">Eric Lind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschitti_A/0/1/0/all/0/1\">Alessandro Moschitti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LISA: Learning Interpretable Skill Abstractions from Language. (arXiv:2203.00054v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.00054","description":"<p>Learning policies that effectively utilize language instructions in complex,\nmulti-task environments is an important problem in sequential decision-making.\nWhile it is possible to condition on the entire language instruction directly,\nsuch an approach could suffer from generalization issues. In our work, we\npropose \\emph{Learning Interpretable Skill Abstractions (LISA)}, a hierarchical\nimitation learning framework that can learn diverse, interpretable primitive\nbehaviors or skills from language-conditioned demonstrations to better\ngeneralize to unseen instructions. LISA uses vector quantization to learn\ndiscrete skill codes that are highly correlated with language instructions and\nthe behavior of the learned policy. In navigation and robotic manipulation\nenvironments, LISA outperforms a strong non-hierarchical Decision Transformer\nbaseline in the low data regime and is able to compose learned skills to solve\ntasks containing unseen long-range instructions. Our method demonstrates a more\nnatural way to condition on language in sequential decision-making problems and\nachieve interpretable and controllable behavior with the learned skills.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_D/0/1/0/all/0/1\">Divyansh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaidyanath_S/0/1/0/all/0/1\">Skanda Vaidyanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kuno Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiaming Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Grammars: Augmenting Transformer Language Models with Syntactic Inductive Biases at Scale. (arXiv:2203.00633v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.00633","description":"<p>We introduce Transformer Grammars (TGs), a novel class of Transformer\nlanguage models that combine (i) the expressive power, scalability, and strong\nperformance of Transformers and (ii) recursive syntactic compositions, which\nhere are implemented through a special attention mask and deterministic\ntransformation of the linearized tree. We find that TGs outperform various\nstrong baselines on sentence-level language modeling perplexity, as well as on\nmultiple syntax-sensitive language modeling evaluation metrics. Additionally,\nwe find that the recursive syntactic composition bottleneck which represents\neach sentence as a single vector harms perplexity on document-level language\nmodeling, providing evidence that a different kind of memory mechanism -- one\nthat is independent of composed syntactic representations -- plays an important\nrole in current successful models of long text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sartran_L/0/1/0/all/0/1\">Laurent Sartran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrett_S/0/1/0/all/0/1\">Samuel Barrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuncoro_A/0/1/0/all/0/1\">Adhiguna Kuncoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanojevic_M/0/1/0/all/0/1\">Milo&#x161; Stanojevi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blunsom_P/0/1/0/all/0/1\">Phil Blunsom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyer_C/0/1/0/all/0/1\">Chris Dyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Language Models without Positional Encodings Still Learn Positional Information. (arXiv:2203.16634v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16634","description":"<p>Causal transformer language models (LMs), such as GPT-3, typically require\nsome form of positional encoding, such as positional embeddings. However, we\nshow that LMs without any explicit positional encoding are still competitive\nwith standard models, and that this phenomenon is robust across different\ndatasets, model sizes, and sequence lengths. Probing experiments reveal that\nsuch models acquire an implicit notion of absolute positions throughout the\nnetwork, effectively compensating for the missing information. We conjecture\nthat causal attention enables the model to infer the number of predecessors\nthat each token can attend to, thereby approximating its absolute position. Our\nfindings indicate that causal LMs might derive positional awareness not only\nfrom the explicit positioning mechanism, but also from the effects of the\ncausal mask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haviv_A/0/1/0/all/0/1\">Adi Haviv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ram_O/0/1/0/all/0/1\">Ori Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Press_O/0/1/0/all/0/1\">Ofir Press</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izsak_P/0/1/0/all/0/1\">Peter Izsak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Budge: a programming language and a theorem prover. (arXiv:2205.07979v6 [cs.PL] UPDATED)","link":"http://arxiv.org/abs/2205.07979","description":"<p>We present a simple programming language based on G\\\"odel numbering and prime\nfactorization, enhanced with explicit, scoped loops, allowing for easy program\ncomposition. Further, we will present a theorem prover that allows expressing\nand working with formal systems. The theorem prover is simple as it relies\nmerely on a substitution rule and set equality to derive theorems. Finally, we\nwill represent the programming language in the theorem prover. We will show the\nsyntax and semantics of both, and then provide a few example programs and their\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sitnikovski_B/0/1/0/all/0/1\">Boro Sitnikovski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeAttack: Code-based Adversarial Attacks for Pre-Trained Programming Language Models. (arXiv:2206.00052v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.00052","description":"<p>Pre-trained programming language (PL) models (such as CodeT5, CodeBERT,\nGraphCodeBERT, etc.,) have the potential to automate software engineering tasks\ninvolving code understanding and code generation. However, these models operate\nin the natural channel of code, i.e., they are primarily concerned with the\nhuman understanding of the code. They are not robust to changes in the input\nand thus, are potentially susceptible to adversarial attacks in the natural\nchannel. We propose, CodeAttack, a simple yet effective black-box attack model\nthat uses code structure to generate effective, efficient, and imperceptible\nadversarial code samples and demonstrates the vulnerabilities of the\nstate-of-the-art PL models to code-specific adversarial attacks. We evaluate\nthe transferability of CodeAttack on several code-code (translation and repair)\nand code-NL (summarization) tasks across different programming languages.\nCodeAttack outperforms state-of-the-art adversarial NLP attack models to\nachieve the best overall drop in performance while being more efficient,\nimperceptible, consistent, and fluent. The code can be found at\nhttps://github.com/reddy-lab-code-research/CodeAttack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1\">Akshita Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1\">Chandan K. Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jeopardy: An Invertible Functional Programming Language. (arXiv:2209.02422v2 [cs.PL] UPDATED)","link":"http://arxiv.org/abs/2209.02422","description":"<p>Algorithms are ways of mapping problems to solutions. An algorithm is\ninvertible precisely when this mapping is injective, such that the initial\nproblem can be uniquely inferred from its solution.\n</p>\n<p>While invertible algorithms can be described in general-purpose languages, no\nguarantees are generally made by such languages as regards invertibility, so\nensuring invertibility requires additional (and often non-trivial) proof. On\nthe other hand, while reversible programming languages guarantee that their\nprograms are invertible by restricting the permissible operations to those\nwhich are locally invertible, writing programs in the reversible style can be\ncumbersome, and may differ significantly from conventional implementations even\nwhen the implemented algorithm is, in fact, invertible.\n</p>\n<p>In this paper we introduce Jeopardy, a functional programming language that\nguarantees program invertibility without imposing local reversibility. In\nparticular, Jeopardy allows the limited use of uninvertible -- and even\nnondeterministic! -- operations, provided that they are used in a way that can\nbe\n</p>\n<p>statically determined to be invertible. To this end, we outline an\n\\emph{implicitly available arguments analysis} and three further approaches\nthat can give a partial static guarantee to the (generally difficult) problem\nof guaranteeing invertibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kristensen_J/0/1/0/all/0/1\">Joachim Tilsted Kristensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaarsgaard_R/0/1/0/all/0/1\">Robin Kaarsgaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomsen_M/0/1/0/all/0/1\">Michael Kirkedal Thomsen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Tailed Averaging: Anytime Adaptive Once-in-a-while Optimal Iterate Averaging for Stochastic Optimization. (arXiv:2209.12581v2 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2209.12581","description":"<p>Tail averaging improves on Polyak averaging's non-asymptotic behaviour by\nexcluding a number of leading iterates of stochastic optimization from its\ncalculations. In practice, with a finite number of optimization steps and a\nlearning rate that cannot be annealed to zero, tail averaging can get much\ncloser to a local minimum point of the training loss than either the individual\niterates or the Polyak average. However, the number of leading iterates to\nignore is an important hyperparameter, and starting averaging too early or too\nlate leads to inefficient use of resources or suboptimal solutions. Our work\nfocusses on improving generalization, which makes setting this hyperparameter\neven more difficult, especially in the presence of other hyperparameters and\noverfitting. Furthermore, before averaging starts, the loss is only weakly\ninformative of the final performance, which makes early stopping unreliable. To\nalleviate these problems, we propose an anytime variant of tail averaging\nintended for improving generalization not pure optimization, that has no\nhyperparameters and approximates the optimal tail at all optimization steps.\nOur algorithm is based on two running averages with adaptive lengths bounded in\nterms of the optimal tail length, one of which achieves approximate optimality\nwith some regularity. Requiring only the additional storage for two sets of\nweights and periodic evaluation of the loss, the proposed two-tailed averaging\nalgorithm is a practical and widely applicable method for improving\ngeneralization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Melis_G/0/1/0/all/0/1\">G&#xe1;bor Melis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Reason With Relational Abstractions. (arXiv:2210.02615v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.02615","description":"<p>Large language models have recently shown promising progress in mathematical\nreasoning when fine-tuned with human-generated sequences walking through a\nsequence of solution steps. However, the solution sequences are not formally\nstructured and the resulting model-generated sequences may not reflect the kind\nof systematic reasoning we might expect an expert human to produce. In this\npaper, we study how to build stronger reasoning capability in language models\nusing the idea of relational abstractions. We introduce new types of sequences\nthat more explicitly provide an abstract characterization of the transitions\nthrough intermediate solution steps to the goal state. We find that models that\nare supplied with such sequences as prompts can solve tasks with a\nsignificantly higher accuracy, and models that are trained to produce such\nsequences solve problems better than those that are trained with previously\nused human-generated sequences and other baselines. Our work thus takes several\nsteps toward elucidating and improving how language models perform on tasks\nrequiring multi-step mathematical reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nam_A/0/1/0/all/0/1\">Andrew J. Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Mengye Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McClelland_J/0/1/0/all/0/1\">James L. McClelland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Streaming Punctuation for Long-form Dictation with Transformers. (arXiv:2210.05756v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05756","description":"<p>While speech recognition Word Error Rate (WER) has reached human parity for\nEnglish, long-form dictation scenarios still suffer from segmentation and\npunctuation problems resulting from irregular pausing patterns or slow\nspeakers. Transformer sequence tagging models are effective at capturing long\nbi-directional context, which is crucial for automatic punctuation. Automatic\nSpeech Recognition (ASR) production systems, however, are constrained by\nreal-time requirements, making it hard to incorporate the right context when\nmaking punctuation decisions. In this paper, we propose a streaming approach\nfor punctuation or re-punctuation of ASR output using dynamic decoding windows\nand measure its impact on punctuation and segmentation accuracy across\nscenarios. The new system tackles over-segmentation issues, improving\nsegmentation F0.5-score by 13.9%. Streaming punctuation achieves an average\nBLEU-score improvement of 0.66 for the downstream task of Machine Translation\n(MT).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Behre_P/0/1/0/all/0/1\">Piyush Behre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Sharman Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varadharajan_P/0/1/0/all/0/1\">Padma Varadharajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuangyu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models of Code are Few-Shot Commonsense Learners. (arXiv:2210.07128v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07128","description":"<p>We address the general task of structured commonsense reasoning: given a\nnatural language input, the goal is to generate a graph such as an event -- or\na reasoning-graph. To employ large language models (LMs) for this task,\nexisting approaches ``serialize'' the output graph as a flat list of nodes and\nedges. Although feasible, these serialized graphs strongly deviate from the\nnatural language corpora that LMs were pre-trained on, hindering LMs from\ngenerating them correctly. In this paper, we show that when we instead frame\nstructured commonsense reasoning tasks as code generation tasks, pre-trained\nLMs of code are better structured commonsense reasoners than LMs of natural\nlanguage, even when the downstream task does not involve source code at all. We\ndemonstrate our approach across three diverse structured commonsense reasoning\ntasks. In all these natural language tasks, we show that using our approach, a\ncode generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the\ntarget task (e.g., T5) and other strong LMs such as GPT-3 in the few-shot\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1\">Uri Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACES: Translation Accuracy Challenge Sets for Evaluating Machine Translation Metrics. (arXiv:2210.15615v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15615","description":"<p>As machine translation (MT) metrics improve their correlation with human\njudgement every year, it is crucial to understand the limitations of such\nmetrics at the segment level. Specifically, it is important to investigate\nmetric behaviour when facing accuracy errors in MT because these can have\ndangerous consequences in certain contexts (e.g., legal, medical). We curate\nACES, a translation accuracy challenge set, consisting of 68 phenomena ranging\nfrom simple perturbations at the word/character level to more complex errors\nbased on discourse and real-world knowledge. We use ACES to evaluate a wide\nrange of MT metrics including the submissions to the WMT 2022 metrics shared\ntask and perform several analyses leading to general recommendations for metric\ndevelopers. We recommend: a) combining metrics with different strengths, b)\ndeveloping metrics that give more weight to the source and less to\nsurface-level overlap with the reference and c) explicitly modelling additional\nlanguage-specific information beyond what is available via multilingual\nembeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amrhein_C/0/1/0/all/0/1\">Chantal Amrhein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moghe_N/0/1/0/all/0/1\">Nikita Moghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guillou_L/0/1/0/all/0/1\">Liane Guillou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Impact of Social Media Posts by Executives on Stock Prices. (arXiv:2211.01287v2 [q-fin.ST] UPDATED)","link":"http://arxiv.org/abs/2211.01287","description":"<p>Predicting stock market movements has always been of great interest to\ninvestors and an active area of research. Research has proven that popularity\nof products is highly influenced by what people talk about. Social media like\nTwitter, Reddit have become hotspots of such influences. This paper\ninvestigates the impact of social media posts on close price prediction of\nstocks using Twitter and Reddit posts. Our objective is to integrate sentiment\nof social media data with historical stock data and study its effect on closing\nprices using time series models. We carried out rigorous experiments and deep\nanalysis using multiple deep learning based models on different datasets to\nstudy the influence of posts by executives and general people on the close\nprice. Experimental results on multiple stocks (Apple and Tesla) and\ndecentralised currencies (Bitcoin and Ethereum) consistently show improvements\nin prediction on including social media data and greater improvements on\nincluding executive posts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Sarkar_A/0/1/0/all/0/1\">Anubhav Sarkar</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Chakraborty_S/0/1/0/all/0/1\">Swagata Chakraborty</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Ghosh_S/0/1/0/all/0/1\">Sohom Ghosh</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Naskar_S/0/1/0/all/0/1\">Sudip Kumar Naskar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"kogito: A Commonsense Knowledge Inference Toolkit. (arXiv:2211.08451v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08451","description":"<p>In this paper, we present kogito, an open-source tool for generating\ncommonsense inferences about situations described in text. kogito provides an\nintuitive and extensible interface to interact with natural language generation\nmodels that can be used for hypothesizing commonsense knowledge inference from\na textual input. In particular, kogito offers several features for targeted,\nmulti-granularity knowledge generation. These include a standardized API for\ntraining and evaluating knowledge models, and generating and filtering\ninferences from them. We also include helper functions for converting natural\nlanguage texts into a format ingestible by knowledge models - intermediate\npipeline stages such as knowledge head extraction from text, heuristic and\nmodel-based knowledge head-relation matching, and an ability to define and use\ncustom knowledge relations. We make the code for kogito available at\nhttps://github.com/epfl-nlp/kogito along with thorough documentation at\nhttps://kogito.readthedocs.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ismayilzada_M/0/1/0/all/0/1\">Mete Ismayilzada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and Accurate FSA System Using ELBERT: An Efficient and Lightweight BERT. (arXiv:2211.08842v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08842","description":"<p>With the development of deep learning and Transformer-based pre-trained\nmodels like BERT, the accuracy of many NLP tasks has been dramatically\nimproved. However, the large number of parameters and computations also pose\nchallenges for their deployment. For instance, using BERT can improve the\npredictions in the financial sentiment analysis (FSA) task but slow it down,\nwhere speed and accuracy are equally important in terms of profits. To address\nthese issues, we first propose an efficient and lightweight BERT (ELBERT) along\nwith a novel confidence-window-based (CWB) early exit mechanism. Based on\nELBERT, an innovative method to accelerate text processing on the GPU platform\nis developed, solving the difficult problem of making the early exit mechanism\nwork more effectively with a large input batch size. Afterward, a fast and\nhigh-accuracy FSA system is built. Experimental results show that the proposed\nCWB early exit mechanism achieves significantly higher accuracy than existing\nearly exit methods on BERT under the same computation cost. By using this\nacceleration method, our FSA system can boost the processing speed by nearly 40\ntimes to over 1000 texts per second with sufficient accuracy, which is nearly\ntwice as fast as FastBERT, thus providing a more powerful text processing\ncapability for modern trading systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Siyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chenchen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1\">Keli Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongfeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniSumm: Unified Few-shot Summarization with Multi-Task Pre-Training and Prefix-Tuning. (arXiv:2211.09783v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.09783","description":"<p>The diverse demands of different summarization tasks and their high\nannotation costs are driving a need for few-shot summarization. However,\ndespite the emergence of many summarization tasks and datasets, the current\ntraining paradigm for few-shot summarization systems ignores potentially\nshareable knowledge in heterogeneous datasets. To this end, we propose\n\\textsc{UniSumm}, a unified few-shot summarization model pre-trained with\nmultiple summarization tasks and can be prefix-tuned to excel at any few-shot\nsummarization datasets. Meanwhile, to better evaluate few-shot summarization\nsystems, under the principles of diversity and robustness, we assemble and\npublicize a new benchmark \\textsc{SummZoo}. It consists of $8$ diverse\nsummarization tasks with multiple sets of few-shot samples for each task,\ncovering both monologue and dialogue domains. Experimental results and ablation\nstudies show that \\textsc{UniSumm} outperforms strong baseline systems by a\nlarge margin across all tasks in \\textsc{SummZoo} under both automatic and\nhuman evaluations. We release our code and benchmark at\n\\url{https://github.com/microsoft/UniSumm}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous diffusion for categorical data. (arXiv:2211.15089v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.15089","description":"<p>Diffusion models have quickly become the go-to paradigm for generative\nmodelling of perceptual signals (such as images and sound) through iterative\nrefinement. Their success hinges on the fact that the underlying physical\nphenomena are continuous. For inherently discrete and categorical data such as\nlanguage, various diffusion-inspired alternatives have been proposed. However,\nthe continuous nature of diffusion models conveys many benefits, and in this\nwork we endeavour to preserve it. We propose CDCD, a framework for modelling\ncategorical data with diffusion models that are continuous both in time and\ninput space. We demonstrate its efficacy on several language modelling tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dieleman_S/0/1/0/all/0/1\">Sander Dieleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sartran_L/0/1/0/all/0/1\">Laurent Sartran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roshannai_A/0/1/0/all/0/1\">Arman Roshannai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savinov_N/0/1/0/all/0/1\">Nikolay Savinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganin_Y/0/1/0/all/0/1\">Yaroslav Ganin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richemond_P/0/1/0/all/0/1\">Pierre H. Richemond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strudel_R/0/1/0/all/0/1\">Robin Strudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyer_C/0/1/0/all/0/1\">Chris Dyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durkan_C/0/1/0/all/0/1\">Conor Durkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawthorne_C/0/1/0/all/0/1\">Curtis Hawthorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leblond_R/0/1/0/all/0/1\">R&#xe9;mi Leblond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grathwohl_W/0/1/0/all/0/1\">Will Grathwohl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adler_J/0/1/0/all/0/1\">Jonas Adler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers are Short Text Classifiers: A Study of Inductive Short Text Classifiers on Benchmarks and Real-world Datasets. (arXiv:2211.16878v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.16878","description":"<p>Short text classification is a crucial and challenging aspect of Natural\nLanguage Processing. For this reason, there are numerous highly specialized\nshort text classifiers. However, in recent short text research, State of the\nArt (SOTA) methods for traditional text classification, particularly the pure\nuse of Transformers, have been unexploited. In this work, we examine the\nperformance of a variety of short text classifiers as well as the top\nperforming traditional text classifier. We further investigate the effects on\ntwo new real-world short text datasets in an effort to address the issue of\nbecoming overly dependent on benchmark datasets with a limited number of\ncharacteristics. Our experiments unambiguously demonstrate that Transformers\nachieve SOTA accuracy on short text classification tasks, raising the question\nof whether specialized short text techniques are necessary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karl_F/0/1/0/all/0/1\">Fabian Karl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Orders Are Unwanted: Dynamic Deep Graph Convolutional Network for Personality Detection. (arXiv:2212.01515v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.01515","description":"<p>Predicting personality traits based on online posts has emerged as an\nimportant task in many fields such as social network analysis. One of the\nchallenges of this task is assembling information from various posts into an\noverall profile for each user. While many previous solutions simply concatenate\nthe posts into a long document and then encode the document by sequential or\nhierarchical models, they introduce unwarranted orders for the posts, which may\nmislead the models. In this paper, we propose a dynamic deep graph\nconvolutional network (D-DGCN) to overcome the above limitation. Specifically,\nwe design a learn-to-connect approach that adopts a dynamic multi-hop structure\ninstead of a deterministic structure, and combine it with a DGCN module to\nautomatically learn the connections between posts. The modules of post encoder,\nlearn-to-connect, and DGCN are jointly trained in an end-to-end manner.\nExperimental results on the Kaggle and Pandora datasets show the superior\nperformance of D-DGCN to state-of-the-art baselines. Our code is available at\nhttps://github.com/djz233/D-DGCN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jinghao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1\">Xiaojun Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of Utterance Embeddings and Clustering Methods Related to Intent Induction for Task-Oriented Dialogue. (arXiv:2212.02021v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.02021","description":"<p>This paper investigates unsupervised approaches to overcome quintessential\nchallenges in designing task-oriented dialog schema: assigning intent labels to\neach dialog turn (intent clustering) and generating a set of intents based on\nthe intent clustering methods (intent induction). We postulate there are two\nsalient factors for automatic induction of intents: (1) clustering algorithm\nfor intent labeling and (2) user utterance embedding space. We compare existing\noff-the-shelf clustering models and embeddings based on DSTC11 evaluation. Our\nextensive experiments demonstrate that we sholud add two huge caveat that\nselection of utterance embedding and clustering method in intent induction task\nshould be very careful. We also present that pretrained MiniLM with\nAgglomerative clustering shows significant improvement in NMI, ARI, F1,\naccuracy and example coverage in intent induction tasks. The source code for\nreimplementation will be available at Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jeiyoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Yoonna Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chanhee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Heuiseok Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-12-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/"}}]}]}