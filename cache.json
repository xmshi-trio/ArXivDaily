{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-11-17T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"SexWEs: Domain-Aware Word Embeddings via Cross-lingual Semantic Specialisation for Chinese Sexism Detection in Social Media. (arXiv:2211.08447v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08447","description":"<p>The goal of sexism detection is to mitigate negative online content targeting\ncertain gender groups of people. However, the limited availability of labeled\nsexism-related datasets makes it problematic to identify online sexism for\nlow-resource languages. In this paper, we address the task of automatic sexism\ndetection in social media for one low-resource language -- Chinese. Rather than\ncollecting new sexism data or building cross-lingual transfer learning models,\nwe develop a cross-lingual domain-aware semantic specialisation system in order\nto make the most of existing data. Semantic specialisation is a technique for\nretrofitting pre-trained distributional word vectors by integrating external\nlinguistic knowledge (such as lexico-semantic relations) into the specialised\nfeature space. To do this, we leverage semantic resources for sexism from a\nhigh-resource language (English) to specialise pre-trained word vectors in the\ntarget language (Chinese) to inject domain knowledge. We demonstrate the\nbenefit of our sexist word embeddings (SexWEs) specialised by our framework via\nintrinsic evaluation of word similarity and extrinsic evaluation of sexism\ndetection. Compared with other specialisation approaches and Chinese baseline\nword vectors, our SexWEs shows an average score improvement of 0.033 and 0.064\nin both intrinsic and extrinsic evaluations, respectively. The ablative results\nand visualisation of SexWEs also prove the effectiveness of our framework on\nretrofitting word vectors in low-resource languages. Our code and\nsexism-related word vectors will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1\">Aiqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"kogito: A Commonsense Knowledge Inference Toolkit. (arXiv:2211.08451v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08451","description":"<p>In this paper, we present kogito, an open-source tool for generating\ncommonsense inferences about situations described in text. kogito provides an\nintuitive and extensible interface to interact with natural language generation\nmodels that can be used for hypothesizing commonsense knowledge inference from\na textual input. In particular, kogito offers several features for targeted,\nmulti-granularity knowledge generation. These include a standardized API for\ntraining and evaluating knowledge models, and generating and filtering\ninferences from them. We also include helper functions for converting natural\nlanguage texts into a format ingestible by knowledge models - intermediate\npipeline stages such as knowledge head extraction from text, heuristic and\nmodel-based knowledge head-relation matching, and an ability to define and use\ncustom knowledge relations. We make the code for kogito available at\nhttps://github.com/epfl-nlp/kogito along with thorough documentation at\nhttps://kogito.readthedocs.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ismayilzada_M/0/1/0/all/0/1\">Mete Ismayilzada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mind Your Bias: A Critical Review of Bias Detection Methods for Contextual Language Models. (arXiv:2211.08461v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08461","description":"<p>The awareness and mitigation of biases are of fundamental importance for the\nfair and transparent use of contextual language models, yet they crucially\ndepend on the accurate detection of biases as a precursor. Consequently,\nnumerous bias detection methods have been proposed, which vary in their\napproach, the considered type of bias, and the data used for evaluation.\nHowever, while most detection methods are derived from the word embedding\nassociation test for static word embeddings, the reported results are\nheterogeneous, inconsistent, and ultimately inconclusive. To address this\nissue, we conduct a rigorous analysis and comparison of bias detection methods\nfor contextual language models. Our results show that minor design and\nimplementation decisions (or errors) have a substantial and often significant\nimpact on the derived bias scores. Overall, we find the state of the field to\nbe both worse than previously acknowledged due to systematic and propagated\nerrors in implementations, yet better than anticipated since divergent results\nin the literature homogenize after accounting for implementation errors. Based\non our findings, we conclude with a discussion of paths towards more robust and\nconsistent bias detection methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Husse_S/0/1/0/all/0/1\">Silke Husse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spitz_A/0/1/0/all/0/1\">Andreas Spitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Navigating Connected Memories with a Task-oriented Dialog System. (arXiv:2211.08462v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08462","description":"<p>Recent years have seen an increasing trend in the volume of personal media\ncaptured by users, thanks to the advent of smartphones and smart glasses,\nresulting in large media collections. Despite conversation being an intuitive\nhuman-computer interface, current efforts focus mostly on single-shot natural\nlanguage based media retrieval to aid users query their media and re-live their\nmemories. This severely limits the search functionality as users can neither\nask follow-up queries nor obtain information without first formulating a\nsingle-turn query.\n</p>\n<p>In this work, we propose dialogs for connected memories as a powerful tool to\nempower users to search their media collection through a multi-turn,\ninteractive conversation. Towards this, we collect a new task-oriented dialog\ndataset COMET, which contains $11.5k$ user&lt;-&gt;assistant dialogs (totaling $103k$\nutterances), grounded in simulated personal memory graphs. We employ a\nresource-efficient, two-phase data collection pipeline that uses: (1) a novel\nmultimodal dialog simulator that generates synthetic dialog flows grounded in\nmemory graphs, and, (2) manual paraphrasing to obtain natural language\nutterances. We analyze COMET, formulate four main tasks to benchmark meaningful\nprogress, and adopt state-of-the-art language models as strong baselines, in\norder to highlight the multimodal challenges captured by our dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Seungwhan Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kottur_S/0/1/0/all/0/1\">Satwik Kottur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geramifard_A/0/1/0/all/0/1\">Alborz Geramifard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damavandi_B/0/1/0/all/0/1\">Babak Damavandi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ED-FAITH: Evaluating Dialogue Summarization on Faithfulness. (arXiv:2211.08464v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08464","description":"<p>Abstractive summarization models typically generate content unfaithful to the\ninput, thus highlighting the significance of evaluating the faithfulness of\ngenerated summaries. Most faithfulness metrics are only evaluated on news\ndomain, can they be transferred to other summarization tasks? In this work, we\nfirst present a systematic study of faithfulness metrics for dialogue\nsummarization. We evaluate common faithfulness metrics on dialogue datasets and\nobserve that most metrics correlate poorly with human judgements despite\nperforming well on news datasets. Given these findings, to improve existing\nmetrics' performance on dialogue summarization, we first finetune on in-domain\ndataset, then apply unlikelihood training on negative samples, and show that\nthey can successfully improve metric performance on dialogue data. Inspired by\nthe strong zero-shot performance of the T0 language model, we further propose\nT0-Score -- a new metric for faithfulness evaluation, which shows consistent\nimprovement against baseline metrics across multiple domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sicong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning Circuits: Few-shot Multihop Question Generation with Structured Rationales. (arXiv:2211.08466v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08466","description":"<p>Multi-hop Question Generation is the task of generating questions which\nrequire the reader to reason over and combine information spread across\nmultiple passages using several reasoning steps. Chain-of-thought rationale\ngeneration has been shown to improve performance on multi-step reasoning tasks\nand make model predictions more interpretable. However, few-shot performance\ngains from including rationales have been largely observed only in +100B\nlanguage models, and otherwise require large scale manual rationale annotation.\nIn this work, we introduce a new framework for applying chain-of-thought\ninspired structured rationale generation to multi-hop question generation under\na very low supervision regime (8- to 128-shot). We propose to annotate a small\nnumber of examples following our proposed multi-step rationale schema, treating\neach reasoning step as a separate task to be performed by a generative language\nmodel. We show that our framework leads to improved control over the difficulty\nof the generated questions and better performance compared to baselines trained\nwithout rationales, both on automatic evaluation metrics and in human\nevaluation. Importantly, we show that this is achievable with a modest model\nsize.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulshreshtha_S/0/1/0/all/0/1\">Saurabh Kulshreshtha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1\">Anna Rumshisky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Compositional Generalization Gap of In-Context Learning. (arXiv:2211.08473v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08473","description":"<p>Pretrained large generative language models have shown great performance on\nmany tasks, but exhibit low compositional generalization abilities. Scaling\nsuch models has been shown to improve their performance on various NLP tasks\neven just by conditioning them on a few examples to solve the task without any\nfine-tuning (also known as in-context learning). In this work, we look at the\ngap between the in-distribution (ID) and out-of-distribution (OOD) performance\nof such models in semantic parsing tasks with in-context learning. In the ID\nsettings, the demonstrations are from the same split (test or train) that the\nmodel is being evaluated on, and in the OOD settings, they are from the other\nsplit. We look at how the relative generalization gap of in-context learning\nevolves as models are scaled up. We evaluate four model families, OPT, BLOOM,\nCodeGen and Codex on three semantic parsing datasets, CFQ, SCAN and GeoQuery\nwith different number of exemplars, and observe a trend of decreasing relative\ngeneralization gap as models are scaled up.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_A/0/1/0/all/0/1\">Arian Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vani_A/0/1/0/all/0/1\">Ankit Vani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahdanau_D/0/1/0/all/0/1\">Dzmitry Bahdanau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1\">Alessandro Sordoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching for Carriers of the Diffuse Interstellar Bands Across Disciplines, using Natural Language Processing. (arXiv:2211.08513v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08513","description":"<p>The explosion of scientific publications overloads researchers with\ninformation. This is even more dramatic for interdisciplinary studies, where\nseveral fields need to be explored. A tool to help researchers overcome this is\nNatural Language Processing (NLP): a machine-learning (ML) technique that\nallows scientists to automatically synthesize information from many articles.\nAs a practical example, we have used NLP to conduct an interdisciplinary search\nfor compounds that could be carriers for Diffuse Interstellar Bands (DIBs), a\nlong-standing open question in astrophysics. We have trained a NLP model on a\ncorpus of 1.5 million cross-domain articles in open access, and fine-tuned this\nmodel with a corpus of astrophysical publications about DIBs. Our analysis\npoints us toward several molecules, studied primarily in biology, having\ntransitions at the wavelengths of several DIBs and composed of abundant\ninterstellar atoms. Several of these molecules contain chromophores, small\nmolecular groups responsible for the molecule's colour, that could be promising\ncandidate carriers. Identifying viable carriers demonstrates the value of using\nNLP to tackle open scientific questions, in an interdisciplinary manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dobrenan_C/0/1/0/all/0/1\">Corentin van den Broek Dobrenan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galliano_F/0/1/0/all/0/1\">Frederic Galliano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minton_J/0/1/0/all/0/1\">Jeremy Minton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botev_V/0/1/0/all/0/1\">Viktor Botev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ronin Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MapQA: A Dataset for Question Answering on Choropleth Maps. (arXiv:2211.08545v1 [cs.CV])","link":"http://arxiv.org/abs/2211.08545","description":"<p>Choropleth maps are a common visual representation for region-specific\ntabular data and are used in a number of different venues (newspapers,\narticles, etc). These maps are human-readable but are often challenging to deal\nwith when trying to extract data for screen readers, analyses, or other related\ntasks. Recent research into Visual-Question Answering (VQA) has studied\nquestion answering on human-generated charts (ChartQA), such as bar, line, and\npie charts. However, little work has paid attention to understanding maps;\ngeneral VQA models, and ChartQA models, suffer when asked to perform this task.\nTo facilitate and encourage research in this area, we present MapQA, a\nlarge-scale dataset of ~800K question-answer pairs over ~60K map images. Our\ntask tests various levels of map understanding, from surface questions about\nmap styles to complex questions that require reasoning on the underlying data.\nWe present the unique challenges of MapQA that frustrate most strong baseline\nalgorithms designed for ChartQA and general VQA tasks. We also present a novel\nalgorithm, Visual Multi-Output Data Extraction based QA (V-MODEQA) for MapQA.\nV-MODEQA extracts the underlying structured data from a map image with a\nmulti-output model and then performs reasoning on the extracted data. Our\nexperimental results show that V-MODEQA has better overall performance and\nrobustness on MapQA than the state-of-the-art ChartQA and VQA algorithms by\ncapturing the unique properties in map question answering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuaichen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palzer_D/0/1/0/all/0/1\">David Palzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jialin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fosler_Lussier_E/0/1/0/all/0/1\">Eric Fosler-Lussier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_N/0/1/0/all/0/1\">Ningchuan Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALIGN-MLM: Word Embedding Alignment is Crucial for Multilingual Pre-training. (arXiv:2211.08547v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08547","description":"<p>Multilingual pre-trained models exhibit zero-shot cross-lingual transfer,\nwhere a model fine-tuned on a source language achieves surprisingly good\nperformance on a target language. While studies have attempted to understand\ntransfer, they focus only on MLM, and the large number of differences between\nnatural languages makes it hard to disentangle the importance of different\nproperties. In this work, we specifically highlight the importance of word\nembedding alignment by proposing a pre-training objective (ALIGN-MLM) whose\nauxiliary loss guides similar words in different languages to have similar word\nembeddings. ALIGN-MLM either outperforms or matches three widely adopted\nobjectives (MLM, XLM, DICT-MLM) when we evaluate transfer between pairs of\nnatural languages and their counterparts created by systematically modifying\nspecific properties like the script. In particular, ALIGN-MLM outperforms XLM\nand MLM by 35 and 30 F1 points on POS-tagging for transfer between languages\nthat differ both in their script and word order (left-to-right v.s.\nright-to-left). We also show a strong correlation between alignment and\ntransfer for all objectives (e.g., rho=0.727 for XNLI), which together with\nALIGN-MLM's strong performance calls for explicitly aligning word embeddings\nfor multilingual models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Henry Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1\">Ameet Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward expanding the scope of radiology report summarization to multiple anatomies and modalities. (arXiv:2211.08584v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08584","description":"<p>Radiology report summarization is a growing area of research. Given the\nFindings and/or Background sections of a radiology report, the goal is to\ngenerate a summary (called an Impression section) that highlights the key\nobservations and conclusions of the radiology study. Recent efforts have\nreleased systems that achieve promising performance as measured by widely used\nsummarization metrics such as BLEU and ROUGE. However, the research area of\nradiology report summarization currently faces important limitations. First,\nmost of the results are reported on private datasets. This limitation prevents\nthe ability to reproduce results and fairly compare different systems and\nsolutions. Secondly, to the best of our knowledge, most research is carried out\non chest X-rays. Sometimes, studies even omit to mention the concerned modality\nand anatomy in the radiology reports used for their experiments. To palliate\nthese limitations, we propose a new dataset of six different modalities and\nanatomies based on the MIMIC-III database. We further release our results and\nthe data splits used to carry out our experiments. Finally, we propose a simple\nreport summarization system that outperforms the previous replicable research\non the existing dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Delbrouck_J/0/1/0/all/0/1\">Jean-Benoit Delbrouck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1\">Maya Varma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Task Relations for Few-shot Text Classification via Self-Supervised Hierarchical Task Clustering. (arXiv:2211.08588v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08588","description":"<p>Few-Shot Text Classification (FSTC) imitates humans to learn a new text\nclassifier efficiently with only few examples, by leveraging prior knowledge\nfrom historical tasks. However, most prior works assume that all the tasks are\nsampled from a single data source, which cannot adapt to real-world scenarios\nwhere tasks are heterogeneous and lie in different distributions. As such,\nexisting methods may suffer from their globally knowledge-shared mechanisms to\nhandle the task heterogeneity. On the other hand, inherent task relation are\nnot explicitly captured, making task knowledge unorganized and hard to transfer\nto new tasks. Thus, we explore a new FSTC setting where tasks can come from a\ndiverse range of data sources. To address the task heterogeneity, we propose a\nself-supervised hierarchical task clustering (SS-HTC) method. SS-HTC not only\ncustomizes cluster-specific knowledge by dynamically organizing heterogeneous\ntasks into different clusters in hierarchical levels but also disentangles\nunderlying relations between tasks to improve the interpretability. Extensive\nexperiments on five public FSTC benchmark datasets demonstrate the\neffectiveness of SS-HTC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zha_J/0/1/0/all/0/1\">Juan Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Ying Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MT Metrics Correlate with Human Ratings of Simultaneous Speech Translation. (arXiv:2211.08633v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08633","description":"<p>There have been several studies on the correlation between human ratings and\nmetrics such as BLEU, chrF2 and COMET in machine translation. Most, if not all\nconsider full-sentence translation. It is unclear whether human ratings of\nsimultaneous speech translation Continuous Rating (CR) correlate with these\nmetrics or not. Therefore, we conduct an extensive correlation analysis of CR\nand the aforementioned automatic metrics on evaluations of candidate systems at\nEnglish-German simultaneous speech translation task at IWSLT 2022. Our studies\nreveal that the offline MT metrics correlate with CR and can be reliably used\nfor evaluating machine translation in the simultaneous mode, with some\nlimitations on the test set size. This implies that automatic metrics can be\nused as proxies for CR, thereby alleviating the need for human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Machacek_D/0/1/0/all/0/1\">Dominik Mach&#xe1;&#x10d;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"#maskUp: Selective Attribute Encryption for Sensitive Vocalization for English language on Social Media Platforms. (arXiv:2211.08653v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08653","description":"<p>Social media has become a platform for people to stand up and raise their\nvoices against social and criminal acts. Vocalization of such information has\nallowed the investigation and identification of criminals. However, revealing\nsuch sensitive information may jeopardize the victim's safety. We propose\n#maskUp, a safe method for information communication in a secure fashion to the\nrelevant authorities, discouraging potential bullying of the victim. This would\nensure security by conserving their privacy through natural language processing\nsupplemented with selective encryption for sensitive attribute masking. To our\nknowledge, this is the first work that aims to protect the privacy of the\nvictims by masking their private details as well as emboldening them to come\nforward to report crimes. The use of masking technology allows only binding\nauthorities to view/un-mask this data. We construct and evaluate the proposed\nmethodology on continual learning tasks, allowing practical implementation of\nthe same in a real-world scenario. #maskUp successfully demonstrates this\nintegration on sample datasets validating the presented objective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vijay_S/0/1/0/all/0/1\">Supriti Vijay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyanshu_A/0/1/0/all/0/1\">Aman Priyanshu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Tuning on Layer Normalization for Pre-trained Language Models. (arXiv:2211.08682v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08682","description":"<p>Conventional fine-tuning encounters increasing difficulties given the size of\ncurrent Pre-trained Language Models, which makes parameter-efficient tuning\nbecome the focal point of frontier research. Previous methods in this field add\ntunable adapters into MHA or/and FFN of Transformer blocks to enable PLMs\nachieve transferability. However, as an important part of Transformer\narchitecture, the power of layer normalization for parameter-efficent tuning is\nignored. In this paper, we first propose LN-tuning, by tuning the gain and bias\nterm of Layer Normalization module with only 0.03\\% parameters, which is of\nhigh time-efficency and significantly superior to baselines which are less than\n0.1\\% tunable parameters. Further, we study the unified framework of combining\nLN-tuning with previous ones and we find that: (1) the unified framework of\ncombining prefix-tuning, the adapter-based method working on MHA, and LN-tuning\nachieves SOTA performance. (2) unified framework which tunes MHA and LayerNorm\nsimultaneously can get performance improvement but those which tune FFN and\nLayerNorm simultaneous will cause performance decrease. Ablation study\nvalidates LN-tuning is of no abundant parameters and gives a further\nunderstanding of it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1\">Wang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Y/0/1/0/all/0/1\">Yu-Ping Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_Y/0/1/0/all/0/1\">Yuan Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Taihao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Unsupervised Reconstruction of Protolanguage Word Forms. (arXiv:2211.08684v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08684","description":"<p>We present a state-of-the-art neural approach to the unsupervised\nreconstruction of ancient word forms. Previous work in this domain used\nexpectation-maximization to predict simple phonological changes between ancient\nword forms and their cognates in modern languages. We extend this work with\nneural models that can capture more complicated phonological and morphological\nchanges. At the same time, we preserve the inductive biases from classical\nmethods by building monotonic alignment constraints into the model and\ndeliberately underfitting during the maximization step. We evaluate our\nperformance on the task of reconstructing Latin from a dataset of cognates\nacross five Romance languages, achieving a notable reduction in edit distance\nfrom the target word forms compared to previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_A/0/1/0/all/0/1\">Andre He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomlin_N/0/1/0/all/0/1\">Nicholas Tomlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reward Gaming in Conditional Text Generation. (arXiv:2211.08714v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08714","description":"<p>To align conditional text generation model outputs with desired behaviors,\nthere has been an increasing focus on training the model using reinforcement\nlearning (RL) with reward functions learned from human annotations. Under this\nframework, we identify three common cases where high rewards are incorrectly\nassigned to undesirable patterns: noise-induced spurious correlation, naturally\noccurring spurious correlation, and covariate shift. We show that even though\nlearned metrics achieve high performance on the distribution of the data used\nto train the reward function, the undesirable patterns may be amplified during\nRL training of the text generation model. While there has been discussion about\nreward gaming in the RL or safety community, in this short discussion piece, we\nwould like to highlight reward gaming in the NLG community using concrete\nconditional text generation examples and discuss potential fixes and areas for\nfuture work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1\">Richard Yuanzhe Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_V/0/1/0/all/0/1\">Vishakh Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellam_T/0/1/0/all/0/1\">Thibault Sellam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_A/0/1/0/all/0/1\">Ankur P. Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noisy Pairing and Partial Supervision for Opinion Summarization. (arXiv:2211.08723v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08723","description":"<p>Current opinion summarization systems simply generate summaries reflecting\nimportant opinions from customer reviews, but the generated summaries may not\nattract the reader's attention. Although it is helpful to automatically\ngenerate professional reviewer-like summaries from customer reviews, collecting\nmany training pairs of customer and professional reviews is generally tricky.\nWe propose a weakly supervised opinion summarization framework, Noisy Pairing\nand Partial Supervision (NAPA) that can build a stylized opinion summarization\nsystem with no customer-professional review pairs. Experimental results show\nconsistent improvements in automatic evaluation metrics, and qualitative\nanalysis shows that our weakly supervised opinion summarization system can\ngenerate summaries that look more like those written by professional reviewers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iso_H/0/1/0/all/0/1\">Hayate Iso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhara_Y/0/1/0/all/0/1\">Yoshi Suhara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Streaming Joint Speech Recognition and Disfluency Detection. (arXiv:2211.08726v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08726","description":"<p>Disfluency detection has mainly been solved in a pipeline approach, as\npost-processing of speech recognition. In this study, we propose\nTransformer-based encoder-decoder models that jointly solve speech recognition\nand disfluency detection, which work in a streaming manner. Compared to\npipeline approaches, the joint models can leverage acoustic information that\nmakes disfluency detection robust to recognition errors and provide non-verbal\nclues. Moreover, joint modeling results in low-latency and lightweight\ninference. We investigate two joint model variants for streaming disfluency\ndetection: a transcript-enriched model and a multi-task model. The\ntranscript-enriched model is trained on text with special tags indicating the\nstarting and ending points of the disfluent part. However, it has problems with\nlatency and standard language model adaptation, which arise from the additional\ndisfluency tags. We propose a multi-task model to solve such problems, which\nhas two output layers at the Transformer decoder; one for speech recognition\nand the other for disfluency detection. It is modeled to be conditioned on the\ncurrently recognized token with an additional token-dependency mechanism. We\nshow that the proposed joint models outperformed a BERT-based pipeline approach\nin both accuracy and latency, on both the Switchboard and the corpus of\nspontaneous Japanese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Futami_H/0/1/0/all/0/1\">Hayato Futami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsunoo_E/0/1/0/all/0/1\">Emiru Tsunoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shibata_K/0/1/0/all/0/1\">Kentaro Shibata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashiwagi_Y/0/1/0/all/0/1\">Yosuke Kashiwagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okuda_T/0/1/0/all/0/1\">Takao Okuda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Siddhant Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lesion Guided Explainable Few Weak-shot Medical Report Generation. (arXiv:2211.08732v1 [cs.CV])","link":"http://arxiv.org/abs/2211.08732","description":"<p>Medical images are widely used in clinical practice for diagnosis.\nAutomatically generating interpretable medical reports can reduce radiologists'\nburden and facilitate timely care. However, most existing approaches to\nautomatic report generation require sufficient labeled data for training. In\naddition, the learned model can only generate reports for the training classes,\nlacking the ability to adapt to previously unseen novel diseases. To this end,\nwe propose a lesion guided explainable few weak-shot medical report generation\nframework that learns correlation between seen and novel classes through visual\nand semantic feature alignment, aiming to generate medical reports for diseases\nnot observed in training. It integrates a lesion-centric feature extractor and\na Transformer-based report generation module. Concretely, the lesion-centric\nfeature extractor detects the abnormal regions and learns correlations between\nseen and novel classes with multi-view (visual and lexical) embeddings. Then,\nfeatures of the detected regions and corresponding embeddings are concatenated\nas multi-view input to the report generation module for explainable report\ngeneration, including text descriptions and corresponding abnormal regions\ndetected in the images. We conduct experiments on FFA-IR, a dataset providing\nexplainable annotations, showing that our framework outperforms others on\nreport generation for novel diseases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jinghan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auditing Algorithmic Fairness in Machine Learning for Health with Severity-Based LOGAN. (arXiv:2211.08742v1 [cs.LG])","link":"http://arxiv.org/abs/2211.08742","description":"<p>Auditing machine learning-based (ML) healthcare tools for bias is critical to\npreventing patient harm, especially in communities that disproportionately face\nhealth inequities. General frameworks are becoming increasingly available to\nmeasure ML fairness gaps between groups. However, ML for health (ML4H) auditing\nprinciples call for a contextual, patient-centered approach to model\nassessment. Therefore, ML auditing tools must be (1) better aligned with ML4H\nauditing principles and (2) able to illuminate and characterize communities\nvulnerable to the most harm. To address this gap, we propose supplementing ML4H\nauditing frameworks with SLOGAN (patient Severity-based LOcal Group biAs\ndetectioN), an automatic tool for capturing local biases in a clinical\nprediction task. SLOGAN adapts an existing tool, LOGAN (LOcal Group biAs\ndetectioN), by contextualizing group bias detection in patient illness severity\nand past medical history. We investigate and compare SLOGAN's bias detection\ncapabilities to LOGAN and other clustering techniques across patient subgroups\nin the MIMIC-III dataset. On average, SLOGAN identifies larger fairness\ndisparities in over 75% of patient groups than LOGAN while maintaining\nclustering quality. Furthermore, in a diabetes case study, health disparity\nliterature corroborates the characterizations of the most biased clusters\nidentified by SLOGAN. Our results contribute to the broader discussion of how\nmachine learning biases may perpetuate existing healthcare disparities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ovalle_A/0/1/0/all/0/1\">Anaelia Ovalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jieyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarrafzadeh_M/0/1/0/all/0/1\">Majid Sarrafzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RetroMAE v2: Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models. (arXiv:2211.08769v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08769","description":"<p>To better support retrieval applications such as web search and question\nanswering, growing effort is made to develop retrieval-oriented language\nmodels. Most of the existing works focus on improving the semantic\nrepresentation capability for the contextualized embedding of [CLS] token.\nHowever, recent study shows that the ordinary tokens besides [CLS] may provide\nextra information, which helps to produce a better representation effect. As\nsuch, it's necessary to extend the current methods where all contextualized\nembeddings can be jointly pre-trained for the retrieval tasks.\n</p>\n<p>With this motivation, we propose a new pre-training method: duplex masked\nauto-encoder, a.k.a. DupMAE, which targets on improving the semantic\nrepresentation capacity for the contextualized embeddings of both [CLS] and\nordinary tokens. It introduces two decoding tasks: one is to reconstruct the\noriginal input sentence based on the [CLS] embedding, the other one is to\nminimize the bag-of-words loss (BoW) about the input sentence based on the\nentire ordinary tokens' embeddings. The two decoding losses are added up to\ntrain a unified encoding model. The embeddings from [CLS] and ordinary tokens,\nafter dimension reduction and aggregation, are concatenated as one unified\nsemantic representation for the input. DupMAE is simple but empirically\ncompetitive: with a small decoding cost, it substantially contributes to the\nmodel's representation capability and transferability, where remarkable\nimprovements are achieved on MS MARCO and BEIR benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shitao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CSCD-IME: Correcting Spelling Errors Generated by Pinyin IME. (arXiv:2211.08788v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08788","description":"<p>Chinese Spelling Correction (CSC) is a task to detect and correct spelling\nmistakes in texts. In fact, most of Chinese input is based on pinyin input\nmethod, so the study of spelling errors in this process is more practical and\nvaluable. However, there is still no research dedicated to this essential\nscenario. In this paper, we first present a Chinese Spelling Correction Dataset\nfor errors generated by pinyin IME (CSCD-IME), including 40,000 annotated\nsentences from real posts of official media on Sina Weibo. Furthermore, we\npropose a novel method to automatically construct large-scale and high-quality\npseudo data by simulating the input through pinyin IME. A series of analyses\nand experiments on CSCD-IME show that spelling errors produced by pinyin IME\nhold a particular distribution at pinyin level and semantic level and are\nchallenging enough. Meanwhile, our proposed pseudo-data construction method can\nbetter fit this error distribution and improve the performance of CSC systems.\nFinally, we also provide a useful guide to using pseudo data, including the\ndata scale, the data source, and the training strategy\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations. (arXiv:2211.08794v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08794","description":"<p>Due to the huge amount of parameters, fine-tuning of pretrained language\nmodels (PLMs) is prone to overfitting in the low resource scenarios. In this\nwork, we present a novel method that operates on the hidden representations of\na PLM to reduce overfitting. During fine-tuning, our method inserts random\nautoencoders between the hidden layers of a PLM, which transform activations\nfrom the previous layers into a multi-view compressed representation before\nfeeding it into the upper layers. The autoencoders are plugged out after\nfine-tuning, so our method does not add extra parameters or increase\ncomputation cost during inference. Our method demonstrates promising\nperformance improvement across a wide range of sequence- and token-level\nlow-resource NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_M/0/1/0/all/0/1\">Megh Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cognitive Simplification Operations Improve Text Simplification. (arXiv:2211.08825v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08825","description":"<p>Text Simplification (TS) is the task of converting a text into a form that is\neasier to read while maintaining the meaning of the original text. A sub-task\nof TS is Cognitive Simplification (CS), converting text to a form that is\nreadily understood by people with cognitive disabilities without rendering it\nchildish or simplistic. This sub-task has yet to be explored with neural\nmethods in NLP, and resources for it are scarcely available. In this paper, we\npresent a method for incorporating knowledge from the cognitive accessibility\ndomain into a TS model, by introducing an inductive bias regarding what\nsimplification operations to use. We show that by adding this inductive bias to\na TS-trained model, it is able to adapt better to CS without ever seeing CS\ndata, and outperform a baseline model on a traditional TS benchmark. In\naddition, we provide a novel test dataset for CS, and analyze the differences\nbetween CS corpora and existing TS corpora, in terms of how simplification\noperations are applied.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chamovitz_E/0/1/0/all/0/1\">Eytan Chamovitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and Accurate FSA System Using ELBERT: An Efficient and Lightweight BERT. (arXiv:2211.08842v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08842","description":"<p>As an application of Natural Language Processing (NLP) techniques, financial\nsentiment analysis (FSA) has become an invaluable tool for investors. Its speed\nand accuracy can significantly impact the returns of trading strategies.With\nthe development of deep learning and Transformer-based pre-trained models like\nBERT, the accuracy of FSA has been much improved, but these time-consuming big\nmodels will also slow down the computation. To boost the processing speed of\nthe FSA system and ensure high precision, we first propose an efficient and\nlightweight BERT (ELBERT) along with a novel confidence-window-based (CWB)\nearly exit mechanism. Based on ELBERT, an innovative method to accelerate text\nprocessing on the GPU platform is developed, solving the difficult problem of\nmaking the early exit mechanism work more effectively with a large input batch\nsize. Afterward, a fast and high-accuracy FSA system is built. Experimental\nresults show that the proposed CWB early exit mechanism achieves significantly\nhigher accuracy than existing early exit methods on BERT under the same\ncomputation cost. Besides, our FSA system can boost the processing speed to\nover 1000 texts per second with sufficient accuracy by using this acceleration\nmethod, which is nearly twice as fast as the FastBERT. Hence, this system can\nenable modern trading systems to quickly and accurately process financial text\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Siyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chenchen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1\">Keli Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongfeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L2 proficiency assessment using self-supervised speech representations. (arXiv:2211.08849v1 [eess.AS])","link":"http://arxiv.org/abs/2211.08849","description":"<p>There has been a growing demand for automated spoken language assessment\nsystems in recent years. A standard pipeline for this process is to start with\na speech recognition system and derive features, either hand-crafted or based\non deep-learning, that exploit the transcription and audio. Though these\napproaches can yield high performance systems, they require speech recognition\nsystems that can be used for L2 speakers, and preferably tuned to the specific\nform of test being deployed. Recently a self-supervised speech representation\nbased scheme, requiring no speech recognition, was proposed. This work extends\nthe initial analysis conducted on this approach to a large scale proficiency\ntest, Linguaskill, that comprises multiple parts, each designed to assess\ndifferent attributes of a candidate's speaking proficiency. The performance of\nthe self-supervised, wav2vec 2.0, system is compared to a high performance\nhand-crafted assessment system and a BERT-based text system both of which use\nspeech transcriptions. Though the wav2vec 2.0 based system is found to be\nsensitive to the nature of the response, it can be configured to yield\ncomparable performance to systems requiring a speech transcription, and yields\ngains when appropriately combined with standard approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Banno_S/0/1/0/all/0/1\">Stefano Bann&#xf2;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Knill_K/0/1/0/all/0/1\">Kate M. Knill</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matassoni_M/0/1/0/all/0/1\">Marco Matassoni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raina_V/0/1/0/all/0/1\">Vyas Raina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gales_M/0/1/0/all/0/1\">Mark J. F. Gales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consecutive Question Generation via Dynamic Multitask Learning. (arXiv:2211.08850v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08850","description":"<p>In this paper, we propose the task of consecutive question generation (CQG),\nwhich generates a set of logically related question-answer pairs to understand\na whole passage, with a comprehensive consideration of the aspects including\naccuracy, coverage, and informativeness. To achieve this, we first examine the\nfour key elements of CQG, i.e., question, answer, rationale, and context\nhistory, and propose a novel dynamic multitask framework with one main task\ngenerating a question-answer pair, and four auxiliary tasks generating other\nelements. It directly helps the model generate good questions through both\njoint training and self-reranking. At the same time, to fully explore the\nworth-asking information in a given passage, we make use of the reranking\nlosses to sample the rationales and search for the best question series\nglobally. Finally, we measure our strategy by QA data augmentation and manual\nevaluation, as well as a novel application of generated question-answer pairs\non DocNLI. We prove that our strategy can improve question generation\nsignificantly and benefit multiple related NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sujian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xing Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TSMind: Alibaba and Soochow University's Submission to the WMT22 Translation Suggestion Task. (arXiv:2211.08987v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08987","description":"<p>This paper describes the joint submission of Alibaba and Soochow University,\nTSMind, to the WMT 2022 Shared Task on Translation Suggestion (TS). We\nparticipate in the English-German and English-Chinese tasks. Basically, we\nutilize the model paradigm fine-tuning on the downstream tasks based on\nlarge-scale pre-trained models, which has recently achieved great success. We\nchoose FAIR's WMT19 English-German news translation system and MBART50 for\nEnglish-Chinese as our pre-trained models. Considering the task's condition of\nlimited use of training data, we follow the data augmentation strategies\nproposed by WeTS to boost our TS model performance. The difference is that we\nfurther involve the dual conditional cross-entropy model and GPT-2 language\nmodel to filter augmented data. The leader board finally shows that our\nsubmissions are ranked first in three of four language directions in the Naive\nTS task of the WMT22 Translation Suggestion task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Ke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiayi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_N/0/1/0/all/0/1\">Nini Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_X/0/1/0/all/0/1\">Xiangyu Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuqi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Avoid Overthinking in Self-Supervised Models for Speech Recognition. (arXiv:2211.08989v1 [cs.CL])","link":"http://arxiv.org/abs/2211.08989","description":"<p>Self-supervised learning (SSL) models reshaped our approach to speech,\nlanguage and vision. However their huge size and the opaque relations between\ntheir layers and tasks result in slow inference and network overthinking, where\npredictions made from the last layer of large models is worse than those made\nfrom intermediate layers. Early exit (EE) strategies can solve both issues by\ndynamically reducing computations at inference time for certain samples.\nAlthough popular for classification tasks in vision and language, EE has seen\nless use for sequence-to-sequence speech recognition (ASR) tasks where outputs\nfrom early layers are often degenerate. This challenge is further compounded\nwhen speech SSL models are applied on out-of-distribution (OOD) data. This\npaper first shows that SSL models do overthinking in ASR. We then motivate\nfurther research in EE by computing an optimal bound for performance versus\nspeed trade-offs. To approach this bound we propose two new strategies for ASR:\n(1) we adapt the recently proposed patience strategy to ASR; and (2) we design\na new EE strategy specific to ASR that performs better than all strategies\npreviously introduced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berrebbi_D/0/1/0/all/0/1\">Dan Berrebbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniRel: Unified Representation and Interaction for Joint Relational Triple Extraction. (arXiv:2211.09039v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09039","description":"<p>Relational triple extraction is challenging for its difficulty in capturing\nrich correlations between entities and relations. Existing works suffer from 1)\nheterogeneous representations of entities and relations, and 2) heterogeneous\nmodeling of entity-entity interactions and entity-relation interactions.\nTherefore, the rich correlations are not fully exploited by existing works. In\nthis paper, we propose UniRel to address these challenges. Specifically, we\nunify the representations of entities and relations by jointly encoding them\nwithin a concatenated natural language sequence, and unify the modeling of\ninteractions with a proposed Interaction Map, which is built upon the\noff-the-shelf self-attention mechanism within any Transformer block. With\ncomprehensive experiments on two popular relational triple extraction datasets,\nwe demonstrate that UniRel is more effective and computationally efficient. The\nsource code is available at https://github.com/wtangdev/UniRel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wei Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Benfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuyue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhendong Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Haiyong Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A generative grammar of cooking. (arXiv:2211.09059v1 [physics.soc-ph])","link":"http://arxiv.org/abs/2211.09059","description":"<p>Cooking is a uniquely human endeavor for transforming raw ingredients into\ndelicious dishes. Over centuries, cultures worldwide have evolved diverse\ncooking practices ingrained in their culinary traditions. Recipes, thus, are\ncultural capsules that capture culinary knowledge in elaborate cooking\nprotocols. While simple quantitative models have probed the patterns in recipe\ncomposition and the process of cuisine evolution, unlike other cultural quirks\nsuch as language, the principles of cooking remain hitherto unexplored. The\nfundamental rules that drive the act of cooking, shaping recipe composition and\ncuisine architecture, are unclear. Here we present a generative grammar of\ncooking that captures the underlying culinary logic. By studying an extensive\nrepository of structured recipes, we identify core concepts and rules that\ntogether forge a combinatorial system for culinary synthesis. Building on the\nbody of work done in the context of language, the demonstration of a logically\nconsistent generative framework offers profound insights into the act of\ncooking. Given the central role of food in nutrition and lifestyle disorders,\nculinary grammar provides leverage to improve public health through dietary\ninterventions beyond applications for creative pursuits such as novel recipe\ngeneration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Bagler_G/0/1/0/all/0/1\">Ganesh Bagler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teaching Algorithmic Reasoning via In-context Learning. (arXiv:2211.09066v1 [cs.LG])","link":"http://arxiv.org/abs/2211.09066","description":"<p>Large language models (LLMs) have shown increasing in-context learning\ncapabilities through scaling up model and data size. Despite this progress,\nLLMs are still unable to solve algorithmic reasoning problems. While providing\na rationale with the final answer has led to further improvements in multi-step\nreasoning problems, Anil et al. 2022 showed that even simple algorithmic\nreasoning tasks such as parity are far from solved. In this work, we identify\nand study four key stages for successfully teaching algorithmic reasoning to\nLLMs: (1) formulating algorithms as skills, (2) teaching multiple skills\nsimultaneously (skill accumulation), (3) teaching how to combine skills (skill\ncomposition) and (4) teaching how to use skills as tools. We show that it is\npossible to teach algorithmic reasoning to LLMs via in-context learning, which\nwe refer to as algorithmic prompting. We evaluate our approach on a variety of\narithmetic and quantitative reasoning tasks, and demonstrate significant boosts\nin performance over existing prompting techniques. In particular, for long\nparity, addition, multiplication and subtraction, we achieve an error reduction\nof approximately 10x, 9x, 5x and 2x respectively compared to the best available\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hattie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nova_A/0/1/0/all/0/1\">Azade Nova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larochelle_H/0/1/0/all/0/1\">Hugo Larochelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1\">Behnam Neyshabur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedghi_H/0/1/0/all/0/1\">Hanie Sedghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Computationally Verifiable Semantic Grounding for Language Models. (arXiv:2211.09070v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09070","description":"<p>The paper presents an approach to semantic grounding of language models (LMs)\nthat conceptualizes the LM as a conditional model generating text given a\ndesired semantic message formalized as a set of entity-relationship triples. It\nembeds the LM in an auto-encoder by feeding its output to a semantic parser\nwhose output is in the same representation domain as the input message.\nCompared to a baseline that generates text using greedy search, we demonstrate\ntwo techniques that improve the fluency and semantic accuracy of the generated\ntext: The first technique samples multiple candidate text sequences from which\nthe semantic parser chooses. The second trains the language model while keeping\nthe semantic parser frozen to improve the semantic accuracy of the\nauto-encoder. We carry out experiments on the English WebNLG 3.0 data set,\nusing BLEU to measure the fluency of generated text and standard parsing\nmetrics to measure semantic accuracy. We show that our proposed approaches\nsignificantly improve on the greedy search baseline. Human evaluation\ncorroborates the results of the automatic evaluation experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alberti_C/0/1/0/all/0/1\">Chris Alberti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganchev_K/0/1/0/all/0/1\">Kuzman Ganchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1\">Michael Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chelba_C/0/1/0/all/0/1\">Ciprian Chelba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Galactica: A Large Language Model for Science. (arXiv:2211.09085v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09085","description":"<p>Information overload is a major obstacle to scientific progress. The\nexplosive growth in scientific literature and data has made it ever harder to\ndiscover useful insights in a large mass of information. Today scientific\nknowledge is accessed through search engines, but they are unable to organize\nscientific knowledge alone. In this paper we introduce Galactica: a large\nlanguage model that can store, combine and reason about scientific knowledge.\nWe train on a large scientific corpus of papers, reference material, knowledge\nbases and many other sources. We outperform existing models on a range of\nscientific tasks. On technical knowledge probes such as LaTeX equations,\nGalactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also\nperforms well on reasoning, outperforming Chinchilla on mathematical MMLU by\n41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It\nalso sets a new state-of-the-art on downstream tasks such as PubMedQA and\nMedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general\ncorpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these\nresults demonstrate the potential for language models as a new interface for\nscience. We open source the model for the benefit of the scientific community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1\">Ross Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kardas_M/0/1/0/all/0/1\">Marcin Kardas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucurull_G/0/1/0/all/0/1\">Guillem Cucurull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartshorn_A/0/1/0/all/0/1\">Anthony Hartshorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saravia_E/0/1/0/all/0/1\">Elvis Saravia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poulton_A/0/1/0/all/0/1\">Andrew Poulton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerkez_V/0/1/0/all/0/1\">Viktor Kerkez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stojnic_R/0/1/0/all/0/1\">Robert Stojnic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting PaLM for Translation: Assessing Strategies and Performance. (arXiv:2211.09102v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09102","description":"<p>Large language models (LLMs) that have been trained on multilingual but not\nparallel text exhibit a remarkable ability to translate between languages. We\nprobe this ability in an in-depth study of the pathways language model (PaLM),\nwhich has demonstrated the strongest machine translation (MT) performance among\nsimilarly-trained LLMs to date. We investigate various strategies for choosing\ntranslation examples for few-shot prompting, concluding that example quality is\nthe most important factor. Using optimized prompts, we revisit previous\nassessments of PaLM's MT capabilities with more recent test sets, modern MT\nmetrics, and human evaluation, and find that its performance, while impressive,\nstill lags that of state-of-the-art supervised systems. We conclude by\nproviding an analysis of PaLM's MT output which reveals some interesting\nproperties and prospects for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vilar_D/0/1/0/all/0/1\">David Vilar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherry_C/0/1/0/all/0/1\">Colin Cherry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiaming Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratnakar_V/0/1/0/all/0/1\">Viresh Ratnakar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_G/0/1/0/all/0/1\">George Foster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Holistic Evaluation of Language Models. (arXiv:2211.09110v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09110","description":"<p>Language models (LMs) are becoming the foundation for almost all major\nlanguage technologies, but their capabilities, limitations, and risks are not\nwell understood. We present Holistic Evaluation of Language Models (HELM) to\nimprove the transparency of language models. First, we taxonomize the vast\nspace of potential scenarios (i.e. use cases) and metrics (i.e. desiderata)\nthat are of interest for LMs. Then we select a broad subset based on coverage\nand feasibility, noting what's missing or underrepresented (e.g. question\nanswering for neglected English dialects, metrics for trustworthiness). Second,\nwe adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration,\nrobustness, fairness, bias, toxicity, and efficiency) for each of 16 core\nscenarios when possible (87.5% of the time). This ensures metrics beyond\naccuracy don't fall to the wayside, and that trade-offs are clearly exposed. We\nalso perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze\nspecific aspects (e.g. reasoning, disinformation). Third, we conduct a\nlarge-scale evaluation of 30 prominent language models (spanning open,\nlimited-access, and closed models) on all 42 scenarios, 21 of which were not\npreviously used in mainstream LM evaluation. Prior to HELM, models on average\nwere evaluated on just 17.9% of the core HELM scenarios, with some prominent\nmodels not sharing a single scenario in common. We improve this to 96.0%: now\nall 30 models have been densely benchmarked on the same core scenarios and\nmetrics under standardized conditions. Our evaluation surfaces 25 top-level\nfindings. For full transparency, we release all raw model prompts and\ncompletions publicly for further analysis, as well as a general modular\ntoolkit. We intend for HELM to be a living benchmark for the community,\ncontinuously updated with new scenarios, metrics, and models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bommasani_R/0/1/0/all/0/1\">Rishi Bommasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tony Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsipras_D/0/1/0/all/0/1\">Dimitris Tsipras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soylu_D/0/1/0/all/0/1\">Dilara Soylu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_D/0/1/0/all/0/1\">Deepak Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhuai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ananya Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newman_B/0/1/0/all/0/1\">Benjamin Newman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Binhang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bobby Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosgrove_C/0/1/0/all/0/1\">Christian Cosgrove</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acosta_Navas_D/0/1/0/all/0/1\">Diana Acosta-Navas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudson_D/0/1/0/all/0/1\">Drew A. Hudson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelikman_E/0/1/0/all/0/1\">Eric Zelikman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ladhak_F/0/1/0/all/0/1\">Faisal Ladhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_F/0/1/0/all/0/1\">Frieda Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Huaxiu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santhanam_K/0/1/0/all/0/1\">Keshav Santhanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orr_L/0/1/0/all/0/1\">Laurel Orr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lucia Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuksekgonul_M/0/1/0/all/0/1\">Mert Yuksekgonul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzgun_M/0/1/0/all/0/1\">Mirac Suzgun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Nathan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guha_N/0/1/0/all/0/1\">Neel Guha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterji_N/0/1/0/all/0/1\">Niladri Chatterji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1\">Omar Khattab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_P/0/1/0/all/0/1\">Peter Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_R/0/1/0/all/0/1\">Ryan Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sang Michael Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santurkar_S/0/1/0/all/0/1\">Shibani Santurkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Surya Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Icard_T/0/1/0/all/0/1\">Thomas Icard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1\">Vishrav Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuechen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_Y/0/1/0/all/0/1\">Yifan Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koreeda_Y/0/1/0/all/0/1\">Yuta Koreeda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Measuring the Intrinsic Few-Shot Hardness of Datasets. (arXiv:2211.09113v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09113","description":"<p>While advances in pre-training have led to dramatic improvements in few-shot\nlearning of NLP tasks, there is limited understanding of what drives successful\nfew-shot adaptation in datasets. In particular, given a new dataset and a\npre-trained model, what properties of the dataset make it \\emph{few-shot\nlearnable} and are these properties independent of the specific adaptation\ntechniques used? We consider an extensive set of recent few-shot learning\nmethods, and show that their performance across a large number of datasets is\nhighly correlated, showing that few-shot hardness may be intrinsic to datasets,\nfor a given pre-trained model. To estimate intrinsic few-shot hardness, we then\npropose a simple and lightweight metric called \"Spread\" that captures the\nintuition that few-shot learning is made possible by exploiting feature-space\ninvariances between training and test samples. Our metric better accounts for\nfew-shot hardness compared to existing notions of hardness, and is ~8-100x\nfaster to compute.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xinran Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murty_S/0/1/0/all/0/1\">Shikhar Murty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back Translation Survey for Improving Text Augmentation. (arXiv:2102.09708v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.09708","description":"<p>Natural Language Processing (NLP) relies heavily on training data.\nTransformers, as they have gotten bigger, have required massive amounts of\ntraining data. To satisfy this requirement, text augmentation should be looked\nat as a way to expand your current dataset and to generalize your models. One\ntext augmentation we will look at is translation augmentation. We take an\nEnglish sentence and translate it to another language before translating it\nback to English. In this paper, we look at the effect of 108 different language\nback translations on various metrics and text embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ciolino_M/0/1/0/all/0/1\">Matthew Ciolino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1\">David Noever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalin_J/0/1/0/all/0/1\">Josh Kalin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Actionable Entities Recognition Benchmark for Interactive Fiction. (arXiv:2109.13855v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13855","description":"<p>This paper presents a new natural language processing task - Actionable\nEntities Recognition (AER) - recognition of entities that protagonists could\ninteract with for further plot development. Though similar to classical Named\nEntity Recognition (NER), it has profound differences. In particular, it is\ncrucial for interactive fiction, where the agent needs to detect entities that\nmight be useful in the future. We also discuss if AER might be further helpful\nfor the systems dealing with narrative processing since actionable entities\nprofoundly impact the causal relationship in a story. We validate the proposed\ntask on two previously available datasets and present a new benchmark dataset\nfor the AER task that includes 5550 descriptions with one or more actionable\nentities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Literature-Augmented Clinical Outcome Prediction. (arXiv:2111.08374v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.08374","description":"<p>We present BEEP (Biomedical Evidence-Enhanced Predictions), a novel approach\nfor clinical outcome prediction that retrieves patient-specific medical\nliterature and incorporates it into predictive models. Based on each individual\npatient's clinical notes, we train language models (LMs) to find relevant\npapers and fuse them with information from notes to predict outcomes such as\nin-hospital mortality. We develop methods to retrieve literature based on\nnoisy, information-dense patient notes, and to augment existing outcome\nprediction models with retrieved papers in a manner that maximizes predictive\naccuracy. Our approach boosts predictive performance on three important\nclinical tasks in comparison to strong recent LM baselines, increasing F1 by up\nto 5 points and precision@Top-K by a large margin of over 25%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1\">Aakanksha Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parasa_S/0/1/0/all/0/1\">Sravanthi Parasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_S/0/1/0/all/0/1\">Sergey Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lucy Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kernelized Concept Erasure. (arXiv:2201.12191v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12191","description":"<p>The representation space of neural models for textual data emerges in an\nunsupervised manner during training. Understanding how those representations\nencode human-interpretable concepts is a fundamental problem. One prominent\napproach for the identification of concepts in neural representations is\nsearching for a linear subspace whose erasure prevents the prediction of the\nconcept from the representations. However, while many linear erasure algorithms\nare tractable and interpretable, neural networks do not necessarily represent\nconcepts in a linear manner. To identify non-linearly encoded concepts, we\npropose a kernelization of a linear minimax game for concept erasure. We\ndemonstrate that it is possible to prevent specific nonlinear adversaries from\npredicting the concept. However, the protection does not transfer to different\nnonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded\nconcept remains an open problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vargas_F/0/1/0/all/0/1\">Francisco Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Has Been Enhanced in my Knowledge-Enhanced Language Model?. (arXiv:2202.00964v7 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00964","description":"<p>Pretrained language models (LMs) do not capture factual knowledge very well.\nThis has led to the development of a number of knowledge integration (KI)\nmethods which aim to incorporate external knowledge into pretrained LMs. Even\nthough KI methods show some performance gains over vanilla LMs, the\ninner-workings of these methods are not well-understood. For instance, it is\nunclear how and what kind of knowledge is effectively integrated into these\nmodels and if such integration may lead to catastrophic forgetting of already\nlearned knowledge. This paper revisits the KI process in these models with an\ninformation-theoretic view and shows that KI can be interpreted using a graph\nconvolution operation. We propose a probe model called \\textit{Graph\nConvolution Simulator} (GCS) for interpreting knowledge-enhanced LMs and\nexposing what kind of knowledge is integrated into these models. We conduct\nexperiments to verify that our GCS can indeed be used to correctly interpret\nthe KI process, and we use it to analyze two well-known knowledge-enhanced LMs:\nERNIE and K-Adapter, and find that only a small amount of factual knowledge is\nintegrated in them. We stratify knowledge in terms of various relation types\nand find that ERNIE and K-Adapter integrate different kinds of knowledge to\ndifferent extent. Our analysis also shows that simply increasing the size of\nthe KI corpus may not lead to better KI; fundamental advances may be needed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yifan Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1\">Guoji Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LISA: Learning Interpretable Skill Abstractions from Language. (arXiv:2203.00054v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.00054","description":"<p>Learning policies that effectively utilize language instructions in complex,\nmulti-task environments is an important problem in imitation learning. While it\nis possible to condition on the entire language instruction directly, such an\napproach could suffer from generalization issues. To encode complex\ninstructions into skills that can generalize to unseen instructions, we propose\nLearning Interpretable Skill Abstractions (LISA), a hierarchical imitation\nlearning framework that can learn diverse, interpretable skills from\nlanguage-conditioned demonstrations. LISA uses vector quantization to learn\ndiscrete skill codes that are highly correlated with language instructions and\nthe behavior of the learned policy. In navigation and robotic manipulation\nenvironments, LISA outperforms a strong non-hierarchical baseline in the low\ndata regime and is able to compose learned skills to solve tasks containing\nunseen long-range instructions. Our method demonstrates a more natural way to\ncondition on language in sequential decision-making problems and achieve\ninterpretable and controllable behavior with the learned skills.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_D/0/1/0/all/0/1\">Divyansh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaidyanath_S/0/1/0/all/0/1\">Skanda Vaidyanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kuno Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiaming Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language and Culture Internalisation for Human-Like Autotelic AI. (arXiv:2206.01134v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2206.01134","description":"<p>Building autonomous agents able to grow open-ended repertoires of skills\nacross their lives is a fundamental goal of artificial intelligence (AI). A\npromising developmental approach recommends the design of intrinsically\nmotivated agents that learn new skills by generating and pursuing their own\ngoals - autotelic agents. But despite recent progress, existing algorithms\nstill show serious limitations in terms of goal diversity, exploration,\ngeneralisation or skill composition. This perspective calls for the immersion\nof autotelic agents into rich socio-cultural worlds, an immensely important\nattribute of our environment that shapes human cognition but is mostly omitted\nin modern AI. Inspired by the seminal work of Vygotsky, we propose Vygotskian\nautotelic agents - agents able to internalise their interactions with others\nand turn them into cognitive tools. We focus on language and show how its\nstructure and informational content may support the development of new\ncognitive functions in artificial agents as it does in humans. We justify the\napproach by uncovering several examples of new artificial cognitive functions\nemerging from interactions between language and embodiment in recent works at\nthe intersection of deep reinforcement learning and natural language\nprocessing. Looking forward, we highlight future opportunities and challenges\nfor Vygotskian Autotelic AI research, including the use of language models as\ncultural models supporting artificial cognitive development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colas_C/0/1/0/all/0/1\">C&#xe9;dric Colas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karch_T/0/1/0/all/0/1\">Tristan Karch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moulin_Frier_C/0/1/0/all/0/1\">Cl&#xe9;ment Moulin-Frier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Mandarin Speech Recogntion with Block-augmented Transformer. (arXiv:2207.11697v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.11697","description":"<p>Recently Convolution-augmented Transformer (Conformer) has shown promising\nresults in Automatic Speech Recognition (ASR), outperforming the previous best\npublished Transformer Transducer. In this work, we believe that the output\ninformation of each block in the encoder and decoder is not completely\ninclusive, in other words, their output information may be complementary. We\nstudy how to take advantage of the complementary information of each block in a\nparameter-efficient way, and it is expected that this may lead to more robust\nperformance. Therefore we propose the Block-augmented Transformer for speech\nrecognition, named Blockformer. We have implemented two block ensemble methods:\nthe base Weighted Sum of the Blocks Output (Base-WSBO), and the\nSqueeze-and-Excitation module to Weighted Sum of the Blocks Output (SE-WSBO).\nExperiments have proved that the Blockformer significantly outperforms the\nstate-of-the-art Conformer-based models on AISHELL-1, our model achieves a CER\nof 4.29\\% without using a language model and 4.05\\% with an external language\nmodel on the testset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaoming Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Huifeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Liuwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minghui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1\">Jie Hao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models and the Reverse Turing Test. (arXiv:2207.14382v9 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.14382","description":"<p>Large Language Models (LLMs) have been transformative. They are pre-trained\nfoundational models that are self-supervised and can be adapted with fine\ntuning to a wide range of natural language tasks, each of which previously\nwould have required a separate network model. This is one step closer to the\nextraordinary versatility of human language. GPT-3 and more recently LaMDA can\ncarry on dialogs with humans on many topics after minimal priming with a few\nexamples. However, there has been a wide range of reactions and debate on\nwhether these LLMs understand what they are saying or exhibit signs of\nintelligence. This high variance is exhibited in three interviews with LLMs\nreaching wildly different conclusions. A new possibility was uncovered that\ncould explain this divergence. What appears to be intelligence in LLMs may in\nfact be a mirror that reflects the intelligence of the interviewer, a\nremarkable twist that could be considered a Reverse Turing Test. If so, then by\nstudying interviews we may be learning more about the intelligence and beliefs\nof the interviewer than the intelligence of the LLMs. As LLMs become more\ncapable they may transform the way we interact with machines and how they\ninteract with each other. Increasingly, LLMs are being coupled with\nsensorimotor devices. LLMs can talk the talk, but can they walk the walk? A\nroad map for achieving artificial general autonomy is outlined with seven major\nimprovements inspired by brain systems. LLMs could be used to uncover new\ninsights into brain function by downloading brain data during natural\nbehaviors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sejnowski_T/0/1/0/all/0/1\">Terrence Sejnowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Atlas: Few-shot Learning with Retrieval Augmented Language Models. (arXiv:2208.03299v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.03299","description":"<p>Large language models have shown impressive few-shot results on a wide range\nof tasks. However, when knowledge is key for such results, as is the case for\ntasks such as question answering and fact checking, massive parameter counts to\nstore knowledge seem to be needed. Retrieval augmented models are known to\nexcel at knowledge intensive tasks without the need for as many parameters, but\nit is unclear whether they work in few-shot settings. In this work we present\nAtlas, a carefully designed and pre-trained retrieval augmented language model\nable to learn knowledge intensive tasks with very few training examples. We\nperform evaluations on a wide range of tasks, including MMLU, KILT and\nNaturalQuestions, and study the impact of the content of the document index,\nshowing that it can easily be updated. Notably, Atlas reaches over 42% accuracy\non Natural Questions using only 64 examples, outperforming a 540B parameters\nmodel by 3% despite having 50x fewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Izacard_G/0/1/0/all/0/1\">Gautier Izacard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomeli_M/0/1/0/all/0/1\">Maria Lomeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_L/0/1/0/all/0/1\">Lucas Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1\">Fabio Petroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schick_T/0/1/0/all/0/1\">Timo Schick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwivedi_Yu_J/0/1/0/all/0/1\">Jane Dwivedi-Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grave_E/0/1/0/all/0/1\">Edouard Grave</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting Pretrained Text-to-Text Models for Long Text Sequences. (arXiv:2209.10052v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.10052","description":"<p>We present an empirical study of adapting an existing pretrained text-to-text\nmodel for long-sequence inputs. Through a comprehensive study along three axes\nof the pretraining pipeline -- model architecture, optimization objective, and\npretraining corpus, we propose an effective recipe to build long-context models\nfrom existing short-context models. Specifically, we replace the full attention\nin transformers with pooling-augmented blockwise attention, and pretrain the\nmodel with a masked-span prediction task with spans of varying length. In terms\nof the pretraining corpus, we find that using randomly concatenated\nshort-documents from a large open-domain corpus results in better performance\nthan using existing long document corpora which are typically limited in their\ndomain coverage. With these findings, we build a long-context model that\nachieves competitive performance on long-text QA tasks and establishes the new\nstate of the art on five long-text summarization datasets, often outperforming\nprevious methods with larger model sizes. Our code has been released at\nhttps://github.com/facebookresearch/bart_ls.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Wenhan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anchit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toshniwal_S/0/1/0/all/0/1\">Shubham Toshniwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emb-GAM: an Interpretable and Efficient Predictor using Pre-trained Language Models. (arXiv:2209.11799v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2209.11799","description":"<p>Deep learning models have achieved impressive prediction performance but\noften sacrifice interpretability, a critical consideration in high-stakes\ndomains such as healthcare or policymaking. In contrast, generalized additive\nmodels (GAMs) can maintain interpretability but often suffer from poor\nprediction performance due to their inability to effectively capture feature\ninteractions. In this work, we aim to bridge this gap by using pre-trained\nneural language models to extract embeddings for each input before learning a\nlinear model in the embedding space. The final model (which we call Emb-GAM) is\na transparent, linear function of its input features and feature interactions.\nLeveraging the language model allows Emb-GAM to learn far fewer linear\ncoefficients, model larger interactions, and generalize well to novel inputs\n(e.g. unseen ngrams in text). Across a variety of natural-language-processing\ndatasets, Emb-GAM achieves strong prediction performance without sacrificing\ninterpretability. All code is made available on Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1\">Chandan Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding COVID-19 Vaccine Campaign on Facebook using Minimal Supervision. (arXiv:2210.10031v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10031","description":"<p>In the age of social media, where billions of internet users share\ninformation and opinions, the negative impact of pandemics is not limited to\nthe physical world. It provokes a surge of incomplete, biased, and incorrect\ninformation, also known as an infodemic. This global infodemic jeopardizes\nmeasures to control the pandemic by creating panic, vaccine hesitancy, and\nfragmented social response. Platforms like Facebook allow advertisers to adapt\ntheir messaging to target different demographics and help alleviate or\nexacerbate the infodemic problem depending on their content. In this paper, we\npropose a minimally supervised multi-task learning framework for understanding\nmessaging on Facebook related to the COVID vaccine by identifying ad themes and\nmoral foundations. Furthermore, we perform a more nuanced thematic analysis of\nmessaging tactics of vaccine campaigns on social media so that policymakers can\nmake better decisions on pandemic control.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tunazzina Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transcending Scaling Laws with 0.1% Extra Compute. (arXiv:2210.11399v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.11399","description":"<p>Scaling language models improves performance but comes with significant\ncomputational costs. This paper proposes UL2R, a method that substantially\nimproves existing language models and their scaling curves with a relatively\ntiny amount of extra compute. The key idea is to continue training a\nstate-of-the-art large language model (e.g., PaLM) on a few more steps with\nUL2's mixture-of-denoiser objective. We show that, with almost negligible extra\ncomputational costs and no new sources of data, we are able to substantially\nimprove the scaling properties of large language models on downstream metrics.\nIn this paper, we continue training PaLM with UL2R, introducing a new set of\nmodels at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B\nscale, we show an approximately 2x computational savings rate where U-PaLM\nachieves the same performance as the final PaLM 540B model at around half its\ncomputational budget (i.e., saving $\\sim$4.4 million TPUv4 hours). We further\nshow that this improved scaling curve leads to 'emergent abilities' on\nchallenging BIG-Bench tasks -- for instance, U-PaLM does much better than PaLM\non some tasks or demonstrates better quality at much smaller scale (62B as\nopposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many\nfew-shot setups, i.e., English NLP tasks (e.g., commonsense reasoning, question\nanswering), reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual\ntasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks. Finally, we provide\nqualitative examples showing the new capabilities of U-PaLM for single and\nmulti-span infilling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+So_D/0/1/0/all/0/1\">David R. So</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakeri_S/0/1/0/all/0/1\">Siamak Shakeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huaixiu Steven Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jinfeng Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhery_A/0/1/0/all/0/1\">Aakanksha Chowdhery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrov_S/0/1/0/all/0/1\">Slav Petrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Instruction-Finetuned Language Models. (arXiv:2210.11416v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.11416","description":"<p>Finetuning language models on a collection of datasets phrased as\ninstructions has been shown to improve model performance and generalization to\nunseen tasks. In this paper we explore instruction finetuning with a particular\nfocus on (1) scaling the number of tasks, (2) scaling the model size, and (3)\nfinetuning on chain-of-thought data. We find that instruction finetuning with\nthe above aspects dramatically improves performance on a variety of model\nclasses (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and\nevaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For\ninstance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM\n540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves\nstate-of-the-art performance on several benchmarks, such as 75.2% on five-shot\nMMLU. We also publicly release Flan-T5 checkpoints, which achieve strong\nfew-shot performance even compared to much larger models, such as PaLM 62B.\nOverall, instruction finetuning is a general method for improving the\nperformance and usability of pretrained language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Le Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1\">Barret Zoph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_W/0/1/0/all/0/1\">William Fedus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahma_S/0/1/0/all/0/1\">Siddhartha Brahma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shixiang Shane Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zhuyun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzgun_M/0/1/0/all/0/1\">Mirac Suzgun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhery_A/0/1/0/all/0/1\">Aakanksha Chowdhery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_G/0/1/0/all/0/1\">Gaurav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_V/0/1/0/all/0/1\">Vincent Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrov_S/0/1/0/all/0/1\">Slav Petrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed H. Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dean_J/0/1/0/all/0/1\">Jeff Dean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devlin_J/0/1/0/all/0/1\">Jacob Devlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Large Language Models for Multiple Choice Question Answering. (arXiv:2210.12353v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12353","description":"<p>While large language models (LLMs) like GPT-3 have achieved impressive\nresults on multiple choice question answering (MCQA) tasks in the zero, one,\nand few-shot settings, they generally lag behind the MCQA state of the art\n(SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks.\nAn LLM is conditioned on a question (without the associated answer options) and\nits chosen option is the one assigned the highest probability after\nnormalization (for length, etc.). A more natural prompting approach is to\npresent the question and answer options to the LLM jointly and have it output\nthe symbol (e.g., \"A\") associated with its chosen answer option. This approach\nallows the model to explicitly compare answer options, reduces computational\ncosts, and mitigates the effects of tokenization scheme and answer option\nrepresentations on answer selection. For the natural approach to be effective\nthe LLM it is used with must be able to associate answer options with the\nsymbols that represent them. The LLM needs what we term multiple choice symbol\nbinding (MCSB) ability. This ability varies greatly by model. We show that a\nmodel with high MCSB ability performs much better with the natural approach\nthan with the traditional approach across 20 diverse datasets and largely\ncloses the gap with the SOTA, suggesting that the MCQA ability of LLMs has been\npreviously underestimated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1\">Joshua Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rytting_C/0/1/0/all/0/1\">Christopher Michael Rytting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wingate_D/0/1/0/all/0/1\">David Wingate</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MasakhaNER 2.0: Africa-centric Transfer Learning for Named Entity Recognition. (arXiv:2210.12391v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12391","description":"<p>African languages are spoken by over a billion people, but are\nunderrepresented in NLP research and development. The challenges impeding\nprogress include the limited availability of annotated datasets, as well as a\nlack of understanding of the settings where current methods are effective. In\nthis paper, we make progress towards solutions for these challenges, focusing\non the task of named entity recognition (NER). We create the largest\nhuman-annotated NER dataset for 20 African languages, and we study the behavior\nof state-of-the-art cross-lingual transfer methods in an Africa-centric\nsetting, demonstrating that the choice of source language significantly affects\nperformance. We show that choosing the best transfer language improves\nzero-shot F1 scores by an average of 14 points across 20 languages compared to\nusing English. Our results highlight the need for benchmark datasets and models\nthat cover typologically-diverse African languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijhwani_S/0/1/0/all/0/1\">Shruti Rijhwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beukman_M/0/1/0/all/0/1\">Michael Beukman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palen_Michel_C/0/1/0/all/0/1\">Chester Palen-Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lignos_C/0/1/0/all/0/1\">Constantine Lignos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba O. Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen H. Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabende_P/0/1/0/all/0/1\">Peter Nabende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dione_C/0/1/0/all/0/1\">Cheikh M. Bamba Dione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bukula_A/0/1/0/all/0/1\">Andiswa Bukula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mabuya_R/0/1/0/all/0/1\">Rooweither Mabuya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dossou_B/0/1/0/all/0/1\">Bonaventure F. P. Dossou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sibanda_B/0/1/0/all/0/1\">Blessing Sibanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buzaaba_H/0/1/0/all/0/1\">Happy Buzaaba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukiibi_J/0/1/0/all/0/1\">Jonathan Mukiibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalipe_G/0/1/0/all/0/1\">Godson Kalipe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mbaye_D/0/1/0/all/0/1\">Derguene Mbaye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_A/0/1/0/all/0/1\">Amelia Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabore_F/0/1/0/all/0/1\">Fatoumata Kabore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Chinenye Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aremu_A/0/1/0/all/0/1\">Anuoluwapo Aremu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogayo_P/0/1/0/all/0/1\">Perez Ogayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gitau_C/0/1/0/all/0/1\">Catherine Gitau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munkoh_Buabeng_E/0/1/0/all/0/1\">Edwin Munkoh-Buabeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koagne_V/0/1/0/all/0/1\">Victoire M. Koagne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tapo_A/0/1/0/all/0/1\">Allahsera Auguste Tapo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macucwa_T/0/1/0/all/0/1\">Tebogo Macucwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1\">Vukosi Marivate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mboning_E/0/1/0/all/0/1\">Elvis Mboning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwadabe_T/0/1/0/all/0/1\">Tajuddeen Gwadabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1\">Tosin Adewumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahia_O/0/1/0/all/0/1\">Orevaoghene Ahia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakatumba_Nabende_J/0/1/0/all/0/1\">Joyce Nakatumba-Nabende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokono_N/0/1/0/all/0/1\">Neo L. Mokono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ezeani_I/0/1/0/all/0/1\">Ignatius Ezeani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chukwuneke_C/0/1/0/all/0/1\">Chiamaka Chukwuneke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeyemi_M/0/1/0/all/0/1\">Mofetoluwa Adeyemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hacheme_G/0/1/0/all/0/1\">Gilles Q. Hacheme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdulmumin_I/0/1/0/all/0/1\">Idris Abdulmumin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogundepo_O/0/1/0/all/0/1\">Odunayo Ogundepo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yousuf_O/0/1/0/all/0/1\">Oreen Yousuf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngoli_T/0/1/0/all/0/1\">Tatiana Moteu Ngoli</a>, et al. (1 additional author not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KnowGL: Knowledge Generation and Linking from Text. (arXiv:2210.13952v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13952","description":"<p>We propose KnowGL, a tool that allows converting text into structured\nrelational data represented as a set of ABox assertions compliant with the TBox\nof a given Knowledge Graph (KG), such as Wikidata. We address this problem as a\nsequence generation task by leveraging pre-trained sequence-to-sequence\nlanguage models, e.g. BART. Given a sentence, we fine-tune such models to\ndetect pairs of entity mentions and jointly generate a set of facts consisting\nof the full set of semantic annotations for a KG, such as entity labels, entity\ntypes, and their relationships. To showcase the capabilities of our tool, we\nbuild a web application consisting of a set of UI widgets that help users to\nnavigate through the semantic data extracted from a given input text. We make\nthe KnowGL model available at https://huggingface.co/ibm/knowgl-large.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1\">Gaetano Rossiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1\">Md. Mahbub Faisal Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1\">Nandana Mihindukulasooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornec_O/0/1/0/all/0/1\">Owen Cornec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Massimiliano Gliozzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CCS Explorer: Relevance Prediction, Extractive Summarization, and Named Entity Recognition from Clinical Cohort Studies. (arXiv:2211.00201v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.00201","description":"<p>Clinical Cohort Studies (CCS), such as randomized clinical trials, are a\ngreat source of documented clinical research. Ideally, a clinical expert\ninspects these articles for exploratory analysis ranging from drug discovery\nfor evaluating the efficacy of existing drugs in tackling emerging diseases to\nthe first test of newly developed drugs. However, more than 100 articles are\npublished daily on a single prevalent disease like COVID-19 in PubMed. As a\nresult, it can take days for a physician to find articles and extract relevant\ninformation. Can we develop a system to sift through the long list of these\narticles faster and document the crucial takeaways from each of these articles?\nIn this work, we propose CCS Explorer, an end-to-end system for relevance\nprediction of sentences, extractive summarization, and patient, outcome, and\nintervention entity detection from CCS. CCS Explorer is packaged in a web-based\ngraphical user interface where the user can provide any disease name. CCS\nExplorer then extracts and aggregates all relevant information from articles on\nPubMed based on the results of an automatically generated query produced on the\nback-end. For each task, CCS Explorer fine-tunes pre-trained language\nrepresentation models based on transformers with additional layers. The models\nare evaluated using two publicly available datasets. CCS Explorer obtains a\nrecall of 80.2%, AUC-ROC of 0.843, and an accuracy of 88.3% on sentence\nrelevance prediction using BioBERT and achieves an average Micro F1-Score of\n77.8% on Patient, Intervention, Outcome detection (PIO) using PubMedBERT. Thus,\nCCS Explorer can reliably extract relevant information to summarize articles,\nsaving time by $\\sim \\text{660}\\times$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_Hussaini_I/0/1/0/all/0/1\">Irfan Al-Hussaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1\">Davi Nakajima An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Albert J. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1\">Sarah Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_C/0/1/0/all/0/1\">Cassie S. Mitchell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grammatical Error Correction: A Survey of the State of the Art. (arXiv:2211.05166v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05166","description":"<p>Grammatical Error Correction (GEC) is the task of automatically detecting and\ncorrecting errors in text. The task not only includes the correction of\ngrammatical errors, such as missing prepositions and mismatched subject-verb\nagreement, but also orthographic and semantic errors, such as misspellings and\nword choice errors respectively. The field has seen significant progress in the\nlast decade, motivated in part by a series of five shared tasks, which drove\nthe development of rule-based methods, statistical classifiers, statistical\nmachine translation, and finally neural machine translation systems which\nrepresent the current dominant state of the art. In this survey paper, we\ncondense the field into a single article and first outline some of the\nlinguistic challenges of the task, introduce the most popular datasets that are\navailable to researchers (for both English and other languages), and summarise\nthe various methods and techniques that have been developed with a particular\nfocus on artificial error generation. We next describe the many different\napproaches to evaluation as well as concerns surrounding metric reliability,\nespecially in relation to subjective human judgements, before concluding with\nan overview of recent progress and suggestions for future work and remaining\nchallenges. We hope that this survey will serve as comprehensive resource for\nresearchers who are new to the field or who want to be kept apprised of recent\ndevelopments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bryant_C/0/1/0/all/0/1\">Christopher Bryant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qorib_M/0/1/0/all/0/1\">Muhammad Reza Qorib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Hannan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_H/0/1/0/all/0/1\">Hwee Tou Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briscoe_T/0/1/0/all/0/1\">Ted Briscoe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation. (arXiv:2211.05719v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05719","description":"<p>Responding with multi-modal content has been recognized as an essential\ncapability for an intelligent conversational agent. In this paper, we introduce\nthe MMDialog dataset to better facilitate multi-modal conversation. MMDialog is\ncomposed of a curated set of 1.08 million real-world dialogues with 1.53\nmillion unique images across 4,184 topics. MMDialog has two main and unique\nadvantages. First, it is the largest multi-modal conversation dataset by the\nnumber of dialogues by 88x. Second, it contains massive topics to generalize\nthe open-domain. To build engaging dialogue system with this dataset, we\npropose and normalize two response producing tasks based on retrieval and\ngenerative scenarios. In addition, we build two baselines for above tasks with\nstate-of-the-art techniques and report their experimental performance. We also\npropose a novel evaluation metric MM-Relevance to measure the multi-modal\nresponses. Our dataset and scripts are available in\nhttps://github.com/victorsungo/MMDialog.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiazhan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qingfeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qingwei Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model Classifier Aligns Better with Physician Word Sensitivity than XGBoost on Readmission Prediction. (arXiv:2211.07047v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07047","description":"<p>Traditional evaluation metrics for classification in natural language\nprocessing such as accuracy and area under the curve fail to differentiate\nbetween models with different predictive behaviors despite their similar\nperformance metrics. We introduce sensitivity score, a metric that scrutinizes\nmodels' behaviors at the vocabulary level to provide insights into disparities\nin their decision-making logic. We assess the sensitivity score on a set of\nrepresentative words in the test set using two classifiers trained for hospital\nreadmission classification with similar performance statistics. Our experiments\ncompare the decision-making logic of clinicians and classifiers based on rank\ncorrelations of sensitivity scores. The results indicate that the language\nmodel's sensitivity score aligns better with the professionals than the xgboost\nclassifier on tf-idf embeddings, which suggests that xgboost uses some spurious\nfeatures. Overall, this metric offers a novel perspective on assessing models'\nrobustness by quantifying their discrepancy with professional opinions. Our\ncode is available on GitHub (https://github.com/nyuolab/Model_Sensitivity).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Grace Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Ming Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lavender Y. Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xujin C. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_A/0/1/0/all/0/1\">Alexander T.M. Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_H/0/1/0/all/0/1\">Hannah Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurland_D/0/1/0/all/0/1\">David Kurland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oermann_E/0/1/0/all/0/1\">Eric K. Oermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaptKeyBERT: An Attention-Based approach towards Few-Shot & Zero-Shot Domain Adaptation of KeyBERT. (arXiv:2211.07499v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07499","description":"<p>Keyword extraction has been an important topic for modern natural language\nprocessing. With its applications ranging from ontology generation, fact\nverification in summarized text, and recommendation systems. While it has had\nsignificant data-intensive applications, it is often hampered when the data set\nis small. Downstream training for keyword extractors is a lengthy process and\nrequires a significant amount of data. Recently, Few-shot Learning (FSL) and\nZero-Shot Learning (ZSL) have been proposed to tackle this problem. Therefore,\nwe propose AdaptKeyBERT, a pipeline for training keyword extractors with LLM\nbases by incorporating the concept of regularized attention into a pre-training\nphase for downstream domain adaptation. As we believe our work has implications\nto be utilized in the pipeline of FSL/ZSL and keyword extraction, we\nopen-source our code as well as provide the fine-tuning library of the same\nname AdaptKeyBERT at https://github.com/AmanPriyanshu/AdaptKeyBERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Priyanshu_A/0/1/0/all/0/1\">Aman Priyanshu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijay_S/0/1/0/all/0/1\">Supriti Vijay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilevel Transformer For Multimodal Emotion Recognition. (arXiv:2211.07711v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07711","description":"<p>Multimodal emotion recognition has attracted much attention recently. Fusing\nmultiple modalities effectively with limited labeled data is a challenging\ntask. Considering the success of pre-trained model and fine-grained nature of\nemotion expression, it is reasonable to take these two aspects into\nconsideration. Unlike previous methods that mainly focus on one aspect, we\nintroduce a novel multi-granularity framework, which combines fine-grained\nrepresentation with pre-trained utterance-level representation. Inspired by\nTransformer TTS, we propose a multilevel transformer model to perform\nfine-grained multimodal emotion recognition. Specifically, we explore different\nmethods to incorporate phoneme-level embedding with word-level embedding. To\nperform multi-granularity learning, we simply combine multilevel transformer\nmodel with Albert. Extensive experimental results show that both our multilevel\ntransformer model and multi-granularity model outperform previous\nstate-of-the-art approaches on IEMOCAP dataset with text transcripts and speech\nsignal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junyi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Meimei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Meng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaobo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Feng Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Phrase-based Sequence-to-Sequence Learning. (arXiv:2211.07906v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07906","description":"<p>We describe a neural transducer that maintains the flexibility of standard\nsequence-to-sequence (seq2seq) models while incorporating hierarchical phrases\nas a source of inductive bias during training and as explicit constraints\nduring inference. Our approach trains two models: a discriminative parser based\non a bracketing transduction grammar whose derivation tree hierarchically\naligns source and target phrases, and a neural seq2seq model that learns to\ntranslate the aligned phrases one-by-one. We use the same seq2seq model to\ntranslate at all phrase scales, which results in two inference modes: one mode\nin which the parser is discarded and only the seq2seq component is used at the\nsequence-level, and another in which the parser is combined with the seq2seq\nmodel. Decoding in the latter mode is done with the cube-pruned CKY algorithm,\nwhich is more involved but can make use of new translation rules during\ninference. We formalize our model as a source-conditioned synchronous grammar\nand develop an efficient variational inference algorithm for training. When\napplied on top of both randomly initialized and pretrained seq2seq models, we\nfind that both inference modes performs well compared to baselines on small\nscale machine translation benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bailin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Speech Emotion Recognition With Multi-Gating Mechanism and Neural Architecture Search. (arXiv:2211.08237v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2211.08237","description":"<p>Speech emotion recognition (SER) classifies audio into emotion categories\nsuch as Happy, Angry, Fear, Disgust and Neutral. While Speech Emotion\nRecognition (SER) is a common application for popular languages, it continues\nto be a problem for low-resourced languages, i.e., languages with no pretrained\nspeech-to-text recognition models. This paper firstly proposes a\nlanguage-specific model that extract emotional information from multiple\npre-trained speech models, and then designs a multi-domain model that\nsimultaneously performs SER for various languages. Our multidomain model\nemploys a multi-gating mechanism to generate unique weighted feature\ncombination for each language, and also searches for specific neural network\nstructure for each language through a neural architecture search module. In\naddition, we introduce a contrastive auxiliary loss to build more separable\nrepresentations for audio data. Our experiments show that our model raises the\nstate-of-the-art accuracy by 3% for German and 14.3% for French.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Qi Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_H/0/1/0/all/0/1\">HaiFeng Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">XinRui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1\">KeHao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akshat Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-11-16T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}}]}]}