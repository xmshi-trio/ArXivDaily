{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-04-07T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"To Asymmetry and Beyond: Structured Pruning of Sequence to Sequence Models for Improved Inference Efficiency. (arXiv:2304.02721v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02721","description":"<p>Sequence-to-sequence language models can be used to produce abstractive\nsummaries which are coherent, relevant, and concise. Still, model sizes can\nmake deployment in latency-sensitive or web-scale implementations difficult.\nThis paper studies the relationship between model size, structured pruning,\ninference efficiency, and summarization accuracy on widely used summarization\ndatasets. We show that model accuracy is tied to the encoder size while\ninference efficiency is connected to the decoder. Using asymmetric pruning can\nlead to nearly 3x improvement in inference latency with ~1 point loss in\nRouge-2. Moreover, we find both the average degradation and the role of\nasymmetry to be consistent across model sizes and variations in datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Campos_D/0/1/0/all/0/1\">Daniel Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Core Challenges in Embodied Vision-Language Planning. (arXiv:2304.02738v1 [cs.RO])","link":"http://arxiv.org/abs/2304.02738","description":"<p>Recent advances in the areas of Multimodal Machine Learning and Artificial\nIntelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Robotics.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly leverage computer vision and natural language for\ninteraction in physical environments. We propose a taxonomy to unify these\ntasks and provide an in-depth analysis and comparison of the current and new\nalgorithmic approaches, metrics, simulators, and datasets used for EVLP tasks.\nFinally, we present the core challenges that we believe new EVLP works should\nseek to address, and we advocate for task construction that enables model\ngeneralisability and furthers real-world deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bengali Fake Review Detection using Semi-supervised Generative Adversarial Networks. (arXiv:2304.02739v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02739","description":"<p>This paper investigates the potential of semi-supervised Generative\nAdversarial Networks (GANs) to fine-tune pretrained language models in order to\nclassify Bengali fake reviews from real reviews with a few annotated data. With\nthe rise of social media and e-commerce, the ability to detect fake or\ndeceptive reviews is becoming increasingly important in order to protect\nconsumers from being misled by false information. Any machine learning model\nwill have trouble identifying a fake review, especially for a low resource\nlanguage like Bengali. We have demonstrated that the proposed semi-supervised\nGAN-LM architecture (generative adversarial network on top of a pretrained\nlanguage model) is a viable solution in classifying Bengali fake reviews as the\nexperimental results suggest that even with only 1024 annotated samples,\nBanglaBERT with semi-supervised GAN (SSGAN) achieved an accuracy of 83.59% and\na f1-score of 84.89% outperforming other pretrained language models -\nBanglaBERT generator, Bangla BERT Base and Bangla-Electra by almost 3%, 4% and\n10% respectively in terms of accuracy. The experiments were conducted on a\nmanually labeled food review dataset consisting of total 6014 real and fake\nreviews collected from various social media groups. Researchers that are\nexperiencing difficulty recognizing not just fake reviews but other\nclassification issues owing to a lack of labeled data may find a solution in\nour proposed methodology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shawon_M/0/1/0/all/0/1\">Md. Tanvir Rouf Shawon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahariar_G/0/1/0/all/0/1\">G. M. Shahariar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_F/0/1/0/all/0/1\">Faisal Muhammad Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mohammad Shafiul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahbub_M/0/1/0/all/0/1\">Md. Shahriar Mahbub</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sejarah dan Perkembangan Teknik Natural Language Processing (NLP) Bahasa Indonesia: Tinjauan tentang sejarah, perkembangan teknologi, dan aplikasi NLP dalam bahasa Indonesia. (arXiv:2304.02746v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02746","description":"<p>This study provides an overview of the history of the development of Natural\nLanguage Processing (NLP) in the context of the Indonesian language, with a\nfocus on the basic technologies, methods, and practical applications that have\nbeen developed. This review covers developments in basic NLP technologies such\nas stemming, part-of-speech tagging, and related methods; practical\napplications in cross-language information retrieval systems, information\nextraction, and sentiment analysis; and methods and techniques used in\nIndonesian language NLP research, such as machine learning, statistics-based\nmachine translation, and conflict-based approaches. This study also explores\nthe application of NLP in Indonesian language industry and research and\nidentifies challenges and opportunities in Indonesian language NLP research and\ndevelopment. Recommendations for future Indonesian language NLP research and\ndevelopment include developing more efficient methods and technologies,\nexpanding NLP applications, increasing sustainability, further research into\nthe potential of NLP, and promoting interdisciplinary collaboration. It is\nhoped that this review will help researchers, practitioners, and the government\nto understand the development of Indonesian language NLP and identify\nopportunities for further research and development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amien_M/0/1/0/all/0/1\">Mukhlis Amien</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Behavioral estimates of conceptual structure are robust across tasks in humans but not large language models. (arXiv:2304.02754v1 [cs.AI])","link":"http://arxiv.org/abs/2304.02754","description":"<p>Neural network models of language have long been used as a tool for\ndeveloping hypotheses about conceptual representation in the mind and brain.\nFor many years, such use involved extracting vector-space representations of\nwords and using distances among these to predict or understand human behavior\nin various semantic tasks. In contemporary language AIs, however, it is\npossible to interrogate the latent structure of conceptual representations\nusing methods nearly identical to those commonly used with human participants.\nThe current work uses two common techniques borrowed from cognitive psychology\nto estimate and compare lexical-semantic structure in both humans and a\nwell-known AI, the DaVinci variant of GPT-3. In humans, we show that conceptual\nstructure is robust to differences in culture, language, and method of\nestimation. Structures estimated from AI behavior, while individually fairly\nconsistent with those estimated from human behavior, depend much more upon the\nparticular task used to generate behavior responses--responses generated by the\nvery same model in the two tasks yield estimates of conceptual structure that\ncohere less with one another than do human structure estimates. The results\nsuggest one important way that knowledge inhering in contemporary AIs can\ndiffer from human cognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suresh_S/0/1/0/all/0/1\">Siddharth Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padua_L/0/1/0/all/0/1\">Lisa Padua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_K/0/1/0/all/0/1\">Kushin Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_T/0/1/0/all/0/1\">Timothy T Rogers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Saudi Privacy Policy Dataset. (arXiv:2304.02757v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02757","description":"<p>This paper introduces the Saudi Privacy Policy Dataset, a diverse compilation\nof Arabic privacy policies from various sectors in Saudi Arabia, annotated\naccording to the 10 principles of the Personal Data Protection Law (PDPL); the\nPDPL was established to be compatible with General Data Protection Regulation\n(GDPR); one of the most comprehensive data regulations worldwide. Data were\ncollected from multiple sources, including the Saudi Central Bank, the Saudi\nArabia National United Platform, the Council of Health Insurance, and general\nwebsites using Google and Wikipedia. The final dataset includes 1,000 websites\nbelonging to 7 sectors, 4,638 lines of text, 775,370 tokens, and a corpus size\nof 8,353 KB. The annotated dataset offers significant reuse potential for\nassessing privacy policy compliance, benchmarking privacy practices across\nindustries, and developing automated tools for monitoring adherence to data\nprotection regulations. By providing a comprehensive and annotated dataset of\nprivacy policies, this paper aims to facilitate further research and\ndevelopment in the areas of privacy policy analysis, natural language\nprocessing, and machine learning applications related to privacy and data\nprotection, while also serving as an essential resource for researchers,\npolicymakers, and industry professionals interested in understanding and\npromoting compliance with privacy regulations in Saudi Arabia.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_Khalifa_H/0/1/0/all/0/1\">Hend Al-Khalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mashaabi_M/0/1/0/all/0/1\">Malak Mashaabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Yahya_G/0/1/0/all/0/1\">Ghadi Al-Yahya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alnashwan_R/0/1/0/all/0/1\">Raghad Alnashwan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Application of Transformers based methods in Electronic Medical Records: A Systematic Literature Review. (arXiv:2304.02768v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02768","description":"<p>The combined growth of available data and their unstructured nature has\nreceived increased interest in natural language processing (NLP) techniques to\nmake value of these data assets since this format is not suitable for\nstatistical analysis. This work presents a systematic literature review of\nstate-of-the-art advances using transformer-based methods on electronic medical\nrecords (EMRs) in different NLP tasks. To the best of our knowledge, this work\nis unique in providing a comprehensive review of research on transformer-based\nmethods for NLP applied to the EMR field. In the initial query, 99 articles\nwere selected from three public databases and filtered into 65 articles for\ndetailed analysis. The papers were analyzed with respect to the business\nproblem, NLP task, models and techniques, availability of datasets,\nreproducibility of modeling, language, and exchange format. The paper presents\nsome limitations of current research and some recommendations for further\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Batista_V/0/1/0/all/0/1\">Vitor Alcantara Batista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evsukoff_A/0/1/0/all/0/1\">Alexandre Gon&#xe7;alves Evsukoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Performance of Data Augmentation Methods for Brazilian Portuguese Text Classification. (arXiv:2304.02785v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02785","description":"<p>Improving machine learning performance while increasing model generalization\nhas been a constantly pursued goal by AI researchers. Data augmentation\ntechniques are often used towards achieving this target, and most of its\nevaluation is made using English corpora. In this work, we took advantage of\ndifferent existing data augmentation methods to analyze their performances\napplied to text classification problems using Brazilian Portuguese corpora. As\na result, our analysis shows some putative improvements in using some of these\ntechniques; however, it also suggests further exploitation of language bias and\nnon-English text data scarcity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amadeus_M/0/1/0/all/0/1\">Marcellus Amadeus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Branco_P/0/1/0/all/0/1\">Paulo Branco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Aware Classification of Legal Document Pages. (arXiv:2304.02787v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02787","description":"<p>For many business applications that require the processing, indexing, and\nretrieval of professional documents such as legal briefs (in PDF format etc.),\nit is often essential to classify the pages of any given document into their\ncorresponding types beforehand. Most existing studies in the field of document\nimage classification either focus on single-page documents or treat multiple\npages in a document independently. Although in recent years a few techniques\nhave been proposed to exploit the context information from neighboring pages to\nenhance document page classification, they typically cannot be utilized with\nlarge pre-trained language models due to the constraint on input length. In\nthis paper, we present a simple but effective approach that overcomes the above\nlimitation. Specifically, we enhance the input with extra tokens carrying\nsequential information about previous pages - introducing recurrence - which\nenables the usage of pre-trained Transformer models like BERT for context-aware\npage classification. Our experiments conducted on two legal datasets in English\nand Portuguese respectively show that the proposed approach can significantly\nimprove the performance of document page classification compared to the\nnon-recurrent setup as well as the other context-aware baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fragkogiannis_P/0/1/0/all/0/1\">Pavlos Fragkogiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forster_M/0/1/0/all/0/1\">Martina Forster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Grace E. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dell Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pragmatically Appropriate Diversity for Dialogue Evaluation. (arXiv:2304.02812v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02812","description":"<p>Linguistic pragmatics state that a conversation's underlying speech acts can\nconstrain the type of response which is appropriate at each turn in the\nconversation. When generating dialogue responses, neural dialogue agents\nstruggle to produce diverse responses. Currently, dialogue diversity is\nassessed using automatic metrics, but the underlying speech acts do not inform\nthese metrics.\n</p>\n<p>To remedy this, we propose the notion of Pragmatically Appropriate Diversity,\ndefined as the extent to which a conversation creates and constrains the\ncreation of multiple diverse responses. Using a human-created multi-response\ndataset, we find significant support for the hypothesis that speech acts\nprovide a signal for the diversity of the set of next responses. Building on\nthis result, we propose a new human evaluation task where creative writers\npredict the extent to which conversations inspire the creation of multiple\ndiverse responses. Our studies find that writers' judgments align with the\nPragmatically Appropriate Diversity of conversations. Our work suggests that\nexpectations for diversity metric scores should vary depending on the speech\nact.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stasaski_K/0/1/0/all/0/1\">Katherine Stasaski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hearst_M/0/1/0/all/0/1\">Marti A. Hearst</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT detectors are biased against non-native English writers. (arXiv:2304.02819v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02819","description":"<p>The rapid adoption of generative language models has brought about\nsubstantial advancements in digital communication, while simultaneously raising\nconcerns regarding the potential misuse of AI-generated content. Although\nnumerous detection methods have been proposed to differentiate between AI and\nhuman-generated content, the fairness and robustness of these detectors remain\nunderexplored. In this study, we evaluate the performance of several\nwidely-used GPT detectors using writing samples from native and non-native\nEnglish writers. Our findings reveal that these detectors consistently\nmisclassify non-native English writing samples as AI-generated, whereas native\nwriting samples are accurately identified. Furthermore, we demonstrate that\nsimple prompting strategies can not only mitigate this bias but also\neffectively bypass GPT detectors, suggesting that GPT detectors may\nunintentionally penalize writers with constrained linguistic expressions. Our\nresults call for a broader conversation about the ethical implications of\ndeploying ChatGPT content detectors and caution against their use in evaluative\nor educational settings, particularly when they may inadvertently penalize or\nexclude non-native English speakers from the global discourse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1\">Weixin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuksekgonul_M/0/1/0/all/0/1\">Mert Yuksekgonul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yining Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_E/0/1/0/all/0/1\">Eric Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions. (arXiv:2304.02868v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02868","description":"<p>Large language models (LLMs) such as ChatGPT and GPT-4 have recently\ndemonstrated their remarkable abilities of communicating with human users. In\nthis technical report, we take an initiative to investigate their capacities of\nplaying text games, in which a player has to understand the environment and\nrespond to situations by having dialogues with the game world. Our experiments\nshow that ChatGPT performs competitively compared to all the existing systems\nbut still exhibits a low level of intelligence. Precisely, ChatGPT can not\nconstruct the world model by playing the game or even reading the game manual;\nit may fail to leverage the world knowledge that it already has; it cannot\ninfer the goal of each step as the game progresses. Our results open up new\nresearch questions at the intersection of artificial intelligence, machine\nlearning, and natural language processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsai_C/0/1/0/all/0/1\">Chen Feng Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaochen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sierra S. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1\">Hongyuan Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic ICD-10 Code Association: A Challenging Task on French Clinical Texts. (arXiv:2304.02886v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02886","description":"<p>Automatically associating ICD codes with electronic health data is a\nwell-known NLP task in medical research. NLP has evolved significantly in\nrecent years with the emergence of pre-trained language models based on\nTransformers architecture, mainly in the English language. This paper adapts\nthese models to automatically associate the ICD codes. Several neural network\narchitectures have been experimented with to address the challenges of dealing\nwith a large set of both input tokens and labels to be guessed. In this paper,\nwe propose a model that combines the latest advances in NLP and multi-label\nclassification for ICD-10 code association. Fair experiments on a Clinical\ndataset in the French language show that our approach increases the $F_1$-score\nmetric by more than 55\\% compared to state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tchouka_Y/0/1/0/all/0/1\">Yakini Tchouka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couchot_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Couchot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laiymani_D/0/1/0/all/0/1\">David Laiymani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selles_P/0/1/0/all/0/1\">Philippe Selles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_A/0/1/0/all/0/1\">Azzedine Rahmani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Affect as a proxy for literary mood. (arXiv:2304.02894v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02894","description":"<p>We propose to use affect as a proxy for mood in literary texts. In this\nstudy, we explore the differences in computationally detecting tone versus\ndetecting mood. Methodologically we utilize affective word embeddings to look\nat the affective distribution in different text segments. We also present a\nsimple yet efficient and effective method of enhancing emotion lexicons to take\nboth semantic shift and the domain of the text into account producing\nreal-world congruent results closely matching both contemporary and modern\nqualitative analyses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ohman_E/0/1/0/all/0/1\">Emily &#xd6;hman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1\">Riikka Rossi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpanRE: Entities and Overlapping Relations Extraction Based on Spans and Entity Attention. (arXiv:2304.02901v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02901","description":"<p>Extracting entities and relations is an essential task of information\nextraction. Triplets extracted from a sentence might overlap with each other.\nPrevious methods either did not address the overlapping issues or solved\noverlapping issues partially. To tackle triplet overlapping problems\ncompletely, firstly we extract candidate subjects with a standard span\nmechanism. Then we present a labeled span mechanism to extract the objects and\nrelations simultaneously, we use the labeled span mechanism to generate labeled\nspans whose start and end positions indicate the objects, and whose labels\ncorrespond to relations of subject and objects. Besides, we design an entity\nattention mechanism to enhance the information fusion between subject and\nsentence during extracting objects and relations. We test our method on two\npublic datasets, our method achieves the best performances on these two\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-label classification of open-ended questions with BERT. (arXiv:2304.02945v1 [stat.AP])","link":"http://arxiv.org/abs/2304.02945","description":"<p>Open-ended questions in surveys are valuable because they do not constrain\nthe respondent's answer, thereby avoiding biases. However, answers to\nopen-ended questions are text data which are harder to analyze. Traditionally,\nanswers were manually classified as specified in the coding manual. Most of the\neffort to automate coding has gone into the easier problem of single label\nprediction, where answers are classified into a single code. However, open-ends\nthat require multi-label classification, i.e., that are assigned multiple\ncodes, occur frequently. This paper focuses on multi-label classification of\ntext answers to open-ended survey questions in social science surveys. We\nevaluate the performance of the transformer-based architecture BERT for the\nGerman language in comparison to traditional multi-label algorithms (Binary\nRelevance, Label Powerset, ECC) in a German social science survey, the GLES\nPanel (N=17,584, 55 labels). We find that classification with BERT (forcing at\nleast one label) has the smallest 0/1 loss (13.1%) among methods considered\n(18.9%-21.6%). As expected, it is much easier to correctly predict answer texts\nthat correspond to a single label (7.1% loss) than those that correspond to\nmultiple labels ($\\sim$50% loss). Because BERT predicts zero labels for only\n1.5% of the answers, forcing at least one label, while recommended, ultimately\ndoes not lower the 0/1 loss by much. Our work has important implications for\nsocial scientists: 1) We have shown multi-label classification with BERT works\nin the German language for open-ends. 2) For mildly multi-label classification\ntasks, the loss now appears small enough to allow for fully automatic\nclassification (as compared to semi-automatic approaches). 3) Multi-label\nclassification with BERT requires only a single model. The leading competitor,\nECC, iterates through individual single label predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Schonlau_M/0/1/0/all/0/1\">Matthias Schonlau</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Weiss_J/0/1/0/all/0/1\">Julia Wei&#xdf;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Marquardt_J/0/1/0/all/0/1\">Jan Marquardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Social Interactions to Detect Misinformation on Social Media. (arXiv:2304.02983v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02983","description":"<p>Detecting misinformation threads is crucial to guarantee a healthy\nenvironment on social media. We address the problem using the data set created\nduring the COVID-19 pandemic. It contains cascades of tweets discussing\ninformation weakly labeled as reliable or unreliable, based on a previous\nevaluation of the information source. The models identifying unreliable threads\nusually rely on textual features. But reliability is not just what is said, but\nby whom and to whom. We additionally leverage on network information. Following\nthe homophily principle, we hypothesize that users who interact are generally\ninterested in similar topics and spreading similar kind of news, which in turn\nis generally reliable or not. We test several methods to learn representations\nof the social interactions within the cascades, combining them with deep neural\nlanguage models in a Multi-Input (MI) framework. Keeping track of the sequence\nof the interactions during the time, we improve over previous state-of-the-art\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fornaciari_T/0/1/0/all/0/1\">Tommaso Fornaciari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luceri_L/0/1/0/all/0/1\">Luca Luceri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrara_E/0/1/0/all/0/1\">Emilio Ferrara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Robot Programming: NLP integrated with autonomous robotic grasping. (arXiv:2304.02993v1 [cs.RO])","link":"http://arxiv.org/abs/2304.02993","description":"<p>In this paper, we present a grammar-based natural language framework for\nrobot programming, specifically for pick-and-place tasks. Our approach uses a\ncustom dictionary of action words, designed to store together words that share\nmeaning, allowing for easy expansion of the vocabulary by adding more action\nwords from a lexical database. We validate our Natural Language Robot\nProgramming (NLRP) framework through simulation and real-world experimentation,\nusing a Franka Panda robotic arm equipped with a calibrated camera-in-hand and\na microphone. Participants were asked to complete a pick-and-place task using\nverbal commands, which were converted into text using Google's Speech-to-Text\nAPI and processed through the NLRP framework to obtain joint space trajectories\nfor the robot. Our results indicate that our approach has a high system\nusability score. The framework's dictionary can be easily extended without\nrelying on transfer learning or large data sets. In the future, we plan to\ncompare the presented framework with different approaches of human-assisted\npick-and-place tasks via a comprehensive user study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Muhammad Arshad Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kenney_M/0/1/0/all/0/1\">Max Kenney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Painter_J/0/1/0/all/0/1\">Jack Painter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamale_D/0/1/0/all/0/1\">Disha Kamale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batista_Navarro_R/0/1/0/all/0/1\">Riza Batista-Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghalamzan_E_A/0/1/0/all/0/1\">Amir Ghalamzan-E</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compression of enumerations and gain. (arXiv:2304.03030v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03030","description":"<p>We study the compressibility of enumerations, and its role in the relative\nKolmogorov complexity of computably enumerable sets, with respect to density.\nWith respect to a strong and a weak form of compression, we examine the gain:\nthe amount of auxiliary information embedded in the compressed enumeration.\nStrong compression and weak gainless compression is shown for any computably\nenumerable set, and a positional game is studied toward understanding strong\ngainless compression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barmpalias_G/0/1/0/all/0/1\">George Barmpalias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_B/0/1/0/all/0/1\">Bohua Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments. (arXiv:2304.03047v1 [cs.CV])","link":"http://arxiv.org/abs/2304.03047","description":"<p>Vision-language navigation is a task that requires an agent to follow\ninstructions to navigate in environments. It becomes increasingly crucial in\nthe field of embodied AI, with potential applications in autonomous navigation,\nsearch and rescue, and human-robot interaction. In this paper, we propose to\naddress a more practical yet challenging counterpart setting - vision-language\nnavigation in continuous environments (VLN-CE). To develop a robust VLN-CE\nagent, we propose a new navigation framework, ETPNav, which focuses on two\ncritical skills: 1) the capability to abstract environments and generate\nlong-range navigation plans, and 2) the ability of obstacle-avoiding control in\ncontinuous environments. ETPNav performs online topological mapping of\nenvironments by self-organizing predicted waypoints along a traversed path,\nwithout prior environmental experience. It privileges the agent to break down\nthe navigation procedure into high-level planning and low-level control.\nConcurrently, ETPNav utilizes a transformer-based cross-modal planner to\ngenerate navigation plans based on topological maps and instructions. The plan\nis then performed through an obstacle-avoiding controller that leverages a\ntrial-and-error heuristic to prevent navigation from getting stuck in\nobstacles. Experimental results demonstrate the effectiveness of the proposed\nmethod. ETPNav yields more than 10% and 20% improvements over prior\nstate-of-the-art on R2R-CE and RxR-CE datasets, respectively. Our code is\navailable at https://github.com/MarSaKi/ETPNav.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1\">Dong An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Keji He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT for Shaping the Future of Dentistry: The Potential of Multi-Modal Large Language Model. (arXiv:2304.03086v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03086","description":"<p>The ChatGPT, as a lite and conversational variant of Generative Pretrained\nTransformer 4 (GPT-4) developed by OpenAI, is one of the milestone Large\nLanguage Models (LLMs) with billions of parameters. LLMs, in fact, have stirred\nup a lot of interest among researchers and practitioners by their impressive\nskills in natural language processing tasks, which have a profound impact on a\nwide range of fields. This paper mainly discusses the future applications of\nLLMs in dentistry. We introduce two primary LLM deployment methods in\ndentistry, including automated dental diagnosis and cross-modal dental\ndiagnosis, and examine their potential applications. Especially, equipped with\na cross-modal encoder, a single LLM can manage multi-source data and conduct\nadvanced natural language reasoning to perform complex clinical operations. A\nuse case is presented to demonstrate the potential of a fully automatic\nMulti-Modal LLM AI system for dentistry clinical application. While LLMs offer\nsignificant potential benefits, the challenges, such as data privacy, data\nquality, and model bias, need further study. Overall, LLMs have the potential\nto revolutionize dental diagnosis and treatment, which indicates a promising\navenue for clinical application and research in dentistry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hanyao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_O/0/1/0/all/0/1\">Ou Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongdong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jiayi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shengxuan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Heng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Renjie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qian Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bing Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Chain-of-thought with ChatGPT for Stance Detection on Social Media. (arXiv:2304.03087v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03087","description":"<p>Stance detection predicts attitudes towards targets in texts and has gained\nattention with the rise of social media. Traditional approaches include\nconventional machine learning, early deep neural networks, and pre-trained\nfine-tuning models. However, with the evolution of very large pre-trained\nlanguage models (VLPLMs) like ChatGPT (GPT-3.5), traditional methods face\ndeployment challenges. The parameter-free Chain-of-Thought (CoT) approach, not\nrequiring backpropagation training, has emerged as a promising alternative.\nThis paper examines CoT's effectiveness in stance detection tasks,\ndemonstrating its superior accuracy and discussing associated challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xianghua Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">Daijun Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liwen Jing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Static Fuzzy Bag-of-Words: a lightweight sentence embedding algorithm. (arXiv:2304.03098v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03098","description":"<p>The introduction of embedding techniques has pushed forward significantly the\nNatural Language Processing field. Many of the proposed solutions have been\npresented for word-level encoding; anyhow, in the last years, new mechanism to\ntreat information at an higher level of aggregation, like at sentence- and\ndocument-level, have emerged. With this work we address specifically the\nsentence embeddings problem, presenting the Static Fuzzy Bag-of-Word model. Our\nmodel is a refinement of the Fuzzy Bag-of-Words approach, providing sentence\nembeddings with a predefined dimension. SFBoW provides competitive performances\nin Semantic Textual Similarity benchmarks, while requiring low computational\nresources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muffo_M/0/1/0/all/0/1\">Matteo Muffo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tedesco_R/0/1/0/all/0/1\">Roberto Tedesco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sbattella_L/0/1/0/all/0/1\">Licia Sbattella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scotti_V/0/1/0/all/0/1\">Vincenzo Scotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Robustness of Machine Reading Comprehension Models to Low Resource Entity Renaming. (arXiv:2304.03145v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03145","description":"<p>Question answering (QA) models have shown compelling results in the task of\nMachine Reading Comprehension (MRC). Recently these systems have proved to\nperform better than humans on held-out test sets of datasets e.g. SQuAD, but\ntheir robustness is not guaranteed. The QA model's brittleness is exposed when\nevaluated on adversarial generated examples by a performance drop. In this\nstudy, we explore the robustness of MRC models to entity renaming, with\nentities from low-resource regions such as Africa. We propose EntSwap, a method\nfor test-time perturbations, to create a test set whose entities have been\nrenamed. In particular, we rename entities of type: country, person,\nnationality, location, organization, and city, to create AfriSQuAD2. Using the\nperturbed test set, we evaluate the robustness of three popular MRC models. We\nfind that compared to base models, large models perform well comparatively on\nnovel entities. Furthermore, our analysis indicates that entity type person\nhighly challenges the MRC models' performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siro_C/0/1/0/all/0/1\">Clemencia Siro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajayi_T/0/1/0/all/0/1\">Tunde Oluwaseyi Ajayi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Next-Item Recommendation using Large Pretrained Language Models. (arXiv:2304.03153v1 [cs.IR])","link":"http://arxiv.org/abs/2304.03153","description":"<p>Large language models (LLMs) have achieved impressive zero-shot performance\nin various natural language processing (NLP) tasks, demonstrating their\ncapabilities for inference without training examples. Despite their success, no\nresearch has yet explored the potential of LLMs to perform next-item\nrecommendations in the zero-shot setting. We have identified two major\nchallenges that must be addressed to enable LLMs to act effectively as\nrecommenders. First, the recommendation space can be extremely large for LLMs,\nand LLMs do not know about the target user's past interacted items and\npreferences. To address this gap, we propose a prompting strategy called\nZero-Shot Next-Item Recommendation (NIR) prompting that directs LLMs to make\nnext-item recommendations. Specifically, the NIR-based strategy involves using\nan external module to generate candidate items based on user-filtering or\nitem-filtering. Our strategy incorporates a 3-step prompting that guides GPT-3\nto carry subtasks that capture the user's preferences, select representative\npreviously watched movies, and recommend a ranked list of 10 movies. We\nevaluate the proposed approach using GPT-3 on MovieLens 100K dataset and show\nthat it achieves strong zero-shot performance, even outperforming some strong\nsequential recommendation models trained on the entire training dataset. These\npromising results highlight the ample research opportunities to use LLMs as\nrecommenders. The code can be found at\nhttps://github.com/AGI-Edgerunners/LLM-Next-Item-Rec.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1\">Ee-Peng Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval. (arXiv:2304.03158v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03158","description":"<p>Growing techniques have been emerging to improve the performance of passage\nretrieval. As an effective representation bottleneck pretraining technique, the\ncontextual masked auto-encoder utilizes contextual embedding to assist in the\nreconstruction of passages. However, it only uses a single auto-encoding\npre-task for dense representation pre-training. This study brings multi-view\nmodeling to the contextual masked auto-encoder. Firstly, multi-view\nrepresentation utilizes both dense and sparse vectors as multi-view\nrepresentations, aiming to capture sentence semantics from different aspects.\nMoreover, multiview decoding paradigm utilizes both autoencoding and\nauto-regressive decoders in representation bottleneck pre-training, aiming to\nprovide both reconstructive and generative signals for better contextual\nrepresentation pretraining. We refer to this multi-view pretraining method as\nCoT-MAE v2. Through extensive experiments, we show that CoT-MAE v2 is effective\nand robust on large-scale passage retrieval benchmarks and out-of-domain\nzero-shot benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1\">Guangyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Meng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zijia Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fuzheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songlin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the Language Gap: Knowledge Injected Multilingual Question Answering. (arXiv:2304.03159v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03159","description":"<p>Question Answering (QA) is the task of automatically answering questions\nposed by humans in natural languages. There are different settings to answer a\nquestion, such as abstractive, extractive, boolean, and multiple-choice QA. As\na popular topic in natural language processing tasks, extractive question\nanswering task (extractive QA) has gained extensive attention in the past few\nyears. With the continuous evolvement of the world, generalized cross-lingual\ntransfer (G-XLT), where question and answer context are in different languages,\nposes some unique challenges over cross-lingual transfer (XLT), where question\nand answer context are in the same language. With the boost of corresponding\ndevelopment of related benchmarks, many works have been done to improve the\nperformance of various language QA tasks. However, only a few works are\ndedicated to the G-XLT task. In this work, we propose a generalized\ncross-lingual transfer framework to enhance the model's ability to understand\ndifferent languages. Specifically, we first assemble triples from different\nlanguages to form multilingual knowledge. Since the lack of knowledge between\ndifferent languages greatly limits models' reasoning ability, we further design\na knowledge injection strategy via leveraging link prediction techniques to\nenrich the model storage of multilingual knowledge. In this way, we can\nprofoundly exploit rich semantic knowledge. Experiment results on real-world\ndatasets MLQA demonstrate that the proposed method can improve the performance\nby a large margin, outperforming the baseline method by 13.18%/12.00% F1/EM on\naverage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1\">Zhichao Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiuxing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selective Data Augmentation for Robust Speech Translation. (arXiv:2304.03169v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03169","description":"<p>Speech translation (ST) systems translate speech in one language to text in\nanother language. End-to-end ST systems (e2e-ST) have gained popularity over\ncascade systems because of their enhanced performance due to reduced latency\nand computational cost. Though resource intensive, e2e-ST systems have the\ninherent ability to retain para and non-linguistic characteristics of the\nspeech unlike cascade systems. In this paper, we propose to use an e2e\narchitecture for English-Hindi (en-hi) ST. We use two imperfect machine\ntranslation (MT) services to translate Libri-trans en text into hi text. While\neach service gives MT data individually to generate parallel ST data, we\npropose a data augmentation strategy of noisy MT data to aid robust ST. The\nmain contribution of this paper is the proposal of a data augmentation\nstrategy. We show that this results in better ST (BLEU score) compared to brute\nforce augmentation of MT data. We observed an absolute improvement of 1.59 BLEU\nscore with our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Acharya_R/0/1/0/all/0/1\">Rajul Acharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_A/0/1/0/all/0/1\">Ashish Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopparapu_S/0/1/0/all/0/1\">Sunil Kumar Kopparapu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster. (arXiv:2304.03208v1 [cs.LG])","link":"http://arxiv.org/abs/2304.03208","description":"<p>We study recent research advances that improve large language models through\nefficient pre-training and scaling, and open datasets and tools. We combine\nthese advances to introduce Cerebras-GPT, a family of open compute-optimal\nlanguage models scaled from 111M to 13B parameters. We train Cerebras-GPT\nmodels on the Eleuther Pile dataset following DeepMind Chinchilla scaling rules\nfor efficient pre-training (highest accuracy for a given compute budget). We\ncharacterize the predictable power-law scaling and compare Cerebras-GPT with\nother publicly-available models to show all Cerebras-GPT models have\nstate-of-the-art training efficiency on both pre-training and downstream\nobjectives. We describe our learnings including how Maximal Update\nParameterization ($\\mu$P) can further improve large model scaling, improving\naccuracy and hyperparameter predictability at scale. We release our pre-trained\nmodels and code, making this paper the first open and reproducible work\ncomparing compute-optimal model scaling to models trained on fixed dataset\nsizes. Cerebras-GPT models are available on HuggingFace:\nhttps://huggingface.co/cerebras.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dey_N/0/1/0/all/0/1\">Nolan Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gosal_G/0/1/0/all/0/1\">Gurpreet Gosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhiming/0/1/0/all/0/1\">Zhiming</a> (Charles) <a href=\"http://arxiv.org/find/cs/1/au:+Chen/0/1/0/all/0/1\">Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khachane_H/0/1/0/all/0/1\">Hemant Khachane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshall_W/0/1/0/all/0/1\">William Marshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathria_R/0/1/0/all/0/1\">Ribhu Pathria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tom_M/0/1/0/all/0/1\">Marvin Tom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hestness_J/0/1/0/all/0/1\">Joel Hestness</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Pareto Front of Multilingual Neural Machine Translation. (arXiv:2304.03216v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03216","description":"<p>In this work, we study how the generalization performance of a given\ndirection changes with its sampling ratio in Multilingual Neural Machine\nTranslation (MNMT). By training over 200 multilingual models with various model\nsizes, directions, and total numbers of tasks, we find that scalarization leads\nto a multitask trade-off front that deviates from the traditional Pareto front\nwhen there exists data imbalance in the training corpus. That is, the\nperformance of certain translation directions does not improve with the\nincrease of its weight in the multi-task optimization objective, which poses\ngreater challenge to improve the overall performance of all directions. Based\non our observations, we propose the Double Power Law to predict the unique\nperformance trade-off front in MNMT, which is robust across various languages,\ndata adequacy and number of tasks. Finally, we formulate sample ratio selection\nin MNMT as an optimization problem based on the Double Power Law, which\nachieves better performance than temperature searching and gradient\nmanipulation methods using up to half of the total training budget in our\nexperiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedBot: Enhancing Privacy in Chatbots with Federated Learning. (arXiv:2304.03228v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03228","description":"<p>Chatbots are mainly data-driven and usually based on utterances that might be\nsensitive. However, training deep learning models on shared data can violate\nuser privacy. Such issues have commonly existed in chatbots since their\ninception. In the literature, there have been many approaches to deal with\nprivacy, such as differential privacy and secure multi-party computation, but\nmost of them need to have access to users' data. In this context, Federated\nLearning (FL) aims to protect data privacy through distributed learning methods\nthat keep the data in its location. This paper presents Fedbot, a\nproof-of-concept (POC) privacy-preserving chatbot that leverages large-scale\ncustomer support data. The POC combines Deep Bidirectional Transformer models\nand federated learning algorithms to protect customer data privacy during\ncollaborative model training. The results of the proof-of-concept showcase the\npotential for privacy-preserving chatbots to transform the customer support\nindustry by delivering personalized and efficient customer service that meets\ndata privacy regulations and legal requirements. Furthermore, the system is\nspecifically designed to improve its performance and accuracy over time by\nleveraging its ability to learn from previous interactions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ait_Mlouk_A/0/1/0/all/0/1\">Addi Ait-Mlouk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alawadi_S/0/1/0/all/0/1\">Sadi Alawadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toor_S/0/1/0/all/0/1\">Salman Toor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellander_A/0/1/0/all/0/1\">Andreas Hellander</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large language models effectively leverage document-level context for literary translation, but critical errors persist. (arXiv:2304.03245v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03245","description":"<p>Large language models (LLMs) are competitive with the state of the art on a\nwide range of sentence-level translation datasets. However, their ability to\ntranslate paragraphs and documents remains unexplored because evaluation in\nthese settings is costly and difficult. We show through a rigorous human\nevaluation that asking the Gpt-3.5 (text-davinci-003) LLM to translate an\nentire literary paragraph (e.g., from a novel) at once results in\nhigher-quality translations than standard sentence-by-sentence translation\nacross 18 linguistically-diverse language pairs (e.g., translating into and out\nof Japanese, Polish, and English). Our evaluation, which took approximately 350\nhours of effort for annotation and analysis, is conducted by hiring translators\nfluent in both the source and target language and asking them to provide both\nspan-level error annotations as well as preference judgments of which system's\ntranslations are better. We observe that discourse-level LLM translators commit\nfewer mistranslations, grammar errors, and stylistic inconsistencies than\nsentence-level approaches. With that said, critical errors still abound,\nincluding occasional content omissions, and a human translator's intervention\nremains necessary to ensure that the author's voice remains intact. We publicly\nrelease our dataset and error annotations to spur future research on evaluation\nof document-level literary translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karpinska_M/0/1/0/all/0/1\">Marzena Karpinska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instruction Tuning with GPT-4. (arXiv:2304.03277v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03277","description":"<p>Prior work has shown that finetuning large language models (LLMs) using\nmachine-generated instruction-following data enables such models to achieve\nremarkable zero-shot capabilities on new tasks, and no human-written\ninstructions are needed. In this paper, we present the first attempt to use\nGPT-4 to generate instruction-following data for LLM finetuning. Our early\nexperiments on instruction-tuned LLaMA models show that the 52K English and\nChinese instruction-following data generated by GPT-4 leads to superior\nzero-shot performance on new tasks to the instruction-following data generated\nby previous state-of-the-art models. We also collect feedback and comparison\ndata from GPT-4 to enable a comprehensive evaluation and reward model training.\nWe make our data generated using GPT-4 as well as our codebase publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1\">Michel Galley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. (arXiv:2304.03279v1 [cs.LG])","link":"http://arxiv.org/abs/2304.03279","description":"<p>Artificial agents have traditionally been trained to maximize reward, which\nmay incentivize power-seeking and deception, analogous to how next-token\nprediction in language models (LMs) may incentivize toxicity. So do agents\nnaturally learn to be Machiavellian? And how do we measure these behaviors in\ngeneral-purpose models such as GPT-4? Towards answering these questions, we\nintroduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games\ncontaining over half a million rich, diverse scenarios that center on social\ndecision-making. Scenario labeling is automated with LMs, which are more\nperformant than human annotators. We mathematize dozens of harmful behaviors\nand use our annotations to evaluate agents' tendencies to be power-seeking,\ncause disutility, and commit ethical violations. We observe some tension\nbetween maximizing reward and behaving ethically. To improve this trade-off, we\ninvestigate LM-based methods to steer agents' towards less harmful behaviors.\nOur results show that agents can both act competently and morally, so concrete\nprogress can currently be made in machine ethics--designing agents that are\nPareto improvements in both safety and capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_A/0/1/0/all/0/1\">Alexander Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shern_C/0/1/0/all/0/1\">Chan Jun Shern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1\">Andy Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nathaniel Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodside_T/0/1/0/all/0/1\">Thomas Woodside</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_J/0/1/0/all/0/1\">Jonathan Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emmons_S/0/1/0/all/0/1\">Scott Emmons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Term Rewriting Based On Set Automaton Matching. (arXiv:2202.08687v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.08687","description":"<p>In this article we investigate how a subterm pattern matching algorithm can\nbe exploited to implement efficient term rewriting procedures. From the\nleft-hand sides of the rewrite system we construct a set automaton, which can\nbe used to find all redexes in a term efficiently. We formally describe a\nprocedure that, given a rewrite strategy, interleaves pattern matching steps\nand rewriting steps and thus smoothly integrates redex discovery and subterm\nreplacement. We then present an efficient implementation that instantiates this\nprocedure with outermost rewriting, and present the results of some\nexperiments. Our implementation shows to be competitive with comparable tools.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bouwman_M/0/1/0/all/0/1\">Mark Bouwman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erkens_R/0/1/0/all/0/1\">Rick Erkens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse*BERT: Sparse Models Generalize To New tasks and Domains. (arXiv:2205.12452v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12452","description":"<p>Large Language Models have become the core architecture upon which most\nmodern natural language processing (NLP) systems build. These models can\nconsistently deliver impressive accuracy and robustness across tasks and\ndomains, but their high computational overhead can make inference difficult and\nexpensive. To make using these models less costly, recent work has explored\nleveraging structured and unstructured pruning, quantization, and distillation\nto improve inference speed and decrease size. This paper studies how models\npruned using Gradual Unstructured Magnitude Pruning can transfer between\ndomains and tasks. Our experimentation shows that models that are pruned during\npretraining using general domain masked language models can transfer to novel\ndomains and tasks without extensive hyperparameter exploration or specialized\napproaches. We demonstrate that our general sparse model Sparse*BERT can become\nSparseBioBERT simply by pretraining the compressed architecture on unstructured\nbiomedical text. Moreover, we show that SparseBioBERT can match the quality of\nBioBERT with only 10\\% of the parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Campos_D/0/1/0/all/0/1\">Daniel Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marques_A/0/1/0/all/0/1\">Alexandre Marques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtz_M/0/1/0/all/0/1\">Mark Kurtz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Multi-Modal E-commerce Attribute Value Extraction via Unified Learning Scheme and Dynamic Range Minimization. (arXiv:2207.07278v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.07278","description":"<p>With the prosperity of e-commerce industry, various modalities, e.g., vision\nand language, are utilized to describe product items. It is an enormous\nchallenge to understand such diversified data, especially via extracting the\nattribute-value pairs in text sequences with the aid of helpful image regions.\nAlthough a series of previous works have been dedicated to this task, there\nremain seldomly investigated obstacles that hinder further improvements: 1)\nParameters from up-stream single-modal pretraining are inadequately applied,\nwithout proper jointly fine-tuning in a down-stream multi-modal task. 2) To\nselect descriptive parts of images, a simple late fusion is widely applied,\nregardless of priori knowledge that language-related information should be\nencoded into a common linguistic embedding space by stronger encoders. 3) Due\nto diversity across products, their attribute sets tend to vary greatly, but\ncurrent approaches predict with an unnecessary maximal range and lead to more\npotential false positives. To address these issues, we propose in this paper a\nnovel approach to boost multi-modal e-commerce attribute value extraction via\nunified learning scheme and dynamic range minimization: 1) Firstly, a unified\nscheme is designed to jointly train a multi-modal task with pretrained\nsingle-modal parameters. 2) Secondly, a text-guided information range\nminimization method is proposed to adaptively encode descriptive parts of each\nmodality into an identical space with a powerful pretrained linguistic model.\n3) Moreover, a prototype-guided attribute range minimization method is proposed\nto first determine the proper attribute set of the current product, and then\nselect prototypes to guide the prediction of the chosen attributes. Experiments\non the popular multi-modal e-commerce benchmarks show that our approach\nachieves superior performance over the other state-of-the-art techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengyin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Hongyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1\">Weibo Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongfa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xu-cheng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toxicity in Multilingual Machine Translation at Scale. (arXiv:2210.03070v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03070","description":"<p>Machine Translation systems can produce different types of errors, some of\nwhich are characterized as critical or catastrophic due to the specific\nnegative impact that they can have on users. In this paper we focus on one type\nof critical error: added toxicity. We evaluate and analyze added toxicity when\ntranslating a large evaluation dataset (HOLISTICBIAS, over 472k sentences,\ncovering 13 demographic axes) from English into 164 languages. An automatic\ntoxicity evaluation shows that added toxicity across languages varies from 0%\nto 5%. The output languages with the most added toxicity tend to be\nlow-resource ones, and the demographic axes with the most added toxicity\ninclude sexual orientation, gender and sex, and ability. We also perform human\nevaluation on a subset of 8 translation directions, confirming the prevalence\nof true added toxicity. We use a measurement of the amount of source\ncontribution to the translation, where a low source contribution implies\nhallucination, to interpret what causes toxicity. Making use of the input\nattributions allows us to explain toxicity, because the source contributions\nsignificantly correlate with toxicity for 84% of languages studied. Given our\nfindings, our recommendations to reduce added toxicity are to curate training\ndata to avoid mistranslations, mitigate hallucination and check unstable\ntranslations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1\">Eric Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ropers_C/0/1/0/all/0/1\">Christophe Ropers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Licht_D/0/1/0/all/0/1\">Daniel Licht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maillard_J/0/1/0/all/0/1\">Jean Maillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrando_J/0/1/0/all/0/1\">Javier Ferrando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escolano_C/0/1/0/all/0/1\">Carlos Escolano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From exemplar to copy: the scribal appropriation of a Hadewijch manuscript computationally explored. (arXiv:2210.14061v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.14061","description":"<p>This study is devoted to two of the oldest known manuscripts in which the\noeuvre of the medieval mystical author Hadewijch has been preserved: Brussels,\nKBR, 2879-2880 (ms. A) and Brussels, KBR, 2877-2878 (ms. B). On the basis of\ncodicological and contextual arguments, it is assumed that the scribe who\nproduced B used A as an exemplar. While the similarities in both layout and\ncontent between the two manuscripts are striking, the present article seeks to\nidentify the differences. After all, regardless of the intention to produce a\ncopy that closely follows the exemplar, subtle linguistic variation is\napparent. Divergences relate to spelling conventions, but also to the way in\nwhich words are abbreviated (and the extent to which abbreviations occur). The\npresent study investigates the spelling profiles of the scribes who produced\nmss. A and B in a computational way. In the first part of this study, we will\npresent both manuscripts in more detail, after which we will consider prior\nresearch carried out on scribal profiling. The current study both builds and\nexpands on Kestemont (2015). Next, we outline the methodology used to analyse\nand measure the degree of scribal appropriation that took place when ms. B was\ncopied off the exemplar ms. A. After this, we will discuss the results\nobtained, focusing on the scribal variation that can be found both at the level\nof individual words and n-grams. To this end, we use machine learning to\nidentify the most distinctive features that separate manuscript A from B.\nFinally, we look at possible diachronic trends in the appropriation by B's\nscribe of his exemplar. We argue that scribal takeovers in the exemplar impacts\nthe practice of the copying scribe, while transitions to a different content\nmatter cause little to no effect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haverals_W/0/1/0/all/0/1\">Wouter Haverals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kestemont_M/0/1/0/all/0/1\">Mike Kestemont</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dataless Knowledge Fusion by Merging Weights of Language Models. (arXiv:2212.09849v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09849","description":"<p>Fine-tuning pre-trained language models has become the prevalent paradigm for\nbuilding downstream NLP models. Oftentimes fine-tuned models are readily\navailable but their training data is not, due to data privacy or intellectual\nproperty concerns. This creates a barrier to fusing knowledge across individual\nmodels to yield a better single model. In this paper, we study the problem of\nmerging individual models built on different training data sets to obtain a\nsingle model that performs well both across all data set domains and can\ngeneralize on out-of-domain data. We propose a dataless knowledge fusion method\nthat merges models in their parameter space, guided by weights that minimize\nprediction differences between the merged model and the individual models. Over\na battery of evaluation settings, we show that the proposed method\nsignificantly outperforms baselines such as Fisher-weighted averaging or model\nensembling. Further, we find that our method is a promising alternative to\nmulti-task learning that can preserve or sometimes improve over the individual\nmodels without access to the training data. Finally, model merging is more\nefficient than training a multi-task model, thus making it applicable to a\nwider set of scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xisen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preotiuc_Pietro_D/0/1/0/all/0/1\">Daniel Preotiuc-Pietro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pengxiang Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Cross-Lingual Summarization via Large Language Models. (arXiv:2302.14229v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.14229","description":"<p>Given a document in a source language, cross-lingual summarization (CLS) aims\nto generate a summary in a different target language. Recently, the emergence\nof Large Language Models (LLMs), such as GPT-3.5, ChatGPT and GPT-4, has\nattracted wide attention from the computational linguistics community. However,\nit is not yet known the performance of LLMs on CLS. In this report, we\nempirically use various prompts to guide LLMs to perform zero-shot CLS from\ndifferent paradigms (i.e., end-to-end and pipeline), and provide a preliminary\nevaluation on the generated summaries. We find that ChatGPT and GPT-4\noriginally prefer to produce lengthy summaries with detailed information. These\ntwo LLMs can further balance informativeness and conciseness with the help of\nan interactive prompt, significantly improving their CLS performance.\nExperimental results on three widely-used CLS datasets show that GPT-4 achieves\nstate-of-the-art zero-shot CLS performance, and performs competitively compared\nwith the fine-tuned mBART-50. Moreover, we also find some multi-lingual and\nbilingual LLMs (i.e., BLOOMZ, ChatGLM-6B, Vicuna-13B and ChatYuan) have limited\nzero-shot CLS ability. Due to the composite nature of CLS, which requires\nmodels to perform summarization and translation simultaneously, accomplishing\nthis task in a zero-shot manner is even a challenge for LLMs. Therefore, we\nsincerely hope and recommend future LLM research could use CLS as a testbed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_B/0/1/0/all/0/1\">Beiqi Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1\">Jianfeng Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Return of the RNN: Residual Recurrent Networks for Invertible Sentence Embeddings. (arXiv:2303.13570v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.13570","description":"<p>This study presents a novel model for invertible sentence embeddings using a\nresidual recurrent network trained on an unsupervised encoding task. Rather\nthan the probabilistic outputs common to neural machine translation models, our\napproach employs a regression-based output layer to reconstruct the input\nsequence's word vectors. The model achieves high accuracy and fast training\nwith the ADAM optimizer, a significant finding given that RNNs typically\nrequire memory units, such as LSTMs, or second-order optimization methods. We\nincorporate residual connections and introduce a \"match drop\" technique, where\ngradients are calculated only for incorrect words. Our approach demonstrates\npotential for various natural language processing applications, particularly in\nneural network-based systems that require high-quality sentence embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilkerson_J/0/1/0/all/0/1\">Jeremy Wilkerson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-oriented Memory-efficient Pruning-Adapter. (arXiv:2303.14704v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.14704","description":"<p>The Outstanding performance and growing size of Large Language Models has led\nto increased attention in parameter efficient learning. The two predominant\napproaches are Adapters and Pruning. Adapters are to freeze the model and give\nit a new weight matrix on the side, which can significantly reduce the time and\nmemory of training, but the cost is that the evaluation and testing will\nincrease the time and memory consumption. Pruning is to cut off some weight and\nre-distribute the remaining weight, which sacrifices the complexity of training\nat the cost of extremely high memory and training time, making the cost of\nevaluation and testing relatively low. So efficiency of training and inference\ncan't be obtained in the same time. In this work, we propose a task-oriented\nPruning-Adapter method that achieve a high memory efficiency of training and\nmemory, and speeds up training time and ensures no significant decrease in\naccuracy in GLUE tasks, achieving training and inference efficiency at the same\ntime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guorun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yaoru Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance. (arXiv:2303.16894v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2303.16894","description":"<p>Understanding 3D scenes from multi-view inputs has been proven to alleviate\nthe view discrepancy issue in 3D visual grounding. However, existing methods\nnormally neglect the view cues embedded in the text modality and fail to weigh\nthe relative importance of different views. In this paper, we propose\nViewRefer, a multi-view framework for 3D visual grounding exploring how to\ngrasp the view knowledge from both text and 3D modalities. For the text branch,\nViewRefer leverages the diverse linguistic knowledge of large-scale language\nmodels, e.g., GPT, to expand a single grounding text to multiple\ngeometry-consistent descriptions. Meanwhile, in the 3D modality, a transformer\nfusion module with inter-view attention is introduced to boost the interaction\nof objects across views. On top of that, we further present a set of learnable\nmulti-view prototypes, which memorize scene-agnostic knowledge for different\nviews, and enhance the framework from two perspectives: a view-guided attention\nmodule for more robust text features, and a view-guided scoring strategy during\nthe final prediction. With our designed paradigm, ViewRefer achieves superior\nperformance on three benchmarks and surpasses the second-best by +2.8%, +1.2%,\nand +0.73% on Sr3D, Nr3D, and ScanRefer. Code will be released at\nhttps://github.com/ZiyuGuo99/ViewRefer3D.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Ziyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yiwen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhigang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RPTQ: Reorder-based Post-training Quantization for Large Language Models. (arXiv:2304.01089v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01089","description":"<p>Large-scale language models (LLMs) have demonstrated outstanding performance\non various tasks, but their deployment poses challenges due to their enormous\nmodel size. In this paper, we identify that the main challenge in quantizing\nLLMs stems from the different activation ranges between the channels, rather\nthan just the issue of outliers.We propose a novel reorder-based quantization\napproach, RPTQ, that addresses the issue of quantizing the activations of LLMs.\nRPTQ rearranges the channels in the activations and then quantizing them in\nclusters, thereby reducing the impact of range difference of channels. In\naddition, we reduce the storage and computation overhead by avoiding explicit\nreordering. By implementing this approach, we achieved a significant\nbreakthrough by pushing LLM models to 3 bit activation for the first time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhihang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Lin Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1\">Yuzhang Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaxiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bingzhe Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing. (arXiv:2304.02017v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.02017","description":"<p>Large language models have revolutionized the field of artificial\nintelligence and have been used in various applications. Among these models,\nChatGPT (Chat Generative Pre-trained Transformer) has been developed by OpenAI,\nit stands out as a powerful tool that has been widely adopted. ChatGPT has been\nsuccessfully applied in numerous areas, including chatbots, content generation,\nlanguage translation, personalized recommendations, and even medical diagnosis\nand treatment. Its success in these applications can be attributed to its\nability to generate human-like responses, understand natural language, and\nadapt to different contexts. Its versatility and accuracy make it a powerful\ntool for natural language processing (NLP). However, there are also limitations\nto ChatGPT, such as its tendency to produce biased responses and its potential\nto perpetuate harmful language patterns. This article provides a comprehensive\noverview of ChatGPT, its applications, advantages, and limitations.\nAdditionally, the paper emphasizes the importance of ethical considerations\nwhen using this robust tool in real-world scenarios. Finally, This paper\ncontributes to ongoing discussions surrounding artificial intelligence and its\nimpact on vision and NLP domains by providing insights into prompt engineering\ntechniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hariri_W/0/1/0/all/0/1\">Walid Hariri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT. (arXiv:2304.02213v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.02213","description":"<p>This article presents a new NLP task called structured information inference\n(SII) to address the complexities of information extraction at the device level\nin materials science. We accomplished this task by tuning GPT-3 on an existed\nperovskite solar cell FAIR(Findable, Accessible, Interoperable, Reusable)\ndataset with 91.8 F1-score and we updated the dataset with all related\nscientific papers up to now. The produced dataset is formatted and normalized,\nenabling its direct utilization as input in subsequent data analysis. This\nfeature will enable materials scientists to develop their own models by\nselecting high-quality review papers within their domain. Furthermore, we\ndesigned experiments to predict solar cells' electrical performance and\nreverse-predict parameters on both material gene and FAIR datesets through LLM.\nWe obtained comparable performance with traditional machine learning methods\nwithout feature selection, which demonstrates the potential of large language\nmodels to judge materials and design new materials like a materials scientist.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yuwei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yufei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linghu_Q/0/1/0/all/0/1\">Qingyuan Linghu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaozhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kit_C/0/1/0/all/0/1\">Chunyu Kit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grazian_C/0/1/0/all/0/1\">Clara Grazian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoex_B/0/1/0/all/0/1\">Bram Hoex</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ParroT: Translating During Chat Using Large Language Models. (arXiv:2304.02426v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.02426","description":"<p>Large language models (LLMs) like ChatGPT and GPT-4 have exhibited remarkable\nabilities on a wide range of natural language processing (NLP) tasks, including\nvarious machine translation abilities accomplished during chat. However, these\nmodels are only accessible through restricted APIs, which creates barriers to\nnew research and advancements in the field. Therefore, we propose the\n$\\mathbf{ParroT}$ framework to enhance and regulate the translation abilities\nduring chat based on open-sourced LLMs (i.e., LLaMA-7b) and human written\ntranslation and evaluation data. Specifically, ParroT reformulates translation\ndata into the instruction-following style, and introduces a \"Hint\" field for\nincorporating extra requirements to regulate the translation process.\nAccordingly, we propose three instruction types for finetuning ParroT models,\nincluding translation instruction, contrastive instruction, and error-guided\ninstruction. Experiments on Flores subsets and WMT22 test sets suggest that\ntranslation instruction improves the translation performance of vanilla LLMs\nsignificantly while error-guided instruction can lead to a further improvement,\nwhich demonstrates the importance of learning from low-quality translations\nannotated by human. Meanwhile, the ParroT models can also preserve the ability\non general tasks with the Alpaca multi-task dataset involved in finetuning.\nCodes: https://github.com/wxjiao/ParroT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jen-tse Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-04-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}