{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-04-06T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"How well do Large Language Models perform in Arithmetic tasks?. (arXiv:2304.02015v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02015","description":"<p>Large language models have emerged abilities including chain-of-thought to\nanswer math word problems step by step. Solving math word problems not only\nrequires abilities to disassemble problems via chain-of-thought but also needs\nto calculate arithmetic expressions correctly for each step. To the best of our\nknowledge, there is no work to focus on evaluating the arithmetic ability of\nlarge language models. In this work, we propose an arithmetic dataset MATH 401\nto test the latest large language models including GPT-4, ChatGPT, InstrctGPT,\nGalactica, and LLaMA with various arithmetic expressions and provide a detailed\nanalysis of the ability of large language models. MATH 401 and evaluation codes\nare released at \\url{https://github.com/GanjinZero/math401-llm}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongyi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Multimodal And Modular Ai Chef: Complex Recipe Generation From Imagery. (arXiv:2304.02016v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02016","description":"<p>The AI community has embraced multi-sensory or multi-modal approaches to\nadvance this generation of AI models to resemble expected intelligent\nunderstanding. Combining language and imagery represents a familiar method for\nspecific tasks like image captioning or generation from descriptions. This\npaper compares these monolithic approaches to a lightweight and specialized\nmethod based on employing image models to label objects, then serially\nsubmitting this resulting object list to a large language model (LLM). This use\nof multiple Application Programming Interfaces (APIs) enables better than 95%\nmean average precision for correct object lists, which serve as input to the\nlatest Open AI text generator (GPT-4). To demonstrate the API as a modular\nalternative, we solve the problem of a user taking a picture of ingredients\navailable in a refrigerator, then generating novel recipe cards tailored to\ncomplex constraints on cost, preparation time, dietary restrictions, portion\nsizes, and multiple meal plans. The research concludes that monolithic\nmultimodal models currently lack the coherent memory to maintain context and\nformat for this task and that until recently, the language models like GPT-2/3\nstruggled to format similar problems without degenerating into repetitive or\nnon-sensical combinations of ingredients. For the first time, an AI chef or\ncook seems not only possible but offers some enhanced capabilities to augment\nhuman recipe libraries in pragmatic ways. The work generates a 100-page recipe\nbook featuring the thirty top ingredients using over 2000 refrigerator images\nas initializing lists.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1\">David Noever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noever_S/0/1/0/all/0/1\">Samantha Elizabeth Miller Noever</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing. (arXiv:2304.02017v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02017","description":"<p>ChatGPT is a powerful tool in the field of artificial intelligence that has\nbeen widely used in various applications. ChatGPT has been applied successfully\nin chatbots, content generation, language translation, personalized\nrecommendations, and medical diagnosis and treatment. Its versatility and\naccuracy make it a powerful tool for natural language processing (NLP).\nHowever, there are also limitations to ChatGPT, such as its tendency to produce\nbiased responses and its potential to perpetuate harmful language patterns.\nThis article provides a comprehensive overview of ChatGPT, its applications,\nadvantages, and limitations. Additionally, the paper emphasizes the importance\nof ethical considerations when using this robust tool in real-world scenarios.\nFinally, This paper contributes to ongoing discussions surrounding artificial\nintelligence and its impact on vision and NLP domains by providing insights\ninto prompt engineering techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hariri_W/0/1/0/all/0/1\">Walid Hariri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Bibliometric Review of Large Language Models Research from 2017 to 2023. (arXiv:2304.02020v1 [cs.DL])","link":"http://arxiv.org/abs/2304.02020","description":"<p>Large language models (LLMs) are a class of language models that have\ndemonstrated outstanding performance across a range of natural language\nprocessing (NLP) tasks and have become a highly sought-after research area,\nbecause of their ability to generate human-like language and their potential to\nrevolutionize science and technology. In this study, we conduct bibliometric\nand discourse analyses of scholarly literature on LLMs. Synthesizing over 5,000\npublications, this paper serves as a roadmap for researchers, practitioners,\nand policymakers to navigate the current landscape of LLMs research. We present\nthe research trends from 2017 to early 2023, identifying patterns in research\nparadigms and collaborations. We start with analyzing the core algorithm\ndevelopments and NLP tasks that are fundamental in LLMs research. We then\ninvestigate the applications of LLMs in various fields and domains including\nmedicine, engineering, social science, and humanities. Our review also reveals\nthe dynamic, fast-paced evolution of LLMs research. Overall, this paper offers\nvaluable insights into the current state, impact, and potential of LLMs\nresearch and its applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lizhou Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lingyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zihui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sanggyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Huizi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemphill_L/0/1/0/all/0/1\">Libby Hemphill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Theory of Transformers at Initialization. (arXiv:2304.02034v1 [cs.LG])","link":"http://arxiv.org/abs/2304.02034","description":"<p>We perform an effective-theory analysis of forward-backward signal\npropagation in wide and deep Transformers, i.e., residual neural networks with\nmulti-head self-attention blocks and multilayer perceptron blocks. This\nanalysis suggests particular width scalings of initialization and training\nhyperparameters for these models. We then take up such suggestions, training\nVision and Language Transformers in practical setups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dinan_E/0/1/0/all/0/1\">Emily Dinan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaida_S/0/1/0/all/0/1\">Sho Yaida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Susan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable and Accurate Self-supervised Multimodal Representation Learning without Aligned Video and Text Data. (arXiv:2304.02080v1 [cs.CV])","link":"http://arxiv.org/abs/2304.02080","description":"<p>Scaling up weakly-supervised datasets has shown to be highly effective in the\nimage-text domain and has contributed to most of the recent state-of-the-art\ncomputer vision and multimodal neural networks. However, existing large-scale\nvideo-text datasets and mining techniques suffer from several limitations, such\nas the scarcity of aligned data, the lack of diversity in the data, and the\ndifficulty of collecting aligned data. Currently popular video-text data mining\napproach via automatic speech recognition (ASR) used in HowTo100M provides\nlow-quality captions that often do not refer to the video content. Other mining\napproaches do not provide proper language descriptions (video tags) and are\nbiased toward short clips (alt text). In this work, we show how recent advances\nin image captioning allow us to pre-train high-quality video models without any\nparallel video-text data. We pre-train several video captioning models that are\nbased on an OPT language model and a TimeSformer visual backbone. We fine-tune\nthese networks on several video captioning datasets. First, we demonstrate that\nimage captioning pseudolabels work better for pre-training than the existing\nHowTo100M ASR captions. Second, we show that pre-training on both images and\nvideos produces a significantly better network (+4 CIDER on MSR-VTT) than\npre-training on a single modality. Our methods are complementary to the\nexisting pre-training or data mining approaches and can be used in a variety of\nsettings. Given the efficacy of the pseudolabeling method, we are planning to\npublicly release the generated captions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lialin_V/0/1/0/all/0/1\">Vladislav Lialin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawls_S/0/1/0/all/0/1\">Stephen Rawls</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1\">David Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shalini Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1\">Anna Rumshisky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamza_W/0/1/0/all/0/1\">Wael Hamza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geotechnical Parrot Tales (GPT): Overcoming GPT hallucinations with prompt engineering for geotechnical applications. (arXiv:2304.02138v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02138","description":"<p>The widespread adoption of large language models (LLMs), such as OpenAI's\nChatGPT, could revolutionized various industries, including geotechnical\nengineering. However, GPT models can sometimes generate plausible-sounding but\nfalse outputs, leading to hallucinations. In this article, we discuss the\nimportance of prompt engineering in mitigating these risks and harnessing the\nfull potential of GPT for geotechnical applications. We explore the challenges\nand pitfalls associated with LLMs and highlight the role of context in ensuring\naccurate and valuable responses. Furthermore, we examine the development of\ncontext-specific search engines and the potential of LLMs to become a natural\ninterface for complex tasks, such as data analysis and design. We also develop\na unified interface using natural language to handle complex geotechnical\nengineering tasks and data analysis. By integrating GPT into geotechnical\nengineering workflows, professionals can streamline their work and develop\nsustainable and resilient infrastructure systems for the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_K/0/1/0/all/0/1\">Krishna Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Data Fusion Framework for Multi-Domain Morality Learning. (arXiv:2304.02144v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02144","description":"<p>Language models can be trained to recognize the moral sentiment of text,\ncreating new opportunities to study the role of morality in human life. As\ninterest in language and morality has grown, several ground truth datasets with\nmoral annotations have been released. However, these datasets vary in the\nmethod of data collection, domain, topics, instructions for annotators, etc.\nSimply aggregating such heterogeneous datasets during training can yield models\nthat fail to generalize well. We describe a data fusion framework for training\non multiple heterogeneous datasets that improve performance and\ngeneralizability. The model uses domain adversarial training to align the\ndatasets in feature space and a weighted loss function to deal with label\nshift. We show that the proposed framework achieves state-of-the-art\nperformance in different datasets compared to prior works in morality\ninference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Siyi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokhberian_N/0/1/0/all/0/1\">Negar Mokhberian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_K/0/1/0/all/0/1\">Kristina Lerman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I2I: Initializing Adapters with Improvised Knowledge. (arXiv:2304.02168v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02168","description":"<p>Adapters present a promising solution to the catastrophic forgetting problem\nin continual learning. However, training independent Adapter modules for every\nnew task misses an opportunity for cross-task knowledge transfer. We propose\nImprovise to Initialize (I2I), a continual learning algorithm that initializes\nAdapters for incoming tasks by distilling knowledge from previously-learned\ntasks' Adapters. We evaluate I2I on CLiMB, a multimodal continual learning\nbenchmark, by conducting experiments on sequences of visual question answering\ntasks. Adapters trained with I2I consistently achieve better task accuracy than\nindependently-trained Adapters, demonstrating that our algorithm facilitates\nknowledge transfer between task Adapters. I2I also results in better cross-task\nknowledge transfer than the state-of-the-art AdapterFusion without incurring\nthe associated parametric cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_T/0/1/0/all/0/1\">Tejas Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1\">Furong Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1\">Mohammad Rostami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Impact of Voice Anonymization on Speech-Based COVID-19 Detection. (arXiv:2304.02181v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02181","description":"<p>With advances seen in deep learning, voice-based applications are burgeoning,\nranging from personal assistants, affective computing, to remote disease\ndiagnostics. As the voice contains both linguistic and paralinguistic\ninformation (e.g., vocal pitch, intonation, speech rate, loudness), there is\ngrowing interest in voice anonymization to preserve speaker privacy and\nidentity. Voice privacy challenges have emerged over the last few years and\nfocus has been placed on removing speaker identity while keeping linguistic\ncontent intact. For affective computing and disease monitoring applications,\nhowever, the paralinguistic content may be more critical. Unfortunately, the\neffects that anonymization may have on these systems are still largely unknown.\nIn this paper, we fill this gap and focus on one particular health monitoring\napplication: speech-based COVID-19 diagnosis. We test two popular anonymization\nmethods and their impact on five different state-of-the-art COVID-19 diagnostic\nsystems using three public datasets. We validate the effectiveness of the\nanonymization methods, compare their computational complexity, and quantify the\nimpact across different testing scenarios for both within- and across-dataset\nconditions. Lastly, we show the benefits of anonymization as a data\naugmentation tool to help recover some of the COVID-19 diagnostic accuracy loss\nseen with anonymized data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imoussaine_Aikous_M/0/1/0/all/0/1\">Mohamed Imoussa&#xef;ne-A&#xef;kous</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cote_Lussier_C/0/1/0/all/0/1\">Carolyn C&#xf4;t&#xe9;-Lussier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falk_T/0/1/0/all/0/1\">Tiago H. Falk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unleashing the Power of ChatGPT for Translation: An Empirical Study. (arXiv:2304.02182v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02182","description":"<p>The recently released ChatGPT has demonstrated surprising abilities in\nnatural language understanding and natural language generation. Machine\ntranslation is an important and extensively studied task in the field of\nnatural language processing, which heavily relies on the abilities of language\nunderstanding and generation. Thus, in this paper, we explore how to assist\nmachine translation with ChatGPT. We adopt several translation prompts on a\nwide range of translations. Our experimental results show that ChatGPT with\ndesigned translation prompts can achieve comparable or better performance over\nprofessional translation systems for high-resource language translations but\nlags behind significantly on low-resource translations. We further evaluate the\ntranslation quality using multiple references, and ChatGPT achieves superior\nperformance compared to the professional systems. We also conduct experiments\non domain-specific translations, the final results show that ChatGPT is able to\ncomprehend the provided domain keyword and adjust accordingly to output proper\ntranslations. At last, we perform few-shot prompts that show consistent\nimprovement across different base prompts. Our work provides empirical evidence\nthat ChatGPT still has great potential in translations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1\">Feng Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Document-Level Machine Translation with Large Language Models. (arXiv:2304.02210v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02210","description":"<p>Large language models (LLMs) such as Chat-GPT can produce coherent, cohesive,\nrelevant, and fluent answers for various natural language processing (NLP)\ntasks. Taking document-level machine translation (MT) as a testbed, this paper\nprovides an in-depth evaluation of LLMs' ability on discourse modeling. The\nstudy fo-cuses on three aspects: 1) Effects of Discourse-Aware Prompts, where\nwe investigate the impact of different prompts on document-level translation\nquality and discourse phenomena; 2) Comparison of Translation Models, where we\ncompare the translation performance of Chat-GPT with commercial MT systems and\nadvanced document-level MT methods; 3) Analysis of Discourse Modelling\nAbilities, where we further probe discourse knowledge encoded in LLMs and\nexamine the impact of training techniques on discourse modeling. By evaluating\na number of benchmarks, we surprisingly find that 1) leveraging their powerful\nlong-text mod-eling capabilities, ChatGPT outperforms commercial MT systems in\nterms of human evaluation. 2) GPT-4 demonstrates a strong ability to explain\ndiscourse knowledge, even through it may select incorrect translation\ncandidates in contrastive testing. 3) ChatGPT and GPT-4 have demonstrated\nsuperior performance and show potential to become a new and promising paradigm\nfor document-level translation. This work highlights the challenges and\nopportunities of discourse modeling for LLMs, which we hope can inspire the\nfuture design and evaluation of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chenyang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_T/0/1/0/all/0/1\">Tianbo Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT. (arXiv:2304.02213v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02213","description":"<p>This article presents a new NLP task called structured information inference\n(SIS) to address the complexities of information extraction at the device level\nin materials science. We accomplished this task by finetuning GPT-3 on a\nexsiting perovskite solar cell FAIR dataset with 91.8 F1-score and we updated\nthe dataset with all related scientific papers up to now. The produced dataset\nis formatted and normalized, enabling its direct utilization as input in\nsubsequent data analysis. This feature will enable materials scientists to\ndevelop their own models by selecting high-quality review papers within their\ndomain. Furthermore, we designed experiments to predict PCE and reverse-predict\nparameters and obtained comparable performance with DFT, which demonstrates the\npotential of large language models to judge materials and design new materials\nlike a materials scientist.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wa_Y/0/1/0/all/0/1\">Yuwei Wa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yufei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linghu_Q/0/1/0/all/0/1\">Qingyuan Linghu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaozhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kit_C/0/1/0/all/0/1\">Chunyu Kit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grazian_C/0/1/0/all/0/1\">Clara Grazian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoex_B/0/1/0/all/0/1\">Bram Hoex</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ericson: An Interactive Open-Domain Conversational Search Agent. (arXiv:2304.02233v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02233","description":"<p>Open-domain conversational search (ODCS) aims to provide valuable, up-to-date\ninformation, while maintaining natural conversations to help users refine and\nultimately answer information needs. However, creating an effective and robust\nODCS agent is challenging. In this paper, we present a fully functional ODCS\nsystem, Ericson, which includes state-of-the-art question answering and\ninformation retrieval components, as well as intent inference and dialogue\nmanagement models for proactive question refinement and recommendations. Our\nsystem was stress-tested in the Amazon Alexa Prize, by engaging in live\nconversations with thousands of Alexa users, thus providing empirical basis for\nthe analysis of the ODCS system in real settings. Our interaction data analysis\nrevealed that accurate intent classification, encouraging user engagement, and\ncareful proactive recommendations contribute most to the users satisfaction.\nOur study further identifies limitations of the existing search techniques, and\ncan serve as a building block for the next generation of ODCS agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmadvand_A/0/1/0/all/0/1\">Ali Ahmadvand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jason Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karisani_P/0/1/0/all/0/1\">Payam Karisani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agichtein_E/0/1/0/all/0/1\">Eugene Agichtein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Structure and Style: Political Bias Detection in News by Inducing Document Hierarchy. (arXiv:2304.02247v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02247","description":"<p>We address an important gap in detection of political bias in news articles.\nPrevious works that perform supervised document classification can be biased\ntowards the writing style of each news outlet, leading to overfitting and\nlimited generalizability. Our approach overcomes this limitation by considering\nboth the sentence-level semantics and the document-level rhetorical structure,\nresulting in a more robust and style-agnostic approach to detecting political\nbias in news articles. We introduce a novel multi-head hierarchical attention\nmodel that effectively encodes the structure of long documents through a\ndiverse ensemble of attention heads. While journalism follows a formalized\nrhetorical structure, the writing style may vary by news outlet. We demonstrate\nthat our method overcomes this domain dependency and outperforms previous\napproaches for robustness and accuracy. Further analysis demonstrates the\nability of our model to capture the discourse structures commonly used in the\njournalism domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jiwoo Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1\">Yejin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">Jaemin Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiyoung Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1\">James Thorne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personality-aware Human-centric Multimodal Reasoning: A New Task. (arXiv:2304.02313v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02313","description":"<p>Multimodal reasoning, an area of artificial intelligence that aims at make\ninferences from multimodal signals such as vision, language and speech, has\ndrawn more and more attention in recent years. People with different\npersonalities may respond differently to the same situation. However, such\nindividual personalities were ignored in the previous studies. In this work, we\nintroduce a new Personality-aware Human-centric Multimodal Reasoning\n(Personality-aware HMR) task, and accordingly construct a new dataset based on\nThe Big Bang Theory television shows, to predict the behavior of a specific\nperson at a specific moment, given the multimodal information of its past and\nfuture moments. The Myers-Briggs Type Indicator (MBTI) was annotated and\nutilized in the task to represent individuals' personalities. We benchmark the\ntask by proposing three baseline methods, two were adapted from the related\ntasks and one was newly proposed for our task. The experimental results\ndemonstrate that personality can effectively improve the performance of\nhuman-centric multimodal reasoning. To further solve the lack of personality\nannotation in real-life scenes, we introduce an extended task called\nPersonality-predicted HMR, and propose the corresponding methods, to predict\nthe MBTI personality at first, and then use the predicted personality to help\nmultimodal reasoning. The experimental results show that our method can\naccurately predict personality and achieves satisfactory multimodal reasoning\nperformance without relying on personality annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaochen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiangqing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_R/0/1/0/all/0/1\">Rui Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Multimodal Entity and Relation Extraction with Variational Information Bottleneck. (arXiv:2304.02328v1 [cs.MM])","link":"http://arxiv.org/abs/2304.02328","description":"<p>This paper studies the multimodal named entity recognition (MNER) and\nmultimodal relation extraction (MRE), which are important for multimedia social\nplatform analysis. The core of MNER and MRE lies in incorporating evident\nvisual information to enhance textual semantics, where two issues inherently\ndemand investigations. The first issue is modality-noise, where the\ntask-irrelevant information in each modality may be noises misleading the task\nprediction. The second issue is modality-gap, where representations from\ndifferent modalities are inconsistent, preventing from building the semantic\nalignment between the text and image. To address these issues, we propose a\nnovel method for MNER and MRE by Multi-Modal representation learning with\nInformation Bottleneck (MMIB). For the first issue, a refinement-regularizer\nprobes the information-bottleneck principle to balance the predictive evidence\nand noisy information, yielding expressive representations for prediction. For\nthe second issue, an alignment-regularizer is proposed, where a mutual\ninformation-based item works in a contrastive manner to regularize the\nconsistent text-image representations. To our best knowledge, we are the first\nto explore variational IB estimation for MNER and MRE. Experiments show that\nMMIB achieves the state-of-the-art performances on three public benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shiyao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiangxia Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1\">Xin Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_J/0/1/0/all/0/1\">Jiawei Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Quangang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tingwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jinqiao Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ParroT: Translating During Chat Using Large Language Models. (arXiv:2304.02426v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02426","description":"<p>Large language models (LLMs) like ChatGPT and GPT-4 have exhibited remarkable\nabilities on a wide range of natural language processing (NLP) tasks, including\nvarious machine translation abilities accomplished during chat. However, these\nmodels are only accessible through restricted APIs, which creates barriers to\nnew research and advancements in the field. Therefore, we propose the\n$\\mathbf{ParroT}$ framework to enhance and regulate the translation abilities\nduring chat based on open-sourced LLMs (i.e., LLaMA-7b) and human written\ntranslation and evaluation data. Specifically, ParroT reformulates translation\ndata into the instruction-following style, and introduces a \"Hint\" field for\nincorporating extra requirements to regulate the translation process.\nAccordingly, we propose three instruction types for finetuning ParroT models,\nincluding translation instruction, contrastive instruction, and error-guided\ninstruction. Experiments on two Flores subsets and WMT22 test sets suggest that\ntranslation instruction improves the translation performance of vanilla LLMs\nsignificantly while error-guided instruction can lead to a further improvement,\nwhich demonstrates the importance of learning from low-quality translations\nannotated by human. Meanwhile, the ParroT models can also preserve the ability\non general tasks with the Alpaca multi-task dataset involved in finetuning.\nCodes: https://github.com/wxjiao/ParroT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jen-tse Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparative Analysis of CHATGPT and the evolution of language models. (arXiv:2304.02468v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02468","description":"<p>Interest in Large Language Models (LLMs) has increased drastically since the\nemergence of ChatGPT and the outstanding positive societal response to the ease\nwith which it performs tasks in Natural Language Processing (NLP). The triumph\nof ChatGPT, however, is how it seamlessly bridges the divide between language\ngeneration and knowledge models. In some cases, it provides anecdotal evidence\nof a framework for replicating human intuition over a knowledge domain. This\npaper highlights the prevailing ideas in NLP, including machine translation,\nmachine summarization, question-answering, and language generation, and\ncompares the performance of ChatGPT with the major algorithms in each of these\ncategories using the Spontaneous Quality (SQ) score. A strategy for validating\nthe arguments and results of ChatGPT is presented summarily as an example of\nsafe, large-scale adoption of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ogundare_O/0/1/0/all/0/1\">Oluwatosin Ogundare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araya_G/0/1/0/all/0/1\">Gustavo Quiros Araya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring AI-Generated Text in Student Writing: How Does AI Help?. (arXiv:2304.02478v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02478","description":"<p>English as foreign language_EFL_students' use of text generated from\nartificial intelligence_AI_natural language generation_NLG_tools may improve\ntheir writing quality. However, it remains unclear to what extent AI-generated\ntext in these students' writing might lead to higher-quality writing. We\nexplored 23 Hong Kong secondary school students' attempts to write stories\ncomprising their own words and AI-generated text. Human experts scored the\nstories for dimensions of content, language and organization. We analyzed the\nbasic organization and structure and syntactic complexity of the stories'\nAI-generated text and performed multiple linear regression and cluster\nanalyses. The results show the number of human words and the number of\nAI-generated words contribute significantly to scores. Besides, students can be\ngrouped into competent and less competent writers who use more AI-generated\ntext or less AI-generated text compared to their peers. Comparisons of clusters\nreveal some benefit of AI-generated text in improving the quality of both\nhigh-scoring students' and low-scoring students' writing. The findings can\ninform pedagogical strategies to use AI-generated text for EFL students'\nwriting and to address digital divides. This study contributes designs of NLG\ntools and writing activities to implement AI-generated text in schools.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Woo_D/0/1/0/all/0/1\">David James Woo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Susanto_H/0/1/0/all/0/1\">Hengky Susanto</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_C/0/1/0/all/0/1\">Chi Ho Yeung</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1\">Kai Guo</a> (3), (4) <a href=\"http://arxiv.org/find/cs/1/au:+Fung_A/0/1/0/all/0/1\">April Ka Yeng Fung</a> ((1) Precious Blood Secondary School, Hong Kong, (2) Department of Science and Environmental Studies, The Education University of Hong Kong, Hong Kong, (3) Faculty of Education, The University of Hong Kong, Hong Kong, and (4) Hoi Ping Chamber of Commerce Secondary School, Hong Kong)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rediscovering Hashed Random Projections for Efficient Quantization of Contextualized Sentence Embeddings. (arXiv:2304.02481v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02481","description":"<p>Training and inference on edge devices often requires an efficient setup due\nto computational limitations. While pre-computing data representations and\ncaching them on a server can mitigate extensive edge device computation, this\nleads to two challenges. First, the amount of storage required on the server\nthat scales linearly with the number of instances. Second, the bandwidth\nrequired to send extensively large amounts of data to an edge device. To reduce\nthe memory footprint of pre-computed data representations, we propose a simple,\nyet effective approach that uses randomly initialized hyperplane projections.\nTo further reduce their size by up to 98.96%, we quantize the resulting\nfloating-point representations into binary vectors. Despite the greatly reduced\nsize, we show that the embeddings remain effective for training models across\nvarious English and German sentence classification tasks that retain 94%--99%\nof their floating-point.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamster_U/0/1/0/all/0/1\">Ulf A. Hamster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Ji-Ung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geyken_A/0/1/0/all/0/1\">Alexander Geyken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying the Roles of Visual, Linguistic, and Visual-Linguistic Complexity in Verb Acquisition. (arXiv:2304.02492v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02492","description":"<p>Children typically learn the meanings of nouns earlier than the meanings of\nverbs. However, it is unclear whether this asymmetry is a result of complexity\nin the visual structure of categories in the world to which language refers,\nthe structure of language itself, or the interplay between the two sources of\ninformation. We quantitatively test these three hypotheses regarding early verb\nlearning by employing visual and linguistic representations of words sourced\nfrom large-scale pre-trained artificial neural networks. Examining the\nstructure of both visual and linguistic embedding spaces, we find, first, that\nthe representation of verbs is generally more variable and less discriminable\nwithin domain than the representation of nouns. Second, we find that if only\none learning instance per category is available, visual and linguistic\nrepresentations are less well aligned in the verb system than in the noun\nsystem. However, in parallel with the course of human language development, if\nmultiple learning instances per category are available, visual and linguistic\nrepresentations become almost as well aligned in the verb system as in the noun\nsystem. Third, we compare the relative contributions of factors that may\npredict learning difficulty for individual words. A regression analysis reveals\nthat visual variability is the strongest factor that internally drives verb\nlearning, followed by visual-linguistic alignment and linguistic variability.\nBased on these results, we conclude that verb acquisition is influenced by all\nthree sources of complexity, but that the variability of visual structure poses\nthe most significant challenge for verb learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuchen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarr_M/0/1/0/all/0/1\">Michael J. Tarr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yurovsky_D/0/1/0/all/0/1\">Daniel Yurovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification. (arXiv:2304.02496v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02496","description":"<p>Recent advances in large language models (LLMs) have shown impressive ability\nin biomedical question-answering, but have not been adequately investigated for\nmore specific biomedical applications. This study investigates the performance\nof LLMs such as the ChatGPT family of models (GPT-3.5s, GPT-4) in biomedical\ntasks beyond question-answering. Because no patient data can be passed to the\nOpenAI API public interface, we evaluated model performance with over 10000\nsamples as proxies for two fundamental tasks in the clinical domain -\nclassification and reasoning. The first task is classifying whether statements\nof clinical and policy recommendations in scientific literature constitute\nhealth advice. The second task is causal relation detection from the biomedical\nliterature. We compared LLMs with simpler models, such as bag-of-words (BoW)\nwith logistic regression, and fine-tuned BioBERT models. Despite the excitement\naround viral ChatGPT, we found that fine-tuning for two fundamental NLP tasks\nremained the best strategy. The simple BoW model performed on par with the most\ncomplex LLM prompting. Prompt engineering required significant investment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Sheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Van_H/0/1/0/all/0/1\">Hoang Van</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aerts_H/0/1/0/all/0/1\">Hugo JWL Aerts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savova_G/0/1/0/all/0/1\">Guergana K. Savova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitterman_D/0/1/0/all/0/1\">Danielle S. Bitterman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PWESuite: Phonetic Word Embeddings and Tasks They Facilitate. (arXiv:2304.02541v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02541","description":"<p>Word embeddings that map words into a fixed-dimensional vector space are the\nbackbone of modern NLP. Most word embedding methods encode semantic\ninformation. However, phonetic information, which is important for some tasks,\nis often overlooked. In this work, we develop several novel methods which\nleverage articulatory features to build phonetically informed word embeddings,\nand present a set of phonetic word embeddings to encourage their community\ndevelopment, evaluation and use. While several methods for learning phonetic\nword embeddings already exist, there is a lack of consistency in evaluating\ntheir effectiveness. Thus, we also proposes several ways to evaluate both\nintrinsic aspects of phonetic word embeddings, such as word retrieval and\ncorrelation with sound similarity, and extrinsic performances, such as rhyme\nand cognate detection and sound analogies. We hope that our suite of tasks will\npromote reproducibility and provide direction for future research on phonetic\nword embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zouhar_V/0/1/0/all/0/1\">Vil&#xe9;m Zouhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kalvin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Chenxuan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlson_N/0/1/0/all/0/1\">Nathaniel Carlson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_N/0/1/0/all/0/1\">Nathaniel Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortensen_D/0/1/0/all/0/1\">David Mortensen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-like Summarization Evaluation with ChatGPT. (arXiv:2304.02554v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02554","description":"<p>Evaluating text summarization is a challenging problem, and existing\nevaluation metrics are far from satisfactory. In this study, we explored\nChatGPT's ability to perform human-like summarization evaluation using four\nhuman evaluation methods on five datasets. We found that ChatGPT was able to\ncomplete annotations relatively smoothly using Likert scale scoring, pairwise\ncomparison, Pyramid, and binary factuality evaluation. Additionally, it\noutperformed commonly used automatic evaluation metrics on some datasets.\nFurthermore, we discussed the impact of different prompts, compared its\nperformance with that of human evaluation, and analyzed the generated\nexplanations and invalid responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_J/0/1/0/all/0/1\">Jie Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Renliang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xunjian Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shiping Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Summarization: Designing AI Support for Real-World Expository Writing Tasks. (arXiv:2304.02623v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02623","description":"<p>Large language models have introduced exciting new opportunities and\nchallenges in designing and developing new AI-assisted writing support tools.\nRecent work has shown that leveraging this new technology can transform writing\nin many scenarios such as ideation during creative writing, editing support,\nand summarization. However, AI-supported expository writing--including\nreal-world tasks like scholars writing literature reviews or doctors writing\nprogress notes--is relatively understudied. In this position paper, we argue\nthat developing AI supports for expository writing has unique and exciting\nresearch challenges and can lead to high real-world impacts. We characterize\nexpository writing as evidence-based and knowledge-generating: it contains\nsummaries of external documents as well as new information or knowledge. It can\nbe seen as the product of authors' sensemaking process over a set of source\ndocuments, and the interplay between reading, reflection, and writing opens up\nnew opportunities for designing AI support. We sketch three components for AI\nsupport design and discuss considerations for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zejiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+August_T/0/1/0/all/0/1\">Tal August</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siangliulue_P/0/1/0/all/0/1\">Pao Siangliulue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1\">Jonathan Bragg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammerbacher_J/0/1/0/all/0/1\">Jeff Hammerbacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Joseph Chee Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1\">David Sontag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Explainable AI Writing Assistants for Non-native English Speakers. (arXiv:2304.02625v1 [cs.CL])","link":"http://arxiv.org/abs/2304.02625","description":"<p>We highlight the challenges faced by non-native speakers when using AI\nwriting assistants to paraphrase text. Through an interview study with 15\nnon-native English speakers (NNESs) with varying levels of English proficiency,\nwe observe that they face difficulties in assessing paraphrased texts generated\nby AI writing assistants, largely due to the lack of explanations accompanying\nthe suggested paraphrases. Furthermore, we examine their strategies to assess\nAI-generated texts in the absence of such explanations. Drawing on the needs of\nNNESs identified in our interview, we propose four potential user interfaces to\nenhance the writing experience of NNESs using AI writing assistants. The\nproposed designs focus on incorporating explanations to better support NNESs in\nunderstanding and evaluating the AI-generated paraphrasing suggestions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yewon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Mina Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghwi Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sung-Ju Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CokeBERT: Contextual Knowledge Selection and Embedding towards Enhanced Pre-Trained Language Models. (arXiv:2009.13964v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.13964","description":"<p>Several recent efforts have been devoted to enhancing pre-trained language\nmodels (PLMs) by utilizing extra heterogeneous knowledge in knowledge graphs\n(KGs) and achieved consistent improvements on various knowledge-driven NLP\ntasks. However, most of these knowledge-enhanced PLMs embed static sub-graphs\nof KGs (\"knowledge context\"), regardless of that the knowledge required by PLMs\nmay change dynamically according to specific text (\"textual context\"). In this\npaper, we propose a novel framework named Coke to dynamically select contextual\nknowledge and embed knowledge context according to textual context for PLMs,\nwhich can avoid the effect of redundant and ambiguous knowledge in KGs that\ncannot match the input text. Our experimental results show that Coke\noutperforms various baselines on typical knowledge-driven NLP tasks, indicating\nthe effectiveness of utilizing dynamic knowledge context for language\nunderstanding. Besides the performance improvements, the dynamically selected\nknowledge in Coke can describe the semantics of text-related knowledge in a\nmore interpretable form than the conventional PLMs. Our source code and\ndatasets will be available to provide more details for Coke.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yusheng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Undivided Attention: Are Intermediate Layers Necessary for BERT?. (arXiv:2012.11881v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.11881","description":"<p>In recent times, BERT-based models have been extremely successful in solving\na variety of natural language processing (NLP) tasks such as reading\ncomprehension, natural language inference, sentiment analysis, etc. All\nBERT-based architectures have a self-attention block followed by a block of\nintermediate layers as the basic building component. However, a strong\njustification for the inclusion of these intermediate layers remains missing in\nthe literature. In this work we investigate the importance of intermediate\nlayers on the overall network performance of downstream tasks. We show that\nreducing the number of intermediate layers and modifying the architecture for\nBERT-BASE results in minimal loss in fine-tuning accuracy for downstream tasks\nwhile decreasing the number of parameters and training time of the model.\nAdditionally, we use centered kernel alignment and probing linear classifiers\nto gain insight into our architectural modifications and justify that removal\nof intermediate layers has little impact on the fine-tuned accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1\">Sharath Nittur Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarah_A/0/1/0/all/0/1\">Anthony Sarah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Translation from Signed to Spoken Languages: State of the Art and Challenges. (arXiv:2202.03086v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.03086","description":"<p>Automatic translation from signed to spoken languages is an interdisciplinary\nresearch domain, lying on the intersection of computer vision, machine\ntranslation and linguistics. Nevertheless, research in this domain is performed\nmostly by computer scientists in isolation. As the domain is becoming\nincreasingly popular - the majority of scientific papers on the topic of sign\nlanguage translation have been published in the past three years - we provide\nan overview of the state of the art as well as some required background in the\ndifferent related disciplines. We give a high-level introduction to sign\nlanguage linguistics and machine translation to illustrate the requirements of\nautomatic sign language translation. We present a systematic literature review\nto illustrate the state of the art in the domain and then, harking back to the\nrequirements, lay out several challenges for future research. We find that\nsignificant advances have been made on the shoulders of spoken language machine\ntranslation research. However, current approaches are often not linguistically\nmotivated or are not adapted to the different input modality of sign languages.\nWe explore challenges related to the representation of sign language data, the\ncollection of datasets, the need for interdisciplinary research and\nrequirements for moving beyond research, towards applications. Based on our\nfindings, we advocate for interdisciplinary research and to base future\nresearch on linguistic analysis of sign languages. Furthermore, the inclusion\nof deaf and hearing end users of sign language translation applications in use\ncase identification, data collection and evaluation is of the utmost importance\nin the creation of useful sign language translation models. We recommend\niterative, human-in-the-loop, design and development of sign language\ntranslation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coster_M/0/1/0/all/0/1\">Mathieu De Coster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shterionov_D/0/1/0/all/0/1\">Dimitar Shterionov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herreweghe_M/0/1/0/all/0/1\">Mieke Van Herreweghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dambre_J/0/1/0/all/0/1\">Joni Dambre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding. (arXiv:2203.05711v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05711","description":"<p>Despite recent advances of AI, story understanding remains an open and\nunder-investigated problem. We collect, preprocess, and publicly release a\nvideo-language story dataset, Synopses of Movie Narratives (SyMoN), containing\n5,193 video summaries of popular movies and TV series with a total length of\n869 hours. SyMoN captures naturalistic storytelling videos made by human\ncreators and intended for a human audience. As a prototypical and naturalistic\nstory dataset, SyMoN features high coverage of multimodal story events and\nabundant mental-state descriptions. Its use of storytelling techniques cause\ncross-domain semantic gaps that provide appropriate challenges to existing\nmodels. We establish benchmarks on video-text retrieval and zero-shot alignment\non movie summary videos, which showcase the importance of in-domain data and\nlong-term memory in story understanding. With SyMoN, we hope to lay the\ngroundwork for progress in multimodal story understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yidan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_Q/0/1/0/all/0/1\">Qin Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The optimality of word lengths. Theoretical foundations and an empirical study. (arXiv:2208.10384v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.10384","description":"<p>Zipf's law of abbreviation, namely the tendency of more frequent words to be\nshorter, has been viewed as a manifestation of compression, i.e. the\nminimization of the length of forms -- a universal principle of natural\ncommunication. Although the claim that languages are optimized has become\ntrendy, attempts to measure the degree of optimization of languages have been\nrather scarce. Here we present two optimality scores that are dualy normalized,\nnamely, they are normalized with respect to both the minimum and the random\nbaseline. We analyze the theoretical and statistical pros and cons of these and\nother scores. Harnessing the best score, we quantify for the first time the\ndegree of optimality of word lengths in languages. This indicates that\nlanguages are optimized to 62 or 67 percent on average (depending on the\nsource) when word lengths are measured in characters, and to 65 percent on\naverage when word lengths are measured in time. In general, spoken word\ndurations are more optimized than written word lengths in characters. Our work\npaves the way to measure the degree of optimality of the vocalizations or\ngestures of other species, and to compare them against written, spoken, or\nsigned human languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petrini_S/0/1/0/all/0/1\">Sonia Petrini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casas_i_Munoz_A/0/1/0/all/0/1\">Antoni Casas-i-Mu&#xf1;oz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cluet_i_Martinell_J/0/1/0/all/0/1\">Jordi Cluet-i-Martinell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengxue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bentz_C/0/1/0/all/0/1\">Christian Bentz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Paragraph-Level Vision-Language Semantic Alignment for Multi-Modal Summarization. (arXiv:2208.11303v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.11303","description":"<p>Most current multi-modal summarization methods follow a cascaded manner,\nwhere an off-the-shelf object detector is first used to extract visual\nfeatures, then these features are fused with language representations to\ngenerate the summary with an encoder-decoder model. The cascaded way cannot\ncapture the semantic alignments between images and paragraphs, which are\ncrucial to a precise summary. In this paper, we propose ViL-Sum to jointly\nmodel paragraph-level \\textbf{Vi}sion-\\textbf{L}anguage Semantic Alignment and\nMulti-Modal \\textbf{Sum}marization. The core of ViL-Sum is a joint multi-modal\nencoder with two well-designed tasks, image reordering and image selection. The\njoint multi-modal encoder captures the interactions between modalities, where\nthe reordering task guides the model to learn paragraph-level semantic\nalignment and the selection task guides the model to selected summary-related\nimages in the final summary. Experimental results show that our proposed\nViL-Sum significantly outperforms current state-of-the-art methods. In further\nanalysis, we find that two well-designed tasks and joint multi-modal encoder\ncan effectively guide the model to learn reasonable paragraphs-images and\nsummary-images relations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Chenhao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xinnian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuangzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversion of Legal Agreements into Smart Legal Contracts using NLP. (arXiv:2210.08954v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2210.08954","description":"<p>A Smart Legal Contract (SLC) is a specialized digital agreement comprising\nnatural language and computable components. The Accord Project provides an\nopen-source SLC framework containing three main modules: Cicero, Concerto, and\nErgo. Currently, we need lawyers, programmers, and clients to work together\nwith great effort to create a usable SLC using the Accord Project. This paper\nproposes a pipeline to automate the SLC creation process with several Natural\nLanguage Processing (NLP) models to convert law contracts to the Accord\nProject's Concerto model. After evaluating the proposed pipeline, we discovered\nthat our NER pipeline accurately detects CiceroMark from Accord Project\ntemplate text with an accuracy of 0.8. Additionally, our Question Answering\nmethod can extract one-third of the Concerto variables from the template text.\nWe also delve into some limitations and possible future research for the\nproposed pipeline. Finally, we describe a web interface enabling users to build\nSLCs. This interface leverages the proposed pipeline to convert text documents\nto Smart Legal Contracts by using NLP models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Eason Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roche_N/0/1/0/all/0/1\">Niall Roche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_Y/0/1/0/all/0/1\">Yuen-Hsien Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_W/0/1/0/all/0/1\">Walter Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shangguan_J/0/1/0/all/0/1\">Jiangbo Shangguan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moore_A/0/1/0/all/0/1\">Alastair Moore</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DreamArtist: Towards Controllable One-Shot Text-to-Image Generation via Positive-Negative Prompt-Tuning. (arXiv:2211.11337v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.11337","description":"<p>Large-scale text-to-image generation models have achieved remarkable progress\nin synthesizing high-quality, feature-rich images with high resolution guided\nby texts. However, these models often struggle with novel concepts, eg, new\nstyles, object entities, etc. Although recent attempts have employed\nfine-tuning or prompt-tuning strategies to teach the pre-trained diffusion\nmodel novel concepts from a reference image set,they have the drawback of\noverfitting to the given reference images, particularly in one-shot\napplications, which is harmful to generate diverse and high-quality images\nwhile maintaining generation controllability.\n</p>\n<p>To tackle this challenge, we present a simple yet effective method called\nDreamArtist, which employs a positive-negative prompt-tuning learning strategy.\nSpecifically, DreamArtist incorporates both positive and negative embeddings\nand jointly trains them. The positive embedding aggressively captures the\nsalient characteristics of the reference image to drive diversified generation\nand the negative embedding rectifies inadequacies from the positive embedding.\nIt learns not only what is correct, but also what can be avoided or improved.\nWe have conducted extensive experiments and evaluated the proposed method from\nimage similarity and diversity, generation controllability, and style cloning.\nAnd our DreamArtist has achieved a superior generation performance over\nexisting methods. Besides, our additional evaluation on extended tasks,\nincluding concept compositions and prompt-guided image editing, demonstrates\nits effectiveness for more applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Ziyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1\">Pengxu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformers are Parameter-Efficient Audio-Visual Learners. (arXiv:2212.07983v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2212.07983","description":"<p>Vision transformers (ViTs) have achieved impressive results on various\ncomputer vision tasks in the last several years. In this work, we study the\ncapability of frozen ViTs, pretrained only on visual data, to generalize to\naudio-visual data without finetuning any of its original parameters. To do so,\nwe propose a latent audio-visual hybrid (LAVISH) adapter that adapts pretrained\nViTs to audio-visual tasks by injecting a small number of trainable parameters\ninto every layer of a frozen ViT. To efficiently fuse visual and audio cues,\nour LAVISH adapter uses a small set of latent tokens, which form an attention\nbottleneck, thus, eliminating the quadratic cost of standard cross-attention.\nCompared to the existing modality-specific audio-visual methods, our approach\nachieves competitive or even better performance on various audio-visual tasks\nwhile using fewer tunable parameters and without relying on costly audio\npretraining or external audio encoders. Our code is available at\nhttps://genjib.github.io/project_page/LAVISH/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yan-Bo Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yi-Lin Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Generation of Paired Antibody Chain Sequences through Encoder-Decoder Language Model. (arXiv:2301.02748v3 [q-bio.BM] UPDATED)","link":"http://arxiv.org/abs/2301.02748","description":"<p>Protein language models (LMs) have been successful in sequence, structural\nand functional predictions. However, currently, protein LMs are limited to\nencoder- or decoder-only architectures for single sequences while many\nbiological contexts involve protein-protein interactions. Here, we introduce\npAbT5, which models antibody chain pairing as forward- and back-translations\nusing a T5-based architecture. We show that pAbT5 accurately reflects chain\npairing through sequence generation. Our protein LM generates variable-length\nsequences and its next-word prediction probability agrees with\nposition-specific scoring matrix from sequence alignment. Like other works in\nprotein LM, pAbT5 performs state-of-the-art unsupervised prediction on\nexperimental measurements. To the best of our knowledge, pAbT5 is the first\ngenerative encoder-decoder protein LM for protein-protein interactions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Chu_S/0/1/0/all/0/1\">Simon K.S. Chu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wei_K/0/1/0/all/0/1\">Kathy Y. Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Learners Meet Web Image-Text Pairs. (arXiv:2301.07088v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2301.07088","description":"<p>Most recent self-supervised learning methods are pre-trained on the\nwell-curated ImageNet-1K dataset. In this work, given the excellent scalability\nof web data, we consider self-supervised pre-training on noisy web sourced\nimage-text paired data. First, we conduct a benchmark study of representative\nself-supervised pre-training methods on large-scale web data in a like-for-like\nsetting. We compare a range of methods, including single-modal ones that use\nmasked training objectives and multi-modal ones that use image-text\nconstrastive training. We observe that existing multi-modal methods do not\noutperform their single-modal counterparts on vision transfer learning tasks.\nWe derive an information-theoretical view to explain these benchmark results,\nwhich provides insight into how to design a novel vision learner. Inspired by\nthis insight, we present a new visual representation pre-training method,\nMUlti-modal Generator~(MUG), that learns from scalable web sourced image-text\ndata. MUG achieves state-of-the-art transfer performance on a variety of tasks\nand demonstrates promising scaling properties. Pre-trained models and code will\nbe made public upon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bingchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1\">Quan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshie_O/0/1/0/all/0/1\">Osamu Yoshie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1\">Oisin Mac Aodha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Encoding Sentence Position in Context-Aware Neural Machine Translation with Concatenation. (arXiv:2302.06459v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.06459","description":"<p>Context-aware translation can be achieved by processing a concatenation of\nconsecutive sentences with the standard Transformer architecture. This paper\ninvestigates the intuitive idea of providing the model with explicit\ninformation about the position of the sentences contained in the concatenation\nwindow. We compare various methods to encode sentence positions into token\nrepresentations, including novel methods. Our results show that the Transformer\nbenefits from certain sentence position encoding methods on English to Russian\ntranslation if trained with a context-discounted loss (Lupo et al., 2022).\nHowever, the same benefits are not observed in English to German. Further\nempirical efforts are necessary to define the conditions under which the\nproposed approach is beneficial.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lupo_L/0/1/0/all/0/1\">Lorenzo Lupo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinarelli_M/0/1/0/all/0/1\">Marco Dinarelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1\">Laurent Besacier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis. (arXiv:2302.08624v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.08624","description":"<p>In this paper, we present InstructABSA, Aspect Based Sentiment Analysis\n(ABSA) using the instruction learning paradigm for all ABSA subtasks: Aspect\nTerm Extraction (ATE), Aspect Term Sentiment Classification (ATSC), and Joint\nTask modeling. Our method introduces positive, negative, and neutral examples\nto each training sample, and instruction tunes the model (Tk-Instruct) for each\nABSA subtask, yielding significant performance improvements. Experimental\nresults on the Sem Eval 2014, 15, and 16 datasets demonstrate that InstructABSA\noutperforms the previous state-of-the-art (SOTA) approaches on all three ABSA\nsubtasks (ATE, ATSC, and Joint Task) by a significant margin, outperforming 7x\nlarger models. In particular, InstructABSA surpasses the SOTA on the Rest14 ATE\nsubtask by 7.31% points, Rest15 ATSC subtask by and on the Lapt14 Joint Task by\n8.63% points. Our results also suggest a strong generalization ability to new\ndomains across all three subtasks\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scaria_K/0/1/0/all/0/1\">Kevin Scaria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1\">Siddharth Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawant_S/0/1/0/all/0/1\">Saurabh Arjun Sawant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KHAN: Knowledge-Aware Hierarchical Attention Networks for Accurate Political Stance Prediction. (arXiv:2302.12126v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12126","description":"<p>The political stance prediction for news articles has been widely studied to\nmitigate the echo chamber effect -- people fall into their thoughts and\nreinforce their pre-existing beliefs. The previous works for the political\nstance problem focus on (1) identifying political factors that could reflect\nthe political stance of a news article and (2) capturing those factors\neffectively. Despite their empirical successes, they are not sufficiently\njustified in terms of how effective their identified factors are in the\npolitical stance prediction. Motivated by this, in this work, we conduct a user\nstudy to investigate important factors in political stance prediction, and\nobserve that the context and tone of a news article (implicit) and external\nknowledge for real-world entities appearing in the article (explicit) are\nimportant in determining its political stance. Based on this observation, we\npropose a novel knowledge-aware approach to political stance prediction (KHAN),\nemploying (1) hierarchical attention networks (HAN) to learn the relationships\namong words and sentences in three different levels and (2) knowledge encoding\n(KE) to incorporate external knowledge for real-world entities into the process\nof political stance prediction. Also, to take into account the subtle and\nimportant difference between opposite political stances, we build two\nindependent political knowledge graphs (KG) (i.e., KG-lib and KG-con) by\nourselves and learn to fuse the different political knowledge. Through\nextensive evaluations on three real-world datasets, we demonstrate the\nsuperiority of DASH in terms of (1) accuracy, (2) efficiency, and (3)\neffectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ko_Y/0/1/0/all/0/1\">Yunyong Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_S/0/1/0/all/0/1\">Seongeun Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soeun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_Y/0/1/0/all/0/1\">Youngseung Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sohyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kyungsik Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1\">Hanghang Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sang-Wook Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preparing the Vuk'uzenzele and ZA-gov-multilingual South African multilingual corpora. (arXiv:2303.03750v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.03750","description":"<p>This paper introduces two multilingual government themed corpora in various\nSouth African languages. The corpora were collected by gathering the South\nAfrican Government newspaper (Vuk'uzenzele), as well as South African\ngovernment speeches (ZA-gov-multilingual), that are translated into all 11\nSouth African official languages. The corpora can be used for a myriad of\ndownstream NLP tasks. The corpora were created to allow researchers to study\nthe language used in South African government publications, with a focus on\nunderstanding how South African government officials communicate with their\nconstituents. In this paper we highlight the process of gathering, cleaning and\nmaking available the corpora. We create parallel sentence corpora for Neural\nMachine Translation (NMT) tasks using Language-Agnostic Sentence\nRepresentations (LASER) embeddings. With these aligned sentences we then\nprovide NMT benchmarks for 9 indigenous languages by fine-tuning a massively\nmultilingual pre-trained language model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lastrucci_R/0/1/0/all/0/1\">Richard Lastrucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dzingirai_I/0/1/0/all/0/1\">Isheanesu Dzingirai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajab_J/0/1/0/all/0/1\">Jenalea Rajab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madodonga_A/0/1/0/all/0/1\">Andani Madodonga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shingange_M/0/1/0/all/0/1\">Matimba Shingange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Njini_D/0/1/0/all/0/1\">Daniel Njini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1\">Vukosi Marivate</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Difficulty in learning chirality for Transformer fed with SMILES. (arXiv:2303.11593v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2303.11593","description":"<p>Recent years have seen development of descriptor generation based on\nrepresentation learning of extremely diverse molecules, especially those that\napply natural language processing (NLP) models to SMILES, a literal\nrepresentation of molecular structure. However, little research has been done\non how these models understand chemical structure. To address this, we\ninvestigated the relationship between the learning progress of SMILES and\nchemical structure using a representative NLP model, the Transformer. The\nresults suggest that while the Transformer learns partial structures of\nmolecules quickly, it requires extended training to understand overall\nstructures. Consistently, the accuracy of molecular property predictions using\ndescriptors generated from models at different learning steps was similar from\nthe beginning to the end of training. Furthermore, we found that the\nTransformer requires particularly long training to learn chirality and\nsometimes stagnates with low translation accuracy due to misunderstanding of\nenantiomers. These findings are expected to deepen understanding of NLP models\nin chemistry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoshikai_Y/0/1/0/all/0/1\">Yasuhiro Yoshikai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mizuno_T/0/1/0/all/0/1\">Tadahaya Mizuno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nemoto_S/0/1/0/all/0/1\">Shumpei Nemoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusuhara_H/0/1/0/all/0/1\">Hiroyuki Kusuhara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Universal Transformer: block reusing with adaptor in Transformer for automatic speech recognition. (arXiv:2303.13072v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2303.13072","description":"<p>Transformer-based models have recently made significant achievements in the\napplication of end-to-end (E2E) automatic speech recognition (ASR). It is\npossible to deploy the E2E ASR system on smart devices with the help of\nTransformer-based models. While these models still have the disadvantage of\nrequiring a large number of model parameters. To overcome the drawback of\nuniversal Transformer models for the application of ASR on edge devices, we\npropose a solution that can reuse the block in Transformer models for the\noccasion of the small footprint ASR system, which meets the objective of\naccommodating resource limitations without compromising recognition accuracy.\nSpecifically, we design a novel block-reusing strategy for speech Transformer\n(BRST) to enhance the effectiveness of parameters and propose an adapter module\n(ADM) that can produce a compact and adaptable model with only a few additional\ntrainable parameters accompanying each reusing block. We conducted an\nexperiment with the proposed method on the public AISHELL-1 corpus, and the\nresults show that the proposed approach achieves the character error rate (CER)\nof 9.3%/6.63% with only 7.6M/8.3M parameters without and with the ADM,\nrespectively. In addition, we also make a deeper analysis to show the effect of\nADM in the general block-reusing method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haoyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaoyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinfeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation. (arXiv:2303.15413v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2303.15413","description":"<p>The view inconsistency problem in score-distilling text-to-3D generation,\nalso known as the Janus problem, arises from the intrinsic bias of 2D diffusion\nmodels, which leads to the unrealistic generation of 3D objects. In this work,\nwe explore score-distilling text-to-3D generation and identify the main causes\nof the Janus problem. Based on these findings, we propose two approaches to\ndebias the score-distillation frameworks for robust text-to-3D generation. Our\nfirst approach, called score debiasing, involves gradually increasing the\ntruncation value for the score estimated by 2D diffusion models throughout the\noptimization process. Our second approach, called prompt debiasing, identifies\nconflicting words between user prompts and view prompts utilizing a language\nmodel and adjusts the discrepancy between view prompts and object-space camera\nposes. Our experimental results show that our methods improve realism by\nsignificantly reducing artifacts and achieve a good trade-off between\nfaithfulness to the 2D diffusion models and 3D consistency with little\noverhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Susung Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_D/0/1/0/all/0/1\">Donghoon Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating GPT-4 and ChatGPT on Japanese Medical Licensing Examinations. (arXiv:2303.18027v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.18027","description":"<p>As large language models (LLMs) gain popularity among speakers of diverse\nlanguages, we believe that it is crucial to benchmark them to better understand\nmodel behaviors, failures, and limitations in languages beyond English. In this\nwork, we evaluate LLM APIs (ChatGPT, GPT-3, and GPT-4) on the Japanese national\nmedical licensing examinations from the past five years, including the current\nyear. Our team comprises native Japanese-speaking NLP researchers and a\npracticing cardiologist based in Japan. Our experiments show that GPT-4\noutperforms ChatGPT and GPT-3 and passes all six years of the exams,\nhighlighting LLMs' potential in a language that is typologically distant from\nEnglish. However, our evaluation also exposes critical limitations of the\ncurrent LLM APIs. First, LLMs sometimes select prohibited choices that should\nbe strictly avoided in medical practice in Japan, such as suggesting\neuthanasia. Further, our analysis shows that the API costs are generally higher\nand the maximum context size is smaller for Japanese because of the way\nnon-Latin scripts are currently tokenized in the pipeline. We release our\nbenchmark as Igaku QA as well as all model outputs and exam metadata. We hope\nthat our results and benchmark will spur progress on more diverse applications\nof LLMs. Our benchmark is available at https://github.com/jungokasai/IgakuQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasai_Y/0/1/0/all/0/1\">Yuhei Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1\">Keisuke Sakaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamada_Y/0/1/0/all/0/1\">Yutaro Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AUDIT: Audio Editing by Following Instructions with Latent Diffusion Models. (arXiv:2304.00830v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2304.00830","description":"<p>Audio editing is applicable for various purposes, such as adding background\nsound effects, replacing a musical instrument, and repairing damaged audio.\nRecently, some diffusion-based methods achieved zero-shot audio editing by\nusing a diffusion and denoising process conditioned on the text description of\nthe output audio. However, these methods still have some problems: 1) they have\nnot been trained on editing tasks and cannot ensure good editing effects; 2)\nthey can erroneously modify audio segments that do not require editing; 3) they\nneed a complete description of the output audio, which is not always available\nor necessary in practical scenarios. In this work, we propose AUDIT, an\ninstruction-guided audio editing model based on latent diffusion models.\nSpecifically, AUDIT has three main design features: 1) we construct triplet\ntraining data (instruction, input audio, output audio) for different audio\nediting tasks and train a diffusion model using instruction and input (to be\nedited) audio as conditions and generating output (edited) audio; 2) it can\nautomatically learn to only modify segments that need to be edited by comparing\nthe difference between the input and output audio; 3) it only needs edit\ninstructions instead of full target audio descriptions as text input. AUDIT\nachieves state-of-the-art results in both objective and subjective metrics for\nseveral audio editing tasks (e.g., adding, dropping, replacement, inpainting,\nsuper-resolution). Demo samples are available at https://audit-demo.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuancheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_Z/0/1/0/all/0/1\">Zeqian Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhizheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection. (arXiv:2304.01238v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01238","description":"<p>This paper investigates the effectiveness of large language models (LLMs) in\nemail spam detection by comparing prominent models from three distinct\nfamilies: BERT-like, Sentence Transformers, and Seq2Seq. Additionally, we\nexamine well-established machine learning techniques for spam detection, such\nas Na\\\"ive Bayes and LightGBM, as baseline methods. We assess the performance\nof these models across four public datasets, utilizing different numbers of\ntraining samples (full training set and few-shot settings). Our findings reveal\nthat, in the majority of cases, LLMs surpass the performance of the popular\nbaseline techniques, particularly in few-shot scenarios. This adaptability\nrenders LLMs uniquely suited to spam detection tasks, where labeled samples are\nlimited in number and models require frequent updates. Additionally, we\nintroduce Spam-T5, a Flan-T5 model that has been specifically adapted and\nfine-tuned for the purpose of detecting email spam. Our results demonstrate\nthat Spam-T5 surpasses baseline models and other LLMs in the majority of\nscenarios, particularly when there are a limited number of training samples\navailable. Our code is publicly available at\nhttps://github.com/jpmorganchase/emailspamdetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Labonne_M/0/1/0/all/0/1\">Maxime Labonne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1\">Sean Moran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Mentions of Pain in Mental Health Records Text: A Natural Language Processing Approach. (arXiv:2304.01240v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01240","description":"<p>Pain is a common reason for accessing healthcare resources and is a growing\narea of research, especially in its overlap with mental health. Mental health\nelectronic health records are a good data source to study this overlap.\nHowever, much information on pain is held in the free text of these records,\nwhere mentions of pain present a unique natural language processing problem due\nto its ambiguous nature. This project uses data from an anonymised mental\nhealth electronic health records database. The data are used to train a machine\nlearning based classification algorithm to classify sentences as discussing\npatient pain or not. This will facilitate the extraction of relevant pain\ninformation from large databases, and the use of such outputs for further\nstudies on pain and mental health. 1,985 documents were manually\ntriple-annotated for creation of gold standard training data, which was used to\ntrain three commonly used classification algorithms. The best performing model\nachieved an F1-score of 0.98 (95% CI 0.98-0.99).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_J/0/1/0/all/0/1\">Jaya Chaturvedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velupillai_S/0/1/0/all/0/1\">Sumithra Velupillai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stewart_R/0/1/0/all/0/1\">Robert Stewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Angus Roberts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple and Effective Method of Cross-Lingual Plagiarism Detection. (arXiv:2304.01352v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01352","description":"<p>We present a simple cross-lingual plagiarism detection method applicable to a\nlarge number of languages. The presented approach leverages open multilingual\nthesauri for candidate retrieval task and pre-trained multilingual BERT-based\nlanguage models for detailed analysis. The method does not rely on machine\ntranslation and word sense disambiguation when in use, and therefore is\nsuitable for a large number of languages, including under-resourced languages.\nThe effectiveness of the proposed approach is demonstrated for several existing\nand new benchmarks, achieving state-of-the-art results for French, Russian, and\nArmenian languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avetisyan_K/0/1/0/all/0/1\">Karen Avetisyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malajyan_A/0/1/0/all/0/1\">Arthur Malajyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghukasyan_T/0/1/0/all/0/1\">Tsolak Ghukasyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avetisyan_A/0/1/0/all/0/1\">Arutyun Avetisyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The StatCan Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents. (arXiv:2304.01412v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01412","description":"<p>We introduce the StatCan Dialogue Dataset consisting of 19,379 conversation\nturns between agents working at Statistics Canada and online users looking for\npublished data tables. The conversations stem from genuine intents, are held in\nEnglish or French, and lead to agents retrieving one of over 5000 complex data\ntables. Based on this dataset, we propose two tasks: (1) automatic retrieval of\nrelevant tables based on a on-going conversation, and (2) automatic generation\nof appropriate agent responses at each turn. We investigate the difficulty of\neach task by establishing strong baselines. Our experiments on a temporal data\nsplit reveal that all models struggle to generalize to future conversations, as\nwe observe a significant drop in performance across both tasks when we move\nfrom the validation to the test set. In addition, we find that response\ngeneration models struggle to decide when to return a table. Considering that\nthe tasks pose significant challenges to existing models, we encourage the\ncommunity to develop models for our task, which can be directly used to help\nknowledge workers find relevant tables for live chat users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xing Han Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vries_H/0/1/0/all/0/1\">Harm de Vries</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To ChatGPT, or not to ChatGPT: That is the question!. (arXiv:2304.01487v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2304.01487","description":"<p>ChatGPT has become a global sensation. As ChatGPT and other Large Language\nModels (LLMs) emerge, concerns of misusing them in various ways increase, such\nas disseminating fake news, plagiarism, manipulating public opinion, cheating,\nand fraud. Hence, distinguishing AI-generated from human-generated becomes\nincreasingly essential. Researchers have proposed various detection\nmethodologies, ranging from basic binary classifiers to more complex\ndeep-learning models. Some detection techniques rely on statistical\ncharacteristics or syntactic patterns, while others incorporate semantic or\ncontextual information to improve accuracy. The primary objective of this study\nis to provide a comprehensive and contemporary assessment of the most recent\ntechniques in ChatGPT detection. Additionally, we evaluated other AI-generated\ntext detection tools that do not specifically claim to detect ChatGPT-generated\ncontent to assess their performance in detecting ChatGPT-generated content. For\nour evaluation, we have curated a benchmark dataset consisting of prompts from\nChatGPT and humans, including diverse questions from medical, open Q&amp;A, and\nfinance domains and user-generated responses from popular social networking\nplatforms. The dataset serves as a reference to assess the performance of\nvarious techniques in detecting ChatGPT-generated content. Our evaluation\nresults demonstrate that none of the existing methods can effectively detect\nChatGPT-generated content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pegoraro_A/0/1/0/all/0/1\">Alessandro Pegoraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumari_K/0/1/0/all/0/1\">Kavita Kumari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fereidooni_H/0/1/0/all/0/1\">Hossein Fereidooni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeghi_A/0/1/0/all/0/1\">Ahmad-Reza Sadeghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Contrastive Transfer Framework with Propagation Structure for Boosting Low-Resource Rumor Detection. (arXiv:2304.01492v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01492","description":"<p>The truth is significantly hampered by massive rumors that spread along with\nbreaking news or popular topics. Since there is sufficient corpus gathered from\nthe same domain for model training, existing rumor detection algorithms show\npromising performance on yesterday's news. However, due to a lack of training\ndata and prior expert knowledge, they are poor at spotting rumors concerning\nunforeseen events, especially those propagated in different languages (i.e.,\nlow-resource regimes). In this paper, we propose a unified contrastive transfer\nframework to detect rumors by adapting the features learned from well-resourced\nrumor data to that of the low-resourced. More specifically, we first represent\nrumor circulated on social media as an undirected topology, and then train a\nMulti-scale Graph Convolutional Network via a unified contrastive paradigm. Our\nmodel explicitly breaks the barriers of the domain and/or language issues, via\nlanguage alignment and a novel domain-adaptive contrastive learning mechanism.\nTo enhance the representation learning from a small set of target events, we\nreveal that rumor-indicative signal is closely correlated with the uniformity\nof the distribution of these events. We design a target-wise contrastive\ntraining mechanism with three data augmentation strategies, capable of unifying\nthe representations by distinguishing target events. Extensive experiments\nconducted on four low-resource datasets collected from real-world microblog\nplatforms demonstrate that our framework achieves much better performance than\nstate-of-the-art methods and exhibits a superior capacity for detecting rumors\nat early stages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongzhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhiwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Mingfei Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01890","description":"<p>We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for\nthe countries of Brazil, Germany, India and Kenya, to aid training and\ninterpretability of models. We demonstrate how our lexicon can be used to\ninterpret model predictions, showing that models developed to classify extreme\nspeech rely heavily on target words when making predictions. Further, we\npropose a method to aid shot selection for training in low-resource settings\nvia HATELEXICON. In few-shot learning, the selection of shots is of paramount\nimportance to model performance. In our work, we simulate a few-shot setting\nfor German and Hindi, using HASOC data for training and the Multilingual\nHateCheck (MHC) as a benchmark. We show that selecting shots based on our\nlexicon leads to models performing better on MHC than models trained on shots\nsampled randomly. Thus, when given only a few training examples, using our\nlexicon to select shots containing more sociocultural information leads to\nbetter few-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maronikolakis_A/0/1/0/all/0/1\">Antonis Maronikolakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koksal_A/0/1/0/all/0/1\">Abdullatif K&#xf6;ksal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Attention Neural Transducers for Efficient Wake Word Spotting in Speech Recognition. (arXiv:2304.01905v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2304.01905","description":"<p>We present dual-attention neural biasing, an architecture designed to boost\nWake Words (WW) recognition and improve inference time latency on speech\nrecognition tasks. This architecture enables a dynamic switch for its runtime\ncompute paths by exploiting WW spotting to select which branch of its attention\nnetworks to execute for an input audio frame. With this approach, we\neffectively improve WW spotting accuracy while saving runtime compute cost as\ndefined by floating point operations (FLOPs). Using an in-house de-identified\ndataset, we demonstrate that the proposed dual-attention network can reduce the\ncompute cost by $90\\%$ for WW audio frames, with only $1\\%$ increase in the\nnumber of parameters. This architecture improves WW F1 score by $16\\%$ relative\nand improves generic rare word error rate by $3\\%$ relative compared to the\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahai_S/0/1/0/all/0/1\">Saumya Y. Sahai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muniyappa_T/0/1/0/all/0/1\">Thejaswi Muniyappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sathyendra_K/0/1/0/all/0/1\">Kanthashree M. Sathyendra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexandridis_A/0/1/0/all/0/1\">Anastasios Alexandridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strimel_G/0/1/0/all/0/1\">Grant P. Strimel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGowan_R/0/1/0/all/0/1\">Ross McGowan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastrow_A/0/1/0/all/0/1\">Ariya Rastrow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_F/0/1/0/all/0/1\">Feng-Ju Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouchtaris_A/0/1/0/all/0/1\">Athanasios Mouchtaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunzmann_S/0/1/0/all/0/1\">Siegfried Kunzmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-04-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}