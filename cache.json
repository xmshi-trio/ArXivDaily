{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-04-03T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes. (arXiv:2303.17612v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17612","description":"<p>In this paper, we introduce the range of oBERTa language models, an\neasy-to-use set of language models, which allows Natural Language Processing\n(NLP) practitioners to obtain between 3.8 and 24.3 times faster models without\nexpertise in model compression. Specifically, oBERTa extends existing work on\npruning, knowledge distillation, and quantization and leverages frozen\nembeddings to improve knowledge distillation, and improved model initialization\nto deliver higher accuracy on a a broad range of transfer tasks. In generating\noBERTa, we explore how the highly optimized RoBERTa differs from the BERT with\nrespect to pruning during pre-training and fine-tuning and find it less\namenable to compression during fine-tuning. We explore the use of oBERTa on a\nbroad seven representative NLP tasks and find that the improved compression\ntechniques allow a pruned oBERTa model to match the performance of BERTBASE and\nexceed the performance of Prune OFA Large on the SQUAD V1.1 Question Answering\ndataset, despite being 8x and 2x, respectively, faster in inference. We release\nour code, training regimes, and associated model for broad usage to encourage\nusage and experimentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Campos_D/0/1/0/all/0/1\">Daniel Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marques_A/0/1/0/all/0/1\">Alexandre Marques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtz_M/0/1/0/all/0/1\">Mark Kurtz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting and Grounding Important Characters in Visual Stories. (arXiv:2303.17647v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17647","description":"<p>Characters are essential to the plot of any story. Establishing the\ncharacters before writing a story can improve the clarity of the plot and the\noverall flow of the narrative. However, previous work on visual storytelling\ntends to focus on detecting objects in images and discovering relationships\nbetween them. In this approach, characters are not distinguished from other\nobjects when they are fed into the generation pipeline. The result is a\ncoherent sequence of events rather than a character-centric story. In order to\naddress this limitation, we introduce the VIST-Character dataset, which\nprovides rich character-centric annotations, including visual and textual\nco-reference chains and importance ratings for characters. Based on this\ndataset, we propose two new tasks: important character detection and character\ngrounding in visual stories. For both tasks, we develop simple, unsupervised\nmodels based on distributional similarity and pre-trained vision-and-language\nmodels. Our new dataset, together with these models, can serve as the\nfoundation for subsequent work on analysing and generating stories from a\ncharacter-centric perspective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Danyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aligning a medium-size GPT model in English to a small closed domain in Spanish using reinforcement learning. (arXiv:2303.17649v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17649","description":"<p>In this paper, we propose a methodology to align a medium-sized GPT model,\noriginally trained in English for an open domain, to a small closed domain in\nSpanish. The application for which the model is finely tuned is the question\nanswering task. To achieve this we also needed to train and implement another\nneural network (which we called the reward model) that could score and\ndetermine whether an answer is appropriate for a given question. This component\nserved to improve the decoding and generation of the answers of the system.\nNumerical metrics such as BLEU and perplexity were used to evaluate the model,\nand human judgment was also used to compare the decoding technique with others.\nFinally, the results favored the proposed method, and it was determined that it\nis feasible to use a reward model to align the generation of responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Navarrete_Parra_O/0/1/0/all/0/1\">Oscar R. Navarrete-Parra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uc_Cetina_V/0/1/0/all/0/1\">Victor Uc-Cetina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_Magana_J/0/1/0/all/0/1\">Jorge Reyes-Magana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Abstractive Summaries Generated by ChatGPT to Real Summaries Through Blinded Reviewers and Text Classification Algorithms. (arXiv:2303.17650v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17650","description":"<p>Large Language Models (LLMs) have gathered significant attention due to their\nimpressive performance on a variety of tasks. ChatGPT, developed by OpenAI, is\na recent addition to the family of language models and is being called a\ndisruptive technology by a few, owing to its human-like text-generation\ncapabilities. Although, many anecdotal examples across the internet have\nevaluated ChatGPT's strength and weakness, only a few systematic research\nstudies exist. To contribute to the body of literature of systematic research\non ChatGPT, we evaluate the performance of ChatGPT on Abstractive Summarization\nby the means of automated metrics and blinded human reviewers. We also build\nautomatic text classifiers to detect ChatGPT generated summaries. We found that\nwhile text classification algorithms can distinguish between real and generated\nsummaries, humans are unable to distinguish between real summaries and those\nproduced by ChatGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soni_M/0/1/0/all/0/1\">Mayank Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wade_V/0/1/0/all/0/1\">Vincent Wade</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Refine: Iterative Refinement with Self-Feedback. (arXiv:2303.17651v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17651","description":"<p>Like people, LLMs do not always generate the best text for a given generation\nproblem on their first try (e.g., summaries, answers, explanations). Just as\npeople then refine their text, we introduce SELF-REFINE, a framework for\nsimilarly improving initial outputs from LLMs through iterative feedback and\nrefinement. The main idea is to generate an output using an LLM, then allow the\nsame model to provide multi-aspect feedback for its own output; finally, the\nsame model refines its previously generated output given its own feedback.\nUnlike earlier work, our iterative refinement framework does not require\nsupervised training data or reinforcement learning, and works with a single\nLLM. We experiment with 7 diverse tasks, ranging from review rewriting to math\nreasoning, demonstrating that our approach outperforms direct generation. In\nall tasks, outputs generated with SELF-REFINE are preferred by humans and by\nautomated metrics over those generated directly with GPT-3.5 and GPT-4,\nimproving on average by absolute 20% across tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1\">Niket Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Prakhar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hallinan_S/0/1/0/all/0/1\">Skyler Hallinan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Luyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiegreffe_S/0/1/0/all/0/1\">Sarah Wiegreffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1\">Uri Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1\">Nouha Dziri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhumoye_S/0/1/0/all/0/1\">Shrimai Prabhumoye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_B/0/1/0/all/0/1\">Bodhisattwa Prasad Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shashank Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazdanbakhsh_A/0/1/0/all/0/1\">Amir Yazdanbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Tuning BERT with Character-Level Noise for Zero-Shot Transfer to Dialects and Closely-Related Languages. (arXiv:2303.17683v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17683","description":"<p>In this work, we induce character-level noise in various forms when\nfine-tuning BERT to enable zero-shot cross-lingual transfer to unseen dialects\nand languages. We fine-tune BERT on three sentence-level classification tasks\nand evaluate our approach on an assortment of unseen dialects and languages. We\nfind that character-level noise can be an extremely effective agent of\ncross-lingual transfer under certain conditions, while it is not as helpful in\nothers. Specifically, we explore these differences in terms of the nature of\nthe task and the relationships between source and target languages, finding\nthat introduction of character-level noise during fine-tuning is particularly\nhelpful when a task draws on surface level cues and the source-target\ncross-lingual pair has a relatively high lexical overlap with shorter (i.e.,\nless meaningful) unseen tokens on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Aarohi Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task Oriented Conversational Modelling With Subjective Knowledge. (arXiv:2303.17695v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17695","description":"<p>Existing conversational models are handled by a database(DB) and API based\nsystems. However, very often users' questions require information that cannot\nbe handled by such systems. Nonetheless, answers to these questions are\navailable in the form of customer reviews and FAQs. DSTC-11 proposes a three\nstage pipeline consisting of knowledge seeking turn detection, knowledge\nselection and response generation to create a conversational model grounded on\nthis subjective knowledge. In this paper, we focus on improving the knowledge\nselection module to enhance the overall system performance. In particular, we\npropose entity retrieval methods which result in an accurate and faster\nknowledge search. Our proposed Named Entity Recognition (NER) based entity\nretrieval method results in 7X faster search compared to the baseline model.\nAdditionally, we also explore a potential keyword extraction method which can\nimprove the accuracy of knowledge selection. Preliminary results show a 4 \\%\nimprovement in exact match score on knowledge selection task. The code is\navailable https://github.com/raja-kumar/knowledge-grounded-TODS\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Raja Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Types of Questions Require Conversation to Answer? A Case Study of AskReddit Questions. (arXiv:2303.17710v1 [cs.HC])","link":"http://arxiv.org/abs/2303.17710","description":"<p>The proliferation of automated conversational systems such as chatbots,\nspoken-dialogue systems, and smart speakers, has significantly impacted modern\ndigital life. However, these systems are primarily designed to provide answers\nto well-defined questions rather than to support users in exploring complex,\nill-defined questions. In this paper, we aim to push the boundaries of\nconversational systems by examining the types of nebulous, open-ended questions\nthat can best be answered through conversation. We first sampled 500 questions\nfrom one million open-ended requests posted on AskReddit, and then recruited\nonline crowd workers to answer eight inquiries about these questions. We also\nperformed open coding to categorize the questions into 27 different domains. We\nfound that the issues people believe require conversation to resolve\nsatisfactorily are highly social and personal. Our work provides insights into\nhow future research could be geared to align with users' needs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shih-Hong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chieh-Yang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Ya-Fang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Hao &#x27;Kenneth&#x27; Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of GPT and BERT-based models on identifying protein-protein interactions in biomedical text. (arXiv:2303.17728v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17728","description":"<p>Detecting protein-protein interactions (PPIs) is crucial for understanding\ngenetic mechanisms, disease pathogenesis, and drug design. However, with the\nfast-paced growth of biomedical literature, there is a growing need for\nautomated and accurate extraction of PPIs to facilitate scientific knowledge\ndiscovery. Pre-trained language models, such as generative pre-trained\ntransformer (GPT) and bidirectional encoder representations from transformers\n(BERT), have shown promising results in natural language processing (NLP)\ntasks. We evaluated the PPI identification performance of various GPT and BERT\nmodels using a manually curated benchmark corpus of 164 PPIs in 77 sentences\nfrom learning language in logic (LLL). BERT-based models achieved the best\noverall performance, with PubMedBERT achieving the highest precision (85.17%)\nand F1-score (86.47%) and BioM-ALBERT achieving the highest recall (93.83%).\nDespite not being explicitly trained for biomedical texts, GPT-4 achieved\ncomparable performance to the best BERT models with 83.34% precision, 76.57%\nrecall, and 79.18% F1-score. These findings suggest that GPT models can\neffectively detect PPIs from text data and have the potential for use in\nbiomedical literature mining tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rehana_H/0/1/0/all/0/1\">Hasin Rehana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cam_N/0/1/0/all/0/1\">Nur Bengisu &#xc7;am</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basmaci_M/0/1/0/all/0/1\">Mert Basmaci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yongqun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozgur_A/0/1/0/all/0/1\">Arzucan &#xd6;zg&#xfc;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hur_J/0/1/0/all/0/1\">Junguk Hur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Design by Contract Framework for Quantum Software. (arXiv:2303.17750v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17750","description":"<p>To realize reliable quantum software, techniques to automatically ensure the\nquantum software's correctness have recently been investigated. However, they\nprimarily focus on fixed quantum circuits rather than the procedure of building\nquantum circuits. Despite being a common approach, the correctness of building\ncircuits using different parameters following the same procedure is not\nguaranteed. To this end, we propose a design-by-contract framework for quantum\nsoftware. Our framework provides a python-embedded language to write assertions\non the input and output states of all quantum circuits built by certain\nprocedures. Additionally, it provides a method to write assertions about the\nstatistical processing of measurement results to ensure the procedure's\ncorrectness for obtaining the final result. These assertions are automatically\nchecked using a quantum computer simulator. For evaluation, we implemented our\nframework and wrote assertions for some widely used quantum algorithms.\nConsequently, we found that our framework has sufficient expressive power to\nverify the whole procedure of quantum software.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_M/0/1/0/all/0/1\">Masaomi Yamaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshioka_N/0/1/0/all/0/1\">Nobukazu Yoshioka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society. (arXiv:2303.17760v1 [cs.AI])","link":"http://arxiv.org/abs/2303.17760","description":"<p>The rapid advancement of conversational and chat-based language models has\nled to remarkable progress in complex task-solving. However, their success\nheavily relies on human input to guide the conversation, which can be\nchallenging and time-consuming. This paper explores the potential of building\nscalable techniques to facilitate autonomous cooperation among communicative\nagents and provide insight into their \"cognitive\" processes. To address the\nchallenges of achieving autonomous cooperation, we propose a novel\ncommunicative agent framework named role-playing. Our approach involves using\ninception prompting to guide chat agents toward task completion while\nmaintaining consistency with human intentions. We showcase how role-playing can\nbe used to generate conversational data for studying the behaviors and\ncapabilities of chat agents, providing a valuable resource for investigating\nconversational language models. Our contributions include introducing a novel\ncommunicative agent framework, offering a scalable approach for studying the\ncooperative behaviors and capabilities of multi-agent systems, and\nopen-sourcing our library to support research on communicative agents and\nbeyond. The GitHub repository of this project is made publicly available on:\nhttps://github.com/lightaime/camel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guohao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammoud_H/0/1/0/all/0/1\">Hasan Abed Al Kader Hammoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itani_H/0/1/0/all/0/1\">Hani Itani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khizbullin_D/0/1/0/all/0/1\">Dmitrii Khizbullin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention is Not Always What You Need: Towards Efficient Classification of Domain-Specific Text. (arXiv:2303.17786v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17786","description":"<p>For large-scale IT corpora with hundreds of classes organized in a hierarchy,\nthe task of accurate classification of classes at the higher level in the\nhierarchies is crucial to avoid errors propagating to the lower levels. In the\nbusiness world, an efficient and explainable ML model is preferred over an\nexpensive black-box model, especially if the performance increase is marginal.\nA current trend in the Natural Language Processing (NLP) community is towards\nemploying huge pre-trained language models (PLMs) or what is known as\nself-attention models (e.g., BERT) for almost any kind of NLP task (e.g.,\nquestion-answering, sentiment analysis, text classification). Despite the\nwidespread use of PLMs and the impressive performance in a broad range of NLP\ntasks, there is a lack of a clear and well-justified need to as why these\nmodels are being employed for domain-specific text classification (TC) tasks,\ngiven the monosemic nature of specialized words (i.e., jargon) found in\ndomain-specific text which renders the purpose of contextualized embeddings\n(e.g., PLMs) futile. In this paper, we compare the accuracies of some\nstate-of-the-art (SOTA) models reported in the literature against a Linear SVM\nclassifier and TFIDF vectorization model on three TC datasets. Results show a\ncomparable performance for the LinearSVM. The findings of this study show that\nfor domain-specific TC tasks, a linear model can provide a comparable, cheap,\nreproducible, and interpretable alternative to attention-based models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahba_Y/0/1/0/all/0/1\">Yasmen Wahba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhavji_N/0/1/0/all/0/1\">Nazim Madhavji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinbacher_J/0/1/0/all/0/1\">John Steinbacher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialog act guided contextual adapter for personalized speech recognition. (arXiv:2303.17799v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17799","description":"<p>Personalization in multi-turn dialogs has been a long standing challenge for\nend-to-end automatic speech recognition (E2E ASR) models. Recent work on\ncontextual adapters has tackled rare word recognition using user catalogs. This\nadaptation, however, does not incorporate an important cue, the dialog act,\nwhich is available in a multi-turn dialog scenario. In this work, we propose a\ndialog act guided contextual adapter network. Specifically, it leverages dialog\nacts to select the most relevant user catalogs and creates queries based on\nboth -- the audio as well as the semantic relationship between the carrier\nphrase and user catalogs to better guide the contextual biasing. On industrial\nvoice assistant datasets, our model outperforms both the baselines - dialog act\nencoder-only model, and the contextual adaptation, leading to the most\nimprovement over the no-context model: 58% average relative word error rate\nreduction (WERR) in the multi-turn dialog scenario, in comparison to the\nprior-art contextual adapter, which has achieved 39% WERR over the no-context\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_F/0/1/0/all/0/1\">Feng-Ju Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muniyappa_T/0/1/0/all/0/1\">Thejaswi Muniyappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sathyendra_K/0/1/0/all/0/1\">Kanthashree Mysore Sathyendra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Kai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strimel_G/0/1/0/all/0/1\">Grant P. Strimel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGowan_R/0/1/0/all/0/1\">Ross McGowan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Potential of Large Language models in Traditional Korean Medicine: A Foundation Model Approach to Culturally-Adapted Healthcare. (arXiv:2303.17807v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17807","description":"<p>Introduction: Traditional Korean medicine (TKM) emphasizes individualized\ndiagnosis and treatment, making AI modeling difficult due to limited data and\nimplicit processes. GPT-3.5 and GPT-4, large language models, have shown\nimpressive medical knowledge despite lacking medicine-specific training. This\nstudy aimed to assess the capabilities of GPT-3.5 and GPT-4 for TKM using the\nKorean National Licensing Examination for Korean Medicine Doctors. Methods:\nGPT-3.5 (February 2023) and GPT-4 (March 2023) models answered 340 questions\nfrom the 2022 examination across 12 subjects. Each question was independently\nevaluated five times in an initialized session. Results: GPT-3.5 and GPT-4\nachieved 42.06% and 57.29% accuracy, respectively, with GPT-4 nearing passing\nperformance. There were significant differences in accuracy by subjects, with\n83.75% accuracy for neuropsychiatry compared to 28.75% for internal medicine\n(2). Both models showed high accuracy in recall-based and diagnosis-based\nquestions but struggled with intervention-based ones. The accuracy for\nquestions that require TKM-specialized knowledge was relatively lower than the\naccuracy for questions that do not GPT-4 showed high accuracy for table-based\nquestions, and both models demonstrated consistent responses. A positive\ncorrelation between consistency and accuracy was observed. Conclusion: Models\nin this study showed near-passing performance in decision-making for TKM\nwithout domain-specific training. However, limits were also observed that were\nbelieved to be caused by culturally-biased learning. Our study suggests that\nfoundation models have potential in culturally-adapted medicine, specifically\nTKM, for clinical assistance, medical education, and medical research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_D/0/1/0/all/0/1\">Dongyeop Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chang-Eop Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Referring Image Segmentation with Global-Local Context Features. (arXiv:2303.17811v1 [cs.CV])","link":"http://arxiv.org/abs/2303.17811","description":"<p>Referring image segmentation (RIS) aims to find a segmentation mask given a\nreferring expression grounded to a region of the input image. Collecting\nlabelled datasets for this task, however, is notoriously costly and\nlabor-intensive. To overcome this issue, we propose a simple yet effective\nzero-shot referring image segmentation method by leveraging the pre-trained\ncross-modal knowledge from CLIP. In order to obtain segmentation masks grounded\nto the input text, we propose a mask-guided visual encoder that captures global\nand local contextual information of an input image. By utilizing instance masks\nobtained from off-the-shelf mask proposal techniques, our method is able to\nsegment fine-detailed Istance-level groundings. We also introduce a\nglobal-local text encoder where the global feature captures complex\nsentence-level semantics of the entire input expression while the local feature\nfocuses on the target noun phrase extracted by a dependency parser. In our\nexperiments, the proposed method outperforms several zero-shot baselines of the\ntask and even the weakly supervised referring expression segmentation method\nwith substantial margins. Our code is available at\nhttps://github.com/Seonghoon-Yu/Zero-shot-RIS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Seonghoon Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_P/0/1/0/all/0/1\">Paul Hongsuch Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_J/0/1/0/all/0/1\">Jeany Son</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Procedure-aware Video Representation from Instructional Videos and Their Narrations. (arXiv:2303.17839v1 [cs.CV])","link":"http://arxiv.org/abs/2303.17839","description":"<p>The abundance of instructional videos and their narrations over the Internet\noffers an exciting avenue for understanding procedural activities. In this\nwork, we propose to learn video representation that encodes both action steps\nand their temporal ordering, based on a large-scale dataset of web\ninstructional videos and their narrations, without using human annotations. Our\nmethod jointly learns a video representation to encode individual step\nconcepts, and a deep probabilistic model to capture both temporal dependencies\nand immense individual variations in the step ordering. We empirically\ndemonstrate that learning temporal ordering not only enables new capabilities\nfor procedure reasoning, but also reinforces the recognition of individual\nsteps. Our model significantly advances the state-of-the-art results on step\nclassification (+2.8% / +3.3% on COIN / EPIC-Kitchens) and step forecasting\n(+7.4% on COIN). Moreover, our model attains promising results in zero-shot\ninference for step classification and forecasting, as well as in predicting\ndiverse and plausible steps for incomplete procedures. Our code is available at\nhttps://github.com/facebookresearch/ProcedureVRL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiwu Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Licheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shangwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xueting Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can AI Put Gamma-Ray Astrophysicists Out of a Job?. (arXiv:2303.17853v1 [physics.pop-ph])","link":"http://arxiv.org/abs/2303.17853","description":"<p>In what will likely be a litany of generative-model-themed arXiv submissions\ncelebrating April the 1st, we evaluate the capacity of state-of-the-art\ntransformer models to create a paper detailing the detection of a Pulsar Wind\nNebula with a non-existent Imaging Atmospheric Cherenkov Telescope (IACT)\nArray. We do this to evaluate the ability of such models to interpret\nastronomical observations and sources based on language information alone, and\nto assess potential means by which fraudulently generated scientific papers\ncould be identified during peer review (given that reliable generative model\nwatermarking has yet to be deployed for these tools). We conclude that our jobs\nas astronomers are safe for the time being. From this point on, prompts given\nto ChatGPT and Stable Diffusion are shown in orange, text generated by ChatGPT\nis shown in black, whereas analysis by the (human) authors is in blue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Spencer_S/0/1/0/all/0/1\">Samuel Timothy Spencer</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Joshi_V/0/1/0/all/0/1\">Vikas Joshi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mitchell_A/0/1/0/all/0/1\">Alison Mairi Wallace Mitchell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebQAmGaze: A Multilingual Webcam Eye-Tracking-While-Reading Dataset. (arXiv:2303.17876v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17876","description":"<p>We create WebQAmGaze, a multilingual low-cost eye-tracking-while-reading\ndataset, designed to support the development of fair and transparent NLP\nmodels. WebQAmGaze includes webcam eye-tracking data from 332 participants\nnaturally reading English, Spanish, and German texts. Each participant performs\ntwo reading tasks composed of five texts, a normal reading and an\ninformation-seeking task. After preprocessing the data, we find that fixations\non relevant spans seem to indicate correctness when answering the comprehension\nquestions. Additionally, we perform a comparative analysis of the data\ncollected to high-quality eye-tracking data. The results show a moderate\ncorrelation between the features obtained with the webcam-ET compared to those\nof a commercial ET device. We believe this data can advance webcam-based\nreading studies and open a way to cheaper and more accessible data collection.\nWebQAmGaze is useful to learn about the cognitive processes behind question\nanswering (QA) and to apply these insights to computational models of language\nunderstanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_T/0/1/0/all/0/1\">Tiago Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandl_S/0/1/0/all/0/1\">Stephanie Brandl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollenstein_N/0/1/0/all/0/1\">Nora Hollenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation. (arXiv:2303.17910v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17910","description":"<p>Benefiting from the sequence-level knowledge distillation, the\nNon-Autoregressive Transformer (NAT) achieves great success in neural machine\ntranslation tasks. However, existing knowledge distillation has side effects,\nsuch as propagating errors from the teacher to NAT students, which may limit\nfurther improvements of NAT models and are rarely discussed in existing\nresearch. In this paper, we introduce selective knowledge distillation by\nintroducing an NAT evaluator to select NAT-friendly targets that are of high\nquality and easy to learn. In addition, we introduce a simple yet effective\nprogressive distillation method to boost NAT performance. Experiment results on\nmultiple WMT language directions and several representative NAT models show\nthat our approach can realize a flexible trade-off between the quality and\ncomplexity of training data for NAT models, achieving strong performances.\nFurther analysis shows that distilling only 5% of the raw translations can help\nan NAT outperform its counterpart trained on raw data by about 2.4 BLEU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Min Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yu Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chengqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Cultural Transfer Learning for Chinese Offensive Language Detection. (arXiv:2303.17927v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17927","description":"<p>Detecting offensive language is a challenging task. Generalizing across\ndifferent cultures and languages becomes even more challenging: besides\nlexical, syntactic and semantic differences, pragmatic aspects such as cultural\nnorms and sensitivities, which are particularly relevant in this context, vary\ngreatly. In this paper, we target Chinese offensive language detection and aim\nto investigate the impact of transfer learning using offensive language\ndetection data from different cultural backgrounds, specifically Korean and\nEnglish. We find that culture-specific biases in what is considered offensive\nnegatively impact the transferability of language models (LMs) and that LMs\ntrained on diverse cultural data are sensitive to different features in Chinese\noffensive language detection. In a few-shot learning scenario, however, our\nstudy shows promising prospects for non-English offensive language detection\nwith limited resources. Our findings highlight the importance of cross-cultural\ntransfer learning in improving offensive language detection and promoting\ninclusive digital spaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Li Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabello_L/0/1/0/all/0/1\">Laura Cabello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JobHam-place with smart recommend job options and candidate filtering options. (arXiv:2303.17930v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17930","description":"<p>Due to the increasing number of graduates, many applicants experience the\nsituation about finding a job, and employers experience difficulty filtering\njob applicants, which might negatively impact their effectiveness. However,\nmost job-hunting websites lack job recommendation and CV filtering or ranking\nfunctionality, which are not integrated into the system. Thus, a smart job\nhunter combined with the above functionality will be conducted in this project,\nwhich contains job recommendations, CV ranking and even a job dashboard for\nskills and job applicant functionality. Job recommendation and CV ranking\nstarts from the automatic keyword extraction and end with the Job/CV ranking\nalgorithm. Automatic keyword extraction is implemented by Job2Skill and the\nCV2Skill model based on Bert. Job2Skill consists of two components, text\nencoder and Gru-based layers, while CV2Skill is mainly based on Bert and\nfine-tunes the pre-trained model by the Resume- Entity dataset. Besides, to\nmatch skills from CV and job description and rank lists of jobs and candidates,\njob/CV ranking algorithms have been provided to compute the occurrence ratio of\nskill words based on TFIDF score and match ratio of the total skill numbers.\nBesides, some advanced features have been integrated into the website to\nimprove user experiences, such as the calendar and sweetalert2 plugin. And some\nbasic features to go through job application processes, such as job application\ntracking and interview arrangement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shiyao Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trimming Phonetic Alignments Improves the Inference of Sound Correspondence Patterns from Multilingual Wordlists. (arXiv:2303.17932v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17932","description":"<p>Sound correspondence patterns form the basis of cognate detection and\nphonological reconstruction in historical language comparison. Methods for the\nautomatic inference of correspondence patterns from phonetically aligned\ncognate sets have been proposed, but their application to multilingual\nwordlists requires extremely well annotated datasets. Since annotation is\ntedious and time consuming, it would be desirable to find ways to improve\naligned cognate data automatically. Taking inspiration from trimming techniques\nin evolutionary biology, which improve alignments by excluding problematic\nsites, we propose a workflow that trims phonetic alignments in comparative\nlinguistics prior to the inference of correspondence patterns. Testing these\ntechniques on a large standardized collection of ten datasets with expert\nannotations from different language families, we find that the best trimming\ntechnique substantially improves the overall consistency of the alignments. The\nresults show a clear increase in the proportion of frequent correspondence\npatterns and words exhibiting regular cognate relations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blum_F/0/1/0/all/0/1\">Frederic Blum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+List_J/0/1/0/all/0/1\">Johann-Mattis List</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$\\mathcal{E}$ K\\'U [MASK]: Integrating Yor\\`ub\\'a cultural greetings into machine translation. (arXiv:2303.17972v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17972","description":"<p>This paper investigates the performance of massively multilingual neural\nmachine translation (NMT) systems in translating Yor\\`ub\\'a greetings\n($\\mathcal{E}$ k\\'u [MASK]), which are a big part of Yor\\`ub\\'a language and\nculture, into English. To evaluate these models, we present IkiniYor\\`ub\\'a, a\nYor\\`ub\\'a-English translation dataset containing some Yor\\`ub\\'a greetings,\nand sample use cases. We analysed the performance of different multilingual NMT\nsystems including Google and NLLB and show that these models struggle to\naccurately translate Yor\\`ub\\'a greetings into English. In addition, we trained\na Yor\\`ub\\'a-English model by finetuning an existing NMT model on the training\nsplit of IkiniYor\\`ub\\'a and this achieved better performance when compared to\nthe pre-trained multilingual NMT models, although they were trained on a large\nvolume of data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akinade_I/0/1/0/all/0/1\">Idris Akinade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odoje_C/0/1/0/all/0/1\">Clement Odoje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Multilingualism in Low-resource Neural Machine Translation via Adversarial Learning. (arXiv:2303.18011v1 [cs.CL])","link":"http://arxiv.org/abs/2303.18011","description":"<p>Generative Adversarial Networks (GAN) offer a promising approach for Neural\nMachine Translation (NMT). However, feeding multiple morphologically languages\ninto a single model during training reduces the NMT's performance. In GAN,\nsimilar to bilingual models, multilingual NMT only considers one reference\ntranslation for each sentence during model training. This single reference\ntranslation limits the GAN model from learning sufficient information about the\nsource sentence representation. Thus, in this article, we propose Denoising\nAdversarial Auto-encoder-based Sentence Interpolation (DAASI) approach to\nperform sentence interpolation by learning the intermediate latent\nrepresentation of the source and target sentences of multilingual language\npairs. Apart from latent representation, we also use the Wasserstein-GAN\napproach for the multilingual NMT model by incorporating the model generated\nsentences of multiple languages for reward computation. This computed reward\noptimizes the performance of the GAN-based multilingual model in an effective\nmanner. We demonstrate the experiments on low-resource language pairs and find\nthat our approach outperforms the existing state-of-the-art approaches for\nmultilingual NMT with a performance gain of up to 4 BLEU points. Moreover, we\nuse our trained model on zero-shot language pairs under an unsupervised\nscenario and show the robustness of the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Amit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratap_A/0/1/0/all/0/1\">Ajay Pratap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Anil Kumar Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating GPT-4 and ChatGPT on Japanese Medical Licensing Examinations. (arXiv:2303.18027v1 [cs.CL])","link":"http://arxiv.org/abs/2303.18027","description":"<p>As large language models (LLMs) gain popularity among speakers of diverse\nlanguages, we believe that it is crucial to benchmark them to better understand\nmodel behaviors, failures, and limitations in languages beyond English. In this\nwork, we evaluate LLM APIs (ChatGPT, GPT-3, and GPT-4) on the Japanese national\nmedical licensing examinations from the past five years. Our team comprises\nnative Japanese-speaking NLP researchers and a practicing cardiologist based in\nJapan. Our experiments show that GPT-4 outperforms ChatGPT and GPT-3 and passes\nall five years of the exams, highlighting LLMs' potential in a language that is\ntypologically distant from English. However, our evaluation also exposes\ncritical limitations of the current LLM APIs. First, LLMs sometimes select\nprohibited choices that should be strictly avoided in medical practice in\nJapan, such as suggesting euthanasia. Further, our analysis shows that the API\ncosts are generally higher and the maximum context size is smaller for Japanese\nbecause of the way non-Latin scripts are currently tokenized in the pipeline.\nWe release our benchmark as Igaku QA as well as all model outputs and exam\nmetadata. We hope that our results and benchmark will spur progress on more\ndiverse applications of LLMs. Our benchmark is available at\nhttps://github.com/jungokasai/IgakuQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasai_Y/0/1/0/all/0/1\">Yuhei Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1\">Keisuke Sakaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamada_Y/0/1/0/all/0/1\">Yutaro Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No Place to Hide: Dual Deep Interaction Channel Network for Fake News Detection based on Data Augmentation. (arXiv:2303.18049v1 [cs.CL])","link":"http://arxiv.org/abs/2303.18049","description":"<p>Online Social Network (OSN) has become a hotbed of fake news due to the low\ncost of information dissemination. Although the existing methods have made many\nattempts in news content and propagation structure, the detection of fake news\nis still facing two challenges: one is how to mine the unique key features and\nevolution patterns, and the other is how to tackle the problem of small samples\nto build the high-performance model. Different from popular methods which take\nfull advantage of the propagation topology structure, in this paper, we propose\na novel framework for fake news detection from perspectives of semantic,\nemotion and data enhancement, which excavates the emotional evolution patterns\nof news participants during the propagation process, and a dual deep\ninteraction channel network of semantic and emotion is designed to obtain a\nmore comprehensive and fine-grained news representation with the consideration\nof comments. Meanwhile, the framework introduces a data enhancement module to\nobtain more labeled data with high quality based on confidence which further\nimproves the performance of the classification model. Experiments show that the\nproposed approach outperforms the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Biwei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_L/0/1/0/all/0/1\">Lulu Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiuxin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_J/0/1/0/all/0/1\">Jie Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwok_J/0/1/0/all/0/1\">James Tin-Yau Kwok</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving morphological analogies: from retrieval to generation. (arXiv:2303.18062v1 [cs.CL])","link":"http://arxiv.org/abs/2303.18062","description":"<p>Analogical inference is a remarkable capability of human reasoning, and has\nbeen used to solve hard reasoning tasks. Analogy based reasoning (AR) has\ngained increasing interest from the artificial intelligence community and has\nshown its potential in multiple machine learning tasks such as classification,\ndecision making and recommendation with competitive results. We propose a deep\nlearning (DL) framework to address and tackle two key tasks in AR: analogy\ndetection and solving. The framework is thoroughly tested on the Siganalogies\ndataset of morphological analogical proportions (APs) between words, and shown\nto outperform symbolic approaches in many languages. Previous work have\nexplored the behavior of the Analogy Neural Network for classification (ANNc)\non analogy detection and of the Analogy Neural Network for retrieval (ANNr) on\nanalogy solving by retrieval, as well as the potential of an autoencoder (AE)\nfor analogy solving by generating the solution word. In this article we\nsummarize these findings and we extend them by combining ANNr and the AE\nembedding model, and checking the performance of ANNc as an retrieval method.\nThe combination of ANNr and AE outperforms the other approaches in almost all\ncases, and ANNc as a retrieval method achieves competitive or better\nperformance than 3CosMul. We conclude with general guidelines on using our\nframework to tackle APs with DL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marquer_E/0/1/0/all/0/1\">Esteban Marquer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couceiro_M/0/1/0/all/0/1\">Miguel Couceiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dataset and Baseline System for Multi-lingual Extraction and Normalization of Temporal and Numerical Expressions. (arXiv:2303.18103v1 [cs.CL])","link":"http://arxiv.org/abs/2303.18103","description":"<p>Temporal and numerical expression understanding is of great importance in\nmany downstream Natural Language Processing (NLP) and Information Retrieval\n(IR) tasks. However, much previous work covers only a few sub-types and focuses\nonly on entity extraction, which severely limits the usability of identified\nmentions. In order for such entities to be useful in downstream scenarios,\ncoverage and granularity of sub-types are important; and, even more so,\nproviding resolution into concrete values that can be manipulated. Furthermore,\nmost previous work addresses only a handful of languages. Here we describe a\nmulti-lingual evaluation dataset - NTX - covering diverse temporal and\nnumerical expressions across 14 languages and covering extraction,\nnormalization, and resolution. Along with the dataset we provide a robust\nrule-based system as a strong baseline for comparisons against other models to\nbe evaluated in this dataset. Data and code are available at\n\\url{https://aka.ms/NTX}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sanxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yongqiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_B/0/1/0/all/0/1\">B&#xf6;rje F. Karlsson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Edinburgh International Accents of English Corpus: Towards the Democratization of English ASR. (arXiv:2303.18110v1 [cs.CL])","link":"http://arxiv.org/abs/2303.18110","description":"<p>English is the most widely spoken language in the world, used daily by\nmillions of people as a first or second language in many different contexts. As\na result, there are many varieties of English. Although the great many advances\nin English automatic speech recognition (ASR) over the past decades, results\nare usually reported based on test datasets which fail to represent the\ndiversity of English as spoken today around the globe. We present the first\nrelease of The Edinburgh International Accents of English Corpus (EdAcc). This\ndataset attempts to better represent the wide diversity of English,\nencompassing almost 40 hours of dyadic video call conversations between\nfriends. Unlike other datasets, EdAcc includes a wide range of first and\nsecond-language varieties of English and a linguistic background profile of\neach speaker. Results on latest public, and commercial models show that EdAcc\nhighlights shortcomings of current English ASR models. The best performing\nmodel, trained on 680 thousand hours of transcribed data, obtains an average of\n19.7% word error rate (WER) -- in contrast to the 2.7% WER obtained when\nevaluated on US English clean read speech. Across all models, we observe a drop\nin performance on Indian, Jamaican, and Nigerian English speakers. Recordings,\nlinguistic backgrounds, data statement, and evaluation scripts are released on\nour website (https://groups.inf.ed.ac.uk/edacc/) under CC-BY-SA license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanabria_R/0/1/0/all/0/1\">Ramon Sanabria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogoychev_N/0/1/0/all/0/1\">Nikolay Bogoychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markl_N/0/1/0/all/0/1\">Nina Markl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmantini_A/0/1/0/all/0/1\">Andrea Carmantini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klejch_O/0/1/0/all/0/1\">Ondrej Klejch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bell_P/0/1/0/all/0/1\">Peter Bell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pair Programming with Large Language Models for Sampling and Estimation of Copulas. (arXiv:2303.18116v1 [cs.CL])","link":"http://arxiv.org/abs/2303.18116","description":"<p>Without writing a single line of code by a human, an example Monte Carlo\nsimulation based application for stochastic dependence modeling with copulas is\ndeveloped using a state-of-the-art large language model (LLM) fine-tuned for\nconversations. This includes interaction with ChatGPT in natural language and\nusing mathematical formalism, which, under careful supervision by a\nhuman-expert, led to producing a working code in MATLAB, Python and R for\nsampling from a given copula model, evaluation of the model's density,\nperforming maximum likelihood estimation, optimizing the code for parallel\ncomputing for CPUs as well as for GPUs, and visualization of the computed\nresults. In contrast to other emerging studies that assess the accuracy of LLMs\nlike ChatGPT on tasks from a selected area, this work rather investigates ways\nhow to achieve a successful solution of a standard statistical task in a\ncollaboration of a human-expert and artificial intelligence (AI). Particularly,\nthrough careful prompt engineering, we separate successful solutions generated\nby ChatGPT from unsuccessful ones, resulting in a comprehensive list of related\npros and cons. It is demonstrated that if the typical pitfalls are avoided, we\ncan substantially benefit from collaborating with an AI partner. For example,\nwe show that if ChatGPT is not able to provide a correct solution due to a lack\nof or incorrect knowledge, the human-expert can feed it with the correct\nknowledge, e.g., in the form of mathematical theorems and formulas, and make it\nto apply the gained knowledge in order to provide a solution that is correct.\nSuch ability presents an attractive opportunity to achieve a programmed\nsolution even for users with rather limited knowledge of programming\ntechniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gorecki_J/0/1/0/all/0/1\">Jan G&#xf3;recki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UKP-SQuARE v3: A Platform for Multi-Agent QA Research. (arXiv:2303.18120v1 [cs.CL])","link":"http://arxiv.org/abs/2303.18120","description":"<p>The continuous development of Question Answering (QA) datasets has drawn the\nresearch community's attention toward multi-domain models. A popular approach\nis to use multi-dataset models, which are models trained on multiple datasets\nto learn their regularities and prevent overfitting to a single dataset.\nHowever, with the proliferation of QA models in online repositories such as\nGitHub or Hugging Face, an alternative is becoming viable. Recent works have\ndemonstrated that combining expert agents can yield large performance gains\nover multi-dataset models. To ease research in multi-agent models, we extend\nUKP-SQuARE, an online platform for QA research, to support three families of\nmulti-agent systems: i) agent selection, ii) early-fusion of agents, and iii)\nlate-fusion of agents. We conduct experiments to evaluate their inference speed\nand discuss the performance vs. speed trade-off compared to multi-dataset\nmodels. UKP-SQuARE is open-source and publicly available at\n<a href=\"http://square.ukp-lab.de.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Puerto_H/0/1/0/all/0/1\">Haritz Puerto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_T/0/1/0/all/0/1\">Tim Baumg&#xe4;rtner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_R/0/1/0/all/0/1\">Rachneet Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Haishuo Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tariverdian_S/0/1/0/all/0/1\">Sewin Tariverdian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTino: an Italian DistilBERT model. (arXiv:2303.18121v1 [cs.CL])","link":"http://arxiv.org/abs/2303.18121","description":"<p>The recent introduction of Transformers language representation models\nallowed great improvements in many natural language processing (NLP) tasks.\nHowever, if on one hand the performances achieved by this kind of architectures\nare surprising, on the other their usability is limited by the high number of\nparameters which constitute their network, resulting in high computational and\nmemory demands. In this work we present BERTino, a DistilBERT model which\nproposes to be the first lightweight alternative to the BERT architecture\nspecific for the Italian language. We evaluated BERTino on the Italian ISDT,\nItalian ParTUT, Italian WikiNER and multiclass classification tasks, obtaining\nF1 scores comparable to those obtained by a BERTBASE with a remarkable\nimprovement in training and inference speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muffo_M/0/1/0/all/0/1\">Matteo Muffo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertino_E/0/1/0/all/0/1\">Enrico Bertino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can AI Chatbots Pass the Fundamentals of Engineering (FE) and Principles and Practice of Engineering (PE) Structural Exams?. (arXiv:2303.18149v1 [cs.CL])","link":"http://arxiv.org/abs/2303.18149","description":"<p>The engineering community has recently witnessed the emergence of chatbot\ntechnology with the release of OpenAI ChatGPT-4 and Google Bard. While these\nchatbots have been reported to perform well and even pass various standardized\ntests, including medical and law exams, this forum paper explores whether these\nchatbots can also pass the Fundamentals of Engineering (FE) and Principles and\nPractice of Engineering (PE) exams. A diverse range of civil and environmental\nengineering questions and scenarios are used to evaluate the chatbots'\nperformance, as commonly present in the FE and PE exams. The chatbots'\nresponses were analyzed based on their relevance, accuracy, and clarity and\nthen compared against the recommendations of the National Council of Examiners\nfor Engineering and Surveying (NCEES). Our report shows that ChatGPT-4 and\nBard, respectively scored 70.9% and 39.2% in the FE exam and 46.2% and 41% in\nthe PE exam. It is evident that the current version of ChatGPT-4 could\npotentially pass the FE exam. While future editions are much more likely to\npass both exams, this study also highlights the potential of using chatbots as\nteaching assistants and guiding engineers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naser_M/0/1/0/all/0/1\">M.Z. Naser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_B/0/1/0/all/0/1\">Brandon Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogle_J/0/1/0/all/0/1\">Jennier Ogle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kodur_V/0/1/0/all/0/1\">Venkatesh Kodur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawileh_R/0/1/0/all/0/1\">Rami Hawileh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdalla_J/0/1/0/all/0/1\">Jamal Abdalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thai_H/0/1/0/all/0/1\">Huu-Tai Thai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multiple Choices Reading Comprehension Corpus for Vietnamese Language Education. (arXiv:2303.18162v1 [cs.CL])","link":"http://arxiv.org/abs/2303.18162","description":"<p>Machine reading comprehension has been an interesting and challenging task in\nrecent years, with the purpose of extracting useful information from texts. To\nattain the computer ability to understand the reading text and answer relevant\ninformation, we introduce ViMMRC 2.0 - an extension of the previous ViMMRC for\nthe task of multiple-choice reading comprehension in Vietnamese Textbooks which\ncontain the reading articles for students from Grade 1 to Grade 12. This\ndataset has 699 reading passages which are prose and poems, and 5,273\nquestions. The questions in the new dataset are not fixed with four options as\nin the previous version. Moreover, the difficulty of questions is increased,\nwhich challenges the models to find the correct choice. The computer must\nunderstand the whole context of the reading passage, the question, and the\ncontent of each choice to extract the right answers. Hence, we propose the\nmulti-stage approach that combines the multi-step attention network (MAN) with\nthe natural language inference (NLI) task to enhance the performance of the\nreading comprehension model. Then, we compare the proposed methodology with the\nbaseline BERTology models on the new dataset and the ViMMRC 1.0. Our\nmulti-stage models achieved 58.81% by Accuracy on the test set, which is 5.34%\nbetter than the highest BERTology models. From the results of the error\nanalysis, we found the challenge of the reading comprehension models is\nunderstanding the implicit context in texts and linking them together in order\nto find the correct answers. Finally, we hope our new dataset will motivate\nfurther research in enhancing the language understanding ability of computers\nin the Vietnamese language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_K/0/1/0/all/0/1\">Khoi Trong Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1\">Tuong Quang Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Language Model Deployment with Risk Cards. (arXiv:2303.18190v1 [cs.CL])","link":"http://arxiv.org/abs/2303.18190","description":"<p>This paper introduces RiskCards, a framework for structured assessment and\ndocumentation of risks associated with an application of language models. As\nwith all language, text generated by language models can be harmful, or used to\nbring about harm. Automating language generation adds both an element of scale\nand also more subtle or emergent undesirable tendencies to the generated text.\nPrior work establishes a wide variety of language model harms to many different\nactors: existing taxonomies identify categories of harms posed by language\nmodels; benchmarks establish automated tests of these harms; and documentation\nstandards for models, tasks and datasets encourage transparent reporting.\nHowever, there is no risk-centric framework for documenting the complexity of a\nlandscape in which some risks are shared across models and contexts, while\nothers are specific, and where certain conditions may be required for risks to\nmanifest as harms. RiskCards address this methodological gap by providing a\ngeneric framework for assessing the use of a given language model in a given\nscenario. Each RiskCard makes clear the routes for the risk to manifest harm,\ntheir placement in harm taxonomies, and example prompt-output pairs. While\nRiskCards are designed to be open-source, dynamic and participatory, we present\na \"starter set\" of RiskCards taken from a broad literature survey, each of\nwhich details a concrete risk presentation. Language model RiskCards initiate a\ncommunity knowledge base which permits the mapping of risks and harms to a\nspecific model or its application scenario, ultimately contributing to a\nbetter, safer and shared understanding of the risk landscape.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1\">Leon Derczynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Rose Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandran_V/0/1/0/all/0/1\">Vidhisha Balachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sachin Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leiser_M/0/1/0/all/0/1\">M. R. Leiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])","link":"http://arxiv.org/abs/2303.18223","description":"<p>Language is essentially a complex, intricate system of human expressions\ngoverned by grammatical rules. It poses a significant challenge to develop\ncapable AI algorithms for comprehending and grasping a language. As a major\napproach, language modeling has been widely studied for language understanding\nand generation in the past two decades, evolving from statistical language\nmodels to neural language models. Recently, pre-trained language models (PLMs)\nhave been proposed by pre-training Transformer models over large-scale corpora,\nshowing strong capabilities in solving various NLP tasks. Since researchers\nhave found that model scaling can lead to performance improvement, they further\nstudy the scaling effect by increasing the model size to an even larger size.\nInterestingly, when the parameter scale exceeds a certain level, these enlarged\nlanguage models not only achieve a significant performance improvement but also\nshow some special abilities that are not present in small-scale language\nmodels. To discriminate the difference in parameter scale, the research\ncommunity has coined the term large language models (LLM) for the PLMs of\nsignificant size. Recently, the research on LLMs has been largely advanced by\nboth academia and industry, and a remarkable progress is the launch of ChatGPT,\nwhich has attracted widespread attention from society. The technical evolution\nof LLMs has been making an important impact on the entire AI community, which\nwould revolutionize the way how we develop and use AI algorithms. In this\nsurvey, we review the recent advances of LLMs by introducing the background,\nkey findings, and mainstream techniques. In particular, we focus on four major\naspects of LLMs, namely pre-training, adaptation tuning, utilization, and\ncapacity evaluation. Besides, we also summarize the available resources for\ndeveloping LLMs and discuss the remaining issues for future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yupeng Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1\">Yingqian Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Beichen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zican Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yifan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yushuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhipeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jinhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1\">Ruiyang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xinyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zikang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated scholarly paper review: Concepts, technologies, and challenges. (arXiv:2111.07533v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2111.07533","description":"<p>Peer review is a widely accepted mechanism for research evaluation, playing a\npivotal role in academic publishing. However, criticisms have long been leveled\non this mechanism, mostly because of its poor efficiency and low\nreproducibility. Recent years have seen the application of artificial\nintelligence (AI) in assisting the peer review process. Nonetheless, with the\ninvolvement of humans, such limitations remain inevitable. In this paper, we\npropose the concept and pipeline of automated scholarly paper review (ASPR) and\nreview the relevant literature and technologies of achieving a full-scale\ncomputerized review process. On the basis of the review and discussion, we\nconclude that there is already corresponding research and preliminary\nimplementation at each stage of ASPR. We further look into the challenges in\nASPR with the existing technologies. The major difficulties lie in imperfect\ndocument parsing and representation, inadequate data, defective human-computer\ninteraction, and flawed deep logical reasoning. Moreover, we discuss the\npossible moral and ethical issues and point out the future directions of ASPR.\nIn the foreseeable future, ASPR and peer review will coexist in a reinforcing\nmanner before ASPR is able to fully undertake the reviewing workload from\nhumans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jialiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiaxin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhangping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yidong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaodong Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Natural Language to Simulations: Applying GPT-3 Codex to Automate Simulation Modeling of Logistics Systems. (arXiv:2202.12107v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2202.12107","description":"<p>Our work is the first attempt to apply Natural Language Processing to\nautomate the development of simulation models of systems vitally important for\nlogistics. We demonstrated that the framework built on top of the fine-tuned\nGPT-3 Codex, a Transformer-based language model, could produce functionally\nvalid simulations of queuing and inventory control systems given the verbal\ndescription. In conducted experiments, GPT-3 Codex demonstrated convincing\nexpertise in Python as well as an understanding of the domain-specific\nvocabulary. As a result, the language model could produce simulations of a\nsingle-product inventory-control system and single-server queuing system given\nthe domain-specific context, a detailed description of the process, and a list\nof variables with the corresponding values. The demonstrated results, along\nwith the rapid improvement of language models, open the door for significant\nsimplification of the workflow behind the simulation model development, which\nwill allow experts to focus on the high-level consideration of the problem and\nholistic thinking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jackson_I/0/1/0/all/0/1\">Ilya Jackson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenz_M/0/1/0/all/0/1\">Maria Jesus Saenz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M-MELD: A Multilingual Multi-Party Dataset for Emotion Recognition in Conversations. (arXiv:2203.16799v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16799","description":"<p>Expression of emotions is a crucial part of daily human communication.\nEmotion recognition in conversations (ERC) is an emerging field of study, where\nthe primary task is to identify the emotion behind each utterance in a\nconversation. Though a lot of work has been done on ERC in the past, these\nworks only focus on ERC in the English language, thereby ignoring any other\nlanguages. In this paper, we present Multilingual MELD (M-MELD), where we\nextend the Multimodal EmotionLines Dataset (MELD) \\cite{poria2018meld} to 4\nother languages beyond English, namely Greek, Polish, French, and Spanish.\nBeyond just establishing strong baselines for all of these 4 languages, we also\npropose a novel architecture, DiscLSTM, that uses both sequential and\nconversational discourse context in a conversational dialogue for ERC. Our\nproposed approach is computationally efficient, can transfer across languages\nusing just a cross-lingual encoder, and achieves better performance than most\nuni-modal text approaches in the literature on both MELD and M-MELD. We make\nour data and code publicly on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramaneswaran_S/0/1/0/all/0/1\">S Ramaneswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_U/0/1/0/all/0/1\">Utkarsh Tyagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_H/0/1/0/all/0/1\">Harshvardhan Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepcha_S/0/1/0/all/0/1\">Samden Lepcha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakshi_S/0/1/0/all/0/1\">S Sakshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting prompt learning with pre-trained language models for Alzheimer's Disease detection. (arXiv:2210.16539v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.16539","description":"<p>Early diagnosis of Alzheimer's disease (AD) is crucial in facilitating\npreventive care and to delay further progression. Speech based automatic AD\nscreening systems provide a non-intrusive and more scalable alternative to\nother clinical screening techniques. Textual embedding features produced by\npre-trained language models (PLMs) such as BERT are widely used in such\nsystems. However, PLM domain fine-tuning is commonly based on the masked word\nor sentence prediction costs that are inconsistent with the back-end AD\ndetection task. To this end, this paper investigates the use of prompt-based\nfine-tuning of PLMs that consistently uses AD classification errors as the\ntraining objective function. Disfluency features based on hesitation or pause\nfiller token frequencies are further incorporated into prompt phrases during\nPLM fine-tuning. The decision voting based combination among systems using\ndifferent PLMs (BERT and RoBERTa) or systems with different fine-tuning\nparadigms (conventional masked-language modelling fine-tuning and prompt-based\nfine-tuning) is further applied. Mean, standard deviation and the maximum among\naccuracy scores over 15 experiment runs are adopted as performance measurements\nfor the AD detection system. Mean detection accuracy of 84.20% (with std 2.09%,\nbest 87.5%) and 82.64% (with std 4.0%, best 89.58%) were obtained using manual\nand ASR speech transcripts respectively on the ADReSS20 test set consisting of\n48 elderly speakers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiajun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianzi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shoukang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xunying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrated Interpretation: Confidence Estimation in Semantic Parsing. (arXiv:2211.07443v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07443","description":"<p>Sequence generation models are increasingly being used to translate language\ninto executable programs, i.e. to perform executable semantic parsing. The fact\nthat semantic parsing aims to execute actions in the real world motivates\ndeveloping safe systems, which in turn makes measuring calibration -- a central\ncomponent to safety -- particularly important. We investigate the calibration\nof common generation models across four popular semantic parsing datasets,\nfinding that it varies across models and datasets. We then analyze factors\nassociated with calibration error and release new confidence-based challenge\nsplits of two parsing datasets. To facilitate the inclusion of calibration in\nsemantic parsing evaluations, we release a library for computing calibration\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stengel_Eskin_E/0/1/0/all/0/1\">Elias Stengel-Eskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SexWEs: Domain-Aware Word Embeddings via Cross-lingual Semantic Specialisation for Chinese Sexism Detection in Social Media. (arXiv:2211.08447v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08447","description":"<p>The goal of sexism detection is to mitigate negative online content targeting\ncertain gender groups of people. However, the limited availability of labeled\nsexism-related datasets makes it problematic to identify online sexism for\nlow-resource languages. In this paper, we address the task of automatic sexism\ndetection in social media for one low-resource language -- Chinese. Rather than\ncollecting new sexism data or building cross-lingual transfer learning models,\nwe develop a cross-lingual domain-aware semantic specialisation system in order\nto make the most of existing data. Semantic specialisation is a technique for\nretrofitting pre-trained distributional word vectors by integrating external\nlinguistic knowledge (such as lexico-semantic relations) into the specialised\nfeature space. To do this, we leverage semantic resources for sexism from a\nhigh-resource language (English) to specialise pre-trained word vectors in the\ntarget language (Chinese) to inject domain knowledge. We demonstrate the\nbenefit of our sexist word embeddings (SexWEs) specialised by our framework via\nintrinsic evaluation of word similarity and extrinsic evaluation of sexism\ndetection. Compared with other specialisation approaches and Chinese baseline\nword vectors, our SexWEs shows an average score improvement of 0.033 and 0.064\nin both intrinsic and extrinsic evaluations, respectively. The ablative results\nand visualisation of SexWEs also prove the effectiveness of our framework on\nretrofitting word vectors in low-resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1\">Aiqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoP: Factual Inconsistency Detection by Controlling the Preference. (arXiv:2212.01611v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.01611","description":"<p>Abstractive summarization is the process of generating a summary given a\ndocument as input. Although significant progress has been made, the factual\ninconsistency between the document and the generated summary still limits its\npractical applications. Previous work found that the probabilities assigned by\nthe generation model reflect its preferences for the generated summary,\nincluding the preference for factual consistency, and the preference for the\nlanguage or knowledge prior as well. To separate the preference for factual\nconsistency, we propose an unsupervised framework named CoP by controlling the\npreference of the generation model with the help of prompt. More specifically,\nthe framework performs an extra inference step in which a text prompt is\nintroduced as an additional input. In this way, another preference is described\nby the generation probability of this extra inference process. The difference\nbetween the above two preferences, i.e. the difference between the\nprobabilities, could be used as measurements for detecting factual\ninconsistencies. Interestingly, we found that with the properly designed\nprompt, our framework could evaluate specific preferences and serve as\nmeasurements for fine-grained categories of inconsistency, such as\nentity-related inconsistency, coreference-related inconsistency, etc. Moreover,\nour framework could also be extended to the supervised setting to learn better\nprompt from the labeled data as well. Experiments show that our framework\nachieves new SOTA results on three factual inconsistency detection tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+She_S/0/1/0/all/0/1\">Shuaijie She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiang Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Editing Models with Task Arithmetic. (arXiv:2212.04089v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2212.04089","description":"<p>Changing how pre-trained models behave -- e.g., improving their performance\non a downstream task or mitigating biases learned during pre-training -- is a\ncommon practice when developing machine learning systems. In this work, we\npropose a new paradigm for steering the behavior of neural networks, centered\naround \\textit{task vectors}. A task vector specifies a direction in the weight\nspace of a pre-trained model, such that movement in that direction improves\nperformance on the task. We build task vectors by subtracting the weights of a\npre-trained model from the weights of the same model after fine-tuning on a\ntask. We show that these task vectors can be modified and combined together\nthrough arithmetic operations such as negation and addition, and the behavior\nof the resulting model is steered accordingly. Negating a task vector decreases\nperformance on the target task, with little change in model behavior on control\ntasks. Moreover, adding task vectors together can improve performance on\nmultiple tasks at once. Finally, when tasks are linked by an analogy\nrelationship of the form ``A is to B as C is to D\", combining task vectors from\nthree of the tasks can improve performance on the fourth, even when no data\nfrom the fourth task is used for training. Overall, our experiments with\nseveral models, modalities and tasks show that task arithmetic is a simple,\nefficient and effective way of editing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Marco Tulio Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1\">Suchin Gururangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Learning for Cross-Target Stance Detection by Aggregating Multimodal Embeddings. (arXiv:2301.04535v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.04535","description":"<p>Despite the increasing popularity of the stance detection task, existing\napproaches are predominantly limited to using the textual content of social\nmedia posts for the classification, overlooking the social nature of the task.\nThe stance detection task becomes particularly challenging in cross-target\nclassification scenarios, where even in few-shot training settings the model\nneeds to predict the stance towards new targets for which the model has only\nseen few relevant samples during training. To address the cross-target stance\ndetection in social media by leveraging the social nature of the task, we\nintroduce CT-TN, a novel model that aggregates multimodal embeddings derived\nfrom both textual and network features of the data. We conduct experiments in a\nfew-shot cross-target scenario on six different combinations of\nsource-destination target pairs. By comparing CT-TN with state-of-the-art\ncross-target stance detection models, we demonstrate the effectiveness of our\nmodel by achieving average performance improvements ranging from 11% to 21%\nacross different baseline models. Experiments with different numbers of shots\nshow that CT-TN can outperform other models after seeing 300 instances of the\ndestination target. Further, ablation experiments demonstrate the positive\ncontribution of each of the components of CT-TN towards the final performance.\nWe further analyse the network interactions between social media users, which\nreveal the potential of using social features for cross-target stance\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khiabani_P/0/1/0/all/0/1\">Parisa Jamadi Khiabani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rewarding Chatbots for Real-World Engagement with Millions of Users. (arXiv:2303.06135v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.06135","description":"<p>The emergence of pretrained large language models has led to the deployment\nof a range of social chatbots for chitchat. Although these chatbots demonstrate\nlanguage ability and fluency, they are not guaranteed to be engaging and can\nstruggle to retain users. This work investigates the development of social\nchatbots that prioritize user engagement to enhance retention, specifically\nexamining the use of human feedback to efficiently develop highly engaging\nchatbots. The proposed approach uses automatic pseudo-labels collected from\nuser interactions to train a reward model that can be used to reject\nlow-scoring sample responses generated by the chatbot model at inference time.\nIntuitive evaluation metrics, such as mean conversation length (MCL), are\nintroduced as proxies to measure the level of engagement of deployed chatbots.\nA/B testing on groups of 10,000 new daily chatbot users on the Chai Research\nplatform shows that this approach increases the MCL by up to 70%, which\ntranslates to a more than 30% increase in user retention for a GPT-J 6B model.\nFuture work aims to use the reward model to realise a data fly-wheel, where the\nlatest user conversations can be used to alternately fine-tune the language\nmodel and the reward model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Irvine_R/0/1/0/all/0/1\">Robert Irvine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boubert_D/0/1/0/all/0/1\">Douglas Boubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vyas Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liusie_A/0/1/0/all/0/1\">Adian Liusie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Ziyi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mudupalli_V/0/1/0/all/0/1\">Vineet Mudupalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korshuk_A/0/1/0/all/0/1\">Aliaksei Korshuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zongyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremer_F/0/1/0/all/0/1\">Fritz Cremer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assassi_V/0/1/0/all/0/1\">Valentin Assassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beauchamp_C/0/1/0/all/0/1\">Christie-Carol Beauchamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaoding Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rialan_T/0/1/0/all/0/1\">Thomas Rialan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beauchamp_W/0/1/0/all/0/1\">William Beauchamp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fairness-guided Few-shot Prompting for Large Language Models. (arXiv:2303.13217v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.13217","description":"<p>Large language models have demonstrated surprising ability to perform\nin-context learning, i.e., these models can be directly applied to solve\nnumerous downstream tasks by conditioning on a prompt constructed by a few\ninput-output examples. However, prior research has shown that in-context\nlearning can suffer from high instability due to variations in training\nexamples, example order, and prompt formats. Therefore, the construction of an\nappropriate prompt is essential for improving the performance of in-context\nlearning. In this paper, we revisit this problem from the view of predictive\nbias. Specifically, we introduce a metric to evaluate the predictive bias of a\nfixed prompt against labels or a given attributes. Then we empirically show\nthat prompts with higher bias always lead to unsatisfactory predictive quality.\nBased on this observation, we propose a novel search strategy based on the\ngreedy search to identify the near-optimal prompt for improving the performance\nof in-context learning. We perform comprehensive experiments with\nstate-of-the-art mainstream models such as GPT-3 on various downstream tasks.\nOur results indicate that our method can enhance the model's in-context\nlearning performance in an effective and interpretable manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Huan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1\">Yatao Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bingzhe Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT and a New Academic Reality: Artificial Intelligence-Written Research Papers and the Ethics of the Large Language Models in Scholarly Publishing. (arXiv:2303.13367v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.13367","description":"<p>This paper discusses OpenAIs ChatGPT, a generative pre-trained transformer,\nwhich uses natural language processing to fulfill text-based user requests\n(i.e., a chatbot). The history and principles behind ChatGPT and similar models\nare discussed. This technology is then discussed in relation to its potential\nimpact on academia and scholarly research and publishing. ChatGPT is seen as a\npotential model for the automated preparation of essays and other types of\nscholarly manuscripts. Potential ethical issues that could arise with the\nemergence of large language models like GPT-3, the underlying technology behind\nChatGPT, and its usage by academics and researchers, are discussed and situated\nwithin the context of broader advancements in artificial intelligence, machine\nlearning, and natural language processing for research and scholarly\npublishing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lund_B/0/1/0/all/0/1\">Brady Lund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Ting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannuru_N/0/1/0/all/0/1\">Nishith Reddy Mannuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_B/0/1/0/all/0/1\">Bing Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimray_S/0/1/0/all/0/1\">Somipam Shimray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages. (arXiv:2303.13592v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.13592","description":"<p>While code-mixing is a common linguistic practice in many parts of the world,\ncollecting high-quality and low-cost code-mixed data remains a challenge for\nnatural language processing (NLP) research. The proliferation of Large Language\nModels (LLMs) in recent times compels one to ask: can these systems be used for\ndata generation? In this article, we explore prompting multilingual LLMs in a\nzero-shot manner to create code-mixed data for five languages in South East\nAsia (SEA) -- Indonesian, Malay, Chinese, Tagalog, Vietnamese, as well as the\ncreole language Singlish. We find that ChatGPT shows the most potential,\ncapable of producing code-mixed text 68% of the time when the term\n\"code-mixing\" is explicitly defined. Moreover, both ChatGPT's and InstructGPT's\n(davinci-003) performances in generating Singlish texts are noteworthy,\naveraging a 96% success rate across a variety of prompts. Their code-mixing\nproficiency, however, is dampened by word choice errors that lead to semantic\ninaccuracies. Other multilingual models such as BLOOMZ and Flan-T5-XXL are\nunable to produce code-mixed texts altogether. By highlighting the limited\npromises of LLMs in a specific form of low-resource data generation, we call\nfor a measured approach when applying similar techniques to other data-scarce\nNLP contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yong_Z/0/1/0/all/0/1\">Zheng-Xin Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruochen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forde_J/0/1/0/all/0/1\">Jessica Zosa Forde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Skyler Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutawika_L/0/1/0/all/0/1\">Lintang Sutawika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1\">Jan Christian Blaise Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_L/0/1/0/all/0/1\">Long Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yin Lin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP. (arXiv:2303.16166v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.16166","description":"<p>Despite its pivotal role in research experiments, code correctness is often\npresumed only on the basis of the perceived quality of the results. This comes\nwith the risk of erroneous outcomes and potentially misleading findings. To\naddress this issue, we posit that the current focus on result reproducibility\nshould go hand in hand with the emphasis on coding best practices. We bolster\nour call to the NLP community by presenting a case study, in which we identify\n(and correct) three bugs in widely used open-source implementations of the\nstate-of-the-art Conformer architecture. Through comparative experiments on\nautomatic speech recognition and translation in various language settings, we\ndemonstrate that the existence of bugs does not prevent the achievement of good\nand reproducible results and can lead to incorrect conclusions that potentially\nmisguide future research. In response to this, this study is a call to action\ntoward the adoption of coding best practices aimed at fostering correctness and\nimproving the quality of the developed software.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilzer_A/0/1/0/all/0/1\">Andrea Pilzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Did You Mean...? Confidence-based Trade-offs in Semantic Parsing. (arXiv:2303.16857v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.16857","description":"<p>We illustrate how a calibrated model can help balance common trade-offs in\ntask-oriented parsing. In a simulated annotator-in-the-loop experiment, we show\nthat well-calibrated confidence scores allow us to balance cost with annotator\nload, improving accuracy with a small number of interactions. We then examine\nhow confidence scores can help optimize the trade-off between usability and\nsafety. We show that confidence-based thresholding can substantially reduce the\nnumber of incorrect low-confidence programs executed; however, this comes at a\ncost to usability. We propose the DidYouMean system which better balances\nusability and safety.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stengel_Eskin_E/0/1/0/all/0/1\">Elias Stengel-Eskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Cross-Cultural Alignment between ChatGPT and Human Societies: An Empirical Study. (arXiv:2303.17466v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.17466","description":"<p>The recent release of ChatGPT has garnered widespread recognition for its\nexceptional ability to generate human-like responses in dialogue. Given its\nusage by users from various nations and its training on a vast multilingual\ncorpus that incorporates diverse cultural and societal norms, it is crucial to\nevaluate its effectiveness in cultural adaptation. In this paper, we\ninvestigate the underlying cultural background of ChatGPT by analyzing its\nresponses to questions designed to quantify human cultural differences. Our\nfindings suggest that, when prompted with American context, ChatGPT exhibits a\nstrong alignment with American culture, but it adapts less effectively to other\ncultural contexts. Furthermore, by using different prompts to probe the model,\nwe show that English prompts reduce the variance in model responses, flattening\nout cultural differences and biasing them towards American culture. This study\nprovides valuable insights into the cultural implications of ChatGPT and\nhighlights the necessity of greater diversity and cultural awareness in\nlanguage technologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Li Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seolhwa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabello_L/0/1/0/all/0/1\">Laura Cabello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Min Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-04-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/"}}]}]}