{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-05-15T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Quran Recitation Recognition using End-to-End Deep Learning. (arXiv:2305.07034v1 [eess.AS])","link":"http://arxiv.org/abs/2305.07034","description":"<p>The Quran is the holy scripture of Islam, and its recitation is an important\naspect of the religion. Recognizing the recitation of the Holy Quran\nautomatically is a challenging task due to its unique rules that are not\napplied in normal speaking speeches. A lot of research has been done in this\ndomain, but previous works have detected recitation errors as a classification\ntask or used traditional automatic speech recognition (ASR). In this paper, we\nproposed a novel end-to-end deep learning model for recognizing the recitation\nof the Holy Quran. The proposed model is a CNN-Bidirectional GRU encoder that\nuses CTC as an objective function, and a character-based decoder which is a\nbeam search decoder. Moreover, all previous works were done on small private\ndatasets consisting of short verses and a few chapters of the Holy Quran. As a\nresult of using private datasets, no comparisons were done. To overcome this\nissue, we used a public dataset that has recently been published (Ar-DAD) and\ncontains about 37 chapters that were recited by 30 reciters, with different\nrecitation speeds and different types of pronunciation rules. The proposed\nmodel performance was evaluated using the most common evaluation metrics in\nspeech recognition, word error rate (WER), and character error rate (CER). The\nresults were 8.34% WER and 2.42% CER. We hope this research will be a baseline\nfor comparisons with future research on this public new dataset (Ar-DAD).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Harere_A/0/1/0/all/0/1\">Ahmad Al Harere</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jallad_K/0/1/0/all/0/1\">Khloud Al Jallad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Contrastive Learning with Noise-Guided Attack: Towards Continual Relation Extraction in the Wild. (arXiv:2305.07085v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07085","description":"<p>The principle of continual relation extraction~(CRE) involves adapting to\nemerging novel relations while preserving od knowledge. While current endeavors\nin CRE succeed in preserving old knowledge, they tend to fail when exposed to\ncontaminated data streams. We assume this is attributed to their reliance on an\nartificial hypothesis that the data stream has no annotation errors, which\nhinders real-world applications for CRE. Considering the ubiquity of noisy\nlabels in real-world datasets, in this paper, we formalize a more practical\nlearning scenario, termed as \\textit{noisy-CRE}. Building upon this challenging\nsetting, we develop a noise-resistant contrastive framework named as\n\\textbf{N}oise-guided \\textbf{a}ttack in \\textbf{C}ontrative\n\\textbf{L}earning~(NaCL) to learn incremental corrupted relations. Compared to\ndirect noise discarding or inaccessible noise relabeling, we present modifying\nthe feature space to match the given noisy labels via attacking can better\nenrich contrastive representations. Extensive empirical validations highlight\nthat NaCL can achieve consistent performance improvements with increasing noise\nrates, outperforming state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Ting Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Rui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-Text Rationales. (arXiv:2305.07095v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07095","description":"<p>Among the remarkable emergent capabilities of large language models (LMs) is\nfree-text rationalization; beyond a certain scale, large LMs are capable of\ngenerating seemingly useful rationalizations, which in turn, can dramatically\nenhance their performances on leaderboards. This phenomenon raises a question:\ncan machine generated rationales also be useful for humans, especially when lay\nhumans try to answer questions based on those machine rationales? We observe\nthat human utility of existing rationales is far from satisfactory, and\nexpensive to estimate with human studies. Existing metrics like task\nperformance of the LM generating the rationales, or similarity between\ngenerated and gold rationales are not good indicators of their human utility.\nWhile we observe that certain properties of rationales like conciseness and\nnovelty are correlated with their human utility, estimating them without human\ninvolvement is challenging. We show that, by estimating a rationale's\nhelpfulness in answering similar unseen instances, we can measure its human\nutility to a better extent. We also translate this finding into an automated\nscore, GEN-U, that we propose, which can help improve LMs' ability to generate\nrationales with better human utility, while maintaining most of its task\nperformance. Lastly, we release all code and collected data with this project.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_B/0/1/0/all/0/1\">Brihi Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramnath_S/0/1/0/all/0/1\">Sahana Ramnath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Aaron Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1\">Zhewei Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1\">Shaoliang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overinformative Question Answering by Humans and Machines. (arXiv:2305.07151v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07151","description":"<p>When faced with a polar question, speakers often provide overinformative\nanswers going beyond a simple \"yes\" or \"no\". But what principles guide the\nselection of additional information? In this paper, we provide experimental\nevidence from two studies suggesting that overinformativeness in human\nanswering is driven by considerations of relevance to the questioner's goals\nwhich they flexibly adjust given the functional context in which the question\nis uttered. We take these human results as a strong benchmark for investigating\nquestion-answering performance in state-of-the-art neural language models,\nconducting an extensive evaluation on items from human experiments. We find\nthat most models fail to adjust their answering behavior in a human-like way\nand tend to include irrelevant information. We show that GPT-3 is highly\nsensitive to the form of the prompt and only achieves human-like answer\npatterns when guided by an example and cognitively-motivated explanation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsvilodub_P/0/1/0/all/0/1\">Polina Tsvilodub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franke_M/0/1/0/all/0/1\">Michael Franke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawkins_R/0/1/0/all/0/1\">Robert D. Hawkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah D. Goodman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Zero and Few-shot Techniques for Intent Classification. (arXiv:2305.07157v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07157","description":"<p>Conversational NLU providers often need to scale to thousands of\nintent-classification models where new customers often face the cold-start\nproblem. Scaling to so many customers puts a constraint on storage space as\nwell. In this paper, we explore four different zero and few-shot intent\nclassification approaches with this low-resource constraint: 1) domain\nadaptation, 2) data augmentation, 3) zero-shot intent classification using\ndescriptions large language models (LLMs), and 4) parameter-efficient\nfine-tuning of instruction-finetuned language models. Our results show that all\nthese approaches are effective to different degrees in low-resource settings.\nParameter-efficient fine-tuning using T-few recipe (Liu et al., 2022) on\nFlan-T5 (Chang et al., 2022) yields the best performance even with just one\nsample per intent. We also show that the zero-shot method of prompting LLMs\nusing intent descriptions\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parikh_S/0/1/0/all/0/1\">Soham Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vohra_Q/0/1/0/all/0/1\">Quaizar Vohra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tumbade_P/0/1/0/all/0/1\">Prashil Tumbade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_M/0/1/0/all/0/1\">Mitul Tiwari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OneCAD: One Classifier for All image Datasets using multimodal learning. (arXiv:2305.07167v1 [cs.CV])","link":"http://arxiv.org/abs/2305.07167","description":"<p>Vision-Transformers (ViTs) and Convolutional neural networks (CNNs) are\nwidely used Deep Neural Networks (DNNs) for classification task. These model\narchitectures are dependent on the number of classes in the dataset it was\ntrained on. Any change in number of classes leads to change (partial or full)\nin the model's architecture. This work addresses the question: Is it possible\nto create a number-of-class-agnostic model architecture?. This allows model's\narchitecture to be independent of the dataset it is trained on. This work\nhighlights the issues with the current architectures (ViTs and CNNs). Also,\nproposes a training and inference framework OneCAD (One Classifier for All\nimage Datasets) to achieve close-to number-of-class-agnostic transformer model.\nTo best of our knowledge this is the first work to use Mask-Image-Modeling\n(MIM) with multimodal learning for classification task to create a DNN model\narchitecture agnostic to the number of classes. Preliminary results are shown\non natural and medical image datasets. Datasets: MNIST, CIFAR10, CIFAR100 and\nCOVIDx. Code will soon be publicly available on github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wadekar_S/0/1/0/all/0/1\">Shakti N. Wadekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Culurciello_E/0/1/0/all/0/1\">Eugenio Culurciello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asymmetric feature interaction for interpreting model predictions. (arXiv:2305.07224v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07224","description":"<p>In natural language processing (NLP), deep neural networks (DNNs) could model\ncomplex interactions between context and have achieved impressive results on a\nrange of NLP tasks. Prior works on feature interaction attribution mainly focus\non studying symmetric interaction that only explains the additional influence\nof a set of words in combination, which fails to capture asymmetric influence\nthat contributes to model prediction. In this work, we propose an asymmetric\nfeature interaction attribution explanation model that aims to explore\nasymmetric higher-order feature interactions in the inference of deep neural\nNLP models. By representing our explanation with an directed interaction graph,\nwe experimentally demonstrate interpretability of the graph to discover\nasymmetric feature interactions. Experimental results on two sentiment\nclassification datasets show the superiority of our model against the\nstate-of-the-art feature interaction attribution methods in identifying\ninfluential features for model predictions. Our code is available at\nhttps://github.com/StillLu/ASIV.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaolei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jianghong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haode Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Giant Language Brains Just Aren't Enough! Domain Pizzazz with Knowledge Sparkle Dust. (arXiv:2305.07230v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07230","description":"<p>Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing, with GPT models at the forefront. While their remarkable\nperformance spans a range of tasks, adapting LLMs for real-world business\nscenarios still poses challenges warranting further investigation. This paper\npresents an empirical analysis aimed at bridging the gap in adapting LLMs to\npractical use cases. To do that, we select the question answering (QA) task of\ninsurance as a case study due to its challenge of reasoning. Based on the task\nwe design a new model relied on LLMs which are empowered by domain-specific\nknowledge extracted from insurance policy rulebooks. The domain-specific\nknowledge helps LLMs to understand new concepts of insurance for domain\nadaptation. Preliminary results on real QA pairs show that knowledge\nenhancement from policy rulebooks significantly improves the reasoning ability\nof GPT-3.5 of 50.4% in terms of accuracy. The analysis also indicates that\nexisting public knowledge bases, e.g., DBPedia is beneficial for knowledge\nenhancement. Our findings reveal that the inherent complexity of business\nscenarios often necessitates the incorporation of domain-specific knowledge and\nexternal resources for effective problem-solving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh-Tien Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duy-Hung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabahi_S/0/1/0/all/0/1\">Shahab Sabahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jeff Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hotta_H/0/1/0/all/0/1\">Hajime Hotta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better speech synthesis through scaling. (arXiv:2305.07243v1 [cs.SD])","link":"http://arxiv.org/abs/2305.07243","description":"<p>In recent years, the field of image generation has been revolutionized by the\napplication of autoregressive transformers and DDPMs. These approaches model\nthe process of image generation as a step-wise probabilistic processes and\nleverage large amounts of compute and data to learn the image distribution.\nThis methodology of improving performance need not be confined to images. This\npaper describes a way to apply advances in the image generative domain to\nspeech synthesis. The result is TorToise -- an expressive, multi-voice\ntext-to-speech system.\n</p>\n<p>All model code and trained weights have been open-sourced at\nhttps://github.com/neonbjb/tortoise-tts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Betker_J/0/1/0/all/0/1\">James Betker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gaussian Prior Reinforcement Learning for Nested Named Entity Recognition. (arXiv:2305.07266v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07266","description":"<p>Named Entity Recognition (NER) is a well and widely studied task in natural\nlanguage processing. Recently, the nested NER has attracted more attention\nsince its practicality and difficulty. Existing works for nested NER ignore the\nrecognition order and boundary position relation of nested entities. To address\nthese issues, we propose a novel seq2seq model named GPRL, which formulates the\nnested NER task as an entity triplet sequence generation process. GPRL adopts\nthe reinforcement learning method to generate entity triplets decoupling the\nentity order in gold labels and expects to learn a reasonable recognition order\nof entities via trial and error. Based on statistics of boundary distance for\nnested entities, GPRL designs a Gaussian prior to represent the boundary\ndistance distribution between nested entities and adjust the output probability\ndistribution of nested boundary tokens. Experiments on three nested NER\ndatasets demonstrate that GPRL outperforms previous nested NER models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yawen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fukun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shu&#x27;ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harvesting Event Schemas from Large Language Models. (arXiv:2305.07280v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07280","description":"<p>Event schema provides a conceptual, structural and formal language to\nrepresent events and model the world event knowledge. Unfortunately, it is\nchallenging to automatically induce high-quality and high-coverage event\nschemas due to the open nature of real-world events, the diversity of event\nexpressions, and the sparsity of event knowledge. In this paper, we propose a\nnew paradigm for event schema induction -- knowledge harvesting from\nlarge-scale pre-trained language models, which can effectively resolve the\nabove challenges by discovering, conceptualizing and structuralizing event\nschemas from PLMs. And an Event Schema Harvester (ESHer) is designed to\nautomatically induce high-quality event schemas via in-context generation-based\nconceptualization, confidence-aware schema structuralization and graph-based\nschema aggregation. Empirical results show that ESHer can induce high-quality\nand high-coverage event schemas on varying domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jialong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuoqun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yaojie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-WikiTable: Dataset for Open Domain Question Answering with Complex Reasoning over Table. (arXiv:2305.07288v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07288","description":"<p>Despite recent interest in open domain question answering (ODQA) over tables,\nmany studies still rely on datasets that are not truly optimal for the task\nwith respect to utilizing structural nature of table. These datasets assume\nanswers reside as a single cell value and do not necessitate exploring over\nmultiple cells such as aggregation, comparison, and sorting. Thus, we release\nOpen-WikiTable, the first ODQA dataset that requires complex reasoning over\ntables. Open-WikiTable is built upon WikiSQL and WikiTableQuestions to be\napplicable in the open-domain setting. As each question is coupled with both\ntextual answers and SQL queries, Open-WikiTable opens up a wide range of\npossibilities for future research, as both reader and parser methods can be\napplied. The dataset and code are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kweon_S/0/1/0/all/0/1\">Sunjun Kweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Yeonsu Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Seonhee Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1\">Yohan Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RepCL: Exploring Effective Representation for Continual Text Classification. (arXiv:2305.07289v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07289","description":"<p>Continual learning (CL) aims to constantly learn new knowledge over time\nwhile avoiding catastrophic forgetting on old tasks. In this work, we focus on\ncontinual text classification under the class-incremental setting. Recent CL\nstudies find that the representations learned in one task may not be effective\nfor other tasks, namely representation bias problem. For the first time we\nformally analyze representation bias from an information bottleneck perspective\nand suggest that exploiting representations with more class-relevant\ninformation could alleviate the bias. To this end, we propose a novel\nreplay-based continual text classification method, RepCL. Our approach utilizes\ncontrastive and generative representation learning objectives to capture more\nclass-relevant features. In addition, RepCL introduces an adversarial replay\nstrategy to alleviate the overfitting problem of replay. Experiments\ndemonstrate that RepCL effectively alleviates forgetting and achieves\nstate-of-the-art performance on three text classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yifan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dawei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sujian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions. (arXiv:2305.07303v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07303","description":"<p>Neural-based word embeddings using solely distributional information have\nconsistently produced useful meaning representations for downstream tasks.\nHowever, existing approaches often result in representations that are hard to\ninterpret and control. Natural language definitions, on the other side, possess\na recursive, self-explanatory semantic structure that can support novel\nrepresentation learning paradigms able to preserve explicit conceptual\nrelations and constraints in the vector space.\n</p>\n<p>This paper proposes a neuro-symbolic, multi-relational framework to learn\nword embeddings exclusively from natural language definitions by jointly\nmapping defined and defining terms along with their corresponding semantic\nrelations. By automatically extracting the relations from definitions corpora\nand formalising the learning problem via a translational objective, we\nspecialise the framework in hyperbolic space to capture the hierarchical and\nmulti-resolution structure induced by the definitions. An extensive empirical\nanalysis demonstrates that the framework can help impose the desired structural\nconstraints while preserving the mapping required for controllable and\ninterpretable semantic navigation. Moreover, the experiments reveal the\nsuperiority of the hyperbolic word embeddings over the euclidean counterparts\nand demonstrate that the multi-relational framework can obtain competitive\nresults when compared to state-of-the-art neural approaches (including\nTransformers), with the advantage of being significantly more efficient and\nintrinsically interpretable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1\">Marco Valentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_D/0/1/0/all/0/1\">Danilo S. Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Zero-shot Multilingual Neural Machine Translation by Leveraging Cross-lingual Consistency Regularization. (arXiv:2305.07310v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07310","description":"<p>The multilingual neural machine translation (NMT) model has a promising\ncapability of zero-shot translation, where it could directly translate between\nlanguage pairs unseen during training. For good transfer performance from\nsupervised directions to zero-shot directions, the multilingual NMT model is\nexpected to learn universal representations across different languages. This\npaper introduces a cross-lingual consistency regularization, CrossConST, to\nbridge the representation gap among different languages and boost zero-shot\ntranslation performance. The theoretical analysis shows that CrossConST\nimplicitly maximizes the probability distribution for zero-shot translation,\nand the experimental results on both low-resource and high-resource benchmarks\nshow that CrossConST consistently improves the translation performance. The\nexperimental analysis also proves that CrossConST could close the sentence\nrepresentation gap and better align the representation space. Given the\nuniversality and simplicity of CrossConST, we believe it can serve as a strong\nbaseline for future multilingual NMT research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pengzhi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhongjun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MedGPTEval: A Dataset and Benchmark to Evaluate Responses of Large Language Models in Medicine. (arXiv:2305.07340v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07340","description":"<p>METHODS: First, a set of evaluation criteria is designed based on a\ncomprehensive literature review. Second, existing candidate criteria are\noptimized for using a Delphi method by five experts in medicine and\nengineering. Third, three clinical experts design a set of medical datasets to\ninteract with LLMs. Finally, benchmarking experiments are conducted on the\ndatasets. The responses generated by chatbots based on LLMs are recorded for\nblind evaluations by five licensed medical experts. RESULTS: The obtained\nevaluation criteria cover medical professional capabilities, social\ncomprehensive capabilities, contextual capabilities, and computational\nrobustness, with sixteen detailed indicators. The medical datasets include\ntwenty-seven medical dialogues and seven case reports in Chinese. Three\nchatbots are evaluated, ChatGPT by OpenAI, ERNIE Bot by Baidu Inc., and Doctor\nPuJiang (Dr. PJ) by Shanghai Artificial Intelligence Laboratory. Experimental\nresults show that Dr. PJ outperforms ChatGPT and ERNIE Bot in both\nmultiple-turn medical dialogue and case report scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Lu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_B/0/1/0/all/0/1\">Bilin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xinwei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiali Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jinru Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaoming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lingrui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Huan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model-based Programming: Redefining the Atomic Unit of Programming for the Deep Learning Era. (arXiv:2305.07341v1 [cs.LG])","link":"http://arxiv.org/abs/2305.07341","description":"<p>This paper introduces and explores a new programming paradigm, Model-based\nProgramming, designed to address the challenges inherent in applying deep\nlearning models to real-world applications. Despite recent significant\nsuccesses of deep learning models across a range of tasks, their deployment in\nreal business scenarios remains fraught with difficulties, such as complex\nmodel training, large computational resource requirements, and integration\nissues with existing programming languages. To ameliorate these challenges, we\npropose the concept of 'Model-based Programming' and present a novel\nprogramming language - M Language, tailored to a prospective model-centered\nprogramming paradigm. M Language treats models as basic computational units,\nenabling developers to concentrate more on crucial tasks such as model loading,\nfine-tuning, evaluation, and deployment, thereby enhancing the efficiency of\ncreating deep learning applications. We posit that this innovative programming\nparadigm will stimulate the extensive application and advancement of deep\nlearning technology and provide a robust foundation for a model-driven future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Meng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZARA: Improving Few-Shot Self-Rationalization for Small Language Models. (arXiv:2305.07355v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07355","description":"<p>Language models (LMs) that jointly generate end-task answers as well as\nfree-text rationales are known as self-rationalization models. Recent works\ndemonstrate great performance gain for self-rationalization by few-shot\nprompting LMs with rationale-augmented exemplars. However, the ability to\nbenefit from explanations only emerges with large-scale LMs, which have poor\naccessibility. In this work, we explore the less-studied setting of leveraging\nexplanations for small LMs to improve few-shot self-rationalization. We first\nrevisit the relationship between rationales and answers. Inspired by the\nimplicit mental process of how human beings assess explanations, we present a\nnovel approach, Zero-shot Augmentation of Rationale-Answer pairs (ZARA), to\nautomatically construct pseudo-parallel data for self-training by reducing the\nproblem of plausibility judgement to natural language inference. Experimental\nresults show ZARA achieves SOTA performance on the FEB benchmark, for both the\ntask accuracy and the explanation metric. In addition, we conduct human and\nquantitative evaluation validating ZARA's ability to automatically identify\nplausible and accurate rationale-answer pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Lin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_A/0/1/0/all/0/1\">An-Zi Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hen-Hsen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Cheng-Kuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hsin-Hsi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Versatile and Efficient Visual Knowledge Injection into Pre-trained Language Models with Cross-Modal Adapters. (arXiv:2305.07358v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07358","description":"<p>Humans learn language via multi-modal knowledge. However, due to the\ntext-only pre-training scheme, most existing pre-trained language models (PLMs)\nare hindered from the multi-modal information.\n</p>\n<p>To inject visual knowledge into PLMs, existing methods incorporate either the\ntext or image encoder of vision-language models (VLMs) to encode the visual\ninformation and update all the original parameters of PLMs for knowledge\nfusion.\n</p>\n<p>In this paper, we propose a new plug-and-play module, X-adapter, to flexibly\nleverage the aligned visual and textual knowledge learned in pre-trained VLMs\nand efficiently inject them into PLMs.\n</p>\n<p>Specifically, we insert X-adapters into PLMs, and only the added parameters\nare updated during adaptation.\n</p>\n<p>To fully exploit the potential in VLMs, X-adapters consist of two\nsub-modules, V-expert and T-expert, to fuse VLMs' image and text\nrepresentations, respectively.\n</p>\n<p>We can opt for activating different sub-modules depending on the downstream\ntasks.\n</p>\n<p>Experimental results show that our method can significantly improve the\nperformance on object-color reasoning and natural language understanding (NLU)\ntasks compared with PLM baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Haochen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_M/0/1/0/all/0/1\">Mingjie Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bei Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Quality of Neural Machine Translation Through Proper Translation of Name Entities. (arXiv:2305.07360v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07360","description":"<p>In this paper, we have shown a method of improving the quality of neural\nmachine translation by translating/transliterating name entities as a\npreprocessing step. Through experiments we have shown the performance gain of\nour system. For evaluation we considered three types of name entities viz\nperson names, location names and organization names. The system was able to\ncorrectly translate mostly all the name entities. For person names the accuracy\nwas 99.86%, for location names the accuracy was 99.63% and for organization\nnames the accuracy was 99.05%. Overall, the accuracy of the system was 99.52%\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Radhika Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katyayan_P/0/1/0/all/0/1\">Pragya Katyayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Nisheeth Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Transliteration between Sindhi Scripts from Devanagari to Perso-Arabic. (arXiv:2305.07365v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07365","description":"<p>In this paper, we have shown a script conversion (transliteration) technique\nthat converts Sindhi text in the Devanagari script to the Perso-Arabic script.\nWe showed this by incorporating a hybrid approach where some part of the text\nis converted using a rule base and in case an ambiguity arises then a\nprobabilistic model is used to resolve the same. Using this approach, the\nsystem achieved an overall accuracy of 99.64%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rathore_S/0/1/0/all/0/1\">Shivani Singh Rathore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nathani_B/0/1/0/all/0/1\">Bharti Nathani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Nisheeth Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katyayan_P/0/1/0/all/0/1\">Pragya Katyayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dadlani_C/0/1/0/all/0/1\">Chander Prakash Dadlani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Text-to-SQL Generation via Editable Step-by-Step Explanations. (arXiv:2305.07372v1 [cs.DB])","link":"http://arxiv.org/abs/2305.07372","description":"<p>Relational databases play an important role in this Big Data era. However, it\nis challenging for non-experts to fully unleash the analytical power of\nrelational databases, since they are not familiar with database languages such\nas SQL. Many techniques have been proposed to automatically generate SQL from\nnatural language, but they suffer from two issues: (1) they still make many\nmistakes, particularly for complex queries, and (2) they do not provide a\nflexible way for non-expert users to validate and refine the incorrect queries.\nTo address these issues, we introduce a new interaction mechanism that allows\nusers directly edit a step-by-step explanation of an incorrect SQL to fix SQL\nerrors. Experiments on the Spider benchmark show that our approach outperforms\nthree SOTA approaches by at least 31.6% in terms of execution accuracy. A user\nstudy with 24 participants further shows that our approach helped users solve\nsignificantly more SQL tasks with less time and higher confidence,\ndemonstrating its potential to expand access to databases, particularly for\nnon-experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Toby Jia-Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kummerfeld_J/0/1/0/all/0/1\">Jonathan K. Kummerfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implications of Deep Circuits in Improving Quality of Quantum Question Answering. (arXiv:2305.07374v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07374","description":"<p>Question Answering (QA) has proved to be an arduous challenge in the area of\nnatural language processing (NLP) and artificial intelligence (AI). Many\nattempts have been made to develop complete solutions for QA as well as\nimproving significant sub-modules of the QA systems to improve the overall\nperformance through the course of time. Questions are the most important piece\nof QA, because knowing the question is equivalent to knowing what counts as an\nanswer (Harrah in Philos Sci, 1961 [1]). In this work, we have attempted to\nunderstand questions in a better way by using Quantum Machine Learning (QML).\nThe properties of Quantum Computing (QC) have enabled classically intractable\ndata processing. So, in this paper, we have performed question classification\non questions from two classes of SelQA (Selection-based Question Answering)\ndataset using quantum-based classifier algorithms-quantum support vector\nmachine (QSVM) and variational quantum classifier (VQC) from Qiskit (Quantum\nInformation Science toolKIT) for Python. We perform classification with both\nclassifiers in almost similar environments and study the effects of circuit\ndepths while comparing the results of both classifiers. We also use these\nclassification results with our own rule-based QA system and observe\nsignificant performance improvement. Hence, this experiment has helped in\nimproving the quality of QA in general.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katyayan_P/0/1/0/all/0/1\">Pragya Katyayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Nisheeth Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation. (arXiv:2305.07375v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07375","description":"<p>Causal reasoning ability is crucial for numerous NLP applications. Despite\nthe impressive emerging ability of ChatGPT in various NLP tasks, it is unclear\nhow well ChatGPT performs in causal reasoning. In this paper, we conduct the\nfirst comprehensive evaluation of the ChatGPT's causal reasoning capabilities.\nExperiments show that ChatGPT is not a good causal reasoner, but a good causal\ninterpreter. Besides, ChatGPT has a serious hallucination on causal reasoning,\npossibly due to the reporting biases between causal and non-causal\nrelationships in natural language, as well as ChatGPT's upgrading processes,\nsuch as RLHF. The In-Context Learning (ICL) and Chain-of-Though (COT)\ntechniques can further exacerbate such causal hallucination. Additionally, the\ncausal reasoning ability of ChatGPT is sensitive to the words used to express\nthe causal concept in prompts, and close-ended prompts perform better than\nopen-ended prompts. For events in sentences, ChatGPT excels at capturing\nexplicit causality rather than implicit causality, and performs better in\nsentences with lower event density and smaller lexical distance between events.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jinglong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surfacing Biases in Large Language Models using Contrastive Input Decoding. (arXiv:2305.07378v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07378","description":"<p>Ensuring that large language models (LMs) are fair, robust and useful\nrequires an understanding of how different modifications to their inputs impact\nthe model's behaviour. In the context of open-text generation tasks, however,\nsuch an evaluation is not trivial. For example, when introducing a model with\nan input text and a perturbed, \"contrastive\" version of it, meaningful\ndifferences in the next-token predictions may not be revealed with standard\ndecoding strategies. With this motivation in mind, we propose Contrastive Input\nDecoding (CID): a decoding algorithm to generate text given two inputs, where\nthe generated text is likely given one input but unlikely given the other. In\nthis way, the contrastive generations can highlight potentially subtle\ndifferences in how the LM output differs for the two inputs in a simple and\ninterpretable manner. We use CID to highlight context-specific biases that are\nhard to detect with standard decoding strategies and quantify the effect of\ndifferent input perturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yona_G/0/1/0/all/0/1\">Gal Yona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honovich_O/0/1/0/all/0/1\">Or Honovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laish_I/0/1/0/all/0/1\">Itay Laish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1\">Roee Aharoni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Sensitivity of Automatic Speech Recognition Systems to Phonetic Variation in L2 Englishes. (arXiv:2305.07389v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07389","description":"<p>Automatic Speech Recognition (ASR) systems exhibit the best performance on\nspeech that is similar to that on which it was trained. As such,\nunderrepresented varieties including regional dialects, minority-speakers, and\nlow-resource languages, see much higher word error rates (WERs) than those\nvarieties seen as 'prestigious', 'mainstream', or 'standard'. This can act as a\nbarrier to incorporating ASR technology into the annotation process for\nlarge-scale linguistic research since the manual correction of the erroneous\nautomated transcripts can be just as time and resource consuming as manual\ntranscriptions. A deeper understanding of the behaviour of an ASR system is\nthus beneficial from a speech technology standpoint, in terms of improving ASR\naccuracy, and from an annotation standpoint, where knowing the likely errors\nmade by an ASR system can aid in this manual correction. This work demonstrates\na method of probing an ASR system to discover how it handles phonetic variation\nacross a number of L2 Englishes. Specifically, how particular phonetic\nrealisations which were rare or absent in the system's training data can lead\nto phoneme level misrecognitions and contribute to higher WERs. It is\ndemonstrated that the behaviour of the ASR is systematic and consistent across\nspeakers with similar spoken varieties (in this case the same L1) and phoneme\nsubstitution errors are typically in agreement with human annotators. By\nidentifying problematic productions specific weaknesses can be addressed by\nsourcing such realisations for training and fine-tuning thus making the system\nmore robust to pronunciation variation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+ONeill_E/0/1/0/all/0/1\">Emma O&#x27;Neill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carson_Berndsen_J/0/1/0/all/0/1\">Julie Carson-Berndsen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt Learning to Mitigate Catastrophic Forgetting in Cross-lingual Transfer for Open-domain Dialogue Generation. (arXiv:2305.07393v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07393","description":"<p>Dialogue systems for non-English languages have long been under-explored. In\nthis paper, we take the first step to investigate few-shot cross-lingual\ntransfer learning (FS-XLT) and multitask learning (MTL) in the context of\nopen-domain dialogue generation for non-English languages with limited data. We\nobserved catastrophic forgetting in both FS-XLT and MTL for all 6 languages in\nour preliminary experiments. To mitigate the issue, we propose a simple yet\neffective prompt learning approach that can preserve the multilinguality of\nmultilingual pre-trained language model (mPLM) in FS-XLT and MTL by bridging\nthe gap between pre-training and fine-tuning with Fixed-prompt LM Tuning and\nour hand-crafted prompts. Experimental results on all 6 languages in terms of\nboth automatic and human evaluations demonstrate the effectiveness of our\napproach. Our code is available at https://github.com/JeremyLeiLiu/XLinguDial.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jimmy Xiangji Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Refinement via Interaction Between Search Engines and Large Language Models. (arXiv:2305.07402v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07402","description":"<p>Information retrieval (IR) plays a crucial role in locating relevant\nresources from vast amounts of data, and its applications have evolved from\ntraditional knowledge bases to modern search engines (SEs). The emergence of\nlarge language models (LLMs) has further revolutionized the field by enabling\nusers to interact with search systems in natural language. In this paper, we\nexplore the advantages and disadvantages of LLMs and SEs, highlighting their\nrespective strengths in understanding user-issued queries and retrieving\nup-to-date information. To leverage the benefits of both paradigms while\ncircumventing their limitations, we propose InteR, a novel framework that\nfacilitates knowledge refinement through interaction between SEs and LLMs.\nInteR allows SEs to refine knowledge in query using LLM-generated summaries and\nenables LLMs to enhance prompts using SE-retrieved documents. This iterative\nrefinement process augments the inputs of SEs and LLMs, leading to more\naccurate retrieval. Experimental evaluations on two large-scale retrieval\nbenchmarks demonstrate that InteR achieves superior zero-shot document\nretrieval performance compared to state-of-the-art methods, regardless of the\nuse of relevance judgement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiazhan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-in-One: A Model Hijacking Attack Against Text Generation Models. (arXiv:2305.07406v1 [cs.CR])","link":"http://arxiv.org/abs/2305.07406","description":"<p>Machine learning has progressed significantly in various applications ranging\nfrom face recognition to text generation. However, its success has been\naccompanied by different attacks. Recently a new attack has been proposed which\nraises both accountability and parasitic computing risks, namely the model\nhijacking attack. Nevertheless, this attack has only focused on image\nclassification tasks. In this work, we broaden the scope of this attack to\ninclude text generation and classification models, hence showing its broader\napplicability. More concretely, we propose a new model hijacking attack, Ditto,\nthat can hijack different text classification tasks into multiple generation\nones, e.g., language translation, text summarization, and language modeling. We\nuse a range of text benchmark datasets such as SST-2, TweetEval, AGnews, QNLI,\nand IMDB to evaluate the performance of our attacks. Our results show that by\nusing Ditto, an adversary can successfully hijack text generation models\nwithout jeopardizing their utility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_W/0/1/0/all/0/1\">Wai Man Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Backes_M/0/1/0/all/0/1\">Michael Backes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salem_A/0/1/0/all/0/1\">Ahmed Salem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance Smoothed Contrastive Learning for Unsupervised Sentence Embedding. (arXiv:2305.07424v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07424","description":"<p>Contrastive learning-based methods, such as unsup-SimCSE, have achieved\nstate-of-the-art (SOTA) performances in learning unsupervised sentence\nembeddings. However, in previous studies, each embedding used for contrastive\nlearning only derived from one sentence instance, and we call these embeddings\ninstance-level embeddings. In other words, each embedding is regarded as a\nunique class of its own, whichmay hurt the generalization performance. In this\nstudy, we propose IS-CSE (instance smoothing contrastive sentence embedding) to\nsmooth the boundaries of embeddings in the feature space. Specifically, we\nretrieve embeddings from a dynamic memory buffer according to the semantic\nsimilarity to get a positive embedding group. Then embeddings in the group are\naggregated by a self-attention operation to produce a smoothed instance\nembedding for further analysis. We evaluate our method on standard semantic\ntext similarity (STS) tasks and achieve an average of 78.30%, 79.47%, 77.73%,\nand 79.42% Spearman's correlation on the base of BERT-base, BERT-large,\nRoBERTa-base, and RoBERTa-large respectively, a 2.05%, 1.06%, 1.16% and 0.52%\nimprovement compared to unsup-SimCSE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+zhang_J/0/1/0/all/0/1\">Junlei zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1\">Zhenzhong Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QVoice: Arabic Speech Pronunciation Learning Application. (arXiv:2305.07445v1 [eess.AS])","link":"http://arxiv.org/abs/2305.07445","description":"<p>This paper introduces a novel Arabic pronunciation learning application\nQVoice, powered with end-to-end mispronunciation detection and feedback\ngenerator module. The application is designed to support non-native Arabic\nspeakers in enhancing their pronunciation skills, while also helping native\nspeakers mitigate any potential influence from regional dialects on their\nModern Standard Arabic (MSA) pronunciation. QVoice employs various learning\ncues to aid learners in comprehending meaning, drawing connections with their\nexisting knowledge of English language, and offers detailed feedback for\npronunciation correction, along with contextual examples showcasing word usage.\nThe learning cues featured in QVoice encompass a wide range of meaningful\ninformation, such as visualizations of phrases/words and their translations, as\nwell as phonetic transcriptions and transliterations. QVoice provides\npronunciation feedback at the character level and assesses performance at the\nword level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kheir_Y/0/1/0/all/0/1\">Yassine El Kheir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khnaisser_F/0/1/0/all/0/1\">Fouad Khnaisser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Afzal_S/0/1/0/all/0/1\">Shazia Afzal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation. (arXiv:2305.07455v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07455","description":"<p>Most of the speech translation models heavily rely on parallel data, which is\nhard to collect especially for low-resource languages. To tackle this issue, we\npropose to build a cascaded speech translation system without leveraging any\nkind of paired data. We use fully unpaired data to train our unsupervised\nsystems and evaluate our results on CoVoST 2 and CVSS. The results show that\nour work is comparable with some other early supervised methods in some\nlanguage pairs. While cascaded systems always suffer from severe error\npropagation problems, we proposed denoising back-translation (DBT), a novel\napproach to building robust unsupervised neural machine translation (UNMT). DBT\nsuccessfully increases the BLEU score by 0.7--0.9 in all three translation\ndirections. Moreover, we simplified the pipeline of our cascaded system to\nreduce inference latency and conducted a comprehensive analysis of every part\nof our work. We also demonstrate our unsupervised speech translation results on\nthe established website.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yu-Kuan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_L/0/1/0/all/0/1\">Liang-Hsuan Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen-An Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_T/0/1/0/all/0/1\">Tsu-Yuan Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perturbation-based QE: An Explainable, Unsupervised Word-level Quality Estimation Method for Blackbox Machine Translation. (arXiv:2305.07457v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07457","description":"<p>Quality Estimation (QE) is the task of predicting the quality of Machine\nTranslation (MT) system output, without using any gold-standard translation\nreferences. State-of-the-art QE models are supervised: they require\nhuman-labeled quality of some MT system output on some datasets for training,\nmaking them domain-dependent and MT-system-dependent. There has been research\non unsupervised QE, which requires glass-box access to the MT systems, or\nparallel MT data to generate synthetic errors for training QE models. In this\npaper, we present Perturbation-based QE - a word-level Quality Estimation\napproach that works simply by analyzing MT system output on perturbed input\nsource sentences. Our approach is unsupervised, explainable, and can evaluate\nany type of blackbox MT systems, including the currently prominent large\nlanguage models (LLMs) with opaque internal processes. For language directions\nwith no labeled QE data, our approach has similar or better performance than\nthe zero-shot supervised approach on the WMT21 shared task. Our approach is\nbetter at detecting gender bias and word-sense-disambiguation errors in\ntranslation than supervised QE, indicating its robustness to out-of-domain\nusage. The performance gap is larger when detecting errors on a nontraditional\ntranslation-prompting LLM, indicating that our approach is more generalizable\nto different MT systems. We give examples demonstrating our approach's\nexplainability power, where it shows which input source words have influence on\na certain MT output word.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1\">Tu Anh Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1\">Jan Niehues</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BactInt: A domain driven transfer learning approach and a corpus for extracting inter-bacterial interactions from biomedical text. (arXiv:2305.07468v1 [cs.IR])","link":"http://arxiv.org/abs/2305.07468","description":"<p>The community of different types of microbes present in a biological niche\nplays a very important role in functioning of the system. The crosstalk or\ninteractions among the different microbes contributes to the building blocks of\nsuch microbial community structures. Evidence reported in biomedical text\nserves as a reliable source for predicting such interactions. However, going\nthrough the vast and ever-increasing volume of biomedical literature is an\nintimidating and time consuming process. This necessitates development of\nautomated methods capable of accurately extracting bacterial relations reported\nin biomedical literature. In this paper, we introduce a method for automated\nextraction of microbial interactions (specifically between bacteria) from\nbiomedical literature along with ways of using transfer learning to improve its\naccuracy. We also describe a pipeline using which relations among specific\nbacteria groups can be mined. Additionally, we introduce the first publicly\navailable dataset which can be used to develop bacterial interaction extraction\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baksi_K/0/1/0/all/0/1\">Krishanu Das Baksi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pokhrel_V/0/1/0/all/0/1\">Vatsala Pokhrel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhusan_K/0/1/0/all/0/1\">Kuntal Kumar Bhusan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mande_S/0/1/0/all/0/1\">Sharmila Mande</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning. (arXiv:2305.07475v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07475","description":"<p>Numerical reasoning over table-and-text hybrid passages, such as financial\nreports, poses significant challenges and has numerous potential applications.\nNoise and irrelevant variables in the model input have been a hindrance to its\nperformance. Additionally, coarse-grained supervision of the whole solution\nprogram has impeded the model's ability to learn the underlying numerical\nreasoning process. In this paper, we propose three pretraining tasks that\noperate at both the whole program and sub-program level: Variable Integrity\nRanking, which guides the model to focus on useful variables; Variable Operator\nPrediction, which decomposes the supervision into fine-grained single operator\nprediction; and Variable Keyphrase Masking, which encourages the model to\nidentify key evidence that sub-programs are derived from. Experimental results\ndemonstrate the effectiveness of our proposed methods, surpassing\ntransformer-based model baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qianying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dongsheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wenjie Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1\">Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1\">Sadao Kurohashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4. (arXiv:2305.07490v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07490","description":"<p>In recent years, large language models (LLMs) have made significant progress\nin natural language processing (NLP), with models like ChatGPT and GPT-4\nachieving impressive capabilities in various linguistic tasks. However,\ntraining models on such a large scale is challenging, and finding datasets that\nmatch the model's scale is often difficult. Fine-tuning and training models\nwith fewer parameters using novel methods have emerged as promising approaches\nto overcome these challenges. One such model is MiniGPT-4, which achieves\ncomparable vision-language understanding to GPT-4 by leveraging novel\npre-training models and innovative training strategies. However, the model\nstill faces some challenges in image understanding, particularly in artistic\npictures. A novel multimodal model called ArtGPT-4 has been proposed to address\nthese limitations. ArtGPT-4 was trained on image-text pairs using a Tesla A100\ndevice in just 2 hours, using only about 200 GB of data. The model can depict\nimages with an artistic flair and generate visual code, including aesthetically\npleasing HTML/CSS web pages. Furthermore, the article proposes novel benchmarks\nfor evaluating the performance of vision-language models. In the subsequent\nevaluation methods, ArtGPT-4 scored more than 1 point higher than the current\n\\textbf{state-of-the-art} model and was only 0.25 points lower than artists on\na 6-point scale. Our code and pre-trained model are available at\n\\url{https://huggingface.co/Tyrannosaurus/ArtGPT-4}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhengqing Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Huiwen Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhuanzhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Analysis of Adapter Efficiency. (arXiv:2305.07491v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07491","description":"<p>Adapters have been positioned as a parameter-efficient fine-tuning (PEFT)\napproach, whereby a minimal number of parameters are added to the model and\nfine-tuned. However, adapters have not been sufficiently analyzed to understand\nif PEFT translates to benefits in training/deployment efficiency and\nmaintainability/extensibility. Through extensive experiments on many adapters,\ntasks, and languages in supervised and cross-lingual zero-shot settings, we\nclearly show that for Natural Language Understanding (NLU) tasks, the parameter\nefficiency in adapters does not translate to efficiency gains compared to full\nfine-tuning of models. More precisely, adapters are relatively expensive to\ntrain and have slightly higher deployment latency. Furthermore, the\nmaintainability/extensibility benefits of adapters can be achieved with simpler\napproaches like multi-task training via full fine-tuning, which also provide\nrelatively faster training times. We, therefore, recommend that for moderately\nsized models for NLU tasks, practitioners should rely on full fine-tuning or\nmulti-task training rather than using adapters. Our code is available at\nhttps://github.com/AI4Bharat/adapter-efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mundra_N/0/1/0/all/0/1\">Nandini Mundra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doddapaneni_S/0/1/0/all/0/1\">Sumanth Doddapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puduppully_R/0/1/0/all/0/1\">Ratish Puduppully</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development. (arXiv:2305.07507v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07507","description":"<p>In this work, we conduct a detailed analysis on the performance of\nlegal-oriented pre-trained language models (PLMs). We examine the interplay\nbetween their original objective, acquired knowledge, and legal language\nunderstanding capacities which we define as the upstream, probing, and\ndownstream performance, respectively. We consider not only the models' size but\nalso the pre-training corpora used as important dimensions in our study. To\nthis end, we release a multinational English legal corpus (LeXFiles) and a\nlegal knowledge probing benchmark (LegalLAMA) to facilitate training and\ndetailed analysis of legal-oriented PLMs. We release two new legal PLMs trained\non LeXFiles and evaluate them alongside others on LegalLAMA and LexGLUE. We\nfind that probing performance strongly correlates with upstream performance in\nrelated legal topics. On the other hand, downstream performance is mainly\ndriven by the model's size and prior legal knowledge which can be estimated by\nupstream and probing performance. Based on these findings, we can conclude that\nboth dimensions are important for those seeking the development of\ndomain-specific PLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garneau_N/0/1/0/all/0/1\">Nicolas Garneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goanta_C/0/1/0/all/0/1\">Catalina Goanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_D/0/1/0/all/0/1\">Daniel Martin Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Progress in Fine-grained Vision-and-Language Understanding. (arXiv:2305.07558v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07558","description":"<p>While pretraining on large-scale image-text data from the Web has facilitated\nrapid progress on many vision-and-language (V&amp;L) tasks, recent work has\ndemonstrated that pretrained models lack \"fine-grained\" understanding, such as\nthe ability to recognise relationships, verbs, and numbers in images. This has\nresulted in an increased interest in the community to either develop new\nbenchmarks or models for such capabilities. To better understand and quantify\nprogress in this direction, we investigate four competitive V&amp;L models on four\nfine-grained benchmarks. Through our analysis, we find that X-VLM (Zeng et al.,\n2022) consistently outperforms other baselines, and that modelling innovations\ncan impact performance more than scaling Web data, which even degrades\nperformance sometimes. Through a deeper investigation of X-VLM, we highlight\nthe importance of both novel losses and rich data sources for learning\nfine-grained skills. Finally, we inspect training dynamics, and discover that\nfor some tasks, performance peaks early in training or significantly\nfluctuates, never converging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1\">Emanuele Bugliarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sartran_L/0/1/0/all/0/1\">Laurent Sartran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Aishwarya Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendricks_L/0/1/0/all/0/1\">Lisa Anne Hendricks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1\">Aida Nematzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Memory Model for Question Answering from Streaming Data Supported by Rehearsal and Anticipation of Coreference Information. (arXiv:2305.07565v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07565","description":"<p>Existing question answering methods often assume that the input content\n(e.g., documents or videos) is always accessible to solve the task.\nAlternatively, memory networks were introduced to mimic the human process of\nincremental comprehension and compression of the information in a\nfixed-capacity memory. However, these models only learn how to maintain memory\nby backpropagating errors in the answers through the entire network. Instead,\nit has been suggested that humans have effective mechanisms to boost their\nmemorization capacities, such as rehearsal and anticipation. Drawing\ninspiration from these, we propose a memory model that performs rehearsal and\nanticipation while processing inputs to memorize important information for\nsolving question answering tasks from streaming data. The proposed mechanisms\nare applied self-supervised during training through masked modeling tasks\nfocused on coreference information. We validate our model on a short-sequence\n(bAbI) dataset as well as large-sequence textual (NarrativeQA) and video\n(ActivityNet-QA) question answering datasets, where it achieves substantial\nimprovements over previous memory network approaches. Furthermore, our ablation\nstudy confirms the proposed mechanisms' importance for memory models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Araujo_V/0/1/0/all/0/1\">Vladimir Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1\">Alvaro Soto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation. (arXiv:2305.07609v1 [cs.IR])","link":"http://arxiv.org/abs/2305.07609","description":"<p>The remarkable achievements of Large Language Models (LLMs) have led to the\nemergence of a novel recommendation paradigm -- Recommendation via LLM\n(RecLLM). Nevertheless, it is important to note that LLMs may contain social\nprejudices, and therefore, the fairness of recommendations made by RecLLM\nrequires further investigation. To avoid the potential risks of RecLLM, it is\nimperative to evaluate the fairness of RecLLM with respect to various sensitive\nattributes on the user side. Due to the differences between the RecLLM paradigm\nand the traditional recommendation paradigm, it is problematic to directly use\nthe fairness benchmark of traditional recommendation. To address the dilemma,\nwe propose a novel benchmark called Fairness of Recommendation via LLM\n(FaiRLLM). This benchmark comprises carefully crafted metrics and a dataset\nthat accounts for eight sensitive attributes1 in two recommendation scenarios:\nmusic and movies. By utilizing our FaiRLLM benchmark, we conducted an\nevaluation of ChatGPT and discovered that it still exhibits unfairness to some\nsensitive attributes when generating recommendations. Our code and dataset can\nbe found at https://github.com/jizhi-zhang/FaiRLLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jizhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_K/0/1/0/all/0/1\">Keqin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fuli Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Sentiment Analysis: A Survey. (arXiv:2305.07611v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07611","description":"<p>Multimodal sentiment analysis has become an important research area in the\nfield of artificial intelligence. With the latest advances in deep learning,\nthis technology has reached new heights. It has great potential for both\napplication and research, making it a popular research topic. This review\nprovides an overview of the definition, background, and development of\nmultimodal sentiment analysis. It also covers recent datasets and advanced\nmodels, emphasizing the challenges and future prospects of this technology.\nFinally, it looks ahead to future research directions. It should be noted that\nthis review provides constructive suggestions for promising research directions\nand building better performing multimodal sentiment analysis models, which can\nhelp researchers in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Songning Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoxuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xifeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaoxia Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NevIR: Negation in Neural Information Retrieval. (arXiv:2305.07614v1 [cs.IR])","link":"http://arxiv.org/abs/2305.07614","description":"<p>Negation is a common everyday phenomena and has been a consistent area of\nweakness for language models (LMs). Although the Information Retrieval (IR)\ncommunity has adopted LMs as the backbone of modern IR architectures, there has\nbeen little to no research in understanding how negation impacts neural IR. We\ntherefore construct a straightforward benchmark on this theme: asking IR models\nto rank two documents that differ only by negation. We show that the results\nvary widely according to the type of IR architecture: cross-encoders perform\nbest, followed by late-interaction models, and in last place are bi-encoder and\nsparse neural architectures. We find that most current information retrieval\nmodels do not consider negation, performing similarly or worse than randomly\nranking. We show that although the obvious approach of continued fine-tuning on\na dataset of contrastive documents containing negations increases performance\n(as does model size), there is still a large gap between machine and human\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weller_O/0/1/0/all/0/1\">Orion Weller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawrie_D/0/1/0/all/0/1\">Dawn Lawrie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What are the Desired Characteristics of Calibration Sets? Identifying Correlates on Long Form Scientific Summarization. (arXiv:2305.07615v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07615","description":"<p>Summarization models often generate text that is poorly calibrated to quality\nmetrics because they are trained to maximize the likelihood of a single\nreference (MLE). To address this, recent work has added a calibration step,\nwhich exposes a model to its own ranked outputs to improve relevance or, in a\nseparate line of work, contrasts positive and negative sets to improve\nfaithfulness. While effective, much of this work has focused on how to generate\nand optimize these sets. Less is known about why one setup is more effective\nthan another. In this work, we uncover the underlying characteristics of\neffective sets. For each training instance, we form a large, diverse pool of\ncandidates and systematically vary the subsets used for calibration\nfine-tuning. Each selection strategy targets distinct aspects of the sets, such\nas lexical diversity or the size of the gap between positive and negatives. On\nthree diverse scientific long-form summarization datasets (spanning biomedical,\nclinical, and chemical domains), we find, among others, that faithfulness\ncalibration is optimal when the negative sets are extractive and more likely to\nbe generated, whereas for relevance calibration, the metric margin between\ncandidates should be maximized and surprise--the disagreement between model and\nmetric defined candidate rankings--minimized. Code to create, select, and\noptimize calibration sets is available at\nhttps://github.com/griff4692/calibrating-summaries\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adams_G/0/1/0/all/0/1\">Griffin Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Bichlien H Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">Jake Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yingce Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shufang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostropolets_A/0/1/0/all/0/1\">Anna Ostropolets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deb_B/0/1/0/all/0/1\">Budhaditya Deb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuan-Jyue Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhadad_N/0/1/0/all/0/1\">No&#xe9;mie Elhadad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PALR: Personalization Aware LLMs for Recommendation. (arXiv:2305.07622v1 [cs.IR])","link":"http://arxiv.org/abs/2305.07622","description":"<p>Large language models (LLMs) have recently received significant attention for\ntheir exceptional capabilities. Despite extensive efforts in developing\ngeneral-purpose LLMs that can be utilized in various natural language\nprocessing (NLP) tasks, there has been less research exploring their potential\nin recommender systems. In this paper, we propose a novel framework, named\nPALR, which aiming to combine user history behaviors (such as clicks,\npurchases, ratings, etc.) with LLMs to generate user preferred items.\nSpecifically, we first use user/item interactions as guidance for candidate\nretrieval. Then we adopt a LLM-based ranking model to generate recommended\nitems. Unlike existing approaches that typically adopt general-purpose LLMs for\nzero/few-shot recommendation testing or training on small-sized language models\n(with less than 1 billion parameters), which cannot fully elicit LLMs'\nreasoning abilities and leverage rich item side parametric knowledge, we\nfine-tune a 7 billion parameters LLM for the ranking purpose. This model takes\nretrieval candidates in natural language format as input, with instruction\nwhich explicitly asking to select results from input candidates during\ninference. Our experimental results demonstrate that our solution outperforms\nstate-of-the-art models on various sequential recommendation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zheng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text2Cohort: Democratizing the NCI Imaging Data Commons with Natural Language Cohort Discovery. (arXiv:2305.07637v1 [cs.LG])","link":"http://arxiv.org/abs/2305.07637","description":"<p>The Imaging Data Commons (IDC) is a cloud-based database that provides\nresearchers with open access to cancer imaging data and tools for analysis,\nwith the goal of facilitating collaboration in medical imaging research.\nHowever, querying the IDC database for cohort discovery and access to imaging\ndata has a significant learning curve for researchers due to its complex and\ntechnical nature. We developed Text2Cohort, a large language model (LLM) based\ntoolkit to facilitate natural language cohort discovery by translating user\ninput into IDC database queries through prompt engineering and returning the\nquery's response to the user. Furthermore, autocorrection is implemented to\nresolve syntax and semantic errors in queries by passing the errors back to the\nmodel for interpretation and correction. We evaluate Text2Cohort on 50 natural\nlanguage user inputs ranging from information extraction to cohort discovery.\nThe resulting queries and outputs were verified by two computer scientists to\nmeasure Text2Cohort's accuracy and F1 score. Text2Cohort successfully generated\nqueries and their responses with an 88% accuracy and F1 score of 0.94. However,\nit failed to generate queries for six user inputs due to syntax and semantic\nerrors. Our results indicate that Text2Cohort succeeded at generating queries\nwith correct responses, but occasionally failed due to a poor understanding of\nthe data schema. Despite these shortcomings, Text2Cohort demonstrates the\nutility of LLMs to enable researchers to discover and curate cohorts using data\nhosted on IDC with incredible accuracy using natural language in a more\nintuitive and user-friendly way, thus democratizing access to the IDC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_P/0/1/0/all/0/1\">Pranav Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanhere_A/0/1/0/all/0/1\">Adway Kanhere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_P/0/1/0/all/0/1\">Paul H. Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_V/0/1/0/all/0/1\">Vishwa S. Parekh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A decomposition of book structure through ousiometric fluctuations in cumulative word-time. (arXiv:2208.09496v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.09496","description":"<p>While quantitative methods have been used to examine changes in word usage in\nbooks, studies have focused on overall trends, such as the shapes of\nnarratives, which are independent of book length. We instead look at how words\nchange over the course of a book as a function of the number of words, rather\nthan the fraction of the book, completed at any given point; we define this\nmeasure as \"cumulative word-time\". Using ousiometrics, a reinterpretation of\nthe valence-arousal-dominance framework of meaning obtained from semantic\ndifferentials, we convert text into time series of power and danger scores in\ncumulative word-time. Each time series is then decomposed using empirical mode\ndecomposition into a sum of constituent oscillatory modes and a non-oscillatory\ntrend. By comparing the decomposition of the original power and danger time\nseries with those derived from shuffled text, we find that shorter books\nexhibit only a general trend, while longer books have fluctuations in addition\nto the general trend. These fluctuations typically have a period of a few\nthousand words regardless of the book length or library classification code,\nbut vary depending on the content and structure of the book. Our findings\nsuggest that, in the ousiometric sense, longer books are not expanded versions\nof shorter books, but are more similar in structure to a concatenation of\nshorter texts. Further, they are consistent with editorial practices that\nrequire longer texts to be broken down into sections, such as chapters. Our\nmethod also provides a data-driven denoising approach that works for texts of\nvarious lengths, in contrast to the more traditional approach of using large\nwindow sizes that may inadvertently smooth out relevant information, especially\nfor shorter texts. These results open up avenues for future work in\ncomputational literary analysis, particularly the measurement of a basic unit\nof narrative.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fudolig_M/0/1/0/all/0/1\">Mikaela Irene Fudolig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshaabi_T/0/1/0/all/0/1\">Thayer Alshaabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cramer_K/0/1/0/all/0/1\">Kathryn Cramer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danforth_C/0/1/0/all/0/1\">Christopher M. Danforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodds_P/0/1/0/all/0/1\">Peter Sheridan Dodds</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discourse Analysis via Questions and Answers: Parsing Dependency Structures of Questions Under Discussion. (arXiv:2210.05905v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05905","description":"<p>Automatic discourse processing is bottlenecked by data: current discourse\nformalisms pose highly demanding annotation tasks involving large taxonomies of\ndiscourse relations, making them inaccessible to lay annotators. This work\ninstead adopts the linguistic framework of Questions Under Discussion (QUD) for\ndiscourse analysis and seeks to derive QUD structures automatically. QUD views\neach sentence as an answer to a question triggered in prior context; thus, we\ncharacterize relationships between sentences as free-form questions, in\ncontrast to exhaustive fine-grained taxonomies. We develop the\nfirst-of-its-kind QUD parser that derives a dependency structure of questions\nover full documents, trained using a large, crowdsourced question-answering\ndataset DCQA (Ko et al., 2022). Human evaluation results show that QUD\ndependency parsing is possible for language models trained with this\ncrowdsourced, generalizable annotation scheme. We illustrate how our QUD\nstructure is distinct from RST trees, and demonstrate the utility of QUD\nanalysis in the context of document simplification. Our findings show that QUD\nparsing is an appealing alternative for automatic discourse processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ko_W/0/1/0/all/0/1\">Wei-Jen Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yating Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalton_C/0/1/0/all/0/1\">Cutter Dalton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivas_D/0/1/0/all/0/1\">Dananjay Srinivas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tuning Language Models as Training Data Generators for Augmentation-Enhanced Few-Shot Learning. (arXiv:2211.03044v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.03044","description":"<p>Recent studies have revealed the intriguing few-shot learning ability of\npretrained language models (PLMs): They can quickly adapt to a new task when\nfine-tuned on a small amount of labeled data formulated as prompts, without\nrequiring abundant task-specific annotations. Despite their promising\nperformance, most existing few-shot approaches that only learn from the small\ntraining set still underperform fully supervised training by nontrivial\nmargins. In this work, we study few-shot learning with PLMs from a different\nperspective: We first tune an autoregressive PLM on the few-shot samples and\nthen use it as a generator to synthesize a large amount of novel training\nsamples which augment the original training set. To encourage the generator to\nproduce label-discriminative samples, we train it via weighted maximum\nlikelihood where the weight of each token is automatically adjusted based on a\ndiscriminative meta-learning objective. A classification PLM can then be\nfine-tuned on both the few-shot and the synthetic samples with regularization\nfor better generalization and stability. Our approach FewGen achieves an\noverall better result across seven classification tasks of the GLUE benchmark\nthan existing few-shot learning methods, improving no-augmentation methods by\n5+ average points, and outperforming augmentation methods by 3+ average points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalski_M/0/1/0/all/0/1\">Martin Michalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelzaher_T/0/1/0/all/0/1\">Tarek Abdelzaher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective. (arXiv:2211.08073v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08073","description":"<p>Pre-trained language models (PLMs) are known to improve the generalization\nperformance of natural language understanding models by leveraging large\namounts of data during the pre-training phase. However, the out-of-distribution\n(OOD) generalization problem remains a challenge in many NLP tasks, limiting\nthe real-world deployment of these methods. This paper presents the first\nattempt at creating a unified benchmark named \\method for evaluating OOD\nrobustness in NLP models, highlighting the importance of OOD robustness and\nproviding insights on how to measure the robustness of a model and how to\nimprove it. The benchmark includes 13 publicly available datasets for OOD\ntesting, and evaluations are conducted on 8 classic NLP tasks over 21 popularly\nused PLMs, including GPT-3 and GPT-3.5. Our findings confirm the need for\nimproved OOD accuracy in NLP tasks, as significant performance degradation was\nobserved in all settings compared to in-distribution (ID) accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuibai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yafu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanmeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RHO ($\\rho$): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding. (arXiv:2212.01588v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.01588","description":"<p>Dialogue systems can leverage large pre-trained language models and knowledge\nto generate fluent and informative responses. However, these models are still\nprone to produce hallucinated responses not supported by the input source,\nwhich greatly hinders their application. The heterogeneity between external\nknowledge and dialogue context challenges representation learning and source\nintegration, and further contributes to unfaithfulness. To handle this\nchallenge and generate more faithful responses, this paper presents RHO\n($\\rho$) utilizing the representations of linked entities and relation\npredicates from a knowledge graph (KG). We propose (1) local knowledge\ngrounding to combine textual embeddings with the corresponding KG embeddings;\nand (2) global knowledge grounding to equip RHO with multi-hop reasoning\nabilities via the attention mechanism. In addition, we devise a response\nre-ranking technique based on walks over KG sub-graphs for better\nconversational reasoning. Experimental results on OpenDialKG show that our\napproach significantly outperforms state-of-the-art methods on both automatic\nand human evaluation by a large margin, especially in hallucination reduction\n(17.54% in FeQA).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Ziwei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1\">Bryan Wilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Min Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PLUE: Language Understanding Evaluation Benchmark for Privacy Policies in English. (arXiv:2212.10011v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10011","description":"<p>Privacy policies provide individuals with information about their rights and\nhow their personal information is handled. Natural language understanding (NLU)\ntechnologies can support individuals and practitioners to understand better\nprivacy practices described in lengthy and complex documents. However, existing\nefforts that use NLU technologies are limited by processing the language in a\nway exclusive to a single task focusing on certain privacy practices. To this\nend, we introduce the Privacy Policy Language Understanding Evaluation (PLUE)\nbenchmark, a multi-task benchmark for evaluating the privacy policy language\nunderstanding across various tasks. We also collect a large corpus of privacy\npolicies to enable privacy policy domain-specific language model pre-training.\nWe evaluate several generic pre-trained language models and continue\npre-training them on the collected corpus. We demonstrate that domain-specific\ncontinual pre-training offers performance improvements across all tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_J/0/1/0/all/0/1\">Jianfeng Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-NER: Named Entity Recognition via Large Language Models. (arXiv:2304.10428v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.10428","description":"<p>Despite the fact that large-scale Language Models (LLM) have achieved SOTA\nperformances on a variety of NLP tasks, its performance on NER is still\nsignificantly below supervised baselines. This is due to the gap between the\ntwo tasks the NER and LLMs: the former is a sequence labeling task in nature\nwhile the latter is a text-generation model.\n</p>\n<p>In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the\ngap by transforming the sequence labeling task to a generation task that can be\neasily adapted by LLMs e.g., the task of finding location entities in the input\ntext \"Columbus is a city\" is transformed to generate the text sequence\n\"@@Columbus## is a city\", where special tokens @@## marks the entity to\nextract. To efficiently address the \"hallucination\" issue of LLMs, where LLMs\nhave a strong inclination to over-confidently label NULL inputs as entities, we\npropose a self-verification strategy by prompting LLMs to ask itself whether\nthe extracted entities belong to a labeled entity tag.\n</p>\n<p>We conduct experiments on five widely adopted NER datasets, and GPT-NER\nachieves comparable performances to fully supervised baselines, which is the\nfirst time as far as we are concerned. More importantly, we find that GPT-NER\nexhibits a greater ability in the low-resource and few-shot setups, when the\namount of training data is extremely scarce, GPT-NER performs significantly\nbetter than supervised models. This demonstrates the capabilities of GPT-NER in\nreal-world NER applications where the number of labeled examples is limited.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_R/0/1/0/all/0/1\">Rongbin Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations. (arXiv:2304.14827v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.14827","description":"<p>This paper aims to quantitatively evaluate the performance of ChatGPT, an\ninteractive large language model, on inter-sentential relations such as\ntemporal relations, causal relations, and discourse relations. Given ChatGPT's\npromising performance across various tasks, we conduct extensive evaluations on\nthe whole test sets of 13 datasets, including temporal and causal relations,\nPDTB2.0-based and dialogue-based discourse relations, and downstream\napplications on discourse understanding. To achieve reliable results, we adopt\nthree tailored prompt templates for each task, including the zero-shot prompt\ntemplate, zero-shot prompt engineering (PE) template, and in-context learning\n(ICL) prompt template, to establish the initial baseline scores for all popular\nsentence-pair relation classification tasks for the first time. We find that\nChatGPT exhibits strong performance in detecting and reasoning about causal\nrelations, while it may not be proficient in identifying the temporal order\nbetween two events. It can recognize most discourse relations with existing\nexplicit discourse connectives, but the implicit discourse relation still\nremains a challenging task. Meanwhile, ChatGPT performs poorly in the dialogue\ndiscourse parsing task that requires structural understanding in a dialogue\nbefore being aware of the discourse relation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Chunkit Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jiayang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1\">Tianqing Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neuromodulation Gated Transformer. (arXiv:2305.03232v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03232","description":"<p>We introduce a novel architecture, the Neuromodulation Gated Transformer\n(NGT), which is a simple implementation of neuromodulation in transformers via\na multiplicative effect. We compare it to baselines and show that it results in\nthe best average performance on the SuperGLUE benchmark validation sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Knowles_K/0/1/0/all/0/1\">Kobe Knowles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bensemann_J/0/1/0/all/0/1\">Joshua Bensemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benavides_Prado_D/0/1/0/all/0/1\">Diana Benavides-Prado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogarajan_V/0/1/0/all/0/1\">Vithya Yogarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witbrock_M/0/1/0/all/0/1\">Michael Witbrock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobbie_G/0/1/0/all/0/1\">Gillian Dobbie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MGR: Multi-generator based Rationalization. (arXiv:2305.04492v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.04492","description":"<p>Rationalization is to employ a generator and a predictor to construct a\nself-explaining NLP model in which the generator selects a subset of\nhuman-intelligible pieces of the input text to the following predictor.\nHowever, rationalization suffers from two key challenges, i.e., spurious\ncorrelation and degeneration, where the predictor overfits the spurious or\nmeaningless pieces solely selected by the not-yet well-trained generator and in\nturn deteriorates the generator. Although many studies have been proposed to\naddress the two challenges, they are usually designed separately and do not\ntake both of them into account. In this paper, we propose a simple yet\neffective method named MGR to simultaneously solve the two problems. The key\nidea of MGR is to employ multiple generators such that the occurrence stability\nof real pieces is improved and more meaningful pieces are delivered to the\npredictor. Empirically, we show that MGR improves the F1 score by up to 20.9%\nas compared to state-of-the-art methods. Codes are available at\nhttps://github.com/jugechengzi/Rationalization-MGR .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haozhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuankai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yang Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models. (arXiv:2305.05189v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.05189","description":"<p>Diffusion models, which have emerged to become popular text-to-image\ngeneration models, can produce high-quality and content-rich images guided by\ntextual prompts. However, there are limitations to semantic understanding and\ncommonsense reasoning in existing models when the input prompts are concise\nnarrative, resulting in low-quality image generation. To improve the capacities\nfor narrative prompts, we propose a simple-yet-effective parameter-efficient\nfine-tuning approach called the Semantic Understanding and Reasoning adapter\n(SUR-adapter) for pre-trained diffusion models. To reach this goal, we first\ncollect and annotate a new dataset SURD which consists of more than 57,000\nsemantically corrected multi-modal samples. Each sample contains a simple\nnarrative prompt, a complex keyword-based prompt, and a high-quality image.\nThen, we align the semantic representation of narrative prompts to the complex\nprompts and transfer knowledge of large language models (LLMs) to our\nSUR-adapter via knowledge distillation so that it can acquire the powerful\nsemantic understanding and reasoning capabilities to build a high-quality\ntextual semantic representation for text-to-image generation. We conduct\nexperiments by integrating multiple LLMs and popular pre-trained diffusion\nmodels to show the effectiveness of our approach in enabling diffusion models\nto understand and reason concise natural language without image quality\ndegradation. Our approach can make text-to-image diffusion models easier to use\nwith better user experience, which demonstrates our approach has the potential\nfor further advancing the development of user-friendly text-to-image generation\nmodels by bridging the semantic gap between simple narrative prompts and\ncomplex keyword-based prompts. The code is released at\nhttps://github.com/Qrange-group/SUR-adapter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1\">Shanshan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongzhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1\">Wushao Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jinghui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable multimodal sentiment analysis based on textual modality descriptions by using large-scale language models. (arXiv:2305.06162v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06162","description":"<p>Multimodal sentiment analysis is an important area for understanding the\nuser's internal states. Deep learning methods were effective, but the problem\nof poor interpretability has gradually gained attention. Previous works have\nattempted to use attention weights or vector distributions to provide\ninterpretability. However, their explanations were not intuitive and can be\ninfluenced by different trained models. This study proposed a novel approach to\nprovide interpretability by converting nonverbal modalities into text\ndescriptions and by using large-scale language models for sentiment\npredictions. This provides an intuitive approach to directly interpret what\nmodels depend on with respect to making decisions from input texts, thus\nsignificantly improving interpretability. Specifically, we convert descriptions\nbased on two feature patterns for the audio modality and discrete action units\nfor the facial modality. Experimental results on two sentiment analysis tasks\ndemonstrated that the proposed approach maintained, or even improved\neffectiveness for sentiment analysis compared to baselines using conventional\nfeatures, with the highest improvement of 2.49% on the F1 score. The results\nalso showed that multimodal descriptions have similar characteristics on fusing\nmodalities as those of conventional fusion methods. The results demonstrated\nthat the proposed approach is interpretable and effective for multimodal\nsentiment analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sixia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okada_S/0/1/0/all/0/1\">Shogo Okada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps. (arXiv:2305.06472v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.06472","description":"<p>Prognostics and health management (PHM) technology plays a critical role in\nindustrial production and equipment maintenance by identifying and predicting\npossible equipment failures and damages, thereby allowing necessary maintenance\nmeasures to be taken to enhance equipment service life and reliability while\nreducing production costs and downtime. In recent years, PHM technology based\non artificial intelligence (AI) has made remarkable achievements in the context\nof the industrial IoT and big data, and it is widely used in various\nindustries, such as railway, energy, and aviation, for condition monitoring,\nfault prediction, and health management. The emergence of large-scale\nfoundation models (LSF-Models) such as ChatGPT and DALLE-E marks the entry of\nAI into a new era of AI-2.0 from AI-1.0, where deep models have rapidly evolved\nfrom a research paradigm of single-modal, single-task, and limited-data to a\nmulti-modal, multi-task, massive data, and super-large model paradigm. ChatGPT\nrepresents a landmark achievement in this research paradigm, offering hope for\ngeneral artificial intelligence due to its highly intelligent natural language\nunderstanding ability. However, the PHM field lacks a consensus on how to\nrespond to this significant change in the AI field, and a systematic review and\nroadmap is required to elucidate future development directions. To fill this\ngap, this paper systematically expounds on the key components and latest\ndevelopments of LSF-Models. Then, we systematically answered how to build the\nLSF-Model applicable to PHM tasks and outlined the challenges and future\ndevelopment roadmaps for this research paradigm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yan-Fu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Muxia Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Musketeer (All for One, and One for All): A Generalist Vision-Language Model with Task Explanation Prompts. (arXiv:2305.07019v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2305.07019","description":"<p>We present a sequence-to-sequence vision-language model whose parameters are\njointly trained on all tasks (all for one) and fully shared among multiple\ntasks (one for all), resulting in a single model which we named Musketeer. The\nintegration of knowledge across heterogeneous tasks is enabled by a novel\nfeature called Task Explanation Prompt (TEP). TEP reduces interference among\ntasks, allowing the model to focus on their shared structure. With a single\nmodel, Musketeer achieves results comparable to or better than strong baselines\ntrained on single tasks, almost uniformly across multiple tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yantao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Kunyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhaowei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jun Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Siqi Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modolo_D/0/1/0/all/0/1\">Davide Modolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuowen Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-05-14T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}