{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-05-08T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Modeling What-to-ask and How-to-ask for Answer-unaware Conversational Question Generation. (arXiv:2305.03088v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03088","description":"<p>Conversational Question Generation (CQG) is a critical task for machines to\nassist humans in fulfilling their information needs through conversations. The\ntask is generally cast into two different settings: answer-aware and\nanswer-unaware. While the former facilitates the models by exposing the\nexpected answer, the latter is more realistic and receiving growing attentions\nrecently. What-to-ask and how-to-ask are the two main challenges in the\nanswer-unaware setting. To address the first challenge, existing methods mainly\nselect sequential sentences in context as the rationales. We argue that the\nconversation generated using such naive heuristics may not be natural enough as\nin reality, the interlocutors often talk about the relevant contents that are\nnot necessarily sequential in context. Additionally, previous methods decide\nthe type of question to be generated (boolean/span-based) implicitly. Modeling\nthe question type explicitly is crucial as the answer, which hints the models\nto generate a boolean or span-based question, is unavailable. To this end, we\npresent SG-CQG, a two-stage CQG framework. For the what-to-ask stage, a\nsentence is selected as the rationale from a semantic graph that we construct,\nand extract the answer span from it. For the how-to-ask stage, a classifier\ndetermines the target answer type of the question via two explicit control\nsignals before generating and filtering. In addition, we propose Conv-Distinct,\na novel evaluation metric for CQG, to evaluate the diversity of the generated\nconversation from a context. Compared with the existing answer-unaware CQG\nmodels, the proposed SG-CQG achieves state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Do_X/0/1/0/all/0/1\">Xuan Long Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_B/0/1/0/all/0/1\">Bowei Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1\">Anh Tai Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liangming Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aw_A/0/1/0/all/0/1\">Ai Ti Aw</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curating corpora with classifiers: A case study of clean energy sentiment online. (arXiv:2305.03092v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03092","description":"<p>Well curated, large-scale corpora of social media posts containing broad\npublic opinion offer an alternative data source to complement traditional\nsurveys. While surveys are effective at collecting representative samples and\nare capable of achieving high accuracy, they can be both expensive to run and\nlag public opinion by days or weeks. Both of these drawbacks could be overcome\nwith a real-time, high volume data stream and fast analysis pipeline. A central\nchallenge in orchestrating such a data pipeline is devising an effective method\nfor rapidly selecting the best corpus of relevant documents for analysis.\nQuerying with keywords alone often includes irrelevant documents that are not\neasily disambiguated with bag-of-words natural language processing methods.\nHere, we explore methods of corpus curation to filter irrelevant tweets using\npre-trained transformer-based models, fine-tuned for our binary classification\ntask on hand-labeled tweets. We are able to achieve F1 scores of up to 0.95.\nThe low cost and high performance of fine-tuning such a model suggests that our\napproach could be of broad benefit as a pre-processing step for social media\ndatasets with uncertain corpus boundaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1\">Michael V. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodds_P/0/1/0/all/0/1\">Peter Sheridan Dodds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danforth_C/0/1/0/all/0/1\">Chris M. Danforth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Transducer and Attention based Encoder-Decoder Modeling for Speech-to-Text Tasks. (arXiv:2305.03101v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03101","description":"<p>Transducer and Attention based Encoder-Decoder (AED) are two widely used\nframeworks for speech-to-text tasks. They are designed for different purposes\nand each has its own benefits and drawbacks for speech-to-text tasks. In order\nto leverage strengths of both modeling methods, we propose a solution by\ncombining Transducer and Attention based Encoder-Decoder (TAED) for\nspeech-to-text tasks. The new method leverages AED's strength in non-monotonic\nsequence to sequence learning while retaining Transducer's streaming property.\nIn the proposed framework, Transducer and AED share the same speech encoder.\nThe predictor in Transducer is replaced by the decoder in the AED model, and\nthe outputs of the decoder are conditioned on the speech inputs instead of\noutputs from an unconditioned language model. The proposed solution ensures\nthat the model is optimized by covering all possible read/write scenarios and\ncreates a matched environment for streaming applications. We evaluate the\nproposed approach on the \\textsc{MuST-C} dataset and the findings demonstrate\nthat TAED performs significantly better than Transducer for offline automatic\nspeech recognition (ASR) and speech-to-text translation (ST) tasks. In the\nstreaming case, TAED outperforms Transducer in the ASR task and one ST\ndirection while comparable results are achieved in another translation\ndirection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Anna Y. Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyue Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1\">Ning Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xutai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomasello_P/0/1/0/all/0/1\">Paden D. Tomasello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs. (arXiv:2305.03111v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03111","description":"<p>Text-to-SQL parsing, which aims at converting natural language instructions\ninto executable SQLs, has gained increasing attention in recent years. In\nparticular, Codex and ChatGPT have shown impressive results in this task.\nHowever, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on\ndatabase schema with few rows of database contents leaving the gap between\nacademic study and real-world applications. To mitigate this gap, we present\nBird, a big benchmark for large-scale database grounded in text-to-SQL tasks,\ncontaining 12,751 pairs of text-to-SQL data and 95 databases with a total size\nof 33.4 GB, spanning 37 professional domains. Our emphasis on database values\nhighlights the new challenges of dirty database contents, external knowledge\nbetween NL questions and database contents, and SQL efficiency, particularly in\nthe context of massive databases. To solve these problems, text-to-SQL models\nmust feature database value comprehension in addition to semantic parsing. The\nexperimental results demonstrate the significance of database values in\ngenerating accurate text-to-SQLs for big databases. Furthermore, even the most\neffective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution\naccuracy, which is still far from the human result of 92.96%, proving that\nchallenges still stand. Besides, we also provide an efficiency analysis to\noffer insights into generating text-to-efficient-SQLs that are beneficial to\nindustries. We believe that BIRD will contribute to advancing real-world\napplications of text-to-SQL research. The leaderboard and source code are\navailable: https://bird-bench.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1\">Binyuan Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_G/0/1/0/all/0/1\">Ge Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Binhua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaxi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bailin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bowen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Rongyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_R/0/1/0/all/0/1\">Ruiying Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_N/0/1/0/all/0/1\">Nan Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chenhao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin C.C. Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1\">Reynold Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations. (arXiv:2305.03117v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03117","description":"<p>Human-annotated labels and explanations are critical for training explainable\nNLP models. However, unlike human-annotated labels whose quality is easier to\ncalibrate (e.g., with a majority vote), human-crafted free-form explanations\ncan be quite subjective, as some recent works have discussed. Before blindly\nusing them as ground truth to train ML models, a vital question needs to be\nasked: How do we evaluate a human-annotated explanation's quality? In this\npaper, we build on the view that the quality of a human-annotated explanation\ncan be measured based on its helpfulness (or impairment) to the ML models'\nperformance for the desired NLP tasks for which the annotations were collected.\nIn comparison to the commonly used Simulatability score, we define a new metric\nthat can take into consideration the helpfulness of an explanation for model\nperformance at both fine-tuning and inference. With the help of a unified\ndataset format, we evaluated the proposed metric on five datasets (e.g.,\ne-SNLI) against two model architectures (T5 and BART), and the results show\nthat our proposed metric can objectively evaluate the quality of\nhuman-annotated explanations, while Simulatability falls short.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bingsheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_P/0/1/0/all/0/1\">Prithviraj Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popa_L/0/1/0/all/0/1\">Lucian Popa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendler_J/0/1/0/all/0/1\">James Hendler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review. (arXiv:2305.03123v1 [cs.CY])","link":"http://arxiv.org/abs/2305.03123","description":"<p>ChatGPT is another large language model (LLM) inline but due to its\nperformance and ability to converse effectively, it has gained a huge\npopularity amongst research as well as industrial community. Recently, many\nstudies have been published to show the effectiveness, efficiency, integration,\nand sentiments of chatGPT and other LLMs. In contrast, this study focuses on\nthe important aspects that are mostly overlooked, i.e. sustainability, privacy,\ndigital divide, and ethics and suggests that not only chatGPT but every\nsubsequent entry in the category of conversational bots should undergo\nSustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. This\npaper discusses in detail about the issues and concerns raised over chatGPT in\nline with aforementioned characteristics. We support our hypothesis by some\npreliminary data collection and visualizations along with hypothesized facts.\nWe also suggest mitigations and recommendations for each of the concerns.\nFurthermore, we also suggest some policies and recommendations for AI policy\nact, if designed by the governments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khowaja_S/0/1/0/all/0/1\">Sunder Ali Khowaja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khuwaja_P/0/1/0/all/0/1\">Parus Khuwaja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_K/0/1/0/all/0/1\">Kapal Dev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain-of-Skills: A Configurable Model for Open-domain Question Answering. (arXiv:2305.03130v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03130","description":"<p>The retrieval model is an indispensable component for real-world\nknowledge-intensive tasks, e.g., open-domain question answering (ODQA). As\nseparate retrieval skills are annotated for different datasets, recent work\nfocuses on customized methods, limiting the model transferability and\nscalability. In this work, we propose a modular retriever where individual\nmodules correspond to key skills that can be reused across datasets. Our\napproach supports flexible skill configurations based on the target domain to\nboost performance. To mitigate task interference, we design a novel\nmodularization parameterization inspired by sparse Transformer. We demonstrate\nthat our model can benefit from self-supervised pretraining on Wikipedia and\nfine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our\napproach outperforms recent self-supervised retrievers in zero-shot evaluations\nand achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA\nand OTT-QA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaixin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyberg_E/0/1/0/all/0/1\">Eric Nyberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Role of Global and Local Context in Named Entity Recognition. (arXiv:2305.03132v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03132","description":"<p>Pre-trained transformer-based models have recently shown great performance\nwhen applied to Named Entity Recognition (NER). As the complexity of their\nself-attention mechanism prevents them from processing long documents at once,\nthese models are usually applied in a sequential fashion. Such an approach\nunfortunately only incorporates local context and prevents leveraging global\ndocument context in long documents such as novels, which might hinder\nperformance. In this article, we explore the impact of global document context,\nand its relationships with local context. We find that correctly retrieving\nglobal document context has a greater impact on performance than only\nleveraging local context, prompting for further research on how to better\nretrieve that context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amalvy_A/0/1/0/all/0/1\">Arthur Amalvy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labatut_V/0/1/0/all/0/1\">Vincent Labatut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufour_R/0/1/0/all/0/1\">Richard Dufour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Influence of various text embeddings on clustering performance in NLP. (arXiv:2305.03144v1 [cs.LG])","link":"http://arxiv.org/abs/2305.03144","description":"<p>With the advent of e-commerce platforms, reviews are crucial for customers to\nassess the credibility of a product. The star ratings do not always match the\nreview text written by the customer. For example, a three star rating (out of\nfive) may be incongruous with the review text, which may be more suitable for a\nfive star review. A clustering approach can be used to relabel the correct star\nratings by grouping the text reviews into individual groups. In this work, we\nexplore the task of choosing different text embeddings to represent these\nreviews and also explore the impact the embedding choice has on the performance\nof various classes of clustering algorithms. We use contextual (BERT) and\nnon-contextual (Word2Vec) text embeddings to represent the text and measure\ntheir impact of three classes on clustering algorithms - partitioning based\n(KMeans), single linkage agglomerative hierarchical, and density based (DBSCAN\nand HDBSCAN), each with various experimental settings. We use the silhouette\nscore, adjusted rand index score, and cluster purity score metrics to evaluate\nthe performance of the algorithms and discuss the impact of different\nembeddings on the clustering performance. Our results indicate that the type of\nembedding chosen drastically affects the performance of the algorithm, the\nperformance varies greatly across different types of clustering algorithms, no\nembedding type is better than the other, and DBSCAN outperforms KMeans and\nsingle linkage agglomerative clustering but also labels more data points as\noutliers. We provide a thorough comparison of the performances of different\nalgorithms and provide numerous ideas to foster further research in the domain\nof text clustering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_R/0/1/0/all/0/1\">Rohan Saha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sensitive Data Detection with High-Throughput Machine Learning Models in Electrical Health Records. (arXiv:2305.03169v1 [cs.CR])","link":"http://arxiv.org/abs/2305.03169","description":"<p>In the era of big data, there is an increasing need for healthcare providers,\ncommunities, and researchers to share data and collaborate to improve health\noutcomes, generate valuable insights, and advance research. The Health\nInsurance Portability and Accountability Act of 1996 (HIPAA) is a federal law\ndesigned to protect sensitive health information by defining regulations for\nprotected health information (PHI). However, it does not provide efficient\ntools for detecting or removing PHI before data sharing. One of the challenges\nin this area of research is the heterogeneous nature of PHI fields in data\nacross different parties. This variability makes rule-based sensitive variable\nidentification systems that work on one database fail on another. To address\nthis issue, our paper explores the use of machine learning algorithms to\nidentify sensitive variables in structured data, thus facilitating the\nde-identification process. We made a key observation that the distributions of\nmetadata of PHI fields and non-PHI fields are very different. Based on this\nnovel finding, we engineered over 30 features from the metadata of the original\nfeatures and used machine learning to build classification models to\nautomatically identify PHI fields in structured Electronic Health Record (EHR)\ndata. We trained the model on a variety of large EHR databases from different\ndata sources and found that our algorithm achieves 99% accuracy when detecting\nPHI-related fields for unseen datasets. The implications of our study are\nsignificant and can benefit industries that handle sensitive data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gpt-4: A Review on Advancements and Opportunities in Natural Language Processing. (arXiv:2305.03195v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03195","description":"<p>Generative Pre-trained Transformer 4 (GPT-4) is the fourth-generation\nlanguage model in the GPT series, developed by OpenAI, which promises\nsignificant advancements in the field of natural language processing (NLP). In\nthis research article, we have discussed the features of GPT-4, its potential\napplications, and the challenges that it might face. We have also compared\nGPT-4 with its predecessor, GPT-3. GPT-4 has a larger model size (more than one\ntrillion), better multilingual capabilities, improved contextual understanding,\nand reasoning capabilities than GPT-3. Some of the potential applications of\nGPT-4 include chatbots, personal assistants, language translation, text\nsummarization, and question-answering. However, GPT-4 poses several challenges\nand limitations such as computational requirements, data requirements, and\nethical concerns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baktash_J/0/1/0/all/0/1\">Jawid Ahmad Baktash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawodi_M/0/1/0/all/0/1\">Mursal Dawodi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Employing Hybrid Deep Neural Networks on Dari Speech. (arXiv:2305.03200v1 [eess.AS])","link":"http://arxiv.org/abs/2305.03200","description":"<p>This paper is an extension of our previous conference paper. In recent years,\nthere has been a growing interest among researchers in developing and improving\nspeech recognition systems to facilitate and enhance human-computer\ninteraction. Today, Automatic Speech Recognition (ASR) systems have become\nubiquitous, used in everything from games to translation systems, robots, and\nmore. However, much research is still needed on speech recognition systems for\nlow-resource languages. This article focuses on the recognition of individual\nwords in the Dari language using the Mel-frequency cepstral coefficients\n(MFCCs) feature extraction method and three different deep neural network\nmodels: Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), and\nMultilayer Perceptron (MLP), as well as two hybrid models combining CNN and\nRNN. We evaluate these models using an isolated Dari word corpus that we have\ncreated, consisting of 1000 utterances for 20 short Dari terms. Our study\nachieved an impressive average accuracy of 98.365%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Baktash_J/0/1/0/all/0/1\">Jawid Ahmad Baktash</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dawodi_M/0/1/0/all/0/1\">Mursal Dawodi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Pashto Text Classification using Language Processing Techniques for Single And Multi-Label Analysis. (arXiv:2305.03201v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03201","description":"<p>Text classification has become a crucial task in various fields, leading to a\nsignificant amount of research on developing automated text classification\nsystems for national and international languages. However, there is a growing\nneed for automated text classification systems that can handle local languages.\nThis study aims to establish an automated classification system for Pashto\ntext. To achieve this goal, we constructed a dataset of Pashto documents and\napplied various models, including statistical and neural machine learning\nmodels such as DistilBERT-base-multilingual-cased, Multilayer Perceptron,\nSupport Vector Machine, K Nearest Neighbor, decision tree, Gaussian na\\\"ive\nBayes, multinomial na\\\"ive Bayes, random forest, and logistic regression, to\nidentify the most effective approach. We also evaluated two different feature\nextraction methods, bag of words and Term Frequency Inverse Document Frequency.\nThe study achieved an average testing accuracy rate of 94% using the MLP\nclassification algorithm and TFIDF feature extraction method in single-label\nmulticlass classification. Similarly, MLP+TFIDF yielded the best results, with\nan F1-measure of 0.81. Furthermore, the use of pre-trained language\nrepresentation models, such as DistilBERT, showed promising results for Pashto\ntext classification; however, the study highlights the importance of developing\na specific tokenizer for a particular language to achieve reasonable results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dawodi_M/0/1/0/all/0/1\">Mursal Dawodi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baktash_J/0/1/0/all/0/1\">Jawid Ahmad Baktash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VideoOFA: Two-Stage Pre-Training for Video-to-Text Generation. (arXiv:2305.03204v1 [cs.CV])","link":"http://arxiv.org/abs/2305.03204","description":"<p>We propose a new two-stage pre-training framework for video-to-text\ngeneration tasks such as video captioning and video question answering: A\ngenerative encoder-decoder model is first jointly pre-trained on massive\nimage-text data to learn fundamental vision-language concepts, and then adapted\nto video data in an intermediate video-text pre-training stage to learn\nvideo-specific skills such as spatio-temporal reasoning. As a result, our\nVideoOFA model achieves new state-of-the-art performance on four Video\nCaptioning benchmarks, beating prior art by an average of 9.7 points in CIDEr\nscore. It also outperforms existing models on two open-ended Video Question\nAnswering datasets, showcasing its generalization capability as a universal\nvideo-to-text model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lili Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Wenhan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas O&#x11f;uz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Lexical Sharing in Multilingual Machine Translation for Indian Languages. (arXiv:2305.03207v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03207","description":"<p>Multilingual language models have shown impressive cross-lingual transfer\nability across a diverse set of languages and tasks. To improve the\ncross-lingual ability of these models, some strategies include transliteration\nand finer-grained segmentation into characters as opposed to subwords. In this\nwork, we investigate lexical sharing in multilingual machine translation (MT)\nfrom Hindi, Gujarati, Nepali into English. We explore the trade-offs that exist\nin translation performance between data sampling and vocabulary size, and we\nexplore whether transliteration is useful in encouraging cross-script\ngeneralisation. We also verify how the different settings generalise to unseen\nlanguages (Marathi and Bengali). We find that transliteration does not give\npronounced improvements and our analysis suggests that our multilingual MT\nmodels trained on original scripts seem to already be robust to cross-script\ndifferences even for relatively low-resource languages\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sannigrahi_S/0/1/0/all/0/1\">Sonal Sannigrahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bawden_R/0/1/0/all/0/1\">Rachel Bawden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AttentionViz: A Global View of Transformer Attention. (arXiv:2305.03210v1 [cs.HC])","link":"http://arxiv.org/abs/2305.03210","description":"<p>Transformer models are revolutionizing machine learning, but their inner\nworkings remain mysterious. In this work, we present a new visualization\ntechnique designed to help researchers understand the self-attention mechanism\nin transformers that allows these models to learn rich, contextual\nrelationships between elements of a sequence. The main idea behind our method\nis to visualize a joint embedding of the query and key vectors used by\ntransformer models to compute attention. Unlike previous attention\nvisualization techniques, our approach enables the analysis of global patterns\nacross multiple input sequences. We create an interactive visualization tool,\nAttentionViz, based on these joint query-key embeddings, and use it to study\nattention mechanisms in both language and vision transformers. We demonstrate\nthe utility of our approach in improving model understanding and offering new\ninsights about query-key interactions through several application scenarios and\nexpert feedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1\">Catherine Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yida Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Aoyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cynthia Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viegas_F/0/1/0/all/0/1\">Fernanda Vi&#xe9;gas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenberg_M/0/1/0/all/0/1\">Martin Wattenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neuromodulation Gated Transformer. (arXiv:2305.03232v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03232","description":"<p>We introduce a novel architecture, the Neuromodulation Gated Transformer\n(NGT), which is a simple implementation of neuromodulation in transformers via\na multiplicative effect. We compare it to baselines and show that it results in\nthe best average performance on the SuperGLUE benchmark validation sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Knowles_K/0/1/0/all/0/1\">Kobe Knowles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bensemann_J/0/1/0/all/0/1\">Joshua Bensemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prado_D/0/1/0/all/0/1\">Diana Benavides Prado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogarajan_V/0/1/0/all/0/1\">Vithya Yogarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witbrock_M/0/1/0/all/0/1\">Michael Witbrock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobbie_G/0/1/0/all/0/1\">Gillian Dobbie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Out-of-Distribution Detection in NLP. (arXiv:2305.03236v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03236","description":"<p>Out-of-distribution (OOD) detection is essential for the reliable and safe\ndeployment of machine learning systems in the real world. Great progress has\nbeen made over the past years. This paper presents the first review of recent\nadvances in OOD detection with a particular focus on natural language\nprocessing approaches. First, we provide a formal definition of OOD detection\nand discuss several related fields. We then categorize recent algorithms into\nthree classes according to the data they used: (1) OOD data available, (2) OOD\ndata unavailable + in-distribution (ID) label available, and (3) OOD data\nunavailable + ID label unavailable. Third, we introduce datasets, applications,\nand metrics. Finally, we summarize existing work and present potential future\nresearch topics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_H/0/1/0/all/0/1\">Hao Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Out-of-Domain Intent Detection Considering Multi-turn Dialogue Contexts. (arXiv:2305.03237v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03237","description":"<p>Out-of-Domain (OOD) intent detection is vital for practical dialogue systems,\nand it usually requires considering multi-turn dialogue contexts. However, most\nprevious OOD intent detection approaches are limited to single dialogue turns.\nIn this paper, we introduce a context-aware OOD intent detection (Caro)\nframework to model multi-turn contexts in OOD intent detection tasks.\nSpecifically, we follow the information bottleneck principle to extract robust\nrepresentations from multi-turn dialogue contexts. Two different views are\nconstructed for each input sample and the superfluous information not related\nto intent detection is removed using a multi-view information bottleneck loss.\nMoreover, we also explore utilizing unlabeled data in Caro. A two-stage\ntraining process is introduced to mine OOD samples from these unlabeled data,\nand these OOD samples are used to train the resulting model with a\nbootstrapping approach. Comprehensive experiments demonstrate that Caro\nestablishes state-of-the-art performances on multi-turn OOD detection tasks by\nimproving the F1-OOD score of over $29\\%$ compared to the previous best method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_H/0/1/0/all/0/1\">Hao Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1\">Binyuan Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VicunaNER: Zero/Few-shot Named Entity Recognition using Vicuna. (arXiv:2305.03253v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03253","description":"<p>Large Language Models (LLMs, e.g., ChatGPT) have shown impressive zero- and\nfew-shot capabilities in Named Entity Recognition (NER). However, these models\ncan only be accessed via online APIs, which may cause data leak and\nnon-reproducible problems. In this paper, we propose VicunaNER, a zero/few-shot\nNER framework based on the newly released open-source LLM -- Vicuna. VicunaNER\nis a two-phase framework, where each phase leverages multi-turn dialogues with\nVicuna to recognize entities from texts. We name the second phase as\nRe-Recognition, which recognizes those entities not recognized in the first\nphase (a.k.a. Recognition). Moreover, we set entity correctness check dialogues\nin each phase to filter out wrong entities. We evaluate VicunaNER's zero-shot\ncapacity on 10 datasets crossing 5 domains and few-shot capacity on Few-NERD.\nExperimental results demonstrate that VicunaNER achieves superior performance\nin both shot settings. Additionally, we conduct comprehensive investigations on\nVicuna from multiple perspectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_B/0/1/0/all/0/1\">Bin Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stylized Data-to-Text Generation: A Case Study in the E-Commerce Domain. (arXiv:2305.03256v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03256","description":"<p>Existing data-to-text generation efforts mainly focus on generating a\ncoherent text from non-linguistic input data, such as tables and\nattribute-value pairs, but overlook that different application scenarios may\nrequire texts of different styles. Inspired by this, we define a new task,\nnamely stylized data-to-text generation, whose aim is to generate coherent text\nfor the given non-linguistic data according to a specific style. This task is\nnon-trivial, due to three challenges: the logic of the generated text,\nunstructured style reference, and biased training samples. To address these\nchallenges, we propose a novel stylized data-to-text generation model, named\nStyleD2T, comprising three components: logic planning-enhanced data embedding,\nmask-based style embedding, and unbiased stylized text generation. In the first\ncomponent, we introduce a graph-guided logic planner for attribute organization\nto ensure the logic of generated text. In the second component, we devise\nfeature-level mask-based style embedding to extract the essential style signal\nfrom the given unstructured style reference. In the last one, pseudo triplet\naugmentation is utilized to achieve unbiased text generation, and a\nmulti-condition based confidence assignment function is designed to ensure the\nquality of pseudo samples. Extensive experiments on a newly collected dataset\nfrom Taobao have been conducted, and the results show the superiority of our\nmodel over existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liqiang Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xuemeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xuming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhongzhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rescue Conversations from Dead-ends: Efficient Exploration for Task-oriented Dialogue Policy Optimization. (arXiv:2305.03262v1 [cs.HC])","link":"http://arxiv.org/abs/2305.03262","description":"<p>Training a dialogue policy using deep reinforcement learning requires a lot\nof exploration of the environment. The amount of wasted invalid exploration\nmakes their learning inefficient. In this paper, we find and define an\nimportant reason for the invalid exploration: dead-ends. When a conversation\nenters a dead-end state, regardless of the actions taken afterward, it will\ncontinue in a dead-end trajectory until the agent reaches a termination state\nor maximum turn. We propose a dead-end resurrection (DDR) algorithm that\ndetects the initial dead-end state in a timely and efficient manner and\nprovides a rescue action to guide and correct the exploration direction. To\nprevent dialogue policies from repeatedly making the same mistake, DDR also\nperforms dialogue data augmentation by adding relevant experiences containing\ndead-end states. We first validate the dead-end detection reliability and then\ndemonstrate the effectiveness and generality of the method by reporting\nexperimental results on several dialogue datasets from different domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yangyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dastani_M/0/1/0/all/0/1\">Mehdi Dastani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shihan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework. (arXiv:2305.03268v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03268","description":"<p>As large language models (LLMs) have become the norm in NLP, demonstrating\ngood performance in generation and reasoning tasks, one of its most fatal\ndisadvantages is the lack of factual correctness. Generating unfactual texts\nnot only leads to lower performances but also degrades the trust and validity\nof their applications. Chain-of-Thought (CoT) prompting improves trust and\nmodel performance on complex reasoning tasks by generating interpretable\nreasoning chains, but still suffers from factuality concerns in\nknowledge-intensive tasks. In this paper, we propose the Verify-and-Edit\nframework for CoT prompting, which seeks to increase prediction factuality by\npost-editing reasoning chains according to external knowledge. Building on top\nof GPT-3, our framework lead to accuracy improvements in multiple open-domain\nquestion-answering tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruochen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chengwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expository Text Generation: Imitate, Retrieve, Paraphrase. (arXiv:2305.03276v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03276","description":"<p>Expository documents are vital resources for conveying complex information to\nreaders. Despite their usefulness, writing expository documents by hand is a\ntime-consuming and labor-intensive process that requires knowledge of the\ndomain of interest, careful content planning, and the ability to synthesize\ninformation from multiple sources. To ease these burdens, we introduce the task\nof expository text generation, which seeks to automatically generate an\naccurate and informative expository document from a knowledge source. We solve\nour task by developing IRP, an iterative framework that overcomes the\nlimitations of language models and separately tackles the steps of content\nplanning, fact selection, and rephrasing. Through experiments on three diverse\ndatasets, we demonstrate that IRP produces high-quality expository documents\nthat accurately inform readers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balepur_N/0/1/0/all/0/1\">Nishant Balepur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Resource Multi-Granularity Academic Function Recognition Based on Multiple Prompt Knowledge. (arXiv:2305.03287v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03287","description":"<p>Fine-tuning pre-trained language models (PLMs), e.g., SciBERT, generally\nrequires large numbers of annotated data to achieve state-of-the-art\nperformance on a range of NLP tasks in the scientific domain. However,\nobtaining the fine-tune data for scientific NLP task is still challenging and\nexpensive. Inspired by recent advancement in prompt learning, in this paper, we\npropose the Mix Prompt Tuning (MPT), which is a semi-supervised method to\nalleviate the dependence on annotated data and improve the performance of\nmulti-granularity academic function recognition tasks with a small number of\nlabeled examples. Specifically, the proposed method provides multi-perspective\nrepresentations by combining manual prompt templates with automatically learned\ncontinuous prompt templates to help the given academic function recognition\ntask take full advantage of knowledge in PLMs. Based on these prompt templates\nand the fine-tuned PLM, a large number of pseudo labels are assigned to the\nunlabeled examples. Finally, we fine-tune the PLM using the pseudo training\nset. We evaluate our method on three academic function recognition tasks of\ndifferent granularity including the citation function, the abstract sentence\nfunction, and the keyword function, with datasets from computer science domain\nand biomedical domain. Extensive experiments demonstrate the effectiveness of\nour method and statistically significant improvements against strong baselines.\nIn particular, it achieves an average increase of 5% in Macro-F1 score compared\nwith fine-tuning, and 6% in Macro-F1 score compared with other semi-supervised\nmethod under low-resource settings. In addition, MPT is a general method that\ncan be easily applied to other low-resource scientific classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yongqiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qikai Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransESC: Smoothing Emotional Support Conversation via Turn-Level State Transition. (arXiv:2305.03296v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03296","description":"<p>Emotion Support Conversation (ESC) is an emerging and challenging task with\nthe goal of reducing the emotional distress of people. Previous attempts fail\nto maintain smooth transitions between utterances in ESC because they ignore to\ngrasp the fine-grained transition information at each dialogue turn. To solve\nthis problem, we propose to take into account turn-level state\n\\textbf{Trans}itions of \\textbf{ESC} (\\textbf{TransESC}) from three\nperspectives, including semantics transition, strategy transition and emotion\ntransition, to drive the conversation in a smooth and natural way.\nSpecifically, we construct the state transition graph with a two-step way,\nnamed transit-then-interact, to grasp such three types of turn-level transition\ninformation. Finally, they are injected into the transition-aware decoder to\ngenerate more engaging responses. Both automatic and human evaluations on the\nbenchmark dataset demonstrate the superiority of TransESC to generate more\nsmooth and effective supportive responses. Our source code is available at\n\\url{https://github.com/circle-hit/TransESC}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weixiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Information Extraction via Chunks. (arXiv:2305.03299v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03299","description":"<p>Open Information Extraction (OIE) aims to extract relational tuples from\nopen-domain sentences. Existing OIE systems split a sentence into tokens and\nrecognize token spans as tuple relations and arguments. We instead propose\nSentence as Chunk sequence (SaC) and recognize chunk spans as tuple relations\nand arguments. We argue that SaC has better quantitative and qualitative\nproperties for OIE than sentence as token sequence, and evaluate four choices\nof chunks (i.e., CoNLL chunks, simple phrases, NP chunks, and spans from\nSpanOIE) against gold OIE tuples. Accordingly, we propose a simple BERT-based\nmodel for sentence chunking, and propose Chunk-OIE for tuple extraction on top\nof SaC. Chunk-OIE achieves state-of-the-art results on multiple OIE datasets,\nshowing that SaC benefits OIE task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_K/0/1/0/all/0/1\">Kuicai Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jung-Jae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoli Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM-RM at SemEval-2023 Task 2: Multilingual Complex NER using XLM-RoBERTa. (arXiv:2305.03300v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03300","description":"<p>Named Entity Recognition(NER) is a task of recognizing entities at a token\nlevel in a sentence. This paper focuses on solving NER tasks in a multilingual\nsetting for complex named entities. Our team, LLM-RM participated in the\nrecently organized SemEval 2023 task, Task 2: MultiCoNER II,Multilingual\nComplex Named Entity Recognition. We approach the problem by leveraging\ncross-lingual representation provided by fine-tuning XLM-Roberta base model on\ndatasets of all of the 12 languages provided -- Bangla, Chinese, English,\nFarsi, French, German, Hindi, Italian, Portuguese, Spanish, Swedish and\nUkrainian\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_R/0/1/0/all/0/1\">Rahul Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_V/0/1/0/all/0/1\">Vasudeva Varma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Block the Label and Noise: An N-Gram Masked Speller for Chinese Spell Checking. (arXiv:2305.03314v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03314","description":"<p>Recently, Chinese Spell Checking(CSC), a task to detect erroneous characters\nin a sentence and correct them, has attracted extensive interest because of its\nwide applications in various NLP tasks. Most of the existing methods have\nutilized BERT to extract semantic information for CSC task. However, these\nmethods directly take sentences with only a few errors as inputs, where the\ncorrect characters may leak answers to the model and dampen its ability to\ncapture distant context; while the erroneous characters may disturb the\nsemantic encoding process and result in poor representations. Based on such\nobservations, this paper proposes an n-gram masking layer that masks current\nand/or surrounding tokens to avoid label leakage and error disturbance.\nMoreover, considering that the mask strategy may ignore multi-modal information\nindicated by errors, a novel dot-product gating mechanism is proposed to\nintegrate the phonological and morphological information with semantic\nrepresentation. Extensive experiments on SIGHAN datasets have demonstrated that\nthe pluggable n-gram masking mechanism can improve the performance of prevalent\nCSC models and the proposed methods in this paper outperform multiple powerful\nstate-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haiyun Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HiPool: Modeling Long Documents Using Graph Neural Networks. (arXiv:2305.03319v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03319","description":"<p>Encoding long sequences in Natural Language Processing (NLP) is a challenging\nproblem. Though recent pretraining language models achieve satisfying\nperformances in many NLP tasks, they are still restricted by a pre-defined\nmaximum length, making them challenging to be extended to longer sequences. So\nsome recent works utilize hierarchies to model long sequences. However, most of\nthem apply sequential models for upper hierarchies, suffering from long\ndependency issues. In this paper, we alleviate these issues through a\ngraph-based method. We first chunk the sequence with a fixed length to model\nthe sentence-level information. We then leverage graphs to model intra- and\ncross-sentence correlations with a new attention mechanism. Additionally, due\nto limited standard benchmarks for long document classification (LDC), we\npropose a new challenging benchmark, totaling six datasets with up to 53k\nsamples and 4034 average tokens' length. Evaluation shows our model surpasses\ncompetitive baselines by 2.6% in F1 score, and 4.8% on the longest sequence\ndataset. Our method is shown to outperform hierarchical sequential models with\nbetter performance and scalability, especially for longer sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_A/0/1/0/all/0/1\">Aosong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1\">Rex Ying</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QCRI at SemEval-2023 Task 3: News Genre, Framing and Persuasion Techniques Detection using Multilingual Models. (arXiv:2305.03336v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03336","description":"<p>Misinformation spreading in mainstream and social media has been misleading\nusers in different ways. Manual detection and verification efforts by\njournalists and fact-checkers can no longer cope with the great scale and quick\nspread of misleading information. This motivated research and industry efforts\nto develop systems for analyzing and verifying news spreading online. The\nSemEval-2023 Task 3 is an attempt to address several subtasks under this\noverarching problem, targeting writing techniques used in news articles to\naffect readers' opinions. The task addressed three subtasks with six languages,\nin addition to three ``surprise'' test languages, resulting in 27 different\ntest setups. This paper describes our participating system to this task. Our\nteam is one of the 6 teams that successfully submitted runs for all setups. The\nofficial results show that our system is ranked among the top 3 systems for 10\nout of the 27 setups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hasanain_M/0/1/0/all/0/1\">Maram Hasanain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Shangiti_A/0/1/0/all/0/1\">Ahmed Oumar El-Shangiti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandi_R/0/1/0/all/0/1\">Rabindra Nath Nandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic. (arXiv:2305.03353v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03353","description":"<p>Theory of Mind (ToM) is a critical component of intelligence, yet accurately\nmeasuring it continues to be a subject of debate. Prior research has attempted\nto apply human ToM assessments to natural language processing models using\neither human-created standardized tests or rule-based templates. However, these\nmethods primarily focus on simplistic reasoning and require further validation.\nIn this study, we utilize dynamic epistemic logic, which has established\noverlaps with ToM, to generate more intricate problems. We also introduce novel\nverbalization techniques to express these problems using natural language. Our\nfindings indicate that certain language model scaling (from 70M to 6B and 350M\nto 174B) does not consistently yield results better than random chance. While\nGPT-4 demonstrates improved epistemic reasoning capabilities, there is still\nroom for enhancement. Our code and datasets are publicly available\nhttps://github.com/antoinelrnld/modlog\nhttps://huggingface.co/datasets/sileod/mindgames\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sileo_D/0/1/0/all/0/1\">Damien Sileo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lernould_A/0/1/0/all/0/1\">Antoine Lernould</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser for Complex Question Answering over Knowledge Base. (arXiv:2305.03356v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03356","description":"<p>Parsing questions into executable logical forms has showed impressive results\nfor knowledge-base question answering (KBQA). However, complex KBQA is a more\nchallenging task that requires to perform complex multi-step reasoning.\nRecently, a new semantic parser called KoPL has been proposed to explicitly\nmodel the reasoning processes, which achieved the state-of-the-art on complex\nKBQA. In this paper, we further explore how to unlock the reasoning ability of\nsemantic parsers by a simple proposed parse-execute-refine paradigm. We refine\nand improve the KoPL parser by demonstrating the executed intermediate\nreasoning steps to the KBQA model. We show that such simple strategy can\nsignificantly improve the ability of complex reasoning. Specifically, we\npropose three components: a parsing stage, an execution stage and a refinement\nstage, to enhance the ability of complex reasoning. The parser uses the KoPL to\ngenerate the transparent logical forms. Then, the execution stage aligns and\nexecutes the logical forms over knowledge base to obtain intermediate reasoning\nprocesses. Finally, the intermediate step-by-step reasoning processes are\ndemonstrated to the KBQA model in the refinement stage. With the explicit\nreasoning processes, it is much easier to answer the complex questions.\nExperiments on benchmark dataset shows that the proposed PER-KBQA performs\nsignificantly better than the stage-of-the-art baselines on the complex KBQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wangzhen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Linyin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1\">Hanjiang Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jian Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The MuSe 2023 Multimodal Sentiment Analysis Challenge: Mimicked Emotions, Cross-Cultural Humour, and Personalisation. (arXiv:2305.03369v1 [cs.LG])","link":"http://arxiv.org/abs/2305.03369","description":"<p>The MuSe 2023 is a set of shared tasks addressing three different\ncontemporary multimodal affect and sentiment analysis problems: In the Mimicked\nEmotions Sub-Challenge (MuSe-Mimic), participants predict three continuous\nemotion targets. This sub-challenge utilises the Hume-Vidmimic dataset\ncomprising of user-generated videos. For the Cross-Cultural Humour Detection\nSub-Challenge (MuSe-Humour), an extension of the Passau Spontaneous Football\nCoach Humour (Passau-SFCH) dataset is provided. Participants predict the\npresence of spontaneous humour in a cross-cultural setting. The Personalisation\nSub-Challenge (MuSe-Personalisation) is based on the Ulm-Trier Social Stress\nTest (Ulm-TSST) dataset, featuring recordings of subjects in a stressed\nsituation. Here, arousal and valence signals are to be predicted, whereas parts\nof the test labels are made available in order to facilitate personalisation.\nMuSe 2023 seeks to bring together a broad audience from different research\ncommunities such as audio-visual emotion recognition, natural language\nprocessing, signal processing, and health informatics. In this baseline paper,\nwe introduce the datasets, sub-challenges, and provided feature sets. As a\ncompetitive baseline system, a Gated Recurrent Unit (GRU)-Recurrent Neural\nNetwork (RNN) is employed. On the respective sub-challenges' test datasets, it\nachieves a mean (across three continuous intensity targets) Pearson's\nCorrelation Coefficient of .4727 for MuSe-Mimic, an Area Under the Curve (AUC)\nvalue of .8310 for MuSe-Humor and Concordance Correlation Coefficient (CCC)\nvalues of .7482 for arousal and .7827 for valence in the MuSe-Personalisation\nsub-challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Christ_L/0/1/0/all/0/1\">Lukas Christ</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amiriparian_S/0/1/0/all/0/1\">Shahin Amiriparian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baird_A/0/1/0/all/0/1\">Alice Baird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kathan_A/0/1/0/all/0/1\">Alexander Kathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_N/0/1/0/all/0/1\">Niklas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klug_S/0/1/0/all/0/1\">Steffen Klug</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1\">Chris Gagne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzirakis_P/0/1/0/all/0/1\">Panagiotis Tzirakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messner_E/0/1/0/all/0/1\">Eva-Maria Me&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konig_A/0/1/0/all/0/1\">Andreas K&#xf6;nig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cowen_A/0/1/0/all/0/1\">Alan Cowen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Gesture Recognition using Transformer and Natural Language Processing. (arXiv:2305.03407v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03407","description":"<p>The Transformer architecture is shown to provide a powerful machine\ntransduction framework for online handwritten gestures corresponding to glyph\nstrokes of natural language sentences. The attention mechanism is successfully\nused to create latent representations of an end-to-end encoder-decoder model,\nsolving multi-level segmentation while also learning some language features and\nsyntax rules. The additional use of a large decoding space with some learned\nByte-Pair-Encoding (BPE) is shown to provide robustness to ablated inputs and\nsyntax rules. The encoder stack was directly fed with spatio-temporal data\ntokens potentially forming an infinitely large input vocabulary, an approach\nthat finds applications beyond that of this work. Encoder transfer learning\ncapabilities is also demonstrated on several languages resulting in faster\noptimisation and shared parameters. A new supervised dataset of online\nhandwriting gestures suitable for generic handwriting recognition tasks was\nused to successfully train a small transformer model to an average normalised\nLevenshtein accuracy of 96% on English or German sentences and 94% in French.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silvestre_G/0/1/0/all/0/1\">G.C.M. Silvestre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balado_F/0/1/0/all/0/1\">F. Balado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akinremi_O/0/1/0/all/0/1\">O. Akinremi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramo_M/0/1/0/all/0/1\">M. Ramo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using ChatGPT for Entity Matching. (arXiv:2305.03423v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03423","description":"<p>Entity Matching is the task of deciding if two entity descriptions refer to\nthe same real-world entity. State-of-the-art entity matching methods often rely\non fine-tuning Transformer models such as BERT or RoBERTa. Two major drawbacks\nof using these models for entity matching are that (i) the models require\nsignificant amounts of fine-tuning data for reaching a good performance and\n(ii) the fine-tuned models are not robust concerning out-of-distribution\nentities. In this paper, we investigate using ChatGPT for entity matching as a\nmore robust, training data-efficient alternative to traditional Transformer\nmodels. We perform experiments along three dimensions: (i) general prompt\ndesign, (ii) in-context learning, and (iii) provision of higher-level matching\nknowledge. We show that ChatGPT is competitive with a fine-tuned RoBERTa model,\nreaching an average zero-shot performance of 83% F1 on a challenging matching\ntask on which RoBERTa requires 2000 training examples for reaching a similar\nperformance. Adding in-context demonstrations to the prompts further improves\nthe F1 by up to 5% even using only a small set of 20 handpicked examples.\nFinally, we show that guiding the zero-shot model by stating higher-level\nmatching rules leads to similar gains as providing in-context examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peeters_R/0/1/0/all/0/1\">Ralph Peeters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bizer_C/0/1/0/all/0/1\">Christian Bizer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simulating H.P. Lovecraft horror literature with the ChatGPT large language model. (arXiv:2305.03429v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03429","description":"<p>In this paper, we present a novel approach to simulating H.P. Lovecraft's\nhorror literature using the ChatGPT large language model, specifically the\nGPT-4 architecture. Our study aims to generate text that emulates Lovecraft's\nunique writing style and themes, while also examining the effectiveness of\nprompt engineering techniques in guiding the model's output. To achieve this,\nwe curated a prompt containing several specialized literature references and\nemployed advanced prompt engineering methods. We conducted an empirical\nevaluation of the generated text by administering a survey to a sample of\nundergraduate students. Utilizing statistical hypothesis testing, we assessed\nthe students ability to distinguish between genuine Lovecraft works and those\ngenerated by our model. Our findings demonstrate that the participants were\nunable to reliably differentiate between the two, indicating the effectiveness\nof the GPT-4 model and our prompt engineering techniques in emulating\nLovecraft's literary style. In addition to presenting the GPT model's\ncapabilities, this paper provides a comprehensive description of its underlying\narchitecture and offers a comparative analysis with related work that simulates\nother notable authors and philosophers, such as Dennett. By exploring the\npotential of large language models in the context of literary emulation, our\nstudy contributes to the body of research on the applications and limitations\nof these models in various creative domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garrido_Merchan_E/0/1/0/all/0/1\">Eduardo C. Garrido-Merch&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arroyo_Barriguete_J/0/1/0/all/0/1\">Jos&#xe9; Luis Arroyo-Barrig&#xfc;ete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gozalo_Brihuela_R/0/1/0/all/0/1\">Roberto Gozalo-Brihuela</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LMs stand their Ground: Investigating the Effect of Embodiment in Figurative Language Interpretation by Language Models. (arXiv:2305.03445v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03445","description":"<p>Figurative language is a challenge for language models since its\ninterpretation is based on the use of words in a way that deviates from their\nconventional order and meaning. Yet, humans can easily understand and interpret\nmetaphors, similes or idioms as they can be derived from embodied metaphors.\nLanguage is a proxy for embodiment and if a metaphor is conventional and\nlexicalised, it becomes easier for a system without a body to make sense of\nembodied concepts. Yet, the intricate relation between embodiment and features\nsuch as concreteness or age of acquisition has not been studied in the context\nof figurative language interpretation concerning language models. Hence, the\npresented study shows how larger language models perform better at interpreting\nmetaphoric sentences when the action of the metaphorical sentence is more\nembodied. The analysis rules out multicollinearity with other features (e.g.\nword length or concreteness) and provides initial evidence that larger language\nmodels conceptualise embodied concepts to a degree that facilitates figurative\nlanguage understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wicke_P/0/1/0/all/0/1\">Philipp Wicke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering. (arXiv:2305.03453v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03453","description":"<p>Large Language Models (LLMs) have recently demonstrated exceptional\nperformance in various Natural Language Processing (NLP) tasks. They have also\nshown the ability to perform chain-of-thought (CoT) reasoning to solve complex\nproblems. Recent studies have explored CoT reasoning in complex multimodal\nscenarios, such as the science question answering task, by fine-tuning\nmultimodal models with high-quality human-annotated CoT rationales. However,\ncollecting high-quality COT rationales is usually time-consuming and costly.\nBesides, the annotated rationales are hardly accurate due to the redundant\ninformation involved or the essential information missed. To address these\nissues, we propose a novel method termed \\emph{T-SciQ} that aims at teaching\nscience question answering with LLM signals. The T-SciQ approach generates\nhigh-quality CoT rationales as teaching signals and is advanced to train much\nsmaller models to perform CoT reasoning in complex modalities. Additionally, we\nintroduce a novel data mixing strategy to produce more effective teaching data\nsamples for simple and complex science question answer problems. Extensive\nexperimental results show that our T-SciQ method achieves a new\nstate-of-the-art performance on the ScienceQA benchmark, with an accuracy of\n96.18%. Moreover, our approach outperforms the most powerful fine-tuned\nbaseline by 4.5%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiabang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Heng Tao Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Graph Representation Learning for Answering Hybrid Numerical Reasoning Question. (arXiv:2305.03458v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03458","description":"<p>Hybrid question answering (HybridQA) over the financial report contains both\ntextual and tabular data, and requires the model to select the appropriate\nevidence for the numerical reasoning task. Existing methods based on\nencoder-decoder framework employ a expression tree-based decoder to solve\nnumerical reasoning problems. However, encoders rely more on Machine Reading\nComprehension (MRC) methods, which take table serialization and text splicing\nas input, damaging the granularity relationship between table and text as well\nas the spatial structure information of table itself. In order to solve these\nproblems, the paper proposes a Multi-View Graph (MVG) Encoder to take the\nrelations among the granularity into account and capture the relations from\nmultiple view. By utilizing MVGE as a module, we constuct Tabular View,\nRelation View and Numerical View which aim to retain the original\ncharacteristics of the hybrid data. We validate our model on the publicly\navailable table-text hybrid QA benchmark (TAT-QA) and outperform the\nstate-of-the-art model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yifan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_F/0/1/0/all/0/1\">Fangyu Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanzhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Acquisition of Fine-grained Visual Concepts by Exploiting Semantics of Generic Characterizations in Discourse. (arXiv:2305.03461v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03461","description":"<p>Interactive Task Learning (ITL) concerns learning about unforeseen domain\nconcepts via natural interactions with human users. The learner faces a number\nof significant constraints: learning should be online, incremental and\nfew-shot, as it is expected to perform tangible belief updates right after\nnovel words denoting unforeseen concepts are introduced. In this work, we\nexplore a challenging symbol grounding task--discriminating among object\nclasses that look very similar--within the constraints imposed by ITL. We\ndemonstrate empirically that more data-efficient grounding results from\nexploiting the truth-conditions of the teacher's generic statements (e.g., \"Xs\nhave attribute Z.\") and their implicatures in context (e.g., as an answer to\n\"How are Xs and Ys different?\", one infers Y lacks attribute Z).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jonghyuk Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lascarides_A/0/1/0/all/0/1\">Alex Lascarides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamoorthy_S/0/1/0/all/0/1\">Subramanian Ramamoorthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search. (arXiv:2305.03495v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03495","description":"<p>Large Language Models (LLMs) have shown impressive performance as general\npurpose agents, but their abilities remain highly dependent on prompts which\nare hand written with onerous trial-and-error effort. We propose a simple and\nnonparametric solution to this problem, Automatic Prompt Optimization (APO),\nwhich is inspired by numerical gradient descent to automatically improve\nprompts, assuming access to training data and an LLM API. The algorithm uses\nminibatches of data to form natural language ``gradients'' that criticize the\ncurrent prompt. The gradients are then ``propagated'' into the prompt by\nediting the prompt in the opposite semantic direction of the gradient. These\ngradient descent steps are guided by a beam search and bandit selection\nprocedure which significantly improves algorithmic efficiency. Preliminary\nresults across three benchmark NLP tasks and the novel problem of LLM jailbreak\ndetection suggest that Automatic Prompt Optimization can outperform prior\nprompt editing techniques and improve an initial prompt's performance by up to\n31\\%, by using data to rewrite vague task descriptions into more precise\nannotation instructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1\">Reid Pryzant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iter_D/0/1/0/all/0/1\">Dan Iter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jerry Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yin Tat Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Natural Language Processing Models on Encrypted Text for Enhanced Privacy. (arXiv:2305.03497v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03497","description":"<p>With the increasing use of cloud-based services for training and deploying\nmachine learning models, data privacy has become a major concern. This is\nparticularly important for natural language processing (NLP) models, which\noften process sensitive information such as personal communications and\nconfidential documents. In this study, we propose a method for training NLP\nmodels on encrypted text data to mitigate data privacy concerns while\nmaintaining similar performance to models trained on non-encrypted data. We\ndemonstrate our method using two different architectures, namely\nDoc2Vec+XGBoost and Doc2Vec+LSTM, and evaluate the models on the 20 Newsgroups\ndataset. Our results indicate that both encrypted and non-encrypted models\nachieve comparable performance, suggesting that our encryption method is\neffective in preserving data privacy without sacrificing model accuracy. In\norder to replicate our experiments, we have provided a Colab notebook at the\nfollowing address: https://t.ly/lR-TP\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tasar_D/0/1/0/all/0/1\">Davut Emre Tasar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tasar_C/0/1/0/all/0/1\">Ceren Ocal Tasar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cancer Hallmark Classification Using Bidirectional Encoder Representations From Transformers. (arXiv:2305.03501v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03501","description":"<p>This paper presents a novel approach to accurately classify the hallmarks of\ncancer, which is a crucial task in cancer research. Our proposed method\nutilizes the Bidirectional Encoder Representations from Transformers (BERT)\narchitecture, which has shown exceptional performance in various downstream\napplications. By applying transfer learning, we fine-tuned the pre-trained BERT\nmodel on a small corpus of biomedical text documents related to cancer. The\noutcomes of our experimental investigations demonstrate that our approach\nattains a noteworthy accuracy of 94.45%, surpassing almost all prior findings\nwith a substantial increase of at least 8.04% as reported in the literature.\nThese findings highlight the effectiveness of our proposed model in accurately\nclassifying and comprehending text documents for cancer research, thus\ncontributing significantly to the field. As cancer remains one of the top ten\nleading causes of death globally, our approach holds great promise in advancing\ncancer research and improving patient outcomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zavrak_S/0/1/0/all/0/1\">Sultan Zavrak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_S/0/1/0/all/0/1\">Seyhmus Yilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explore the difficulty of words and its influential attributes based on the Wordle game. (arXiv:2305.03502v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03502","description":"<p>We adopt the distribution and expectation of guessing times in game Wordle as\nmetrics to predict the difficulty of words and explore their influence factors.\nIn order to predictthe difficulty distribution, we use Monte Carlo to simulate\nthe guessing process of players and then narrow the gap between raw and actual\ndistribution of guessing times for each word with Markov which generates the\nassociativity of words. Afterwards, we take advantage of lasso regression to\npredict the deviation of guessing times expectation and quadratic programming\nto obtain the correction of the original distribution.To predict the difficulty\nlevels, we first use hierarchical clustering to classify the difficulty levels\nbased on the expectation of guessing times. Afterwards we downscale the\nvariables of lexical attributes based on factor analysis. Significant factors\ninclude the number of neighboring words, letter similarity, sub-string\nsimilarity, and word frequency. Finally, we build the relationship between\nlexical attributes and difficulty levels through ordered logistic regression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Beibei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanfang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Think Rationally about What You See: Continuous Rationale Extraction for Relation Extraction. (arXiv:2305.03503v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03503","description":"<p>Relation extraction (RE) aims to extract potential relations according to the\ncontext of two entities, thus, deriving rational contexts from sentences plays\nan important role. Previous works either focus on how to leverage the entity\ninformation (e.g., entity types, entity verbalization) to inference relations,\nbut ignore context-focused content, or use counterfactual thinking to remove\nthe model's bias of potential relations in entities, but the relation reasoning\nprocess will still be hindered by irrelevant content. Therefore, how to\npreserve relevant content and remove noisy segments from sentences is a crucial\ntask. In addition, retained content needs to be fluent enough to maintain\nsemantic coherence and interpretability. In this work, we propose a novel\nrationale extraction framework named RE2, which leverages two continuity and\nsparsity factors to obtain relevant and coherent rationales from sentences. To\nsolve the problem that the gold rationales are not labeled, RE2 applies an\noptimizable binary mask to each token in the sentence, and adjust the\nrationales that need to be selected according to the relation label.\nExperiments on four datasets show that RE2 surpasses baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1\">Zhaochen Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SI-LSTM: Speaker Hybrid Long-short Term Memory and Cross Modal Attention for Emotion Recognition in Conversation. (arXiv:2305.03506v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03506","description":"<p>Emotion Recognition in Conversation~(ERC) across modalities is of vital\nimportance for a variety of applications, including intelligent healthcare,\nartificial intelligence for conversation, and opinion mining over chat history.\nThe crux of ERC is to model both cross-modality and cross-time interactions\nthroughout the conversation. Previous methods have made progress in learning\nthe time series information of conversation while lacking the ability to trace\ndown the different emotional states of each speaker in a conversation. In this\npaper, we propose a recurrent structure called Speaker Information Enhanced\nLong-Short Term Memory (SI-LSTM) for the ERC task, where the emotional states\nof the distinct speaker can be tracked in a sequential way to enhance the\nlearning of the emotion in conversation. Further, to improve the learning of\nmultimodal features in ERC, we utilize a cross-modal attention component to\nfuse the features between different modalities and model the interaction of the\nimportant information from different modalities. Experimental results on two\nbenchmark datasets demonstrate the superiority of the proposed SI-LSTM against\nthe state-of-the-art baseline methods in the ERC task on multimodal data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xingwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">You Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Read it Twice: Towards Faithfully Interpretable Fact Verification by Revisiting Evidence. (arXiv:2305.03507v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03507","description":"<p>Real-world fact verification task aims to verify the factuality of a claim by\nretrieving evidence from the source document. The quality of the retrieved\nevidence plays an important role in claim verification. Ideally, the retrieved\nevidence should be faithful (reflecting the model's decision-making process in\nclaim verification) and plausible (convincing to humans), and can improve the\naccuracy of verification task. Although existing approaches leverage the\nsimilarity measure of semantic or surface form between claims and documents to\nretrieve evidence, they all rely on certain heuristics that prevent them from\nsatisfying all three requirements. In light of this, we propose a fact\nverification model named ReRead to retrieve evidence and verify claim that: (1)\nTrain the evidence retriever to obtain interpretable evidence (i.e.,\nfaithfulness and plausibility criteria); (2) Train the claim verifier to\nrevisit the evidence retrieved by the optimized evidence retriever to improve\nthe accuracy. The proposed system is able to achieve significant improvements\nupon best-reported models under different settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1\">Zhaochen Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhijiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CiteCaseLAW: Citation Worthiness Detection in Caselaw for Legal Assistive Writing. (arXiv:2305.03508v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03508","description":"<p>In legal document writing, one of the key elements is properly citing the\ncase laws and other sources to substantiate claims and arguments. Understanding\nthe legal domain and identifying appropriate citation context or cite-worthy\nsentences are challenging tasks that demand expensive manual annotation. The\npresence of jargon, language semantics, and high domain specificity makes legal\nlanguage complex, making any associated legal task hard for automation. The\ncurrent work focuses on the problem of citation-worthiness identification. It\nis designed as the initial step in today's citation recommendation systems to\nlighten the burden of extracting an adequate set of citation contexts. To\naccomplish this, we introduce a labeled dataset of 178M sentences for\ncitation-worthiness detection in the legal domain from the Caselaw Access\nProject (CAP). The performance of various deep learning models was examined on\nthis novel dataset. The domain-specific pre-trained model tends to outperform\nother models, with an 88% F1-score for the citation-worthiness detection task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khatri_M/0/1/0/all/0/1\">Mann Khatri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadhwa_P/0/1/0/all/0/1\">Pritish Wadhwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satija_G/0/1/0/all/0/1\">Gitansh Satija</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheik_R/0/1/0/all/0/1\">Reshma Sheik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_Y/0/1/0/all/0/1\">Yaman Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1\">Ponnurangam Kumaraguru</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion. (arXiv:2305.03509v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03509","description":"<p>Diffusion-based generative models' impressive ability to create convincing\nimages has captured global attention. However, their complex internal\nstructures and operations often make them difficult for non-experts to\nunderstand. We present Diffusion Explainer, the first interactive visualization\ntool that explains how Stable Diffusion transforms text prompts into images.\nDiffusion Explainer tightly integrates a visual overview of Stable Diffusion's\ncomplex components with detailed explanations of their underlying operations,\nenabling users to fluidly transition between multiple levels of abstraction\nthrough animations and interactive elements. By comparing the evolutions of\nimage representations guided by two related text prompts over refinement\ntimesteps, users can discover the impact of prompts on image generation.\nDiffusion Explainer runs locally in users' web browsers without the need for\ninstallation or specialized hardware, broadening the public's education access\nto modern AI techniques. Our open-sourced tool is available at:\nhttps://poloclub.github.io/diffusion-explainer/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seongmin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoover_B/0/1/0/all/0/1\">Benjamin Hoover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1\">Hendrik Strobelt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijie J. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">ShengYun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wright_A/0/1/0/all/0/1\">Austin Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kevin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Haekyu Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haoyang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1\">Duen Horng Chau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment. (arXiv:2305.03510v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03510","description":"<p>Pre-trained vision and language models such as CLIP have witnessed remarkable\nsuccess in connecting images and texts with a primary focus on English texts.\nDespite recent efforts to extend CLIP to support other languages, disparities\nin performance among different languages have been observed due to uneven\nresource availability. Additionally, current cross-lingual transfer methods of\nthose pre-trained models would consume excessive resources for a large number\nof languages. Therefore, we propose a new parameter-efficient cross-lingual\ntransfer learning framework that utilizes a translation-based alignment method\nto mitigate multilingual disparities and explores parameter-efficient\nfine-tuning methods for parameter-efficient cross-lingual transfer. Extensive\nexperiments on XTD and Multi30K datasets, covering 11 languages under\nzero-shot, few-shot, and full-dataset learning scenarios, show that our\nframework significantly reduces the multilingual disparities among languages\nand improves cross-lingual transfer results, especially in low-resource\nscenarios, while only keeping and fine-tuning an extremely small number of\nparameters compared to the full model (e.g., Our framework only requires 0.16\\%\nadditional parameters of a full-model for each language in the few-shot\nlearning scenario).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jialu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shared Latent Space by Both Languages in Non-Autoregressive Neural Machine Translation. (arXiv:2305.03511v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03511","description":"<p>Latent variable modeling in non-autoregressive neural machine translation\n(NAT) is a promising approach to mitigate the multimodality problem. In the\nprevious works, they added an auxiliary model to estimate the posterior\ndistribution of the latent variable conditioned on the source and target\nsentences. However, it causes several disadvantages, such as redundant\ninformation extraction in the latent variable, increasing parameters, and a\ntendency to ignore a part of the information from the inputs. In this paper, we\npropose a new latent variable modeling that is based on a dual reconstruction\nperspective and an advanced hierarchical latent modeling approach. Our proposed\nmethod, {\\em LadderNMT}, shares a latent space across both languages so that it\nhypothetically alleviates or solves the above disadvantages. Experimental\nresults quantitatively and qualitatively demonstrate that our proposed latent\nvariable modeling learns an advantageous latent space and significantly\nimproves translation quality in WMT translation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heo_D/0/1/0/all/0/1\">DongNyeong Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Heeyoul Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Multimodal AI Chatbots. (arXiv:2305.03512v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03512","description":"<p>This work aims to create a multimodal AI system that chats with humans and\nshares relevant photos. While earlier works were limited to dialogues about\nspecific objects or scenes within images, recent works have incorporated images\ninto open-domain dialogues. However, their response generators are unimodal,\naccepting text input but no image input, thus prone to generating responses\ncontradictory to the images shared in the dialogue. Therefore, this work\nproposes a complete chatbot system using two multimodal deep learning models:\nan image retriever that understands texts and a response generator that\nunderstands images. The image retriever, implemented by ViT and BERT, selects\nthe most relevant image given the dialogue history and a database of images.\nThe response generator, implemented by ViT and GPT-2/DialoGPT, generates an\nappropriate response given the dialogue history and the most recently retrieved\nimage. The two models are trained and evaluated on PhotoChat, an open-domain\ndialogue dataset in which a photo is shared in each session. In automatic\nevaluation, the proposed image retriever outperforms existing baselines VSE++\nand SCAN with Recall@1/5/10 of 0.1/0.3/0.4 and MRR of 0.2 when ranking 1,000\nimages. The proposed response generator also surpasses the baseline Divter with\nPPL of 16.9, BLEU-1/2 of 0.13/0.03, and Distinct-1/2 of 0.97/0.86, showing a\nsignificant improvement in PPL by -42.8 and BLEU-1/2 by +0.07/0.02. In human\nevaluation with a Likert scale of 1-5, the complete multimodal chatbot system\nreceives higher image-groundedness of 4.3 and engagingness of 4.3, along with\ncompetitive fluency of 4.1, coherence of 3.9, and humanness of 3.1, when\ncompared to other chatbot variants. The source code is available at:\nhttps://github.com/minniie/multimodal_chat.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Min Young Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs. (arXiv:2305.03513v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03513","description":"<p>ChatGPT, as a recently launched large language model (LLM), has shown\nsuperior performance in various natural language processing (NLP) tasks.\nHowever, two major limitations hinder its potential applications: (1) the\ninflexibility of finetuning on downstream tasks and (2) the lack of\ninterpretability in the decision-making process. To tackle these limitations,\nwe propose a novel framework that leverages the power of ChatGPT for specific\ntasks, such as text classification, while improving its interpretability. The\nproposed framework conducts a knowledge graph extraction task to extract\nrefined and structural knowledge from the raw data using ChatGPT. The rich\nknowledge is then converted into a graph, which is further used to train an\ninterpretable linear classifier to make predictions. To evaluate the\neffectiveness of our proposed method, we conduct experiments on four datasets.\nThe result shows that our method can significantly improve the performance\ncompared to directly utilizing ChatGPT for text classification tasks. And our\nmethod provides a more transparent decision-making process compared with\nprevious text classification methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yucheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hehuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wenliang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_G/0/1/0/all/0/1\">Gengchen Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Large Language Models Transform Computational Social Science?. (arXiv:2305.03514v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03514","description":"<p>Large Language Models (LLMs) like ChatGPT are capable of successfully\nperforming many language processing tasks zero-shot (without the need for\ntraining data). If this capacity also applies to the coding of social phenomena\nlike persuasiveness and political ideology, then LLMs could effectively\ntransform Computational Social Science (CSS). This work provides a road map for\nusing LLMs as CSS tools. Towards this end, we contribute a set of prompting\nbest practices and an extensive evaluation pipeline to measure the zero-shot\nperformance of 13 language models on 24 representative CSS benchmarks. On\ntaxonomic labeling tasks (classification), LLMs fail to outperform the best\nfine-tuned models but still achieve fair levels of agreement with humans. On\nfree-form coding tasks (generation), LLMs produce explanations that often\nexceed the quality of crowdworkers' gold references. We conclude that today's\nLLMs can radically augment the CSS research pipeline in two ways: (1) serving\nas zero-shot data annotators on human annotation teams, and (2) bootstrapping\nchallenging creative generation tasks (e.g., explaining the hidden meaning\nbehind text). In summary, LLMs can significantly reduce costs and increase\nefficiency of social science analysis in partnership with humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ziems_C/0/1/0/all/0/1\">Caleb Ziems</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_W/0/1/0/all/0/1\">William Held</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_O/0/1/0/all/0/1\">Omar Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhehao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Domain-Adaptive Visually-fused Event Detection from Text. (arXiv:2305.03517v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03517","description":"<p>Incorporating auxiliary modalities such as images into event detection models\nhas attracted increasing interest over the last few years. The complexity of\nnatural language in describing situations has motivated researchers to leverage\nthe related visual context to improve event detection performance. However,\ncurrent approaches in this area suffer from data scarcity, where a large amount\nof labelled text-image pairs are required for model training. Furthermore,\nlimited access to the visual context at inference time negatively impacts the\nperformance of such models, which makes them practically ineffective in\nreal-world scenarios. In this paper, we present a novel domain-adaptive\nvisually-fused event detection approach that can be trained on a few labelled\nimage-text paired data points. Specifically, we introduce a visual imaginator\nmethod that synthesises images from text in the absence of visual context.\nMoreover, the imaginator can be customised to a specific domain. In doing so,\nour model can leverage the capabilities of pre-trained vision-language models\nand can be trained in a few-shot setting. This also allows for effective\ninference where only single-modality data (i.e. text) is available. The\nexperimental evaluation on the benchmark M2E2 dataset shows that our model\noutperforms existing state-of-the-art models, by up to 11 points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shiri_F/0/1/0/all/0/1\">Fatemeh Shiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moghimifar_F/0/1/0/all/0/1\">Farhad Moghimifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_R/0/1/0/all/0/1\">Reza Haffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Black-box Prompt Tuning with Subspace Learning. (arXiv:2305.03518v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03518","description":"<p>Black-box prompt tuning uses derivative-free optimization algorithms to learn\nprompts in low-dimensional subspaces instead of back-propagating through the\nnetwork of Large Language Models (LLMs). Recent studies have found that\nblack-box prompt tuning lacks versatility across tasks and LLMs, which we\nbelieve is related to the inappropriate choice of subspaces. In this paper, we\npropose Black-box prompt tuning with Subspace Learning (BSL) to improve the\nversatility of black-box prompt tuning. Based on the assumption that nearly\noptimal prompts for similar tasks exist in a common subspace, we propose\nidentifying such subspaces by meta-learning on a set of similar source tasks.\nTherefore, for a target task that shares similarities with source tasks, we\nguarantee that optimizing in the subspace can find a prompt that performs well\non the target task. Experiments confirm that our BSL framework consistently\nachieves competitive performance regardless of downstream tasks and LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yuanhang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhixing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging BERT Language Model for Arabic Long Document Classification. (arXiv:2305.03519v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03519","description":"<p>Given the number of Arabic speakers worldwide and the notably large amount of\ncontent in the web today in some fields such as law, medicine, or even news,\ndocuments of considerable length are produced regularly. Classifying those\ndocuments using traditional learning models is often impractical since extended\nlength of the documents increases computational requirements to an\nunsustainable level. Thus, it is necessary to customize these models\nspecifically for long textual documents. In this paper we propose two simple\nbut effective models to classify long length Arabic documents. We also\nfine-tune two different models-namely, Longformer and RoBERT, for the same task\nand compare their results to our models. Both of our models outperform the\nLongformer and RoBERT in this task over two different datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+AL_Qurishi_M/0/1/0/all/0/1\">Muhammad AL-Qurishi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Aware Semantic Similarity Measurement for Unsupervised Word Sense Disambiguation. (arXiv:2305.03520v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03520","description":"<p>The issue of word sense ambiguity poses a significant challenge in natural\nlanguage processing due to the scarcity of annotated data to feed machine\nlearning models to face the challenge. Therefore, unsupervised word sense\ndisambiguation methods have been developed to overcome that challenge without\nrelying on annotated data. This research proposes a new context-aware approach\nto unsupervised word sense disambiguation, which provides a flexible mechanism\nfor incorporating contextual information into the similarity measurement\nprocess. We experiment with a popular benchmark dataset to evaluate the\nproposed strategy and compare its performance with state-of-the-art\nunsupervised word sense disambiguation techniques. The experimental results\nindicate that our approach substantially enhances disambiguation accuracy and\nsurpasses the performance of several existing techniques. Our findings\nunderscore the significance of integrating contextual information in semantic\nsimilarity measurements to manage word sense ambiguity in unsupervised\nscenarios effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Gil_J/0/1/0/all/0/1\">Jorge Martinez-Gil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-context Learning as Maintaining Coherency: A Study of On-the-fly Machine Translation Using Large Language Models. (arXiv:2305.03573v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03573","description":"<p>The phenomena of in-context learning has typically been thought of as\n\"learning from examples\". In this work which focuses on Machine Translation, we\npresent a perspective of in-context learning as the desired generation task\nmaintaining coherency with its context, i.e., the prompt examples. We first\ninvestigate randomly sampled prompts across 4 domains, and find that\ntranslation performance improves when shown in-domain prompts. Next, we\ninvestigate coherency for the in-domain setting, which uses prompt examples\nfrom a moving window. We study this with respect to other factors that have\npreviously been identified in the literature such as length, surface similarity\nand sentence embedding similarity. Our results across 3 models (GPTNeo2.7B,\nBloom3B, XGLM2.9B), and three translation directions\n(\\texttt{en}$\\rightarrow$\\{\\texttt{pt, de, fr}\\}) suggest that the long-term\ncoherency of the prompts and the test sentence is a good indicator of\ndownstream translation performance. In doing so, we demonstrate the efficacy of\nIn-context Machine Translation for on-the-fly adaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sia_S/0/1/0/all/0/1\">Suzanna Sia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duh_K/0/1/0/all/0/1\">Kevin Duh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Now It Sounds Like You: Learning Personalized Vocabulary On Device. (arXiv:2305.03584v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03584","description":"<p>In recent years, Federated Learning (FL) has shown significant advancements\nin its ability to perform various natural language processing (NLP) tasks. This\nwork focuses on applying personalized FL for on-device language modeling. Due\nto limitations of memory and latency, these models cannot support the\ncomplexity of sub-word tokenization or beam search decoding, resulting in the\ndecision to deploy a closed-vocabulary language model. However,\nclosed-vocabulary models are unable to handle out-of-vocabulary (OOV) words\nbelonging to specific users. To address this issue, We propose a novel\ntechnique called \"OOV expansion\" that improves OOV coverage and increases model\naccuracy while minimizing the impact on memory and latency. This method\nintroduces a personalized \"OOV adapter\" that effectively transfers knowledge\nfrom a central model and learns word embedding for personalized vocabulary. OOV\nexpansion significantly outperforms standard FL personalization methods on a\nset of common FL benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sid Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_A/0/1/0/all/0/1\">Ashish Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_P/0/1/0/all/0/1\">Pierce Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_J/0/1/0/all/0/1\">John Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports. (arXiv:2305.03598v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03598","description":"<p>How can we interpret and retrieve medical evidence to support clinical\ndecisions? Clinical trial reports (CTR) amassed over the years contain\nindispensable information for the development of personalized medicine.\nHowever, it is practically infeasible to manually inspect over 400,000+\nclinical trial reports in order to find the best evidence for experimental\ntreatments. Natural Language Inference (NLI) offers a potential solution to\nthis problem, by allowing the scalable computation of textual entailment.\nHowever, existing NLI models perform poorly on biomedical corpora, and\npreviously published datasets fail to capture the full complexity of inference\nover CTRs. In this work, we present a novel resource to advance research on NLI\nfor reasoning on CTRs. The resource includes two main tasks. Firstly, to\ndetermine the inference relation between a natural language statement, and a\nCTR. Secondly, to retrieve supporting facts to justify the predicted relation.\nWe provide NLI4CT, a corpus of 2400 statements and CTRs, annotated for these\ntasks. Baselines on this corpus expose the limitations of existing NLI models,\nwith 6 state-of-the-art NLI models achieving a maximum F1 score of 0.627. To\nthe best of our knowledge, we are the first to design a task that covers the\ninterpretation of full CTRs. To encourage further work on this challenging\ndataset, we make the corpus, competition leaderboard, website and code to\nreplicate the baseline experiments available at:\nhttps://github.com/ai-systems/nli4ct\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jullien_M/0/1/0/all/0/1\">Ma&#xeb;l Jullien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1\">Marco Valentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frost_H/0/1/0/all/0/1\">Hannah Frost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ORegan_P/0/1/0/all/0/1\">Paul O&#x27;Regan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landers_D/0/1/0/all/0/1\">Donal Landers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dual Semantic-Aware Recurrent Global-Adaptive Network For Vision-and-Language Navigation. (arXiv:2305.03602v1 [cs.CV])","link":"http://arxiv.org/abs/2305.03602","description":"<p>Vision-and-Language Navigation (VLN) is a realistic but challenging task that\nrequires an agent to locate the target region using verbal and visual cues.\nWhile significant advancements have been achieved recently, there are still two\nbroad limitations: (1) The explicit information mining for significant guiding\nsemantics concealed in both vision and language is still under-explored; (2)\nThe previously structured map method provides the average historical appearance\nof visited nodes, while it ignores distinctive contributions of various images\nand potent information retention in the reasoning process. This work proposes a\ndual semantic-aware recurrent global-adaptive network (DSRG) to address the\nabove problems. First, DSRG proposes an instruction-guidance linguistic module\n(IGL) and an appearance-semantics visual module (ASV) for boosting vision and\nlanguage semantic learning respectively. For the memory mechanism, a global\nadaptive aggregation module (GAA) is devised for explicit panoramic observation\nfusion, and a recurrent memory fusion module (RMF) is introduced to supply\nimplicit temporal hidden states. Extensive experimental results on the R2R and\nREVERIE datasets demonstrate that our method achieves better performance than\nexisting methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liuyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zongtao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiagui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_R/0/1/0/all/0/1\">Ronghao Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Naijia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chengju Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qijun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Curation for Image Captioning with Text-to-Image Generative Models. (arXiv:2305.03610v1 [cs.CV])","link":"http://arxiv.org/abs/2305.03610","description":"<p>Recent advances in image captioning are mainly driven by large-scale\nvision-language pretraining, relying heavily on computational resources and\nincreasingly large multimodal datasets. Instead of scaling up pretraining data,\nwe ask whether it is possible to improve performance by improving the quality\nof the samples in existing datasets. We pursue this question through two\napproaches to data curation: one that assumes that some examples should be\navoided due to mismatches between the image and caption, and one that assumes\nthat the mismatch can be addressed by replacing the image, for which we use the\nstate-of-the-art Stable Diffusion model. These approaches are evaluated using\nthe BLIP model on MS COCO and Flickr30K in both finetuning and few-shot\nlearning settings. Our simple yet effective approaches consistently outperform\nbaselines, indicating that better image captioning models can be trained by\ncurating existing resources. Finally, we conduct a human study to understand\nthe errors made by the Stable Diffusion model and highlight directions for\nfuture work in text-to-image generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotz_J/0/1/0/all/0/1\">Jonas F. Lotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_C/0/1/0/all/0/1\">Chen Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1\">Desmond Elliott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs. (arXiv:2305.03642v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03642","description":"<p>Results from Randomized Controlled Trials (RCTs) establish the comparative\neffectiveness of interventions, and are in turn critical inputs for\nevidence-based care. However, results from RCTs are presented in (often\nunstructured) natural language articles describing the design, execution, and\noutcomes of trials; clinicians must manually extract findings pertaining to\ninterventions and outcomes of interest from such articles. This onerous manual\nprocess has motivated work on (semi-)automating extraction of structured\nevidence from trial reports. In this work we propose and evaluate a\ntext-to-text model built on instruction-tuned Large Language Models (LLMs) to\njointly extract Interventions, Outcomes, and Comparators (ICO elements) from\nclinical abstracts, and infer the associated results reported. Manual (expert)\nand automated evaluations indicate that framing evidence extraction as a\nconditional generation task and fine-tuning LLMs for this purpose realizes\nconsiderable ($\\sim$20 point absolute F1 score) gains over the previous SOTA.\nWe perform ablations and error analyses to assess aspects that contribute to\nmodel performance, and to highlight potential directions for further\nimprovements. We apply our model to a collection of published RCTs through\nmid-2022, and release a searchable database of structured findings (anonymously\nfor now): bit.ly/joint-relations-extraction-mlhc\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wadhwa_S/0/1/0/all/0/1\">Somin Wadhwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeYoung_J/0/1/0/all/0/1\">Jay DeYoung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nye_B/0/1/0/all/0/1\">Benjamin Nye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amir_S/0/1/0/all/0/1\">Silvio Amir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"White-Box Multi-Objective Adversarial Attack on Dialogue Generation. (arXiv:2305.03655v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03655","description":"<p>Pre-trained transformers are popular in state-of-the-art dialogue generation\n(DG) systems. Such language models are, however, vulnerable to various\nadversarial samples as studied in traditional tasks such as text\nclassification, which inspires our curiosity about their robustness in DG\nsystems. One main challenge of attacking DG models is that perturbations on the\ncurrent sentence can hardly degrade the response accuracy because the unchanged\nchat histories are also considered for decision-making. Instead of merely\npursuing pitfalls of performance metrics such as BLEU, ROUGE, we observe that\ncrafting adversarial samples to force longer generation outputs benefits attack\neffectiveness -- the generated responses are typically irrelevant, lengthy, and\nrepetitive. To this end, we propose a white-box multi-objective attack method\ncalled DGSlow. Specifically, DGSlow balances two objectives -- generation\naccuracy and length, via a gradient-based multi-objective optimizer and applies\nan adaptive searching mechanism to iteratively craft adversarial samples with\nonly a few modifications. Comprehensive experiments on four benchmark datasets\ndemonstrate that DGSlow could significantly degrade state-of-the-art DG models\nwith a higher success rate than traditional accuracy-based methods. Besides,\nour crafted sentences also exhibit strong transferability in attacking other\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yufei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zexin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yingfan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT models. (arXiv:2305.03660v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03660","description":"<p>We propose Retrieval Augmented Generation (RAG) as an approach for automated\nradiology report writing that leverages multimodally aligned embeddings from a\ncontrastively pretrained vision language model for retrieval of relevant\ncandidate radiology text for an input radiology image and a general domain\ngenerative model like OpenAI text-davinci-003, gpt-3.5-turbo and gpt-4 for\nreport generation using the relevant radiology text retrieved. This approach\nkeeps hallucinated generations under check and provides capabilities to\ngenerate report content in the format we desire leveraging the instruction\nfollowing capabilities of these generative models. Our approach achieves better\nclinical metrics with a BERTScore of 0.2865 ({\\Delta}+ 25.88%) and Semb score\nof 0.4026 ({\\Delta}+ 6.31%). Our approach can be broadly relevant for different\nclinical settings as it allows to augment the automated radiology report\ngeneration process with content relevant for that setting while also having the\nability to inject user intents and requirements in the prompts as part of the\nreport generation process to modulate the content and format of the generated\nreports as applicable for that clinical setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ranjit_M/0/1/0/all/0/1\">Mercy Ranjit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganapathy_G/0/1/0/all/0/1\">Gopinath Ganapathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manuel_R/0/1/0/all/0/1\">Ranjit Manuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganu_T/0/1/0/all/0/1\">Tanuja Ganu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting COVID-19 and pneumonia complications from admission texts. (arXiv:2305.03661v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03661","description":"<p>In this paper we present a novel approach to risk assessment for patients\nhospitalized with pneumonia or COVID-19 based on their admission reports. We\napplied a Longformer neural network to admission reports and other textual data\navailable shortly after admission to compute risk scores for the patients. We\nused patient data of multiple European hospitals to demonstrate that our\napproach outperforms the Transformer baselines. Our experiments show that the\nproposed model generalises across institutions and diagnoses. Also, our method\nhas several other advantages described in the paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Umerenkov_D/0/1/0/all/0/1\">Dmitriy Umerenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherkashin_O/0/1/0/all/0/1\">Oleg Cherkashin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nesterov_A/0/1/0/all/0/1\">Alexander Nesterov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gombolevskiy_V/0/1/0/all/0/1\">Victor Gombolevskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demko_I/0/1/0/all/0/1\">Irina Demko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yalunin_A/0/1/0/all/0/1\">Alexander Yalunin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kokh_V/0/1/0/all/0/1\">Vladimir Kokh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding. (arXiv:2305.03668v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03668","description":"<p>Webpages have been a rich, scalable resource for vision-language and language\nonly tasks. Yet only pieces of webpages are kept: image-caption pairs, long\ntext articles, or raw HTML, never all in one place. Webpage tasks have\nresultingly received little attention and structured image-text data left\nunderused. To study multimodal webpage understanding, we introduce the\nWikipedia Webpage suite (WikiWeb2M) of 2M pages. We verify its utility on three\ngenerative tasks: page description generation, section summarization, and\ncontextual image captioning. We design a novel attention mechanism Prefix\nGlobal, which selects the most relevant image and text content as global tokens\nto attend to the rest of the webpage for context. By using page structure to\nseparate such tokens, it performs better than full attention with lower\ncomputational complexity. Experiments show that the new annotations from\nWikiWeb2M improve task performance compared to data from prior work. We also\ninclude ablations on sequence length, input features, and model size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burns_A/0/1/0/all/0/1\">Andrea Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_K/0/1/0/all/0/1\">Krishna Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_G/0/1/0/all/0/1\">Geoff Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1\">Bryan A. Plummer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jianmo Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mandy Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAMO-NLP at SemEval-2023 Task 2: A Unified Retrieval-augmented System for Multilingual Named Entity Recognition. (arXiv:2305.03688v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03688","description":"<p>The MultiCoNER \\RNum{2} shared task aims to tackle multilingual named entity\nrecognition (NER) in fine-grained and noisy scenarios, and it inherits the\nsemantic ambiguity and low-context setting of the MultiCoNER \\RNum{1} task. To\ncope with these problems, the previous top systems in the MultiCoNER \\RNum{1}\neither incorporate the knowledge bases or gazetteers. However, they still\nsuffer from insufficient knowledge, limited context length, single retrieval\nstrategy. In this paper, our team \\textbf{DAMO-NLP} proposes a unified\nretrieval-augmented system (U-RaNER) for fine-grained multilingual NER. We\nperform error analysis on the previous top systems and reveal that their\nperformance bottleneck lies in insufficient knowledge. Also, we discover that\nthe limited context length causes the retrieval knowledge to be invisible to\nthe model. To enhance the retrieval context, we incorporate the entity-centric\nWikidata knowledge base, while utilizing the infusion approach to broaden the\ncontextual scope of the model. Also, we explore various search strategies and\nrefine the quality of retrieval knowledge. Our system\\footnote{We will release\nthe dataset, code, and scripts of our system at {\\small\n\\url{https://github.com/modelscope/AdaSeq/tree/master/examples/U-RaNER}}.} wins\n9 out of 13 tracks in the MultiCoNER \\RNum{2} shared task. Additionally, we\ncompared our system with ChatGPT, one of the large language models which have\nunlocked strong capabilities on many tasks. The results show that there is\nstill much room for improvement for ChatGPT on the extraction task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zeqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zixia Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jiong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weiming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements. (arXiv:2305.03695v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03695","description":"<p>Despite the much discussed capabilities of today's language models, they are\nstill prone to silly and unexpected commonsense failures. We consider a\nretrospective verification approach that reflects on the correctness of LM\noutputs, and introduce Vera, a general-purpose model that estimates the\nplausibility of declarative statements based on commonsense knowledge. Trained\non ~7M commonsense statements created from 19 QA datasets and two large-scale\nknowledge bases, and with a combination of three training objectives, Vera is a\nversatile model that effectively separates correct from incorrect statements\nacross diverse commonsense domains. When applied to solving commonsense\nproblems in the verification format, Vera substantially outperforms existing\nmodels that can be repurposed for commonsense verification, and it further\nexhibits generalization capabilities to unseen tasks and provides\nwell-calibrated outputs. We find that Vera excels at filtering LM-generated\ncommonsense knowledge and is useful in detecting erroneous commonsense\nstatements generated by models like ChatGPT in real-world settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiacheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenya Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dianzhuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models in Ambulatory Devices for Home Health Diagnostics: A case study of Sickle Cell Anemia Management. (arXiv:2305.03715v1 [cs.CL])","link":"http://arxiv.org/abs/2305.03715","description":"<p>This study investigates the potential of an ambulatory device that\nincorporates Large Language Models (LLMs) in cadence with other specialized ML\nmodels to assess anemia severity in sickle cell patients in real time. The\ndevice would rely on sensor data that measures angiogenic material levels to\nassess anemia severity, providing real-time information to patients and\nclinicians to reduce the frequency of vaso-occlusive crises because of the\nearly detection of anemia severity, allowing for timely interventions and\npotentially reducing the likelihood of serious complications. The main\nchallenges in developing such a device are the creation of a reliable\nnon-invasive tool for angiogenic level assessment, a biophysics model and the\npractical consideration of an LLM communicating with emergency personnel on\nbehalf of an incapacitated patient. A possible system is proposed, and the\nlimitations of this approach are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ogundare_O/0/1/0/all/0/1\">Oluwatosin Ogundare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sofolahan_S/0/1/0/all/0/1\">Subuola Sofolahan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Otter: A Multi-Modal Model with In-Context Instruction Tuning. (arXiv:2305.03726v1 [cs.CV])","link":"http://arxiv.org/abs/2305.03726","description":"<p>Large language models (LLMs) have demonstrated significant universal\ncapabilities as few/zero-shot learners in various tasks due to their\npre-training on vast amounts of text data, as exemplified by GPT-3, which\nboosted to InstrctGPT and ChatGPT, effectively following natural language\ninstructions to accomplish real-world tasks. In this paper, we propose to\nintroduce instruction tuning into multi-modal models, motivated by the Flamingo\nmodel's upstream interleaved format pretraining dataset. We adopt a similar\napproach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT)\ndataset. We then introduce Otter, a multi-modal model based on OpenFlamingo\n(open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and\nshowcasing improved instruction-following ability and in-context learning. We\nalso optimize OpenFlamingo's implementation for researchers, democratizing the\nrequired training resources from 1$\\times$ A100 GPU to 4$\\times$ RTX-3090 GPUs,\nand integrate both OpenFlamingo and Otter into Huggingface Transformers for\nmore researchers to incorporate the models into their customized training and\ninference pipelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liangyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinghao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AVATAR: A Parallel Corpus for Java-Python Program Translation. (arXiv:2108.11590v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2108.11590","description":"<p>Program translation refers to migrating source code from one programming\nlanguage to another. It has tremendous practical value in software development,\nas porting software across languages is time-consuming and costly. Automating\nprogram translation is of paramount importance in software migration, and\nrecently researchers explored unsupervised approaches due to the unavailability\nof parallel corpora. However, the availability of pre-trained language models\nfor programming languages enables supervised fine-tuning with a small number of\nlabeled examples. Therefore, we present AVATAR, a collection of 9,515\nprogramming problems and their solutions written in two popular languages, Java\nand Python. AVATAR is collected from competitive programming sites, online\nplatforms, and open-source repositories. Furthermore, AVATAR includes unit\ntests for 250 examples to facilitate functional correctness evaluation. We\nbenchmark several pre-trained language models fine-tuned on AVATAR. Experiment\nresults show that the models lack in generating functionally accurate code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tushar_M/0/1/0/all/0/1\">Md Golam Rahman Tushar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Saikat Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LOGEN: Few-shot Logical Knowledge-Conditioned Text Generation with Self-training. (arXiv:2112.01404v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.01404","description":"<p>Natural language generation from structured data mainly focuses on\nsurface-level descriptions, suffering from uncontrollable content selection and\nlow fidelity. Previous works leverage logical forms to facilitate logical\nknowledge-conditioned text generation. Though achieving remarkable progress,\nthey are data-hungry, which makes the adoption for real-world applications\nchallenging with limited data. To this end, this paper proposes a unified\nframework for logical knowledge-conditioned text generation in the few-shot\nsetting. With only a few seeds logical forms (e.g., 20/100 shot), our approach\nleverages self-training and samples pseudo logical forms based on content and\nstructure consistency. Experimental results demonstrate that our approach can\nobtain better few-shot performance than baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiacheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Discriminative Representations and Decision Boundaries for Open Intent Detection. (arXiv:2203.05823v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.05823","description":"<p>Open intent detection is a significant problem in natural language\nunderstanding, which aims to identify the unseen open intent while ensuring\nknown intent identification performance. However, current methods face two\nmajor challenges. Firstly, they struggle to learn friendly representations to\ndetect the open intent with prior knowledge of only known intents. Secondly,\nthere is a lack of an effective approach to obtaining specific and compact\ndecision boundaries for known intents. To address these issues, this paper\npresents an original framework called DA-ADB, which successively learns\ndistance-aware intent representations and adaptive decision boundaries for open\nintent detection. Specifically, we first leverage distance information to\nenhance the distinguishing capability of the intent representations. Then, we\ndesign a novel loss function to obtain appropriate decision boundaries by\nbalancing both empirical and open space risks. Extensive experiments\ndemonstrate the effectiveness of the proposed distance-aware and boundary\nlearning strategies. Compared to state-of-the-art methods, our framework\nachieves substantial improvements on three benchmark datasets. Furthermore, it\nyields robust performance with varying proportions of labeled data and known\ncategories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shaojie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qianrui Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are We Really Making Much Progress? Bag-of-Words vs. Sequence vs. Graph vs. Hierarchy for Single- and Multi-Label Text Classification. (arXiv:2204.03954v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.03954","description":"<p>The popularity of graph neural networks has triggered a resurgence of\ngraph-based methods for single-label and multi-label text classification.\nHowever, it is unclear whether these graph-based methods are beneficial\ncompared to standard machine learning methods and modern pretrained language\nmodels. We compare a rich selection of bag-of-words, sequence-based,\ngraph-based, and hierarchical methods for text classification. We aggregate\nresults from the literature over 5 single-label and 7 multi-label datasets and\nrun our own experiments. Our findings unambiguously demonstrate that for\nsingle-label and multi-label classification tasks, the graph-based methods fail\nto outperform fine-tuned language models and sometimes even perform worse than\nstandard machine learning methods like multilayer perceptron (MLP) on a\nbag-of-words. This questions the enormous amount of effort put into the\ndevelopment of new graph-based methods in the last years and the promises they\nmake for text classification. Given our extensive experiments, we confirm that\npretrained language models remain state-of-the-art in text classification\ndespite all recent specialized advances. We argue that future work in text\nclassification should thoroughly test against strong baselines like MLPs to\nproperly assess the true scientific progress.\n</p>\n<p>The source code is available: https://github.com/drndr/multilabel-text-clf\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galke_L/0/1/0/all/0/1\">Lukas Galke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diera_A/0/1/0/all/0/1\">Andor Diera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bao Xin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khera_B/0/1/0/all/0/1\">Bhakti Khera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuser_T/0/1/0/all/0/1\">Tim Meuser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_T/0/1/0/all/0/1\">Tushar Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karl_F/0/1/0/all/0/1\">Fabian Karl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Lose Yourself! Empathetic Response Generation via Explicit Self-Other Awareness. (arXiv:2210.03884v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03884","description":"<p>As a critical step to achieve human-like chatbots, empathetic response\ngeneration has attained increasing interests. Previous attempts are incomplete\nand not sufficient enough to elicit empathy because they only focus on the\ninitial aspect of empathy to automatically mimic the feelings and thoughts of\nthe user via other-awareness. However, they ignore to maintain and take the own\nviews of the system into account, which is a crucial process to achieve the\nempathy called self-other awareness. To this end, we propose to generate\nEmpathetic response with explicit Self-Other Awareness (EmpSOA). Specifically,\nthree stages, self-other differentiation, self-other modulation and self-other\ngeneration, are devised to clearly maintain, regulate and inject the self-other\naware information into the process of empathetic response generation. Both\nautomatic and human evaluations on the benchmark dataset demonstrate the\nsuperiority of EmpSOA to generate more empathetic responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weixiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Event Coding Pipeline with Prompt Entailment. (arXiv:2210.05257v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05257","description":"<p>For monitoring crises, political events are extracted from the news. The\nlarge amount of unstructured full-text event descriptions makes a case-by-case\nanalysis unmanageable, particularly for low-resource humanitarian aid\norganizations. This creates a demand to classify events into event types, a\ntask referred to as event coding. Typically, domain experts craft an event type\nontology, annotators label a large dataset and technical experts develop a\nsupervised coding system. In this work, we propose PR-ENT, a new event coding\napproach that is more flexible and resource-efficient, while maintaining\ncompetitive accuracy: first, we extend an event description such as \"Military\ninjured two civilians'' by a template, e.g. \"People were [Z]\" and prompt a\npre-trained (cloze) language model to fill the slot Z. Second, we select answer\ncandidates Z* = {\"injured'', \"hurt\"...} by treating the event description as\npremise and the filled templates as hypothesis in a textual entailment task.\nThis allows domain experts to draft the codebook directly as labeled prompts\nand interpretable answer candidates. This human-in-the-loop process is guided\nby our interactive codebook design tool. We evaluate PR-ENT in several\nrobustness checks: perturbing the event description and prompt template,\nrestricting the vocabulary and removing contextual information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lefebvre_C/0/1/0/all/0/1\">Cl&#xe9;ment Lefebvre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoehr_N/0/1/0/all/0/1\">Niklas Stoehr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"xTrimoABFold: De novo Antibody Structure Prediction without MSA. (arXiv:2212.00735v3 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2212.00735","description":"<p>In the field of antibody engineering, an essential task is to design a novel\nantibody whose paratopes bind to a specific antigen with correct epitopes.\nUnderstanding antibody structure and its paratope can facilitate a mechanistic\nunderstanding of its function. Therefore, antibody structure prediction from\nits sequence alone has always been a highly valuable problem for de novo\nantibody design. AlphaFold2, a breakthrough in the field of structural biology,\nprovides a solution to predict protein structure based on protein sequences and\ncomputationally expensive coevolutionary multiple sequence alignments (MSAs).\nHowever, the computational efficiency and undesirable prediction accuracy of\nantibodies, especially on the complementarity-determining regions (CDRs) of\nantibodies limit their applications in the industrially high-throughput drug\ndesign. To learn an informative representation of antibodies, we employed a\ndeep antibody language model (ALM) on curated sequences from the observed\nantibody space database via a transformer model. We also developed a novel\nmodel named xTrimoABFold to predict antibody structure from antibody sequence\nbased on the pretrained ALM as well as efficient evoformers and structural\nmodules. The model was trained end-to-end on the antibody structures in PDB by\nminimizing the ensemble loss of domain-specific focal loss on CDR and the\nframe-aligned point loss. xTrimoABFold outperforms AlphaFold2 and other protein\nlanguage model based SOTAs, e.g., OmegaFold, HelixFold-Single, and IgFold with\na large significant margin (30+\\% improvement on RMSD) while performing 151\ntimes faster than AlphaFold2. To the best of our knowledge, xTrimoABFold\nachieved state-of-the-art antibody structure prediction. Its improvement in\nboth accuracy and efficiency makes it a valuable tool for de novo antibody\ndesign and could make further improvements in immuno-theory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_Y/0/1/0/all/0/1\">Yining Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gong_X/0/1/0/all/0/1\">Xumeng Gong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Li_S/0/1/0/all/0/1\">Shaochuan Li</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yang_B/0/1/0/all/0/1\">Bing Yang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sun_Y/0/1/0/all/0/1\">YiWu Sun</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Shi_C/0/1/0/all/0/1\">Chuan Shi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_Y/0/1/0/all/0/1\">Yangang Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Song_L/0/1/0/all/0/1\">Le Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can In-context Learners Learn a Reasoning Concept from Demonstrations?. (arXiv:2212.01692v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.01692","description":"<p>Large language models show an emergent ability to learn a new task from a\nsmall number of input-output demonstrations. However, recent work shows that\nin-context learners largely rely on their pre-trained knowledge, such as the\nsentiment of the labels, instead of finding new associations in the input.\nHowever, the commonly-used few-shot evaluation settings using a random\nselection of in-context demonstrations can not disentangle models' ability to\nlearn a new skill from demonstrations, as most of the randomly-selected\ndemonstrations do not present relations informative for prediction beyond\nexposing the new task distribution.\n</p>\n<p>To disentangle models' in-context learning ability independent of models'\nmemory, we introduce a Conceptual few-shot learning method selecting the\ndemonstrations sharing a possibly-informative concept with the predicted\nsample. We extract a set of such concepts from annotated explanations and\nmeasure how much can models benefit from presenting these concepts in few-shot\ndemonstrations.\n</p>\n<p>We find that smaller models are more sensitive to the presented concepts.\nWhile some of the models are able to benefit from concept-presenting\ndemonstrations for each assessed concept, we find that none of the assessed\nin-context learners can benefit from all presented reasoning concepts\nconsistently, leaving the in-context concept learning an open challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1\">Michal &#x160;tef&#xe1;nik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadlcik_M/0/1/0/all/0/1\">Marek Kadl&#x10d;&#xed;k</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PVGRU: Generating Diverse and Relevant Dialogue Responses via Pseudo-Variational Mechanism. (arXiv:2212.09086v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09086","description":"<p>We investigate response generation for multi-turn dialogue in\ngenerative-based chatbots. Existing generative models based on RNNs (Recurrent\nNeural Networks) usually employ the last hidden state to summarize the\nsequences, which makes models unable to capture the subtle variability observed\nin different dialogues and cannot distinguish the differences between dialogues\nthat are similar in composition. In this paper, we propose a Pseudo-Variational\nGated Recurrent Unit (PVGRU) component without posterior knowledge through\nintroducing a recurrent summarizing variable into the GRU, which can aggregate\nthe accumulated distribution variations of subsequences. PVGRU can perceive the\nsubtle semantic variability through summarizing variables that are optimized by\nthe devised distribution consistency and reconstruction objectives. In\naddition, we build a Pseudo-Variational Hierarchical Dialogue (PVHD) model\nbased on PVGRU. Experimental results demonstrate that PVGRU can broadly improve\nthe diversity and relevance of responses on two benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongkang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Blind Spots of Model-Based Evaluation Metrics for Text Generation. (arXiv:2212.10020v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10020","description":"<p>In this work, we explore a useful but often neglected methodology for\nrobustness analysis of text generation evaluation metrics: stress tests with\nsynthetic data. Basically, we design and synthesize a wide range of potential\nerrors and check whether they result in a commensurate drop in the metric\nscores. We examine a range of recently proposed evaluation metrics based on\npretrained language models, for the tasks of open-ended generation,\ntranslation, and summarization. Our experiments reveal interesting\ninsensitivities, biases, or even loopholes in existing metrics. For example, we\nfind that BERTScore is confused by truncation errors in summarization, and\nMAUVE (built on top of GPT-2) is insensitive to errors at the beginning or\nmiddle of generations. Further, we investigate the reasons behind these blind\nspots and suggest practical workarounds for a more reliable evaluation of text\ngeneration. We have released our code and data at\nhttps://github.com/cloudygoose/blindspot_nlg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianxing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianle Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sachin Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. (arXiv:2302.12173v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2302.12173","description":"<p>Large Language Models (LLMs) are increasingly being integrated into various\napplications. The functionalities of recent LLMs can be flexibly modulated via\nnatural language prompts. This renders them susceptible to targeted adversarial\nprompting, e.g., Prompt Injection (PI) attacks enable attackers to override\noriginal instructions and employed controls. So far, it was assumed that the\nuser is directly prompting the LLM. But, what if it is not the user prompting?\nWe argue that LLM-Integrated Applications blur the line between data and\ninstructions. We reveal new attack vectors, using Indirect Prompt Injection,\nthat enable adversaries to remotely (without a direct interface) exploit\nLLM-integrated applications by strategically injecting prompts into data likely\nto be retrieved. We derive a comprehensive taxonomy from a computer security\nperspective to systematically investigate impacts and vulnerabilities,\nincluding data theft, worming, information ecosystem contamination, and other\nnovel security risks. We demonstrate our attacks' practical viability against\nboth real-world systems, such as Bing's GPT-4 powered Chat and code-completion\nengines, and synthetic applications built on GPT-4. We show how processing\nretrieved prompts can act as arbitrary code execution, manipulate the\napplication's functionality, and control how and if other APIs are called.\nDespite the increasing integration and reliance on LLMs, effective mitigations\nof these emerging threats are currently lacking. By raising awareness of these\nvulnerabilities and providing key insights into their implications, we aim to\npromote the safe and responsible deployment of these powerful models and the\ndevelopment of robust defenses that protect users and systems from potential\nattacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Greshake_K/0/1/0/all/0/1\">Kai Greshake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelnabi_S/0/1/0/all/0/1\">Sahar Abdelnabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shailesh Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Endres_C/0/1/0/all/0/1\">Christoph Endres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holz_T/0/1/0/all/0/1\">Thorsten Holz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1\">Mario Fritz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models are Few-shot Learners for Prognostic Prediction. (arXiv:2302.12692v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12692","description":"<p>Clinical prediction is an essential task in the healthcare industry. However,\nthe recent success of transformers, on which large language models are built,\nhas not been extended to this domain. In this research, we explore the use of\ntransformers and language models in prognostic prediction for immunotherapy\nusing real-world patients' clinical data and molecular profiles. This paper\ninvestigates the potential of transformers to improve clinical prediction\ncompared to conventional machine learning approaches and addresses the\nchallenge of few-shot learning in predicting rare disease areas. The study\nbenchmarks the efficacy of baselines and language models on prognostic\nprediction across multiple cancer types and investigates the impact of\ndifferent pretrained language models under few-shot regimes. The results\ndemonstrate significant improvements in accuracy and highlight the potential of\nNLP in clinical research to improve early detection and intervention for\ndifferent diseases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zekai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balan_M/0/1/0/all/0/1\">Mariann Micsinai Balan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_K/0/1/0/all/0/1\">Kevin Brown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains. (arXiv:2304.00958v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.00958","description":"<p>In recent years, pre-trained language models (PLMs) achieve the best\nperformance on a wide range of natural language processing (NLP) tasks. While\nthe first models were trained on general domain data, specialized ones have\nemerged to more effectively treat specific domains. In this paper, we propose\nan original study of PLMs in the medical domain on French language. We compare,\nfor the first time, the performance of PLMs trained on both public data from\nthe web and private data from healthcare establishments. We also evaluate\ndifferent learning strategies on a set of biomedical tasks. In particular, we\nshow that we can take advantage of already existing biomedical PLMs in a\nforeign language by further pre-train it on our targeted data. Finally, we\nrelease the first specialized PLMs for the biomedical field in French, called\nDrBERT, as well as the largest corpus of medical data under free license on\nwhich these models are trained.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Labrak_Y/0/1/0/all/0/1\">Yanis Labrak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazoge_A/0/1/0/all/0/1\">Adrien Bazoge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufour_R/0/1/0/all/0/1\">Richard Dufour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouvier_M/0/1/0/all/0/1\">Mickael Rouvier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morin_E/0/1/0/all/0/1\">Emmanuel Morin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daille_B/0/1/0/all/0/1\">B&#xe9;atrice Daille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gourraud_P/0/1/0/all/0/1\">Pierre-Antoine Gourraud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4. (arXiv:2304.03439v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.03439","description":"<p>Harnessing logical reasoning ability is a comprehensive natural language\nunderstanding endeavor. With the release of Generative Pretrained Transformer 4\n(GPT-4), highlighted as \"advanced\" at reasoning tasks, we are eager to learn\nthe GPT-4 performance on various logical reasoning tasks. This report analyses\nmultiple logical reasoning datasets, with popular benchmarks like LogiQA and\nReClor, and newly-released datasets like AR-LSAT. We test the multi-choice\nreading comprehension and natural language inference tasks with benchmarks\nrequiring logical reasoning. We further construct a logical reasoning\nout-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4.\nWe also make a performance comparison between ChatGPT and GPT-4. Experiment\nresults show that ChatGPT performs significantly better than the RoBERTa\nfine-tuning method on most logical reasoning benchmarks. With early access to\nthe GPT-4 API we are able to conduct intense experiments on the GPT-4 model.\nThe results show GPT-4 yields even higher performance on most logical reasoning\ndatasets. Among benchmarks, ChatGPT and GPT-4 do relatively well on well-known\ndatasets like LogiQA and ReClor. However, the performance drops significantly\nwhen handling newly released and out-of-distribution datasets. Logical\nreasoning remains challenging for ChatGPT and GPT-4, especially on\nout-of-distribution and natural language inference datasets. We release the\nprompt-style logical reasoning datasets as a benchmark suite and name it\nLogiEval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanmeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_R/0/1/0/all/0/1\">Ruoxi Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1\">Zhiyang Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qiji Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive-Hint Prompting Improves Reasoning in Large Language Models. (arXiv:2304.09797v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.09797","description":"<p>The performance of Large Language Models (LLMs) in reasoning tasks depends\nheavily on prompt design, with Chain-of-Thought (CoT) and self-consistency\nbeing critical methods that enhance this ability. However, these methods do not\nfully exploit the answers generated by the LLM to guide subsequent responses.\nThis paper proposes a new prompting method, named Progressive-Hint Prompting\n(PHP), that enables automatic multiple interactions between users and LLMs by\nusing previously generated answers as hints to progressively guide toward the\ncorrect answers. PHP is orthogonal to CoT and self-consistency, making it easy\nto combine with state-of-the-art techniques to further improve performance. We\nconducted an extensive and comprehensive evaluation to demonstrate the\neffectiveness of the proposed method. Our experimental results on six\nbenchmarks show that combining CoT and self-consistency with PHP significantly\nimproves accuracy while remaining highly efficient. For instance, with\ntext-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding\ncompared to Complex CoT, and a 46.17% reduction in sample paths with\nself-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances\non SVAMP (89.1% -&gt; 91.9%), GSM8K (92% -&gt; 95.5%), AQuA (76.4% -&gt; 79.9%) and MATH\n(50.2% -&gt; 53.9%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chuanyang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks. (arXiv:2304.14732v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.14732","description":"<p>With the wide application of Large Language Models (LLMs) such as ChatGPT,\nhow to make the contents generated by LLM accurate and credible becomes very\nimportant, especially in complex knowledge-intensive tasks. In this paper, we\npropose a novel framework called Search-in-the-Chain (SearChain) to improve the\naccuracy, credibility and traceability of LLM-generated content for multi-hop\nquestion answering, which is a typical complex knowledge-intensive task.\nSearChain is a framework that deeply integrates LLM and information retrieval\n(IR). In SearChain, LLM constructs a chain-of-query, which is the decomposition\nof the multi-hop question. Each node of the chain is a query-answer pair\nconsisting of an IR-oriented query and the answer generated by LLM for this\nquery. IR verifies, completes, and traces the information of each node of the\nchain, so as to guide LLM to construct the correct chain-of-query, and finally\nanswer the multi-hop question. SearChain makes LLM change from trying to give a\nanswer to trying to construct the chain-of-query when faced with the multi-hop\nquestion, which can stimulate the knowledge-reasoning ability and provides the\ninterface for IR to be deeply involved in reasoning process of LLM. IR\ninteracts with each node of chain-of-query of LLM. It verifies the information\nof the node and provides the unknown knowledge to LLM, which ensures the\naccuracy of the whole chain in the process of LLM generating the answer.\nBesides, the contents returned by LLM to the user include not only the final\nanswer but also the reasoning process for the question, that is, the\nchain-of-query and the supporting documents retrieved by IR for each node of\nthe chain, which improves the credibility and traceability of the contents\ngenerated by LLM. Experimental results show SearChain outperforms related\nbaselines on four multi-hop question-answering datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shicheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNTER: A Unified Knowledge Interface for Enhancing Pre-trained Language Models. (arXiv:2305.01624v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01624","description":"<p>Recent research demonstrates that external knowledge injection can advance\npre-trained language models (PLMs) in a variety of downstream NLP tasks.\nHowever, existing knowledge injection methods are either applicable to\nstructured knowledge or unstructured knowledge, lacking a unified usage. In\nthis paper, we propose a UNified knowledge inTERface, UNTER, to provide a\nunified perspective to exploit both structured knowledge and unstructured\nknowledge. In UNTER, we adopt the decoder as a unified knowledge interface,\naligning span representations obtained from the encoder with their\ncorresponding knowledge. This approach enables the encoder to uniformly invoke\nspan-related knowledge from its parameters for downstream applications.\nExperimental results show that, with both forms of knowledge injected, UNTER\ngains continuous improvements on a series of knowledge-driven NLP tasks,\nincluding entity typing, named entity recognition and relation extraction,\nespecially in low-resource scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1\">Deming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text. (arXiv:2305.02265v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.02265","description":"<p>Pretrained Vision-Language Models (VLMs) have achieved remarkable performance\nin image retrieval from text. However, their performance drops drastically when\nconfronted with linguistically complex texts that they struggle to comprehend.\nInspired by the Divide-and-Conquer algorithm and dual-process theory, in this\npaper, we regard linguistically complex texts as compound proposition texts\ncomposed of multiple simple proposition sentences and propose an end-to-end\nNeural Divide-and-Conquer Reasoning framework, dubbed NDCR. It contains three\nmain components: 1) Divide: a proposition generator divides the compound\nproposition text into simple proposition sentences and produces their\ncorresponding representations, 2) Conquer: a pretrained VLMs-based\nvisual-linguistic interactor achieves the interaction between decomposed\nproposition sentences and images, 3) Combine: a neural-symbolic reasoner\ncombines the above reasoning states to obtain the final solution via a neural\nlogic reasoning approach. According to the dual-process theory, the\nvisual-linguistic interactor and neural-symbolic reasoner could be regarded as\nanalogical reasoning System 1 and logical reasoning System 2. We conduct\nextensive experiments on a challenging image retrieval from contextual\ndescriptions data set. Experimental results and analyses indicate NDCR\nsignificantly improves performance in the complex image-text reasoning problem.\nCode link: https://github.com/YunxinLi/NDCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Baotian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuxin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge. (arXiv:2305.02459v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.02459","description":"<p>While transformer-based systems have enabled greater accuracies with fewer\ntraining examples, data acquisition obstacles still persist for rare-class\ntasks -- when the class label is very infrequent (e.g. &lt; 5% of samples). Active\nlearning has in general been proposed to alleviate such challenges, but choice\nof selection strategy, the criteria by which rare-class examples are chosen,\nhas not been systematically evaluated. Further, transformers enable iterative\ntransfer-learning approaches. We propose and investigate transfer- and active\nlearning solutions to the rare class problem of dissonance detection through\nutilizing models trained on closely related tasks and the evaluation of\nacquisition strategies, including a proposed probability-of-rare-class (PRC)\napproach. We perform these experiments for a specific rare class problem:\ncollecting language samples of cognitive dissonance from social media. We find\nthat PRC is a simple and effective strategy to guide annotations and ultimately\nimprove model accuracy while transfer-learning in a specific order can improve\nthe cold-start performance of the learner but does not benefit iterations of\nactive learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varadarajan_V/0/1/0/all/0/1\">Vasudha Varadarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juhng_S/0/1/0/all/0/1\">Swanie Juhng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahwish_S/0/1/0/all/0/1\">Syeda Mahwish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoran Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luby_J/0/1/0/all/0/1\">Jonah Luby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luhmann_C/0/1/0/all/0/1\">Christian Luhmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_H/0/1/0/all/0/1\">H. Andrew Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Code generation for Information Technology Tasks in YAML through Large Language Models. (arXiv:2305.02783v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2305.02783","description":"<p>The recent improvement in code generation capabilities due to the use of\nlarge language models has mainly benefited general purpose programming\nlanguages. Domain specific languages, such as the ones used for IT Automation,\nhave received far less attention, despite involving many active developers and\nbeing an essential component of modern cloud platforms. This work focuses on\nthe generation of Ansible-YAML, a widely used markup language for IT\nAutomation. We present Ansible Wisdom, a natural-language to Ansible-YAML code\ngeneration tool, aimed at improving IT automation productivity. Ansible Wisdom\nis a transformer-based model, extended by training with a new dataset\ncontaining Ansible-YAML. We also develop two novel performance metrics for YAML\nand Ansible to capture the specific characteristics of this domain. Results\nshow that Ansible Wisdom can accurately generate Ansible script from natural\nlanguage prompts with performance comparable or better than existing state of\nthe art code generation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pujar_S/0/1/0/all/0/1\">Saurabh Pujar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buratti_L/0/1/0/all/0/1\">Luca Buratti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaojie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupuis_N/0/1/0/all/0/1\">Nicolas Dupuis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_B/0/1/0/all/0/1\">Burn Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suneja_S/0/1/0/all/0/1\">Sahil Suneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sood_A/0/1/0/all/0/1\">Atin Sood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nalawade_G/0/1/0/all/0/1\">Ganesh Nalawade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1\">Matthew Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morari_A/0/1/0/all/0/1\">Alessandro Morari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puri_R/0/1/0/all/0/1\">Ruchir Puri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-05-07T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}