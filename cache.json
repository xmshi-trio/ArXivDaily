{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-11-09T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Graph Neural Networks for Topological Feature Extraction in ECG Classification. (arXiv:2311.04228v1 [eess.SP])","link":"http://arxiv.org/abs/2311.04228","description":"<p>The electrocardiogram (ECG) is a dependable instrument for assessing the\nfunction of the cardiovascular system. There has recently been much emphasis on\nprecisely classifying ECGs. While ECG situations have numerous similarities,\nlittle attention has been paid to categorizing ECGs using graph neural\nnetworks. In this study, we offer three distinct techniques for classifying\nheartbeats using deep graph neural networks to classify the ECG signals\naccurately. We suggest using different methods to extract topological features\nfrom the ECG signal and then using a branch of the graph neural network named\ngraph isomorphism network for classifying the ECGs. On the PTB Diagnostics data\nset, we tested the three proposed techniques. According to the findings, the\nthree proposed techniques are capable of making arrhythmia classification\npredictions with the accuracy of 99.38, 98.76, and 91.93 percent, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zeinalipour_K/0/1/0/all/0/1\">Kamyar Zeinalipour</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gori_M/0/1/0/all/0/1\">Marco Gori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can LLMs Follow Simple Rules?. (arXiv:2311.04235v1 [cs.AI])","link":"http://arxiv.org/abs/2311.04235","description":"<p>As Large Language Models (LLMs) are deployed with increasing real-world\nresponsibilities, it is important to be able to specify and constrain the\nbehavior of these systems in a reliable manner. Model developers may wish to\nset explicit rules for the model, such as \"do not generate abusive content\",\nbut these may be circumvented by jailbreaking techniques. Evaluating how well\nLLMs follow developer-provided rules in the face of adversarial inputs\ntypically requires manual review, which slows down monitoring and methods\ndevelopment. To address this issue, we propose Rule-following Language\nEvaluation Scenarios (RuLES), a programmatic framework for measuring\nrule-following ability in LLMs. RuLES consists of 15 simple text scenarios in\nwhich the model is instructed to obey a set of rules in natural language while\ninteracting with the human user. Each scenario has a concise evaluation program\nto determine whether the model has broken any rules in a conversation. Through\nmanual exploration of model behavior in our scenarios, we identify 6 categories\nof attack strategies and collect two suites of test cases: one consisting of\nunique conversations from manual testing and one that systematically implements\nstrategies from the 6 categories. Across various popular proprietary and open\nmodels such as GPT-4 and Llama 2, we find that all models are susceptible to a\nwide variety of adversarial hand-crafted user inputs, though GPT-4 is the\nbest-performing model. Additionally, we evaluate open models under\ngradient-based attacks and find significant vulnerabilities. We propose RuLES\nas a challenging new setting for research into exploring and defending against\nboth manual and automatic attacks on LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_N/0/1/0/all/0/1\">Norman Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sarah Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karamardian_D/0/1/0/all/0/1\">David Karamardian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aljeraisy_L/0/1/0/all/0/1\">Lulwa Aljeraisy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_D/0/1/0/all/0/1\">David Wagner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unifying Structure and Language Semantic for Efficient Contrastive Knowledge Graph Completion with Structured Entity Anchors. (arXiv:2311.04250v1 [cs.AI])","link":"http://arxiv.org/abs/2311.04250","description":"<p>The goal of knowledge graph completion (KGC) is to predict missing links in a\nKG using trained facts that are already known. In recent, pre-trained language\nmodel (PLM) based methods that utilize both textual and structural information\nare emerging, but their performances lag behind state-of-the-art (SOTA)\nstructure-based methods or some methods lose their inductive inference\ncapabilities in the process of fusing structure embedding to text encoder. In\nthis paper, we propose a novel method to effectively unify structure\ninformation and language semantics without losing the power of inductive\nreasoning. We adopt entity anchors and these anchors and textual description of\nKG elements are fed together into the PLM-based encoder to learn unified\nrepresentations. In addition, the proposed method utilizes additional random\nnegative samples which can be reused in the each mini-batch during contrastive\nlearning to learn a generalized entity representations. We verify the\neffectiveness of the our proposed method through various experiments and\nanalysis. The experimental results on standard benchmark widely used in link\nprediction task show that the proposed model outperforms existing the SOTA KGC\nmodels. Especially, our method show the largest performance improvement on\nFB15K-237, which is competitive to the SOTA of structure-based KGC methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Je_S/0/1/0/all/0/1\">Sang-Hyun Je</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1\">Wontae Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_K/0/1/0/all/0/1\">Kwangjin Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration. (arXiv:2311.04257v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04257","description":"<p>Multi-modal Large Language Models (MLLMs) have demonstrated impressive\ninstruction abilities across various open-ended tasks. However, previous\nmethods primarily focus on enhancing multi-modal capabilities. In this work, we\nintroduce a versatile multi-modal large language model, mPLUG-Owl2, which\neffectively leverages modality collaboration to improve performance in both\ntext and multi-modal tasks. mPLUG-Owl2 utilizes a modularized network design,\nwith the language decoder acting as a universal interface for managing\ndifferent modalities. Specifically, mPLUG-Owl2 incorporates shared functional\nmodules to facilitate modality collaboration and introduces a modality-adaptive\nmodule that preserves modality-specific features. Extensive experiments reveal\nthat mPLUG-Owl2 is capable of generalizing both text tasks and multi-modal\ntasks and achieving state-of-the-art performances with a single generic model.\nNotably, mPLUG-Owl2 is the first MLLM model that demonstrates the modality\ncollaboration phenomenon in both pure-text and multi-modal scenarios, setting a\npioneering path in the development of future multi-modal foundation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinghao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiabo Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haowei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1\">Qi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Automated Task Management for Generation, Execution, and Evaluation: A Framework for Fetch-and-Carry Tasks with Natural Language Instructions in Continuous Space. (arXiv:2311.04260v1 [cs.RO])","link":"http://arxiv.org/abs/2311.04260","description":"<p>This paper aims to develop a framework that enables a robot to execute tasks\nbased on visual information, in response to natural language instructions for\nFetch-and-Carry with Object Grounding (FCOG) tasks. Although there have been\nmany frameworks, they usually rely on manually given instruction sentences.\nTherefore, evaluations have only been conducted with fixed tasks. Furthermore,\nmany multimodal language understanding models for the benchmarks only consider\ndiscrete actions. To address the limitations, we propose a framework for the\nfull automation of the generation, execution, and evaluation of FCOG tasks. In\naddition, we introduce an approach to solving the FCOG tasks by dividing them\ninto four distinct subtasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kambara_M/0/1/0/all/0/1\">Motonari Kambara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Komei Sugiura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CRAB: Assessing the Strength of Causal Relationships Between Real-world Events. (arXiv:2311.04284v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04284","description":"<p>Understanding narratives requires reasoning about the cause-and-effect\nrelationships between events mentioned in the text. While existing foundation\nmodels yield impressive results in many NLP tasks requiring reasoning, it is\nunclear whether they understand the complexity of the underlying network of\ncausal relationships of events in narratives. In this work, we present CRAB, a\nnew Causal Reasoning Assessment Benchmark designed to evaluate causal\nunderstanding of events in real-world narratives. CRAB contains fine-grained,\ncontextual causality annotations for ~2.7K pairs of real-world events that\ndescribe various newsworthy event timelines (e.g., the acquisition of Twitter\nby Elon Musk). Using CRAB, we measure the performance of several large language\nmodels, demonstrating that most systems achieve poor performance on the task.\nMotivated by classical causal principles, we also analyze the causal structures\nof groups of events in CRAB, and find that models perform worse on causal\nreasoning when events are derived from complex causal structures compared to\nsimple linear causal chains. We make our dataset and code available to the\nresearch community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Romanou_A/0/1/0/all/0/1\">Angelika Romanou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montariol_S/0/1/0/all/0/1\">Syrielle Montariol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_D/0/1/0/all/0/1\">Debjit Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laugier_L/0/1/0/all/0/1\">Leo Laugier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aberer_K/0/1/0/all/0/1\">Karl Aberer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aspect-based Meeting Transcript Summarization: A Two-Stage Approach with Weak Supervision on Sentence Classification. (arXiv:2311.04292v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04292","description":"<p>Aspect-based meeting transcript summarization aims to produce multiple\nsummaries, each focusing on one aspect of content in a meeting transcript. It\nis challenging as sentences related to different aspects can mingle together,\nand those relevant to a specific aspect can be scattered throughout the long\ntranscript of a meeting. The traditional summarization methods produce one\nsummary mixing information of all aspects, which cannot deal with the above\nchallenges of aspect-based meeting transcript summarization. In this paper, we\npropose a two-stage method for aspect-based meeting transcript summarization.\nTo select the input content related to specific aspects, we train a sentence\nclassifier on a dataset constructed from the AMI corpus with pseudo-labeling.\nThen we merge the sentences selected for a specific aspect as the input for the\nsummarizer to produce the aspect-based summary. Experimental results on the AMI\ncorpus outperform many strong baselines, which verifies the effectiveness of\nour proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhongfen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quan Hung Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaiqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenting Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Formal Aspects of Language Modeling. (arXiv:2311.04329v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04329","description":"<p>Large language models have become one of the most commonly deployed NLP\ninventions. In the past half-decade, their integration into core natural\nlanguage processing tools has dramatically increased the performance of such\ntools, and they have entered the public discourse surrounding artificial\nintelligence. Consequently, it is important for both developers and researchers\nalike to understand the mathematical foundations of large language models, as\nwell as how to implement them. These notes are the accompaniment to the\ntheoretical portion of the ETH Z\\\"urich course on large language models,\ncovering what constitutes a language model from a formal, theoretical\nperspective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svete_A/0/1/0/all/0/1\">Anej Svete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Li Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sub-Sentence Encoder: Contrastive Learning of Propositional Semantic Representations. (arXiv:2311.04335v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04335","description":"<p>We introduce sub-sentence encoder, a contrastively-learned contextual\nembedding model for fine-grained semantic representation of text. In contrast\nto the standard practice with sentence embeddings, where the meaning of an\nentire sequence of text is encoded into a fixed-length vector, the sub-sentence\nencoder learns to produce distinct contextual embeddings corresponding to\ndifferent atomic propositions, i.e. atomic units of meaning expressed within a\ntext sequence. The sub-sentence embeddings are contrastively learned to\nrecognize (inferred) semantic equivalence between propositions across different\ntext sequences. Our experiments show the effectiveness of sub-sentence encoders\nin applications, such as retrieving supporting facts for fine-grained text\nattribution or recognizing the conditional semantic similarity between texts.\nIn practice, we demonstrate that sub-sentence encoders keep the same level of\ninference cost and space complexity compared to sentence encoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sihao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Ben Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Taxonomy of Rater Disagreements: Surveying Challenges & Opportunities from the Perspective of Annotating Online Toxicity. (arXiv:2311.04345v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04345","description":"<p>Toxicity is an increasingly common and severe issue in online spaces.\nConsequently, a rich line of machine learning research over the past decade has\nfocused on computationally detecting and mitigating online toxicity. These\nefforts crucially rely on human-annotated datasets that identify toxic content\nof various kinds in social media texts. However, such annotations historically\nyield low inter-rater agreement, which was often dealt with by taking the\nmajority vote or other such approaches to arrive at a single ground truth\nlabel. Recent research has pointed out the importance of accounting for the\nsubjective nature of this task when building and utilizing these datasets, and\nthis has triggered work on analyzing and better understanding rater\ndisagreements, and how they could be effectively incorporated into the machine\nlearning developmental pipeline. While these efforts are filling an important\ngap, there is a lack of a broader framework about the root causes of rater\ndisagreement, and therefore, we situate this work within that broader\nlandscape. In this survey paper, we analyze a broad set of literature on the\nreasons behind rater disagreements focusing on online toxicity, and propose a\ndetailed taxonomy for the same. Further, we summarize and discuss the potential\nsolutions targeting each reason for disagreement. We also discuss several open\nissues, which could promote the future development of online toxicity research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hangzhi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kivlichan_I/0/1/0/all/0/1\">Ian D Kivlichan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1\">Vinodkumar Prabhakaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_D/0/1/0/all/0/1\">Davis Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_A/0/1/0/all/0/1\">Amulya Yadav</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Effectiveness of Retrieval-Augmented Large Language Models in Scientific Document Reasoning. (arXiv:2311.04348v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04348","description":"<p>Despite the dramatic progress in Large Language Model (LLM) development, LLMs\noften provide seemingly plausible but not factual information, often referred\nto as hallucinations. Retrieval-augmented LLMs provide a non-parametric\napproach to solve these issues by retrieving relevant information from external\ndata sources and augment the training process. These models help to trace\nevidence from an externally provided knowledge base allowing the model\npredictions to be better interpreted and verified. In this work, we critically\nevaluate these models in their ability to perform in scientific document\nreasoning tasks. To this end, we tuned multiple such model variants with\nscience-focused instructions and evaluated them on a scientific document\nreasoning benchmark for the usefulness of the retrieved document passages. Our\nfindings suggest that models justify predictions in science tasks with\nfabricated evidence and leveraging scientific corpus as pretraining data does\nnot alleviate the risk of evidence fabrication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Munikoti_S/0/1/0/all/0/1\">Sai Munikoti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1\">Anurag Acharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagle_S/0/1/0/all/0/1\">Sridevi Wagle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horawalavithana_S/0/1/0/all/0/1\">Sameera Horawalavithana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncovering Causal Variables in Transformers using Circuit Probing. (arXiv:2311.04354v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04354","description":"<p>Neural network models have achieved high performance on a wide variety of\ncomplex tasks, but the algorithms that they implement are notoriously difficult\nto interpret. In order to understand these algorithms, it is often necessary to\nhypothesize intermediate variables involved in the network's computation. For\nexample, does a language model depend on particular syntactic properties when\ngenerating a sentence? However, existing analysis tools make it difficult to\ntest hypotheses of this type. We propose a new analysis technique -- circuit\nprobing -- that automatically uncovers low-level circuits that compute\nhypothesized intermediate variables. This enables causal analysis through\ntargeted ablation at the level of model parameters. We apply this method to\nmodels trained on simple arithmetic tasks, demonstrating its effectiveness at\n(1) deciphering the algorithms that models have learned, (2) revealing modular\nstructure within a model, and (3) tracking the development of circuits over\ntraining. We compare circuit probing to other methods across these three\nexperiments, and find it on par or more effective than existing analysis\nmethods. Finally, we demonstrate circuit probing on a real-world use case,\nuncovering circuits that are responsible for subject-verb agreement and\nreflexive anaphora in GPT2-Small and Medium.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lepori_M/0/1/0/all/0/1\">Michael A. Lepori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1\">Thomas Serre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntax-Guided Transformers: Elevating Compositional Generalization and Grounding in Multimodal Environments. (arXiv:2311.04364v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04364","description":"<p>Compositional generalization, the ability of intelligent models to\nextrapolate understanding of components to novel compositions, is a fundamental\nyet challenging facet in AI research, especially within multimodal\nenvironments. In this work, we address this challenge by exploiting the\nsyntactic structure of language to boost compositional generalization. This\npaper elevates the importance of syntactic grounding, particularly through\nattention masking techniques derived from text input parsing. We introduce and\nevaluate the merits of using syntactic information in the multimodal grounding\nproblem. Our results on grounded compositional generalization underscore the\npositive impact of dependency parsing across diverse tasks when utilized with\nWeight Sharing across the Transformer encoder. The results push the\nstate-of-the-art in multimodal grounding and parameter-efficient modeling and\nprovide insights for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamali_D/0/1/0/all/0/1\">Danial Kamali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kordjamshidi_P/0/1/0/all/0/1\">Parisa Kordjamshidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating multiple large language models in pediatric ophthalmology. (arXiv:2311.04368v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04368","description":"<p>IMPORTANCE The response effectiveness of different large language models\n(LLMs) and various individuals, including medical students, graduate students,\nand practicing physicians, in pediatric ophthalmology consultations, has not\nbeen clearly established yet. OBJECTIVE Design a 100-question exam based on\npediatric ophthalmology to evaluate the performance of LLMs in highly\nspecialized scenarios and compare them with the performance of medical students\nand physicians at different levels. DESIGN, SETTING, AND PARTICIPANTS This\nsurvey study assessed three LLMs, namely ChatGPT (GPT-3.5), GPT-4, and PaLM2,\nwere assessed alongside three human cohorts: medical students, postgraduate\nstudents, and attending physicians, in their ability to answer questions\nrelated to pediatric ophthalmology. It was conducted by administering\nquestionnaires in the form of test papers through the LLM network interface,\nwith the valuable participation of volunteers. MAIN OUTCOMES AND MEASURES Mean\nscores of LLM and humans on 100 multiple-choice questions, as well as the\nanswer stability, correlation, and response confidence of each LLM. RESULTS\nGPT-4 performed comparably to attending physicians, while ChatGPT (GPT-3.5) and\nPaLM2 outperformed medical students but slightly trailed behind postgraduate\nstudents. Furthermore, GPT-4 exhibited greater stability and confidence when\nresponding to inquiries compared to ChatGPT (GPT-3.5) and PaLM2. CONCLUSIONS\nAND RELEVANCE Our results underscore the potential for LLMs to provide medical\nassistance in pediatric ophthalmology and suggest significant capacity to guide\nthe education of medical students.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Holmes_J/0/1/0/all/0/1\">Jason Holmes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1\">Rui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Huan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Jie Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yi Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models. (arXiv:2311.04378v1 [cs.LG])","link":"http://arxiv.org/abs/2311.04378","description":"<p>Watermarking generative models consists of planting a statistical signal\n(watermark) in a model's output so that it can be later verified that the\noutput was generated by the given model. A strong watermarking scheme satisfies\nthe property that a computationally bounded attacker cannot erase the watermark\nwithout causing significant quality degradation. In this paper, we study the\n(im)possibility of strong watermarking schemes. We prove that, under\nwell-specified and natural assumptions, strong watermarking is impossible to\nachieve. This holds even in the private detection algorithm setting, where the\nwatermark insertion and detection algorithms share a secret key, unknown to the\nattacker. To prove this result, we introduce a generic efficient watermark\nattack; the attacker is not required to know the private key of the scheme or\neven which scheme is used. Our attack is based on two assumptions: (1) The\nattacker has access to a \"quality oracle\" that can evaluate whether a candidate\noutput is a high-quality response to a prompt, and (2) The attacker has access\nto a \"perturbation oracle\" which can modify an output with a nontrivial\nprobability of maintaining quality, and which induces an efficiently mixing\nrandom walk on high-quality outputs. We argue that both assumptions can be\nsatisfied in practice by an attacker with weaker computational capabilities\nthan the watermarked model itself, to which the attacker has only black-box\naccess. Furthermore, our assumptions will likely only be easier to satisfy over\ntime as models grow in capabilities and modalities. We demonstrate the\nfeasibility of our attack by instantiating it to attack three existing\nwatermarking schemes for large language models: Kirchenbauer et al. (2023),\nKuditipudi et al. (2023), and Zhao et al. (2023). The same attack successfully\nremoves the watermarks planted by all three schemes, with only minor quality\ndegradation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edelman_B/0/1/0/all/0/1\">Benjamin L. Edelman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francati_D/0/1/0/all/0/1\">Danilo Francati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venturi_D/0/1/0/all/0/1\">Daniele Venturi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ateniese_G/0/1/0/all/0/1\">Giuseppe Ateniese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barak_B/0/1/0/all/0/1\">Boaz Barak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Factors for Better Compositional Generalization. (arXiv:2311.04420v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04420","description":"<p>Recent diagnostic datasets on compositional generalization, such as SCAN\n(Lake and Baroni, 2018) and COGS (Kim and Linzen, 2020), expose severe problems\nin models trained from scratch on these datasets. However, in contrast to this\npoor performance, state-of-the-art models trained on larger and more general\ndatasets show better generalization ability. In this work, to reconcile this\ninconsistency, we conduct an empirical analysis by training Transformer models\non a variety of training sets with different data factors, including dataset\nscale, pattern complexity, example difficulty, etc. First, we show that\nincreased dataset complexity can lead to better generalization behavior on\nmultiple different generalization challenges. To further understand this\nimprovement, we show two axes of the benefit from more complex datasets: they\nprovide more diverse examples so compositional understanding becomes more\neffective, and they also prevent ungeneralizable memorization of the examples\ndue to reduced example repetition frequency. Finally, we explore how training\nexamples of different difficulty levels influence generalization differently.\nOn synthetic datasets, simple examples invoke stronger compositionality than\nhard examples do. On larger-scale real language datasets, while hard examples\nbecome more important potentially to ensure decent data coverage, a balanced\nmixture of simple and hard examples manages to induce the strongest\ngeneralizability. The code and data for this work are available at\nhttps://github.com/owenzx/data4comp\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yichen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recursion in Recursion: Two-Level Nested Recursion for Length Generalization with Scalability. (arXiv:2311.04449v1 [cs.LG])","link":"http://arxiv.org/abs/2311.04449","description":"<p>Binary Balanced Tree RvNNs (BBT-RvNNs) enforce sequence composition according\nto a preset balanced binary tree structure. Thus, their non-linear recursion\ndepth is just $\\log_2 n$ ($n$ being the sequence length). Such logarithmic\nscaling makes BBT-RvNNs efficient and scalable on long sequence tasks such as\nLong Range Arena (LRA). However, such computational efficiency comes at a cost\nbecause BBT-RvNNs cannot solve simple arithmetic tasks like ListOps. On the\nflip side, RvNNs (e.g., Beam Tree RvNN) that do succeed on ListOps (and other\nstructure-sensitive tasks like formal logical inference) are generally several\ntimes more expensive than even RNNs. In this paper, we introduce a novel\nframework -- Recursion in Recursion (RIR) to strike a balance between the two\nsides - getting some of the benefits from both worlds. In RIR, we use a form of\ntwo-level nested recursion - where the outer recursion is a $k$-ary balanced\ntree model with another recursive model (inner recursion) implementing its cell\nfunction. For the inner recursion, we choose Beam Tree RvNNs (BT-RvNN). To\nadjust BT-RvNNs within RIR we also propose a novel strategy of beam alignment.\nOverall, this entails that the total recursive depth in RIR is upper-bounded by\n$k \\log_k n$. Our best RIR-based model is the first model that demonstrates\nhigh ($\\geq 90\\%$) length-generalization performance on ListOps while at the\nsame time being scalable enough to be trainable on long sequence inputs from\nLRA. Moreover, in terms of accuracy in the LRA language tasks, it performs\ncompetitively with Structured State Space Models (SSMs) without any special\ninitialization - outperforming Transformers by a large margin. On the other\nhand, while SSMs can marginally outperform RIR on LRA, they (SSMs) fail to\nlength-generalize on ListOps. Our code is available at:\n\\url{https://github.com/JRC1995/BeamRecursionFamily/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_J/0/1/0/all/0/1\">Jishnu Ray Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lewis's Signaling Game as beta-VAE For Natural Word Lengths and Segments. (arXiv:2311.04453v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04453","description":"<p>As a sub-discipline of evolutionary and computational linguistics, emergent\ncommunication (EC) studies communication protocols, called emergent languages,\narising in simulations where agents communicate. A key goal of EC is to give\nrise to languages that share statistical properties with natural languages. In\nthis paper, we reinterpret Lewis's signaling game, a frequently used setting in\nEC, as beta-VAE and reformulate its objective function as ELBO. Consequently,\nwe clarify the existence of prior distributions of emergent languages and show\nthat the choice of the priors can influence their statistical properties.\nSpecifically, we address the properties of word lengths and segmentation, known\nas Zipf's law of abbreviation (ZLA) and Harris's articulation scheme (HAS),\nrespectively. It has been reported that the emergent languages do not follow\nthem when using the conventional objective. We experimentally demonstrate that\nby selecting an appropriate prior distribution, more natural segments emerge,\nwhile suggesting that the conventional one prevents the languages from\nfollowing ZLA and HAS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ueda_R/0/1/0/all/0/1\">Ryo Ueda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taniguchi_T/0/1/0/all/0/1\">Tadahiro Taniguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Pacing in Long-Form Story Planning. (arXiv:2311.04459v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04459","description":"<p>Existing LLM-based systems for writing long-form stories or story outlines\nfrequently suffer from unnatural pacing, whether glossing over important events\nor over-elaborating on insignificant details, resulting in a jarring experience\nfor the reader. We propose a CONCrete Outline ConTrol (CONCOCT) system to\nimprove pacing when automatically generating story outlines. We first train a\nconcreteness evaluator to judge which of two events is more concrete\n(low-level-detailed). This evaluator can then be used to control pacing in\nhierarchical outline generation; in this work, we explore a vaguest-first\nexpansion procedure that aims for uniform pacing. We further use the evaluator\nto filter new outline items based on predicted concreteness. Compared to a\nbaseline hierarchical outline generator, humans judge CONCOCT's pacing to be\nmore consistent over 57% of the time across multiple outline lengths; the gains\nalso translate to downstream stories. All code, data, and models are\nopen-sourced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yichen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RDGCN: Reinforced Dependency Graph Convolutional Network for Aspect-based Sentiment Analysis. (arXiv:2311.04467v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04467","description":"<p>Aspect-based sentiment analysis (ABSA) is dedicated to forecasting the\nsentiment polarity of aspect terms within sentences. Employing graph neural\nnetworks to capture structural patterns from syntactic dependency parsing has\nbeen confirmed as an effective approach for boosting ABSA. In most works, the\ntopology of dependency trees or dependency-based attention coefficients is\noften loosely regarded as edges between aspects and opinions, which can result\nin insufficient and ambiguous syntactic utilization. To address these problems,\nwe propose a new reinforced dependency graph convolutional network (RDGCN) that\nimproves the importance calculation of dependencies in both distance and type\nviews. Initially, we propose an importance calculation criterion for the\nminimum distances over dependency trees. Under the criterion, we design a\ndistance-importance function that leverages reinforcement learning for weight\ndistribution search and dissimilarity control. Since dependency types often do\nnot have explicit syntax like tree distances, we use global attention and mask\nmechanisms to design type-importance functions. Finally, we merge these weights\nand implement feature aggregation and classification. Comprehensive experiments\non three popular datasets demonstrate the effectiveness of the criterion and\nimportance functions. RDGCN outperforms state-of-the-art GNN-based baselines in\nall validations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xusheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qiong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Huailiang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanbing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qinglang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Twitter Sentiment Analysis of Covid Vacciness. (arXiv:2311.04479v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04479","description":"<p>In this paper, we look at a database of tweets sorted by various keywords\nthat could indicate the users sentiment towards covid vaccines. With social\nmedia becoming such a prevalent source of opinion, sorting and ranking tweets\nthat hold important information such as opinions on covid vaccines is of utmost\nimportance. Two different ranking scales were used, and ranking a tweet in this\nway could represent the difference between an opinion being lost and an opinion\nbeing featured on the site, which affects the decisions and behavior of people,\nand why researchers were interested in it. Using natural language processing\ntechniques, our aim is to determine and categorize opinions about covid\nvaccines with the highest accuracy possible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenbo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tiechuan Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLearViD: Curriculum Learning for Video Description. (arXiv:2311.04480v1 [cs.CV])","link":"http://arxiv.org/abs/2311.04480","description":"<p>Video description entails automatically generating coherent natural language\nsentences that narrate the content of a given video. We introduce CLearViD, a\ntransformer-based model for video description generation that leverages\ncurriculum learning to accomplish this task. In particular, we investigate two\ncurriculum strategies: (1) progressively exposing the model to more challenging\nsamples by gradually applying a Gaussian noise to the video data, and (2)\ngradually reducing the capacity of the network through dropout during the\ntraining process. These methods enable the model to learn more robust and\ngeneralizable features. Moreover, CLearViD leverages the Mish activation\nfunction, which provides non-linearity and non-monotonicity and helps alleviate\nthe issue of vanishing gradients. Our extensive experiments and ablation\nstudies demonstrate the effectiveness of the proposed model. The results on two\ndatasets, namely ActivityNet Captions and YouCook2, show that CLearViD\nsignificantly outperforms existing state-of-the-art models in terms of both\naccuracy and diversity metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chuang_C/0/1/0/all/0/1\">Cheng-Yu Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazli_P/0/1/0/all/0/1\">Pooyan Fazli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-label and Multi-target Sampling of Machine Annotation for Computational Stance Detection. (arXiv:2311.04495v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04495","description":"<p>Data collection from manual labeling provides domain-specific and\ntask-aligned supervision for data-driven approaches, and a critical mass of\nwell-annotated resources is required to achieve reasonable performance in\nnatural language processing tasks. However, manual annotations are often\nchallenging to scale up in terms of time and budget, especially when domain\nknowledge, capturing subtle semantic features, and reasoning steps are needed.\nIn this paper, we investigate the efficacy of leveraging large language models\non automated labeling for computational stance detection. We empirically\nobserve that while large language models show strong potential as an\nalternative to human annotators, their sensitivity to task-specific\ninstructions and their intrinsic biases pose intriguing yet unique challenges\nin machine annotation. We introduce a multi-label and multi-target sampling\nstrategy to optimize the annotation quality. Experimental results on the\nbenchmark stance detection corpora show that our method can significantly\nimprove performance and learning efficacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chieu_H/0/1/0/all/0/1\">Hai Leong Chieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NExT-Chat: An LMM for Chat, Detection and Segmentation. (arXiv:2311.04498v1 [cs.CV])","link":"http://arxiv.org/abs/2311.04498","description":"<p>The development of large language models (LLMs) has greatly advanced the\nfield of multimodal understanding, leading to the emergence of large multimodal\nmodels (LMMs). In order to enhance the level of visual comprehension, recent\nstudies have equipped LMMs with region-level understanding capabilities by\nrepresenting object bounding box coordinates as a series of text sequences\n(pixel2seq). In this paper, we introduce a novel paradigm for object location\nmodeling called pixel2emb method, where we ask the LMM to output the location\nembeddings and then decoded by different decoders. This paradigm allows for\ndifferent location formats (such as bounding boxes and masks) to be used in\nmultimodal conversations Furthermore, this kind of embedding based location\nmodeling enables the utilization of existing practices in localization tasks,\nsuch as detection and segmentation. In scenarios with limited resources, our\npixel2emb demonstrates superior performance compared to existing\nstate-of-the-art (SOTA) approaches in both the location input and output tasks\nunder fair comparison. Leveraging the proposed pixel2emb method, we train an\nLMM named NExT-Chat and demonstrate its capability of handling multiple tasks\nlike visual grounding, region caption, and grounded reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chen-Wei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yun Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversation Understanding using Relational Temporal Graph Neural Networks with Auxiliary Cross-Modality Interaction. (arXiv:2311.04507v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04507","description":"<p>Emotion recognition is a crucial task for human conversation understanding.\nIt becomes more challenging with the notion of multimodal data, e.g., language,\nvoice, and facial expressions. As a typical solution, the global- and the local\ncontext information are exploited to predict the emotional label for every\nsingle sentence, i.e., utterance, in the dialogue. Specifically, the global\nrepresentation could be captured via modeling of cross-modal interactions at\nthe conversation level. The local one is often inferred using the temporal\ninformation of speakers or emotional shifts, which neglects vital factors at\nthe utterance level. Additionally, most existing approaches take fused features\nof multiple modalities in an unified input without leveraging modality-specific\nrepresentations. Motivating from these problems, we propose the Relational\nTemporal Graph Neural Network with Auxiliary Cross-Modality Interaction\n(CORECT), an novel neural network framework that effectively captures\nconversation-level cross-modality interactions and utterance-level temporal\ndependencies with the modality-specific manner for conversation understanding.\nExtensive experiments demonstrate the effectiveness of CORECT via its\nstate-of-the-art results on the IEMOCAP and CMU-MOSEI datasets for the\nmultimodal ERC task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cam-Van Thi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_A/0/1/0/all/0/1\">Anh-Tuan Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">The-Son Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kieu_H/0/1/0/all/0/1\">Hai-Dang Kieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duc-Trong Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token Based ASR. (arXiv:2311.04534v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04534","description":"<p>Recently, unified speech-text models, such as SpeechGPT, VioLA, and\nAudioPaLM, have achieved remarkable performance on speech tasks. These models\nconvert continuous speech signals into discrete tokens (speech discretization)\nand merge text and speech tokens into a shared vocabulary. Then they train a\nsingle decoder-only Transformer on a mixture of speech tasks. Specifically, all\nthese models utilize Loss Masking on the input speech tokens for the ASR task,\nwhich means that these models do not explicitly model the dependency between\nthe speech tokens. In this paper, we attempt to model the sequence of speech\ntokens in an autoregressive manner like text. However, we find that applying\nthe conventional cross-entropy loss on input speech tokens does not\nconsistently improve the ASR performance over Loss Masking. Therefore, we\npropose a novel approach denoted Smoothed Label Distillation (SLD), which\nintroduces a KL divergence loss with smoothed labels on the input speech tokens\nto effectively model speech tokens. Experiments demonstrate that our SLD\napproach alleviates the limitations of the cross-entropy loss and consistently\noutperforms Loss Masking for decoder-only Transformer based ASR using different\nspeech discretization methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Siqi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yukun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RankAug: Augmented data ranking for text classification. (arXiv:2311.04535v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04535","description":"<p>Research on data generation and augmentation has been focused majorly on\nenhancing generation models, leaving a notable gap in the exploration and\nrefinement of methods for evaluating synthetic data. There are several text\nsimilarity metrics within the context of generated data filtering which can\nimpact the performance of specific Natural Language Understanding (NLU) tasks,\nspecifically focusing on intent and sentiment classification. In this study, we\npropose RankAug, a text-ranking approach that detects and filters out the top\naugmented texts in terms of being most similar in meaning with lexical and\nsyntactical diversity. Through experiments conducted on multiple datasets, we\ndemonstrate that the judicious selection of filtering techniques can yield a\nsubstantial improvement of up to 35% in classification accuracy for\nunder-represented classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_T/0/1/0/all/0/1\">Tiasa Singha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_P/0/1/0/all/0/1\">Priyam Basu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large GPT-like Models are Bad Babies: A Closer Look at the Relationship between Linguistic Competence and Psycholinguistic Measures. (arXiv:2311.04547v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04547","description":"<p>Research on the cognitive plausibility of language models (LMs) has so far\nmostly concentrated on modelling psycholinguistic response variables such as\nreading times, gaze durations and N400/P600 EEG signals, while mostly leaving\nout the dimension of what Mahowald et al. (2023) described as formal and\nfunctional linguistic competence, and developmental plausibility. We address\nthis gap by training a series of GPT-like language models of different sizes on\nthe strict version of the BabyLM pretraining corpus, evaluating on the\nchallenge tasks (BLiMP, GLUE, MSGS) and an additional reading time prediction\ntask. We find a positive correlation between LM size and performance on all\nthree challenge tasks, with different preferences for model width and depth in\neach of the tasks. In contrast, a negative correlation was found between LM\nsize and reading time fit of linear mixed-effects models using LM surprisal as\na predictor, with the second-smallest LM achieving the largest log-likelihood\nreduction over a baseline model without surprisal. This suggests that modelling\nprocessing effort and linguistic competence may require an approach different\nfrom training GPT-like LMs on a developmentally plausible corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Steuer_J/0/1/0/all/0/1\">Julius Steuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosbach_M/0/1/0/all/0/1\">Marius Mosbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Distractors in Multiple-Choice Tests. (arXiv:2311.04554v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04554","description":"<p>Multiple-choice tests are a common approach for assessing candidates'\ncomprehension skills. Standard multiple-choice reading comprehension exams\nrequire candidates to select the correct answer option from a discrete set\nbased on a question in relation to a contextual passage. For appropriate\nassessment, the distractor answer options must by definition be incorrect but\nplausible and diverse. However, generating good quality distractors satisfying\nthese criteria is a challenging task for content creators. We propose automated\nassessment metrics for the quality of distractors in multiple-choice reading\ncomprehension tests. Specifically, we define quality in terms of the\nincorrectness, plausibility and diversity of the distractor options. We assess\nincorrectness using the classification ability of a binary multiple-choice\nreading comprehension system. Plausibility is assessed by considering the\ndistractor confidence - the probability mass associated with the distractor\noptions for a standard multi-class multiple-choice reading comprehension\nsystem. Diversity is assessed by pairwise comparison of an embedding-based\nequivalence metric between the distractors of a question. To further validate\nthe plausibility metric we compare against candidate distributions over\nmultiple-choice questions and agreement with a ChatGPT model's interpretation\nof distractor plausibility and diversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vatsal Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liusie_A/0/1/0/all/0/1\">Adian Liusie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark Gales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Nature of Disagreements on Mid-Scale Ratings: A Case Study on the Abstractness-Concreteness Continuum. (arXiv:2311.04563v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04563","description":"<p>Humans tend to strongly agree on ratings on a scale for extreme cases (e.g.,\na CAT is judged as very concrete), but judgements on mid-scale words exhibit\nmore disagreement. Yet, collected rating norms are heavily exploited across\ndisciplines. Our study focuses on concreteness ratings and (i) implements\ncorrelations and supervised classification to identify salient multi-modal\ncharacteristics of mid-scale words, and (ii) applies a hard clustering to\nidentify patterns of systematic disagreement across raters. Our results suggest\nto either fine-tune or filter mid-scale target words before utilising them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Knuples_U/0/1/0/all/0/1\">Urban Knuple&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frassinelli_D/0/1/0/all/0/1\">Diego Frassinelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walde_S/0/1/0/all/0/1\">Sabine Schulte im Walde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models. (arXiv:2311.04589v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04589","description":"<p>Despite Multi-modal Large Language Models (MM-LLMs) have made exciting\nstrides recently, they are still struggling to efficiently model the\ninteractions among multi-modal inputs and the generation in non-textual\nmodalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an\napproach to treat the input from any modality as a token sequence and learn a\njoint embedding space for all modalities. Specifically, for the input from any\nmodality, TEAL first discretizes it into a token sequence with the\noff-the-shelf tokenizer and embeds the token sequence into a joint embedding\nspace with a learnable embedding matrix. MM-LLMs just need to predict the\nmulti-modal tokens autoregressively as the textual LLMs do. Finally, the\ncorresponding de-tokenizer is applied to generate the output in each modality\nbased on the predicted token sequence. With the joint embedding space, TEAL\nenables the frozen LLMs to perform both understanding and generation tasks\ninvolving non-textual modalities, such as image and audio. Thus, the textual\nLLM can just work as an interface and maintain its high performance in textual\nunderstanding and generation. Experiments show that TEAL achieves substantial\nimprovements in multi-modal understanding, and implements a simple scheme for\nmulti-modal generations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingxue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Massive Editing for Large Language Models via Meta Learning. (arXiv:2311.04661v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04661","description":"<p>While large language models (LLMs) have enabled learning knowledge from the\npre-training corpora, the acquired knowledge may be fundamentally incorrect or\noutdated over time, which necessitates rectifying the knowledge of the language\nmodel (LM) after the training. A promising approach involves employing a\nhyper-network to generate parameter shift, whereas existing hyper-networks\nsuffer from inferior scalability in synchronous editing operation amount. To\nmitigate the problem, we propose the MAssive Language Model Editing Network\n(MALMEN), which formulates the parameter shift aggregation as the least square\nproblem, subsequently updating the LM parameters using the normal equation. To\naccommodate editing multiple facts simultaneously with limited memory budgets,\nwe separate the computation on the hyper-network and LM, enabling arbitrary\nbatch size on both neural networks. Our method is evaluated by editing up to\nthousands of facts on LMs with different architectures, i.e., BERT-base, GPT-2,\nT5-XL (2.8B), and GPT-J (6B), across various knowledge-intensive NLP tasks,\ni.e., closed book fact-checking and question answering. Remarkably, MALMEN is\ncapable of editing hundreds of times more facts than strong baselines with the\nidentical hyper-network architecture and outperforms editor specifically\ndesigned for GPT. Our code is available at\nhttps://github.com/ChenmienTan/malmen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenmien Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Ge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech language models lack important brain-relevant semantics. (arXiv:2311.04664v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04664","description":"<p>Despite known differences between reading and listening in the brain, recent\nwork has shown that text-based language models predict both text-evoked and\nspeech-evoked brain activity to an impressive degree. This poses the question\nof what types of information language models truly predict in the brain. We\ninvestigate this question via a direct approach, in which we eliminate\ninformation related to specific low-level stimulus features (textual, speech,\nand visual) in the language model representations, and observe how this\nintervention affects the alignment with fMRI brain recordings acquired while\nparticipants read versus listened to the same naturalistic stories. We further\ncontrast our findings with speech-based language models, which would be\nexpected to predict speech-evoked brain activity better, provided they model\nlanguage processing in the brain well. Using our direct approach, we find that\nboth text-based and speech-based language models align well with early sensory\nregions due to shared low-level features. Text-based models continue to align\nwell with later language regions even after removing these features, while,\nsurprisingly, speech-based models lose most of their alignment. These findings\nsuggest that speech-based models can be further improved to better reflect\nbrain-like language processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oota_S/0/1/0/all/0/1\">Subba Reddy Oota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celik_E/0/1/0/all/0/1\">Emin &#xc7;elik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deniz_F/0/1/0/all/0/1\">Fatma Deniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toneva_M/0/1/0/all/0/1\">Mariya Toneva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-training LLMs using human-like development data corpus. (arXiv:2311.04666v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04666","description":"<p>Pre-trained Large Language Models (LLMs) have shown success in a diverse set\nof language inference and understanding tasks. The pre-training stage of LLMs\nlooks at a large corpus of raw textual data. The BabyLM shared task compares\nLLM pre-training to human language acquisition, where the number of tokens seen\nby 13-year-old kids is magnitudes smaller than the number of tokens seen by\nLLMs. In this work, we pre-train and evaluate LLMs on their ability to learn\ncontextual word representations using roughly the same number of tokens as seen\nby children. We provide a strong set of baselines; with different\narchitectures, evaluation of changes in performance across epochs, and reported\npre-training metrics for the strict small and strict tracks of the task. We\nalso try to loosely replicate the RoBERTa baseline given by the task organizers\nto observe the training robustness to hyperparameter selection and\nreplicability. We provide the submission details to the strict and strict-small\ntracks in this report.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_K/0/1/0/all/0/1\">Khushi Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Raj Sanjay Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_S/0/1/0/all/0/1\">Sashank Varma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Generative Ad Hoc Information Retrieval. (arXiv:2311.04694v1 [cs.IR])","link":"http://arxiv.org/abs/2311.04694","description":"<p>Recent advances in large language models have enabled the development of\nviable generative information retrieval systems. A generative retrieval system\nreturns a grounded generated text in response to an information need instead of\nthe traditional document ranking. Quantifying the utility of these types of\nresponses is essential for evaluating generative retrieval systems. As the\nestablished evaluation methodology for ranking-based ad hoc retrieval may seem\nunsuitable for generative retrieval, new approaches for reliable, repeatable,\nand reproducible experimentation are required. In this paper, we survey the\nrelevant information retrieval and natural language processing literature,\nidentify search tasks and system architectures in generative retrieval, develop\na corresponding user model, and study its operationalization. This theoretical\nanalysis provides a foundation and new insights for the evaluation of\ngenerative ad hoc retrieval systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gienapp_L/0/1/0/all/0/1\">Lukas Gienapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scells_H/0/1/0/all/0/1\">Harrisen Scells</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deckers_N/0/1/0/all/0/1\">Niklas Deckers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bevendorff_J/0/1/0/all/0/1\">Janek Bevendorff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiesel_J/0/1/0/all/0/1\">Johannes Kiesel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1\">Shahbaz Syed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frobe_M/0/1/0/all/0/1\">Maik Fr&#xf6;be</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zucoon_G/0/1/0/all/0/1\">Guide Zucoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1\">Benno Stein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagen_M/0/1/0/all/0/1\">Matthias Hagen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using large language models to study human memory for meaningful narratives. (arXiv:2311.04742v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04742","description":"<p>One of the most impressive achievements of the AI revolution is the\ndevelopment of large language models that can generate meaningful text and\nrespond to instructions in plain English with no additional training necessary.\nHere we show that language models can be used as a scientific instrument for\nstudying human memory for meaningful material. We developed a pipeline for\ndesigning large scale memory experiments and analyzing the obtained results. We\nperformed online memory experiments with a large number of participants and\ncollected recognition and recall data for narratives of different lengths. We\nfound that both recall and recognition performance scale linearly with\nnarrative length. Furthermore, in order to investigate the role of narrative\ncomprehension in memory, we repeated these experiments using scrambled versions\nof the presented stories. We found that even though recall performance declined\nsignificantly, recognition remained largely unaffected. Interestingly, recalls\nin this condition seem to follow the original narrative order rather than the\nscrambled presentation, pointing to a contextual reconstruction of the story in\nmemory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Can_A/0/1/0/all/0/1\">Antonios Georgiou Tankut Can</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katkov_M/0/1/0/all/0/1\">Mikhail Katkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsodyks_M/0/1/0/all/0/1\">Misha Tsodyks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Determination of toxic comments and unintended model bias minimization using Deep learning approach. (arXiv:2311.04789v1 [cs.LG])","link":"http://arxiv.org/abs/2311.04789","description":"<p>Online conversations can be toxic and subjected to threats, abuse, or\nharassment. To identify toxic text comments, several deep learning and machine\nlearning models have been proposed throughout the years. However, recent\nstudies demonstrate that because of the imbalances in the training data, some\nmodels are more likely to show unintended biases including gender bias and\nidentity bias. In this research, our aim is to detect toxic comment and reduce\nthe unintended bias concerning identity features such as race, gender, sex,\nreligion by fine-tuning an attention based model called BERT(Bidirectional\nEncoder Representation from Transformers). We apply weighted loss to address\nthe issue of unbalanced data and compare the performance of a fine-tuned BERT\nmodel with a traditional Logistic Regression model in terms of classification\nand bias minimization. The Logistic Regression model with the TFIDF vectorizer\nachieve 57.1% accuracy, and fine-tuned BERT model's accuracy is 89%. Code is\navailable at\nhttps://github.com/zim10/Determine_Toxic_comment_and_identity_bias.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Md Azim Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DACBERT: Leveraging Dependency Agreement for Cost-Efficient Bert Pretraining. (arXiv:2311.04799v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04799","description":"<p>Building on the cost-efficient pretraining advancements brought about by\nCrammed BERT, we enhance its performance and interpretability further by\nintroducing a novel pretrained model Dependency Agreement Crammed BERT\n(DACBERT) and its two-stage pretraining framework - Dependency Agreement\nPretraining. This framework, grounded by linguistic theories, seamlessly weaves\nsyntax and semantic information into the pretraining process. The first stage\nemploys four dedicated submodels to capture representative dependency\nagreements at the chunk level, effectively converting these agreements into\nembeddings. The second stage uses these refined embeddings, in tandem with\nconventional BERT embeddings, to guide the pretraining of the rest of the\nmodel. Evaluated on the GLUE benchmark, our DACBERT demonstrates notable\nimprovement across various tasks, surpassing Crammed BERT by 3.13% in the RTE\ntask and by 2.26% in the MRPC task. Furthermore, our method boosts the average\nGLUE score by 0.83%, underscoring its significant potential. The pretraining\nprocess can be efficiently executed on a single GPU within a 24-hour cycle,\nnecessitating no supplementary computational resources or extending the\npretraining duration compared with the Crammed BERT. Extensive studies further\nilluminate our approach's instrumental role in bolstering the interpretability\nof pretrained language models for natural language understanding tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuo_M/0/1/0/all/0/1\">Martin Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiran Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MTGER: Multi-view Temporal Graph Enhanced Temporal Reasoning over Time-Involved Document. (arXiv:2311.04816v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04816","description":"<p>The facts and time in the document are intricately intertwined, making\ntemporal reasoning over documents challenging. Previous work models time\nimplicitly, making it difficult to handle such complex relationships. To\naddress this issue, we propose MTGER, a novel Multi-view Temporal Graph\nEnhanced Temporal Reasoning framework for temporal reasoning over time-involved\ndocuments. Concretely, MTGER explicitly models the temporal relationships among\nfacts by multi-view temporal graphs. On the one hand, the heterogeneous\ntemporal graphs explicitly model the temporal and discourse relationships among\nfacts; on the other hand, the multi-view mechanism captures both time-focused\nand fact-focused information, allowing the two views to complement each other\nthrough adaptive fusion. To further improve the implicit reasoning capability\nof the model, we design a self-supervised time-comparing objective. Extensive\nexperimental results demonstrate the effectiveness of our method on the TimeQA\nand SituatedQA datasets. Furthermore, MTGER gives more consistent answers under\nquestion perturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1\">Zheng Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zekun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiafeng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchically Gated Recurrent Neural Network for Sequence Modeling. (arXiv:2311.04823v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04823","description":"<p>Transformers have surpassed RNNs in popularity due to their superior\nabilities in parallel training and long-term dependency modeling. Recently,\nthere has been a renewed interest in using linear RNNs for efficient sequence\nmodeling. These linear RNNs often employ gating mechanisms in the output of the\nlinear recurrence layer while ignoring the significance of using forget gates\nwithin the recurrence. In this paper, we propose a gated linear RNN model\ndubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes\nforget gates that are lower bounded by a learnable value. The lower bound\nincreases monotonically when moving up layers. This allows the upper layers to\nmodel long-term dependencies and the lower layers to model more local,\nshort-term dependencies. Experiments on language modeling, image\nclassification, and long-range arena benchmarks showcase the efficiency and\neffectiveness of our proposed model. The source code is available at\nhttps://github.com/OpenNLPLab/HGRN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Songlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Benchmark and Contamination for Language Models with Rephrased Samples. (arXiv:2311.04850v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04850","description":"<p>Large language models are increasingly trained on all the data ever produced\nby humans. Many have raised concerns about the trustworthiness of public\nbenchmarks due to potential contamination in pre-training or fine-tuning\ndatasets. While most data decontamination efforts apply string matching (e.g.,\nn-gram overlap) to remove benchmark data, we show that these methods are\ninsufficient, and simple variations of test data (e.g., paraphrasing,\ntranslation) can easily bypass these decontamination measures. Furthermore, we\ndemonstrate that if such variation of test data is not eliminated, a 13B model\ncan easily overfit a test benchmark and achieve drastically high performance,\non par with GPT-4. We validate such observations in widely used benchmarks such\nas MMLU, GSK8k, and HumanEval. To address this growing risk, we propose a\nstronger LLM-based decontamination method and apply it to widely used\npre-training and fine-tuning datasets, revealing significant previously unknown\ntest overlap. For example, in pre-training sets such as RedPajama-Data-1T and\nStarCoder-Data, we identified that 8-18\\% of the HumanEval benchmark overlaps.\nInterestingly, we also find such contamination in synthetic dataset generated\nby GPT-3.5/4, suggesting a potential risk of unintentional contamination. We\nurge the community to adopt stronger decontamination approaches when using\npublic benchmarks. Moreover, we call for the community to actively develop\nfresh one-time exams to evaluate models accurately. Our decontamination tool is\npublicly available at https://github.com/lm-sys/llm-decontaminator.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_W/0/1/0/all/0/1\">Wei-Lin Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lianmin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1\">Ion Stoica</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LongQLoRA: Efficient and Effective Method to Extend Context Length of Large Language Models. (arXiv:2311.04879v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04879","description":"<p>We present LongQLoRA, an efficient and effective method to extend context\nlength of large language models with less training resources. LongQLoRA\ncombines the advantages of Position Interpolation, QLoRA and Shift Short\nAttention of LongLoRA. With a single 32GB V100 GPU, LongQLoRA can extend the\ncontext length of LLaMA2 7B and 13B from 4096 to 8192 and even to 12k within\n1000 finetuning steps. LongQLoRA achieves competitive perplexity performance on\nPG19 and Proof-pile datasets, our model outperforms LongLoRA and is very close\nto MPT-7B-8K within the evaluation context length of 8192. We collect and build\n39k long instruction data to extend context length of Vicuna-13B from 4096 to\n8192 and achieve good performance both in long and short context generation\ntask. We also do some ablation experiments to study the effect of LoRA rank,\nfinetuning steps and attention patterns in inference.The model weights,\ntraining data and code are avaliable at\nhttps://github.com/yangjianxin1/LongQLoRA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianxin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Profiling Irony & Stereotype: Exploring Sentiment, Topic, and Lexical Features. (arXiv:2311.04885v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04885","description":"<p>Social media has become a very popular source of information. With this\npopularity comes an interest in systems that can classify the information\nproduced. This study tries to create such a system detecting irony in Twitter\nusers. Recent work emphasize the importance of lexical features, sentiment\nfeatures and the contrast herein along with TF-IDF and topic models. Based on a\nthorough feature selection process, the resulting model contains specific\nsub-features from these areas. Our model reaches an F1-score of 0.84, which is\nabove the baseline. We find that lexical features, especially TF-IDF,\ncontribute the most to our models while sentiment and topic modeling features\ncontribute less to overall performance. Lastly, we highlight multiple\ninteresting and important paths for further exploration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krols_T/0/1/0/all/0/1\">Tibor L. R. Krols</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortensen_M/0/1/0/all/0/1\">Marie Mortensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oldenburg_N/0/1/0/all/0/1\">Ninell Oldenburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SEMQA: Semi-Extractive Multi-Source Question Answering. (arXiv:2311.04886v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04886","description":"<p>Recently proposed long-form question answering (QA) systems, supported by\nlarge language models (LLMs), have shown promising capabilities. Yet,\nattributing and verifying their generated abstractive answers can be difficult,\nand automatically evaluating their accuracy remains an ongoing challenge.\n</p>\n<p>In this work, we introduce a new QA task for answering multi-answer questions\nby summarizing multiple diverse sources in a semi-extractive fashion.\nSpecifically, Semi-extractive Multi-source QA (SEMQA) requires models to output\na comprehensive answer, while mixing factual quoted spans -- copied verbatim\nfrom given input sources -- and non-factual free-text connectors that glue\nthese spans together into a single cohesive passage. This setting bridges the\ngap between the outputs of well-grounded but constrained extractive QA systems\nand more fluent but harder to attribute fully abstractive answers.\nParticularly, it enables a new mode for language models that leverages their\nadvanced language generation capabilities, while also producing fine in-line\nattributions by-design that are easy to verify, interpret, and evaluate.\n</p>\n<p>To study this task, we create the first dataset of this kind, QuoteSum, with\nhuman-written semi-extractive answers to natural and generated questions, and\ndefine text-based evaluation metrics. Experimenting with several LLMs in\nvarious settings, we find this task to be surprisingly challenging,\ndemonstrating the importance of QuoteSum for developing and studying such\nconsolidation capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lelkes_A/0/1/0/all/0/1\">Adam D. Lelkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haitian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs. (arXiv:2311.04892v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04892","description":"<p>Recent works have showcased the ability of large-scale language models (LLMs)\nto embody diverse personas in their responses, exemplified by prompts like 'You\nare Yoda. Explain the Theory of Relativity.' While this ability allows\npersonalization of LLMs and enables human behavior simulation, its effect on\nLLMs' capabilities remain unclear. To fill this gap, we present the first\nextensive study of the unintended side-effects of persona assignment on the\nability of LLMs, specifically ChatGPT, to perform basic reasoning tasks. Our\nstudy covers 24 reasoning datasets and 16 diverse personas spanning 5\nsocio-demographic groups: race, gender, religion, disability, and political\naffiliation. Our experiments unveil that ChatGPT carries deep rooted bias\nagainst various socio-demographics underneath a veneer of fairness. While it\novertly rejects stereotypes when explicitly asked ('Are Black people less\nskilled at mathematics?'), it manifests stereotypical and often erroneous\npresumptions when prompted to answer questions while taking on a persona. These\ncan be observed as abstentions in the model responses, e.g., 'As a Black\nperson, I am unable to answer this question as it requires math knowledge', and\ngenerally result in a substantial drop in performance on reasoning tasks. We\nfind that this inherent deep bias is ubiquitous - 80% of our personas\ndemonstrated bias; it is significant - certain datasets had relative drops in\nperformance of 70%+; and can be especially harmful for certain groups - certain\npersonas had stat. sign. drops on more than 80% of the datasets. Further\nanalysis shows that these persona-induced errors can be hard-to-discern and\nhard-to-avoid. Our findings serve as a cautionary tale that the practice of\nassigning personas to LLMs - a trend on the rise - can surface their\ndeep-rooted biases and have unforeseeable and detrimental side-effects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shashank Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_V/0/1/0/all/0/1\">Vaishnavi Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1\">Ameet Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1\">Ashwin Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1\">Tushar Khot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Future Lens: Anticipating Subsequent Tokens from a Single Hidden State. (arXiv:2311.04897v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04897","description":"<p>We conjecture that hidden state vectors corresponding to individual input\ntokens encode information sufficient to accurately predict several tokens\nahead. More concretely, in this paper we ask: Given a hidden (internal)\nrepresentation of a single token at position $t$ in an input, can we reliably\nanticipate the tokens that will appear at positions $\\geq t + 2$? To test this,\nwe measure linear approximation and causal intervention methods in GPT-J-6B to\nevaluate the degree to which individual hidden states in the network contain\nsignal rich enough to predict future hidden states and, ultimately, token\noutputs. We find that, at some layers, we can approximate a model's output with\nmore than 48% accuracy with respect to its prediction of subsequent tokens\nthrough a single hidden state. Finally we present a \"Future Lens\" visualization\nthat uses these methods to create a new view of transformer states.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pal_K/0/1/0/all/0/1\">Koyena Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiuding Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_A/0/1/0/all/0/1\">Andrew Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure. (arXiv:2311.04900v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04900","description":"<p>Language models are typically evaluated on their success at predicting the\ndistribution of specific words in specific contexts. Yet linguistic knowledge\nalso encodes relationships between contexts, allowing inferences between word\ndistributions. We investigate the degree to which pre-trained Transformer-based\nlarge language models (LLMs) represent such relationships, focusing on the\ndomain of argument structure. We find that LLMs perform well in generalizing\nthe distribution of a novel noun argument between related contexts that were\nseen during pre-training (e.g., the active object and passive subject of the\nverb spray), succeeding by making use of the semantically-organized structure\nof the embedding space for word embeddings. However, LLMs fail at\ngeneralizations between related contexts that have not been observed during\npre-training, but which instantiate more abstract, but well-attested structural\ngeneralizations (e.g., between the active object and passive subject of an\narbitrary verb). Instead, in this case, LLMs show a bias to generalize based on\nlinear order. This finding points to a limitation with current models and\npoints to a reason for which their training is data-intensive.s reported here\nare available at https://github.com/clay-lab/structural-alternations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilson_M/0/1/0/all/0/1\">Michael Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petty_J/0/1/0/all/0/1\">Jackson Petty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_R/0/1/0/all/0/1\">Robert Frank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models. (arXiv:2311.04902v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04902","description":"<p>Large Language Models (LLMs) with a billion or more parameters are prime\ntargets for network pruning, which aims to reduce a portion of the network\nweights without compromising performance. Prior approaches such as Weights\nMagnitude, SparseGPT, and Wanda, either concentrated solely on weights or\nintegrated weights with activations for sparsity. However, they overlooked the\ninformative gradients derived from pretrained large language models. In this\npaper, we present a novel sparsity-centric pruning method for pretrained LLMs,\ntermed Gradient-based Language Model Pruner (GBLM-Pruner). GBLM-Pruner\nleverages the first-order term of the Taylor expansion, operating in a\ntraining-free manner by harnessing properly normalized gradients from a few\ncalibration samples to determine the importance pruning score, and\nsubstantially outperforms competitive counterparts like SparseGPT and Wanda in\nmultiple benchmarks. Intriguing, after incorporating gradients, the\nunstructured pruning method tends to reveal some structural patterns\npost-pruning, which mirrors the geometric interdependence inherent in the LLMs'\nparameter structure. Additionally, GBLM-Pruner functions without any subsequent\nretraining or weight updates to maintain its simplicity as other counterparts.\nExtensive evaluations on LLaMA-1 and LLaMA-2 across various language benchmarks\nand perplexity show that GBLM-Pruner surpasses magnitude pruning, Wanda\n(weights+activations) and SparseGPT (weights+activations+weight update) by\nsignificant margins. Our code and models are available at\nhttps://github.com/RocktimJyotiDas/GBLM-Pruner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Rocktim Jyoti Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Liqun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhiqiang Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoFormer: Enhanced Transformer with Rotary Position Embedding. (arXiv:2104.09864v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.09864","description":"<p>Position encoding recently has shown effective in the transformer\narchitecture. It enables valuable supervision for dependency modeling between\nelements at different positions of the sequence. In this paper, we first\ninvestigate various methods to integrate positional information into the\nlearning process of transformer-based language models. Then, we propose a novel\nmethod named Rotary Position Embedding(RoPE) to effectively leverage the\npositional information. Specifically, the proposed RoPE encodes the absolute\nposition with a rotation matrix and meanwhile incorporates the explicit\nrelative position dependency in self-attention formulation. Notably, RoPE\nenables valuable properties, including the flexibility of sequence length,\ndecaying inter-token dependency with increasing relative distances, and the\ncapability of equipping the linear self-attention with relative position\nencoding. Finally, we evaluate the enhanced transformer with rotary position\nembedding, also called RoFormer, on various long text classification benchmark\ndatasets. Our experiments show that it consistently overcomes its alternatives.\nFurthermore, we provide a theoretical analysis to explain some experimental\nresults. RoFormer is already integrated into Huggingface:\n\\url{https://huggingface.co/docs/transformers/model_doc/roformer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jianlin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shengfeng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murtadha_A/0/1/0/all/0/1\">Ahmed Murtadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bo Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunfeng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decomposing Natural Logic Inferences in Neural NLI. (arXiv:2112.08289v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08289","description":"<p>In the interest of interpreting neural NLI models and their reasoning\nstrategies, we carry out a systematic probing study which investigates whether\nthese models capture the crucial semantic features central to natural logic:\nmonotonicity and concept inclusion. Correctly identifying valid inferences in\ndownward-monotone contexts is a known stumbling block for NLI performance,\nsubsuming linguistic phenomena such as negation scope and generalized\nquantifiers. To understand this difficulty, we emphasize monotonicity as a\nproperty of a context and examine the extent to which models capture\nmonotonicity information in the contextual embeddings which are intermediate to\ntheir decision making process. Drawing on the recent advancement of the probing\nparadigm, we compare the presence of monotonicity features across various\nmodels. We find that monotonicity information is notably weak in the\nrepresentations of popular NLI models which achieve high scores on benchmarks,\nand observe that previous improvements to these models based on fine-tuning\nstrategies have introduced stronger monotonicity features together with their\nimproved performance on challenge sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rozanova_J/0/1/0/all/0/1\">Julia Rozanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_D/0/1/0/all/0/1\">Deborah Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1\">Marco Valentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thayaparan_M/0/1/0/all/0/1\">Mokanrarangan Thayaparan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andre Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Large (Visual) Language Models for Robot 3D Scene Understanding. (arXiv:2209.05629v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2209.05629","description":"<p>Abstract semantic 3D scene understanding is a problem of critical importance\nin robotics. As robots still lack the common-sense knowledge about household\nobjects and locations of an average human, we investigate the use of\npre-trained language models to impart common sense for scene understanding. We\nintroduce and compare a wide range of scene classification paradigms that\nleverage language only (zero-shot, embedding-based, and structured-language) or\nvision and language (zero-shot and fine-tuned). We find that the best\napproaches in both categories yield $\\sim 70\\%$ room classification accuracy,\nexceeding the performance of pure-vision and graph classifiers. We also find\nsuch methods demonstrate notable generalization and transfer capabilities\nstemming from their use of language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">William Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Siyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talak_R/0/1/0/all/0/1\">Rajat Talak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Designing Robust Transformers using Robust Kernel Density Estimation. (arXiv:2210.05794v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.05794","description":"<p>Recent advances in Transformer architectures have empowered their empirical\nsuccess in a variety of tasks across different domains. However, existing works\nmainly focus on predictive accuracy and computational cost, without considering\nother practical issues, such as robustness to contaminated samples. Recent work\nby Nguyen et al., (2022) has shown that the self-attention mechanism, which is\nthe center of the Transformer architecture, can be viewed as a non-parametric\nestimator based on kernel density estimation (KDE). This motivates us to\nleverage a set of robust kernel density estimation methods for alleviating the\nissue of data contamination. Specifically, we introduce a series of\nself-attention mechanisms that can be incorporated into different Transformer\narchitectures and discuss the special properties of each method. We then\nperform extensive empirical studies on language modeling and image\nclassification tasks. Our methods demonstrate robust performance in multiple\nscenarios while maintaining competitive results on clean datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1\">Tongzheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tan Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_J/0/1/0/all/0/1\">Joydeep Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Nhat Ho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neuro-Symbolic Causal Reasoning Meets Signaling Game for Emergent Semantic Communications. (arXiv:2210.12040v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.12040","description":"<p>Semantic communication (SC) aims to communicate reliably with minimal data\ntransfer while simultaneously providing seamless connectivity to heterogeneous\nservices and users. In this paper, a novel emergent SC (ESC) system framework\nis proposed and is composed of a signaling game for emergent language design\nand a neuro-symbolic (NeSy) artificial intelligence (AI) approach for causal\nreasoning. In order to design the language, the signaling game is solved using\nan alternating maximization between the communicating node's utilities. The\nemergent language helps create a context-aware transmit vocabulary (minimal\nsemantic representation) and aids the reasoning process (enabling\ngeneralization to unseen scenarios) by splitting complex messages into simpler\nreasoning tasks for the receiver. The causal description at the transmitter is\nthen modeled (a neural component) as a posterior distribution of the relevant\nattributes present in the data. Using the reconstructed causal state, the\nreceiver evaluates a set of logical formulas (symbolic part) to execute its\ntask. The nodes NeSy reasoning components are implemented by the recently\nproposed AI tool called Generative Flow Networks, and they are optimized for\nhigher semantic reliability. The ESC system is designed to enhance the novel\nmetrics of semantic information, reliability, distortion and similarity that\nare designed using rigorous algebraic properties from category theory thereby\ngeneralizing the metrics beyond Shannon's notion of uncertainty. Simulation\nresults validate the ability of ESC to communicate efficiently (with reduced\nbits) and achieve better semantic reliability than conventional wireless and\nstate-of-the-art systems that do not exploit causal reasoning capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thomas_C/0/1/0/all/0/1\">Christo Kurisummoottil Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1\">Walid Saad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint processing of linguistic properties in brains and language models. (arXiv:2212.08094v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08094","description":"<p>Language models have been shown to be very effective in predicting brain\nrecordings of subjects experiencing complex language stimuli. For a deeper\nunderstanding of this alignment, it is important to understand the\ncorrespondence between the detailed processing of linguistic information by the\nhuman brain versus language models. We investigate this correspondence via a\ndirect approach, in which we eliminate information related to specific\nlinguistic properties in the language model representations and observe how\nthis intervention affects the alignment with fMRI brain recordings obtained\nwhile participants listened to a story. We investigate a range of linguistic\nproperties (surface, syntactic, and semantic) and find that the elimination of\neach one results in a significant decrease in brain alignment. Specifically, we\nfind that syntactic properties (i.e. Top Constituents and Tree Depth) have the\nlargest effect on the trend of brain alignment across model layers. These\nfindings provide clear evidence for the role of specific linguistic information\nin the alignment between brain and language models, and open new avenues for\nmapping the joint information processing in both systems. We make the code\npublicly available\n[https://github.com/subbareddy248/linguistic-properties-brain-alignment].\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oota_S/0/1/0/all/0/1\">Subba Reddy Oota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toneva_M/0/1/0/all/0/1\">Mariya Toneva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MarioGPT: Open-Ended Text2Level Generation through Large Language Models. (arXiv:2302.05981v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2302.05981","description":"<p>Procedural Content Generation (PCG) is a technique to generate complex and\ndiverse environments in an automated way. However, while generating content\nwith PCG methods is often straightforward, generating meaningful content that\nreflects specific intentions and constraints remains challenging. Furthermore,\nmany PCG algorithms lack the ability to generate content in an open-ended\nmanner. Recently, Large Language Models (LLMs) have shown to be incredibly\neffective in many diverse domains. These trained LLMs can be fine-tuned,\nre-using information and accelerating training for new tasks. Here, we\nintroduce MarioGPT, a fine-tuned GPT2 model trained to generate tile-based game\nlevels, in our case Super Mario Bros levels. MarioGPT can not only generate\ndiverse levels, but can be text-prompted for controllable level generation,\naddressing one of the key challenges of current PCG techniques. As far as we\nknow, MarioGPT is the first text-to-level model and combined with novelty\nsearch it enables the generation of diverse levels with varying play-style\ndynamics (i.e. player paths) and the open-ended discovery of an increasingly\ndiverse range of content. Code available at\nhttps://github.com/shyamsn97/mario-gpt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sudhakaran_S/0/1/0/all/0/1\">Shyam Sudhakaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Duque_M/0/1/0/all/0/1\">Miguel Gonz&#xe1;lez-Duque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glanois_C/0/1/0/all/0/1\">Claire Glanois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freiberger_M/0/1/0/all/0/1\">Matthias Freiberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najarro_E/0/1/0/all/0/1\">Elias Najarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risi_S/0/1/0/all/0/1\">Sebastian Risi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Debiasing Scores and Prompts of 2D Diffusion for View-consistent Text-to-3D Generation. (arXiv:2303.15413v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2303.15413","description":"<p>Existing score-distilling text-to-3D generation techniques, despite their\nconsiderable promise, often encounter the view inconsistency problem. One of\nthe most notable issues is the Janus problem, where the most canonical view of\nan object (\\textit{e.g}., face or head) appears in other views. In this work,\nwe explore existing frameworks for score-distilling text-to-3D generation and\nidentify the main causes of the view inconsistency problem -- the embedded bias\nof 2D diffusion models. Based on these findings, we propose two approaches to\ndebias the score-distillation frameworks for view-consistent text-to-3D\ngeneration. Our first approach, called score debiasing, involves cutting off\nthe score estimated by 2D diffusion models and gradually increasing the\ntruncation value throughout the optimization process. Our second approach,\ncalled prompt debiasing, identifies conflicting words between user prompts and\nview prompts using a language model, and adjusts the discrepancy between view\nprompts and the viewing direction of an object. Our experimental results show\nthat our methods improve the realism of the generated 3D objects by\nsignificantly reducing artifacts and achieve a good trade-off between\nfaithfulness to the 2D diffusion models and 3D consistency with little\noverhead. Our project page is available\nat~\\url{https://susunghong.github.io/Debiased-Score-Distillation-Sampling/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Susung Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_D/0/1/0/all/0/1\">Donghoon Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Intended Uses of Automated Fact-Checking Artefacts: Why, How and Who. (arXiv:2304.14238v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.14238","description":"<p>Automated fact-checking is often presented as an epistemic tool that\nfact-checkers, social media consumers, and other stakeholders can use to fight\nmisinformation. Nevertheless, few papers thoroughly discuss how. We document\nthis by analysing 100 highly-cited papers, and annotating epistemic elements\nrelated to intended use, i.e., means, ends, and stakeholders. We find that\nnarratives leaving out some of these aspects are common, that many papers\npropose inconsistent means and ends, and that the feasibility of suggested\nstrategies rarely has empirical backing. We argue that this vagueness actively\nhinders the technology from reaching its goals, as it encourages overclaiming,\nlimits criticism, and prevents stakeholder feedback. Accordingly, we provide\nseveral recommendations for thinking and writing about the use of fact-checking\nartefacts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1\">Michael Schlichtkrull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ousidhoum_N/0/1/0/all/0/1\">Nedjma Ousidhoum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses. (arXiv:2305.11243v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11243","description":"<p>Developmental psychologists have spent decades devising experiments to test\nthe intelligence and knowledge of infants and children, tracing the origin of\ncrucial concepts and capacities. Moreover, experimental techniques in\ndevelopmental psychology have been carefully designed to discriminate the\ncognitive capacities that underlie particular behaviors. We propose that using\nclassical experiments from child development is a particularly effective way to\nprobe the computational abilities of AI models, in general, and LLMs in\nparticular. First, the methodological techniques of developmental psychology,\nsuch as the use of novel stimuli to control for past experience or control\nconditions to determine whether children are using simple associations, can be\nequally helpful for assessing the capacities of LLMs. In parallel, testing LLMs\nin this way can tell us whether the information that is encoded in text is\nsufficient to enable particular responses, or whether those responses depend on\nother kinds of information, such as information from exploration of the\nphysical world. In this work we adapt classical developmental experiments to\nevaluate the capabilities of LaMDA, a large language model from Google. We\npropose a novel LLM Response Score (LRS) metric which can be used to evaluate\nother language models, such as GPT. We find that LaMDA generates appropriate\nresponses that are similar to those of children in experiments involving social\nunderstanding, perhaps providing evidence that knowledge of these domains is\ndiscovered through language. On the other hand, LaMDA's responses in early\nobject and action understanding, theory of mind, and especially causal\nreasoning tasks are very different from those of young children, perhaps\nshowing that these domains require more real-world, self-initiated exploration\nand cannot simply be learned from patterns in language input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kosoy_E/0/1/0/all/0/1\">Eliza Kosoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reagan_E/0/1/0/all/0/1\">Emily Rose Reagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1\">Leslie Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopnik_A/0/1/0/all/0/1\">Alison Gopnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobb_D/0/1/0/all/0/1\">Danielle Krettek Cobb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextualizing Argument Quality Assessment with Relevant Knowledge. (arXiv:2305.12280v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12280","description":"<p>Automatic assessment of the quality of arguments has been recognized as a\nchallenging task with significant implications for misinformation and targeted\nspeech. While real-world arguments are tightly anchored in context, existing\ncomputational methods analyze their quality in isolation, which affects their\naccuracy and generalizability. We propose SPARK: a novel method for scoring\nargument quality based on contextualization via relevant knowledge. We devise\nfour augmentations that leverage large language models to provide feedback,\ninfer hidden assumptions, supply a similar-quality argument, or give a\ncounter-argument. SPARK uses a dual-encoder Transformer architecture to enable\nthe original argument and its augmentation to be considered jointly. Our\nexperiments in both in-domain and zero-shot setups show that SPARK consistently\noutperforms existing techniques across multiple metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_D/0/1/0/all/0/1\">Darshan Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sourati_Z/0/1/0/all/0/1\">Zhivar Sourati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morstatter_F/0/1/0/all/0/1\">Fred Morstatter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fair Without Leveling Down: A New Intersectional Fairness Definition. (arXiv:2305.12495v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.12495","description":"<p>In this work, we consider the problem of intersectional group fairness in the\nclassification setting, where the objective is to learn discrimination-free\nmodels in the presence of several intersecting sensitive groups. First, we\nillustrate various shortcomings of existing fairness measures commonly used to\ncapture intersectional fairness. Then, we propose a new definition called the\n$\\alpha$-Intersectional Fairness, which combines the absolute and the relative\nperformance across sensitive groups and can be seen as a generalization of the\nnotion of differential fairness. We highlight several desirable properties of\nthe proposed definition and analyze its relation to other fairness measures.\nFinally, we benchmark multiple popular in-processing fair machine learning\napproaches using our new fairness definition and show that they do not achieve\nany improvement over a simple baseline. Our results reveal that the increase in\nfairness measured by previous definitions hides a \"leveling down\" effect, i.e.,\ndegrading the best performance over groups rather than improving the worst one.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_G/0/1/0/all/0/1\">Gaurav Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellet_A/0/1/0/all/0/1\">Aur&#xe9;lien Bellet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denis_P/0/1/0/all/0/1\">Pascal Denis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_M/0/1/0/all/0/1\">Mikaela Keller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AVeriTeC: A Dataset for Real-world Claim Verification with Evidence from the Web. (arXiv:2305.13117v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13117","description":"<p>Existing datasets for automated fact-checking have substantial limitations,\nsuch as relying on artificial claims, lacking annotations for evidence and\nintermediate reasoning, or including evidence published after the claim. In\nthis paper we introduce AVeriTeC, a new dataset of 4,568 real-world claims\ncovering fact-checks by 50 different organizations. Each claim is annotated\nwith question-answer pairs supported by evidence available online, as well as\ntextual justifications explaining how the evidence combines to produce a\nverdict. Through a multi-round annotation process, we avoid common pitfalls\nincluding context dependence, evidence insufficiency, and temporal leakage, and\nreach a substantial inter-annotator agreement of $\\kappa=0.619$ on verdicts. We\ndevelop a baseline as well as an evaluation scheme for verifying claims through\nseveral question-answering steps against the open web.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1\">Michael Schlichtkrull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhijiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incongruity-Aware Hierarchical Crossmodal Transformer with Dynamic Modality Gating: A Study on Affect Recognition. (arXiv:2305.13583v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13583","description":"<p>Fusing multiple modalities has proven effective for multimodal information\nprocessing. However, the incongruity between modalities poses a challenge for\nmultimodal fusion, especially in affect recognition. In this study, we first\nanalyze how the salient affective information in one modality can be affected\nby the other, and demonstrate that inter-modal incongruity exists latently in\ncrossmodal attention. Based on this finding, we propose the Hierarchical\nCrossmodal Transformer with Dynamic Modality Gating (HCT-DMG), a lightweight\nincongruity-aware model, which dynamically chooses the primary modality in each\ntraining batch and reduces fusion times by leveraging the learned hierarchy in\nthe latent space to alleviate incongruity. The experimental evaluation on five\nbenchmark datasets: CMU-MOSI, CMU-MOSEI, and IEMOCAP (sentiment and emotion),\nwhere incongruity implicitly lies in hard samples, as well as UR-FUNNY (humour)\nand MUStaRD (sarcasm), where incongruity is common, verifies the efficacy of\nour approach, showing that HCT-DMG: 1) outperforms previous multimodal models\nwith a reduced size of approximately 0.8M parameters; 2) recognizes hard\nsamples where incongruity makes affect recognition difficult; 3) mitigates the\nincongruity at the latent level in crossmodal attention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaoting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bell_P/0/1/0/all/0/1\">Peter Bell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Catherine Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Robustness of Finetuned Transformer-based NLP Models. (arXiv:2305.14453v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14453","description":"<p>Transformer-based pretrained models like BERT, GPT-2 and T5 have been\nfinetuned for a large number of natural language processing (NLP) tasks, and\nhave been shown to be very effective. However, while finetuning, what changes\nacross layers in these models with respect to pretrained checkpoints is\nunder-studied. Further, how robust are these models to perturbations in input\ntext? Does the robustness vary depending on the NLP task for which the models\nhave been finetuned? While there exists some work on studying the robustness of\nBERT finetuned for a few NLP tasks, there is no rigorous study that compares\nthis robustness across encoder only, decoder only and encoder-decoder models.\nIn this paper, we characterize changes between pretrained and finetuned\nlanguage model representations across layers using two metrics: CKA and STIR.\nFurther, we study the robustness of three language models (BERT, GPT-2 and T5)\nwith eight different text perturbations on classification tasks from the\nGeneral Language Understanding Evaluation (GLUE) benchmark, and generation\ntasks like summarization, free-form generation and question generation. GPT-2\nrepresentations are more robust than BERT and T5 across multiple types of input\nperturbation. Although models exhibit good robustness broadly, dropping nouns,\nverbs or changing characters are the most impactful. Overall, this study\nprovides valuable insights into perturbation-specific weaknesses of popular\nTransformer-based models, which should be kept in mind when passing inputs. We\nmake the code and models publicly available\n[https://github.com/PavanNeerudu/Robustness-of-Transformers-models].\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neerudu_P/0/1/0/all/0/1\">Pavan Kalyan Reddy Neerudu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oota_S/0/1/0/all/0/1\">Subba Reddy Oota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marreddy_M/0/1/0/all/0/1\">Mounika Marreddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kagita_V/0/1/0/all/0/1\">Venkateswara Rao Kagita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Reading Comprehension using Case-based Reasoning. (arXiv:2305.14815v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14815","description":"<p>We present an accurate and interpretable method for answer extraction in\nmachine reading comprehension that is reminiscent of case-based reasoning (CBR)\nfrom classical AI. Our method (CBR-MRC) builds upon the hypothesis that\ncontextualized answers to similar questions share semantic similarities with\neach other. Given a test question, CBR-MRC first retrieves a set of similar\ncases from a nonparametric memory and then predicts an answer by selecting the\nspan in the test context that is most similar to the contextualized\nrepresentations of answers in the retrieved cases. The semi-parametric nature\nof our approach allows it to attribute a prediction to the specific set of\nevidence cases, making it a desirable choice for building reliable and\ndebuggable QA systems. We show that CBR-MRC provides high accuracy comparable\nwith large reader models and outperforms baselines by 11.5 and 8.4 EM on\nNaturalQuestions and NewsQA, respectively. Further, we demonstrate the ability\nof CBR-MRC in identifying not just the correct answer tokens but also the span\nwith the most relevant supporting evidence. Lastly, we observe that contexts\nfor certain question types show higher lexical diversity than others and find\nthat CBR-MRC is robust to these variations while performance using\nfully-parametric methods drops.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thai_D/0/1/0/all/0/1\">Dung Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1\">Dhruv Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_M/0/1/0/all/0/1\">Mudit Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenlong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Rajarshi Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Manzil Zaheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jay-Yoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning. (arXiv:2305.18869v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.18869","description":"<p>Chain-of-thought (CoT) is a method that enables language models to handle\ncomplex reasoning tasks by decomposing them into simpler steps. Despite its\nsuccess, the underlying mechanics of CoT are not yet fully understood. In an\nattempt to shed light on this, our study investigates the impact of CoT on the\nability of transformers to in-context learn a simple to study, yet general\nfamily of compositional functions: multi-layer perceptrons (MLPs). In this\nsetting, we find that the success of CoT can be attributed to breaking down\nin-context learning of a compositional function into two distinct phases:\nfocusing on and filtering data related to each step of the composition and\nin-context learning the single-step composition function. Through both\nexperimental and theoretical evidence, we demonstrate how CoT significantly\nreduces the sample complexity of in-context learning (ICL) and facilitates the\nlearning of complex functions that non-CoT methods struggle with. Furthermore,\nwe illustrate how transformers can transition from vanilla in-context learning\nto mastering a compositional function with CoT by simply incorporating\nadditional layers that perform the necessary data-filtering for CoT via the\nattention mechanism. In addition to these test-time benefits, we show CoT helps\naccelerate pretraining by learning shortcuts to represent complex functions and\nfiltering plays an important role in this process. These findings collectively\nprovide insights into the mechanics of CoT, inviting further investigation of\nits role in complex reasoning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingcong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sreenivasan_K/0/1/0/all/0/1\">Kartik Sreenivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannou_A/0/1/0/all/0/1\">Angeliki Giannou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1\">Dimitris Papailiopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1\">Samet Oymak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression. (arXiv:2306.15063v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2306.15063","description":"<p>Pretrained transformers exhibit the remarkable ability of in-context learning\n(ICL): they can learn tasks from just a few examples provided in the prompt\nwithout updating any weights. This raises a foundational question: can ICL\nsolve fundamentally $\\textit{new}$ tasks that are very different from those\nseen during pretraining? To probe this question, we examine ICL's performance\non linear regression while varying the diversity of tasks in the pretraining\ndataset. We empirically demonstrate a $\\textit{task diversity threshold}$ for\nthe emergence of ICL. Below this threshold, the pretrained transformer cannot\nsolve unseen regression tasks, instead behaving like a Bayesian estimator with\nthe $\\textit{non-diverse pretraining task distribution}$ as the prior. Beyond\nthis threshold, the transformer significantly outperforms this estimator; its\nbehavior aligns with that of ridge regression, corresponding to a Gaussian\nprior over $\\textit{all tasks}$, including those not seen during pretraining.\nThus, when pretrained on data with task diversity greater than the threshold,\ntransformers $\\textit{can}$ optimally solve fundamentally new tasks in-context.\nImportantly, this capability hinges on it deviating from the Bayes optimal\nestimator with the pretraining distribution as the prior. This study also\nexplores the effect of regularization, model capacity and task structure and\nunderscores, in a concrete example, the critical role of task diversity,\nalongside data and model scale, in the emergence of ICL. Code is available at\nhttps://github.com/mansheej/icl-task-diversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raventos_A/0/1/0/all/0/1\">Allan Ravent&#xf3;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_M/0/1/0/all/0/1\">Mansheej Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Surya Ganguli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Three Bricks to Consolidate Watermarks for Large Language Models. (arXiv:2308.00113v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.00113","description":"<p>The task of discerning between generated and natural texts is increasingly\nchallenging. In this context, watermarking emerges as a promising technique for\nascribing generated text to a specific model. It alters the sampling generation\nprocess so as to leave an invisible trace in the generated output, facilitating\nlater detection. This research consolidates watermarks for large language\nmodels based on three theoretical and empirical considerations. First, we\nintroduce new statistical tests that offer robust theoretical guarantees which\nremain valid even at low false-positive rates (less than 10$^{\\text{-6}}$).\nSecond, we compare the effectiveness of watermarks using classical benchmarks\nin the field of natural language processing, gaining insights into their\nreal-world applicability. Third, we develop advanced detection schemes for\nscenarios where access to the LLM is available, as well as multi-bit\nwatermarking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_P/0/1/0/all/0/1\">Pierre Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaffin_A/0/1/0/all/0/1\">Antoine Chaffin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tit_K/0/1/0/all/0/1\">Karim Tit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chappelier_V/0/1/0/all/0/1\">Vivien Chappelier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furon_T/0/1/0/all/0/1\">Teddy Furon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predictive Data Analytics with AI: assessing the need for post-editing of MT output by fine-tuning OpenAI LLMs. (arXiv:2308.00158v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.00158","description":"<p>Translation Quality Evaluation (TQE) is an essential step of the modern\ntranslation production process. TQE is critical in assessing both machine\ntranslation (MT) and human translation (HT) quality without reference\ntranslations. The ability to evaluate or even simply estimate the quality of\ntranslation automatically may open significant efficiency gains through process\noptimisation. This work examines whether the state-of-the-art large language\nmodels (LLMs) can be used for this purpose. We take OpenAI models as the best\nstate-of-the-art technology and approach TQE as a binary classification task.\nOn eight language pairs including English to Italian, German, French, Japanese,\nDutch, Portuguese, Turkish, and Chinese, our experimental results show that\nfine-tuned gpt3.5 can demonstrate good performance on translation quality\nprediction tasks, i.e. whether the translation needs to be edited. Another\nfinding is that simply increasing the sizes of LLMs does not lead to apparent\nbetter performances on this task by comparing the performance of three\ndifferent versions of OpenAI models: curie, davinci, and gpt3.5 with 13B, 175B,\nand 175B parameters, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gladkoff_S/0/1/0/all/0/1\">Serge Gladkoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erofeev_G/0/1/0/all/0/1\">Gleb Erofeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorokina_I/0/1/0/all/0/1\">Irina Sorokina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1\">Goran Nenadic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Token-Scaled Logit Distillation for Ternary Weight Generative Language Models. (arXiv:2308.06744v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.06744","description":"<p>Generative Language Models (GLMs) have shown impressive performance in tasks\nsuch as text generation, understanding, and reasoning. However, the large model\nsize poses challenges for practical deployment. To solve this problem,\nQuantization-Aware Training (QAT) has become increasingly popular. However,\ncurrent QAT methods for generative models have resulted in a noticeable loss of\naccuracy. To counteract this issue, we propose a novel knowledge distillation\nmethod specifically designed for GLMs. Our method, called token-scaled logit\ndistillation, prevents overfitting and provides superior learning from the\nteacher model and ground truth. This research marks the first evaluation of\nternary weight quantization-aware training of large-scale GLMs with less than\n1.0 degradation in perplexity and achieves enhanced accuracy in tasks like\ncommon-sense QA and arithmetic reasoning as well as natural language\nunderstanding. Our code is available at https://github.com/aiha-lab/TSLD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sihwa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Janghwan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Sukjin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Du-Seong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_W/0/1/0/all/0/1\">Wonyong Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jungwook Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ToddlerBERTa: Exploiting BabyBERTa for Grammar Learning and Language Understanding. (arXiv:2308.16336v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.16336","description":"<p>We present ToddlerBERTa, a BabyBERTa-like language model, exploring its\ncapabilities through five different models with varied hyperparameters.\nEvaluating on BLiMP, SuperGLUE, MSGS, and a Supplement benchmark from the\nBabyLM challenge, we find that smaller models can excel in specific tasks,\nwhile larger models perform well with substantial data. Despite training on a\nsmaller dataset, ToddlerBERTa demonstrates commendable performance, rivalling\nthe state-of-the-art RoBERTa-base. The model showcases robust language\nunderstanding, even with single-sentence pretraining, and competes with\nbaselines that leverage broader contextual information. Our work provides\ninsights into hyperparameter choices, and data utilization, contributing to the\nadvancement of language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cagatan_O/0/1/0/all/0/1\">Omer Veysel Cagatan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain-of-Thought Reasoning is a Policy Improvement Operator. (arXiv:2309.08589v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2309.08589","description":"<p>Large language models have astounded the world with fascinating new\ncapabilities. However, they currently lack the ability to teach themselves new\nskills, relying instead on large amounts of human-generated training data. We\nintroduce SECToR (Self-Education via Chain-of-Thought Reasoning), a\nproof-of-concept demonstration that language models can teach themselves new\nskills using chain-of-thought reasoning. During the self-learning loop, SECToR\nasks models to solve addition problems using chain-of-thought reasoning before\ntraining the next version of the model to solve those same problems directly\nwithout using such reasoning. This process often results in an improved model\nwhich can, when again augmented with chain-of-thought reasoning, solve even\nharder problems than the original model, allowing the self-learning loop to\ncontinue. Language models trained via SECToR autonomously learn to add up to\nthe longest-length-digit numbers without access to any ground truth examples\nbeyond an initial supervised fine-tuning phase consisting only of numbers with\n6 or fewer digits. Our central hypothesis is that chain-of-thought reasoning\ncan act as a policy improvement operator, similarly to how Monte-Carlo Tree\nSearch is used in AlphaZero (Silver et al., 2017). We hope that this research\ncan lead to new directions in which language models can learn to teach\nthemselves without the need for human demonstrations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hugh Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parkes_D/0/1/0/all/0/1\">David C. Parkes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild. (arXiv:2309.08637v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.08637","description":"<p>Large language models with instruction-following abilities have\nrevolutionized the field of artificial intelligence. These models show\nexceptional generalizability to tackle various real-world tasks through their\nnatural language interfaces. However, their performance heavily relies on\nhigh-quality exemplar data, which is often difficult to obtain. This challenge\nis further exacerbated when it comes to multimodal instruction following. We\nintroduce TextBind, an almost annotation-free framework for empowering larger\nlanguage models with the multi-turn interleaved multimodal\ninstruction-following capabilities. Our approach requires only image-caption\npairs and generates multi-turn multimodal instruction-response conversations\nfrom a language model. To accommodate interleaved image-text inputs and\noutputs, we devise MIM, a language model-centric architecture that seamlessly\nintegrates image encoder and decoder models. We release our dataset, model, and\ndemo to foster future research in the area of multimodal instruction following.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huayang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1\">Taro Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoCA: Fusing position embedding with Collinear Constrained Attention for fine-tuning free context window extending. (arXiv:2309.08646v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2309.08646","description":"<p>Self-attention and position embedding are two key modules in Transformer\nbased LLMs. The potential relationship among them are far from well studied,\nespecially for context window extending. In this paper, we introduce collinear\nconstrained relationship to fuse RoPE and self-attention, and name it as\nCollinear Constrained Attention (CoCA). We've analyzed the computational and\nspatial complexity of CoCA and have determined that it adds only minimal\nadditional overhead compared to the original Transformer-based models. We\nprovide an efficient implementation of CoCA, and make it drop-in replacement\nfor any existing position embedding and attention modules in Transformer based\nmodels. Experiments show that CoCA performs extraordinary well on context\nwindow extending. For instance, a CoCA based GPT model trained with 512 context\nlength can extend the context window up to 8K without perplexity diverging.\nThis indicates more than 16x context window extending without any fine-tuning.\nOur code is released here:\nhttps://github.com/codefuse-ai/Collinear-Constrained-Attention\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shiyi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yifan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianguo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PDFTriage: Question Answering over Long, Structured Documents. (arXiv:2309.08872v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.08872","description":"<p>Large Language Models (LLMs) have issues with document question answering\n(QA) in situations where the document is unable to fit in the small context\nlength of an LLM. To overcome this issue, most existing works focus on\nretrieving the relevant context from the document, representing them as plain\ntext. However, documents such as PDFs, web pages, and presentations are\nnaturally structured with different pages, tables, sections, and so on.\nRepresenting such structured documents as plain text is incongruous with the\nuser's mental model of these documents with rich structure. When a system has\nto query the document for context, this incongruity is brought to the fore, and\nseemingly trivial questions can trip up the QA system. To bridge this\nfundamental gap in handling structured documents, we propose an approach called\nPDFTriage that enables models to retrieve the context based on either structure\nor content. Our experiments demonstrate the effectiveness of the proposed\nPDFTriage-augmented models across several classes of questions where existing\nretrieval-augmented LLMs fail. To facilitate further research on this\nfundamental problem, we release our benchmark dataset consisting of 900+\nhuman-generated questions over 80 structured documents from 10 different\ncategories of question types for document QA. Our code and datasets will be\nreleased soon on Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saad_Falcon_J/0/1/0/all/0/1\">Jon Saad-Falcon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrow_J/0/1/0/all/0/1\">Joe Barrow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siu_A/0/1/0/all/0/1\">Alexa Siu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenkova_A/0/1/0/all/0/1\">Ani Nenkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_D/0/1/0/all/0/1\">David Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1\">Ryan A. Rossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AnglE-optimized Text Embeddings. (arXiv:2309.12871v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.12871","description":"<p>High-quality text embedding is pivotal in improving semantic textual\nsimilarity (STS) tasks, which are crucial components in Large Language Model\n(LLM) applications. However, a common challenge existing text embedding models\nface is the problem of vanishing gradients, primarily due to their reliance on\nthe cosine function in the optimization objective, which has saturation zones.\nTo address this issue, this paper proposes a novel angle-optimized text\nembedding model called AnglE. The core idea of AnglE is to introduce angle\noptimization in a complex space. This novel approach effectively mitigates the\nadverse effects of the saturation zone in the cosine function, which can impede\ngradient and hinder optimization processes. To set up a comprehensive STS\nevaluation, we experimented on existing short-text STS datasets and a newly\ncollected long-text STS dataset from GitHub Issues. Furthermore, we examine\ndomain-specific STS scenarios with limited labeled data and explore how AnglE\nworks with LLM-annotated data. Extensive experiments were conducted on various\ntasks including short-text STS, long-text STS, and domain-specific STS tasks.\nThe results show that AnglE outperforms the state-of-the-art (SOTA) STS models\nthat ignore the cosine saturation zone. These findings demonstrate the ability\nof AnglE to generate high-quality text embeddings and the usefulness of angle\noptimization in STS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PB-LLM: Partially Binarized Large Language Models. (arXiv:2310.00034v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2310.00034","description":"<p>This paper explores network binarization, a radical form of quantization,\ncompressing model weights to a single bit, specifically for Large Language\nModels (LLMs) compression. Due to previous binarization methods collapsing\nLLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can\nachieve extreme low-bit quantization while maintaining the linguistic reasoning\ncapacity of quantized LLMs. Specifically, our exploration first uncovers the\nineffectiveness of naive applications of existing binarization algorithms and\nhighlights the imperative role of salient weights in achieving low-bit\nquantization. Thus, PB-LLM filters a small ratio of salient weights during\nbinarization, allocating them to higher-bit storage, i.e.,\npartially-binarization. PB-LLM is extended to recover the capacities of\nquantized LMMs, by analyzing from the perspective of post-training quantization\n(PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts\nfrom GPTQ, we reconstruct the binarized weight matrix guided by the Hessian\nmatrix and successfully recover the reasoning capacity of PB-LLM in low-bit.\nUnder QAT, we freeze the salient weights during training, explore the\nderivation of optimal scaling factors crucial for minimizing the quantization\nerror, and propose a scaling mechanism based on this derived scaling strategy\nfor residual binarized weights. Those explorations and the developed\nmethodologies significantly contribute to rejuvenating the performance of\nlow-bit quantized LLMs and present substantial advancements in the field of\nnetwork binarization for LLMs.The code is available at\nhttps://github.com/hahnyuan/BinaryLLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1\">Yuzhang Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhihang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AvalonBench: Evaluating LLMs Playing the Game of Avalon. (arXiv:2310.05036v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2310.05036","description":"<p>In this paper, we explore the potential of Large Language Models (LLMs)\nAgents in playing the strategic social deduction game, Resistance Avalon.\nPlayers in Avalon are challenged not only to make informed decisions based on\ndynamically evolving game phases, but also to engage in discussions where they\nmust deceive, deduce, and negotiate with other players. These characteristics\nmake Avalon a compelling test-bed to study the decision-making and\nlanguage-processing capabilities of LLM Agents. To facilitate research in this\nline, we introduce AvalonBench - a comprehensive game environment tailored for\nevaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game\nenvironment for Avalon, (2) rule-based bots as baseline opponents, and (3)\nReAct-style LLM agents with tailored prompts for each role. Notably, our\nevaluations based on AvalonBench highlight a clear capability gap. For\ninstance, models like ChatGPT playing good-role got a win rate of 22.2% against\nrule-based bots playing evil, while good-role bot achieves 38.2% win rate in\nthe same setting. We envision AvalonBench could be a good test-bed for\ndeveloping more advanced LLMs (with self-playing) and agent frameworks that can\neffectively model the layered complexities of such game environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Light_J/0/1/0/all/0/1\">Jonathan Light</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Min Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Ziniu Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Experimenting AI Technologies for Disinformation Combat: the IDMO Project. (arXiv:2310.11097v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.11097","description":"<p>The Italian Digital Media Observatory (IDMO) project, part of a European\ninitiative, focuses on countering disinformation and fake news. This report\noutlines contributions from Rai-CRITS to the project, including: (i) the\ncreation of novel datasets for testing technologies (ii) development of an\nautomatic model for categorizing Pagella Politica verdicts to facilitate\nbroader analysis (iii) creation of an automatic model for recognizing textual\nentailment with exceptional accuracy on the FEVER dataset (iv) assessment using\nGPT-4 to identify textual entailmen (v) a game to raise awareness about fake\nnews at national events.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Canale_L/0/1/0/all/0/1\">Lorenzo Canale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messina_A/0/1/0/all/0/1\">Alberto Messina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quality-Diversity through AI Feedback. (arXiv:2310.13032v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.13032","description":"<p>In many text-generation problems, users may prefer not only a single\nresponse, but a diverse range of high-quality outputs from which to choose.\nQuality-diversity (QD) search algorithms aim at such outcomes, by continually\nimproving and diversifying a population of candidates. However, the\napplicability of QD to qualitative domains, like creative writing, has been\nlimited by the difficulty of algorithmically specifying measures of quality and\ndiversity. Interestingly, recent developments in language models (LMs) have\nenabled guiding search through AI feedback, wherein LMs are prompted in natural\nlanguage to evaluate qualitative aspects of text. Leveraging this development,\nwe introduce Quality-Diversity through AI Feedback (QDAIF), wherein an\nevolutionary algorithm applies LMs to both generate variation and evaluate the\nquality and diversity of candidate text. When assessed on creative writing\ndomains, QDAIF covers more of a specified search space with high-quality\nsamples than do non-QD controls. Further, human evaluation of QDAIF-generated\ncreative texts validates reasonable agreement between AI and human evaluation.\nOur results thus highlight the potential of AI feedback to guide open-ended\nsearch for creative and original solutions, providing a recipe that seemingly\ngeneralizes to many domains and modalities. In this way, QDAIF is a step\ntowards AI systems that can independently search, diversify, evaluate, and\nimprove, which are among the core skills underlying human society's capacity\nfor innovation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bradley_H/0/1/0/all/0/1\">Herbie Bradley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teufel_H/0/1/0/all/0/1\">Hannah Teufel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jenny Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oostermeijer_K/0/1/0/all/0/1\">Koen Oostermeijer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellagente_M/0/1/0/all/0/1\">Marco Bellagente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1\">Jeff Clune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1\">Kenneth Stanley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schott_G/0/1/0/all/0/1\">Gr&#xe9;gory Schott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehman_J/0/1/0/all/0/1\">Joel Lehman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DELPHI: Data for Evaluating LLMs' Performance in Handling Controversial Issues. (arXiv:2310.18130v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.18130","description":"<p>Controversy is a reflection of our zeitgeist, and an important aspect to any\ndiscourse. The rise of large language models (LLMs) as conversational systems\nhas increased public reliance on these systems for answers to their various\nquestions. Consequently, it is crucial to systematically examine how these\nmodels respond to questions that pertaining to ongoing debates. However, few\nsuch datasets exist in providing human-annotated labels reflecting the\ncontemporary discussions. To foster research in this area, we propose a novel\nconstruction of a controversial questions dataset, expanding upon the publicly\nreleased Quora Question Pairs Dataset. This dataset presents challenges\nconcerning knowledge recency, safety, fairness, and bias. We evaluate different\nLLMs using a subset of this dataset, illuminating how they handle controversial\nissues and the stances they adopt. This research ultimately contributes to our\nunderstanding of LLMs' interaction with controversial issues, paving the way\nfor improvements in their comprehension and handling of complex societal\ndebates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">David Q. Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abzaliev_A/0/1/0/all/0/1\">Artem Abzaliev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotek_H/0/1/0/all/0/1\">Hadas Kotek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiu_Z/0/1/0/all/0/1\">Zidi Xiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_C/0/1/0/all/0/1\">Christopher Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1\">Jason D. Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"N-Critics: Self-Refinement of Large Language Models with Ensemble of Critics. (arXiv:2310.18679v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.18679","description":"<p>We propose a self-correction mechanism for Large Language Models (LLMs) to\nmitigate issues such as toxicity and fact hallucination. This method involves\nrefining model outputs through an ensemble of critics and the model's own\nfeedback. Drawing inspiration from human behavior, we explore whether LLMs can\nemulate the self-correction process observed in humans who often engage in\nself-reflection and seek input from others to refine their understanding of\ncomplex topics. Our approach is model-agnostic and can be applied across\nvarious domains to enhance trustworthiness by addressing fairness, bias, and\nrobustness concerns. We consistently observe performance improvements in LLMs\nfor reducing toxicity and correcting factual errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mousavi_S/0/1/0/all/0/1\">Sajad Mousavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_R/0/1/0/all/0/1\">Ricardo Luna Guti&#xe9;rrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rengarajan_D/0/1/0/all/0/1\">Desik Rengarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gundecha_V/0/1/0/all/0/1\">Vineet Gundecha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1\">Ashwin Ramesh Babu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naug_A/0/1/0/all/0/1\">Avisek Naug</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guillen_A/0/1/0/all/0/1\">Antonio Guillen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Soumyendu Sarkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective. (arXiv:2310.19233v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.19233","description":"<p>This paper studies how to effectively build meeting summarization systems for\nreal-world usage using large language models (LLMs). For this purpose, we\nconduct an extensive evaluation and comparison of various closed-source and\nopen-source LLMs, namely, GPT-4, GPT- 3.5, PaLM-2, and LLaMA-2. Our findings\nreveal that most closed-source LLMs are generally better in terms of\nperformance. However, much smaller open-source models like LLaMA- 2 (7B and\n13B) could still achieve performance comparable to the large closed-source\nmodels even in zero-shot scenarios. Considering the privacy concerns of\nclosed-source models for only being accessible via API, alongside the high cost\nassociated with using fine-tuned versions of the closed-source models, the\nopensource models that can achieve competitive performance are more\nadvantageous for industrial use. Balancing performance with associated costs\nand privacy concerns, the LLaMA-2-7B model looks more promising for industrial\nusage. In sum, this paper offers practical insights on using LLMs for\nreal-world business meeting summarization, shedding light on the trade-offs\nbetween performance and cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laskar_M/0/1/0/all/0/1\">Md Tahmid Rahman Laskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xue-Yong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+TN_S/0/1/0/all/0/1\">Shashi Bhushan TN</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Introduction to Natural Language Processing Techniques and Framework for Clinical Implementation in Radiation Oncology. (arXiv:2311.02205v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.02205","description":"<p>Natural Language Processing (NLP) is a key technique for developing Medical\nArtificial Intelligence (AI) systems that leverage Electronic Health Record\n(EHR) data to build diagnostic and prognostic models. NLP enables the\nconversion of unstructured clinical text into structured data that can be fed\ninto AI algorithms. The emergence of the transformer architecture and large\nlanguage models (LLMs) has led to remarkable advances in NLP for various\nhealthcare tasks, such as entity recognition, relation extraction, sentence\nsimilarity, text summarization, and question answering. In this article, we\nreview the major technical innovations that underpin modern NLP models and\npresent state-of-the-art NLP applications that employ LLMs in radiation\noncology research. However, these LLMs are prone to many errors such as\nhallucinations, biases, and ethical violations, which necessitate rigorous\nevaluation and validation before clinical deployment. As such, we propose a\ncomprehensive framework for assessing the NLP models based on their purpose and\nclinical fit, technical performance, bias and trust, legal and ethical\nimplications, and quality assurance, prior to implementation in clinical\nradiation oncology. Our article aims to provide guidance and insights for\nresearchers and clinicians who are interested in developing and using NLP\nmodels in clinical radiation oncology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khanmohammadi_R/0/1/0/all/0/1\">Reza Khanmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1\">Mohammad M. Ghassemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verdecchia_K/0/1/0/all/0/1\">Kyle Verdecchia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_A/0/1/0/all/0/1\">Ahmed I. Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Luo Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chetty_I/0/1/0/all/0/1\">Indrin J. Chetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagher_Ebadian_H/0/1/0/all/0/1\">Hassan Bagher-Ebadian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_F/0/1/0/all/0/1\">Farzan Siddiqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elshaikh_M/0/1/0/all/0/1\">Mohamed Elshaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Movsas_B/0/1/0/all/0/1\">Benjamin Movsas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thind_K/0/1/0/all/0/1\">Kundan Thind</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not all layers are equally as important: Every Layer Counts BERT. (arXiv:2311.02265v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.02265","description":"<p>This paper introduces a novel modification of the transformer architecture,\ntailored for the data-efficient pretraining of language models. This aspect is\nevaluated by participating in the BabyLM challenge, where our solution won both\nthe strict and strict-small tracks. Our approach allows each transformer layer\nto select which outputs of previous layers to process. The empirical results\nverify the potential of this simple modification and show that not all layers\nare equally as important.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Charpentier_L/0/1/0/all/0/1\">Lucas Georges Gabriel Charpentier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samuel_D/0/1/0/all/0/1\">David Samuel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Transformer-Based Reverse Dictionary Model for Quality Estimation of Definitions. (arXiv:2311.02985v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.02985","description":"<p>In the last years, several variants of transformers have emerged. In this\npaper, we compare different transformer-based models for solving the reverse\ndictionary task and explore their use in the context of a serious game called\nThe Dictionary Game.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guite_Vinet_J/0/1/0/all/0/1\">Julien Guit&#xe9;-Vinet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masse_A/0/1/0/all/0/1\">Alexandre Blondin Mass&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadat_F/0/1/0/all/0/1\">Fatiha Sadat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple yet Efficient Ensemble Approach for AI-generated Text Detection. (arXiv:2311.03084v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.03084","description":"<p>Recent Large Language Models (LLMs) have demonstrated remarkable capabilities\nin generating text that closely resembles human writing across wide range of\nstyles and genres. However, such capabilities are prone to potential abuse,\nsuch as fake news generation, spam email creation, and misuse in academic\nassignments. Hence, it is essential to build automated approaches capable of\ndistinguishing between artificially generated text and human-authored text. In\nthis paper, we propose a simple yet efficient solution to this problem by\nensembling predictions from multiple constituent LLMs. Compared to previous\nstate-of-the-art approaches, which are perplexity-based or uses ensembles with\na number of LLMs, our condensed ensembling approach uses only two constituent\nLLMs to achieve comparable performance. Experiments conducted on four benchmark\ndatasets for generative text classification show performance improvements in\nthe range of 0.5 to 100\\% compared to previous state-of-the-art approaches. We\nalso study the influence that the training data from individual LLMs have on\nmodel performance. We found that substituting commercially-restrictive\nGenerative Pre-trained Transformer (GPT) data with data generated from other\nopen language models such as Falcon, Large Language Model Meta AI (LLaMA2), and\nMosaic Pretrained Transformers (MPT) is a feasible alternative when developing\ngenerative text detectors. Furthermore, to demonstrate zero-shot\ngeneralization, we experimented with an English essays dataset, and results\nsuggest that our ensembling approach can handle new data effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abburi_H/0/1/0/all/0/1\">Harika Abburi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kalyani Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suesserman_M/0/1/0/all/0/1\">Michael Suesserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pudota_N/0/1/0/all/0/1\">Nirmala Pudota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veeramani_B/0/1/0/all/0/1\">Balaji Veeramani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowen_E/0/1/0/all/0/1\">Edward Bowen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Sanmitra Bhattacharya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Black-Box Prompt Optimization: Aligning Large Language Models without Model Training. (arXiv:2311.04155v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.04155","description":"<p>Large language models (LLMs) have shown impressive success in various\napplications. However, these models are often not well aligned with human\nintents, which calls for additional treatments on them, that is, the alignment\nproblem. To make LLMs better follow user instructions, existing alignment\nmethods mostly focus on further training them. However, the extra training of\nLLMs are usually expensive in terms of GPU compute; worse still, LLMs of\ninterest are oftentimes not accessible for user-demanded training, such as\nGPTs. In this work, we take a different perspective -- Black-Box Prompt\nOptimization (BPO) -- to perform alignments. The idea is to optimize user\nprompts to suit LLMs' input understanding, so as to best realize users' intents\nwithout updating LLMs' parameters. BPO is model-agnostic and the empirical\nresults demonstrate that the BPO-aligned ChatGPT yields a 22% increase in the\nwin rate against its original version, and 10% for GPT-4. Importantly, the\nBPO-aligned LLMs can outperform the same models aligned by PPO and DPO, and it\nalso brings additional performance gains when combining BPO with PPO or DPO.\nCode and datasets are released at https://github.com/thu-coai/BPO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jiale Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kehan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1\">Pei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-11-08T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}