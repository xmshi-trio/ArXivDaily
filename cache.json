{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-10-28T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Disentangled Text Representation Learning with Information-Theoretic Perspective for Adversarial Robustness. (arXiv:2210.14957v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14957","description":"<p>Adversarial vulnerability remains a major obstacle to constructing reliable\nNLP systems. When imperceptible perturbations are added to raw input text, the\nperformance of a deep learning model may drop dramatically under attacks.\nRecent work argues the adversarial vulnerability of the model is caused by the\nnon-robust features in supervised training. Thus in this paper, we tackle the\nadversarial robustness challenge from the view of disentangled representation\nlearning, which is able to explicitly disentangle robust and non-robust\nfeatures in text. Specifically, inspired by the variation of information (VI)\nin information theory, we derive a disentangled learning objective composed of\nmutual information to represent both the semantic representativeness of latent\nembeddings and differentiation of robust and non-robust features. On the basis\nof this, we design a disentangled learning network to estimate these mutual\ninformation. Experiments on text classification and entailment tasks show that\nour method significantly outperforms the representative methods under\nadversarial attacks, indicating that discarding non-robust features is critical\nfor improving adversarial robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiahao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Wenji Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What's Different between Visual Question Answering for Machine \"Understanding\" Versus for Accessibility?. (arXiv:2210.14966v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14966","description":"<p>In visual question answering (VQA), a machine must answer a question given an\nassociated image. Recently, accessibility researchers have explored whether VQA\ncan be deployed in a real-world setting where users with visual impairments\nlearn about their environment by capturing their visual surroundings and asking\nquestions. However, most of the existing benchmarking datasets for VQA focus on\nmachine \"understanding\" and it remains unclear how progress on those datasets\ncorresponds to improvements in this real-world use case. We aim to answer this\nquestion by evaluating discrepancies between machine \"understanding\" datasets\n(VQA-v2) and accessibility datasets (VizWiz) by evaluating a variety of VQA\nmodels. Based on our findings, we discuss opportunities and challenges in VQA\nfor accessibility and suggest directions for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Trista Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seelman_K/0/1/0/all/0/1\">Kyle Seelman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyungjun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daume_H/0/1/0/all/0/1\">Hal Daum&#xe9; III</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MABEL: Attenuating Gender Bias using Textual Entailment Data. (arXiv:2210.14975v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14975","description":"<p>Pre-trained language models encode undesirable social biases, which are\nfurther exacerbated in downstream use. To this end, we propose MABEL (a Method\nfor Attenuating Gender Bias using Entailment Labels), an intermediate\npre-training approach for mitigating gender bias in contextualized\nrepresentations. Key to our approach is the use of a contrastive learning\nobjective on counterfactually augmented, gender-balanced entailment pairs from\nnatural language inference (NLI) datasets. We also introduce an alignment\nregularizer that pulls identical entailment pairs along opposite gender\ndirections closer. We extensively evaluate our approach on intrinsic and\nextrinsic metrics, and show that MABEL outperforms previous task-agnostic\ndebiasing approaches in terms of fairness. It also preserves task performance\nafter fine-tuning on downstream tasks. Together, these findings demonstrate the\nsuitability of NLI data as an effective means of bias mitigation, as opposed to\nonly using unlabeled sentences in the literature. Finally, we identify that\nexisting approaches often use evaluation settings that are insufficient or\ninconsistent. We make an effort to reproduce and compare previous methods, and\ncall for unifying the evaluation settings across gender debiasing methods for\nbetter future comparison.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jacqueline He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1\">Mengzhou Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fellbaum_C/0/1/0/all/0/1\">Christiane Fellbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Domain Adaptation for Pre-trained Multilingual Neural Machine Translation Models. (arXiv:2210.14979v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14979","description":"<p>Recent literature has demonstrated the potential of multilingual Neural\nMachine Translation (mNMT) models. However, the most efficient models are not\nwell suited to specialized industries. In these cases, internal data is scarce\nand expensive to find in all language pairs. Therefore, fine-tuning a mNMT\nmodel on a specialized domain is hard. In this context, we decided to focus on\na new task: Domain Adaptation of a pre-trained mNMT model on a single pair of\nlanguage while trying to maintain model quality on generic domain data for all\nlanguage pairs. The risk of loss on generic domain and on other pairs is high.\nThis task is key for mNMT model adoption in the industry and is at the border\nof many others. We propose a fine-tuning procedure for the generic mNMT that\ncombines embeddings freezing and adversarial loss. Our experiments demonstrated\nthat the procedure improves performances on specialized data with a minimal\nloss in initial performances on generic domain for all languages pairs,\ncompared to a naive standard approach (+10.0 BLEU score on specialized data,\n-0.01 to -0.5 BLEU on WMT and Tatoeba datasets on the other pairs with M2M100).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grosso_M/0/1/0/all/0/1\">Mathieu Grosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratnamogan_P/0/1/0/all/0/1\">Pirashanth Ratnamogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathey_A/0/1/0/all/0/1\">Alexis Mathey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanhuffel_W/0/1/0/all/0/1\">William Vanhuffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fotso_M/0/1/0/all/0/1\">Michael Fotso Fotso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large language models are not zero-shot communicators. (arXiv:2210.14986v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14986","description":"<p>Despite widespread use of LLMs as conversational agents, evaluations of\nperformance fail to capture a crucial aspect of communication: interpreting\nlanguage in context. Humans interpret language using beliefs and prior\nknowledge about the world. For example, we intuitively understand the response\n\"I wore gloves\" to the question \"Did you leave fingerprints?\" as meaning \"No\".\nTo investigate whether LLMs have the ability to make this type of inference,\nknown as an implicature, we design a simple task and evaluate widely used\nstate-of-the-art models. We find that, despite only evaluating on utterances\nthat require a binary inference (yes or no), most perform close to random.\nModels adapted to be \"aligned with human intent\" perform much better, but still\nshow a significant gap with human performance. We present our findings as the\nstarting point for further research into evaluating how LLMs interpret language\nin context and to drive the development of more pragmatic and useful models of\nhuman discourse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruis_L/0/1/0/all/0/1\">Laura Ruis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Akbir Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1\">Tim Rockt&#xe4;schel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grefenstette_E/0/1/0/all/0/1\">Edward Grefenstette</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TPU-MLIR: A Compiler For TPU Using MLIR. (arXiv:2210.15016v1 [cs.PL])","link":"http://arxiv.org/abs/2210.15016","description":"<p>Multi-level intermediate representations (MLIR) show great promise for\nreducing the cost of building domain-specific compilers by providing a reusable\nand extensible compiler infrastructure. This work presents TPU-MLIR, an\nend-to-end compiler based on MLIR that deploys pre-trained neural network (NN)\nmodels to a custom ASIC called a Tensor Processing Unit (TPU). TPU-MLIR defines\ntwo new dialects to implement its functionality: 1. a Tensor operation (TOP)\ndialect that encodes the deep learning graph semantics and independent of the\ndeep learning framework and 2. a TPU kernel dialect to provide a standard\nkernel computation on TPU. A NN model is translated to the TOP dialect and then\nlowered to the TPU dialect for different TPUs according to the chip's\nconfiguration. We demonstrate how to use the MLIR pass pipeline to organize and\nperform optimization on TPU to generate machine code. The paper also presents a\nverification procedure to ensure the correctness of each transform stage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Pengchao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Man Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1\">Guoyue Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalization Differences between End-to-End and Neuro-Symbolic Vision-Language Reasoning Systems. (arXiv:2210.15037v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15037","description":"<p>For vision-and-language reasoning tasks, both fully connectionist, end-to-end\nmethods and hybrid, neuro-symbolic methods have achieved high in-distribution\nperformance. In which out-of-distribution settings does each paradigm excel? We\ninvestigate this question on both single-image and multi-image visual\nquestion-answering through four types of generalization tests: a novel\nsegment-combine test for multi-image queries, contrast set, compositional\ngeneralization, and cross-benchmark transfer. Vision-and-language end-to-end\ntrained systems exhibit sizeable performance drops across all these tests.\nNeuro-symbolic methods suffer even more on cross-benchmark transfer from GQA to\nVQA, but they show smaller accuracy drops on the other generalization tests and\ntheir performance quickly improves by few-shot training. Overall, our results\ndemonstrate the complementary benefits of these two paradigms, and emphasize\nthe importance of using a diverse suite of generalization tests to fully\ncharacterize model robustness to distribution shift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EW-Tune: A Framework for Privately Fine-Tuning Large Language Models with Differential Privacy. (arXiv:2210.15042v1 [cs.CR])","link":"http://arxiv.org/abs/2210.15042","description":"<p>Pre-trained Large Language Models (LLMs) are an integral part of modern AI\nthat have led to breakthrough performances in complex AI tasks. Major AI\ncompanies with expensive infrastructures are able to develop and train these\nlarge models with billions and millions of parameters from scratch. Third\nparties, researchers, and practitioners are increasingly adopting these\npre-trained models and fine-tuning them on their private data to accomplish\ntheir downstream AI tasks. However, it has been shown that an adversary can\nextract/reconstruct the exact training samples from these LLMs, which can lead\nto revealing personally identifiable information. The issue has raised deep\nconcerns about the privacy of LLMs. Differential privacy (DP) provides a\nrigorous framework that allows adding noise in the process of training or\nfine-tuning LLMs such that extracting the training data becomes infeasible\n(i.e., with a cryptographically small success probability). While the\ntheoretical privacy guarantees offered in most extant studies assume learning\nmodels from scratch through many training iterations in an asymptotic setting,\nthis assumption does not hold in fine-tuning scenarios in which the number of\ntraining iterations is significantly smaller. To address the gap, we present\n\\ewtune, a DP framework for fine-tuning LLMs based on Edgeworth accountant with\nfinite-sample privacy guarantees. Our results across four well-established\nnatural language understanding (NLU) tasks show that while \\ewtune~adds privacy\nguarantees to LLM fine-tuning process, it directly contributes to decreasing\nthe induced noise to up to 5.6\\% and improves the state-of-the-art LLMs\nperformance by up to 1.1\\% across all NLU tasks. We have open-sourced our\nimplementations for wide adoption and public testing purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Behnia_R/0/1/0/all/0/1\">Rouzbeh Behnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_M/0/1/0/all/0/1\">Mohamamdreza Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacheco_J/0/1/0/all/0/1\">Jason Pacheco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmanabhan_b/0/1/0/all/0/1\">balaji Padmanabhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DyREx: Dynamic Query Representation for Extractive Question Answering. (arXiv:2210.15048v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15048","description":"<p>Extractive question answering (ExQA) is an essential task for Natural\nLanguage Processing. The dominant approach to ExQA is one that represents the\ninput sequence tokens (question and passage) with a pre-trained transformer,\nthen uses two learned query vectors to compute distributions over the start and\nend answer span positions. These query vectors lack the context of the inputs,\nwhich can be a bottleneck for the model performance. To address this problem,\nwe propose \\textit{DyREx}, a generalization of the \\textit{vanilla} approach\nwhere we dynamically compute query vectors given the input, using an attention\nmechanism through transformer layers. Empirical observations demonstrate that\nour approach consistently improves the performance over the standard one. The\ncode and accompanying files for running the experiments are available at\n\\url{https://github.com/urchade/DyReX}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaratiana_U/0/1/0/all/0/1\">Urchade Zaratiana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khbir_N/0/1/0/all/0/1\">Niama El Khbir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nunez_D/0/1/0/all/0/1\">Dennis N&#xfa;&#xf1;ez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holat_P/0/1/0/all/0/1\">Pierre Holat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomeh_N/0/1/0/all/0/1\">Nadi Tomeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charnois_T/0/1/0/all/0/1\">Thierry Charnois</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Four-in-One: A Joint Approach to Inverse Text Normalization, Punctuation, Capitalization, and Disfluency for Automatic Speech Recognition. (arXiv:2210.15063v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15063","description":"<p>Features such as punctuation, capitalization, and formatting of entities are\nimportant for readability, understanding, and natural language processing\ntasks. However, Automatic Speech Recognition (ASR) systems produce spoken-form\ntext devoid of formatting, and tagging approaches to formatting address just\none or two features at a time. In this paper, we unify spoken-to-written text\nconversion via a two-stage process: First, we use a single transformer tagging\nmodel to jointly produce token-level tags for inverse text normalization (ITN),\npunctuation, capitalization, and disfluencies. Then, we apply the tags to\ngenerate written-form text and use weighted finite state transducer (WFST)\ngrammars to format tagged ITN entity spans. Despite joining four models into\none, our unified tagging approach matches or outperforms task-specific models\nacross all four tasks on benchmark test sets across several domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Sharman Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behre_P/0/1/0/all/0/1\">Piyush Behre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kibre_N/0/1/0/all/0/1\">Nick Kibre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alphonso_I/0/1/0/all/0/1\">Issac Alphonso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuangyu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"arXivEdits: Understanding the Human Revision Process in Scientific Writing. (arXiv:2210.15067v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15067","description":"<p>Scientific publications are the primary means to communicate research\ndiscoveries, where the writing quality is of crucial importance. However, prior\nwork studying the human editing process in this domain mainly focused on the\nabstract or introduction sections, resulting in an incomplete picture. In this\nwork, we provide a complete computational framework for studying text revision\nin scientific writing. We first introduce arXivEdits, a new annotated corpus of\n751 full papers from arXiv with gold sentence alignment across their multiple\nversions of revision, as well as fine-grained span-level edits and their\nunderlying intentions for 1,000 sentence pairs. It supports our data-driven\nanalysis to unveil the common strategies practiced by researchers for revising\ntheir papers. To scale up the analysis, we also develop automatic methods to\nextract revision at document-, sentence-, and word-levels. A neural CRF\nsentence alignment model trained on our corpus achieves 93.8 F1, enabling the\nreliable matching of sentences between different versions. We formulate the\nedit extraction task as a span alignment problem, and our proposed method\nextracts more fine-grained and explainable edits, compared to the commonly used\ndiff algorithm. An intention classifier trained on our dataset achieves 78.9 F1\non the fine-grained intent classification task. Our data and system are\nreleased at tiny.one/arxivedits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevens_S/0/1/0/all/0/1\">Samuel Stevens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalized Dialogue Generation with Persona-Adaptive Attention. (arXiv:2210.15088v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15088","description":"<p>Persona-based dialogue systems aim to generate consistent responses based on\nhistorical context and predefined persona. Unlike conventional dialogue\ngeneration, the persona-based dialogue needs to consider both dialogue context\nand persona, posing a challenge for coherent training. Specifically, this\nrequires a delicate weight balance between context and persona. To achieve\nthat, in this paper, we propose an effective framework with Persona-Adaptive\nAttention (PAA), which adaptively integrates the weights from the persona and\ncontext information via our designed attention. In addition, a dynamic masking\nmechanism is applied to the PAA to not only drop redundant information in\ncontext and persona but also serve as a regularization mechanism to avoid\noverfitting. Experimental results demonstrate the superiority of the proposed\nPAA framework compared to the strong baselines in both automatic and human\nevaluation. Moreover, the proposed PAA approach can perform equivalently well\nin a low-resource regime compared to models trained in a full-data setting,\nwhich achieve a similar result with only 20% to 30% of data compared to the\nlarger models trained in the full-data setting. To fully exploit the\neffectiveness of our design, we designed several variants for handling the\nweighted information in different ways, showing the necessity and sufficiency\nof our weighting and masking designs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qiushi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_T/0/1/0/all/0/1\">Tom Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xubo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenwu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Lilian Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Decoding: Open-ended Text Generation as Optimization. (arXiv:2210.15097v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15097","description":"<p>Likelihood, although useful as a training loss, is a poor search objective\nfor guiding open-ended generation from language models (LMs). Existing\ngeneration algorithms must avoid both unlikely strings, which are incoherent,\nand highly likely ones, which are short and repetitive. We propose contrastive\ndecoding (CD), a more reliable search objective that returns the difference\nbetween likelihood under a large LM (called the expert, e.g. OPT-13b) and a\nsmall LM (called the amateur, e.g. OPT-125m). CD is inspired by the fact that\nthe failures of larger LMs are even more prevalent in smaller LMs, and that\nthis difference signals exactly which texts should be preferred. CD requires\nzero training, and produces higher quality text than decoding from the larger\nLM alone. It also generalizes across model types (OPT and GPT2) and\nsignificantly outperforms four strong decoding algorithms in automatic and\nhuman evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Lisa Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1\">Jason Eisner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Syntax Complies with the Free-Energy Principle. (arXiv:2210.15098v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15098","description":"<p>Natural language syntax yields an unbounded array of hierarchically\nstructured expressions. We claim that these are used in the service of active\ninference in accord with the free-energy principle (FEP). While conceptual\nadvances alongside modelling and simulation work have attempted to connect\nspeech segmentation and linguistic communication with the FEP, we extend this\nprogram to the underlying computations responsible for generating syntactic\nobjects. We argue that recently proposed principles of economy in language\ndesign - such as \"minimal search\" criteria from theoretical syntax - adhere to\nthe FEP. This affords a greater degree of explanatory power to the FEP - with\nrespect to higher language functions - and offers linguistics a grounding in\nfirst principles with respect to computability. We show how both tree-geometric\ndepth and a Kolmogorov complexity estimate (recruiting a Lempel-Ziv compression\nalgorithm) can be used to accurately predict legal operations on syntactic\nworkspaces, directly in line with formulations of variational free energy\nminimization. This is used to motivate a general principle of language design\nthat we term Turing-Chomsky Compression (TCC). We use TCC to align concerns of\nlinguists with the normative account of self-organization furnished by the FEP,\nby marshalling evidence from theoretical linguistics and psycholinguistics to\nground core principles of efficient syntactic computation within active\ninference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murphy_E/0/1/0/all/0/1\">Elliot Murphy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holmes_E/0/1/0/all/0/1\">Emma Holmes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friston_K/0/1/0/all/0/1\">Karl Friston</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TRScore: A Novel GPT-based Readability Scorer for ASR Segmentation and Punctuation model evaluation and selection. (arXiv:2210.15104v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15104","description":"<p>Punctuation and Segmentation are key to readability in Automatic Speech\nRecognition (ASR), often evaluated using F1 scores that require high-quality\nhuman transcripts and do not reflect readability well. Human evaluation is\nexpensive, time-consuming, and suffers from large inter-observer variability,\nespecially in conversational speech devoid of strict grammatical structures.\nLarge pre-trained models capture a notion of grammatical structure. We present\nTRScore, a novel readability measure using the GPT model to evaluate different\nsegmentation and punctuation systems. We validate our approach with human\nexperts. Additionally, our approach enables quantitative assessment of text\npost-processing techniques such as capitalization, inverse text normalization\n(ITN), and disfluency on overall readability, which traditional word error rate\n(WER) and slot error rate (SER) metrics fail to capture. TRScore is strongly\ncorrelated to traditional F1 and human readability scores, with Pearson's\ncorrelation coefficients of 0.67 and 0.98, respectively. It also eliminates the\nneed for human transcriptions for model selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Behre_P/0/1/0/all/0/1\">Piyush Behre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Sharman Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Amy Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kesavamoorthy_H/0/1/0/all/0/1\">Harini Kesavamoorthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuangyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_F/0/1/0/all/0/1\">Fei Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basoglu_C/0/1/0/all/0/1\">Chris Basoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_S/0/1/0/all/0/1\">Sayan Pathak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Vision-Language Transformer in Fashion. (arXiv:2210.15110v1 [cs.CV])","link":"http://arxiv.org/abs/2210.15110","description":"<p>We present a masked vision-language transformer (MVLT) for fashion-specific\nmulti-modal representation. Technically, we simply utilize vision transformer\narchitecture for replacing the BERT in the pre-training model, making MVLT the\nfirst end-to-end framework for the fashion domain. Besides, we designed masked\nimage reconstruction (MIR) for a fine-grained understanding of fashion. MVLT is\nan extensible and convenient architecture that admits raw multi-modal inputs\nwithout extra pre-processing models (e.g., ResNet), implicitly modeling the\nvision-language alignments. More importantly, MVLT can easily generalize to\nvarious matching and generative tasks. Experimental results show obvious\nimprovements in retrieval (rank@5: 17%) and recognition (accuracy: 3%) tasks\nover the Fashion-Gen 2018 winner Kaleido-BERT. Code is made available at\nhttps://github.com/GewelsJI/MVLT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1\">Ge-Peng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuge_M/0/1/0/all/0/1\">Mingcheng Zhuge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Dehong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaridis_C/0/1/0/all/0/1\">Christos Sakaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards High-Quality Neural TTS for Low-Resource Languages by Learning Compact Speech Representations. (arXiv:2210.15131v1 [cs.SD])","link":"http://arxiv.org/abs/2210.15131","description":"<p>This paper aims to enhance low-resource TTS by reducing training data\nrequirements using compact speech representations. A Multi-Stage Multi-Codebook\n(MSMC) VQ-GAN is trained to learn the representation, MSMCR, and decode it to\nwaveforms. Subsequently, we train the multi-stage predictor to predict MSMCRs\nfrom the text for TTS synthesis. Moreover, we optimize the training strategy by\nleveraging more audio to learn MSMCRs better for low-resource languages. It\nselects audio from other languages using speaker similarity metric to augment\nthe training set, and applies transfer learning to improve training quality. In\nMOS tests, the proposed system significantly outperforms FastSpeech and VITS in\nstandard and low-resource scenarios, showing lower data requirements. The\nproposed training strategy effectively enhances MSMCRs on waveform\nreconstruction. It improves TTS performance further, which wins 77% votes in\nthe preference test for the low-resource TTS with only 15 minutes of paired\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Haohan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1\">Fenglong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xixin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hui Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval. (arXiv:2210.15133v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15133","description":"<p>Pre-trained language model (PTM) has been shown to yield powerful text\nrepresentations for dense passage retrieval task. The Masked Language Modeling\n(MLM) is a major sub-task of the pre-training process. However, we found that\nthe conventional random masking strategy tend to select a large number of\ntokens that have limited effect on the passage retrieval task (e,g. stop-words\nand punctuation). By noticing the term importance weight can provide valuable\ninformation for passage retrieval, we hereby propose alternative retrieval\noriented masking (dubbed as ROM) strategy where more important tokens will have\na higher probability of being masked out, to capture this straightforward yet\nessential information to facilitate the language model pre-training process.\nNotably, the proposed new token masking method will not change the architecture\nand learning objective of original PTM. Our experiments verify that the\nproposed ROM enables term importance information to help language model\npre-training thus achieving better performance on multiple passage retrieval\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_D/0/1/0/all/0/1\">Dingkun Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanzhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Autoregressive Speech Recognition Models with Limited in-domain Supervision. (arXiv:2210.15135v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15135","description":"<p>Advances in self-supervised learning have significantly reduced the amount of\ntranscribed audio required for training. However, the majority of work in this\narea is focused on read speech. We explore limited supervision in the domain of\nconversational speech. While we assume the amount of in-domain data is limited,\nwe augment the model with open source read speech data. The XLS-R model has\nbeen shown to perform well with limited adaptation data and serves as a strong\nbaseline. We use untranscribed data for self-supervised learning and\nsemi-supervised training in an autoregressive encoder-decoder model. We\ndemonstrate that by using the XLS-R model for pseudotranscription, a much\nsmaller autoregressive model can outperform a finetuned XLS-R model when\ntranscribed in-domain data is limited, reducing WER by as much as 8% absolute.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chak-Fai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keith_F/0/1/0/all/0/1\">Francis Keith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_W/0/1/0/all/0/1\">William Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snover_M/0/1/0/all/0/1\">Matthew Snover</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gendered Mental Health Stigma in Masked Language Models. (arXiv:2210.15144v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15144","description":"<p>Mental health stigma prevents many individuals from receiving the appropriate\ncare, and social psychology studies have shown that mental health tends to be\noverlooked in men. In this work, we investigate gendered mental health stigma\nin masked language models. In doing so, we operationalize mental health stigma\nby developing a framework grounded in psychology research: we use clinical\npsychology literature to curate prompts, then evaluate the models' propensity\nto generate gendered words. We find that masked language models capture\nsocietal stigma about gender in mental health: models are consistently more\nlikely to predict female subjects than male in sentences about having a mental\nhealth condition (32% vs. 19%), and this disparity is exacerbated for sentences\nthat indicate treatment-seeking behavior. Furthermore, we find that different\nmodels capture dimensions of stigma differently for men and women, associating\nstereotypes like anger, blame, and pity more with women with mental health\nconditions than with men. In showing the complex nuances of models' gendered\nmental health stigma, we demonstrate that context and overlapping dimensions of\nidentity are important considerations when assessing computational models'\nsocial biases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_I/0/1/0/all/0/1\">Inna Wanyin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Njoo_L/0/1/0/all/0/1\">Lucille Njoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Field_A/0/1/0/all/0/1\">Anjalie Field</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Ashish Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reinecke_K/0/1/0/all/0/1\">Katharina Reinecke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Althoff_T/0/1/0/all/0/1\">Tim Althoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Curriculum Learning Approach for Multi-domain Text Classification Using Keyword weight Ranking. (arXiv:2210.15147v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15147","description":"<p>Text classification is a very classic NLP task, but it has two prominent\nshortcomings: On the one hand, text classification is deeply domain-dependent.\nThat is, a classifier trained on the corpus of one domain may not perform so\nwell in another domain. On the other hand, text classification models require a\nlot of annotated data for training. However, for some domains, there may not\nexist enough annotated data. Therefore, it is valuable to investigate how to\nefficiently utilize text data from different domains to improve the performance\nof models in various domains. Some multi-domain text classification models are\ntrained by adversarial training to extract shared features among all domains\nand the specific features of each domain. We noted that the distinctness of the\ndomain-specific features is different, so in this paper, we propose to use a\ncurriculum learning strategy based on keyword weight ranking to improve the\nperformance of multi-domain text classification models. The experimental\nresults on the Amazon review and FDU-MTL datasets show that our curriculum\nlearning strategy effectively improves the performance of multi-domain text\nclassification models based on adversarial learning and outperforms\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zilin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Rui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dictionary-Assisted Supervised Contrastive Learning. (arXiv:2210.15172v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15172","description":"<p>Text analysis in the social sciences often involves using specialized\ndictionaries to reason with abstract concepts, such as perceptions about the\neconomy or abuse on social media. These dictionaries allow researchers to\nimpart domain knowledge and note subtle usages of words relating to a\nconcept(s) of interest. We introduce the dictionary-assisted supervised\ncontrastive learning (DASCL) objective, allowing researchers to leverage\nspecialized dictionaries when fine-tuning pretrained language models. The text\nis first keyword simplified: a common, fixed token replaces any word in the\ncorpus that appears in the dictionary(ies) relevant to the concept of interest.\nDuring fine-tuning, a supervised contrastive objective draws closer the\nembeddings of the original and keyword-simplified texts of the same class while\npushing further apart the embeddings of different classes. The\nkeyword-simplified texts of the same class are more textually similar than\ntheir original text counterparts, which additionally draws the embeddings of\nthe same class closer together. Combining DASCL and cross-entropy improves\nclassification performance metrics in few-shot learning settings and social\nscience applications compared to using cross-entropy alone and alternative\ncontrastive and data augmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Patrick Y. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonneau_R/0/1/0/all/0/1\">Richard Bonneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tucker_J/0/1/0/all/0/1\">Joshua A. Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagler_J/0/1/0/all/0/1\">Jonathan Nagler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Articulation GAN: Unsupervised modeling of articulatory learning. (arXiv:2210.15173v1 [cs.SD])","link":"http://arxiv.org/abs/2210.15173","description":"<p>Generative deep neural networks are widely used for speech synthesis, but\nmost existing models directly generate waveforms or spectral outputs. Humans,\nhowever, produce speech by controlling articulators, which results in the\nproduction of speech sounds through physical properties of sound propagation.\nWe propose a new unsupervised generative model of speech production/synthesis\nthat includes articulatory representations and thus more closely mimics human\nspeech production. We introduce the Articulatory Generator to the Generative\nAdversarial Network paradigm. The Articulatory Generator needs to learn to\ngenerate articulatory representations (electromagnetic articulography or EMA)\nin a fully unsupervised manner without ever accessing EMA data. A separate\npre-trained physical model (ema2wav) then transforms the generated EMA\nrepresentations to speech waveforms, which get sent to the Discriminator for\nevaluation. Articulatory analysis of the generated EMA representations suggests\nthat the network learns to control articulators in a manner that closely\nfollows human articulators during speech production. Acoustic analysis of the\noutputs suggest that the network learns to generate words that are part of\ntraining data as well as novel innovative words that are absent from training\ndata. Our proposed architecture thus allows modeling of articulatory learning\nwith deep neural networks from raw audio inputs in a fully unsupervised manner.\nWe additionally discuss implications of articulatory representations for\ncognitive models of human language and speech technology in general.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Alan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peter Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anumanchipalli_G/0/1/0/all/0/1\">Gopala K Anumanchipalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled and Robust Representation Learning for Bragging Classification in Social Media. (arXiv:2210.15180v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15180","description":"<p>Researching bragging behavior on social media arouses interest of\ncomputational (socio) linguists. However, existing bragging classification\ndatasets suffer from a serious data imbalance issue. Because labeling a\ndata-balance dataset is expensive, most methods introduce external knowledge to\nimprove model learning. Nevertheless, such methods inevitably introduce noise\nand non-relevance information from external knowledge. To overcome the\ndrawback, we propose a novel bragging classification method with\ndisentangle-based representation augmentation and domain-aware adversarial\nstrategy. Specifically, model learns to disentangle and reconstruct\nrepresentation and generate augmented features via disentangle-based\nrepresentation augmentation. Moreover, domain-aware adversarial strategy aims\nto constrain domain of augmented features to improve their robustness.\nExperimental results demonstrate that our method achieves state-of-the-art\nperformance compared to other methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yucheng Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Outlier-Aware Training for Improving Group Accuracy Disparities. (arXiv:2210.15183v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15183","description":"<p>Methods addressing spurious correlations such as Just Train Twice (JTT,\n<a href=\"/abs/2107.09044\">arXiv:2107.09044v2</a>) involve reweighting a subset of the training set to\nmaximize the worst-group accuracy. However, the reweighted set of examples may\npotentially contain unlearnable examples that hamper the model's learning. We\npropose mitigating this by detecting outliers to the training set and removing\nthem before reweighting. Our experiments show that our method achieves\ncompetitive or better accuracy compared with JTT and can detect and remove\nannotation errors in the subset being reweighted in JTT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li-Kuang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruengkrai_C/0/1/0/all/0/1\">Canasai Kruengkrai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1\">Junichi Yamagishi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Too Brittle To Touch: Comparing the Stability of Quantization and Distillation Towards Developing Lightweight Low-Resource MT Models. (arXiv:2210.15184v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15184","description":"<p>Leveraging shared learning through Massively Multilingual Models,\nstate-of-the-art machine translation models are often able to adapt to the\npaucity of data for low-resource languages. However, this performance comes at\nthe cost of significantly bloated models which are not practically deployable.\nKnowledge Distillation is one popular technique to develop competitive,\nlightweight models: In this work, we first evaluate its use to compress MT\nmodels focusing on languages with extremely limited training data. Through our\nanalysis across 8 languages, we find that the variance in the performance of\nthe distilled models due to their dependence on priors including the amount of\nsynthetic data used for distillation, the student architecture, training\nhyperparameters and confidence of the teacher models, makes distillation a\nbrittle compression mechanism. To mitigate this, we explore the use of\npost-training quantization for the compression of these models. Here, we find\nthat while distillation provides gains across some low-resource languages,\nquantization provides more consistent performance trends for the entire range\nof languages, especially the lowest-resource languages in our target set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diddee_H/0/1/0/all/0/1\">Harshita Diddee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandapat_S/0/1/0/all/0/1\">Sandipan Dandapat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganu_T/0/1/0/all/0/1\">Tanuja Ganu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bali_K/0/1/0/all/0/1\">Kalika Bali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Truncation Sampling as Language Model Desmoothing. (arXiv:2210.15191v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15191","description":"<p>Long samples of text from neural language models can be of poor quality.\nTruncation sampling algorithms--like top-$p$ or top-$k$ -- address this by\nsetting some words' probabilities to zero at each step. This work provides\nframing for the aim of truncation, and an improved algorithm for that aim. We\npropose thinking of a neural language model as a mixture of a true distribution\nand a smoothing distribution that avoids infinite perplexity. In this light,\ntruncation algorithms aim to perform desmoothing, estimating a subset of the\nsupport of the true distribution. Finding a good subset is crucial: we show\nthat top-$p$ unnecessarily truncates high-probability words, for example\ncausing it to truncate all words but Trump for a document that starts with\nDonald. We introduce $\\eta$-sampling, which truncates words below an\nentropy-dependent probability threshold. Compared to previous algorithms,\n$\\eta$-sampling generates more plausible long English documents according to\nhumans, is better at breaking out of repetition, and behaves more reasonably on\na battery of test distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hewitt_J/0/1/0/all/0/1\">John Hewitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COCO-DR: Combating Distribution Shifts in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning. (arXiv:2210.15212v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15212","description":"<p>We present a new zero-shot dense retrieval (ZeroDR) method, COCO-DR, to\nimprove the generalization ability of dense retrieval by combating the\ndistribution shifts between source training tasks and target scenarios. To\nmitigate the impact of document differences, COCO-DR continues pretraining the\nlanguage model on the target corpora to adapt the model to target distributions\nvia COtinuous COtrastive learning. To prepare for unseen target queries,\nCOCO-DR leverages implicit Distributionally Robust Optimization (iDRO) to\nreweight samples from different source query clusters for improving model\nrobustness over rare queries during fine-tuning. COCO-DR achieves superior\naverage performance on BEIR, the zero-shot retrieval benchmark. At BERT Base\nscale, COCO-DR Base outperforms other ZeroDR models with 60x larger size. At\nBERT Large scale, COCO-DR Large outperforms the giant GPT-3 embedding model\nwhich has 500x more parameters. Our analysis show the correlation between\nCOCO-DR's effectiveness in combating distribution shifts and improving\nzero-shot accuracy. Our code and model can be found at\n\\url{https://github.com/OpenMatch/COCO-DR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Si Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Overwijk_A/0/1/0/all/0/1\">Arnold Overwijk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parsing linearizations appreciate PoS tags - but some are fussy about errors. (arXiv:2210.15219v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15219","description":"<p>PoS tags, once taken for granted as a useful resource for syntactic parsing,\nhave become more situational with the popularization of deep learning. Recent\nwork on the impact of PoS tags on graph- and transition-based parsers suggests\nthat they are only useful when tagging accuracy is prohibitively high, or in\nlow-resource scenarios. However, such an analysis is lacking for the emerging\nsequence labeling parsing paradigm, where it is especially relevant as some\nmodels explicitly use PoS tags for encoding and decoding. We undertake a study\nand uncover some trends. Among them, PoS tags are generally more useful for\nsequence labeling parsers than for other paradigms, but the impact of their\naccuracy is highly encoding-dependent, with the PoS-based head-selection\nencoding being best only when both tagging accuracy and resource availability\nare high.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Munoz_Ortiz_A/0/1/0/all/0/1\">Alberto Mu&#xf1;oz-Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_M/0/1/0/all/0/1\">Mark Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vilares_D/0/1/0/all/0/1\">David Vilares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TASA: Deceiving Question Answering Models by Twin Answer Sentences Attack. (arXiv:2210.15221v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15221","description":"<p>We present Twin Answer Sentences Attack (TASA), an adversarial attack method\nfor question answering (QA) models that produces fluent and grammatical\nadversarial contexts while maintaining gold answers. Despite phenomenal\nprogress on general adversarial attacks, few works have investigated the\nvulnerability and attack specifically for QA models. In this work, we first\nexplore the biases in the existing models and discover that they mainly rely on\nkeyword matching between the question and context, and ignore the relevant\ncontextual relations for answer prediction. Based on two biases above, TASA\nattacks the target model in two folds: (1) lowering the model's confidence on\nthe gold answer with a perturbed answer sentence; (2) misguiding the model\ntowards a wrong answer with a distracting answer sentence. Equipped with\ndesigned beam search and filtering methods, TASA can generate more effective\nattacks than existing textual attack methods while sustaining the quality of\ncontexts, in extensive experiments on five QA datasets and human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dianqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Effect of Normalization for Bi-directional Amharic-English Neural Machine Translation. (arXiv:2210.15224v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15224","description":"<p>Machine translation (MT) is one of the main tasks in natural language\nprocessing whose objective is to translate texts automatically from one natural\nlanguage to another. Nowadays, using deep neural networks for MT tasks has\nreceived great attention. These networks require lots of data to learn abstract\nrepresentations of the input and store it in continuous vectors. This paper\npresents the first relatively large-scale Amharic-English parallel sentence\ndataset. Using these compiled data, we build bi-directional Amharic-English\ntranslation models by fine-tuning the existing Facebook M2M100 pre-trained\nmodel achieving a BLEU score of 37.79 in Amharic-English 32.74 in\nEnglish-Amharic translation. Additionally, we explore the effects of Amharic\nhomophone normalization on the machine translation task. The results show that\nthe normalization of Amharic homophone characters increases the performance of\nAmharic-English machine translation in both directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belay_T/0/1/0/all/0/1\">Tadesse Destaw Belay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonja_A/0/1/0/all/0/1\">Atnafu Lambebo Tonja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikova_O/0/1/0/all/0/1\">Olga Kolesnikova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yimam_S/0/1/0/all/0/1\">Seid Muhie Yimam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayele_A/0/1/0/all/0/1\">Abinew Ali Ayele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haile_S/0/1/0/all/0/1\">Silesh Bogale Haile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidorov_G/0/1/0/all/0/1\">Grigori Sidorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1\">Alexander Gelbukh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT-Flow-VAE: A Weakly-supervised Model for Multi-Label Text Classification. (arXiv:2210.15225v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15225","description":"<p>Multi-label Text Classification (MLTC) is the task of categorizing documents\ninto one or more topics. Considering the large volumes of data and varying\ndomains of such tasks, fully supervised learning requires manually fully\nannotated datasets which is costly and time-consuming. In this paper, we\npropose BERT-Flow-VAE (BFV), a Weakly-Supervised Multi-Label Text\nClassification (WSMLTC) model that reduces the need for full supervision. This\nnew model (1) produces BERT sentence embeddings and calibrates them using a\nflow model, (2) generates an initial topic-document matrix by averaging results\nof a seeded sparse topic model and a textual entailment model which only\nrequire surface name of topics and 4-6 seed words per topic, and (3) adopts a\nVAE framework to reconstruct the embeddings under the guidance of the\ntopic-document matrix. Finally, (4) it uses the means produced by the encoder\nmodel in the VAE architecture as predictions for MLTC. Experimental results on\n6 multi-label datasets show that BFV can substantially outperform other\nbaseline WSMLTC models in key metrics and achieve approximately 84% performance\nof a fully-supervised model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grau_Bove_J/0/1/0/all/0/1\">Josep Grau-Bove</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orr_S/0/1/0/all/0/1\">Scott Allan Orr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative pseudo-forced alignment by acoustic CTC loss for self-supervised ASR domain adaptation. (arXiv:2210.15226v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15226","description":"<p>High-quality data labeling from specific domains is costly and human\ntime-consuming. In this work, we propose a self-supervised domain adaptation\nmethod, based upon an iterative pseudo-forced alignment algorithm. The produced\nalignments are employed to customize an end-to-end Automatic Speech Recognition\n(ASR) and iteratively refined. The algorithm is fed with frame-wise character\nposteriors produced by a seed ASR, trained with out-of-domain data, and\noptimized throughout a Connectionist Temporal Classification (CTC) loss. The\nalignments are computed iteratively upon a corpus of broadcast TV. The process\nis repeated by reducing the quantity of text to be aligned or expanding the\nalignment window until finding the best possible audio-text alignment. The\nstarting timestamps, or temporal anchors, are produced uniquely based on the\nconfidence score of the last aligned utterance. This score is computed with the\npaths of the CTC-alignment matrix. With this methodology, no human-revised text\nreferences are required. Alignments from long audio files with low-quality\ntranscriptions, like TV captions, are filtered out by confidence score and\nready for further ASR adaptation. The obtained results, on both the Spanish\nRTVE2022 and CommonVoice databases, underpin the feasibility of using CTC-based\nsystems to perform: highly accurate audio-text alignments, domain adaptation\nand semi-supervised training of end-to-end ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lopez_F/0/1/0/all/0/1\">Fernando L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luque_J/0/1/0/all/0/1\">Jordi Luque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How well can Text-to-Image Generative Models understand Ethical Natural Language Interventions?. (arXiv:2210.15230v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15230","description":"<p>Text-to-image generative models have achieved unprecedented success in\ngenerating high-quality images based on natural language descriptions. However,\nit is shown that these models tend to favor specific social groups when\nprompted with neutral text descriptions (e.g., 'a photo of a lawyer').\nFollowing Zhao et al. (2021), we study the effect on the diversity of the\ngenerated images when adding ethical intervention that supports equitable\njudgment (e.g., 'if all individuals can be a lawyer irrespective of their\ngender') in the input prompts. To this end, we introduce an Ethical NaTural\nLanguage Interventions in Text-to-Image GENeration (ENTIGEN) benchmark dataset\nto evaluate the change in image generations conditional on ethical\ninterventions across three social axes -- gender, skin color, and culture.\nThrough ENTIGEN framework, we find that the generations from minDALL.E,\nDALL.E-mini and Stable Diffusion cover diverse social groups while preserving\nthe image quality. Preliminary studies indicate that a large change in the\nmodel predictions is triggered by certain phrases such as 'irrespective of\ngender' in the context of gender bias in the ethical interventions. We release\ncode and annotated data at https://github.com/Hritikbansal/entigen_emnlp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1\">Hritik Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Da Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monajatipoor_M/0/1/0/all/0/1\">Masoud Monajatipoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Boundary-Aware Language Model Pretraining for Chinese Sequence Labeling. (arXiv:2210.15231v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15231","description":"<p>Boundary information is critical for various Chinese language processing\ntasks, such as word segmentation, part-of-speech tagging, and named entity\nrecognition. Previous studies usually resorted to the use of a high-quality\nexternal lexicon, where lexicon items can offer explicit boundary information.\nHowever, to ensure the quality of the lexicon, great human effort is always\nnecessary, which has been generally ignored. In this work, we suggest\nunsupervised statistical boundary information instead, and propose an\narchitecture to encode the information directly into pre-trained language\nmodels, resulting in Boundary-Aware BERT (BABERT). We apply BABERT for feature\ninduction of Chinese sequence labeling tasks. Experimental results on ten\nbenchmarks of Chinese sequence labeling demonstrate that BABERT can provide\nconsistent improvements on all datasets. In addition, our method can complement\nprevious supervised lexicon exploration, where further improvements can be\nachieved when integrated with external lexicon information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peijie Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_D/0/1/0/all/0/1\">Dingkun Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanzhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Creating a morphological and syntactic tagged corpus for the Uzbek language. (arXiv:2210.15234v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15234","description":"<p>Nowadays, creation of the tagged corpora is becoming one of the most\nimportant tasks of Natural Language Processing (NLP). There are not enough\ntagged corpora to build machine learning models for the low-resource Uzbek\nlanguage. In this paper, we tried to fill that gap by developing a novel Part\nOf Speech (POS) and syntactic tagset for creating the syntactic and\nmorphologically tagged corpus of the Uzbek language. This work also includes\ndetailed description and presentation of a web-based application to work on a\ntagging as well. Based on the developed annotation tool and the software, we\nshare our experience results of the first stage of the tagged corpus creation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharipov_M/0/1/0/all/0/1\">Maksud Sharipov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattiev_J/0/1/0/all/0/1\">Jamolbek Mattiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sobirov_J/0/1/0/all/0/1\">Jasur Sobirov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baltayev_R/0/1/0/all/0/1\">Rustam Baltayev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seq2Seq-SC: End-to-End Semantic Communication Systems with Pre-trained Language Model. (arXiv:2210.15237v1 [eess.SP])","link":"http://arxiv.org/abs/2210.15237","description":"<p>While semantic communication is expected to bring unprecedented communication\nefficiency in comparison to classical communication, many challenges must be\nresolved to realize its potential. In this work, we provide a realistic\nsemantic network dubbed seq2seq-SC, which is compatible to 5G NR and can work\nwith generalized text dataset utilizing pre-trained language model. We also\nutilize a performance metric (SBERT) which can accurately measure semantic\nsimilarity and show that seq2seq-SC achieves superior performance while\nextracting semantically meaningful information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Ju-Hyung Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_D/0/1/0/all/0/1\">Dong-Ho Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sheen_E/0/1/0/all/0/1\">Eunsoo Sheen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Choi_T/0/1/0/all/0/1\">Thomas Choi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1\">Joongheon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Knowledge Graph Construction and Event-centric Knowledge Infusion for Scientific NLI. (arXiv:2210.15248v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15248","description":"<p>With the advance of natural language inference (NLI), a rising demand for NLI\nis to handle scientific texts. Existing methods depend on pre-trained models\n(PTM) which lack domain-specific knowledge. To tackle this drawback, we\nintroduce a scientific knowledge graph to generalize PTM to scientific domain.\nHowever, existing knowledge graph construction approaches suffer from some\ndrawbacks, i.e., expensive labeled data, failure to apply in other domains,\nlong inference time and difficulty extending to large corpora. Therefore, we\npropose an unsupervised knowledge graph construction method to build a\nscientific knowledge graph (SKG) without any labeled data. Moreover, to\nalleviate noise effect from SKG and complement knowledge in sentences better,\nwe propose an event-centric knowledge infusion method to integrate external\nknowledge into each event that is a fine-grained semantic unit in sentences.\nExperimental results show that our method achieves state-of-the-art performance\nand the effectiveness and reliability of SKG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenglin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yucheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaodong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaowei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversation Disentanglement with Bi-Level Contrastive Learning. (arXiv:2210.15265v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15265","description":"<p>Conversation disentanglement aims to group utterances into detached sessions,\nwhich is a fundamental task in processing multi-party conversations. Existing\nmethods have two main drawbacks. First, they overemphasize pairwise utterance\nrelations but pay inadequate attention to the utterance-to-context relation\nmodeling. Second, huge amount of human annotated data is required for training,\nwhich is expensive to obtain in practice. To address these issues, we propose a\ngeneral disentangle model based on bi-level contrastive learning. It brings\ncloser utterances in the same session while encourages each utterance to be\nnear its clustered session prototypes in the representation space. Unlike\nexisting approaches, our disentangle model works in both supervised setting\nwith labeled data and unsupervised setting when no such data is available. The\nproposed method achieves new state-of-the-art performance on both settings\nacross several public datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chengyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hao Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Lizi Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weight Averaging: A Simple Yet Effective Method to Overcome Catastrophic Forgetting in Automatic Speech Recognition. (arXiv:2210.15282v1 [eess.AS])","link":"http://arxiv.org/abs/2210.15282","description":"<p>Adapting a trained Automatic Speech Recognition (ASR) model to new tasks\nresults in catastrophic forgetting of old tasks, limiting the model's ability\nto learn continually and to be extended to new speakers, dialects, languages,\netc. Focusing on End-to-End ASR, in this paper, we propose a simple yet\neffective method to overcome catastrophic forgetting: weight averaging. By\nsimply taking the average of the previous and the adapted model, our method\nachieves high performance on both the old and new tasks. It can be further\nimproved by introducing a knowledge distillation loss during the adaptation. We\nillustrate the effectiveness of our method on both monolingual and multilingual\nASR. In both cases, our method strongly outperforms all baselines, even in its\nsimplest form.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Eeckt_S/0/1/0/all/0/1\">Steven Vander Eeckt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+hamme_H/0/1/0/all/0/1\">Hugo Van hamme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAN: a robust end-to-end ASR model architecture. (arXiv:2210.15285v1 [cs.SD])","link":"http://arxiv.org/abs/2210.15285","description":"<p>In this paper, we propose a novel Siamese Adversarial Network (SAN)\narchitecture for automatic speech recognition, which aims at solving the\ndifficulty of fuzzy audio recognition. Specifically, SAN constructs two\nsub-networks to differentiate the audio feature input and then introduces a\nloss to unify the output distribution of these sub-networks. Adversarial\nlearning enables the network to capture more essential acoustic features and\nhelps the models achieve better performance when encountering fuzzy audio\ninput. We conduct numerical experiments with the SAN model on several datasets\nfor the automatic speech recognition task. All experimental results show that\nthe siamese adversarial nets significantly reduce the character error rate\n(CER). Specifically, we achieve a new state of art 4.37 CER without language\nmodel on the AISHELL-1 dataset, which leads to around 5% relative CER\nreduction. To reveal the generality of the siamese adversarial net, we also\nconduct experiments on the phoneme recognition task, which also shows the\nsuperiority of the siamese adversarial network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_Z/0/1/0/all/0/1\">Zeping Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Q/0/1/0/all/0/1\">Qian Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guanhua Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can language models handle recursively nested grammatical structures? A case study on comparing models and humans. (arXiv:2210.15303v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15303","description":"<p>How should we compare the capabilities of language models and humans? Here, I\nconsider a case study: processing of recursively nested grammatical structures.\nPrior work has suggested that language models cannot handle these structures as\nreliably as humans can. However, the humans were provided with instructions and\ntraining before being evaluated, while the language models were evaluated\nzero-shot. I therefore attempt to more closely match the evaluation paradigms\nby providing language models with few-shot prompts. A simple prompt, which\ncontains substantially less content than the human training, allows large\nlanguage models to consistently outperform the human results. The same prompt\neven allows extrapolation to more-deeply-nested conditions than have been\ntested in humans. Further, a reanalysis of the prior human experiments suggests\nthat the humans may not perform above chance at the difficult structures\ninitially. These results suggest that large language models can in fact process\nrecursively nested grammatical structures comparably to humans. This case study\nhighlights how discrepancies in the quantity of experiment-specific context can\nconfound comparisons of language models and humans. I use this case study to\nreflect on the broader challenge of comparing human and model capabilities, and\nto suggest that there is an important difference between evaluating cognitive\nmodels of a specific phenomenon and evaluating broadly-trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1\">Andrew Kyle Lampinen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Language-centric Scientific AI. (arXiv:2210.15327v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15327","description":"<p>Inspired by recent and revolutionary developments in AI, particularly in\nlanguage understanding and generation, we set about designing AI systems that\nare able to address complex scientific tasks that challenge human capabilities\nto make new discoveries. Central to our approach is the notion of natural\nlanguage as core representation, reasoning, and exchange format between\nscientific AI and human scientists. In this paper, we identify and discuss some\nof the main research challenges to accomplish such vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Perez_J/0/1/0/all/0/1\">Jos&#xe9; Manuel G&#xf3;mez-P&#xe9;rez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dial2vec: Self-Guided Contrastive Learning of Unsupervised Dialogue Embeddings. (arXiv:2210.15332v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15332","description":"<p>In this paper, we introduce the task of learning unsupervised dialogue\nembeddings. Trivial approaches such as combining pre-trained word or sentence\nembeddings and encoding through pre-trained language models (PLMs) have been\nshown to be feasible for this task. However, these approaches typically ignore\nthe conversational interactions between interlocutors, resulting in poor\nperformance. To address this issue, we proposed a self-guided contrastive\nlearning approach named dial2vec. Dial2vec considers a dialogue as an\ninformation exchange process. It captures the conversational interaction\npatterns between interlocutors and leverages them to guide the learning of the\nembeddings corresponding to each interlocutor. The dialogue embedding is\nobtained by an aggregation of the embeddings from all interlocutors. To verify\nour approach, we establish a comprehensive benchmark consisting of six\nwidely-used dialogue datasets. We consider three evaluation tasks: domain\ncategorization, semantic relatedness, and dialogue retrieval. Dial2vec achieves\non average 8.7, 9.0, and 13.8 points absolute improvements in terms of purity,\nSpearman's correlation, and mean average precision (MAP) over the strongest\nbaseline on the three tasks respectively. Further analysis shows that dial2vec\nobtains informative and discriminative embeddings for both interlocutors under\nthe guidance of the conversational interactions and achieves the best\nperformance when aggregating them through the interlocutor-level pooling\nstrategy. All codes and data are publicly available at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/dial2vec.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Che Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junfeng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging knowledge graphs to update scientific word embeddings using latent semantic imputation. (arXiv:2210.15358v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15358","description":"<p>The most interesting words in scientific texts will often be novel or rare.\nThis presents a challenge for scientific word embedding models to determine\nquality embedding vectors for useful terms that are infrequent or newly\nemerging. We demonstrate how \\gls{lsi} can address this problem by imputing\nembeddings for domain-specific words from up-to-date knowledge graphs while\notherwise preserving the original word embedding model. We use the MeSH\nknowledge graph to impute embedding vectors for biomedical terminology without\nretraining and evaluate the resulting embedding model on a domain-specific\nword-pair similarity task. We show that LSI can produce reliable embedding\nvectors for rare and OOV terms in the biomedical domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoelscher_Obermaier_J/0/1/0/all/0/1\">Jason Hoelscher-Obermaier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevinson_E/0/1/0/all/0/1\">Edward Stevinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stauber_V/0/1/0/all/0/1\">Valentin Stauber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhelev_I/0/1/0/all/0/1\">Ivaylo Zhelev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botev_V/0/1/0/all/0/1\">Victor Botev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ronin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minton_J/0/1/0/all/0/1\">Jeremy Minton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FCTalker: Fine and Coarse Grained Context Modeling for Expressive Conversational Speech Synthesis. (arXiv:2210.15360v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15360","description":"<p>Conversational Text-to-Speech (TTS) aims to synthesis an utterance with the\nright linguistic and affective prosody in a conversational context. The\ncorrelation between the current utterance and the dialogue history at the\nutterance level was used to improve the expressiveness of synthesized speech.\nHowever, the fine-grained information in the dialogue history at the word level\nalso has an important impact on the prosodic expression of an utterance, which\nhas not been well studied in the prior work. Therefore, we propose a novel\nexpressive conversational TTS model, termed as FCTalker, that learn the fine\nand coarse grained context dependency at the same time during speech\ngeneration. Specifically, the FCTalker includes fine and coarse grained\nencoders to exploit the word and utterance-level context dependency. To model\nthe word-level dependencies between an utterance and its dialogue history, the\nfine-grained dialogue encoder is built on top of a dialogue BERT model. The\nexperimental results show that the proposed method outperforms all baselines\nand generates more expressive speech that is contextually appropriate. We\nrelease the source code at: https://github.com/walker-hyf/FCTalker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yifan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Guanglai Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Teacher-student Framework for Unsupervised Speech Enhancement Using Noise Remixing Training and Two-stage Inference. (arXiv:2210.15368v1 [cs.SD])","link":"http://arxiv.org/abs/2210.15368","description":"<p>The lack of clean speech is a practical challenge to the development of\nspeech enhancement systems, which means that the training of neural network\nmodels must be done in an unsupervised manner, and there is an inevitable\nmismatch between their training criterion and evaluation metric. In response to\nthis unfavorable situation, we propose a teacher-student training strategy that\ndoes not require any subjective/objective speech quality metrics as learning\nreference by improving the previously proposed noisy-target training (NyTT).\nBecause homogeneity between in-domain noise and extraneous noise is the key to\nthe effectiveness of NyTT, we train various student models by remixing the\nteacher model's estimated speech and noise for clean-target training or raw\nnoisy speech and the teacher model's estimated noise for noisy-target training.\nWe use the NyTT model as the initial teacher model. Experimental results show\nthat our proposed method outperforms several baselines, especially with\ntwo-stage inference, where clean speech is derived successively through the\nbootstrap model and the final student model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yao-Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Shin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CasNet: Investigating Channel Robustness for Speech Separation. (arXiv:2210.15370v1 [cs.SD])","link":"http://arxiv.org/abs/2210.15370","description":"<p>Recording channel mismatch between training and testing conditions has been\nshown to be a serious problem for speech separation. This situation greatly\nreduces the separation performance, and cannot meet the requirement of daily\nuse. In this study, inheriting the use of our previously constructed TAT-2mix\ncorpus, we address the channel mismatch problem by proposing a channel-aware\naudio separation network (CasNet), a deep learning framework for end-to-end\ntime-domain speech separation. CasNet is implemented on top of TasNet. Channel\nembedding (characterizing channel information in a mixture of multiple\nutterances) generated by Channel Encoder is introduced into the separation\nmodule by the FiLM technique. Through two training strategies, we explore two\nroles that channel embedding may play: 1) a real-life noise disturbance, making\nthe model more robust, or 2) a guide, instructing the separation model to\nretain the desired channel information. Experimental results on TAT-2mix show\nthat CasNet trained with both training strategies outperforms the TasNet\nbaseline, which does not use channel embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan-Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yao-Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Shin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-consistent Reasoning For Solving Math Word Problems. (arXiv:2210.15373v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15373","description":"<p>Math word problems (MWPs) is a task that automatically derives solution\nexpression from a giving math problems in text. The previous studies suffer\nfrom spurious correlations between input text and output expression. To\nmitigate this issue, we propose a self-consistent reasoning framework called\nSCR, which attempts to adopt a pruning strategy to correct the output\ndistribution shift so as to implicitly fix those spurious correlative samples.\nSpecifically, we firstly obtain a sub-network by pruning a roberta2tree model,\nfor the sake to use the gap on output distribution between the original\nroberta2tree model and the pruned sub-network to expose spurious correlative\nsamples. Then, we calibrate the output distribution shift by applying symmetric\nKullback-Leibler divergence to alleviate spurious correlations. In addition,\nSCR generates equivalent expressions, thereby, capturing the original text's\nlogic rather than relying on hints from original text. Extensive experiments on\ntwo large-scale benchmarks demonstrate that our model substantially outperforms\nthe strong baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jing Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zhongwei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiping Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengming Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structuring User-Generated Content on Social Media with Multimodal Aspect-Based Sentiment Analysis. (arXiv:2210.15377v1 [cs.IR])","link":"http://arxiv.org/abs/2210.15377","description":"<p>People post their opinions and experiences on social media, yielding rich\ndatabases of end users' sentiments. This paper shows to what extent machine\nlearning can analyze and structure these databases. An automated data analysis\npipeline is deployed to provide insights into user-generated content for\nresearchers in other domains. First, the domain expert can select an image and\na term of interest. Then, the pipeline uses image retrieval to find all images\nshowing similar contents and applies aspect-based sentiment analysis to outline\nusers' opinions about the selected term. As part of an interdisciplinary\nproject between architecture and computer science researchers, an empirical\nstudy of Hamburg's Elbphilharmonie was conveyed on 300 thousand posts from the\nplatform Flickr with the hashtag 'hamburg'. Image retrieval methods generated a\nsubset of slightly more than 1.5 thousand images displaying the\nElbphilharmonie. We found that these posts mainly convey a neutral or positive\nsentiment towards it. With this pipeline, we suggest a new big data analysis\nmethod that offers new insights into end-users opinions, e.g., for architecture\ndomain experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anschutz_M/0/1/0/all/0/1\">Miriam Ansch&#xfc;tz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eder_T/0/1/0/all/0/1\">Tobias Eder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1\">Georg Groh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MorphTE: Injecting Morphology in Tensorized Embeddings. (arXiv:2210.15379v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15379","description":"<p>In the era of deep learning, word embeddings are essential when dealing with\ntext tasks. However, storing and accessing these embeddings requires a large\namount of space. This is not conducive to the deployment of these models on\nresource-limited devices. Combining the powerful compression capability of\ntensor products, we propose a word embedding compression method with\nmorphological augmentation, Morphologically-enhanced Tensorized Embeddings\n(MorphTE). A word consists of one or more morphemes, the smallest units that\nbear meaning or have a grammatical function. MorphTE represents a word\nembedding as an entangled form of its morpheme vectors via the tensor product,\nwhich injects prior semantic and grammatical knowledge into the learning of\nembeddings. Furthermore, the dimensionality of the morpheme vector and the\nnumber of morphemes are much smaller than those of words, which greatly reduces\nthe parameters of the word embeddings. We conduct experiments on tasks such as\nmachine translation and question answering. Experimental results on four\ntranslation datasets of different languages show that MorphTE can compress word\nembedding parameters by about 20 times without performance loss and\nsignificantly outperforms related embedding compression methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gan_G/0/1/0/all/0/1\">Guobing Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sunzhu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiuqing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benyou Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Opening the Black Box of wav2vec Feature Encoder. (arXiv:2210.15386v1 [cs.SD])","link":"http://arxiv.org/abs/2210.15386","description":"<p>Self-supervised models, namely, wav2vec and its variants, have shown\npromising results in various downstream tasks in the speech domain. However,\ntheir inner workings are poorly understood, calling for in-depth analyses on\nwhat the model learns. In this paper, we concentrate on the convolutional\nfeature encoder where its latent space is often speculated to represent\ndiscrete acoustic units. To analyze the embedding space in a reductive manner,\nwe feed the synthesized audio signals, which is the summation of simple sine\nwaves. Through extensive experiments, we conclude that various information is\nembedded inside the feature encoder representations: (1) fundamental frequency,\n(2) formants, and (3) amplitude, packed with (4) sufficient temporal detail.\nFurther, the information incorporated inside the latent representations is\nanalogous to spectrograms but with a fundamental difference: latent\nrepresentations construct a metric space so that closer representations imply\nacoustic similarity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1\">Kwanghee Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_E/0/1/0/all/0/1\">Eun Jung Yeo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Severity Assessment of Dysarthric speech by using Self-supervised Model with Multi-task Learning. (arXiv:2210.15387v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15387","description":"<p>Automatic assessment of dysarthric speech is essential for sustained\ntreatments and rehabilitation. However, obtaining atypical speech is\nchallenging, often leading to data scarcity issues. To tackle the problem, we\npropose a novel automatic severity assessment method for dysarthric speech,\nusing the self-supervised model in conjunction with multi-task learning.\nWav2vec 2.0 XLS-R is jointly trained for two different tasks: severity level\nclassification and an auxilary automatic speech recognition (ASR). For the\nbaseline experiments, we employ hand-crafted features such as eGeMaps and\nlinguistic features, and SVM, MLP, and XGBoost classifiers. Explored on the\nKorean dysarthric speech QoLT database, our model outperforms the traditional\nbaseline methods, with a relative percentage increase of 4.79% for\nclassification accuracy. In addition, the proposed model surpasses the model\ntrained without ASR head, achieving 10.09% relative percentage improvements.\nFurthermore, we present how multi-task learning affects the severity\nclassification performance by analyzing the latent representations and\nregularization effect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeo_E/0/1/0/all/0/1\">Eun Jung Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1\">Kwanghee Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunhee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_M/0/1/0/all/0/1\">Minhwa Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Make More of Your Data: Minimal Effort Data Augmentation for Automatic Speech Recognition and Translation. (arXiv:2210.15398v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15398","description":"<p>Data augmentation is a technique to generate new training data based on\nexisting data. We evaluate the simple and cost-effective method of\nconcatenating the original data examples to build new training instances.\nContinued training with such augmented data is able to improve off-the-shelf\nTransformer and Conformer models that were optimized on the original data only.\nWe demonstrate considerable improvements on the LibriSpeech-960h test sets (WER\n2.83 and 6.87 for test-clean and test-other), which carry over to models\ncombined with shallow fusion (WER 2.55 and 6.27). Our method of continued\ntraining also leads to improvements of up to 0.9 WER on the ASR part of\nCoVoST-2 for four non English languages, and we observe that the gains are\nhighly dependent on the size of the original training data. We compare\ndifferent concatenation strategies and found that our method does not need\nspeaker information to achieve its improvements. Finally, we demonstrate on two\ndatasets that our methods also works for speech translation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1\">Tsz Kin Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schamoni_S/0/1/0/all/0/1\">Shigehiko Schamoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1\">Stefan Riezler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Language Model to Train if You Have One Million GPU Hours?. (arXiv:2210.15424v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15424","description":"<p>The crystallization of modeling methods around the Transformer architecture\nhas been a boon for practitioners. Simple, well-motivated architectural\nvariations can transfer across tasks and scale, increasing the impact of\nmodeling research. However, with the emergence of state-of-the-art 100B+\nparameters models, large language models are increasingly expensive to\naccurately design and train. Notably, it can be difficult to evaluate how\nmodeling decisions may impact emergent capabilities, given that these\ncapabilities arise mainly from sheer scale alone. In the process of building\nBLOOM--the Big Science Large Open-science Open-access Multilingual language\nmodel--our goal is to identify an architecture and training setup that makes\nthe best use of our 1,000,000 A100-GPU-hours budget. Specifically, we perform\nan ablation study at the billion-parameter scale comparing different modeling\npractices and their impact on zero-shot generalization. In addition, we study\nthe impact of various popular pre-training corpora on zero-shot generalization.\nWe also study the performance of a multilingual model and how it compares to\nthe English-only one. Finally, we consider the scaling behaviour of\nTransformers to choose the target model size, shape, and training setup. All\nour models and code are open-sourced at https://huggingface.co/bigscience .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scao_T/0/1/0/all/0/1\">Teven Le Scao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Thomas Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hesslow_D/0/1/0/all/0/1\">Daniel Hesslow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saulnier_L/0/1/0/all/0/1\">Lucile Saulnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekman_S/0/1/0/all/0/1\">Stas Bekman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1\">M Saiful Bari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bideman_S/0/1/0/all/0/1\">Stella Bideman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsahar_H/0/1/0/all/0/1\">Hady Elsahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1\">Niklas Muennighoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phang_J/0/1/0/all/0/1\">Jason Phang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Press_O/0/1/0/all/0/1\">Ofir Press</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanh_V/0/1/0/all/0/1\">Victor Sanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutawika_L/0/1/0/all/0/1\">Lintang Sutawika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tae_J/0/1/0/all/0/1\">Jaesung Tae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yong_Z/0/1/0/all/0/1\">Zheng Xin Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Launay_J/0/1/0/all/0/1\">Julien Launay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Use of Large Pre-Trained Models for Low Resource ASR. (arXiv:2210.15445v1 [eess.AS])","link":"http://arxiv.org/abs/2210.15445","description":"<p>Automatic speech recognition (ASR) has been established as a well-performing\ntechnique for many scenarios where lots of labeled data is available.\nAdditionally, unsupervised representation learning recently helped to tackle\ntasks with limited data. Following this, hardware limitations and applications\ngive rise to the question how to efficiently take advantage of large pretrained\nmodels and reduce their complexity for downstream tasks. In this work, we study\na challenging low resource conversational telephony speech corpus from the\nmedical domain in Vietnamese and German. We show the benefits of using\nunsupervised techniques beyond simple fine-tuning of large pre-trained models,\ndiscuss how to adapt them to a practical telephony task including bandwidth\ntransfer and investigate different data conditions for pre-training and\nfine-tuning. We outperform the project baselines by 22% relative using\npretraining techniques. Further gains of 29% can be achieved by refinements of\narchitecture and training and 6% by adding 0.8 h of in-domain adaptation data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vieting_P/0/1/0/all/0/1\">Peter Vieting</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luscher_C/0/1/0/all/0/1\">Christoph L&#xfc;scher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dierkes_J/0/1/0/all/0/1\">Julian Dierkes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Virtuoso: Massive Multilingual Speech-Text Joint Semi-Supervised Learning for Text-To-Speech. (arXiv:2210.15447v1 [cs.SD])","link":"http://arxiv.org/abs/2210.15447","description":"<p>This paper proposes Virtuoso, a massively multilingual speech-text joint\nsemi-supervised learning framework for text-to-speech synthesis (TTS) models.\nExisting multilingual TTS typically supports tens of languages, which are a\nsmall fraction of the thousands of languages in the world. One difficulty to\nscale multilingual TTS to hundreds of languages is collecting high-quality\nspeech-text paired data in low-resource languages. This study extends Maestro,\na speech-text joint pretraining framework for automatic speech recognition\n(ASR), to speech generation tasks. To train a TTS model from various types of\nspeech and text data, different training schemes are designed to handle\nsupervised (paired TTS and ASR data) and unsupervised (untranscribed speech and\nunspoken text) datasets. Experimental evaluation shows that 1) multilingual TTS\nmodels trained on Virtuoso can achieve significantly better naturalness and\nintelligibility than baseline ones in seen languages, and 2) they can\nsynthesize reasonably intelligible and naturally sounding speech for unseen\nlanguages where no high-quality paired TTS data is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saeki_T/0/1/0/all/0/1\">Takaaki Saeki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zen_H/0/1/0/all/0/1\">Heiga Zen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhehuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morioka_N/0/1/0/all/0/1\">Nobuyuki Morioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gary Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_A/0/1/0/all/0/1\">Andrew Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramabhadran_B/0/1/0/all/0/1\">Bhuvana Ramabhadran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Predictive Uncertainty and Calibration in NLP: A Study on the Impact of Method & Data Scarcity. (arXiv:2210.15452v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15452","description":"<p>We investigate the problem of determining the predictive confidence (or,\nconversely, uncertainty) of a neural classifier through the lens of\nlow-resource languages. By training models on sub-sampled datasets in three\ndifferent languages, we assess the quality of estimates from a wide array of\napproaches and their dependence on the amount of available data. We find that\nwhile approaches based on pre-trained models and ensembles achieve the best\nresults overall, the quality of uncertainty estimates can surprisingly suffer\nwith more data. We also perform a qualitative analysis of uncertainties on\nsequences, discovering that a model's total uncertainty seems to be influenced\nto a large degree by its data uncertainty, not model uncertainty. All model\nimplementations are open-sourced in a software package.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ulmer_D/0/1/0/all/0/1\">Dennis Ulmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frellsen_J/0/1/0/all/0/1\">Jes Frellsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardmeier_C/0/1/0/all/0/1\">Christian Hardmeier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JECC: Commonsense Reasoning Tasks Derived from Interactive Fictions. (arXiv:2210.15456v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15456","description":"<p>Commonsense reasoning simulates the human ability to make presumptions about\nour physical world, and it is an essential cornerstone in building general AI\nsystems. We propose a new commonsense reasoning dataset based on human's\nInteractive Fiction (IF) gameplay walkthroughs as human players demonstrate\nplentiful and diverse commonsense reasoning. The new dataset provides a natural\nmixture of various reasoning types and requires multi-hop reasoning. Moreover,\nthe IF game-based construction procedure requires much less human interventions\nthan previous ones. Experiments show that the introduced dataset is challenging\nto previous machine reading models with a significant 20% performance gap\ncompared to human experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaoxiao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yufei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenspan_M/0/1/0/all/0/1\">Michael Greenspan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_M/0/1/0/all/0/1\">Murray Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models. (arXiv:2210.15458v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15458","description":"<p>Decoding methods for large language models often trade-off between diversity\nof outputs and parallelism of computation. Methods such as beam search and\nGumbel top-k sampling can guarantee a different output for each element of the\nbeam, but are not easy to parallelize. Alternatively, methods such as\ntemperature sampling and its modifications (top-k sampling, nucleus sampling,\ntypical decoding, and others), are embarrassingly parallel, but have no\nguarantees about duplicate samples. We present a framework for sampling\naccording to an arithmetic code book implicitly defined by a large language\nmodel, compatible with common sampling variations, with provable beam diversity\nunder certain conditions, as well as being embarrassingly parallel and\nproviding unbiased and consistent expectations from the original model. We\ndemonstrate the effectiveness of our approach on WMT machine translation,\nshowing substantially reduced variance when estimating expected BLEU score and\nup to 1 point increased BLEU in oracle experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vilnis_L/0/1/0/all/0/1\">Luke Vilnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemlyanskiy_Y/0/1/0/all/0/1\">Yury Zemlyanskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_P/0/1/0/all/0/1\">Patrick Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passos_A/0/1/0/all/0/1\">Alexandre Passos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghai_S/0/1/0/all/0/1\">Sumit Sanghai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation. (arXiv:2210.15461v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15461","description":"<p>Multimodal Machine Translation (MMT) focuses on enhancing text-only\ntranslation with visual features, which has attracted considerable attention\nfrom both natural language processing and computer vision communities. Recent\nadvances still struggle to train a separate model for each language pair, which\nis costly and unaffordable when the number of languages increases in the real\nworld. In other words, the multilingual multimodal machine translation\n(Multilingual MMT) task has not been investigated, which aims to handle the\naforementioned issues by providing a shared semantic space for multiple\nlanguages. Besides, the image modality has no language boundaries, which is\nsuperior to bridging the semantic gap between languages. To this end, we first\npropose the Multilingual MMT task by establishing two new Multilingual MMT\nbenchmark datasets covering seven languages. Then, an effective baseline LVP-M3\nusing visual prompts is proposed to support translations between different\nlanguages, which includes three stages (token encoding, language-aware visual\nprompt generation, and language translation). Extensive experimental results on\nour constructed benchmark datasets demonstrate the effectiveness of LVP-M3\nmethod for Multilingual MMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hongcheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"He Said, She Said: Style Transfer for Shifting the Perspective of Dialogues. (arXiv:2210.15462v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15462","description":"<p>In this work, we define a new style transfer task: perspective shift, which\nreframes a dialogue from informal first person to a formal third person\nrephrasing of the text. This task requires challenging coreference resolution,\nemotion attribution, and interpretation of informal text. We explore several\nbaseline approaches and discuss further directions on this task when applied to\nshort dialogues. As a sample application, we demonstrate that applying\nperspective shifting to a dialogue summarization dataset (SAMSum) substantially\nimproves the zero-shot performance of extractive news summarization models on\nthis data. Additionally, supervised extractive models perform better when\ntrained on perspective shifted data than on the original dialogues. We release\nour code publicly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bertsch_A/0/1/0/all/0/1\">Amanda Bertsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gormley_M/0/1/0/all/0/1\">Matthew R. Gormley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LSG Attention: Extrapolation of pretrained Transformers to long sequences. (arXiv:2210.15497v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15497","description":"<p>Transformer models achieve state-of-the-art performance on a wide range of\nNLP tasks. They however suffer from a prohibitive limitation due to the\nself-attention mechanism, inducing $O(n^2)$ complexity with regard to sequence\nlength. To answer this limitation we introduce the LSG architecture which\nrelies on Local, Sparse and Global attention. We show that LSG attention is\nfast, efficient and competitive in classification and summarization tasks on\nlong documents. Interestingly, it can also be used to adapt existing pretrained\nmodels to efficiently extrapolate to longer sequences with no additional\ntraining. Along with the introduction of the LSG attention mechanism, we\npropose tools to train new models and adapt existing ones based on this\nmechanism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Condevaux_C/0/1/0/all/0/1\">Charles Condevaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harispe_S/0/1/0/all/0/1\">S&#xe9;bastien Harispe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COFFEE: Counterfactual Fairness for Personalized Text Generation in Explainable Recommendation. (arXiv:2210.15500v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15500","description":"<p>Personalized text generation has broad industrial applications, such as\nexplanation generation for recommendations, conversational systems, etc.\nPersonalized text generators are usually trained on user written text, e.g.,\nreviews collected on e-commerce platforms. However, due to historical, social,\nor behavioral reasons, there may exist bias that associates certain linguistic\nquality of user written text with the users' protected attributes such as\ngender, race, etc. The generators can identify and inherit these correlations\nand generate texts discriminately w.r.t. the users' protected attributes.\nWithout proper intervention, such bias can adversarially influence the users'\ntrust and reliance on the system. From a broader perspective, bias in\nauto-generated contents can reinforce the social stereotypes about how online\nusers write through interactions with the users.\n</p>\n<p>In this work, we investigate the fairness of personalized text generation in\nthe setting of explainable recommendation. We develop a general framework for\nachieving measure-specific counterfactual fairness on the linguistic quality of\npersonalized explanations. We propose learning disentangled representations for\ncounterfactual inference and develop a novel policy learning algorithm with\ncarefully designed rewards for fairness optimization. The framework can be\napplied for achieving fairness on any given specifications of linguistic\nquality measures, and can be adapted to most of existing models and real-world\nsettings. Extensive experiments demonstrate the superior ability of our method\nin achieving fairness while maintaining high generation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1\">Shaoliang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi-Chia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1\">Maziar Sanjabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingzhou Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1\">Hamed Firooz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongning Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COST-EFF: Collaborative Optimization of Spatial and Temporal Efficiency with Slenderized Multi-exit Language Models. (arXiv:2210.15523v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15523","description":"<p>Transformer-based pre-trained language models (PLMs) mostly suffer from\nexcessive overhead despite their advanced capacity. For resource-constrained\ndevices, there is an urgent need for a spatially and temporally efficient model\nwhich retains the major capacity of PLMs. However, existing statically\ncompressed models are unaware of the diverse complexities between input\ninstances, potentially resulting in redundancy and inadequacy for simple and\ncomplex inputs. Also, miniature models with early exiting encounter challenges\nin the trade-off between making predictions and serving the deeper layers.\nMotivated by such considerations, we propose a collaborative optimization for\nPLMs that integrates static model compression and dynamic inference\nacceleration. Specifically, the PLM is slenderized in width while the depth\nremains intact, complementing layer-wise early exiting to speed up inference\ndynamically. To address the trade-off of early exiting, we propose a joint\ntraining approach that calibrates slenderization and preserves contributive\nstructures to each exit instead of only the final layer. Experiments are\nconducted on GLUE benchmark and the results verify the Pareto optimality of our\napproach at high compression and acceleration rate with 1/8 parameters and 1/19\nFLOPs of BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bowen Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuanxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Terminology-aware Medical Dialogue Generation. (arXiv:2210.15551v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15551","description":"<p>Medical dialogue generation aims to generate responses according to a history\nof dialogue turns between doctors and patients. Unlike open-domain dialogue\ngeneration, this requires background knowledge specific to the medical domain.\nExisting generative frameworks for medical dialogue generation fall short of\nincorporating domain-specific knowledge, especially with regard to medical\nterminology. In this paper, we propose a novel framework to improve medical\ndialogue generation by considering features centered on domain-specific\nterminology. We leverage an attention mechanism to incorporate terminologically\ncentred features, and fill in the semantic gap between medical background\nknowledge and common utterances by enforcing language models to learn\nterminology representations with an auxiliary terminology recognition task.\nExperimental results demonstrate the effectiveness of our approach, in which\nour proposed framework outperforms SOTA language models. Additionally, we\nprovide a new dataset with medical terminology annotations to support the\nresearch on medical dialogue generation. Our dataset and code are available at\nhttps://github.com/tangg555/meddialog.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loakman_T/0/1/0/all/0/1\">Tyler Loakman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1\">Frank Guerin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving abstractive summarization with energy-based re-ranking. (arXiv:2210.15553v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15553","description":"<p>Current abstractive summarization systems present important weaknesses which\nprevent their deployment in real-world applications, such as the omission of\nrelevant information and the generation of factual inconsistencies (also known\nas hallucinations). At the same time, automatic evaluation metrics such as CTC\nscores have been recently proposed that exhibit a higher correlation with human\njudgments than traditional lexical-overlap metrics such as ROUGE. In this work,\nwe intend to close the loop by leveraging the recent advances in summarization\nmetrics to create quality-aware abstractive summarizers. Namely, we propose an\nenergy-based model that learns to re-rank summaries according to one or a\ncombination of these metrics. We experiment using several metrics to train our\nenergy-based re-ranker and show that it consistently improves the scores\nachieved by the predicted summaries. Nonetheless, human evaluation results show\nthat the re-ranking approach should be used with care for highly abstractive\nsummaries, as the available metrics are not yet sufficiently reliable for this\npurpose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pernes_D/0/1/0/all/0/1\">Diogo Pernes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendes_A/0/1/0/all/0/1\">Afonso Mendes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F.T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Extraction of Materials and Properties from Superconductors Scientific Literature. (arXiv:2210.15600v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15600","description":"<p>The automatic extraction of materials and related properties from the\nscientific literature is gaining attention in data-driven materials science\n(Materials Informatics). In this paper, we discuss Grobid-superconductors, our\nsolution for automatically extracting superconductor material names and\nrespective properties from text. Built as a Grobid module, it combines machine\nlearning and heuristic approaches in a multi-step architecture that supports\ninput data as raw text or PDF documents. Using Grobid-superconductors, we built\nSuperCon2, a database of 40324 materials and properties records from 37700\npapers. The material (or sample) information is represented by name, chemical\nformula, and material class, and is characterized by shape, doping,\nsubstitution variables for components, and substrate as adjoined information.\nThe properties include the Tc superconducting critical temperature and, when\navailable, applied pressure with the Tc measurement method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Foppiano_L/0/1/0/all/0/1\">Luca Foppiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1\">Pedro Baptista de Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_P/0/1/0/all/0/1\">Pedro Ortiz Suarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terashima_K/0/1/0/all/0/1\">Kensei Terashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takano_Y/0/1/0/all/0/1\">Yoshihiko Takano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_M/0/1/0/all/0/1\">Masashi Ishii</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Working Alliance Transformer for Psychotherapy Dialogue Classification. (arXiv:2210.15603v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15603","description":"<p>As a predictive measure of the treatment outcome in psychotherapy, the\nworking alliance measures the agreement of the patient and the therapist in\nterms of their bond, task and goal. Long been a clinical quantity estimated by\nthe patients' and therapists' self-evaluative reports, we believe that the\nworking alliance can be better characterized using natural language processing\ntechnique directly in the dialogue transcribed in each therapy session. In this\nwork, we propose the Working Alliance Transformer (WAT), a Transformer-based\nclassification model that has a psychological state encoder which infers the\nworking alliance scores by projecting the embedding of the dialogues turns onto\nthe embedding space of the clinical inventory for working alliance. We evaluate\nour method in a real-world dataset with over 950 therapy sessions with anxiety,\ndepression, schizophrenia and suicidal patients and demonstrate an empirical\nadvantage of using information about the therapeutic states in this sequence\nclassification task of psychotherapy dialogues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Baihan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cecchi_G/0/1/0/all/0/1\">Guillermo Cecchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouneffouf_D/0/1/0/all/0/1\">Djallel Bouneffouf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACES: Translation Accuracy Challenge Sets for Evaluating Machine Translation Metrics. (arXiv:2210.15615v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15615","description":"<p>As machine translation (MT) metrics improve their correlation with human\njudgement every year, it is crucial to understand the limitations of such\nmetrics at the segment level. Specifically, it is important to investigate\nmetric behaviour when facing accuracy errors in MT because these can have\ndangerous consequences in certain contexts (e.g., legal, medical). We curate\nACES, a translation accuracy challenge set, consisting of 68 phenomena ranging\nfrom simple perturbations at the word/character level to more complex errors\nbased on discourse and real-world knowledge. We use ACES to evaluate a wide\nrange of MT metrics including the submissions to the WMT 2022 metrics shared\ntask and perform several analyses leading to general recommendations for metric\ndevelopers. We recommend: a) combining metrics with different strengths, b)\ndeveloping metrics that give more weight to the source and less to\nsurface-level overlap with the reference and c) explicitly modelling additional\nlanguage-specific information beyond what is available via multilingual\nembeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amrhein_C/0/1/0/all/0/1\">Chantal Amrhein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moghe_N/0/1/0/all/0/1\">Nikita Moghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guillou_L/0/1/0/all/0/1\">Liane Guillou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Domain Neural Entity Linking. (arXiv:2210.15616v1 [cs.CL])","link":"http://arxiv.org/abs/2210.15616","description":"<p>Entity Linking is the task of matching a mention to an entity in a given\nknowledge base (KB). It contributes to annotating a massive amount of documents\nexisting on the Web to harness new facts about their matched entities. However,\nexisting Entity Linking systems focus on developing models that are typically\ndomain-dependent and robust only to a particular knowledge base on which they\nhave been trained. The performance is not as adequate when being evaluated on\ndocuments and knowledge bases from different domains.\n</p>\n<p>Approaches based on pre-trained language models, such as Wu et al. (2020),\nattempt to solve the problem using a zero-shot setup, illustrating some\npotential when evaluated on a general-domain KB. Nevertheless, the performance\nis not equivalent when evaluated on a domain-specific KB. To allow for more\naccurate Entity Linking across different domains, we propose our framework:\nCross-Domain Neural Entity Linking (CDNEL). Our objective is to have a single\nsystem that enables simultaneous linking to both the general-domain KB and the\ndomain-specific KB. CDNEL works by learning a joint representation space for\nthese knowledge bases from different domains. It is evaluated using the\nexternal Entity Linking dataset (Zeshel) constructed by Logeswaran et al.\n(2019) and the Reddit dataset collected by Botzer et al. (2021), to compare our\nproposed method with the state-of-the-art results. The proposed framework uses\ndifferent types of datasets for fine-tuning, resulting in different model\nvariants of CDNEL. When evaluated on four domains included in the Zeshel\ndataset, these variants achieve an average precision gain of 9%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soliman_H/0/1/0/all/0/1\">Hassan Soliman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAD: Language Augmented Diffusion for Reinforcement Learning. (arXiv:2210.15629v1 [cs.LG])","link":"http://arxiv.org/abs/2210.15629","description":"<p>Learning skills from language provides a powerful avenue for generalization\nin reinforcement learning, although it remains a challenging task as it\nrequires agents to capture the complex interdependencies between language,\nactions, and states. In this paper, we propose leveraging Language Augmented\nDiffusion models as a planner conditioned on language (LAD). We demonstrate the\ncomparable performance of LAD with the state-of-the-art on the CALVIN language\nrobotics benchmark with a much simpler architecture that contains no inductive\nbiases specialized to robotics, achieving an average success rate (SR) of 72%\ncompared to the best performance of 76%. We also conduct an analysis on the\nproperties of language conditioned diffusion in reinforcement learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1\">Edwin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Amy Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Effective Distillation of Self-Supervised Speech Models for Automatic Speech Recognition. (arXiv:2210.15631v1 [eess.AS])","link":"http://arxiv.org/abs/2210.15631","description":"<p>Recent years have witnessed great strides in self-supervised learning (SSL)\non the speech processing. The SSL model is normally pre-trained on a great\nvariety of unlabelled data and a large model size is preferred to increase the\nmodeling capacity. However, this might limit its potential applications due to\nthe expensive computation and memory costs introduced by the oversize model.\nMiniaturization for SSL models has become an important research direction of\npractical value. To this end, we explore the effective distillation of\nHuBERT-based SSL models for automatic speech recognition (ASR). First, in order\nto establish a strong baseline, a comprehensive study on different student\nmodel structures is conducted. On top of this, as a supplement to the\nregression loss widely adopted in previous works, a discriminative loss is\nintroduced for HuBERT to enhance the distillation performance, especially in\nlow-resource scenarios. In addition, we design a simple and effective algorithm\nto distill the front-end input from waveform to Fbank feature, resulting in 17%\nparameter reduction and doubling inference speed, at marginal performance\ndegradation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yujin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_C/0/1/0/all/0/1\">Changli Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_Z/0/1/0/all/0/1\">Ziyang Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhisheng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xie Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Qiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LyricJam Sonic: A Generative System for Real-Time Composition and Musical Improvisation. (arXiv:2210.15638v1 [cs.SD])","link":"http://arxiv.org/abs/2210.15638","description":"<p>Electronic music artists and sound designers have unique workflow practices\nthat necessitate specialized approaches for developing music information\nretrieval and creativity support tools. Furthermore, electronic music\ninstruments, such as modular synthesizers, have near-infinite possibilities for\nsound creation and can be combined to create unique and complex audio paths.\nThe process of discovering interesting sounds is often serendipitous and\nimpossible to replicate. For this reason, many musicians in electronic genres\nrecord audio output at all times while they work in the studio. Subsequently,\nit is difficult for artists to rediscover audio segments that might be suitable\nfor use in their compositions from thousands of hours of recordings. In this\npaper, we describe LyricJam Sonic -- a novel creative tool for musicians to\nrediscover their previous recordings, re-contextualize them with other\nrecordings, and create original live music compositions in real-time. A\nbi-modal AI-driven approach uses generated lyric lines to find matching audio\nclips from the artist's past studio recordings, and uses them to generate new\nlyric lines, which in turn are used to find other clips, thus creating a\ncontinuous and evolving stream of music and lyrics. The intent is to keep the\nartists in a state of creative flow conducive to music creation rather than\ntaking them into an analytical/critical state of deliberately searching for\npast audio segments. The system can run in either a fully autonomous mode\nwithout user input, or in a live performance mode, where the artist plays live\nmusic, while the system \"listens\" and creates a continuous stream of music and\nlyrics in response.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vechtomova_O/0/1/0/all/0/1\">Olga Vechtomova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahu_G/0/1/0/all/0/1\">Gaurav Sahu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Attention for Automatic Chest X-ray Report Generation. (arXiv:2106.06965v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06965","description":"<p>Recently, chest X-ray report generation, which aims to automatically generate\ndescriptions of given chest X-ray images, has received growing research\ninterests. The key challenge of chest X-ray report generation is to accurately\ncapture and describe the abnormal regions. In most cases, the normal regions\ndominate the entire chest X-ray image, and the corresponding descriptions of\nthese normal regions dominate the final report. Due to such data bias,\nlearning-based models may fail to attend to abnormal regions. In this work, to\neffectively capture and describe abnormal regions, we propose the Contrastive\nAttention (CA) model. Instead of solely focusing on the current input image,\nthe CA model compares the current input image with normal images to distill the\ncontrastive information. The acquired contrastive information can better\nrepresent the visual features of abnormal regions. According to the experiments\non the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into\nseveral existing models can boost their performance across most metrics. In\naddition, according to the analysis, the CA model can help existing models\nbetter attend to the abnormal regions and provide more accurate descriptions\nwhich are crucial for an interpretable diagnosis. Specifically, we achieve the\nstate-of-the-art results on the two public datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuewei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Changchang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Ping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IndicBART: A Pre-trained Model for Indic Natural Language Generation. (arXiv:2109.02903v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02903","description":"<p>In this paper, we study pre-trained sequence-to-sequence models for a group\nof related languages, with a focus on Indic languages. We present IndicBART, a\nmultilingual, sequence-to-sequence pre-trained model focusing on 11 Indic\nlanguages and English. IndicBART utilizes the orthographic similarity between\nIndic scripts to improve transfer learning between similar Indic languages. We\nevaluate IndicBART on two NLG tasks: Neural Machine Translation (NMT) and\nextreme summarization. Our experiments on NMT and extreme summarization show\nthat a model specific to related languages like IndicBART is competitive with\nlarge pre-trained models like mBART50 despite being significantly smaller. It\nalso performs well on very low-resource translation scenarios where languages\nare not included in pre-training or fine-tuning. Script sharing, multilingual\ntraining, and better utilization of limited model capacity contribute to the\ngood performance of the compact IndicBART model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrotriya_H/0/1/0/all/0/1\">Himani Shrotriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puduppully_R/0/1/0/all/0/1\">Ratish Puduppully</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-NER : Contextual Phrase Generation at Scale. (arXiv:2109.08079v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2109.08079","description":"<p>NLP research has been focused on NER extraction and how to efficiently\nextract them from a sentence. However, generating relevant context of entities\nfrom a sentence has remained under-explored. In this work we introduce the task\nContext-NER in which relevant context of an entity has to be generated. The\nextracted context may not be found exactly as a substring in the sentence. We\nalso introduce the EDGAR10-Q dataset for the same, which is a corpus of 1,500\npublicly traded companies. It is a manually created complex corpus and one of\nthe largest in terms of number of sentences and entities (1 M and 2.8 M). We\nintroduce a baseline approach that leverages phrase generation algorithms and\nuses the pre-trained BERT model to get 33% ROUGE-L score. We also do a one shot\nevaluation with GPT-3 and get 39% score, signifying the hardness and future\nscope of this task. We hope that addition of this dataset and our study will\npave the way for further research in this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1\">Shreyas Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_T/0/1/0/all/0/1\">Tamanna Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badugu_A/0/1/0/all/0/1\">Amogh Badugu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_H/0/1/0/all/0/1\">Himanshu Sharad Bhatt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Guided Exploitation of Interpretable Attention Patterns in Summarization and Topic Segmentation. (arXiv:2112.05364v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.05364","description":"<p>The multi-head self-attention mechanism of the transformer model has been\nthoroughly investigated recently. In one vein of study, researchers are\ninterested in understanding why and how transformers work. In another vein,\nresearchers propose new attention augmentation methods to make transformers\nmore accurate, efficient and interpretable. In this paper, we combine these two\nlines of research in a human-in-the-loop pipeline to first discover important\ntask-specific attention patterns. Then those patterns are injected, not only to\nsmaller models, but also to the original model. The benefits of our pipeline\nand discovered patterns are demonstrated in two case studies with extractive\nsummarization and topic segmentation. After discovering interpretable patterns\nin BERT-based models fine-tuned for the two downstream tasks, experiments\nindicate that when we inject the patterns into attention heads, the models show\nconsiderable improvements in accuracy and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Raymond Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wen Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Linzi Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lanjun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_G/0/1/0/all/0/1\">Gabriel Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1\">Giuseppe Carenini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"First is Better Than Last for Language Data Influence. (arXiv:2202.11844v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.11844","description":"<p>The ability to identify influential training examples enables us to debug\ntraining data and explain model behavior. Existing techniques to do so are\nbased on the flow of training data influence through the model parameters. For\nlarge models in NLP applications, it is often computationally infeasible to\nstudy this flow through all model parameters, therefore techniques usually pick\nthe last layer of weights. However, we observe that since the activation\nconnected to the last layer of weights contains \"shared logic\", the data\ninfluenced calculated via the last layer weights prone to a ``cancellation\neffect'', where the data influence of different examples have large magnitude\nthat contradicts each other. The cancellation effect lowers the discriminative\npower of the influence score, and deleting influential examples according to\nthis measure often does not change the model's behavior by much. To mitigate\nthis, we propose a technique called TracIn-WE that modifies a method called\nTracIn to operate on the word embedding layer instead of the last layer, where\nthe cancellation effect is less severe. One potential concern is that influence\nbased on the word embedding layer may not encode sufficient high level\ninformation. However, we find that gradients (unlike embeddings) do not suffer\nfrom this, possibly because they chain through higher layers. We show that\nTracIn-WE significantly outperforms other data influence methods applied on the\nlast layer significantly on the case deletion evaluation on three language\nclassification tasks for different models. In addition, TracIn-WE can produce\nscores not just at the level of the overall training input, but also at the\nlevel of words within the training input, a further aid in debugging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1\">Chih-Kuan Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taly_A/0/1/0/all/0/1\">Ankur Taly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundararajan_M/0/1/0/all/0/1\">Mukund Sundararajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Frederick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1\">Pradeep Ravikumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IndicNLG Benchmark: Multilingual Datasets for Diverse NLG Tasks in Indic Languages. (arXiv:2203.05437v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.05437","description":"<p>Natural Language Generation (NLG) for non-English languages is hampered by\nthe scarcity of datasets in these languages. In this paper, we present the\nIndicNLG Benchmark, a collection of datasets for benchmarking NLG for 11 Indic\nlanguages. We focus on five diverse tasks, namely, biography generation using\nWikipedia infoboxes, news headline generation, sentence summarization,\nparaphrase generation and, question generation. We describe the created\ndatasets and use them to benchmark the performance of several monolingual and\nmultilingual baselines that leverage pre-trained sequence-to-sequence models.\nOur results exhibit the strong performance of multilingual language-specific\npre-trained models, and the utility of models trained on our dataset for other\nrelated NLG tasks. Our dataset creation methods can be easily applied to\nmodest-resource languages as they involve simple steps such as scraping news\narticles and Wikipedia infoboxes, light cleaning, and pivoting through machine\ntranslation data. To the best of our knowledge, the IndicNLG Benchmark is the\nfirst NLG benchmark for Indic languages and the most diverse multilingual NLG\ndataset, with approximately 8M examples across 5 tasks and 11 languages. The\ndatasets and models are publicly available at\nhttps://ai4bharat.iitm.ac.in/indicnlg-suite.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aman Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrotriya_H/0/1/0/all/0/1\">Himani Shrotriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahu_P/0/1/0/all/0/1\">Prachi Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puduppully_R/0/1/0/all/0/1\">Ratish Puduppully</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1\">Amogh Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reproducibility Issues for BERT-based Evaluation Metrics. (arXiv:2204.00004v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.00004","description":"<p>Reproducibility is of utmost concern in machine learning and natural language\nprocessing (NLP). In the field of natural language generation (especially\nmachine translation), the seminal paper of Post (2018) has pointed out problems\nof reproducibility of the dominant metric, BLEU, at the time of publication.\nNowadays, BERT-based evaluation metrics considerably outperform BLEU. In this\npaper, we ask whether results and claims from four recent BERT-based metrics\ncan be reproduced. We find that reproduction of claims and results often fails\nbecause of (i) heavy undocumented preprocessing involved in the metrics, (ii)\nmissing code and (iii) reporting weaker results for the baseline metrics. (iv)\nIn one case, the problem stems from correlating not to human scores but to a\nwrong column in the csv file, inflating scores by 5 points. Motivated by the\nimpact of preprocessing, we then conduct a second study where we examine its\neffects more closely (for one of the metrics). We find that preprocessing can\nhave large effects, especially for highly inflectional languages. In this case,\nthe effect of preprocessing may be larger than the effect of the aggregation\nmechanism (e.g., greedy alignment vs. Word Mover Distance).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belouadi_J/0/1/0/all/0/1\">Jonas Belouadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Noise Control for Multispeaker Speech Synthesis. (arXiv:2204.05070v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2204.05070","description":"<p>A text-to-speech (TTS) model typically factorizes speech attributes such as\ncontent, speaker and prosody into disentangled representations.Recent works aim\nto additionally model the acoustic conditions explicitly, in order to\ndisentangle the primary speech factors, i.e. linguistic content, prosody and\ntimbre from any residual factors, such as recording conditions and background\nnoise.This paper proposes unsupervised, interpretable and fine-grained noise\nand prosody modeling. We incorporate adversarial training, representation\nbottleneck and utterance-to-frame modeling in order to learn frame-level noise\nrepresentations. To the same end, we perform fine-grained prosody modeling via\na Fully Hierarchical Variational AutoEncoder (FVAE) which additionally results\nin more expressive speech synthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nikitaras_K/0/1/0/all/0/1\">Karolos Nikitaras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vamvoukakis_G/0/1/0/all/0/1\">Georgios Vamvoukakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellinas_N/0/1/0/all/0/1\">Nikolaos Ellinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klapsas_K/0/1/0/all/0/1\">Konstantinos Klapsas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markopoulos_K/0/1/0/all/0/1\">Konstantinos Markopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raptis_S/0/1/0/all/0/1\">Spyros Raptis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_J/0/1/0/all/0/1\">June Sig Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jho_G/0/1/0/all/0/1\">Gunu Jho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalamandaris_A/0/1/0/all/0/1\">Aimilios Chalamandaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsiakoulis_P/0/1/0/all/0/1\">Pirros Tsiakoulis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Just Fine-tune Twice: Selective Differential Privacy for Large Language Models. (arXiv:2204.07667v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07667","description":"<p>Protecting large language models from privacy leakage is becoming\nincreasingly crucial with their wide adoption in real-world products. Yet\napplying differential privacy (DP), a canonical notion with provable privacy\nguarantees for machine learning models, to those models remains challenging due\nto the trade-off between model utility and privacy loss. Utilizing the fact\nthat sensitive information in language data tends to be sparse, Shi et al.\n(2021) formalized a DP notion extension called Selective Differential Privacy\n(SDP) to protect only the sensitive tokens defined by a policy function.\nHowever, their algorithm only works for RNN-based models. In this paper, we\ndevelop a novel framework, Just Fine-tune Twice (JFT), that achieves SDP for\nstate-of-the-art large transformer-based models. Our method is easy to\nimplement: it first fine-tunes the model with redacted in-domain data, and then\nfine-tunes it again with the original in-domain data using a private training\nmechanism. Furthermore, we study the scenario of imperfect implementation of\npolicy functions that misses sensitive tokens and develop systematic methods to\nhandle it. Experiments show that our method achieves strong utility compared to\nprevious baselines. We also analyze the SDP privacy guarantee empirically with\nthe canary insertion attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weiyan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shea_R/0/1/0/all/0/1\">Ryan Shea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Speech Representation Learning: A Review. (arXiv:2205.10643v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10643","description":"<p>Although supervised deep learning has revolutionized speech and audio\nprocessing, it has necessitated the building of specialist models for\nindividual tasks and application scenarios. It is likewise difficult to apply\nthis to dialects and languages for which only limited labeled data is\navailable. Self-supervised representation learning methods promise a single\nuniversal model that would benefit a wide variety of tasks and domains. Such\nmethods have shown success in natural language processing and computer vision\ndomains, achieving new levels of performance while reducing the number of\nlabels required for many downstream scenarios. Speech representation learning\nis experiencing similar progress in three main categories: generative,\ncontrastive, and predictive methods. Other approaches rely on multi-modal data\nfor pre-training, mixing text or visual data streams with speech. Although\nself-supervised speech representation is still a nascent research area, it is\nclosely related to acoustic word embedding and learning with zero lexical\nresources, both of which have seen active research for many years. This review\npresents approaches for self-supervised speech representation learning and\ntheir connection to other research areas. Since many current methods focus\nsolely on automatic speech recognition as a downstream task, we review recent\nefforts on benchmarking learned representations to extend the application\nbeyond speech recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgholt_L/0/1/0/all/0/1\">Lasse Borgholt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Havtorn_J/0/1/0/all/0/1\">Jakob D. Havtorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edin_J/0/1/0/all/0/1\">Joakim Edin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igel_C/0/1/0/all/0/1\">Christian Igel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1\">Katrin Kirchhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maaloe_L/0/1/0/all/0/1\">Lars Maal&#xf8;e</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is a Question Decomposition Unit All We Need?. (arXiv:2205.12538v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12538","description":"<p>Large Language Models (LMs) have achieved state-of-the-art performance on\nmany Natural Language Processing (NLP) benchmarks. With the growing number of\nnew benchmarks, we build bigger and more complex LMs. However, building new LMs\nmay not be an ideal option owing to the cost, time and environmental impact\nassociated with it. We explore an alternative route: can we modify data by\nexpressing it in terms of the model's strengths, so that a question becomes\neasier for models to answer? We investigate if humans can decompose a hard\nquestion into a set of simpler questions that are relatively easier for models\nto solve. We analyze a range of datasets involving various forms of reasoning\nand find that it is indeed possible to significantly improve model performance\n(24% for GPT3 and 29% for RoBERTa-SQuAD along with a symbolic calculator) via\ndecomposition. Our approach provides a viable option to involve people in NLP\nresearch in a meaningful way. Our findings indicate that Human-in-the-loop\nQuestion Decomposition (HQD) can potentially provide an alternate path to\nbuilding large LMs. Code and data is available at\nhttps://github.com/Pruthvi98/QuestionDecomposition\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_P/0/1/0/all/0/1\">Pruthvi Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Mihir Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting ELECTRA: Few-Shot Learning with Discriminative Pre-Trained Models. (arXiv:2205.15223v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.15223","description":"<p>Pre-trained masked language models successfully perform few-shot learning by\nformulating downstream tasks as text infilling. However, as a strong\nalternative in full-shot settings, discriminative pre-trained models like\nELECTRA do not fit into the paradigm. In this work, we adapt prompt-based\nfew-shot learning to ELECTRA and show that it outperforms masked language\nmodels in a wide range of tasks. ELECTRA is pre-trained to distinguish if a\ntoken is generated or original. We naturally extend that to prompt-based\nfew-shot learning by training to score the originality of the target options\nwithout introducing new parameters. Our method can be easily adapted to tasks\ninvolving multi-token predictions without extra computation overhead. Analysis\nshows that ELECTRA learns distributions that align better with downstream\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1\">Mengzhou Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jingfei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1\">Ves Stoyanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks. (arXiv:2206.06565v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.06565","description":"<p>Fine-tuning pretrained language models (LMs) without making any architectural\nchanges has become a norm for learning various language downstream tasks.\nHowever, for non-language downstream tasks, a common practice is to employ\ntask-specific designs for input, output layers, and loss functions. For\ninstance, it is possible to fine-tune an LM into an MNIST classifier by\nreplacing the word embedding layer with an image patch embedding layer, the\nword token output layer with a 10-way output layer, and the word prediction\nloss with a 10-way classification loss, respectively. A natural question\narises: Can LM fine-tuning solve non-language downstream tasks without changing\nthe model architecture or loss function? To answer this, we propose\nLanguage-Interfaced Fine-Tuning (LIFT) and study its efficacy and limitations\nby conducting an extensive empirical study on a suite of non-language\nclassification and regression tasks. LIFT does not make any changes to the\nmodel architecture or loss function, and it solely relies on the natural\nlanguage interface, enabling \"no-code machine learning with LMs.\" We find that\nLIFT performs comparably well across a wide range of low-dimensional\nclassification and regression tasks, matching the performances of the best\nbaselines in many cases, especially for the classification tasks. We also\nreport experimental results on the fundamental properties of LIFT, including\ninductive bias, robustness, and sample complexity. We also analyze the effect\nof pretraining on LIFT and a few properties/techniques specific to LIFT, e.g.,\ncontext-aware learning via appropriate prompting, calibrated predictions, data\ngeneration, and two-stage fine-tuning. Our code is available at\nhttps://github.com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1\">Tuan Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yuchen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruisu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Ziqian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gira_M/0/1/0/all/0/1\">Michael Gira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajput_S/0/1/0/all/0/1\">Shashank Rajput</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_J/0/1/0/all/0/1\">Jy-yong Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1\">Dimitris Papailiopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kangwook Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding the Properties of Generated Corpora. (arXiv:2206.11219v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.11219","description":"<p>Models for text generation have become focal for many research tasks and\nespecially for the generation of sentence corpora. However, understanding the\nproperties of an automatically generated text corpus remains challenging. We\npropose a set of tools that examine the properties of generated text corpora.\nApplying these tools on various generated corpora allowed us to gain new\ninsights into the properties of the generative models. As part of our\ncharacterization process, we found remarkable differences in the corpora\ngenerated by two leading generative technologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zwerdling_N/0/1/0/all/0/1\">Naama Zwerdling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlomov_S/0/1/0/all/0/1\">Segev Shlomov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldbraich_E/0/1/0/all/0/1\">Esther Goldbraich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kour_G/0/1/0/all/0/1\">George Kour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmeli_B/0/1/0/all/0/1\">Boaz Carmeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tepper_N/0/1/0/all/0/1\">Naama Tepper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ronen_I/0/1/0/all/0/1\">Inbal Ronen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zabershinsky_V/0/1/0/all/0/1\">Vitaly Zabershinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anaby_Tavor_A/0/1/0/all/0/1\">Ateret Anaby-Tavor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Similarity is More Valuable than Character Similarity: An Empirical Study for Chinese Spell Checking. (arXiv:2207.09217v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.09217","description":"<p>Chinese Spell Checking (CSC) task aims to detect and correct Chinese spelling\nerrors. In recent years, related researches focus on introducing the character\nsimilarity from confusion set to enhance the CSC models, ignoring the context\nof characters that contain richer information. To make better use of contextual\nsimilarity, we propose a simple yet effective curriculum learning framework for\nthe CSC task. With the help of our designed model-agnostic framework, existing\nCSC models will be trained from easy to difficult as humans learn Chinese\ncharacters and achieve further performance improvements. Extensive experiments\nand detailed analyses on widely used SIGHAN datasets show that our method\noutperforms previous state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Ding Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shirong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReFactor GNNs: Revisiting Factorisation-based Models from a Message-Passing Perspective. (arXiv:2207.09980v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2207.09980","description":"<p>Factorisation-based Models (FMs), such as DistMult, have enjoyed enduring\nsuccess for Knowledge Graph Completion (KGC) tasks, often outperforming Graph\nNeural Networks (GNNs). However, unlike GNNs, FMs struggle to incorporate node\nfeatures and generalise to unseen nodes in inductive settings. Our work bridges\nthe gap between FMs and GNNs by proposing ReFactor GNNs. This new architecture\ndraws upon both modelling paradigms, which previously were largely thought of\nas disjoint. Concretely, using a message-passing formalism, we show how FMs can\nbe cast as GNNs by reformulating the gradient descent procedure as\nmessage-passing operations, which forms the basis of our ReFactor GNNs. Across\na multitude of well-established KGC benchmarks, our ReFactor GNNs achieve\ncomparable transductive performance to FMs, and state-of-the-art inductive\nperformance while using an order of magnitude fewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yihong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1\">Pushkar Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franceschi_L/0/1/0/all/0/1\">Luca Franceschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1\">Pasquale Minervini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Learning for Clinical Natural Language Processing Using Siamese Neural Networks. (arXiv:2208.14923v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.14923","description":"<p>Clinical Natural Language Processing (NLP) has become an emerging technology\nin healthcare that leverages a large amount of free-text data in electronic\nhealth records (EHRs) to improve patient care, support clinical decisions, and\nfacilitate clinical and translational science research. Recently, deep learning\nhas achieved state-of-the-art performance in many clinical NLP tasks. However,\ntraining deep learning models usually requires large annotated datasets, which\nare normally not publicly available and can be time-consuming to build in\nclinical domains. Working with smaller annotated datasets is typical in\nclinical NLP and therefore, ensuring that deep learning models perform well is\ncrucial for the models to be used in real-world applications. A widely adopted\napproach is fine-tuning existing Pre-trained Language Models (PLMs), but these\nattempts fall short when the training dataset contains only a few annotated\nsamples. Few-Shot Learning (FSL) has recently been investigated to tackle this\nproblem. Siamese Neural Network (SNN) has been widely utilized as an FSL\napproach in computer vision, but has not been studied well in NLP. Furthermore,\nthe literature on its applications in clinical domains is scarce. In this\npaper, we propose two SNN-based FSL approaches for clinical NLP, including\nPre-Trained SNN (PT-SNN) and SNN with Second-Order Embeddings (SOE-SNN). We\nevaluated the proposed approaches on two clinical tasks, namely clinical text\nclassification and clinical named entity recognition. We tested three few-shot\nsettings including 4-shot, 8-shot, and 16-shot learning. Both clinical NLP\ntasks were benchmarked using three PLMs, including BERT,BioBERT, and\nBioClinicalBERT. The experimental results verified the effectiveness of the\nproposed SNN-based FSL approaches in both NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oniani_D/0/1/0/all/0/1\">David Oniani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivarajkumar_S/0/1/0/all/0/1\">Sonish Sivarajkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanshan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Focused Study on Sequence Length for Dialogue Summarization. (arXiv:2209.11910v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.11910","description":"<p>Output length is critical to dialogue summarization systems. The dialogue\nsummary length is determined by multiple factors, including dialogue\ncomplexity, summary objective, and personal preferences. In this work, we\napproach dialogue summary length from three perspectives. First, we analyze the\nlength differences between existing models' outputs and the corresponding human\nreferences and find that summarization models tend to produce more verbose\nsummaries due to their pretraining objectives. Second, we identify salient\nfeatures for summary length prediction by comparing different model settings.\nThird, we experiment with a length-aware summarizer and show notable\nimprovement on existing models if summary length can be well incorporated.\nAnalysis and experiments are conducted on popular DialogSum and SAMSum datasets\nto validate our findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chengwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Construction and Applications of Billion-Scale Pre-trained Multimodal Business Knowledge Graph. (arXiv:2209.15214v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2209.15214","description":"<p>Business Knowledge Graphs (KGs) are important to many enterprises today,\nproviding factual knowledge and structured data that steer many products and\nmake them more intelligent. Despite their promising benefits, building business\nKG necessitates solving prohibitive issues of deficient structure and multiple\nmodalities. In this paper, we advance the understanding of the practical\nchallenges related to building KG in non-trivial real-world systems. We\nintroduce the process of building an open business knowledge graph (OpenBG)\nderived from a well-known enterprise, Alibaba Group. Specifically, we define a\ncore ontology to cover various abstract products and consumption demands, with\nfine-grained taxonomy and multimodal facts in deployed applications. OpenBG is\nan open business KG of unprecedented scale: 2.6 billion triples with more than\n88 million entities covering over 1 million core classes/concepts and 2,681\ntypes of relations. We release all the open resources (OpenBG benchmarks)\nderived from it for the community and report experimental results of KG-centric\ntasks. We also run up an online competition based on OpenBG benchmarks, and has\nattracted thousands of teams. We further pre-train OpenBG and apply it to many\nKG- enhanced downstream tasks in business scenarios, demonstrating the\neffectiveness of billion-scale multimodal knowledge for e-commerce. All the\nresources with codes have been released at\n\\url{https://github.com/OpenBGBenchmark/OpenBG}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zelin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hehong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment. (arXiv:2210.01478v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.01478","description":"<p>AI systems are becoming increasingly intertwined with human life. In order to\neffectively collaborate with humans and ensure safety, AI systems need to be\nable to understand, interpret and predict human moral judgments and decisions.\nHuman moral judgments are often guided by rules, but not always. A central\nchallenge for AI safety is capturing the flexibility of the human moral mind --\nthe ability to determine when a rule should be broken, especially in novel or\nunusual situations. In this paper, we present a novel challenge set consisting\nof rule-breaking question answering (RBQA) of cases that involve potentially\npermissible rule-breaking -- inspired by recent moral psychology studies. Using\na state-of-the-art large language model (LLM) as a basis, we propose a novel\nmoral chain of thought (MORALCOT) prompting strategy that combines the\nstrengths of LLMs with theories of moral reasoning developed in cognitive\nscience to predict human moral judgments. MORALCOT outperforms seven existing\nLLMs by 6.2% F1, suggesting that modeling human reasoning might be necessary to\ncapture the flexibility of the human moral mind. We also conduct a detailed\nerror analysis to suggest directions for future work to improve AI safety using\nRBQA. Our data is open-sourced at\nhttps://huggingface.co/datasets/feradauto/MoralExceptQA and code at\nhttps://github.com/feradauto/MoralCoT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sydney Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_F/0/1/0/all/0/1\">Fernando Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamal_O/0/1/0/all/0/1\">Ojasv Kamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Josh Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PQLM -- Multilingual Decentralized Portable Quantum Language Model for Privacy Protection. (arXiv:2210.03221v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.03221","description":"<p>With careful manipulation, malicious agents can reverse engineer private\ninformation encoded in pre-trained language models. Security concerns motivate\nthe development of quantum pre-training. In this work, we propose a highly\nportable quantum language model (PQLM) that can easily transmit information to\ndownstream tasks on classical machines. The framework consists of a cloud PQLM\nbuilt with random Variational Quantum Classifiers (VQC) and local models for\ndownstream applications. We demonstrate the ad hoc portability of the quantum\nmodel by extracting only the word embeddings and effectively applying them to\ndownstream tasks on classical machines. Our PQLM exhibits comparable\nperformance to its classical counterpart on both intrinsic evaluation (loss,\nperplexity) and extrinsic evaluation (multilingual sentiment analysis accuracy)\nmetrics. We also perform ablation studies on the factors affecting PQLM\nperformance to analyze model stability. Our work establishes a theoretical\nfoundation for a portable quantum pre-trained language model that could be\ntrained on private data and made available for public use with privacy\nprotection guarantees.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuyue Stella Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_H/0/1/0/all/0/1\">Hongchao Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1\">Ruixing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hexin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_L/0/1/0/all/0/1\">Leibny Paola Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enriching Biomedical Knowledge for Vietnamese Low-resource Language Through Large-Scale Translation. (arXiv:2210.05598v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05598","description":"<p>Biomedical data and benchmarks are highly valuable yet very limited in\nlow-resource languages other than English such as Vietnamese. In this paper, we\nmake use of a state-of-the-art translation model in English-Vietnamese to\ntranslate and produce both pretrained as well as supervised data in the\nbiomedical domains. Thanks to such large-scale translation, we introduce\nViPubmedT5, a pretrained Encoder-Decoder Transformer model trained on 20\nmillion translated abstracts from the high-quality public PubMed corpus.\nViPubMedT5 demonstrates state-of-the-art results on two different biomedical\nbenchmarks in summarization and acronym disambiguation. Further, we release\nViMedNLI - a new NLP task in Vietnamese translated from MedNLI using the\nrecently public En-vi translation model and carefully refined by human experts,\nwith evaluations of existing methods against ViPubmedT5.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phan_L/0/1/0/all/0/1\">Long Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1\">Tai Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hieu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_V/0/1/0/all/0/1\">Vy Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_L/0/1/0/all/0/1\">Lam D. Chau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trinh_T/0/1/0/all/0/1\">Trieu H. Trinh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HuBERT-TR: Reviving Turkish Automatic Speech Recognition with Self-supervised Speech Representation Learning. (arXiv:2210.07323v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07323","description":"<p>While the Turkish language is listed among low-resource languages, literature\non Turkish automatic speech recognition (ASR) is relatively old. In this paper,\nwe present HuBERT-TR, a speech representation model for Turkish, based on\nHuBERT. HuBERT-TR achieves state-of-the-art results on several Turkish ASR\ndatasets. We investigate pre-training HuBERT for Turkish with large-scale data\ncurated from online resources. We pre-train HuBERT-TR using over 6,500 hours of\nspeech data curated from YouTube that includes extensive variability in terms\nof quality and genre. We show that language-specific models are superior to\nother pre-trained models, where our Turkish model HuBERT-TR/base performs\nbetter than the x10 times larger state-of-the-art multilingual XLS-R-1b model\nin low-resource settings. Moreover, we study the effect of scaling on ASR\nperformance by scaling our models up to 1B parameters. Our best model yields a\nstate-of-the-art word error rate of 4.97% on the Turkish Broadcast News\ndataset. Models are available at https://huggingface.co/asafaya\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Safaya_A/0/1/0/all/0/1\">Ali Safaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erzin_E/0/1/0/all/0/1\">Engin Erzin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Schema-aware Reference as Prompt Improves Data-Efficient Relational Triple and Event Extraction. (arXiv:2210.10709v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10709","description":"<p>Information Extraction, which aims to extract structural relational triple or\nevent from unstructured texts, often suffers from data scarcity issues. With\nthe development of pre-trained language models, many prompt-based approaches to\ndata-efficient information extraction have been proposed and achieved\nimpressive performance. However, existing prompt learning methods for\ninformation extraction are still susceptible to several potential limitations:\n(i) semantic gap between natural language and output structure knowledge with\npre-defined schema; (ii) representation learning with locally individual\ninstances limits the performance given the insufficient features. In this\npaper, we propose a novel approach of schema-aware Reference As Prompt (RAP),\nwhich dynamically leverage schema and knowledge inherited from global\n(few-shot) training data for each sample. Specifically, we propose a\nschema-aware reference store, which unifies symbolic schema and relevant\ntextual instances. Then, we employ a dynamic reference integration module to\nretrieve pertinent knowledge from the datastore as prompts during training and\ninference. Experimental results demonstrate that RAP can be plugged into\nvarious existing models and outperforms baselines in low-resource settings on\nfour datasets of relational triple extraction and event extraction. In\naddition, we provide comprehensive empirical ablations and case analysis\nregarding different types and scales of knowledge in order to better understand\nthe mechanisms of RAP. Code is available in https://github.com/zjunlp/RAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shengyu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model Pre-Training with Sparse Latent Typing. (arXiv:2210.12582v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12582","description":"<p>Modern large-scale Pre-trained Language Models (PLMs) have achieved\ntremendous success on a wide range of downstream tasks. However, most of the LM\npre-training objectives only focus on text reconstruction, but have not sought\nto learn latent-level interpretable representations of sentences. In this\npaper, we manage to push the language models to obtain a deeper understanding\nof sentences by proposing a new pre-training objective, Sparse Latent Typing,\nwhich enables the model to sparsely extract sentence-level keywords with\ndiverse latent types. Experimental results show that our model is able to learn\ninterpretable latent type categories in a self-supervised manner without using\nany external knowledge. Besides, the language model pre-trained with such an\nobjective also significantly improves Information Extraction related downstream\ntasks in both supervised and few-shot settings. Our code is publicly available\nat: https://github.com/renll/SparseLT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Liliang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zixuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Han Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voss_C/0/1/0/all/0/1\">Clare R. Voss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">Chengxiang Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focus Is What You Need For Chinese Grammatical Error Correction. (arXiv:2210.12692v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12692","description":"<p>Chinese Grammatical Error Correction (CGEC) aims to automatically detect and\ncorrect grammatical errors contained in Chinese text. In the long term,\nresearchers regard CGEC as a task with a certain degree of uncertainty, that\nis, an ungrammatical sentence may often have multiple references. However, we\nargue that even though this is a very reasonable hypothesis, it is too harsh\nfor the intelligence of the mainstream models in this era. In this paper, we\nfirst discover that multiple references do not actually bring positive gains to\nmodel training. On the contrary, it is beneficial to the CGEC model if the\nmodel can pay attention to small but essential data during the training\nprocess. Furthermore, we propose a simple yet effective training strategy\ncalled OneTarget to improve the focus ability of the CGEC models and thus\nimprove the CGEC performance. Extensive experiments and detailed analyses\ndemonstrate the correctness of our discovery and the effectiveness of our\nproposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jingheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shirong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Rui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Development of Hybrid ASR Systems for Low Resource Medical Domain Conversational Telephone Speech. (arXiv:2210.13397v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13397","description":"<p>Language barriers present a great challenge in our increasingly connected and\nglobal world. Especially within the medical domain, e.g. hospital or emergency\nroom, communication difficulties and delays may lead to malpractice and\nnon-optimal patient care. In the HYKIST project, we consider patient-physician\ncommunication, more specifically between a German-speaking physician and an\nArabic- or Vietnamese-speaking patient. Currently, a doctor can call the\nTriaphon service to get assistance from an interpreter in order to help\nfacilitate communication. The HYKIST goal is to support the usually\nnon-professional bilingual interpreter with an automatic speech translation\nsystem to improve patient care and help overcome language barriers. In this\nwork, we present our ASR system development efforts for this conversational\ntelephone speech translation task in the medical domain for two languages\npairs, data collection, various acoustic model architectures and\ndialect-induced difficulties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luscher_C/0/1/0/all/0/1\">Christoph L&#xfc;scher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1\">Mohammad Zeineldeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zijian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vieting_P/0/1/0/all/0/1\">Peter Vieting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Duc_K/0/1/0/all/0/1\">Khai Le-Duc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards standardizing Korean Grammatical Error Correction: Datasets and Annotation. (arXiv:2210.14389v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.14389","description":"<p>Research on Korean grammatical error correction (GEC) is limited compared to\nother major languages such as English and Chinese. We attribute this\nproblematic circumstance to the lack of a carefully designed evaluation\nbenchmark for Korean. Thus, in this work, we first collect three datasets from\ndifferent sources (Kor-Lang8, Kor-Native, and Kor-Learner) to cover a wide\nrange of error types and annotate them using our newly proposed tool called\nKorean Automatic Grammatical error Annotation System (KAGAS). KAGAS is a\ncarefully designed edit alignment &amp; classification tool that considers the\nnature of Korean on generating an alignment between a source sentence and a\ntarget sentence, and identifies error types on each aligned edit. We also\npresent baseline models fine-tuned over our datasets. We show that the model\ntrained with our datasets significantly outperforms the public statistical GEC\nsystem (Hanspell) on a wider range of error types, demonstrating the diversity\nand usefulness of the datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Soyoung Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungjoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyuwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Junhee Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kihyo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyu Tae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smart Speech Segmentation using Acousto-Linguistic Features with look-ahead. (arXiv:2210.14446v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.14446","description":"<p>Segmentation for continuous Automatic Speech Recognition (ASR) has\ntraditionally used silence timeouts or voice activity detectors (VADs), which\nare both limited to acoustic features. This segmentation is often overly\naggressive, given that people naturally pause to think as they speak.\nConsequently, segmentation happens mid-sentence, hindering both punctuation and\ndownstream tasks like machine translation for which high-quality segmentation\nis critical. Model-based segmentation methods that leverage acoustic features\nare powerful, but without an understanding of the language itself, these\napproaches are limited. We present a hybrid approach that leverages both\nacoustic and language information to improve segmentation. Furthermore, we show\nthat including one word as a look-ahead boosts segmentation quality. On\naverage, our models improve segmentation-F0.5 score by 9.8% over baseline. We\nshow that this approach works for multiple languages. For the downstream task\nof machine translation, it improves the translation BLEU score by an average of\n1.05 points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Behre_P/0/1/0/all/0/1\">Piyush Behre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parihar_N/0/1/0/all/0/1\">Naveen Parihar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Sharman Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Amy Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_E/0/1/0/all/0/1\">Eva Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Geoffrey Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuangyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalil_H/0/1/0/all/0/1\">Hosam Khalil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basoglu_C/0/1/0/all/0/1\">Chris Basoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_S/0/1/0/all/0/1\">Sayan Pathak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty Sentence Sampling by Virtual Adversarial Perturbation. (arXiv:2210.14576v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.14576","description":"<p>Active learning for sentence understanding attempts to reduce the annotation\ncost by identifying the most informative examples. Common methods for active\nlearning use either uncertainty or diversity sampling in the pool-based\nscenario. In this work, to incorporate both predictive uncertainty and sample\ndiversity, we propose Virtual Adversarial Perturbation for Active Learning\n(VAPAL) , an uncertainty-diversity combination framework, using virtual\nadversarial perturbation (Miyato et al., 2019) as model uncertainty\nrepresentation. VAPAL consistently performs equally well or even better than\nthe strong baselines on four sentence understanding datasets: AGNEWS, IMDB,\nPUBMED, and SST-2, offering a potential option for active learning on sentence\nunderstanding tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanshan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hongfei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Task: Deriving Semantic Class Targets for the Physical Sciences. (arXiv:2210.14760v2 [astro-ph.IM] UPDATED)","link":"http://arxiv.org/abs/2210.14760","description":"<p>We define deriving semantic class targets as a novel multi-modal task. By\ndoing so, we aim to improve classification schemes in the physical sciences\nwhich can be severely abstracted and obfuscating. We address this task for\nupcoming radio astronomy surveys and present the derived semantic radio galaxy\nmorphology class targets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Bowles_M/0/1/0/all/0/1\">Micah Bowles</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Tang_H/0/1/0/all/0/1\">Hongming Tang</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Vardoulaki_E/0/1/0/all/0/1\">Eleni Vardoulaki</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Alexander_E/0/1/0/all/0/1\">Emma L. Alexander</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Luo_Y/0/1/0/all/0/1\">Yan Luo</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Rudnick_L/0/1/0/all/0/1\">Lawrence Rudnick</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Walmsley_M/0/1/0/all/0/1\">Mike Walmsley</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Porter_F/0/1/0/all/0/1\">Fiona Porter</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Scaife_A/0/1/0/all/0/1\">Anna M. M. Scaife</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Slijepcevic_I/0/1/0/all/0/1\">Inigo Val Slijepcevic</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Segal_G/0/1/0/all/0/1\">Gary Segal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Semantic Parsing: From Images to Abstract Meaning Representation. (arXiv:2210.14862v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.14862","description":"<p>The success of scene graphs for visual scene understanding has brought\nattention to the benefits of abstracting a visual input (e.g., image) into a\nstructured representation, where entities (people and objects) are nodes\nconnected by edges specifying their relations. Building these representations,\nhowever, requires expensive manual annotation in the form of images paired with\ntheir scene graphs or frames. These formalisms remain limited in the nature of\nentities and relations they can capture. In this paper, we propose to leverage\na widely-used meaning representation in the field of natural language\nprocessing, the Abstract Meaning Representation (AMR), to address these\nshortcomings. Compared to scene graphs, which largely emphasize spatial\nrelationships, our visual AMR graphs are more linguistically informed, with a\nfocus on higher-level semantic concepts extrapolated from visual input.\nMoreover, they allow us to generate meta-AMR graphs to unify information\ncontained in multiple image descriptions under one representation. Through\nextensive experimentation and analysis, we demonstrate that we can re-purpose\nan existing text-to-AMR parser to parse images into AMRs. Our findings point to\nimportant future research directions for improved scene understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelsalam_M/0/1/0/all/0/1\">Mohamed Ashraf Abdelsalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fancellu_F/0/1/0/all/0/1\">Federico Fancellu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basioti_K/0/1/0/all/0/1\">Kalliopi Basioti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_D/0/1/0/all/0/1\">Dhaivat J. Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1\">Vladimir Pavlovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazly_A/0/1/0/all/0/1\">Afsaneh Fazly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-10-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}