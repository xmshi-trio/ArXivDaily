{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-08-28T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Financial News Analytics Using Fine-Tuned Llama 2 GPT Model. (arXiv:2308.13032v1 [cs.CL])","link":"http://arxiv.org/abs/2308.13032","description":"<p>The paper considers the possibility to fine-tune Llama 2 Large Language Model\n(LLM) for the multitask analysis of financial news. For fine-tuning, the\nPEFT/LoRA based approach was used. In the study, the model was fine-tuned for\nthe following tasks: analysing a text from financial market perspectives,\nhighlighting main points of a text, summarizing a text and extracting named\nentities with appropriate sentiments. The obtained results show that the\nfine-tuned Llama 2 model can perform a multitask financial news analysis with a\nspecified structure of response, part of response can be a structured text and\nanother part of data can have JSON format for further processing. Extracted\nsentiments for named entities can be considered as predictive features in\nsupervised machine learning models with quantitative target variables.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pavlyshenko_B/0/1/0/all/0/1\">Bohdan M. Pavlyshenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lexical Diversity in Kinship Across Languages and Dialects. (arXiv:2308.13056v1 [cs.CL])","link":"http://arxiv.org/abs/2308.13056","description":"<p>Languages are known to describe the world in diverse ways. Across lexicons,\ndiversity is pervasive, appearing through phenomena such as lexical gaps and\nuntranslatability. However, in computational resources, such as multilingual\nlexical databases, diversity is hardly ever represented. In this paper, we\nintroduce a method to enrich computational lexicons with content relating to\nlinguistic diversity. The method is verified through two large-scale case\nstudies on kinship terminology, a domain known to be diverse across languages\nand cultures: one case study deals with seven Arabic dialects, while the other\none with three Indonesian languages. Our results, made available as browseable\nand downloadable computational resources, extend prior linguistics research on\nkinship terminology, and provide insight into the extent of diversity even\nwithin linguistically and culturally close communities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalilia_H/0/1/0/all/0/1\">Hadi Khalilia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bella_G/0/1/0/all/0/1\">G&#xe1;bor Bella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freihat_A/0/1/0/all/0/1\">Abed Alhakim Freihat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darma_S/0/1/0/all/0/1\">Shandy Darma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1\">Fausto Giunchiglia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Parrots: Large Language Models May Talk Causality But Are Not Causal. (arXiv:2308.13067v1 [cs.AI])","link":"http://arxiv.org/abs/2308.13067","description":"<p>Some argue scale is all what is needed to achieve AI, covering even causal\nmodels. We make it clear that large language models (LLMs) cannot be causal and\ngive reason onto why sometimes we might feel otherwise. To this end, we define\nand exemplify a new subgroup of Structural Causal Model (SCM) that we call meta\nSCM which encode causal facts about other SCM within their variables. We\nconjecture that in the cases where LLM succeed in doing causal inference,\nunderlying was a respective meta SCM that exposed correlations between causal\nfacts in natural language on whose data the LLM was ultimately trained. If our\nhypothesis holds true, then this would imply that LLMs are like parrots in that\nthey simply recite the causal knowledge embedded in the data. Our empirical\nanalysis provides favoring evidence that current LLMs are even weak `causal\nparrots.'\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zecevic_M/0/1/0/all/0/1\">Matej Ze&#x10d;evi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willig_M/0/1/0/all/0/1\">Moritz Willig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhami_D/0/1/0/all/0/1\">Devendra Singh Dhami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Formal specification terminology for demographic agent-based models of fixed-step single-clocked simulations. (arXiv:2308.13081v1 [cs.CL])","link":"http://arxiv.org/abs/2308.13081","description":"<p>This document presents adequate formal terminology for the mathematical\nspecification of a subset of Agent Based Models (ABMs) in the field of\nDemography. The simulation of the targeted ABMs follows a fixed-step\nsingle-clocked pattern. The proposed terminology further improves the model\nunderstanding and can act as a stand-alone methodology for the specification\nand optionally the documentation of a significant set of (demographic) ABMs.\nNevertheless, it is imaginable the this terminology probably with further\nextensions can be merged with the largely-informal widely-used model\ndocumentation and communication O.D.D. protocol [Grimm and et al., 2020,\nAmouroux et al., 2010] to reduce many sources of ambiguity, hindering model\nreplications by other modelers. A published demographic model documentation,\nlargely simplified version of the Lone Parent Model [Gostoli and Silverman,\n2020] is separately published in [Elsheikh, 2023b] as illustration for the\nformal terminology. The model was implemented in the Julia language [Elsheikh,\n2023a] based on the Agents.jl julia package [Datseris et al., 2022].\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elsheikh_A/0/1/0/all/0/1\">Atiyah Elsheikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Holistic Approach: Understanding Sociodemographic Biases in NLP Models using an Interdisciplinary Lens. (arXiv:2308.13089v1 [cs.CL])","link":"http://arxiv.org/abs/2308.13089","description":"<p>The rapid growth in the usage and applications of Natural Language Processing\n(NLP) in various sociotechnical solutions has highlighted the need for a\ncomprehensive understanding of bias and its impact on society. While research\non bias in NLP has expanded, several challenges persist that require attention.\nThese include the limited focus on sociodemographic biases beyond race and\ngender, the narrow scope of analysis predominantly centered on models, and the\ntechnocentric implementation approaches. This paper addresses these challenges\nand advocates for a more interdisciplinary approach to understanding bias in\nNLP. The work is structured into three facets, each exploring a specific aspect\nof bias in NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venkit_P/0/1/0/all/0/1\">Pranav Narayanan Venkit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence Embedding Models for Ancient Greek Using Multilingual Knowledge Distillation. (arXiv:2308.13116v1 [cs.CL])","link":"http://arxiv.org/abs/2308.13116","description":"<p>Contextual language models have been trained on Classical languages,\nincluding Ancient Greek and Latin, for tasks such as lemmatization,\nmorphological tagging, part of speech tagging, authorship attribution, and\ndetection of scribal errors. However, high-quality sentence embedding models\nfor these historical languages are significantly more difficult to achieve due\nto the lack of training data. In this work, we use a multilingual knowledge\ndistillation approach to train BERT models to produce sentence embeddings for\nAncient Greek text. The state-of-the-art sentence embedding approaches for\nhigh-resource languages use massive datasets, but our distillation approach\nallows our Ancient Greek models to inherit the properties of these models while\nusing a relatively small amount of translated sentence data. We build a\nparallel sentence dataset using a sentence-embedding alignment method to align\nAncient Greek documents with English translations, and use this dataset to\ntrain our models. We evaluate our models on translation search, semantic\nsimilarity, and semantic retrieval tasks and investigate translation bias. We\nmake our training and evaluation datasets freely available at\nhttps://github.com/kevinkrahn/ancient-greek-datasets .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krahn_K/0/1/0/all/0/1\">Kevin Krahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tate_D/0/1/0/all/0/1\">Derrick Tate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamicela_A/0/1/0/all/0/1\">Andrew C. Lamicela</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models. (arXiv:2308.13137v1 [cs.LG])","link":"http://arxiv.org/abs/2308.13137","description":"<p>Large language models (LLMs) have revolutionized natural language processing\ntasks. However, their practical deployment is hindered by their immense memory\nand computation requirements. Although recent post-training quantization (PTQ)\nmethods are effective in reducing memory footprint and improving the\ncomputational efficiency of LLM, they hand-craft quantization parameters, which\nleads to low performance and fails to deal with extremely low-bit quantization.\nTo tackle this issue, we introduce an Omnidirectionally calibrated Quantization\n(OmniQuant) technique for LLMs, which achieves good performance in diverse\nquantization settings while maintaining the computational efficiency of PTQ by\nefficiently optimizing various quantization parameters. OmniQuant comprises two\ninnovative components including Learnable Weight Clipping (LWC) and Learnable\nEquivalent Transformation (LET). LWC modulates the extreme values of weights by\noptimizing the clipping threshold. Meanwhile, LET tackles activation outliers\nby shifting the challenge of quantization from activations to weights through a\nlearnable equivalent transformation. Operating within a differentiable\nframework using block-wise error minimization, OmniQuant can optimize the\nquantization process efficiently for both weight-only and weight-activation\nquantization. For instance, the LLaMA-2 model family with the size of 7-70B can\nbe processed with OmniQuant on a single A100-40G GPU within 1-16 hours using\n128 samples. Extensive experiments validate OmniQuant's superior performance\nacross diverse quantization configurations such as W4A4, W6A6, W4A16, W3A16,\nand W2A16. Additionally, OmniQuant demonstrates effectiveness in\ninstruction-tuned models and delivers notable improvements in inference speed\nand memory reduction on real devices. Codes and models are available at\n\\url{https://github.com/OpenGVLab/OmniQuant}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wenqi Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mengzhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lirui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MatchXML: An Efficient Text-label Matching Framework for Extreme Multi-label Text Classification. (arXiv:2308.13139v1 [cs.CL])","link":"http://arxiv.org/abs/2308.13139","description":"<p>The eXtreme Multi-label text Classification(XMC) refers to training a\nclassifier that assigns a text sample with relevant labels from an extremely\nlarge-scale label set (e.g., millions of labels). We propose MatchXML, an\nefficient text-label matching framework for XMC. We observe that the label\nembeddings generated from the sparse Term Frequency-Inverse Document\nFrequency(TF-IDF) features have several limitations. We thus propose label2vec\nto effectively train the semantic dense label embeddings by the Skip-gram\nmodel. The dense label embeddings are then used to build a Hierarchical Label\nTree by clustering. In fine-tuning the pre-trained encoder Transformer, we\nformulate the multi-label text classification as a text-label matching problem\nin a bipartite graph. We then extract the dense text representations from the\nfine-tuned Transformer. Besides the fine-tuned dense text embeddings, we also\nextract the static dense sentence embeddings from a pre-trained Sentence\nTransformer. Finally, a linear ranker is trained by utilizing the sparse TF-IDF\nfeatures, the fine-tuned dense text representations and static dense sentence\nfeatures. Experimental results demonstrate that MatchXML achieves\nstate-of-the-art accuracy on five out of six datasets. As for the speed,\nMatchXML outperforms the competing methods on all the six datasets. Our source\ncode is publicly available at https://github.com/huiyegit/MatchXML.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hui Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunderraman_R/0/1/0/all/0/1\">Rajshekhar Sunderraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shihao Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research. (arXiv:2308.13149v1 [cs.CL])","link":"http://arxiv.org/abs/2308.13149","description":"<p>Recently, there has been growing interest in using Large Language Models\n(LLMs) for scientific research. Numerous benchmarks have been proposed to\nevaluate the ability of LLMs for scientific research. However, current\nbenchmarks are mostly based on pre-collected objective questions. This design\nsuffers from data leakage problem and lacks the evaluation of subjective Q/A\nability. In this paper, we propose SciEval, a comprehensive and\nmulti-disciplinary evaluation benchmark to address these issues. Based on\nBloom's taxonomy, SciEval covers four dimensions to systematically evaluate\nscientific research ability. In particular, we design a \"dynamic\" subset based\non scientific principles to prevent evaluation from potential data leakage.\nBoth objective and subjective questions are included in SciEval. These\ncharacteristics make SciEval a more effective benchmark for scientific research\nability evaluation of LLMs. Comprehensive experiments on most advanced LLMs\nshow that, although GPT-4 achieves SOTA performance compared to other LLMs,\nthere is still substantial room for improvement, especially for dynamic\nquestions. The data and codes are now publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Liangtai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zihan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1\">Da Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhennan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baocai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Kai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Spurious Correlation in Classification: 'Clever Hans' in Translationese. (arXiv:2308.13170v1 [cs.CL])","link":"http://arxiv.org/abs/2308.13170","description":"<p>Recent work has shown evidence of 'Clever Hans' behavior in high-performance\nneural translationese classifiers, where BERT-based classifiers capitalize on\nspurious correlations, in particular topic information, between data and target\nclassification labels, rather than genuine translationese signals.\nTranslationese signals are subtle (especially for professional translation) and\ncompete with many other signals in the data such as genre, style, author, and,\nin particular, topic. This raises the general question of how much of the\nperformance of a classifier is really due to spurious correlations in the data\nversus the signals actually targeted for by the classifier, especially for\nsubtle target signals and in challenging (low resource) data settings. We focus\non topic-based spurious correlation and approach the question from two\ndirections: (i) where we have no knowledge about spurious topic information and\nits distribution in the data, (ii) where we have some indication about the\nnature of spurious topic correlations. For (i) we develop a measure from first\nprinciples capturing alignment of unsupervised topics with target\nclassification labels as an indication of spurious topic information in the\ndata. We show that our measure is the same as purity in clustering and propose\na 'topic floor' (as in a 'noise floor') for classification. For (ii) we\ninvestigate masking of known spurious topic carriers in classification. Both\n(i) and (ii) contribute to quantifying and (ii) to mitigating spurious\ncorrelations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borah_A/0/1/0/all/0/1\">Angana Borah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pylypenko_D/0/1/0/all/0/1\">Daria Pylypenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espana_Bonet_C/0/1/0/all/0/1\">Cristina Espana-Bonet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genabith_J/0/1/0/all/0/1\">Josef van Genabith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DISGO: Automatic End-to-End Evaluation for Scene Text OCR. (arXiv:2308.13173v1 [cs.CV])","link":"http://arxiv.org/abs/2308.13173","description":"<p>This paper discusses the challenges of optical character recognition (OCR) on\nnatural scenes, which is harder than OCR on documents due to the wild content\nand various image backgrounds. We propose to uniformly use word error rates\n(WER) as a new measurement for evaluating scene-text OCR, both end-to-end (e2e)\nperformance and individual system component performances. Particularly for the\ne2e metric, we name it DISGO WER as it considers Deletion, Insertion,\nSubstitution, and Grouping/Ordering errors. Finally we propose to utilize the\nconcept of super blocks to automatically compute BLEU scores for e2e OCR\nmachine translation. The small SCUT public test set is used to demonstrate WER\nperformance by a modularized OCR system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_M/0/1/0/all/0/1\">Mei-Yuh Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yangyang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramchandani_A/0/1/0/all/0/1\">Ankit Ramchandani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guan Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_P/0/1/0/all/0/1\">Praveen Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabela_L/0/1/0/all/0/1\">Lucas Kabela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seide_F/0/1/0/all/0/1\">Frank Seide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_S/0/1/0/all/0/1\">Samyak Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Evaluate the Generalization of Detection? A Benchmark for Comprehensive Open-Vocabulary Detection. (arXiv:2308.13177v1 [cs.CV])","link":"http://arxiv.org/abs/2308.13177","description":"<p>Object detection (OD) in computer vision has made significant progress in\nrecent years, transitioning from closed-set labels to open-vocabulary detection\n(OVD) based on large-scale vision-language pre-training (VLP). However, current\nevaluation methods and datasets are limited to testing generalization over\nobject types and referral expressions, which do not provide a systematic,\nfine-grained, and accurate benchmark of OVD models' abilities. In this paper,\nwe propose a new benchmark named OVDEval, which includes 9 sub-tasks and\nintroduces evaluations on commonsense knowledge, attribute understanding,\nposition understanding, object relation comprehension, and more. The dataset is\nmeticulously created to provide hard negatives that challenge models' true\nunderstanding of visual and linguistic input. Additionally, we identify a\nproblem with the popular Average Precision (AP) metric when benchmarking models\non these fine-grained label datasets and propose a new metric called\nNon-Maximum Suppression Average Precision (NMS-AP) to address this issue.\nExtensive experimental results show that existing top OVD models all fail on\nthe new tasks except for simple object types, demonstrating the value of the\nproposed dataset in pinpointing the weakness of current OVD models and guiding\nfuture research. Furthermore, the proposed NMS-AP metric is verified by\nexperiments to provide a much more truthful evaluation of OVD models, whereas\ntraditional AP metrics yield deceptive results. Data is available at\n\\url{https://github.com/om-ai-lab/OVDEval}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yiyang Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiancheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianqian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jiajia Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chunxin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyusong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers. (arXiv:2308.13191v1 [cs.CL])","link":"http://arxiv.org/abs/2308.13191","description":"<p>Although dominant in natural language processing, transformer-based models\nremain challenged by the task of long-sequence processing, because the\ncomputational cost of self-attention operations in transformers swells\nquadratically with the input sequence length. To alleviate the complexity of\nlong-sequence processing, we propose a simple framework to enable the\noffthe-shelf pre-trained transformers to process much longer sequences, while\nthe computation and memory costs remain growing linearly with the input\nsequence lengths. More specifically, our method divides each long-sequence\ninput into a batch of chunks, then aligns the interchunk information during the\nencoding steps, and finally selects the most representative hidden states from\nthe encoder for the decoding process. To extract inter-chunk semantic\ninformation, we align the start and end token embeddings among chunks in each\nencoding transformer block. To learn an effective hidden selection policy, we\ndesign a dual updating scheme inspired by reinforcement learning, which regards\nthe decoders of transformers as environments, and the downstream performance\nmetrics as the rewards to evaluate the hidden selection actions. Our empirical\nresults on real-world long-text summarization and reading comprehension tasks\ndemonstrate effective improvements compared to prior longsequence processing\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiawen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pengyu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1\">Nan Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Formalising Natural Language Quantifiers for Human-Robot Interactions. (arXiv:2308.13192v1 [cs.AI])","link":"http://arxiv.org/abs/2308.13192","description":"<p>We present a method for formalising quantifiers in natural language in the\ncontext of human-robot interactions. The solution is based on first-order logic\nextended with capabilities to represent the cardinality of variables, operating\nsimilarly to generalised quantifiers. To demonstrate the method, we designed an\nend-to-end system able to receive input as natural language, convert it into a\nformal logical representation, evaluate it, and return a result or send a\ncommand to a simulated robot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Morar_S/0/1/0/all/0/1\">Stefan Morar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groza_A/0/1/0/all/0/1\">Adrian Groza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomarlan_M/0/1/0/all/0/1\">Mihai Pomarlan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Journey to the Center of the Knowledge Neurons: Discoveries of Language-Independent Knowledge Neurons and Degenerate Knowledge Neurons. (arXiv:2308.13198v1 [cs.CL])","link":"http://arxiv.org/abs/2308.13198","description":"<p>Pre-trained language models (PLMs) contain vast amounts of factual knowledge,\nbut how the knowledge is stored in the parameters remains unclear. This paper\ndelves into the complex task of understanding how factual knowledge is stored\nin multilingual PLMs, and introduces the Architecture-adapted Multilingual\nIntegrated Gradients method, which successfully localizes knowledge neurons\nmore precisely compared to current methods, and is more universal across\nvarious architectures and languages. Moreover, we conduct an in-depth\nexploration of knowledge neurons, leading to the following two important\ndiscoveries: (1) The discovery of Language-Independent Knowledge Neurons, which\nstore factual knowledge in a form that transcends language. We design\ncross-lingual knowledge editing experiments, demonstrating that the PLMs can\naccomplish this task based on language-independent neurons; (2) The discovery\nof Degenerate Knowledge Neurons, a novel type of neuron showing that different\nknowledge neurons can store the same fact. Its property of functional overlap\nendows the PLMs with a robust mastery of factual knowledge. We design\nfact-checking experiments, proving that the degenerate knowledge neurons can\nhelp the PLMs to detect wrong facts. Experiments corroborate these findings,\nshedding light on the mechanisms of factual knowledge storage in multilingual\nPLMs, and contribute valuable insights to the field. The source code will be\nmade publicly available for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1\">Pengfei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yubo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM2KB: Constructing Knowledge Bases using instruction tuned context aware Large Language Models. (arXiv:2308.13207v1 [cs.CL])","link":"http://arxiv.org/abs/2308.13207","description":"<p>The advent of Large Language Models (LLM) has revolutionized the field of\nnatural language processing, enabling significant progress in various\napplications. One key area of interest is the construction of Knowledge Bases\n(KB) using these powerful models. Knowledge bases serve as repositories of\nstructured information, facilitating information retrieval and inference tasks.\nOur paper proposes LLM2KB, a system for constructing knowledge bases using\nlarge language models, with a focus on the Llama 2 architecture and the\nWikipedia dataset. We perform parameter efficient instruction tuning for\nLlama-2-13b-chat and StableBeluga-13B by training small injection models that\nhave only 0.05 % of the parameters of the base models using the Low Rank\nAdaptation (LoRA) technique. These injection models have been trained with\nprompts that are engineered to utilize Wikipedia page contexts of subject\nentities fetched using a Dense Passage Retrieval (DPR) algorithm, to answer\nrelevant object entities for a given subject entity and relation. Our best\nperforming model achieved an average F1 score of 0.6185 across 21 relations in\nthe LM-KBC challenge held at the ISWC 2023 conference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_A/0/1/0/all/0/1\">Anmol Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timmapathini_H/0/1/0/all/0/1\">Hari Prasad Timmapathini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering. (arXiv:2308.13259v1 [cs.CL])","link":"http://arxiv.org/abs/2308.13259","description":"<p>Equipped with Chain-of-Thought (CoT), Large language models (LLMs) have shown\nimpressive reasoning ability in various downstream tasks. Even so, suffering\nfrom hallucinations and the inability to access external knowledge, LLMs often\ncome with incorrect or unfaithful intermediate reasoning steps, especially in\nthe context of answering knowledge-intensive tasks such as KBQA. To alleviate\nthis issue, we propose a framework called Knowledge-Driven Chain-of-Thought\n(KD-CoT) to verify and modify reasoning traces in CoT via interaction with\nexternal knowledge, and thus overcome the hallucinations and error propagation.\nConcretely, we formulate the CoT rationale process of LLMs into a structured\nmulti-round QA format. In each round, LLMs interact with a QA system that\nretrieves external knowledge and produce faithful reasoning traces based on\nretrieved precise answers. The structured CoT reasoning of LLMs is facilitated\nby our developed KBQA CoT collection, which serves as in-context learning\ndemonstrations and can also be utilized as feedback augmentation to train a\nrobust retriever. Extensive experiments on WebQSP and ComplexWebQuestion\ndatasets demonstrate the effectiveness of proposed KD-CoT in task-solving\nreasoning generation, which outperforms the vanilla CoT ICL with an absolute\nsuccess rate of 8.0% and 5.1%. Furthermore, our proposed feedback-augmented\nretriever outperforms the state-of-the-art baselines for retrieving knowledge,\nachieving significant improvement in Hit performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Keheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_F/0/1/0/all/0/1\">Feiyu Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peiguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1\">Yunsen Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Chuantao Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_W/0/1/0/all/0/1\">Wenge Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhang Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Construction Grammar and Language Models. (arXiv:2308.13315v1 [cs.CL])","link":"http://arxiv.org/abs/2308.13315","description":"<p>Recent progress in deep learning and natural language processing has given\nrise to powerful models that are primarily trained on a cloze-like task and\nshow some evidence of having access to substantial linguistic information,\nincluding some constructional knowledge. This groundbreaking discovery presents\nan exciting opportunity for a synergistic relationship between computational\nmethods and Construction Grammar research. In this chapter, we explore three\ndistinct approaches to the interplay between computational methods and\nConstruction Grammar: (i) computational methods for text analysis, (ii)\ncomputational Construction Grammar, and (iii) deep learning models, with a\nparticular focus on language models. We touch upon the first two approaches as\na contextual foundation for the use of computational methods before providing\nan accessible, yet comprehensive overview of deep learning models, which also\naddresses reservations construction grammarians may have. Additionally, we\ndelve into experiments that explore the emergence of constructionally relevant\ninformation within these models while also examining the aspects of\nConstruction Grammar that may pose challenges for these models. This chapter\naims to foster collaboration between researchers in the fields of natural\nlanguage processing and Construction Grammar. By doing so, we hope to pave the\nway for new insights and advancements in both these fields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madabushi_H/0/1/0/all/0/1\">Harish Tayyar Madabushi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romain_L/0/1/0/all/0/1\">Laurence Romain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milin_P/0/1/0/all/0/1\">Petar Milin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Divjak_D/0/1/0/all/0/1\">Dagmar Divjak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transforming the Output of Generative Pre-trained Transformer: The Influence of the PGI Framework on Attention Dynamics. (arXiv:2308.13317v1 [cs.AI])","link":"http://arxiv.org/abs/2308.13317","description":"<p>This paper presents a novel approach named Persona-Grouping-Intelligence\n(PGI), which has been crafted to tackle the challenges posed by GPT models when\napplied to real-world business issues. PGI leverages the inherent capabilities\nof the GPT model to comprehend intricate language structures and generate\nresponses that are contextually relevant. The experiment occurred in a business\nscenario where human intelligence was being underutilized due to less optimized\nbusiness processes. The primary objective of this approach is to leverage GPT\nmodels to reduce the workload on humans in tasks that are extensive,\nmonotonous, and repetitive. Instead, the focus is redirected toward\ndecision-making activities. Remarkably, the experiment yielded an accuracy rate\nof 93.81% in validating 4,000 responses generated by the model, underscoring\nthe effectiveness of the PGI strategies. Effectively addressing the issue of\nunderutilized human intelligence, this paradigm shift aligns business\nenvironments with dynamic machine intelligence, enabling them to navigate the\nintricacies of real-world challenges. This approach facilitates the practical\nutilization of these models to tackle actual problems. The methodology offers\nan opportunity to reshape the fundamental structure of business processes by\nseamlessly integrating human decision-making with adaptable machine\nintelligence. Consequently, this optimization enhances operational efficiency\nand elevates strategic decision-making across diverse business contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ioste_A/0/1/0/all/0/1\">Aline Ioste</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupled Structure for Improved Adaptability of End-to-End Models. (arXiv:2308.13345v1 [eess.AS])","link":"http://arxiv.org/abs/2308.13345","description":"<p>Although end-to-end (E2E) trainable automatic speech recognition (ASR) has\nshown great success by jointly learning acoustic and linguistic information, it\nstill suffers from the effect of domain shifts, thus limiting potential\napplications. The E2E ASR model implicitly learns an internal language model\n(LM) which characterises the training distribution of the source domain, and\nthe E2E trainable nature makes the internal LM difficult to adapt to the target\ndomain with text-only data To solve this problem, this paper proposes decoupled\nstructures for attention-based encoder-decoder (Decoupled-AED) and neural\ntransducer (Decoupled-Transducer) models, which can achieve flexible domain\nadaptation in both offline and online scenarios while maintaining robust\nintra-domain performance. To this end, the acoustic and linguistic parts of the\nE2E model decoder (or prediction network) are decoupled, making the linguistic\ncomponent (i.e. internal LM) replaceable. When encountering a domain shift, the\ninternal LM can be directly replaced during inference by a target-domain LM,\nwithout re-training or using domain-specific paired speech-text data.\nExperiments for E2E ASR models trained on the LibriSpeech-100h corpus showed\nthat the proposed decoupled structure gave 15.1% and 17.2% relative word error\nrate reductions on the TED-LIUM 2 and AESRC2020 corpora while still maintaining\nperformance on intra-domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Deng_K/0/1/0/all/0/1\">Keqi Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Impact of Language Selection for Training and Evaluating Programming Language Models. (arXiv:2308.13354v1 [cs.SE])","link":"http://arxiv.org/abs/2308.13354","description":"<p>The recent advancements in Transformer-based Language Models have\ndemonstrated significant potential in enhancing the multilingual capabilities\nof these models. The remarkable progress made in this domain not only applies\nto natural language tasks but also extends to the domain of programming\nlanguages. Despite the ability of these models to learn from multiple\nlanguages, evaluations typically focus on particular combinations of the same\nlanguages. In this study, we evaluate the similarity of programming languages\nby analyzing their representations using a CodeBERT-based model. Our\nexperiments reveal that token representation in languages such as C++, Python,\nand Java exhibit proximity to one another, whereas the same tokens in languages\nsuch as Mathematica and R display significant dissimilarity. Our findings\nsuggest that this phenomenon can potentially result in performance challenges\nwhen dealing with diverse languages. Thus, we recommend using our similarity\nmeasure to select a diverse set of programming languages when training and\nevaluating future models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katzy_J/0/1/0/all/0/1\">Jonathan Katzy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izadi_M/0/1/0/all/0/1\">Maliheh Izadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deursen_A/0/1/0/all/0/1\">Arie van Deursen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Keyness using Permutation Tests. (arXiv:2308.13383v1 [cs.CL])","link":"http://arxiv.org/abs/2308.13383","description":"<p>We propose a resampling-based approach for assessing keyness in corpus\nlinguistics based on suggestions by Gries (2006, 2022). Traditional approaches\nbased on hypothesis tests (e.g. Likelihood Ratio) model the copora as\nindependent identically distributed samples of tokens. This model does not\naccount for the often observed uneven distribution of occurences of a word\nacross a corpus. When occurences of a word are concentrated in few documents,\nlarge values of LLR and similar scores are in fact much more likely than\naccounted for by the token-by-token sampling model, leading to false positives.\n</p>\n<p>We replace the token-by-token sampling model by a model where corpora are\nsamples of documents rather than tokens, which is much closer to the way\ncorpora are actually assembled. We then use a permutation approach to\napproximate the distribution of a given keyness score under the null hypothesis\nof equal frequencies and obtain p-values for assessing significance. We do not\nneed any assumption on how the tokens are organized within or across documents,\nand the approach works with basically *any* keyness score. Hence, appart from\nobtaining more accurate p-values for scores like LLR, we can also assess\nsignificance for e.g. the logratio which has been proposed as a measure of\neffect size.\n</p>\n<p>An efficient implementation of the proposed approach is provided in the `R`\npackage `keyperm` available from github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mildenberger_T/0/1/0/all/0/1\">Thoralf Mildenberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs. (arXiv:2308.13387v1 [cs.CL])","link":"http://arxiv.org/abs/2308.13387","description":"<p>With the rapid evolution of large language models (LLMs), new and\nhard-to-predict harmful capabilities are emerging. This requires developers to\nbe able to identify risks through the evaluation of \"dangerous capabilities\" in\norder to responsibly deploy LLMs. In this work, we collect the first\nopen-source dataset to evaluate safeguards in LLMs, and deploy safer\nopen-source LLMs at a low cost. Our dataset is curated and filtered to consist\nonly of instructions that responsible language models should not follow. We\nannotate and assess the responses of six popular LLMs to these instructions.\nBased on our annotation, we proceed to train several BERT-like classifiers, and\nfind that these small classifiers can achieve results that are comparable with\nGPT-4 on automatic safety evaluation. Warning: this paper contains example data\nthat may be offensive, harmful, or biased.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haonan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xudong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text Compression. (arXiv:2308.13399v1 [cs.CL])","link":"http://arxiv.org/abs/2308.13399","description":"<p>We propose an unsupervised method to extract keywords and keyphrases from\ntexts based on a pre-trained language model (LM) and Shannon's information\nmaximization. Specifically, our method extracts phrases having the highest\nconditional entropy under the LM. The resulting set of keyphrases turns out to\nsolve a relevant information-theoretic problem: if provided as side\ninformation, it leads to the expected minimal binary code length in compressing\nthe text using the LM and an entropy encoder. Alternately, the resulting set is\nan approximation via a causal LM to the set of phrases that minimize the\nentropy of the text when conditioned upon it. Empirically, the method provides\nresults comparable to the most commonly used methods in various keyphrase\nextraction benchmark challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kipnis_A/0/1/0/all/0/1\">Alexander Tsvetkov. Alon Kipnis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Poison of Alignment. (arXiv:2308.13449v1 [cs.CL])","link":"http://arxiv.org/abs/2308.13449","description":"<p>From the perspective of content safety issues, alignment has shown to limit\nlarge language models' (LLMs) harmful content generation. This intentional\nmethod of reinforcing models to not respond to certain user inputs seem to be\npresent in many modern open-source instruction tuning datasets such as\nOpenAssistant or Guanaco. We introduce a novel insight to an instruction-tuned\nmodel's performance affected by the presence of alignment in supervised\nfine-tuning dataset. To be specific, we noticed that alignment acts as if it is\npoisoning the instruction dataset. Experimentally, we demonstrate that aligned\nanswers significantly worsen the performance of the resulting fine-tuned\nmodel's on various reasoning benchmarks such as Big Bench (BBH), Massive\nMultitask Language Understanding (MMLU), Human Eval, and Discrete Reasoning\nOver Paragraphs (DROP), performing worse than the counterpart tuned without\nalignment by 4-33%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bekbayev_A/0/1/0/all/0/1\">Aibek Bekbayev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1\">Sungbae Chun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dulat_Y/0/1/0/all/0/1\">Yerzat Dulat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamazaki_J/0/1/0/all/0/1\">James Yamazaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARTIST: ARTificial Intelligence for Simplified Text. (arXiv:2308.13458v1 [cs.CL])","link":"http://arxiv.org/abs/2308.13458","description":"<p>Complex text is a major barrier for many citizens when accessing public\ninformation and knowledge. While often done manually, Text Simplification is a\nkey Natural Language Processing task that aims for reducing the linguistic\ncomplexity of a text while preserving the original meaning. Recent advances in\nGenerative Artificial Intelligence (AI) have enabled automatic text\nsimplification both on the lexical and syntactical levels. However, as\napplications often focus on English, little is understood about the\neffectiveness of Generative AI techniques on low-resource languages such as\nDutch. For this reason, we carry out empirical studies to understand the\nbenefits and limitations of applying generative technologies for text\nsimplification and provide the following outcomes: 1) the design and\nimplementation for a configurable text simplification pipeline that\norchestrates state-of-the-art generative text simplification models, domain and\nreader adaptation, and visualisation modules; 2) insights and lessons learned,\nshowing the strengths of automatic text simplification while exposing the\nchallenges in handling cultural and commonsense knowledge. These outcomes\nrepresent a first step in the exploration of Dutch text simplification and shed\nlight on future endeavours both for research and practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Corti_L/0/1/0/all/0/1\">Lorenzo Corti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Knowledge and Reinforcement Learning for Enhanced Reliability of Language Models. (arXiv:2308.13467v1 [cs.CL])","link":"http://arxiv.org/abs/2308.13467","description":"<p>The Natural Language Processing(NLP) community has been using crowd sourcing\ntechniques to create benchmark datasets such as General Language Understanding\nand Evaluation(GLUE) for training modern Language Models such as BERT. GLUE\ntasks measure the reliability scores using inter annotator metrics i.e. Cohens\nKappa. However, the reliability aspect of LMs has often been overlooked. To\ncounter this problem, we explore a knowledge-guided LM ensembling approach that\nleverages reinforcement learning to integrate knowledge from ConceptNet and\nWikipedia as knowledge graph embeddings. This approach mimics human annotators\nresorting to external knowledge to compensate for information deficits in the\ndatasets. Across nine GLUE datasets, our research shows that ensembling\nstrengthens reliability and accuracy scores, outperforming state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_N/0/1/0/all/0/1\">Nancy Tyagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Surjodeep Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1\">Manas Gaur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting a Large Language Model to Generate Diverse Motivational Messages: A Comparison with Human-Written Messages. (arXiv:2308.13479v1 [cs.CL])","link":"http://arxiv.org/abs/2308.13479","description":"<p>Large language models (LLMs) are increasingly capable and prevalent, and can\nbe used to produce creative content. The quality of content is influenced by\nthe prompt used, with more specific prompts that incorporate examples generally\nproducing better results. On from this, it could be seen that using\ninstructions written for crowdsourcing tasks (that are specific and include\nexamples to guide workers) could prove effective LLM prompts. To explore this,\nwe used a previous crowdsourcing pipeline that gave examples to people to help\nthem generate a collectively diverse corpus of motivational messages. We then\nused this same pipeline to generate messages using GPT-4, and compared the\ncollective diversity of messages from: (1) crowd-writers, (2) GPT-4 using the\npipeline, and (3 &amp; 4) two baseline GPT-4 prompts. We found that the LLM prompts\nusing the crowdsourcing pipeline caused GPT-4 to produce more diverse messages\nthan the two baseline prompts. We also discuss implications from messages\ngenerated by both human writers and LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cox_S/0/1/0/all/0/1\">Samuel Rhys Cox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_A/0/1/0/all/0/1\">Ashraf Abdul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ooi_W/0/1/0/all/0/1\">Wei Tsang Ooi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ngambay-French Neural Machine Translation (sba-Fr). (arXiv:2308.13497v1 [cs.CL])","link":"http://arxiv.org/abs/2308.13497","description":"<p>In Africa, and the world at large, there is an increasing focus on developing\nNeural Machine Translation (NMT) systems to overcome language barriers. NMT for\nLow-resource language is particularly compelling as it involves learning with\nlimited labelled data. However, obtaining a well-aligned parallel corpus for\nlow-resource languages can be challenging. The disparity between the\ntechnological advancement of a few global languages and the lack of research on\nNMT for local languages in Chad is striking. End-to-end NMT trials on\nlow-resource Chad languages have not been attempted. Additionally, there is a\ndearth of online and well-structured data gathering for research in Natural\nLanguage Processing, unlike some African languages. However, a guided approach\nfor data gathering can produce bitext data for many Chadian language\ntranslation pairs with well-known languages that have ample data. In this\nproject, we created the first sba-Fr Dataset, which is a corpus of\nNgambay-to-French translations, and fine-tuned three pre-trained models using\nthis dataset. Our experiments show that the M2M100 model outperforms other\nmodels with high BLEU scores on both original and original+synthetic data. The\npublicly available bitext dataset can be used for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sari_S/0/1/0/all/0/1\">Sakayo Toadoum Sari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1\">Angela Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seknewna_L/0/1/0/all/0/1\">Lema Logamou Seknewna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training and Meta-Evaluating Machine Translation Evaluation Metrics at the Paragraph Level. (arXiv:2308.13506v1 [cs.CL])","link":"http://arxiv.org/abs/2308.13506","description":"<p>As research on machine translation moves to translating text beyond the\nsentence level, it remains unclear how effective automatic evaluation metrics\nare at scoring longer translations. In this work, we first propose a method for\ncreating paragraph-level data for training and meta-evaluating metrics from\nexisting sentence-level data. Then, we use these new datasets to benchmark\nexisting sentence-level metrics as well as train learned metrics at the\nparagraph level. Interestingly, our experimental results demonstrate that using\nsentence-level metrics to score entire paragraphs is equally as effective as\nusing a metric designed to work at the paragraph level. We speculate this\nresult can be attributed to properties of the task of reference-based\nevaluation as well as limitations of our datasets with respect to capturing all\ntypes of phenomena that occur in paragraph-level translations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deutsch_D/0/1/0/all/0/1\">Daniel Deutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juraska_J/0/1/0/all/0/1\">Juraj Juraska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finkelstein_M/0/1/0/all/0/1\">Mara Finkelstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitag_a/0/1/0/all/0/1\">and Markus Freitag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT as Data Augmentation for Compositional Generalization: A Case Study in Open Intent Detection. (arXiv:2308.13517v1 [cs.CL])","link":"http://arxiv.org/abs/2308.13517","description":"<p>Open intent detection, a crucial aspect of natural language understanding,\ninvolves the identification of previously unseen intents in user-generated\ntext. Despite the progress made in this field, challenges persist in handling\nnew combinations of language components, which is essential for compositional\ngeneralization. In this paper, we present a case study exploring the use of\nChatGPT as a data augmentation technique to enhance compositional\ngeneralization in open intent detection tasks. We begin by discussing the\nlimitations of existing benchmarks in evaluating this problem, highlighting the\nneed for constructing datasets for addressing compositional generalization in\nopen intent detection tasks. By incorporating synthetic data generated by\nChatGPT into the training process, we demonstrate that our approach can\neffectively improve model performance. Rigorous evaluation of multiple\nbenchmarks reveals that our method outperforms existing techniques and\nsignificantly enhances open intent detection capabilities. Our findings\nunderscore the potential of large language models like ChatGPT for data\naugmentation in natural language understanding tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yihao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Stephen W. Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simplified Variant of G\\\"odel's Ontological Argument. (arXiv:2202.06264v3 [cs.LO] UPDATED)","link":"http://arxiv.org/abs/2202.06264","description":"<p>A simplified variant of G\\\"odel's ontological argument is presented. The\nsimplified argument is valid already in basic modal logics K or KT, it does not\nsuffer from modal collapse, and it avoids the rather complex predicates of\nessence (Ess.) and necessary existence (NE) as used by G\\\"odel. The variant\npresented has been obtained as a side result of a series of theory\nsimplification experiments conducted in interaction with a modern proof\nassistant system. The starting point for these experiments was the computer\nencoding of G\\\"odel's argument, and then automated reasoning techniques were\nsystematically applied to arrive at the simplified variant presented. The\npresented work thus exemplifies a fruitful human-computer interaction in\ncomputational metaphysics. Whether the presented result increases or decreases\nthe attractiveness and persuasiveness of the ontological argument is a question\nI would like to pass on to philosophy and theology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benzmuller_C/0/1/0/all/0/1\">Christoph Benzm&#xfc;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grammar-Based Grounded Lexicon Learning. (arXiv:2202.08806v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.08806","description":"<p>We present Grammar-Based Grounded Lexicon Learning (G2L2), a lexicalist\napproach toward learning a compositional and grounded meaning representation of\nlanguage from grounded data, such as paired images and texts. At the core of\nG2L2 is a collection of lexicon entries, which map each word to a tuple of a\nsyntactic type and a neuro-symbolic semantic program. For example, the word\nshiny has a syntactic type of adjective; its neuro-symbolic semantic program\nhas the symbolic form {\\lambda}x. filter(x, SHINY), where the concept SHINY is\nassociated with a neural network embedding, which will be used to classify\nshiny objects. Given an input sentence, G2L2 first looks up the lexicon entries\nassociated with each token. It then derives the meaning of the sentence as an\nexecutable neuro-symbolic program by composing lexical meanings based on\nsyntax. The recovered meaning programs can be executed on grounded inputs. To\nfacilitate learning in an exponentially-growing compositional space, we\nintroduce a joint parsing and expected execution algorithm, which does local\nmarginalization over derivations to reduce the training time. We evaluate G2L2\non two domains: visual reasoning and language-driven navigation. Results show\nthat G2L2 can generalize from small amounts of data to novel compositions of\nwords.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiayuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haoyue Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger P. Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grimm in Wonderland: Prompt Engineering with Midjourney to Illustrate Fairytales. (arXiv:2302.08961v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.08961","description":"<p>The quality of text-to-image generation is continuously improving, yet the\nboundaries of its applicability are still unclear. In particular, refinement of\nthe text input with the objective of achieving better results - commonly called\nprompt engineering - so far seems to have not been geared towards work with\npre-existing texts. We investigate whether text-to-image generation and prompt\nengineering could be used to generate basic illustrations of popular\nfairytales. Using Midjourney v4, we engage in action research with a dual aim:\nto attempt to generate 5 believable illustrations for each of 5 popular\nfairytales, and to define a prompt engineering process that starts from a\npre-existing text and arrives at an illustration of it. We arrive at a\ntentative 4-stage process: i) initial prompt, ii) composition adjustment, iii)\nstyle refinement, and iv) variation selection. We also discuss three reasons\nwhy the generation model struggles with certain illustrations: difficulties\nwith counts, bias from stereotypical configurations and inability to depict\noverly fantastic situations. Our findings are not limited to the specific\ngeneration model and are intended to be generalisable to future ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruskov_M/0/1/0/all/0/1\">Martin Ruskov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance. (arXiv:2303.16894v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2303.16894","description":"<p>Understanding 3D scenes from multi-view inputs has been proven to alleviate\nthe view discrepancy issue in 3D visual grounding. However, existing methods\nnormally neglect the view cues embedded in the text modality and fail to weigh\nthe relative importance of different views. In this paper, we propose\nViewRefer, a multi-view framework for 3D visual grounding exploring how to\ngrasp the view knowledge from both text and 3D modalities. For the text branch,\nViewRefer leverages the diverse linguistic knowledge of large-scale language\nmodels, e.g., GPT, to expand a single grounding text to multiple\ngeometry-consistent descriptions. Meanwhile, in the 3D modality, a transformer\nfusion module with inter-view attention is introduced to boost the interaction\nof objects across views. On top of that, we further present a set of learnable\nmulti-view prototypes, which memorize scene-agnostic knowledge for different\nviews, and enhance the framework from two perspectives: a view-guided attention\nmodule for more robust text features, and a view-guided scoring strategy during\nthe final prediction. With our designed paradigm, ViewRefer achieves superior\nperformance on three benchmarks and surpasses the second-best by +2.8%, +1.5%,\nand +1.35% on Sr3D, Nr3D, and ScanRefer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zoey Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yiwen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ray Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhigang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Approximating Online Human Evaluation of Social Chatbots with Prompting. (arXiv:2304.05253v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.05253","description":"<p>As conversational models become increasingly available to the general public,\nusers are engaging with this technology in social interactions. Such\nunprecedented interaction experiences may pose considerable social and\npsychological risks to the users unless the technology is properly controlled.\nThis highlights the need for scalable and robust evaluation metrics for\nconversational chatbots. Existing evaluation metrics aim to automate offline\nuser evaluation and approximate human judgment of pre-curated dialogs. However,\nthey are limited in their ability to capture subjective perceptions of users\nwho actually interact with the bots and might not generalize to real-world\nsettings. To address this limitation, we propose an approach to approximate\nonline human evaluation leveraging large language models (LLMs) from the GPT\nfamily. We introduce a new Dialog system Evaluation framework based on\nPrompting (DEP), which enables a fully automatic evaluation pipeline that\nreplicates live user studies and achieves an impressive correlation with human\njudgment (up to Pearson r=0.95 on a system level). The DEP approach involves\ncollecting synthetic chat logs of evaluated bots with an LLM in the other-play\nsetting, where the LLM is carefully conditioned to follow a specific scenario.\nWe further explore different prompting approaches to produce evaluation scores\nwith the same LLM. The best performing prompts, which contain few-shot\ndemonstrations and instructions, show outstanding performance on the tested\ndataset and demonstrate the ability to generalize to other dialog corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Svikhnushina_E/0/1/0/all/0/1\">Ekaterina Svikhnushina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_P/0/1/0/all/0/1\">Pearl Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PMC-LLaMA: Towards Building Open-source Language Models for Medicine. (arXiv:2304.14454v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.14454","description":"<p>Recently, Large Language Models (LLMs) have showcased remarkable capabilities\nin natural language understanding. While demonstrating proficiency in everyday\nconversations and question-answering situations, these models frequently\nstruggle in domains that require precision, such as medical applications, due\nto their lack of domain-specific knowledge. In this paper, we describe the\nprocedure for building a powerful, open-source language model specifically\ndesigned for medicine applications, termed as PMC-LLaMA. Our contributions are\nthreefold: (i) we systematically investigate the process of adapting a\ngeneral-purpose foundation language model towards medical domain, this involves\ndata-centric knowledge injection through the integration of 4.8M biomedical\nacademic papers and 30K medical textbooks, as well as comprehensive fine-tuning\nfor alignment with domain-specific instructions; (ii) we contribute a\nlarge-scale, comprehensive dataset for instruction tuning. This dataset\nencompasses medical question-answering (QA), rationale for reasoning, and\nconversational dialogues, comprising a total of 202M tokens; (iii) we conduct\nthorough ablation studies to demonstrate the effectiveness of each proposed\ncomponent. While evaluating on various public medical question-answering\nbenchmarks, our lightweight PMCLLaMA, which consists of only 13 billion\nparameters, exhibits superior performance, even surpassing ChatGPT. All models,\ncodes, datasets can be found in https://github.com/chaoyi-wu/PMC-LLaMA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chaoyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weixiong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoman Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds. (arXiv:2305.00969v4 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2305.00969","description":"<p>This paper describes the Ubenwa CryCeleb dataset - a labeled collection of\ninfant cries, and the accompanying CryCeleb 2023 task - a public speaker\nverification challenge based on infant cry sounds. We release for academic\nusage more than 6 hours of manually segmented cry sounds from 786 newborns to\nencourage research in infant cry analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Budaghyan_D/0/1/0/all/0/1\">David Budaghyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onu_C/0/1/0/all/0/1\">Charles C. Onu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorin_A/0/1/0/all/0/1\">Arsenii Gorin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subakan_C/0/1/0/all/0/1\">Cem Subakan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis. (arXiv:2305.00976v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.00976","description":"<p>In this paper, we present TMR, a simple yet effective approach for text to 3D\nhuman motion retrieval. While previous work has only treated retrieval as a\nproxy evaluation metric, we tackle it as a standalone task. Our method extends\nthe state-of-the-art text-to-motion synthesis model TEMOS, and incorporates a\ncontrastive loss to better structure the cross-modal latent space. We show that\nmaintaining the motion generation loss, along with the contrastive training, is\ncrucial to obtain good performance. We introduce a benchmark for evaluation and\nprovide an in-depth analysis by reporting results on several protocols. Our\nextensive experiments on the KIT-ML and HumanML3D datasets show that TMR\noutperforms the prior work by a significant margin, for example reducing the\nmedian rank from 54 to 19. Finally, we showcase the potential of our approach\non moment retrieval. Our code and models are publicly available at\nhttps://mathis.petrovich.fr/tmr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petrovich_M/0/1/0/all/0/1\">Mathis Petrovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_G/0/1/0/all/0/1\">G&#xfc;l Varol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers. (arXiv:2305.07011v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.07011","description":"<p>We present Region-aware Open-vocabulary Vision Transformers (RO-ViT) - a\ncontrastive image-text pretraining recipe to bridge the gap between image-level\npretraining and open-vocabulary object detection. At the pretraining phase, we\npropose to randomly crop and resize regions of positional embeddings instead of\nusing the whole image positional embeddings. This better matches the use of\npositional embeddings at region-level in the detection finetuning phase. In\naddition, we replace the common softmax cross entropy loss in contrastive\nlearning with focal loss to better learn the informative yet difficult\nexamples. Finally, we leverage recent advances in novel object proposals to\nimprove open-vocabulary detection finetuning. We evaluate our full model on the\nLVIS and COCO open-vocabulary detection benchmarks and zero-shot transfer.\nRO-ViT achieves a state-of-the-art 34.1 $AP_r$ on LVIS, surpassing the best\nexisting approach by +7.8 points in addition to competitive zero-shot transfer\ndetection. Surprisingly, RO-ViT improves the image-level representation as well\nand achieves the state of the art on 9 out of 12 metrics on COCO and Flickr\nimage-text retrieval benchmarks, outperforming competitive approaches with\nlarger models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dahun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1\">Anelia Angelova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_W/0/1/0/all/0/1\">Weicheng Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Estimate Model Transferability of Pre-Trained Speech Models?. (arXiv:2306.01015v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.01015","description":"<p>In this work, we introduce a \"score-based assessment\" framework for\nestimating the transferability of pre-trained speech models (PSMs) for\nfine-tuning target tasks. We leverage upon two representation theories,\nBayesian likelihood estimation and optimal transport, to generate rank scores\nfor the PSM candidates using the extracted representations. Our framework\nefficiently computes transferability scores without actual fine-tuning of\ncandidate models or layers by making a temporal independent hypothesis. We\nevaluate some popular supervised speech models (e.g., Conformer RNN-Transducer)\nand self-supervised speech models (e.g., HuBERT) in cross-layer and cross-model\nsettings using public data. Experimental results show a high Spearman's rank\ncorrelation and low $p$-value between our estimation framework and fine-tuning\nground truth. Our proposed transferability framework requires less\ncomputational time and resources, making it a resource-saving and\ntime-efficient approach for tuning speech foundation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zih-Ching Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nanxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shou-Yiin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts. (arXiv:2306.02207v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2306.02207","description":"<p>Large language models (LLMs) have gained considerable attention for\nArtificial Intelligence Generated Content (AIGC), particularly with the\nemergence of ChatGPT. However, the direct adaptation of continuous speech to\nLLMs that process discrete tokens remains an unsolved challenge, hindering the\napplication of LLMs for speech generation. The advanced speech LMs are in the\ncorner, as that speech signals encapsulate a wealth of information, including\nspeaker and emotion, beyond textual data alone. Prompt tuning has demonstrated\nnotable gains in parameter efficiency and competitive performance on some\nspeech classification tasks. However, the extent to which prompts can\neffectively elicit generation tasks from speech LMs remains an open question.\nIn this paper, we present pioneering research that explores the application of\nprompt tuning to stimulate speech LMs for various generation tasks, within a\nunified framework called SpeechGen, with around 10M trainable parameters. The\nproposed unified framework holds great promise for efficiency and\neffectiveness, particularly with the imminent arrival of advanced speech LMs,\nwhich will significantly enhance the capabilities of the framework. The code\nand demos of SpeechGen will be available on the project website:\n\\url{https://ga642381.github.io/SpeechPrompt/speechgen}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1\">Haibin Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yuan-Kuei Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-specific ChatBots for Science using Embeddings. (arXiv:2306.10067v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.10067","description":"<p>Large language models (LLMs) have emerged as powerful machine-learning\nsystems capable of handling a myriad of tasks. Tuned versions of these systems\nhave been turned into chatbots that can respond to user queries on a vast\ndiversity of topics, providing informative and creative replies. However, their\napplication to physical science research remains limited owing to their\nincomplete knowledge in these areas, contrasted with the needs of rigor and\nsourcing in science domains. Here, we demonstrate how existing methods and\nsoftware tools can be easily combined to yield a domain-specific chatbot. The\nsystem ingests scientific documents in existing formats, and uses text\nembedding lookup to provide the LLM with domain-specific contextual information\nwhen composing its reply. We similarly demonstrate that existing image\nembedding methods can be used for search and retrieval across publication\nfigures. These results confirm that LLMs are already suitable for use by\nphysical scientists in accelerating their research efforts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yager_K/0/1/0/all/0/1\">Kevin G. Yager</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACTI at EVALITA 2023: Overview of the Conspiracy Theory Identification Task. (arXiv:2307.06954v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.06954","description":"<p>Conspiracy Theory Identication task is a new shared task proposed for the\nfirst time at the Evalita 2023. The ACTI challenge, based exclusively on\ncomments published on conspiratorial channels of telegram, is divided into two\nsubtasks: (i) Conspiratorial Content Classification: identifying conspiratorial\ncontent and (ii) Conspiratorial Category Classification about specific\nconspiracy theory classification. A total of fifteen teams participated in the\ntask for a total of 81 submissions. We illustrate the best performing\napproaches were based on the utilization of large language models. We finally\ndraw conclusions about the utilization of these models for counteracting the\nspreading of misinformation in online platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Russo_G/0/1/0/all/0/1\">Giuseppe Russo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoehr_N/0/1/0/all/0/1\">Niklas Stoehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Manoel Horta Ribeiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoding ChatGPT: A Taxonomy of Existing Research, Current Challenges, and Possible Future Directions. (arXiv:2307.14107v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.14107","description":"<p>Chat Generative Pre-trained Transformer (ChatGPT) has gained significant\ninterest and attention since its launch in November 2022. It has shown\nimpressive performance in various domains, including passing exams and creative\nwriting. However, challenges and concerns related to biases and trust persist.\nIn this work, we present a comprehensive review of over 100 Scopus-indexed\npublications on ChatGPT, aiming to provide a taxonomy of ChatGPT research and\nexplore its applications. We critically analyze the existing literature,\nidentifying common approaches employed in the studies. Additionally, we\ninvestigate diverse application areas where ChatGPT has found utility, such as\nhealthcare, marketing and financial services, software engineering, academic\nand scientific writing, research and education, environmental science, and\nnatural language processing. Through examining these applications, we gain\nvaluable insights into the potential of ChatGPT in addressing real-world\nchallenges. We also discuss crucial issues related to ChatGPT, including biases\nand trustworthiness, emphasizing the need for further research and development\nin these areas. Furthermore, we identify potential future directions for\nChatGPT research, proposing solutions to current challenges and speculating on\nexpected advancements. By fully leveraging the capabilities of ChatGPT, we can\nunlock its potential across various domains, leading to advancements in\nconversational AI and transformative impacts in society.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sohail_S/0/1/0/all/0/1\">Shahab Saquib Sohail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhat_F/0/1/0/all/0/1\">Faiza Farhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Himeur_Y/0/1/0/all/0/1\">Yassine Himeur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeem_M/0/1/0/all/0/1\">Mohammad Nadeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madsen_D/0/1/0/all/0/1\">Dag &#xd8;ivind Madsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_Y/0/1/0/all/0/1\">Yashbir Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atalla_S/0/1/0/all/0/1\">Shadi Atalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansoor_W/0/1/0/all/0/1\">Wathiq Mansoor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks. (arXiv:2308.01423v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.01423","description":"<p>ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to\npredict and generate metal-organic frameworks (MOFs). By leveraging a\nlarge-scale language model (GPT-4 and GPT-3.5-turbo), ChatMOF extracts key\ndetails from textual inputs and delivers appropriate responses, thus\neliminating the necessity for rigid structured queries. The system is comprised\nof three core components (i.e. an agent, a toolkit, and an evaluator) and it\nforms a robust pipeline that manages a variety of tasks, including data\nretrieval, property prediction, and structure generations. The study further\nexplores the merits and constraints of using large language models (LLMs) AI\nsystem in material sciences using and showcases its transformative potential\nfor future advancements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yeonghun Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jihan Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Neural Network Generalization for Grammar Induction. (arXiv:2308.08253v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.08253","description":"<p>How well do neural networks generalize? Even for grammar induction tasks,\nwhere the target generalization is fully known, previous works have left the\nquestion open, testing very limited ranges beyond the training set and using\ndifferent success criteria. We provide a measure of neural network\ngeneralization based on fully specified formal languages. Given a model and a\nformal grammar, the method assigns a generalization score representing how well\na model generalizes to unseen samples in inverse relation to the amount of data\nit was trained on. The benchmark includes languages such as $a^nb^n$,\n$a^nb^nc^n$, $a^nb^mc^{n+m}$, and Dyck-1 and 2. We evaluate selected\narchitectures using the benchmark and find that networks trained with a Minimum\nDescription Length objective (MDL) generalize better and using less data than\nnetworks trained using standard loss functions. The benchmark is available at\nhttps://github.com/taucompling/bliss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_N/0/1/0/all/0/1\">Nur Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chemla_E/0/1/0/all/0/1\">Emmanuel Chemla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katzir_R/0/1/0/all/0/1\">Roni Katzir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"cantnlp@LT-EDI-2023: Homophobia/Transphobia Detection in Social Media Comments using Spatio-Temporally Retrained Language Models. (arXiv:2308.10370v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.10370","description":"<p>This paper describes our multiclass classification system developed as part\nof the LTEDI@RANLP-2023 shared task. We used a BERT-based language model to\ndetect homophobic and transphobic content in social media comments across five\nlanguage conditions: English, Spanish, Hindi, Malayalam, and Tamil. We\nretrained a transformer-based crosslanguage pretrained language model,\nXLMRoBERTa, with spatially and temporally relevant social media language data.\nWe also retrained a subset of models with simulated script-mixed social media\nlanguage data with varied performance. We developed the best performing\nseven-label classification system for Malayalam based on weighted macro\naveraged F1 score (ranked first out of six) with variable performance for other\nlanguage and class-label conditions. We found the inclusion of this\nspatio-temporal data improved the classification performance for all language\nand task conditions when compared with the baseline. The results suggests that\ntransformer-based language classification systems are sensitive to\nregister-specific and language-specific retraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_S/0/1/0/all/0/1\">Sidney G.-J. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durward_M/0/1/0/all/0/1\">Matthew Durward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_B/0/1/0/all/0/1\">Benjamin Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunn_J/0/1/0/all/0/1\">Jonathan Dunn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Authorship Representation Learning Capture Stylistic Features?. (arXiv:2308.11490v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.11490","description":"<p>Automatically disentangling an author's style from the content of their\nwriting is a longstanding and possibly insurmountable problem in computational\nlinguistics. At the same time, the availability of large text corpora furnished\nwith author labels has recently enabled learning authorship representations in\na purely data-driven manner for authorship attribution, a task that ostensibly\ndepends to a greater extent on encoding writing style than encoding content.\nHowever, success on this surrogate task does not ensure that such\nrepresentations capture writing style since authorship could also be correlated\nwith other latent variables, such as topic. In an effort to better understand\nthe nature of the information these representations convey, and specifically to\nvalidate the hypothesis that they chiefly encode writing style, we\nsystematically probe these representations through a series of targeted\nexperiments. The results of these experiments suggest that representations\nlearned for the surrogate authorship prediction task are indeed sensitive to\nwriting style. As a consequence, authorship representations may be expected to\nbe robust to certain kinds of data shift, such as topic drift over time.\nAdditionally, our findings may open the door to downstream applications that\nrequire stylistic representations, such as style transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Andrew Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggazzotti_C/0/1/0/all/0/1\">Cristina Aggazzotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotula_R/0/1/0/all/0/1\">Rebecca Kotula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_R/0/1/0/all/0/1\">Rafael Rivera Soto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bishop_M/0/1/0/all/0/1\">Marcus Bishop</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andrews_N/0/1/0/all/0/1\">Nicholas Andrews</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Prototype Adapter for Vision-Language Models. (arXiv:2308.11507v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2308.11507","description":"<p>Recently, large-scale pre-trained vision-language models (e.g. CLIP and\nALIGN) have demonstrated remarkable effectiveness in acquiring transferable\nvisual representations. To leverage the valuable knowledge encoded within these\nmodels for downstream tasks, several fine-tuning approaches, including prompt\ntuning methods and adapter-based methods, have been developed to adapt\nvision-language models effectively with supervision. However, these methods\nrely on the availability of annotated samples, which can be labor-intensive and\ntime-consuming to acquire, thus limiting scalability. To address this issue, in\nthis work, we design an unsupervised fine-tuning approach for vision-language\nmodels called Unsupervised Prototype Adapter (UP-Adapter). Specifically, for\nthe unannotated target datasets, we leverage the text-image aligning capability\nof CLIP to automatically select the most confident samples for each class.\nUtilizing these selected samples, we generate class prototypes, which serve as\nthe initialization for the learnable prototype model. After fine-tuning, the\nprototype model prediction is combined with the original CLIP's prediction by a\nresidual connection to perform downstream recognition tasks. Our extensive\nexperimental results on image recognition and domain generalization show that\nthe proposed unsupervised method outperforms 8-shot CoOp, 8-shot Tip-Adapter,\nand also the state-of-the-art UPL method by large margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xueting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhihai He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models. (arXiv:2308.11521v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.11521","description":"<p>Large language models (LLMs), such as ChatGPT, have emerged with astonishing\ncapabilities approaching artificial general intelligence. While providing\nconvenience for various societal needs, LLMs have also lowered the cost of\ngenerating harmful content. Consequently, LLM developers have deployed\nsemantic-level defenses to recognize and reject prompts that may lead to\ninappropriate content. Unfortunately, these defenses are not foolproof, and\nsome attackers have crafted \"jailbreak\" prompts that temporarily hypnotize the\nLLM into forgetting content defense rules and answering any improper questions.\nTo date, there is no clear explanation of the principles behind these\nsemantic-level attacks and defenses in both industry and academia.\n</p>\n<p>This paper investigates the LLM jailbreak problem and proposes an automatic\njailbreak method for the first time. We propose the concept of a semantic\nfirewall and provide three technical implementation approaches. Inspired by the\nattack that penetrates traditional firewalls through reverse tunnels, we\nintroduce a \"self-deception\" attack that can bypass the semantic firewall by\ninducing LLM to generate prompts that facilitate jailbreak. We generated a\ntotal of 2,520 attack payloads in six languages (English, Russian, French,\nSpanish, Chinese, and Arabic) across seven virtual scenarios, targeting the\nthree most common types of violations: violence, hate, and pornography. The\nexperiment was conducted on two models, namely the GPT-3.5-Turbo and GPT-4. The\nsuccess rates on the two models were 86.2% and 67%, while the failure rates\nwere 4.7% and 2.2%, respectively. This highlighted the effectiveness of the\nproposed attack method. All experimental code and raw data will be released as\nopen-source to inspire future research. We believe that manipulating AI\nbehavior through carefully crafted prompts will become an important research\ndirection in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Wei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baosheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_Z/0/1/0/all/0/1\">Zhiwen Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1\">Enze Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning. (arXiv:2308.12219v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.12219","description":"<p>The recent surge of generative AI has been fueled by the generative power of\ndiffusion probabilistic models and the scalable capabilities of large language\nmodels. Despite their potential, it remains elusive whether diffusion language\nmodels can solve general language tasks comparable to their autoregressive\ncounterparts. This paper demonstrates that scaling diffusion models w.r.t.\ndata, sizes, and tasks can effectively make them strong language learners. We\nbuild competent diffusion language models at scale by first acquiring knowledge\nfrom massive data via masked language modeling pretraining thanks to their\nintrinsic connections. We then reprogram pretrained masked language models into\ndiffusion language models via diffusive adaptation, wherein task-specific\nfinetuning and instruction finetuning are explored to unlock their versatility\nin solving general language tasks. Experiments show that scaling diffusion\nlanguage models consistently improves performance across downstream language\ntasks. We further discover that instruction finetuning can elicit zero-shot and\nfew-shot in-context learning abilities that help tackle many unseen tasks by\nfollowing natural language instructions, and show promise in advanced and\nchallenging abilities such as reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiasheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zaixiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yu Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1\">Lihua Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?. (arXiv:2308.12898v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2308.12898","description":"<p>The multimedia community has shown a significant interest in perceiving and\nrepresenting the physical world with multimodal pretrained neural network\nmodels, and among them, the visual-language pertaining (VLP) is, currently, the\nmost captivating topic. However, there have been few endeavors dedicated to the\nexploration of 1) whether essential linguistic knowledge (e.g., semantics and\nsyntax) can be extracted during VLP, and 2) how such linguistic knowledge\nimpact or enhance the multimodal alignment. In response, here we aim to\nelucidate the impact of comprehensive linguistic knowledge, including semantic\nexpression and syntactic structure, on multimodal alignment. Specifically, we\ndesign and release the SNARE, the first large-scale multimodal alignment\nprobing benchmark, to detect the vital linguistic components, e.g., lexical,\nsemantic, and syntax knowledge, containing four tasks: Semantic structure,\nNegation logic, Attribute ownership, and Relationship composition. Based on our\nproposed probing benchmarks, our holistic analyses of five advanced VLP models\nillustrate that the VLP model: i) shows insensitivity towards complex syntax\nstructures and relies on content words for sentence comprehension; ii)\ndemonstrates limited comprehension of combinations between sentences and\nnegations; iii) faces challenges in determining the presence of actions or\nspatial relationships within visual information and struggles with verifying\nthe correctness of triple combinations. We make our benchmark and code\navailable at \\url{https://github.com/WangFei-2019/SNARE/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jun Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code Llama: Open Foundation Models for Code. (arXiv:2308.12950v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.12950","description":"<p>We release Code Llama, a family of large language models for code based on\nLlama 2 providing state-of-the-art performance among open models, infilling\ncapabilities, support for large input contexts, and zero-shot instruction\nfollowing ability for programming tasks. We provide multiple flavors to cover a\nwide range of applications: foundation models (Code Llama), Python\nspecializations (Code Llama - Python), and instruction-following models (Code\nLlama - Instruct) with 7B, 13B and 34B parameters each. All models are trained\non sequences of 16k tokens and show improvements on inputs with up to 100k\ntokens. 7B and 13B Code Llama and Code Llama - Instruct variants support\ninfilling based on surrounding content. Code Llama reaches state-of-the-art\nperformance among open models on several code benchmarks, with scores of up to\n53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python\n7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform\nevery other publicly available model on MultiPL-E. We release Code Llama under\na permissive license that allows for both research and commercial use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roziere_B/0/1/0/all/0/1\">Baptiste Rozi&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehring_J/0/1/0/all/0/1\">Jonas Gehring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gloeckle_F/0/1/0/all/0/1\">Fabian Gloeckle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sootla_S/0/1/0/all/0/1\">Sten Sootla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gat_I/0/1/0/all/0/1\">Itai Gat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiaoqing Ellen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rapin_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;my Rapin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozhevnikov_A/0/1/0/all/0/1\">Artyom Kozhevnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evtimov_I/0/1/0/all/0/1\">Ivan Evtimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitton_J/0/1/0/all/0/1\">Joanna Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_M/0/1/0/all/0/1\">Manish Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_C/0/1/0/all/0/1\">Cristian Canton Ferrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grattafiori_A/0/1/0/all/0/1\">Aaron Grattafiori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Wenhan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Defossez_A/0/1/0/all/0/1\">Alexandre D&#xe9;fossez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azhar_F/0/1/0/all/0/1\">Faisal Azhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Touvron_H/0/1/0/all/0/1\">Hugo Touvron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1\">Louis Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usunier_N/0/1/0/all/0/1\">Nicolas Usunier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph-Based Recommendation System Enhanced with Community Detection. (arXiv:2201.03622v3 [cs.IR] CROSS LISTED)","link":"http://arxiv.org/abs/2201.03622","description":"<p>Many researchers have used tag information to improve the performance of\nrecommendation techniques in recommender systems. Examining the tags of users\nwill help to get their interests and leads to more accuracy in the\nrecommendations. Since user-defined tags are chosen freely and without any\nrestrictions, problems arise in determining their exact meaning and the\nsimilarity of tags. However, using thesaurus and ontologies to find the meaning\nof tags is not very efficient due to their free definition by users and the use\nof different languages in many data sets. Therefore, this article uses\nmathematical and statistical methods to determine lexical similarity and\nco-occurrence tags solution to assign semantic similarity. On the other hand,\ndue to the change of users' interests over time this article has considered the\ntime of tag assignments in co-occurrence tags for determining similarity of\ntags. Then the graph is created based on similarity of tags. For modeling the\ninterests of the users, the communities of tags are determined by using\ncommunity detection methods. So, recommendations based on the communities of\ntags and similarity between resources are done. The performance of the proposed\nmethod has been evaluated using two criteria of precision and recall through\nevaluations on two public datasets. The evaluation results show that the\nprecision and recall of the proposed method have significantly improved,\ncompared to the other methods. According to the experimental results, the\ncriteria of recall and precision have been improved, on average by 5% and 7%\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shokrzadeh_Z/0/1/0/all/0/1\">Zeinab Shokrzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1\">Mohammad-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balafar_M/0/1/0/all/0/1\">Mohammad-Ali Balafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagherzadeh_Mohasefi_J/0/1/0/all/0/1\">Jamshid Bagherzadeh-Mohasefi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Dialects Collide: How Socioeconomic Mixing Affects Language Use. (arXiv:2307.10016v1 [physics.soc-ph] CROSS LISTED)","link":"http://arxiv.org/abs/2307.10016","description":"<p>The socioeconomic background of people and how they use standard forms of\nlanguage are not independent, as demonstrated in various sociolinguistic\nstudies. However, the extent to which these correlations may be influenced by\nthe mixing of people from different socioeconomic classes remains relatively\nunexplored from a quantitative perspective. In this work we leverage geotagged\ntweets and transferable computational methods to map deviations from standard\nEnglish on a large scale, in seven thousand administrative areas of England and\nWales. We combine these data with high-resolution income maps to assign a proxy\nsocioeconomic indicator to home-located users. Strikingly, across eight\nmetropolitan areas we find a consistent pattern suggesting that the more\ndifferent socioeconomic classes mix, the less interdependent the frequency of\ntheir departures from standard grammar and their income become. Further, we\npropose an agent-based model of linguistic variety adoption that sheds light on\nthe mechanisms that produce the observations seen in the data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Louf_T/0/1/0/all/0/1\">Thomas Louf</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ramasco_J/0/1/0/all/0/1\">Jos&#xe9; J. Ramasco</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sanchez_D/0/1/0/all/0/1\">David S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Karsai_M/0/1/0/all/0/1\">M&#xe1;rton Karsai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-08-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}