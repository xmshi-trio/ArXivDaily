{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-12-01T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"DreamSync: Aligning Text-to-Image Generation with Image Understanding Feedback. (arXiv:2311.17946v1 [cs.CV])","link":"http://arxiv.org/abs/2311.17946","description":"<p>Despite their wide-spread success, Text-to-Image models (T2I) still struggle\nto produce images that are both aesthetically pleasing and faithful to the\nuser's input text. We introduce DreamSync, a model-agnostic training algorithm\nby design that improves T2I models to be faithful to the text input. DreamSync\nbuilds off a recent insight from TIFA's evaluation framework -- that large\nvision-language models (VLMs) can effectively identify the fine-grained\ndiscrepancies between generated images and the text inputs. DreamSync uses this\ninsight to train T2I models without any labeled data; it improves T2I models\nusing its own generations. First, it prompts the model to generate several\ncandidate images for a given input text. Then, it uses two VLMs to select the\nbest generation: a Visual Question Answering model that measures the alignment\nof generated images to the text, and another that measures the generation's\naesthetic quality. After selection, we use LoRA to iteratively finetune the T2I\nmodel to guide its generation towards the selected best generations. DreamSync\ndoes not need any additional human annotation. model architecture changes, or\nreinforcement learning. Despite its simplicity, DreamSync improves both the\nsemantic alignment and aesthetic appeal of two diffusion-based T2I models,\nevidenced by multiple benchmarks (+1.7% on TIFA, +2.9% on DSG1K, +3.4% on VILA\naesthetic) and human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1\">Deqing Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yushi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Su Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rassin_R/0/1/0/all/0/1\">Royi Rassin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juan_D/0/1/0/all/0/1\">Da-Cheng Juan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_D/0/1/0/all/0/1\">Dana Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herrmann_C/0/1/0/all/0/1\">Charles Herrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steenkiste_S/0/1/0/all/0/1\">Sjoerd van Steenkiste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashtchian_C/0/1/0/all/0/1\">Cyrus Rashtchian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Infilling Code Generation. (arXiv:2311.17972v1 [cs.PL])","link":"http://arxiv.org/abs/2311.17972","description":"<p>This work introduces a general code generation framework that incorporates\ninfilling operations into auto-regressive decoding. Our approach capitalizes on\nthe observation that recent code language models with infilling capabilities\ncan perform \\emph{self-infilling}: whereas infilling operations aim to fill in\nthe middle based on a predefined prefix and suffix, self-infilling sequentially\ngenerates both such surrounding context and the infilled content. We utilize\nthis feature to develop an infilling-augmented decoding process that\nfacilitates non-monotonic generation. This approach allows for postponing the\ngeneration of uncertain code snippets until a definitive suffix is established,\nleading to improved control over the generation sequence. In addition, it\nfacilitates a looping mechanism, which can iteratively update and synchronize\neach piece of generation in a cyclic manner. Extensive experiments are\nconducted to demonstrate that our proposed decoding process is effective in\nenhancing regularity and quality across several code generation benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jianbo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Filtered Semi-Markov CRF. (arXiv:2311.18028v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18028","description":"<p>Semi-Markov CRF has been proposed as an alternative to the traditional Linear\nChain CRF for text segmentation tasks such as Named Entity Recognition (NER).\nUnlike CRF, which treats text segmentation as token-level prediction, Semi-CRF\nconsiders segments as the basic unit, making it more expressive. However,\nSemi-CRF suffers from two major drawbacks: (1) quadratic complexity over\nsequence length, as it operates on every span of the input sequence, and (2)\ninferior performance compared to CRF for sequence labeling tasks like NER. In\nthis paper, we introduce Filtered Semi-Markov CRF, a variant of Semi-CRF that\naddresses these issues by incorporating a filtering step to eliminate\nirrelevant segments, reducing complexity and search space. Our approach is\nevaluated on several NER benchmarks, where it outperforms both CRF and Semi-CRF\nwhile being significantly faster. The implementation of our method is available\non \\href{https://github.com/urchade/Filtered-Semi-Markov-CRF}{Github}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaratiana_U/0/1/0/all/0/1\">Urchade Zaratiana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomeh_N/0/1/0/all/0/1\">Nadi Tomeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khbir_N/0/1/0/all/0/1\">Niama El Khbir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holat_P/0/1/0/all/0/1\">Pierre Holat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charnois_T/0/1/0/all/0/1\">Thierry Charnois</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings. (arXiv:2311.18034v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18034","description":"<p>Cross-lingual transfer learning is an important property of multilingual\nlarge language models (LLMs). But how do LLMs represent relationships between\nlanguages? Every language model has an input layer that maps tokens to vectors.\nThis ubiquitous layer of language models is often overlooked. We find that\nsimilarities between these input embeddings are highly interpretable and that\nthe geometry of these embeddings differs between model families. In one case\n(XLM-RoBERTa), embeddings encode language: tokens in different writing systems\ncan be linearly separated with an average of 99.2% accuracy. Another family\n(mT5) represents cross-lingual semantic similarity: the 50 nearest neighbors\nfor any token represent an average of 7.61 writing systems, and are frequently\ntranslations. This result is surprising given that there is no explicit\nparallel cross-lingual training corpora and no explicit incentive for\ntranslations in pre-training objectives. Our research opens the door for\ninvestigations in 1) The effect of pre-training and model architectures on\nrepresentations of languages and 2) The applications of cross-lingual\nrepresentations embedded in language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Yi_A/0/1/0/all/0/1\">Andrea W Wen-Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mimno_D/0/1/0/all/0/1\">David Mimno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Conversational Summarization Evaluations with small Large Language Models. (arXiv:2311.18041v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18041","description":"<p>Large Language Models (LLMs) exhibit powerful summarization abilities.\nHowever, their capabilities on conversational summarization remains under\nexplored. In this work we evaluate LLMs (approx. 10 billion parameters) on\nconversational summarization and showcase their performance on various prompts.\nWe show that the summaries generated by models depend on the instructions and\nthe performance of LLMs vary with different instructions sometimes resulting\nsteep drop in ROUGE scores if prompts are not selected carefully. We also\nevaluate the models with human evaluations and discuss the limitations of the\nmodels on conversational summarization\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manuvinakurike_R/0/1/0/all/0/1\">Ramesh Manuvinakurike</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahay_S/0/1/0/all/0/1\">Saurav Sahay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manepalli_S/0/1/0/all/0/1\">Sangeeta Manepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachman_L/0/1/0/all/0/1\">Lama Nachman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text. (arXiv:2311.18054v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18054","description":"<p>Potential harms of Large Language Models such as mass misinformation and\nplagiarism can be partially mitigated if there exists a reliable way to detect\nmachine generated text. In this paper, we propose a new watermarking method to\ndetect machine-generated texts. Our method embeds a unique pattern within the\ngenerated text, ensuring that while the content remains coherent and natural to\nhuman readers, it carries distinct markers that can be identified\nalgorithmically. Specifically, we intervene with the token sampling process in\na way which enables us to trace back our token choices during the detection\nphase. We show how watermarking affects textual quality and compare our\nproposed method with a state-of-the-art watermarking method in terms of\nrobustness and detectability. Through extensive experiments, we demonstrate the\neffectiveness of our watermarking scheme in distinguishing between watermarked\nand non-watermarked text, achieving high detection rates while maintaining\ntextual quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keles_K/0/1/0/all/0/1\">Kaan Efe Kele&#x15f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurbuz_O/0/1/0/all/0/1\">&#xd6;mer Kaan G&#xfc;rb&#xfc;z</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutlu_M/0/1/0/all/0/1\">Mucahid Kutlu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TurkishBERTweet: Fast and Reliable Large Language Model for Social Media Analysis. (arXiv:2311.18063v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18063","description":"<p>Turkish is one of the most popular languages in the world. Wide us of this\nlanguage on social media platforms such as Twitter, Instagram, or Tiktok and\nstrategic position of the country in the world politics makes it appealing for\nthe social network researchers and industry. To address this need, we introduce\nTurkishBERTweet, the first large scale pre-trained language model for Turkish\nsocial media built using almost 900 million tweets. The model shares the same\narchitecture as base BERT model with smaller input length, making\nTurkishBERTweet lighter than BERTurk and can have significantly lower inference\ntime. We trained our model using the same approach for RoBERTa model and\nevaluated on two text classification tasks: Sentiment Classification and Hate\nSpeech Detection. We demonstrate that TurkishBERTweet outperforms the other\navailable alternatives on generalizability and its lower inference time gives\nsignificant advantage to process large-scale datasets. We also compared our\nmodels with the commercial OpenAI solutions in terms of cost and performance to\ndemonstrate TurkishBERTweet is scalable and cost-effective solution. As part of\nour research, we released TurkishBERTweet and fine-tuned LoRA adapters for the\nmentioned tasks under the MIT License to facilitate future research and\napplications on Turkish social media. Our TurkishBERTweet model is available\nat: https://github.com/ViralLab/TurkishBERTweet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Najafi_A/0/1/0/all/0/1\">Ali Najafi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_O/0/1/0/all/0/1\">Onur Varol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ROBBIE: Robust Bias Evaluation of Large Generative Language Models. (arXiv:2311.18140v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18140","description":"<p>As generative large language models (LLMs) grow more performant and\nprevalent, we must develop comprehensive enough tools to measure and improve\ntheir fairness. Different prompt-based datasets can be used to measure social\nbias across multiple text domains and demographic axes, meaning that testing\nLLMs on more datasets can potentially help us characterize their biases more\nfully, and better ensure equal and equitable treatment of marginalized\ndemographic groups. In this work, our focus is two-fold:\n</p>\n<p>(1) Benchmarking: a comparison of 6 different prompt-based bias and toxicity\nmetrics across 12 demographic axes and 5 families of generative LLMs. Out of\nthose 6 metrics, AdvPromptSet and HolisticBiasR are novel datasets proposed in\nthe paper. The comparison of those benchmarks gives us insights about the bias\nand toxicity of the compared models. Therefore, we explore the frequency of\ndemographic terms in common LLM pre-training corpora and how this may relate to\nmodel biases.\n</p>\n<p>(2) Mitigation: we conduct a comprehensive study of how well 3 bias/toxicity\nmitigation techniques perform across our suite of measurements. ROBBIE aims to\nprovide insights for practitioners while deploying a model, emphasizing the\nneed to not only measure potential harms, but also understand how they arise by\ncharacterizing the data, mitigate harms once found, and balance any trade-offs.\nWe open-source our analysis code in hopes of encouraging broader measurements\nof bias in future LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Esiobu_D/0/1/0/all/0/1\">David Esiobu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiaoqing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1\">Saghar Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ung_M/0/1/0/all/0/1\">Megan Ung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuchen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_J/0/1/0/all/0/1\">Jude Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwivedi_Yu_J/0/1/0/all/0/1\">Jane Dwivedi-Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Presani_E/0/1/0/all/0/1\">Eleonora Presani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1\">Eric Michael Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DisCGen: A Framework for Discourse-Informed Counterspeech Generation. (arXiv:2311.18147v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18147","description":"<p>Counterspeech can be an effective method for battling hateful content on\nsocial media. Automated counterspeech generation can aid in this process.\nGenerated counterspeech, however, can be viable only when grounded in the\ncontext of topic, audience and sensitivity as these factors influence both the\nefficacy and appropriateness. In this work, we propose a novel framework based\non theories of discourse to study the inferential links that connect counter\nspeeches to the hateful comment. Within this framework, we propose: i) a\ntaxonomy of counterspeech derived from discourse frameworks, and ii)\ndiscourse-informed prompting strategies for generating contextually-grounded\ncounterspeech. To construct and validate this framework, we present a process\nfor collecting an in-the-wild dataset of counterspeech from Reddit. Using this\nprocess, we manually annotate a dataset of 3.9k Reddit comment pairs for the\npresence of hatespeech and counterspeech. The positive pairs are annotated for\n10 classes in our proposed taxonomy. We annotate these pairs with paraphrased\ncounterparts to remove offensiveness and first-person references. We show that\nby using our dataset and framework, large language models can generate\ncontextually-grounded counterspeech informed by theories of discourse.\nAccording to our human evaluation, our approaches can act as a safeguard\nagainst critical failures of discourse-agnostic models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hassan_S/0/1/0/all/0/1\">Sabit Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty Guided Global Memory Improves Multi-Hop Question Answering. (arXiv:2311.18151v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18151","description":"<p>Transformers have become the gold standard for many natural language\nprocessing tasks and, in particular, for multi-hop question answering (MHQA).\nThis task includes processing a long document and reasoning over the multiple\nparts of it. The landscape of MHQA approaches can be classified into two\nprimary categories. The first group focuses on extracting supporting evidence,\nthereby constraining the QA model's context to predicted facts. Conversely, the\nsecond group relies on the attention mechanism of the long input encoding model\nto facilitate multi-hop reasoning. However, attention-based token\nrepresentations lack explicit global contextual information to connect\nreasoning steps. To address these issues, we propose GEMFormer, a two-stage\nmethod that first collects relevant information over the entire document to the\nmemory and then combines it with local context to solve the task. Our\nexperimental results show that fine-tuning a pre-trained model with\nmemory-augmented input, including the most certain global elements, improves\nthe model's performance on three MHQA datasets compared to the baseline. We\nalso found that the global explicit memory contains information from supporting\nfacts required for the correct answer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sagirova_A/0/1/0/all/0/1\">Alsu Sagirova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burtsev_M/0/1/0/all/0/1\">Mikhail Burtsev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes. (arXiv:2311.18194v1 [cs.LG])","link":"http://arxiv.org/abs/2311.18194","description":"<p>In-context learning (ICL) refers to the ability of a model to condition on a\nfew in-context demonstrations (input-output examples of the underlying task) to\ngenerate the answer for a new query input, without updating parameters. Despite\nthe impressive ICL ability of LLMs, it has also been found that ICL in LLMs is\nsensitive to input demonstrations and limited to short context lengths. To\nunderstand the limitations and principles for successful ICL, we conduct an\ninvestigation with ICL linear regression of transformers. We characterize\nseveral Out-of-Distribution (OOD) cases for ICL inspired by realistic LLM ICL\nfailures and compare transformers with DeepSet, a simple yet powerful\narchitecture for ICL. Surprisingly, DeepSet outperforms transformers across a\nvariety of distribution shifts, implying that preserving permutation invariance\nsymmetry to input demonstrations is crucial for OOD ICL. The phenomenon\nspecifies a fundamental requirement by ICL, which we termed as ICL invariance.\nNevertheless, the positional encodings in LLMs will break ICL invariance. To\nthis end, we further evaluate transformers with identical positional encodings\nand find preserving ICL invariance in transformers achieves state-of-the-art\nperformance across various ICL distribution shifts\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yongqiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_B/0/1/0/all/0/1\">Binghui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiwen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1\">Yatao Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">James Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Vaccine Misinformation in Middle Income Countries. (arXiv:2311.18195v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18195","description":"<p>This paper introduces a multilingual dataset of COVID-19 vaccine\nmisinformation, consisting of annotated tweets from three middle-income\ncountries: Brazil, Indonesia, and Nigeria. The expertly curated dataset\nincludes annotations for 5,952 tweets, assessing their relevance to COVID-19\nvaccines, presence of misinformation, and the themes of the misinformation. To\naddress challenges posed by domain specificity, the low-resource setting, and\ndata imbalance, we adopt two approaches for developing COVID-19 vaccine\nmisinformation detection models: domain-specific pre-training and text\naugmentation using a large language model. Our best misinformation detection\nmodels demonstrate improvements ranging from 2.7 to 15.9 percentage points in\nmacro F1-score compared to the baseline models. Additionally, we apply our\nmisinformation detection models in a large-scale study of 19 million unlabeled\ntweets from the three countries between 2020 and 2022, showcasing the practical\napplication of our dataset and models for detecting and analyzing vaccine\nmisinformation in multiple countries and languages. Our analysis indicates that\npercentage changes in the number of new COVID-19 cases are positively\nassociated with COVID-19 vaccine misinformation rates in a staggered manner for\nBrazil and Indonesia, and there are significant positive associations between\nthe misinformation rates across the three countries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jongin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Back_B/0/1/0/all/0/1\">Byeo Rhee Back</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Aditya Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaxi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wirtz_V/0/1/0/all/0/1\">Veronika J. Wirtz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_T/0/1/0/all/0/1\">Traci Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijaya_D/0/1/0/all/0/1\">Derry Wijaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"INarIG: Iterative Non-autoregressive Instruct Generation Model For Word-Level Auto Completion. (arXiv:2311.18200v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18200","description":"<p>Computer-aided translation (CAT) aims to enhance human translation efficiency\nand is still important in scenarios where machine translation cannot meet\nquality requirements. One fundamental task within this field is Word-Level Auto\nCompletion (WLAC). WLAC predicts a target word given a source sentence,\ntranslation context, and a human typed character sequence. Previous works\neither employ word classification models to exploit contextual information from\nboth sides of the target word or directly disregarded the dependencies from the\nright-side context. Furthermore, the key information, i.e. human typed\nsequences, is only used as prefix constraints in the decoding module. In this\npaper, we propose the INarIG (Iterative Non-autoregressive Instruct Generation)\nmodel, which constructs the human typed sequence into Instruction Unit and\nemploys iterative decoding with subwords to fully utilize input information\ngiven in the task. Our model is more competent in dealing with low-frequency\nwords (core scenario of this task), and achieves state-of-the-art results on\nthe WMT22 and benchmark datasets, with a maximum increase of over 10%\nprediction accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shang_H/0/1/0/all/0/1\">Hengchao Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zongyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Daimeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaxin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Minghan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_L/0/1/0/all/0/1\">Lizhi Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Construction of a Korean Toxic Instruction Dataset for Ethical Tuning of Large Language Models. (arXiv:2311.18215v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18215","description":"<p>Caution: this paper may include material that could be offensive or\ndistressing.\n</p>\n<p>The advent of Large Language Models (LLMs) necessitates the development of\ntraining approaches that mitigate the generation of unethical language and\naptly manage toxic user queries. Given the challenges related to human labor\nand the scarcity of data, we present KoTox, comprising 39K unethical\ninstruction-output pairs. This collection of automatically generated toxic\ninstructions refines the training of LLMs and establishes a foundational\nframework for improving LLMs' ethical awareness and response to various toxic\ninputs, promoting more secure and responsible interactions in Natural Language\nProcessing (NLP) applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Byun_S/0/1/0/all/0/1\">Sungjoo Byun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_D/0/1/0/all/0/1\">Dongjun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_H/0/1/0/all/0/1\">Hyemi Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_H/0/1/0/all/0/1\">Hyopil Shin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models. (arXiv:2311.18232v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18232","description":"<p>Large language models (LLMs) provide excellent text-generation capabilities,\nbut standard prompting and generation methods generally do not lead to\nintentional or goal-directed agents and might necessitate considerable prompt\ntuning. This becomes particularly apparent in multi-turn conversations: even\nthe best current LLMs rarely ask clarifying questions, engage in explicit\ninformation gathering, or take actions now that lead to better decisions after\nmultiple turns. Reinforcement learning has the potential to leverage the\npowerful modeling capabilities of LLMs, as well as their internal\nrepresentation of textual interactions, to create capable goal-directed\nlanguage agents. This can enable intentional and temporally extended\ninteractions, such as with humans, through coordinated persuasion and carefully\ncrafted questions, or in goal-directed play through text games to bring about\ndesired final outcomes. However, enabling this requires the community to\ndevelop stable and reliable reinforcement learning algorithms that can\neffectively train LLMs. Developing such algorithms requires tasks that can\ngauge progress on algorithm design, provide accessible and reproducible\nevaluations for multi-turn interactions, and cover a range of task properties\nand challenges in improving reinforcement learning algorithms. Our paper\nintroduces the LMRL-Gym benchmark for evaluating multi-turn RL for LLMs,\ntogether with an open-source research framework containing a basic toolkit for\ngetting started on multi-turn RL with offline value-based and policy-based RL\nmethods. Our benchmark consists of 8 different language tasks, which require\nmultiple rounds of language interaction and cover a range of tasks in\nopen-ended dialogue and text games.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdulhai_M/0/1/0/all/0/1\">Marwa Abdulhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_I/0/1/0/all/0/1\">Isadora White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snell_C/0/1/0/all/0/1\">Charlie Snell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Charles Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Joey Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1\">Yuexiang Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kelvin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model. (arXiv:2311.18248v1 [cs.MM])","link":"http://arxiv.org/abs/2311.18248","description":"<p>Recently, the strong text creation ability of Large Language Models(LLMs) has\ngiven rise to many tools for assisting paper reading or even writing. However,\nthe weak diagram analysis abilities of LLMs or Multimodal LLMs greatly limit\ntheir application scenarios, especially for scientific academic paper writing.\nIn this work, towards a more versatile copilot for academic paper writing, we\nmainly focus on strengthening the multi-modal diagram analysis ability of\nMultimodal LLMs. By parsing Latex source files of high-quality papers, we\ncarefully build a multi-modal diagram understanding dataset M-Paper. By\naligning diagrams in the paper with related paragraphs, we construct\nprofessional diagram analysis samples for training and evaluation. M-Paper is\nthe first dataset to support joint comprehension of multiple scientific\ndiagrams, including figures and tables in the format of images or Latex codes.\nBesides, to better align the copilot with the user's intention, we introduce\nthe `outline' as the control signal, which could be directly given by the user\nor revised based on auto-generated ones. Comprehensive experiments with a\nstate-of-the-art Mumtimodal LLM demonstrate that training on our dataset shows\nstronger scientific diagram understanding performance, including diagram\ncaptioning, diagram analysis, and outline recommendation. The dataset, code,\nand model are available at\nhttps://github.com/X-PLUG/mPLUG-DocOwl/tree/main/PaperOwl.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Anwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yaya Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiabo Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinghao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1\">Qi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consensus, dissensus and synergy between clinicians and specialist foundation models in radiology report generation. (arXiv:2311.18260v1 [eess.IV])","link":"http://arxiv.org/abs/2311.18260","description":"<p>Radiology reports are an instrumental part of modern medicine, informing key\nclinical decisions such as diagnosis and treatment. The worldwide shortage of\nradiologists, however, restricts access to expert care and imposes heavy\nworkloads, contributing to avoidable errors and delays in report delivery.\nWhile recent progress in automated report generation with vision-language\nmodels offer clear potential in ameliorating the situation, the path to\nreal-world adoption has been stymied by the challenge of evaluating the\nclinical quality of AI-generated reports. In this study, we build a\nstate-of-the-art report generation system for chest radiographs, Flamingo-CXR,\nby fine-tuning a well-known vision-language foundation model on radiology data.\nTo evaluate the quality of the AI-generated reports, a group of 16 certified\nradiologists provide detailed evaluations of AI-generated and human written\nreports for chest X-rays from an intensive care setting in the United States\nand an inpatient setting in India. At least one radiologist (out of two per\ncase) preferred the AI report to the ground truth report in over 60$\\%$ of\ncases for both datasets. Amongst the subset of AI-generated reports that\ncontain errors, the most frequently cited reasons were related to the location\nand finding, whereas for human written reports, most mistakes were related to\nseverity and finding. This disparity suggested potential complementarity\nbetween our AI system and human experts, prompting us to develop an assistive\nscenario in which Flamingo-CXR generates a first-draft report, which is\nsubsequently revised by a clinician. This is the first demonstration of\nclinician-AI collaboration for report writing, and the resultant reports are\nassessed to be equivalent or preferred by at least one radiologist to reports\nwritten by experts alone in 80$\\%$ of in-patient cases and 66$\\%$ of intensive\ncare cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tanno_R/0/1/0/all/0/1\">Ryutaro Tanno</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barrett_D/0/1/0/all/0/1\">David G.T. Barrett</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sellergren_A/0/1/0/all/0/1\">Andrew Sellergren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghaisas_S/0/1/0/all/0/1\">Sumedh Ghaisas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dathathri_S/0/1/0/all/0/1\">Sumanth Dathathri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+See_A/0/1/0/all/0/1\">Abigail See</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Welbl_J/0/1/0/all/0/1\">Johannes Welbl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singhal_K/0/1/0/all/0/1\">Karan Singhal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Azizi_S/0/1/0/all/0/1\">Shekoofeh Azizi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tu_T/0/1/0/all/0/1\">Tao Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schaekermann_M/0/1/0/all/0/1\">Mike Schaekermann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+May_R/0/1/0/all/0/1\">Rhys May</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_R/0/1/0/all/0/1\">Roy Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Man_S/0/1/0/all/0/1\">SiWai Man</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahmed_Z/0/1/0/all/0/1\">Zahra Ahmed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahdavi_S/0/1/0/all/0/1\">Sara Mahdavi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Belgrave_D/0/1/0/all/0/1\">Danielle Belgrave</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Natarajan_V/0/1/0/all/0/1\">Vivek Natarajan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shetty_S/0/1/0/all/0/1\">Shravya Shetty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kohli_P/0/1/0/all/0/1\">Pushmeet Kohli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_P/0/1/0/all/0/1\">Po-Sen Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karthikesalingam_A/0/1/0/all/0/1\">Alan Karthikesalingam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ktena_I/0/1/0/all/0/1\">Ira Ktena</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Rationale Understanding of Critical Reasoning in Logical Reading Comprehension. (arXiv:2311.18353v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18353","description":"<p>To precisely evaluate a language model's capability for logical reading\ncomprehension, we present a dataset for testing the understanding of the\nrationale behind critical reasoning. For questions taken from an existing\nmultiplechoice logical reading comprehension dataset, we crowdsource rationale\ntexts that explain why we should select or eliminate answer options, resulting\nin 3,003 multiple-choice subquestions that are associated with 943 main\nquestions. Experiments on our dataset show that recent large language models\n(e.g., InstructGPT) struggle to answer the subquestions even if they are able\nto answer the main questions correctly. We find that the models perform\nparticularly poorly in answering subquestions written for the incorrect options\nof the main questions, implying that the models have a limited capability for\nexplaining why incorrect alternatives should be eliminated. These results\nsuggest that our dataset encourages further investigation into the critical\nreasoning ability of language models while focusing on the elimination process\nof relevant alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kawabata_A/0/1/0/all/0/1\">Akira Kawabata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugawara_S/0/1/0/all/0/1\">Saku Sugawara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hubness Reduction Improves Sentence-BERT Semantic Spaces. (arXiv:2311.18364v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18364","description":"<p>Semantic representations of text, i.e. representations of natural language\nwhich capture meaning by geometry, are essential for areas such as information\nretrieval and document grouping. High-dimensional trained dense vectors have\nreceived much attention in recent years as such representations. We investigate\nthe structure of semantic spaces that arise from embeddings made with\nSentence-BERT and find that the representations suffer from a well-known\nproblem in high dimensions called hubness. Hubness results in asymmetric\nneighborhood relations, such that some texts (the hubs) are neighbours of many\nother texts while most texts (so-called anti-hubs), are neighbours of few or no\nother texts. We quantify the semantic quality of the embeddings using hubness\nscores and error rate of a neighbourhood based classifier. We find that when\nhubness is high, we can reduce error rate and hubness using hubness reduction\nmethods. We identify a combination of two methods as resulting in the best\nreduction. For example, on one of the tested pretrained models, this combined\nmethod can reduce hubness by about 75% and error rate by about 9%. Thus, we\nargue that mitigating hubness in the embedding space provides better semantic\nrepresentations of text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_B/0/1/0/all/0/1\">Beatrix M. G. Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_L/0/1/0/all/0/1\">Lars Kai Hansen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions. (arXiv:2311.18397v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18397","description":"<p>Retrieval-Augmented Generation (RAG), by incorporating external knowledge\nwith parametric memory of language models, has become the state-of-the-art\narchitecture for open-domain QA tasks. However, common knowledge bases are\ninherently constrained by limited coverage and noisy information, making\nretrieval-based approaches inadequate to answer implicit reasoning questions.\nIn this paper, we propose an Induction-Augmented Generation (IAG) framework\nthat utilizes inductive knowledge along with the retrieved documents for\nimplicit reasoning. We leverage large language models (LLMs) for deriving such\nknowledge via a novel prompting method based on inductive reasoning patterns.\nOn top of this, we implement two versions of IAG named IAG-GPT and IAG-Student,\nrespectively. IAG-GPT directly utilizes the knowledge generated by GPT-3 for\nanswer prediction, while IAG-Student gets rid of dependencies on GPT service at\ninference time by incorporating a student inductor model. The inductor is\nfirstly trained via knowledge distillation and further optimized by\nback-propagating the generator feedback via differentiable beam scores.\nExperimental results show that IAG outperforms RAG baselines as well as ChatGPT\non two Open-Domain QA tasks. Notably, our best models have won the first place\nin the official leaderboards of CSQA2.0 (since Nov 1, 2022) and StrategyQA\n(since Jan 8, 2023).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhebin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yuanhang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Saijiang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Meng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongkang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_R/0/1/0/all/0/1\">Ruofei Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhao Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Use of explicit replies as coordination mechanisms in online student debate. (arXiv:2311.18466v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18466","description":"<p>People in conversation entrain their linguistic behaviours through\nspontaneous alignment mechanisms [7] - both in face-to-face and\ncomputer-mediated communication (CMC) [8]. In CMC, one of the mechanisms\nthrough which linguistic entrainment happens is through explicit replies.\nIndeed, the use of explicit replies influences the structure of conversations,\nfavouring the formation of reply-trees typically delineated by topic shifts\n[5]. The interpersonal coordination mechanisms realized by how actors address\neach other have been studied using a probabilistic framework proposed by David\nGibson [2,3]. Other recent approaches use computational methods and information\ntheory to quantify changes in text. We explore coordination mechanisms\nconcerned with some of the roles utterances play in dialogues - specifically in\nexplicit replies. We identify these roles by finding community structure in the\nconversation's vocabulary using a non-parametric, hierarchical topic model.\nSome conversations may always stay on the ground, remaining at the level of\ngeneral introductory chatter. Some others may develop a specific sub-topic in\nsignificant depth and detail. Even others may jump between general chatter,\nout-of-topic remarks and people agreeing or disagreeing without further\nelaboration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_Saraiva_B/0/1/0/all/0/1\">Bruno D. Ferreira-Saraiva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matos_Carvalho_J/0/1/0/all/0/1\">Joao P. Matos-Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pita_M/0/1/0/all/0/1\">Manuel Pita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ESG Accountability Made Easy: DocQA at Your Service. (arXiv:2311.18481v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18481","description":"<p>We present Deep Search DocQA. This application enables information extraction\nfrom documents via a question-answering conversational assistant. The system\nintegrates several technologies from different AI disciplines consisting of\ndocument conversion to machine-readable format (via computer vision), finding\nrelevant data (via natural language processing), and formulating an eloquent\nresponse (via large language models). Users can explore over 10,000\nEnvironmental, Social, and Governance (ESG) disclosure reports from over 2000\ncorporations. The Deep Search platform can be accessed at:\nhttps://ds4sd.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_L/0/1/0/all/0/1\">Lokesh Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrospi_C/0/1/0/all/0/1\">Cesar Berrospi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinkla_K/0/1/0/all/0/1\">Kasper Dinkla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fusco_F/0/1/0/all/0/1\">Francesco Fusco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bothur_B/0/1/0/all/0/1\">Benedikt Bothur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lysak_M/0/1/0/all/0/1\">Maksym Lysak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livathinos_N/0/1/0/all/0/1\">Nikolaos Livathinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nassar_A/0/1/0/all/0/1\">Ahmed Nassar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vagenas_P/0/1/0/all/0/1\">Panagiotis Vagenas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morin_L/0/1/0/all/0/1\">Lucas Morin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auer_C/0/1/0/all/0/1\">Christoph Auer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolfi_M/0/1/0/all/0/1\">Michele Dolfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staar_P/0/1/0/all/0/1\">Peter Staar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grammatical Gender's Influence on Distributional Semantics: A Causal Perspective. (arXiv:2311.18567v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18567","description":"<p>How much meaning influences gender assignment across languages is an active\narea of research in modern linguistics and cognitive science. We can view\ncurrent approaches as aiming to determine where gender assignment falls on a\nspectrum, from being fully arbitrarily determined to being largely semantically\ndetermined. For the latter case, there is a formulation of the neo-Whorfian\nhypothesis, which claims that even inanimate noun gender influences how people\nconceive of and talk about objects (using the choice of adjective used to\nmodify inanimate nouns as a proxy for meaning). We offer a novel, causal\ngraphical model that jointly represents the interactions between a noun's\ngrammatical gender, its meaning, and adjective choice. In accordance with past\nresults, we find a relationship between the gender of nouns and the adjectives\nwhich modify them. However, when we control for the meaning of the noun, we\nfind that grammatical gender has a near-zero effect on adjective choice,\nthereby calling the neo-Whorfian hypothesis into question.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stanczak_K/0/1/0/all/0/1\">Karolina Sta&#x144;czak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_K/0/1/0/all/0/1\">Kevin Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality, Fairness, Toxicity. (arXiv:2311.18580v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18580","description":"<p>The widespread of generative artificial intelligence has heightened concerns\nabout the potential harms posed by AI-generated texts, primarily stemming from\nfactoid, unfair, and toxic content. Previous researchers have invested much\neffort in assessing the harmlessness of generative language models. However,\nexisting benchmarks are struggling in the era of large language models (LLMs),\ndue to the stronger language generation and instruction following capabilities,\nas well as wider applications. In this paper, we propose FFT, a new benchmark\nwith 2116 elaborated-designed instances, for LLM harmlessness evaluation with\nfactuality, fairness, and toxicity. To investigate the potential harms of LLMs,\nwe evaluate 9 representative LLMs covering various parameter scales, training\nstages, and creators. Experiments show that the harmlessness of LLMs is still\nunder-satisfactory, and extensive analysis derives some insightful findings\nthat could inspire future research for harmless LLM research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shiyao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tingwen Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArthModel: Enhance Arithmetic Skills to Large Language Model. (arXiv:2311.18609v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18609","description":"<p>With the great success of ChatGPT, the research of large language models has\nbecome increasingly popular. However, the models have several limitations, such\nas toxicity and pool performance of arithmetic solving. Meanwhile, LLM may have\nsome potential abilities that have yet to be exploited. In this paper, we\nchoose a different way to enhance the arithmetic ability of LLM. We propose to\ntrain LLM to generate a postfix expression related to the arithmetic problem\nand incorporate it with small pretrained models. Moreover, this small model\ntransfers the token embeddings into real dense numbers and invokes native\nfunctions of a deep learning platform to get the correct answer. To generate\nthe final result, we propose prompt injection for adding the result outputs by\nthe small model to LLM. This work provides different ways of thinking, training\nand using a language model. The codes and models will be released at\n\\url{https://github.com/eteced/arithmetic_finetuning_v1}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yingdi Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArcMMLU: A Library and Information Science Benchmark for Large Language Models. (arXiv:2311.18658v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18658","description":"<p>In light of the rapidly evolving capabilities of large language models\n(LLMs), it becomes imperative to develop rigorous domain-specific evaluation\nbenchmarks to accurately assess their capabilities. In response to this need,\nthis paper introduces ArcMMLU, a specialized benchmark tailored for the Library\n&amp; Information Science (LIS) domain in Chinese. This benchmark aims to measure\nthe knowledge and reasoning capability of LLMs within four key sub-domains:\nArchival Science, Data Science, Library Science, and Information Science.\nFollowing the format of MMLU/CMMLU, we collected over 6,000 high-quality\nquestions for the compilation of ArcMMLU. This extensive compilation can\nreflect the diverse nature of the LIS domain and offer a robust foundation for\nLLM evaluation. Our comprehensive evaluation reveals that while most mainstream\nLLMs achieve an average accuracy rate above 50% on ArcMMLU, there remains a\nnotable performance gap, suggesting substantial headroom for refinement in LLM\ncapabilities within the LIS domain. Further analysis explores the effectiveness\nof few-shot examples on model performance and highlights challenging questions\nwhere models consistently underperform, providing valuable insights for\ntargeted improvements. ArcMMLU fills a critical gap in LLM evaluations within\nthe Chinese LIS domain and paves the way for future development of LLMs\ntailored to this specialized area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shitou Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingshen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Ping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance. (arXiv:2311.18681v1 [cs.CV])","link":"http://arxiv.org/abs/2311.18681","description":"<p>Conversational AI tools that can generate and discuss clinically correct\nradiology reports for a given medical image have the potential to transform\nradiology. Such a human-in-the-loop radiology assistant could facilitate a\ncollaborative diagnostic process, thus saving time and improving the quality of\nreports. Towards this goal, we introduce RaDialog, the first thoroughly\nevaluated and publicly available large vision-language model for radiology\nreport generation and interactive dialog. RaDialog effectively integrates\nvisual image features and structured pathology findings with a large language\nmodel (LLM) while simultaneously adapting it to a specialized domain using\nparameter-efficient fine-tuning. To keep the conversational abilities of the\nunderlying LLM, we propose a comprehensive, semi-automatically labeled,\nimage-grounded instruct dataset for chest X-ray radiology tasks. By training\nwith this dataset, our method achieves state-of-the-art clinical correctness in\nreport generation and shows impressive abilities in interactive tasks such as\ncorrecting reports and answering questions, serving as a foundational step\ntoward clinical dialog systems. Our code is available on github:\nhttps://github.com/ChantalMP/RaDialog.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pellegrini_C/0/1/0/all/0/1\">Chantal Pellegrini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozsoy_E/0/1/0/all/0/1\">Ege &#xd6;zsoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keicher_M/0/1/0/all/0/1\">Matthias Keicher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable Evaluation of Large Language Model Generation. (arXiv:2311.18702v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18702","description":"<p>Since the natural language processing (NLP) community started to make large\nlanguage models (LLMs), such as GPT-4, act as a critic to evaluate the quality\nof generated texts, most of them only train a critique generation model of a\nspecific scale on specific datasets. We argue that a comprehensive\ninvestigation on the key factor of LLM-based evaluation models, such as scaling\nproperties, is lacking, so that it is still inconclusive whether these models\nhave potential to replace GPT-4's evaluation in practical scenarios. In this\npaper, we propose a new critique generation model called CritiqueLLM, which\nincludes a dialogue-based prompting method for high-quality referenced /\nreference-free evaluation data. Experimental results show that our model can\nachieve comparable evaluation performance to GPT-4 especially in system-level\ncorrelations, and even outperform GPT-4 in 3 out of 8 tasks in a challenging\nreference-free setting. We conduct detailed analysis to show promising scaling\nproperties of our model in the quality of generated critiques. We also\ndemonstrate that our generated critiques can act as scalable feedback to\ndirectly improve the generation quality of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1\">Pei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bosi Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhuoer Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1\">Xuanyu Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jiale Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Aohan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Women Are Beautiful, Men Are Leaders: Gender Stereotypes in Machine Translation and Language Modeling. (arXiv:2311.18711v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18711","description":"<p>We present GEST -- a new dataset for measuring gender-stereotypical reasoning\nin masked LMs and English-to-X machine translation systems. GEST contains\nsamples that are compatible with 9 Slavic languages and English for 16 gender\nstereotypes about men and women (e.g., Women are beautiful, Men are leaders).\nThe definition of said stereotypes was informed by gender experts. We used GEST\nto evaluate 11 masked LMs and 4 machine translation systems. We discovered\nsignificant and consistent amounts of stereotypical reasoning in almost all the\nevaluated models and languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pikuliak_M/0/1/0/all/0/1\">Mat&#xfa;&#x161; Pikuliak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hrckova_A/0/1/0/all/0/1\">Andrea Hrckova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oresko_S/0/1/0/all/0/1\">Stefan Oresko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simko_M/0/1/0/all/0/1\">Mari&#xe1;n &#x160;imko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoRec: An Easy Approach for Coordination Recognition. (arXiv:2311.18712v1 [cs.CL])","link":"http://arxiv.org/abs/2311.18712","description":"<p>In this paper, we observe and address the challenges of the coordination\nrecognition task. Most existing methods rely on syntactic parsers to identify\nthe coordinators in a sentence and detect the coordination boundaries. However,\nstate-of-the-art syntactic parsers are slow and suffer from errors, especially\nfor long and complicated sentences. To better solve the problems, we propose a\npipeline model COordination RECognizer (CoRec). It consists of two components:\ncoordinator identifier and conjunct boundary detector. The experimental results\non datasets from various domains demonstrate the effectiveness and efficiency\nof the proposed method. Further experiments show that CoRec positively impacts\ndownstream tasks, improving the yield of state-of-the-art Open IE models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1\">Haojie Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Wenfei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Functional Differentiation in JAX. (arXiv:2311.18727v1 [cs.PL])","link":"http://arxiv.org/abs/2311.18727","description":"<p>We extend JAX with the capability to automatically differentiate higher-order\nfunctions (functionals and operators). By representing functions as a\ngeneralization of arrays, we seamlessly use JAX's existing primitive system to\nimplement higher-order functions. We present a set of primitive operators that\nserve as foundational building blocks for constructing several key types of\nfunctionals. For every introduced primitive operator, we derive and implement\nboth linearization and transposition rules, aligning with JAX's internal\nprotocols for forward and reverse mode automatic differentiation. This\nenhancement allows for functional differentiation in the same syntax\ntraditionally use for functions. The resulting functional gradients are\nthemselves functions ready to be invoked in python. We showcase this tool's\nefficacy and simplicity through applications where functional derivatives are\nindispensable. The source code of this work is released at\nhttps://github.com/sail-sg/autofd .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Min Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Information Extraction by Predicting Relative Time-lines. (arXiv:1808.09401v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1808.09401","description":"<p>The current leading paradigm for temporal information extraction from text\nconsists of three phases: (1) recognition of events and temporal expressions,\n(2) recognition of temporal relations among them, and (3) time-line\nconstruction from the temporal relations. In contrast to the first two phases,\nthe last phase, time-line construction, received little attention and is the\nfocus of this work. In this paper, we propose a new method to construct a\nlinear time-line from a set of (extracted) temporal relations. But more\nimportantly, we propose a novel paradigm in which we directly predict start and\nend-points for events from the text, constituting a time-line without going\nthrough the intermediate step of prediction of temporal relations as in earlier\nwork. Within this paradigm, we propose two models that predict in linear\ncomplexity, and a new training loss using TimeML-style annotations, yielding\npromising results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leeuwenberg_A/0/1/0/all/0/1\">Artuur Leeuwenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-turn Response Selection using Dialogue Dependency Relations. (arXiv:2010.01502v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.01502","description":"<p>Multi-turn response selection is a task designed for developing dialogue\nagents. The performance on this task has a remarkable improvement with\npre-trained language models. However, these models simply concatenate the turns\nin dialogue history as the input and largely ignore the dependencies between\nthe turns. In this paper, we propose a dialogue extraction algorithm to\ntransform a dialogue history into threads based on their dependency relations.\nEach thread can be regarded as a self-contained sub-dialogue. We also propose\nThread-Encoder model to encode threads and candidates into compact\nrepresentations by pre-trained Transformers and finally get the matching score\nthrough an attention layer. The experiments show that dependency relations are\nhelpful for dialogue context understanding, and our model outperforms the\nstate-of-the-art baselines on both DSTC7 and DSTC8*, with competitive results\non UbuntuV2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Q/0/1/0/all/0/1\">Qi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yizhu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Siyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kenny Q. Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haifeng Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating More Pertinent Captions by Leveraging Semantics and Style on Multi-Source Datasets. (arXiv:2111.12727v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12727","description":"<p>This paper addresses the task of generating fluent descriptions by training\non a non-uniform combination of data sources, containing both human-annotated\nand web-collected captions. Large-scale datasets with noisy image-text pairs,\nindeed, provide a sub-optimal source of supervision because of their\nlow-quality descriptive style, while human-annotated datasets are cleaner but\nsmaller in scale. To get the best of both worlds, we propose to leverage and\nseparate semantics and descriptive style through the incorporation of a style\ntoken and keywords extracted through a retrieval component. The proposed model\navoids the need of object detectors, is trained with a single objective of\nprompt language modeling, and can replicate the style of human-collected\ncaptions while training on sources with different input styles. Experimentally,\nthe model shows a strong capability of recognizing real-world concepts and\nproducing high-quality captions. Extensive experiments are performed on\ndifferent image captioning datasets, including CC3M, nocaps, and the\ncompetitive COCO dataset, where our model consistently outperforms baselines\nand state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1\">Giuseppe Fiameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Handwriting recognition and automatic scoring for descriptive answers in Japanese language tests. (arXiv:2201.03215v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.03215","description":"<p>This paper presents an experiment of automatically scoring handwritten\ndescriptive answers in the trial tests for the new Japanese university entrance\nexamination, which were made for about 120,000 examinees in 2017 and 2018.\nThere are about 400,000 answers with more than 20 million characters. Although\nall answers have been scored by human examiners, handwritten characters are not\nlabeled. We present our attempt to adapt deep neural network-based handwriting\nrecognizers trained on a labeled handwriting dataset into this unlabeled answer\nset. Our proposed method combines different training strategies, ensembles\nmultiple recognizers, and uses a language model built from a large general\ncorpus to avoid overfitting into specific data. In our experiment, the proposed\nmethod records character accuracy of over 97% using about 2,000 verified\nlabeled answers that account for less than 0.5% of the dataset. Then, the\nrecognized answers are fed into a pre-trained automatic scoring system based on\nthe BERT model without correcting misrecognized characters and providing rubric\nannotations. The automatic scoring system achieves from 0.84 to 0.98 of\nQuadratic Weighted Kappa (QWK). As QWK is over 0.8, it represents an acceptable\nsimilarity of scoring between the automatic scoring system and the human\nexaminers. These results are promising for further research on end-to-end\nautomatic scoring of descriptive answers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hung Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cuong Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oka_H/0/1/0/all/0/1\">Haruki Oka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishioka_T/0/1/0/all/0/1\">Tsunenori Ishioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakagawa_M/0/1/0/all/0/1\">Masaki Nakagawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Semantic Faithfulness of Language Models via Input Intervention on Question Answering. (arXiv:2212.10696v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10696","description":"<p>Transformer-based language models have been shown to be highly effective for\nseveral NLP tasks. In this paper, we consider three transformer models, BERT,\nRoBERTa, and XLNet, in both small and large versions, and investigate how\nfaithful their representations are with respect to the semantic content of\ntexts. We formalize a notion of semantic faithfulness, in which the semantic\ncontent of a text should causally figure in a model's inferences in question\nanswering. We then test this notion by observing a model's behavior on\nanswering questions about a story after performing two novel semantic\ninterventions: deletion intervention and negation intervention. While\ntransformer models achieve high performance on standard question answering\ntasks, we show that they fail to be semantically faithful once we perform these\ninterventions for a significant number of cases (~50% for deletion\nintervention, and ~20% drop in accuracy for negation intervention). We then\npropose an intervention-based training regime that can mitigate the undesirable\neffects for deletion intervention by a significant margin (from ~ 50% to ~6%).\nWe analyze the inner-workings of the models to better understand the\neffectiveness of intervention-based training for deletion intervention. But we\nshow that this training does not attenuate other aspects of semantic\nunfaithfulness such as the models' inability to deal with negation intervention\nor to capture the predicate-argument structure of texts. We also test\nInstructGPT, via prompting, for its ability to handle the two interventions and\nto capture predicate-argument structure. While InstructGPT models do achieve\nvery high performance on predicate-argument structure task, they fail to\nrespond adequately to our deletion and negation interventions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_A/0/1/0/all/0/1\">Akshay Chaturvedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhar_S/0/1/0/all/0/1\">Swarnadeep Bhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Soumadeep Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garain_U/0/1/0/all/0/1\">Utpal Garain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asher_N/0/1/0/all/0/1\">Nicholas Asher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supporting Human-AI Collaboration in Auditing LLMs with LLMs. (arXiv:2304.09991v3 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2304.09991","description":"<p>Large language models are becoming increasingly pervasive and ubiquitous in\nsociety via deployment in sociotechnical systems. Yet these language models, be\nit for classification or generation, have been shown to be biased and behave\nirresponsibly, causing harm to people at scale. It is crucial to audit these\nlanguage models rigorously. Existing auditing tools leverage either or both\nhumans and AI to find failures. In this work, we draw upon literature in\nhuman-AI collaboration and sensemaking, and conduct interviews with research\nexperts in safe and fair AI, to build upon the auditing tool: AdaTest (Ribeiro\nand Lundberg, 2022), which is powered by a generative large language model\n(LLM). Through the design process we highlight the importance of sensemaking\nand human-AI communication to leverage complementary strengths of humans and\ngenerative models in collaborative auditing. To evaluate the effectiveness of\nthe augmented tool, AdaTest++, we conduct user studies with participants\nauditing two commercial language models: OpenAI's GPT-3 and Azure's sentiment\nanalysis model. Qualitative analysis shows that AdaTest++ effectively leverages\nhuman strengths such as schematization, hypothesis formation and testing.\nFurther, with our tool, participants identified a variety of failures modes,\ncovering 26 different topics over 2 tasks, that have been shown before in\nformal audits and also those previously under-reported.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_C/0/1/0/all/0/1\">Charvi Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Marco Tulio Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_N/0/1/0/all/0/1\">Nicholas King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nori_H/0/1/0/all/0/1\">Harsha Nori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amershi_S/0/1/0/all/0/1\">Saleema Amershi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Minimal Approach for Natural Language Action Space in Text-based Games. (arXiv:2305.04082v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.04082","description":"<p>Text-based games (TGs) are language-based interactive environments for\nreinforcement learning. While language models (LMs) and knowledge graphs (KGs)\nare commonly used for handling large action space in TGs, it is unclear whether\nthese techniques are necessary or overused. In this paper, we revisit the\nchallenge of exploring the action space in TGs and propose $\n\\epsilon$-admissible exploration, a minimal approach of utilizing admissible\nactions, for training phase. Additionally, we present a text-based actor-critic\n(TAC) agent that produces textual commands for game, solely from game\nobservations, without requiring any KG or LM. Our method, on average across 10\ngames from Jericho, outperforms strong baselines and state-of-the-art agents\nthat use LM and KG. Our approach highlights that a much lighter model design,\nwith a fresh perspective on utilizing the information within the environments,\nsuffices for an effective exploration of exponentially large action spaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ryu_D/0/1/0/all/0/1\">Dongwon Kelvin Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shareghi_E/0/1/0/all/0/1\">Ehsan Shareghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Chained Image-Language Model for Video Localization and Question Answering. (arXiv:2305.06988v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.06988","description":"<p>Recent studies have shown promising results on utilizing large pre-trained\nimage-language models for video question answering. While these image-language\nmodels can efficiently bootstrap the representation learning of video-language\nmodels, they typically concatenate uniformly sampled video frames as visual\ninputs without explicit language-aware, temporal modeling. When only a portion\nof a video input is relevant to the language query, such uniform frame sampling\ncan often lead to missing important visual cues. Although humans often find a\nvideo moment to focus on and rewind the moment to answer questions, training a\nquery-aware video moment localizer often requires expensive annotations and\nhigh computational costs. To address this issue, we propose Self-Chained Video\nLocalization-Answering (SeViLA), a novel framework that leverages a single\nimage-language model (BLIP-2) to tackle both temporal keyframe localization and\nQA on videos. SeViLA framework consists of two modules: Localizer and Answerer,\nwhere both are parameter-efficiently fine-tuned from BLIP-2. We propose two\nways of chaining these modules for cascaded inference and self-refinement.\nFirst, in the forward chain, the Localizer finds multiple language-aware\nkeyframes in a video, which the Answerer uses to predict the answer. Second, in\nthe reverse chain, the Answerer generates keyframe pseudo-labels to refine the\nLocalizer, alleviating the need for expensive video moment localization\nannotations. Our SeViLA framework outperforms several strong baselines on 5\nchallenging video QA and event prediction benchmarks, and achieves the\nstate-of-the-art in both fine-tuning (NExT-QA, STAR) and zero-shot (NExT-QA,\nSTAR, How2QA, VLEP) settings. We also analyze the impact of Localizer,\ncomparisons of Localizer with other temporal localization models,\npre-training/self-refinement of Localizer, and varying the number of keyframes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shoubin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1\">Prateek Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Editing Large Language Models: Problems, Methods, and Opportunities. (arXiv:2305.13172v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13172","description":"<p>Despite the ability to train capable LLMs, the methodology for maintaining\ntheir relevancy and rectifying errors remains elusive. To this end, the past\nfew years have witnessed a surge in techniques for editing LLMs, the objective\nof which is to efficiently alter the behavior of LLMs within a specific domain\nwithout negatively impacting performance across other inputs. This paper\nembarks on a deep exploration of the problems, methods, and opportunities\nrelated to model editing for LLMs. In particular, we provide an exhaustive\noverview of the task definition and challenges associated with model editing,\nalong with an in-depth empirical analysis of the most progressive methods\ncurrently at our disposal. We also build a new benchmark dataset to facilitate\na more robust evaluation and pinpoint enduring issues intrinsic to existing\ntechniques. Our objective is to provide valuable insights into the\neffectiveness and feasibility of each editing technique, thereby assisting the\ncommunity in making informed decisions on the selection of the most appropriate\nmethod for a specific task or context. Code and datasets are available at\nhttps://github.com/zjunlp/EasyEdit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1\">Bozhong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlling Pre-trained Language Models for Grade-Specific Text Simplification. (arXiv:2305.14993v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14993","description":"<p>Text simplification (TS) systems rewrite text to make it more readable while\npreserving its content. However, what makes a text easy to read depends on the\nintended readers. Recent work has shown that pre-trained language models can\nsimplify text using a wealth of techniques to control output simplicity,\nranging from specifying only the desired reading grade level, to directly\nspecifying low-level edit operations. Yet it remains unclear how to set these\ncontrol parameters in practice. Existing approaches set them at the corpus\nlevel, disregarding the complexity of individual inputs and considering only\none level of output complexity. In this work, we conduct an empirical study to\nunderstand how different control mechanisms impact the adequacy and simplicity\nof text simplification systems. Based on these insights, we introduce a simple\nmethod that predicts the edit operations required for simplifying a text for a\nspecific grade level on an instance-per-instance basis. This approach improves\nthe quality of the simplified outputs over corpus-level search-based\nheuristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sweta Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carpuat_M/0/1/0/all/0/1\">Marine Carpuat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ANPL: Towards Natural Programming with Interactive Decomposition. (arXiv:2305.18498v2 [cs.PL] UPDATED)","link":"http://arxiv.org/abs/2305.18498","description":"<p>Though LLMs are capable of generating plausible programs, it's challenging to\ninteract with the LLMs further to revise the program, especially if the user's\nspecific requirements are different from the initial proposal. In this paper,\nwe introduce ANPL, an interactive programming system that ensures users can\nalways refine the generated code towards their specific programmatic intents\nvia structured decompositions. Borrowing the paradigm of sketching from program\nsynthesis, an ANPL program consists of a set of input-outputs that it must\nsatisfy, a ``sketch'' -- control/data flow expressed in precise code (e.g.\nPython), and ``holes'' -- sub-modules to be implemented by the LLM specified\nwith natural language. The user revises an ANPL program by either modifying the\nsketch, changing the language used to describe the holes, or providing\nadditional input-outputs to a particular hole, turning it into a sub-ANPL\nprogram that can be solved recursively. This workflow allows the users to\noffload programming burdens to the LLM as much as possible while retaining the\nability to pinpoint and resolve bugs locally, without exposing the rest of the\nprogram to the LLM. We deploy ANPL on the Abstraction and Reasoning Corpus\n(ARC), a set of unique tasks that are challenging for state-of-the-art AI\nsystems, showing it outperforms baseline programming systems that (a) without\nthe ability to decompose tasks interactively and (b) without the guarantee that\nthe modules can be correctly composed together. Additional evaluations on APPS,\nHumanEval, and real-world programming tasks have validated that the ANPL\nframework is applicable to multiple programming domains. We release the ANPL\nsolutions to the ARC tasks as a dataset, providing insights into how humans\ndecompose novel tasks programmatically. See our code at\nhttps://iprc-dip.github.io/ANPL/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_Z/0/1/0/all/0/1\">Ziyuan Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Pengwei Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shaohui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuanbo Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zidong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1\">Yewen Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunji Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KL-Divergence Guided Temperature Sampling. (arXiv:2306.01286v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.01286","description":"<p>Temperature sampling is a conventional approach to diversify large language\nmodel predictions. As temperature increases, the prediction becomes diverse but\nalso vulnerable to hallucinations -- generating tokens that are sensible but\nnot factual. One common approach to mitigate hallucinations is to provide\nsource/grounding documents and the model is trained to produce predictions that\nbind to and are attributable to the provided source. It appears that there is a\ntrade-off between diversity and attribution. To mitigate any such trade-off, we\npropose to relax the constraint of having a fixed temperature over decoding\nsteps, and a mechanism to guide the dynamic temperature according to its\nrelevance to the source through KL-divergence. Our experiments justifies the\ntrade-off, and shows that our sampling algorithm outperforms the conventional\ntop-k and top-p algorithms in conversational question-answering and\nsummarization tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chung-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reitter_D/0/1/0/all/0/1\">David Reitter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aksitov_R/0/1/0/all/0/1\">Renat Aksitov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yun-Hsuan Sung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Quantifier Comprehension in Large Language Models: Another Example of Inverse Scaling. (arXiv:2306.07384v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.07384","description":"<p>With their increasing size, large language models (LLMs) are becoming\nincreasingly good at language understanding tasks. But even with high\nperformance on specific downstream task, LLMs fail at simple linguistic tests\nfor negation or quantifier understanding. Previous work on quantifier\nunderstanding in LLMs show inverse scaling in understanding few-type\nquantifiers. In this paper, we question the claims of of previous work and show\nthat it is a result of inappropriate testing methodology. We also present\nalternate methods to measure quantifier comprehension in LLMs and show that\nLLMs are able to better understand the difference between the meaning of\nfew-type and most-type quantifiers as their size increases, although they are\nnot particularly good at it. We also observe inverse scaling for most-type\nquantifier understanding, which is contrary to human psycho-linguistic\nexperiments and previous work, where the model's understanding of most-type\nquantifier gets worse as the model size increases. We do this evaluation on\nmodels ranging from 125M-175B parameters, which suggests that LLMs do not do as\nwell as expected with quantifiers. We also discuss the possible reasons for\nthis and the relevance of quantifier understanding in evaluating language\nunderstanding in LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akshat Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v4 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2306.08018","description":"<p>Large Language Models (LLMs), with their remarkable task-handling\ncapabilities and innovative outputs, have catalyzed significant advancements\nacross a spectrum of fields. However, their proficiency within specialized\ndomains such as biomolecular studies remains limited. To address this\nchallenge, we introduce Mol-Instructions, a comprehensive instruction dataset\ndesigned for the biomolecular domain. Mol-Instructions encompasses three key\ncomponents: molecule-oriented instructions, protein-oriented instructions, and\nbiomolecular text instructions. Each component aims to improve the\nunderstanding and prediction capabilities of LLMs concerning biomolecular\nfeatures and behaviors. Through extensive instruction tuning experiments on\nLLMs, we demonstrate the effectiveness of Mol-Instructions in enhancing large\nmodels' performance in the intricate realm of biomolecular studies, thus\nfostering progress in the biomolecular research community. Mol-Instructions is\npublicly available for ongoing research and will undergo regular updates to\nenhance its applicability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Fang_Y/0/1/0/all/0/1\">Yin Fang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liu_K/0/1/0/all/0/1\">Kangwei Liu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Huang_R/0/1/0/all/0/1\">Rui Huang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Fan_X/0/1/0/all/0/1\">Xiaohui Fan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing. (arXiv:2306.10012v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2306.10012","description":"<p>Text-guided image editing is widely needed in daily life, ranging from\npersonal use to professional applications such as Photoshop. However, existing\nmethods are either zero-shot or trained on an automatically synthesized\ndataset, which contains a high volume of noise. Thus, they still require lots\nof manual tuning to produce desirable outcomes in practice. To address this\nissue, we introduce MagicBrush (https://osu-nlp-group.github.io/MagicBrush/),\nthe first large-scale, manually annotated dataset for instruction-guided real\nimage editing that covers diverse scenarios: single-turn, multi-turn,\nmask-provided, and mask-free editing. MagicBrush comprises over 10K manually\nannotated triplets (source image, instruction, target image), which supports\ntrainining large-scale text-guided image editing models. We fine-tune\nInstructPix2Pix on MagicBrush and show that the new model can produce much\nbetter images according to human evaluation. We further conduct extensive\nexperiments to evaluate current image editing baselines from multiple\ndimensions including quantitative, qualitative, and human evaluations. The\nresults reveal the challenging nature of our dataset and the gap between\ncurrent baselines and real-world editing needs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_L/0/1/0/all/0/1\">Lingbo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focused Transformer: Contrastive Training for Context Scaling. (arXiv:2307.03170v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.03170","description":"<p>Large language models have an exceptional capability to incorporate new\ninformation in a contextual manner. However, the full potential of such an\napproach is often restrained due to a limitation in the effective context\nlength. One solution to this issue is to endow an attention layer with access\nto an external memory, which comprises of (key, value) pairs. Yet, as the\nnumber of documents increases, the proportion of relevant keys to irrelevant\nones decreases, leading the model to focus more on the irrelevant keys. We\nidentify a significant challenge, dubbed the distraction issue, where keys\nlinked to different semantic values might overlap, making them hard to\ndistinguish. To tackle this problem, we introduce the Focused Transformer\n(FoT), a technique that employs a training process inspired by contrastive\nlearning. This novel approach enhances the structure of the (key, value) space,\nenabling an extension of the context length. Our method allows for fine-tuning\npre-existing, large-scale models to lengthen their effective context. This is\ndemonstrated by our fine-tuning of $3B$ and $7B$ OpenLLaMA checkpoints. The\nresulting models, which we name LongLLaMA, exhibit advancements in tasks\nrequiring a long context. We further illustrate that our LongLLaMA models\nadeptly manage a $256 k$ context length for passkey retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tworkowski_S/0/1/0/all/0/1\">Szymon Tworkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staniszewski_K/0/1/0/all/0/1\">Konrad Staniszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacek_M/0/1/0/all/0/1\">Miko&#x142;aj Pacek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhuai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1\">Henryk Michalewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milos_P/0/1/0/all/0/1\">Piotr Mi&#x142;o&#x15b;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models as Black-Box Optimizers for Vision-Language Models. (arXiv:2309.05950v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.05950","description":"<p>Vision-language models (VLMs) pre-trained on web-scale datasets have\ndemonstrated remarkable capabilities on downstream tasks when fine-tuned with\nminimal data. However, many VLMs rely on proprietary data and are not\nopen-source, which restricts the use of white-box approaches for fine-tuning.\nAs such, we aim to develop a black-box approach to optimize VLMs through\nnatural language prompts, thereby avoiding the need to access model parameters,\nfeature embeddings, or even output logits. We propose employing chat-based LLMs\nto search for the best text prompt for VLMs. Specifically, we adopt an\nautomatic hill-climbing procedure that converges to an effective prompt by\nevaluating the performance of current prompts and asking LLMs to refine them\nbased on textual feedback, all within a conversational process without\nhuman-in-the-loop. In a challenging 1-shot image classification setup, our\nsimple approach surpasses the white-box continuous prompting method (CoOp) by\nan average of 1.5% across 11 datasets including ImageNet. Our approach also\noutperforms both human-engineered and LLM-generated prompts. We highlight the\nadvantage of conversational feedback that incorporates both positive and\nnegative prompts, suggesting that LLMs can utilize the implicit gradient\ndirection in textual feedback for a more efficient search. In addition, we find\nthat the text prompts generated through our strategy are not only more\ninterpretable but also transfer well across different VLM architectures in a\nblack-box manner. Lastly, we demonstrate our framework on a state-of-the-art\nblack-box VLM (DALL-E 3) for text-to-image optimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shihong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhiqiu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Samuel Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Ryan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_T/0/1/0/all/0/1\">Tiffany Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models as a Service: Overview of a New Paradigm and its Challenges. (arXiv:2309.16573v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2309.16573","description":"<p>Some of the most powerful language models currently are proprietary systems,\naccessible only via (typically restrictive) web or software programming\ninterfaces. This is the Language-Models-as-a-Service (LMaaS) paradigm. In\ncontrast with scenarios where full model access is available, as in the case of\nopen-source models, such closed-off language models present specific challenges\nfor evaluating, benchmarking, and testing them. This paper has two goals: on\nthe one hand, we delineate how the aforementioned challenges act as impediments\nto the accessibility, replicability, reliability, and trustworthiness of LMaaS.\nWe systematically examine the issues that arise from a lack of information\nabout language models for each of these four aspects. We conduct a detailed\nanalysis of existing solutions and put forth a number of considered\nrecommendations, and highlight the directions for future advancements. On the\nother hand, it serves as a comprehensive resource for existing knowledge on\ncurrent, major LMaaS, offering a synthesized overview of the licences and\ncapabilities their interfaces offer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malfa_E/0/1/0/all/0/1\">Emanuele La Malfa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrov_A/0/1/0/all/0/1\">Aleksandar Petrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieder_S/0/1/0/all/0/1\">Simon Frieder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinhuber_C/0/1/0/all/0/1\">Christoph Weinhuber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnell_R/0/1/0/all/0/1\">Ryan Burnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nazar_R/0/1/0/all/0/1\">Raza Nazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_A/0/1/0/all/0/1\">Anthony G. Cohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shadbolt_N/0/1/0/all/0/1\">Nigel Shadbolt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wooldridge_M/0/1/0/all/0/1\">Michael Wooldridge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SELF: Language-Driven Self-Evolution for Large Language Models. (arXiv:2310.00533v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.00533","description":"<p>Large Language Models (LLMs) have demonstrated remarkable versatility across\nvarious domains. To further advance LLMs, we propose 'SELF' (Self-Evolution\nwith Language Feedback), a novel approach that enables LLMs to self-improve\nthrough self-reflection, akin to human learning processes. SELF initiates with\na meta-skill learning process that equips the LLMs with capabilities for\nself-feedback and self-refinement. Subsequently, the model undergoes an\niterative process of self-evolution. In each iteration, it utilizes an\nunlabeled dataset of instructions to generate initial responses. These\nresponses are enhanced through self-feedback and self-refinement. The model is\nthen fine-tuned using this enhanced data. The model undergoes progressive\nimprovement through this iterative self-evolution process. Moreover, the SELF\nframework enables the model to apply self-refinement during inference, which\nfurther improves response quality. Our experiments in mathematics and general\ntasks demonstrate that SELF can enhance the capabilities of LLMs without human\nintervention. The SELF framework indicates a promising direction for the\nautonomous evolution of LLMs, transitioning them from passive information\nreceivers to active participants in their development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jianqiao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wanjun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenyong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baojun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Emergent Abilities with Infinite Resolution Evaluation. (arXiv:2310.03262v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.03262","description":"<p>The scientific scale-up of large language models (LLMs) necessitates a\ncomprehensive understanding of their scaling properties. However, the existing\nliterature on the scaling properties only yields an incomplete answer:\noptimization loss decreases predictably as the model size increases, in line\nwith established scaling law; yet no scaling law for task has been established\nand the task performances are far from predictable during scaling. Task\nperformances typically show minor gains on small models until they improve\ndramatically once models exceed a size threshold, exemplifying the ``emergent\nabilities''. In this study, we discover that small models, although they\nexhibit minor performance, demonstrate critical and consistent task performance\nimprovements that are not captured by conventional evaluation strategies due to\ninsufficient measurement resolution. To measure such improvements, we introduce\nPassUntil, an evaluation strategy with theoretically infinite resolution,\nthrough massive sampling in the decoding phase. With PassUntil, we conduct a\nquantitative investigation into the scaling law of task performance. The\ninvestigation contains two parts. Firstly, a strict task scaling law that is\nnot conventionally known to exist, is identified, enhancing the predictability\nof task performances. Remarkably, we are able to predict the performance of the\n2.4B model on code generation with merely 0.05\\% deviation before training\nstarts, which is the first systematic attempt to verify predictable scaling\nproposed by GPT-4's report. Secondly, we are able to study emergent abilities\nquantitatively. We identify a kind of accelerated emergence whose scaling curve\ncannot be fitted by standard scaling law function and has a increasing speed.\nWe then examine two hypothesis and imply that the ``multiple circuits\nhypothesis'' might be responsible for the accelerated emergence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengding Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinrong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chaoqun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zebin Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Guoyang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do pretrained Transformers Really Learn In-context by Gradient Descent?. (arXiv:2310.08540v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.08540","description":"<p>The emergence of In-Context Learning (ICL) in LLMs remains a significant\nphenomenon with little understanding. To explain ICL, recent studies try to\nshed light on ICL by connecting it to Gradient Descent (GD). However, the\nquestion is, do these hold up in practice in actual pre-trained models?\n</p>\n<p>We highlight the limiting assumptions in prior works that make their context\nconsiderably different from the practical context in which language models are\ntrained. For example, the theoretical hand-constructed weights used in these\nstudies have properties that don't match those of real LLMs. Furthermore, their\nexperimental verification uses \\emph{ICL objective} (training models explicitly\nfor ICL), which differs from the emergent ICL in the wild.\n</p>\n<p>We also look for evidence in real models. We observe that ICL and GD have\ndifferent sensitivity to the order in which they observe demonstrations.\nFinally, we probe and compare the ICL vs. GD hypothesis in a natural setting.\nWe conduct comprehensive empirical analyses on language models pre-trained on\nnatural data (LLaMa-7B). Our comparisons of three performance metrics highlight\nthe inconsistent behavior of ICL and GD as a function of various factors such\nas datasets, models, and the number of demonstrations. We observe that ICL and\nGD modify the output distribution of language models differently. These results\nindicate that the equivalence between ICL and GD remains an open hypothesis and\ncalls for further studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lingfeng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1\">Aayush Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Context Utilization in Summarization with Large Language Models. (arXiv:2310.10570v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.10570","description":"<p>Large language models (LLMs) excel in zero-shot abstractive summarization\ntasks, delivering fluent and pertinent summaries. Recent advancements have\nextended their capabilities to handle long-input contexts, surpassing token\nlimits of 100k. However, in the realm of multi-document question answering,\nlanguage models exhibit uneven utilization of their input context. They tend to\nfavor the initial and final segments, resulting in a U-shaped performance\npattern concerning where the answer is located within the input. This bias\nraises concerns, particularly in summarization tasks where crucial content may\nbe dispersed throughout the source document(s). This paper presents a\ncomprehensive investigation encompassing 10 datasets, 5 LLMs, and 5 evaluation\nmetrics to analyze how these models leverage their input for abstractive\nsummarization. Our findings reveal a pronounced bias towards the introductory\ncontent (and to a lesser extent, the final content), posing challenges for LLM\nperformance across a range of diverse summarization benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravaut_M/0/1/0/all/0/1\">Mathieu Ravaut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Locally Differentially Private Document Generation Using Zero Shot Prompting. (arXiv:2310.16111v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.16111","description":"<p>Numerous studies have highlighted the privacy risks associated with\npretrained large language models. In contrast, our research offers a unique\nperspective by demonstrating that pretrained large language models can\neffectively contribute to privacy preservation. We propose a locally\ndifferentially private mechanism called DP-Prompt, which leverages the power of\npretrained large language models and zero-shot prompting to counter author\nde-anonymization attacks while minimizing the impact on downstream utility.\nWhen DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5),\nwe observe a notable reduction in the success rate of de-anonymization attacks,\nshowing that it surpasses existing approaches by a considerable margin despite\nits simpler design. For instance, in the case of the IMDB dataset, DP-Prompt\n(with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving\na 46\\% reduction in author identification F1 score against static attackers and\na 26\\% reduction against adaptive attackers. We conduct extensive experiments\nacross six open-source large language models, ranging up to 7 billion\nparameters, to analyze various effects of the privacy-utility tradeoff.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Utpala_S/0/1/0/all/0/1\">Saiteja Utpala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin Yu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The impact of responding to patient messages with large language model assistance. (arXiv:2310.17703v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.17703","description":"<p>Documentation burden is a major contributor to clinician burnout, which is\nrising nationally and is an urgent threat to our ability to care for patients.\nArtificial intelligence (AI) chatbots, such as ChatGPT, could reduce clinician\nburden by assisting with documentation. Although many hospitals are actively\nintegrating such systems into electronic medical record systems, AI chatbots\nutility and impact on clinical decision-making have not been studied for this\nintended use. We are the first to examine the utility of large language models\nin assisting clinicians draft responses to patient questions. In our two-stage\ncross-sectional study, 6 oncologists responded to 100 realistic synthetic\ncancer patient scenarios and portal messages developed to reflect common\nmedical situations, first manually, then with AI assistance.\n</p>\n<p>We find AI-assisted responses were longer, less readable, but provided\nacceptable drafts without edits 58% of time. AI assistance improved efficiency\n77% of time, with low harm risk (82% safe). However, 7.7% unedited AI responses\ncould severely harm. In 31% cases, physicians thought AI drafts were\nhuman-written. AI assistance led to more patient education recommendations,\nfewer clinical actions than manual responses. Results show promise for AI to\nimprove clinician efficiency and patient care through assisting documentation,\nif used judiciously. Monitoring model outputs and human-AI interaction remains\ncrucial for safe implementation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guevara_M/0/1/0/all/0/1\">Marco Guevara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moningi_S/0/1/0/all/0/1\">Shalini Moningi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoebers_F/0/1/0/all/0/1\">Frank Hoebers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhalawani_H/0/1/0/all/0/1\">Hesham Elhalawani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kann_B/0/1/0/all/0/1\">Benjamin H. Kann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chipidza_F/0/1/0/all/0/1\">Fallon E. Chipidza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leeman_J/0/1/0/all/0/1\">Jonathan Leeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aerts_H/0/1/0/all/0/1\">Hugo J.W.L. Aerts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_T/0/1/0/all/0/1\">Timothy Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savova_G/0/1/0/all/0/1\">Guergana K. Savova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mak_R/0/1/0/all/0/1\">Raymond H. Mak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lustberg_M/0/1/0/all/0/1\">Maryam Lustberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afshar_M/0/1/0/all/0/1\">Majid Afshar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitterman_D/0/1/0/all/0/1\">Danielle S. Bitterman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Segment-to-Segment Framework for Simultaneous Sequence Generation. (arXiv:2310.17940v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.17940","description":"<p>Simultaneous sequence generation is a pivotal task for real-time scenarios,\nsuch as streaming speech recognition, simultaneous machine translation and\nsimultaneous speech translation, where the target sequence is generated while\nreceiving the source sequence. The crux of achieving high-quality generation\nwith low latency lies in identifying the optimal moments for generating,\naccomplished by learning a mapping between the source and target sequences.\nHowever, existing methods often rely on task-specific heuristics for different\nsequence types, limiting the model's capacity to adaptively learn the\nsource-target mapping and hindering the exploration of multi-task learning for\nvarious simultaneous tasks. In this paper, we propose a unified\nsegment-to-segment framework (Seg2Seg) for simultaneous sequence generation,\nwhich learns the mapping in an adaptive and unified manner. During the process\nof simultaneous generation, the model alternates between waiting for a source\nsegment and generating a target segment, making the segment serve as the\nnatural bridge between the source and target. To accomplish this, Seg2Seg\nintroduces a latent segment as the pivot between source to target and explores\nall potential source-target mappings via the proposed expectation training,\nthereby learning the optimal moments for generating. Experiments on multiple\nsimultaneous generation tasks demonstrate that Seg2Seg achieves\nstate-of-the-art performance and exhibits better generality across various\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do LLMs exhibit human-like response biases? A case study in survey design. (arXiv:2311.04076v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.04076","description":"<p>As large language models (LLMs) become more capable, there is growing\nexcitement about the possibility of using LLMs as proxies for humans in\nreal-world tasks where subjective labels are desired, such as in surveys and\nopinion polling. One widely-cited barrier to the adoption of LLMs is their\nsensitivity to prompt wording - but interestingly, humans also display\nsensitivities to instruction changes in the form of response biases. As such,\nwe argue that if LLMs are going to be used to approximate human opinions, it is\nnecessary to investigate the extent to which LLMs also reflect human response\nbiases, if at all. In this work, we use survey design as a case study, where\nhuman response biases caused by permutations in wordings of \"prompts\" have been\nextensively studied. Drawing from prior work in social psychology, we design a\ndataset and propose a framework to evaluate whether LLMs exhibit human-like\nresponse biases in survey questionnaires. Our comprehensive evaluation of nine\nmodels shows that popular open and commercial LLMs generally fail to reflect\nhuman-like behavior. These inconsistencies tend to be more prominent in models\nthat have been instruction fine-tuned. Furthermore, even if a model shows a\nsignificant change in the same direction as humans, we find that perturbations\nthat are not meant to elicit significant changes in humans may also result in a\nsimilar change. These results highlight the potential pitfalls of using LLMs to\nsubstitute humans in parts of the annotation pipeline, and further underscore\nthe importance of finer-grained characterizations of model behavior. Our code,\ndataset, and collected samples are available at\nhttps://github.com/lindiatjuatja/BiasMonkey\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tjuatja_L/0/1/0/all/0/1\">Lindia Tjuatja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_V/0/1/0/all/0/1\">Valerie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Sherry Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1\">Ameet Talwalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-powered Applications. (arXiv:2311.08592v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2311.08592","description":"<p>Adversarial testing of large language models (LLMs) is crucial for their safe\nand responsible deployment. We introduce a novel approach for automated\ngeneration of adversarial evaluation datasets to test the safety of LLM\ngenerations on new downstream applications. We call it AI-assisted Red-Teaming\n(AART) - an automated alternative to current manual red-teaming efforts. AART\noffers a data generation and augmentation pipeline of reusable and customizable\nrecipes that reduce human effort significantly and enable integration of\nadversarial testing earlier in new product development. AART generates\nevaluation datasets with high diversity of content characteristics critical for\neffective adversarial testing (e.g. sensitive and harmful concepts, specific to\na wide range of cultural and geographic regions and application scenarios). The\ndata generation is steered by AI-assisted recipes to define, scope and\nprioritize diversity within the application context. This feeds into a\nstructured LLM-generation process that scales up evaluation priorities.\nCompared to some state-of-the-art tools, AART shows promising results in terms\nof concept coverage and data quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Radharapu_B/0/1/0/all/0/1\">Bhaktipriya Radharapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_K/0/1/0/all/0/1\">Kevin Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aroyo_L/0/1/0/all/0/1\">Lora Aroyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahoti_P/0/1/0/all/0/1\">Preethi Lahoti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Baseline Analysis of Reward Models' Ability To Accurately Analyze Foundation Models Under Distribution Shift. (arXiv:2311.14743v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.14743","description":"<p>Foundation models, specifically Large Language Models (LLM's), have lately\ngained wide-spread attention and adoption. Reinforcement Learning with Human\nFeedback (RLHF) involves training a reward model to capture desired behaviors,\nwhich is then used to align an LLM. These reward models are additionally used\nat inference-time to estimate how well LLM responses adhere to those desired\nbehaviors. However, there is little work measuring how robust these reward\nmodels are to distribution shifts. In this work, we evaluate how reward model\nperformance - measured via accuracy and calibration (i.e. alignment between\naccuracy and confidence) - is affected by distribution shift. We show novel\ncalibration patterns and accuracy drops due to OOD prompts and responses, and\nthat the reward model is more sensitive to shifts in responses than prompts.\nAdditionally, we adapt an OOD detection technique commonly used in\nclassification to the reward model setting in order to detect these\ndistribution shifts in prompts and responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pikus_B/0/1/0/all/0/1\">Ben Pikus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeVine_W/0/1/0/all/0/1\">Will LeVine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tony Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendryx_S/0/1/0/all/0/1\">Sean Hendryx</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLMs for Science: Usage for Code Generation and Data Analysis. (arXiv:2311.16733v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2311.16733","description":"<p>Large language models (LLMs) have been touted to enable increased\nproductivity in many areas of today's work life. Scientific research as an area\nof work is no exception: the potential of LLM-based tools to assist in the\ndaily work of scientists has become a highly discussed topic across\ndisciplines. However, we are only at the very onset of this subject of study.\nIt is still unclear how the potential of LLMs will materialise in research\npractice. With this study, we give first empirical evidence on the use of LLMs\nin the research process. We have investigated a set of use cases for LLM-based\ntools in scientific research, and conducted a first study to assess to which\ndegree current tools are helpful. In this paper we report specifically on use\ncases related to software engineering, such as generating application code and\ndeveloping scripts for data analytics. While we studied seemingly simple use\ncases, results across tools differ significantly. Our results highlight the\npromise of LLM-based tools in general, yet we also observe various issues,\nparticularly regarding the integrity of the output these tools provide.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nejjar_M/0/1/0/all/0/1\">Mohamed Nejjar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zacharias_L/0/1/0/all/0/1\">Luca Zacharias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiehle_F/0/1/0/all/0/1\">Fabian Stiehle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_I/0/1/0/all/0/1\">Ingo Weber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Falcon Series of Open Language Models. (arXiv:2311.16867v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.16867","description":"<p>We introduce the Falcon series: 7B, 40B, and 180B parameters causal\ndecoder-only models trained on a diverse high-quality corpora predominantly\nassembled from web data. The largest model, Falcon-180B, has been trained on\nover 3.5 trillion tokens of text--the largest openly documented pretraining\nrun. Falcon-180B significantly outperforms models such as PaLM or Chinchilla,\nand improves upon concurrently developed models such as LLaMA 2 or\nInflection-1. It nears the performance of PaLM-2-Large at a reduced pretraining\nand inference cost, making it, to our knowledge, one of the three best language\nmodels in the world along with GPT-4 and PaLM-2-Large. We report detailed\nevaluations, as well as a deep dive into the methods and custom tooling\nemployed to pretrain Falcon. Notably, we report on our custom distributed\ntraining codebase, allowing us to efficiently pretrain these models on up to\n4,096 A100s on cloud AWS infrastructure with limited interconnect. We release a\n600B tokens extract of our web dataset, as well as the Falcon-7/40/180B models\nunder a permissive license to foster open-science and accelerate the\ndevelopment of an open ecosystem of large language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Almazrouei_E/0/1/0/all/0/1\">Ebtesam Almazrouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alobeidli_H/0/1/0/all/0/1\">Hamza Alobeidli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshamsi_A/0/1/0/all/0/1\">Abdulaziz Alshamsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cappelli_A/0/1/0/all/0/1\">Alessandro Cappelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cojocaru_R/0/1/0/all/0/1\">Ruxandra Cojocaru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Debbah_M/0/1/0/all/0/1\">M&#xe9;rouane Debbah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goffinet_E/0/1/0/all/0/1\">&#xc9;tienne Goffinet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hesslow_D/0/1/0/all/0/1\">Daniel Hesslow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Launay_J/0/1/0/all/0/1\">Julien Launay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malartic_Q/0/1/0/all/0/1\">Quentin Malartic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazzotta_D/0/1/0/all/0/1\">Daniele Mazzotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noune_B/0/1/0/all/0/1\">Badreddine Noune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pannier_B/0/1/0/all/0/1\">Baptiste Pannier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Penedo_G/0/1/0/all/0/1\">Guilherme Penedo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention. (arXiv:2311.17400v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.17400","description":"<p>Transformer-based models, such as BERT and GPT, have been widely adopted in\nnatural language processing (NLP) due to their exceptional performance.\nHowever, recent studies show their vulnerability to textual adversarial attacks\nwhere the model's output can be misled by intentionally manipulating the text\ninputs. Despite various methods that have been proposed to enhance the model's\nrobustness and mitigate this vulnerability, many require heavy consumption\nresources (e.g., adversarial training) or only provide limited protection\n(e.g., defensive dropout). In this paper, we propose a novel method called\ndynamic attention, tailored for the transformer architecture, to enhance the\ninherent robustness of the model itself against various adversarial attacks.\nOur method requires no downstream task knowledge and does not incur additional\ncosts. The proposed dynamic attention consists of two modules: (I) attention\nrectification, which masks or weakens the attention value of the chosen tokens,\nand (ii) dynamic modeling, which dynamically builds the set of candidate\ntokens. Extensive experiments demonstrate that dynamic attention significantly\nmitigates the impact of adversarial attacks, improving up to 33\\% better\nperformance than previous methods against widely-used adversarial attacks. The\nmodel-level design of dynamic attention enables it to be easily combined with\nother defense methods (e.g., adversarial training) to further enhance the\nmodel's robustness. Furthermore, we demonstrate that dynamic attention\npreserves the state-of-the-art robustness space of the original model compared\nto other dynamic modeling methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lujia Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1\">Yuwen Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changjiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1\">Chunpeng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Ting Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLOMO: Counterfactual Logical Modification with Large Language Models. (arXiv:2311.17438v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.17438","description":"<p>In this study, we delve into the realm of counterfactual reasoning\ncapabilities of large language models (LLMs). Our primary objective is to\ncultivate the counterfactual thought processes within LLMs and rigorously\nassess these processes for their validity. Specifically, we introduce a novel\ntask, Counterfactual Logical Modification (CLOMO), and a high-quality\nhuman-annotated benchmark. In this task, LLMs must adeptly alter a given\nargumentative text to uphold a predetermined logical relationship. To\neffectively evaluate a generation model's counterfactual capabilities, we\npropose an innovative evaluation metric, the LogicAware Counterfactual Score to\ndirectly evaluate the natural language output of LLMs instead of modeling the\ntask as a multiple-choice problem. Analysis shows that the proposed automatic\nmetric aligns well with human preference. Our experimental results show that\nwhile LLMs demonstrate a notable capacity for logical counterfactual thinking,\nthere remains a discernible gap between their current abilities and human\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yinya Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1\">Ruixin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhicheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linqi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Build an AI Tutor that Can Adapt to Any Course and Provide Accurate Answers Using Large Language Model and Retrieval-Augmented Generation. (arXiv:2311.17696v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.17696","description":"<p>Artificial intelligence is transforming education through data-driven,\npersonalized learning solutions. This paper introduces AI Tutor, an innovative\nweb application that provides personalized tutoring in any subject using\nstate-of-the-art Large Language Model (LLM). AI Tutor ingests course materials\nto construct an adaptive knowledge base tailored to the course. When students\npose questions, it retrieves the most relevant information and generates\ndetailed, conversational responses citing supporting evidence. The system is\npowered by advanced large language models and Retrieval-Augmented Generation\n(RAG) techniques for accurate, natural question answering. We present a\nfully-functional web interface and video demonstration that showcase AI Tutor's\nversatility across diverse subjects and its ability to produce pedagogically\ncogent responses. While an initial prototype, this work represents a pioneering\nstep toward AI-enabled tutoring systems that can democratize access to\nhigh-quality, customized educational support.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chenxi Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Pursuit Prompting for Zero-Shot Multimodal Synthesis. (arXiv:2311.17898v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2311.17898","description":"<p>Hallucinations and unfaithful synthesis due to inaccurate prompts with\ninsufficient semantic details are widely observed in multimodal generative\nmodels. A prevalent strategy to align multiple modalities is to fine-tune the\ngenerator with a large number of annotated text-image pairs. However, such a\nprocedure is labor-consuming and resource-draining. The key question we ask is:\ncan we enhance the quality and faithfulness of text-driven generative models\nbeyond extensive text-image pair annotations? To address this question, we\npropose Knowledge Pursuit Prompting (KPP), a zero-shot framework that\niteratively incorporates external knowledge to help generators produce reliable\nvisual content. Instead of training generators to handle generic prompts, KPP\nemploys a recursive knowledge query process to gather informative external\nfacts from the knowledge base, instructs a language model to compress the\nacquired knowledge for prompt refinement, and utilizes text-driven generators\nfor visual synthesis. The entire process is zero-shot, without accessing the\narchitectures and parameters of generative models. We evaluate the framework\nacross multiple text-driven generative tasks (image, 3D rendering, and video)\non datasets of different domains. We further demonstrate the extensibility and\nadaptability of KPP through varying foundation model bases and instructions.\nOur results show that KPP is capable of generating faithful and semantically\nrich content across diverse visual domains, offering a promising solution to\nimprove multimodal generative models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jinqi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1\">Kwan Ho Ryan Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimos_D/0/1/0/all/0/1\">Dimitris Dimos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1\">Ren&#xe9; Vidal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-11-30T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}