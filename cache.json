{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-12-29T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Investigating salient representations and label Variance in Dimensional Speech Emotion Analysis. (arXiv:2312.16180v1 [cs.SD])","link":"http://arxiv.org/abs/2312.16180","description":"<p>Representations derived from models such as BERT (Bidirectional Encoder\nRepresentations from Transformers) and HuBERT (Hidden units BERT), have helped\nto achieve state-of-the-art performance in dimensional speech emotion\nrecognition. Despite their large dimensionality, and even though these\nrepresentations are not tailored for emotion recognition tasks, they are\nfrequently used to train large speech emotion models with high memory and\ncomputational costs. In this work, we show that there exist lower-dimensional\nsubspaces within the these pre-trained representational spaces that offer a\nreduction in downstream model complexity without sacrificing performance on\nemotion estimation. In addition, we model label uncertainty in the form of\ngrader opinion variance, and demonstrate that such information can improve the\nmodels generalization capacity and robustness. Finally, we compare the\nrobustness of the emotion models against acoustic degradations and observed\nthat the reduced dimensional representations were able to retain the\nperformance similar to the full-dimensional representations without significant\nregression in dimensional emotion performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitra_V/0/1/0/all/0/1\">Vikramjit Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jingping Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azemi_E/0/1/0/all/0/1\">Erdrin Azemi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chatbot is Not All You Need: Information-rich Prompting for More Realistic Responses. (arXiv:2312.16233v1 [cs.CL])","link":"http://arxiv.org/abs/2312.16233","description":"<p>Recent Large Language Models (LLMs) have shown remarkable capabilities in\nmimicking fictional characters or real humans in conversational settings.\nHowever, the realism and consistency of these responses can be further enhanced\nby providing richer information of the agent being mimicked. In this paper, we\npropose a novel approach to generate more realistic and consistent responses\nfrom LLMs, leveraging five senses, attributes, emotional states, relationship\nwith the interlocutor, and memories. By incorporating these factors, we aim to\nincrease the LLM's capacity for generating natural and realistic reactions in\nconversational exchanges. Through our research, we expect to contribute to the\ndevelopment of LLMs that demonstrate improved capabilities in mimicking\nfictional characters. We release a new benchmark dataset and all our codes,\nprompts, and sample results on our Github:\nhttps://github.com/srafsasm/InfoRichBot\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1\">Seokhoon Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makhmud_A/0/1/0/all/0/1\">Assentay Makhmud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"More than Correlation: Do Large Language Models Learn Causal Representations of Space?. (arXiv:2312.16257v1 [cs.CL])","link":"http://arxiv.org/abs/2312.16257","description":"<p>Recent work found high mutual information between the learned representations\nof large language models (LLMs) and the geospatial property of its input,\nhinting an emergent internal model of space. However, whether this internal\nspace model has any causal effects on the LLMs' behaviors was not answered by\nthat work, led to criticism of these findings as mere statistical correlation.\nOur study focused on uncovering the causality of the spatial representations in\nLLMs. In particular, we discovered the potential spatial representations in\nDeBERTa, GPT-Neo using representational similarity analysis and linear and\nnon-linear probing. Our casual intervention experiments showed that the spatial\nrepresentations influenced the model's performance on next word prediction and\na downstream task that relies on geospatial information. Our experiments\nsuggested that the LLMs learn and use an internal model of space in solving\ngeospatial related tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yida Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1\">Yixian Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sijia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Li Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaohan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Observable Propagation: A Data-Efficient Approach to Uncover Feature Vectors in Transformers. (arXiv:2312.16291v1 [cs.LG])","link":"http://arxiv.org/abs/2312.16291","description":"<p>A key goal of current mechanistic interpretability research in NLP is to find\nlinear features (also called \"feature vectors\") for transformers: directions in\nactivation space corresponding to concepts that are used by a given model in\nits computation. Present state-of-the-art methods for finding linear features\nrequire large amounts of labelled data -- both laborious to acquire and\ncomputationally expensive to utilize. In this work, we introduce a novel\nmethod, called \"observable propagation\" (in short: ObsProp), for finding linear\nfeatures used by transformer language models in computing a given task -- using\nalmost no data. Our paradigm centers on the concept of observables, linear\nfunctionals corresponding to given tasks. We then introduce a mathematical\ntheory for the analysis of feature vectors: we provide theoretical motivation\nfor why LayerNorm nonlinearities do not affect the direction of feature\nvectors; we also introduce a similarity metric between feature vectors called\nthe coupling coefficient which estimates the degree to which one feature's\noutput correlates with another's. We use ObsProp to perform extensive\nqualitative investigations into several tasks, including gendered occupational\nbias, political party prediction, and programming language detection. Our\nresults suggest that ObsProp surpasses traditional approaches for finding\nfeature vectors in the low-data regime, and that ObsProp can be used to better\nunderstand the mechanisms responsible for bias in large language models. Code\nfor experiments can be found at github.com/jacobdunefsky/ObservablePropagation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dunefsky_J/0/1/0/all/0/1\">Jacob Dunefsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contribuci\\'on de la sem\\'antica combinatoria al desarrollo de herramientas digitales multiling\\\"ues. (arXiv:2312.16309v1 [cs.CL])","link":"http://arxiv.org/abs/2312.16309","description":"<p>This paper describes how the field of Combinatorial Semantics has contributed\nto the design of three prototypes for the automatic generation of argument\npatterns in nominal phrases in Spanish, French and German (Xera, Combinatoria\nand CombiContext). It also shows the importance of knowing about the argument\nsyntactic-semantic interface in a production situation in the context of\nforeign languages. After a descriptive section on the design, typologie and\ninformation levels of the resources, there follows an explanation of the\ncentral role of the combinatorial meaning (roles and ontological features). The\nstudy deals with different semantic f ilters applied in the selection,\norganization and expansion of the lexicon, being these key pieces for the\ngeneration of grammatically correct and semantically acceptable mono- and\nbiargumental nominal phrases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vazquez_M/0/1/0/all/0/1\">Mar&#xed;a Jos&#xe9; Dom&#xed;nguez V&#xe1;zquez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zur Darstellung eines mehrstufigen Prototypbegriffs in der multilingualen automatischen Sprachgenerierung: vom Korpus \\\"uber word embeddings bis hin zum automatischen W\\\"orterbuch. (arXiv:2312.16311v1 [cs.CL])","link":"http://arxiv.org/abs/2312.16311","description":"<p>The multilingual dictionary of noun valency Portlex is considered to be the\ntrigger for the creation of the automatic language generators Xera and\nCombinatoria, whose development and use is presented in this paper. Both\nprototypes are used for the automatic generation of nominal phrases with their\nmono- and bi-argumental valence slots, which could be used, among others, as\ndictionary examples or as integrated components of future autonomous\nE-Learning-Tools. As samples for new types of automatic valency dictionaries\nincluding user interaction, we consider the language generators as we know them\ntoday. In the specific methodological procedure for the development of the\nlanguage generators, the syntactic-semantic description of the noun slots turns\nout to be the main focus from a syntagmatic and paradigmatic point of view.\nAlong with factors such as representativeness, grammatical correctness,\nsemantic coherence, frequency and the variety of lexical candidates, as well as\nsemantic classes and argument structures, which are fixed components of both\nresources, a concept of a multi-sided prototype stands out. The combined\napplication of this prototype concept as well as of word embeddings together\nwith techniques from the field of automatic natural language processing and\ngeneration (NLP and NLG) opens up a new way for the future development of\nautomatically generated plurilingual valency dictionaries. All things\nconsidered, the paper depicts the language generators both from the point of\nview of their development as well as from that of the users. The focus lies on\nthe role of the prototype concept within the development of the resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vazquez_M/0/1/0/all/0/1\">Mar&#xed;a Jos&#xe9; Dom&#xed;nguez V&#xe1;zquez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task Contamination: Language Models May Not Be Few-Shot Anymore. (arXiv:2312.16337v1 [cs.CL])","link":"http://arxiv.org/abs/2312.16337","description":"<p>Large language models (LLMs) offer impressive performance in various\nzero-shot and few-shot tasks. However, their success in zero-shot and few-shot\nsettings may be affected by task contamination, a potential limitation that has\nnot been thoroughly examined. This paper investigates how zero-shot and\nfew-shot performance of LLMs has changed chronologically over time. Utilizing\nGPT-3 series models and several other recent open-sourced LLMs, and controlling\nfor dataset difficulty, we find that on datasets released before the LLM\ntraining data creation date, LLMs perform surprisingly better than on datasets\nreleased after. This strongly indicates that, for many LLMs, there exists task\ncontamination on zero-shot and few-shot evaluation for datasets released prior\nto the LLMs' training data creation date. Additionally, we utilize training\ndata inspection, task example extraction, and a membership inference attack,\nwhich reveal further evidence of task contamination. Importantly, we find that\nfor classification tasks with no possibility of task contamination, LLMs rarely\ndemonstrate statistically significant improvements over simple majority\nbaselines, in both zero and few-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changmao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flanigan_J/0/1/0/all/0/1\">Jeffrey Flanigan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM Polygraph: Uncovering LLMs' Factual Discernment through Intermediate Data Analysis. (arXiv:2312.16374v1 [cs.CL])","link":"http://arxiv.org/abs/2312.16374","description":"<p>Large Language Models (LLMs) have revolutionized various domains with\nextensive knowledge and creative capabilities. However, a critical issue with\nLLMs is their tendency to produce outputs that diverge from factual reality.\nThis phenomenon is particularly concerning in sensitive applications such as\nmedical consultation and legal advice, where accuracy is paramount. In this\npaper, we introduce the LLM factoscope, a novel Siamese network-based model\nthat leverages the inner states of LLMs for factual detection. Our\ninvestigation reveals distinguishable patterns in LLMs' inner states when\ngenerating factual versus non-factual content. We demonstrate the LLM\nfactoscope's effectiveness across various architectures, achieving over 96%\naccuracy in factual detection. Our work opens a new avenue for utilizing LLMs'\ninner states for factual detection and encourages further exploration into\nLLMs' inner workings for enhanced reliability and transparency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jinwen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yujia Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zijin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chengan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yue Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automating Knowledge Acquisition for Content-Centric Cognitive Agents Using LLMs. (arXiv:2312.16378v1 [cs.CL])","link":"http://arxiv.org/abs/2312.16378","description":"<p>The paper describes a system that uses large language model (LLM) technology\nto support the automatic learning of new entries in an intelligent agent's\nsemantic lexicon. The process is bootstrapped by an existing non-toy lexicon\nand a natural language generator that converts formal, ontologically-grounded\nrepresentations of meaning into natural language sentences. The learning method\ninvolves a sequence of LLM requests and includes an automatic quality control\nstep. To date, this learning method has been applied to learning multiword\nexpressions whose meanings are equivalent to those of transitive verbs in the\nagent's lexicon. The experiment demonstrates the benefits of a hybrid learning\narchitecture that integrates knowledge-based methods and resources with both\ntraditional data analytics and LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oruganti_S/0/1/0/all/0/1\">Sanjay Oruganti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nirenburg_S/0/1/0/all/0/1\">Sergei Nirenburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+English_J/0/1/0/all/0/1\">Jesse English</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McShane_M/0/1/0/all/0/1\">Marjorie McShane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer and Alignment Network for Generalized Category Discovery. (arXiv:2312.16467v1 [cs.CL])","link":"http://arxiv.org/abs/2312.16467","description":"<p>Generalized Category Discovery is a crucial real-world task. Despite the\nimproved performance on known categories, current methods perform poorly on\nnovel categories. We attribute the poor performance to two reasons: biased\nknowledge transfer between labeled and unlabeled data and noisy representation\nlearning on the unlabeled data. To mitigate these two issues, we propose a\nTransfer and Alignment Network (TAN), which incorporates two knowledge transfer\nmechanisms to calibrate the biased knowledge and two feature alignment\nmechanisms to learn discriminative features. Specifically, we model different\ncategories with prototypes and transfer the prototypes in labeled data to\ncorrect model bias towards known categories. On the one hand, we pull instances\nwith known categories in unlabeled data closer to these prototypes to form more\ncompact clusters and avoid boundary overlap between known and novel categories.\nOn the other hand, we use these prototypes to calibrate noisy prototypes\nestimated from unlabeled data based on category similarities, which allows for\nmore accurate estimation of prototypes for novel categories that can be used as\nreliable learning targets later. After knowledge transfer, we further propose\ntwo feature alignment mechanisms to acquire both instance- and category-level\nknowledge from unlabeled data by aligning instance features with both augmented\nfeatures and the calibrated prototypes, which can boost model performance on\nboth known and novel categories with less noise. Experiments on three benchmark\ndatasets show that our model outperforms SOTA methods, especially on novel\ncategories. Theoretical analysis is provided for an in-depth understanding of\nour model in general. Our code and data are available at\nhttps://github.com/Lackel/TAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1\">Wenbin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_F/0/1/0/all/0/1\">Feng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Wenkai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yaqiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qianying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Ping Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Source Code is a Graph, Not a Sequence: A Cross-Lingual Perspective on Code Clone Detection. (arXiv:2312.16488v1 [cs.CL])","link":"http://arxiv.org/abs/2312.16488","description":"<p>Source code clone detection is the task of finding code fragments that have\nthe same or similar functionality, but may differ in syntax or structure. This\ntask is important for software maintenance, reuse, and quality assurance (Roy\net al. 2009). However, code clone detection is challenging, as source code can\nbe written in different languages, domains, and styles. In this paper, we argue\nthat source code is inherently a graph, not a sequence, and that graph-based\nmethods are more suitable for code clone detection than sequence-based methods.\nWe compare the performance of two state-of-the-art models: CodeBERT (Feng et\nal. 2020), a sequence-based model, and CodeGraph (Yu et al. 2023), a\ngraph-based model, on two benchmark data-sets: BCB (Svajlenko et al. 2014) and\nPoolC (PoolC no date). We show that CodeGraph outperforms CodeBERT on both\ndata-sets, especially on cross-lingual code clones. To the best of our\nknowledge, this is the first work to demonstrate the superiority of graph-based\nmethods over sequence-based methods on cross-lingual code clone detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Mohammed Ataaur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ive_J/0/1/0/all/0/1\">Julia Ive</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding News Creation Intents: Frame, Dataset, and Method. (arXiv:2312.16490v1 [cs.CL])","link":"http://arxiv.org/abs/2312.16490","description":"<p>As the disruptive changes in the media economy and the proliferation of\nalternative news media outlets, news intent has progressively deviated from\nethical standards that serve the public interest. News intent refers to the\npurpose or intention behind the creation of a news article. While the\nsignificance of research on news intent has been widely acknowledged, the\nabsence of a systematic news intent understanding framework hinders further\nexploration of news intent and its downstream applications. To bridge this gap,\nwe propose News INTent (NINT) frame, the first component-aware formalism for\nunderstanding the news creation intent based on research in philosophy,\npsychology, and cognitive science. Within this frame, we define the news intent\nidentification task and provide a benchmark dataset with fine-grained labels\nalong with an efficient benchmark method. Experiments demonstrate that NINT is\nbeneficial in both the intent identification task and downstream tasks that\ndemand a profound understanding of news. This work marks a foundational step\ntowards a more systematic exploration of news creation intents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhengjia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Danding Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1\">Qiang Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Silong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Beizhe Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Siyuan Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S2M: Converting Single-Turn to Multi-Turn Datasets for Conversational Question Answering. (arXiv:2312.16511v1 [cs.CL])","link":"http://arxiv.org/abs/2312.16511","description":"<p>Supplying data augmentation to conversational question answering (CQA) can\neffectively improve model performance. However, there is less improvement from\nsingle-turn datasets in CQA due to the distribution gap between single-turn and\nmulti-turn datasets. On the other hand, while numerous single-turn datasets are\navailable, we have not utilized them effectively. To solve this problem, we\npropose a novel method to convert single-turn datasets to multi-turn datasets.\nThe proposed method consists of three parts, namely, a QA pair Generator, a QA\npair Reassembler, and a question Rewriter. Given a sample consisting of context\nand single-turn QA pairs, the Generator obtains candidate QA pairs and a\nknowledge graph based on the context. The Reassembler utilizes the knowledge\ngraph to get sequential QA pairs, and the Rewriter rewrites questions from a\nconversational perspective to obtain a multi-turn dataset S2M. Our experiments\nshow that our method can synthesize effective training resources for CQA.\nNotably, S2M ranks 1st place on the QuAC leaderboard at the time of submission\n(Aug 24th, 2022).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baokui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wangshu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yicheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Changlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Sen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Teng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+liu_S/0/1/0/all/0/1\">Siye liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A proposed new metric for the conceptual diversity of a text. (arXiv:2312.16548v1 [cs.CL])","link":"http://arxiv.org/abs/2312.16548","description":"<p>A word may contain one or more hidden concepts. While the \"animal\" word\nevokes many images in our minds and encapsulates many concepts (birds, dogs,\ncats, crocodiles, etc.), the `parrot' word evokes a single image (a colored\nbird with a short, hooked beak and the ability to mimic sounds). In spoken or\nwritten texts, we use some words in a general sense and some in a detailed way\nto point to a specific object. Until now, a text's conceptual diversity value\ncannot be determined using a standard and precise technique. This research\ncontributes to the natural language processing field of AI by offering a\nstandardized method and a generic metric for evaluating and comparing concept\ndiversity in different texts and domains. It also contributes to the field of\nsemantic research of languages. If we give examples for the diversity score of\ntwo sentences, \"He discovered an unknown entity.\" has a high conceptual\ndiversity score (16.6801), and \"The endoplasmic reticulum forms a series of\nflattened sacs within the cytoplasm of eukaryotic cells.\" sentence has a low\nconceptual diversity score which is 3.9068.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phd_I/0/1/0/all/0/1\">&#x130;lknur D&#xf6;nmez Phd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phd_M/0/1/0/all/0/1\">Mehmet Hakl&#x131;d&#x131;r Phd</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Robust are LLMs to In-Context Majority Label Bias?. (arXiv:2312.16549v1 [cs.LG])","link":"http://arxiv.org/abs/2312.16549","description":"<p>In the In-Context Learning (ICL) setup, various forms of label biases can\nmanifest. One such manifestation is majority label bias, which arises when the\ndistribution of labeled examples in the in-context samples is skewed towards\none or more specific classes making Large Language Models (LLMs) more prone to\npredict those labels. Such discrepancies can arise from various factors,\nincluding logistical constraints, inherent biases in data collection methods,\nlimited access to diverse data sources, etc. which are unavoidable in a\nreal-world industry setup. In this work, we study the robustness of in-context\nlearning in LLMs to shifts that occur due to majority label bias within the\npurview of text classification tasks. Prior works have shown that in-context\nlearning with LLMs is susceptible to such biases. In our study, we go one level\ndeeper and show that the robustness boundary varies widely for different models\nand tasks, with certain LLMs being highly robust (~90%) to majority label bias.\nAdditionally, our findings also highlight the impact of model size and the\nrichness of instructional prompts contributing towards model robustness. We\nrestrict our study to only publicly available open-source models to ensure\ntransparency and reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Karan Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roychowdhury_S/0/1/0/all/0/1\">Sumegh Roychowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasa_S/0/1/0/all/0/1\">Siva Rajesh Kasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasa_S/0/1/0/all/0/1\">Santhosh Kumar Kasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhanushali_A/0/1/0/all/0/1\">Anish Bhanushali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pattisapu_N/0/1/0/all/0/1\">Nikhil Pattisapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murthy_P/0/1/0/all/0/1\">Prasanna Srinivasa Murthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relationship between auditory and semantic entrainment using Deep Neural Networks (DNN). (arXiv:2312.16599v1 [cs.CL])","link":"http://arxiv.org/abs/2312.16599","description":"<p>The tendency of people to engage in similar, matching, or synchronized\nbehaviour when interacting is known as entrainment. Many studies examined\nlinguistic (syntactic and lexical structures) and paralinguistic (pitch,\nintensity) entrainment, but less attention was given to finding the\nrelationship between them. In this study, we utilized state-of-the-art DNN\nembeddings such as BERT and TRIpLet Loss network (TRILL) vectors to extract\nfeatures for measuring semantic and auditory similarities of turns within\ndialogues in two comparable spoken corpora of two different languages. We found\npeople's tendency to entrain on semantic features more when compared to\nauditory features. Additionally, we found that entrainment in semantic and\nauditory linguistic features are positively correlated. The findings of this\nstudy might assist in implementing the mechanism of entrainment in\nhuman-machine interaction (HMI).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kejriwal_J/0/1/0/all/0/1\">Jay Kejriwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benus_S/0/1/0/all/0/1\">&#x160;tefan Be&#x148;u&#x161;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Make BERT-based Chinese Spelling Check Model Enhanced by Layerwise Attention and Gaussian Mixture Model. (arXiv:2312.16623v1 [cs.CL])","link":"http://arxiv.org/abs/2312.16623","description":"<p>BERT-based models have shown a remarkable ability in the Chinese Spelling\nCheck (CSC) task recently. However, traditional BERT-based methods still suffer\nfrom two limitations. First, although previous works have identified that\nexplicit prior knowledge like Part-Of-Speech (POS) tagging can benefit in the\nCSC task, they neglected the fact that spelling errors inherent in CSC data can\nlead to incorrect tags and therefore mislead models. Additionally, they ignored\nthe correlation between the implicit hierarchical information encoded by BERT's\nintermediate layers and different linguistic phenomena. This results in\nsub-optimal accuracy. To alleviate the above two issues, we design a\nheterogeneous knowledge-infused framework to strengthen BERT-based CSC models.\nTo incorporate explicit POS knowledge, we utilize an auxiliary task strategy\ndriven by Gaussian mixture model. Meanwhile, to incorporate implicit\nhierarchical linguistic knowledge within the encoder, we propose a novel form\nof n-gram-based layerwise self-attention to generate a multilayer\nrepresentation. Experimental results show that our proposed framework yields a\nstable performance boost over four strong baseline models and outperforms the\nprevious state-of-the-art methods on two datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yongchang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Liang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xinyu Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Large Language Model-based Computational Approach to Improve Identity-Related Write-Ups. (arXiv:2312.16659v1 [cs.CL])","link":"http://arxiv.org/abs/2312.16659","description":"<p>Creating written products is essential to modern life, including writings\nabout one's identity and personal experiences. However, writing is often a\ndifficult activity that requires extensive effort to frame the central ideas,\nthe pursued approach to communicate the central ideas, e.g., using analogies,\nmetaphors, or other possible means, the needed presentation structure, and the\nactual verbal expression. Large Language Models, a recently emerged approach in\nMachine Learning, can offer a significant help in reducing the effort and\nimproving the quality of written products. This paper proposes a new\ncomputational approach to explore prompts that given as inputs to a Large\nLanguage Models can generate cues to improve the considered written products.\nTwo case studies on improving write-ups, one based on an analogy and one on a\nmetaphor, are also presented in the paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doboli_A/0/1/0/all/0/1\">Alex Doboli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Some things are more CRINGE than others: Preference Optimization with the Pairwise Cringe Loss. (arXiv:2312.16682v1 [cs.CL])","link":"http://arxiv.org/abs/2312.16682","description":"<p>Practitioners commonly align large language models using pairwise\npreferences, i.e., given labels of the type response A is preferred to response\nB for a given input. Perhaps less commonly, methods have also been developed\nfor binary feedback, i.e. training models given labels of type response A is\ngood or bad. We show how an existing performant binary feedback method, the\nCringe Loss (Adolphs et al., 2022), can be generalized to the pairwise\npreference setting using a simple soft margin extension. Pairwise Cringe Loss\nis straightforward to implement and efficient to train, and we find it\noutperforms state-of-the-art preference optimization algorithms such as PPO and\nDPO on the AlpacaFarm benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Andrew Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1\">Sainbayar Sukhbaatar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Tabular Data Understanding with Large Language Models. (arXiv:2312.16702v1 [cs.CL])","link":"http://arxiv.org/abs/2312.16702","description":"<p>Large Language Models (LLMs) have shown to be capable of various tasks, yet\ntheir capability in interpreting and reasoning over tabular data remains an\nunderexplored area. In this context, this study investigates from three core\nperspectives: the robustness of LLMs to structural perturbations in tables, the\ncomparative analysis of textual and symbolic reasoning on tables, and the\npotential of boosting model performance through the aggregation of multiple\nreasoning pathways. We discover that structural variance of tables presenting\nthe same content reveals a notable performance decline, particularly in\nsymbolic reasoning tasks. This prompts the proposal of a method for table\nstructure normalization. Moreover, textual reasoning slightly edges out\nsymbolic reasoning, and a detailed error analysis reveals that each exhibits\ndifferent strengths depending on the specific tasks. Notably, the aggregation\nof textual and symbolic reasoning pathways, bolstered by a mix self-consistency\nmechanism, resulted in achieving SOTA performance, with an accuracy of 73.6% on\nWIKITABLEQUESTIONS, representing a substantial advancement over previous\nexisting table processing paradigms of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Reversible Perspective on Petri Nets and Event Structures. (arXiv:2312.16714v1 [cs.CL])","link":"http://arxiv.org/abs/2312.16714","description":"<p>Event structures have emerged as a foundational model for concurrent\ncomputation, explaining computational processes by outlining the events and the\nrelationships that dictate their execution. They play a pivotal role in the\nstudy of key aspects of concurrent computation models, such as causality and\nindependence, and have found applications across a broad range of languages and\nmodels, spanning realms like persistence, probabilities, and quantum computing.\nRecently, event structures have been extended to address reversibility, where\ncomputational processes can undo previous computations. In this context,\nreversible event structures provide abstract representations of processes\ncapable of both forward and backward steps in a computation. Since their\nintroduction, event structures have played a crucial role in bridging\noperational models, traditionally exemplified by Petri nets and process\ncalculi, with denotational ones, i.e., algebraic domains. In this context, we\nrevisit the standard connection between Petri nets and event structures under\nthe lenses of reversibility. Specifically, we introduce a subset of contextual\nPetri nets, dubbed reversible causal nets, that precisely correspond to\nreversible prime event structures. The distinctive feature of reversible causal\nnets lies in deriving causality from inhibitor arcs, departing from the\nconventional dependence on the overlap between the post and preset of\ntransitions. In this way, we are able to operationally explain the full model\nof reversible prime event structures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Melgratti_H/0/1/0/all/0/1\">Hern&#xe1;n Melgratti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mezzina_C/0/1/0/all/0/1\">Claudio Antares Mezzina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinna_G/0/1/0/all/0/1\">G. Michele Pinna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Neural Networks for Antisocial Behavior Detection on Twitter. (arXiv:2312.16755v1 [cs.CL])","link":"http://arxiv.org/abs/2312.16755","description":"<p>Social media resurgence of antisocial behavior has exerted a downward spiral\non stereotypical beliefs, and hateful comments towards individuals and social\ngroups, as well as false or distorted news. The advances in graph neural\nnetworks employed on massive quantities of graph-structured data raise high\nhopes for the future of mediating communication on social media platforms. An\napproach based on graph convolutional data was employed to better capture the\ndependencies between the heterogeneous types of data.\n</p>\n<p>Utilizing past and present experiences on the topic, we proposed and\nevaluated a graph-based approach for antisocial behavior detection, with\ngeneral applicability that is both language- and context-independent. In this\nresearch, we carried out an experimental validation of our graph-based approach\non several PAN datasets provided as part of their shared tasks, that enable the\ndiscussion of the results obtained by the proposed solution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toshevska_M/0/1/0/all/0/1\">Martina Toshevska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalajdziski_S/0/1/0/all/0/1\">Slobodan Kalajdziski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gievska_S/0/1/0/all/0/1\">Sonja Gievska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Representation with Intra-Modal and Inter-Modal Graph Contrastive Learning for Multimodal Emotion Recognition. (arXiv:2312.16778v1 [cs.CL])","link":"http://arxiv.org/abs/2312.16778","description":"<p>With the release of increasing open-source emotion recognition datasets on\nsocial media platforms and the rapid development of computing resources,\nmultimodal emotion recognition tasks (MER) have begun to receive widespread\nresearch attention. The MER task extracts and fuses complementary semantic\ninformation from different modalities, which can classify the speaker's\nemotions. However, the existing feature fusion methods have usually mapped the\nfeatures of different modalities into the same feature space for information\nfusion, which can not eliminate the heterogeneity between different modalities.\nTherefore, it is challenging to make the subsequent emotion class boundary\nlearning. To tackle the above problems, we have proposed a novel Adversarial\nRepresentation with Intra-Modal and Inter-Modal Graph Contrastive for\nMultimodal Emotion Recognition (AR-IIGCN) method. Firstly, we input video,\naudio, and text features into a multi-layer perceptron (MLP) to map them into\nseparate feature spaces. Secondly, we build a generator and a discriminator for\nthe three modal features through adversarial representation, which can achieve\ninformation interaction between modalities and eliminate heterogeneity among\nmodalities. Thirdly, we introduce contrastive graph representation learning to\ncapture intra-modal and inter-modal complementary semantic information and\nlearn intra-class and inter-class boundary information of emotion categories.\nSpecifically, we construct a graph structure for three modal features and\nperform contrastive representation learning on nodes with different emotions in\nthe same modality and the same emotion in different modalities, which can\nimprove the feature representation ability of nodes. Extensive experimental\nworks show that the ARL-IIGCN method can significantly improve emotion\nrecognition accuracy on IEMOCAP and MELD datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shou_Y/0/1/0/all/0/1\">Yuntao Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_T/0/1/0/all/0/1\">Tao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_W/0/1/0/all/0/1\">Wei Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Keqin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Radiology Summarization with Radiograph and Anatomy Prompts. (arXiv:2210.08303v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.08303","description":"<p>The impression is crucial for the referring physicians to grasp key\ninformation since it is concluded from the findings and reasoning of\nradiologists. To alleviate the workload of radiologists and reduce repetitive\nhuman labor in impression writing, many researchers have focused on automatic\nimpression generation. However, recent works on this task mainly summarize the\ncorresponding findings and pay less attention to the radiology images. In\nclinical, radiographs can provide more detailed valuable observations to\nenhance radiologists' impression writing, especially for complicated cases.\nBesides, each sentence in findings usually focuses on single anatomy, so they\nonly need to be matched to corresponding anatomical regions instead of the\nwhole image, which is beneficial for textual and visual features alignment.\nTherefore, we propose a novel anatomy-enhanced multimodal model to promote\nimpression generation. In detail, we first construct a set of rules to extract\nanatomies and put these prompts into each sentence to highlight anatomy\ncharacteristics. Then, two separate encoders are applied to extract features\nfrom the radiograph and findings. Afterward, we utilize a contrastive learning\nmodule to align these two representations at the overall level and use a\nco-attention to fuse them at the sentence level with the help of\nanatomy-enhanced sentence representation. Finally, the decoder takes the fused\ninformation as the input to generate impressions. The experimental results on\ntwo benchmark datasets confirm the effectiveness of the proposed method, which\nachieves state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinpeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhihong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tsung-Hui Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word-Graph2vec: An efficient word embedding approach on word co-occurrence graph using random walk technique. (arXiv:2301.04312v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.04312","description":"<p>Word embedding has become ubiquitous and is widely used in various natural\nlanguage processing (NLP) tasks, such as web retrieval, web semantic analysis,\nand machine translation, and so on. Unfortunately, training the word embedding\nin a relatively large corpus is prohibitively expensive. We propose a\ngraph-based word embedding algorithm, called Word-Graph2vec, which converts the\nlarge corpus into a word co-occurrence graph, then takes the word sequence\nsamples from this graph by randomly traveling and trains the word embedding on\nthis sampling corpus in the end. We posit that because of the limited\nvocabulary, huge idioms, and fixed expressions in English, the size and density\nof the word co-occurrence graph change slightly with the increase in the\ntraining corpus. So that Word-Graph2vec has stable runtime on the large-scale\ndata set, and its performance advantage becomes more and more obvious with the\ngrowth of the training corpus. Extensive experiments conducted on real-world\ndatasets show that the proposed algorithm outperforms traditional Word2vec four\nto five times in terms of efficiency and two to three times than FastText,\nwhile the error generated by the random walk technique is small.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jiahong Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huacan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feijuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuanzhe Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v8 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.10405","description":"<p>Recently decades have witnessed the empirical success of framing Knowledge\nGraph (KG) embeddings via language models. However, language model-based KG\nembeddings are usually deployed as static artifacts, making them difficult to\nmodify post-deployment without re-training after deployment. To address this\nissue, we propose a new task of editing language model-based KG embeddings in\nthis paper. This task is designed to facilitate rapid, data-efficient updates\nto KG embeddings without compromising the performance of other aspects. We\nbuild four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and\nevaluate several knowledge editing baselines demonstrating the limited ability\nof previous models to handle the proposed challenging task. We further propose\na simple yet strong baseline dubbed KGEditor, which utilizes additional\nparametric layers of the hypernetwork to edit/add facts. Our comprehensive\nexperimental results reveal that KGEditor excels in updating specific facts\nwithout impacting the overall performance, even when faced with limited\ntraining resources. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKG/tree/main/deltaKG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1\">Bozhong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingbing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frauds Bargain Attack: Generating Adversarial Text Samples via Word Manipulation Process. (arXiv:2303.01234v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.01234","description":"<p>Recent research has revealed that natural language processing (NLP) models\nare vulnerable to adversarial examples. However, the current techniques for\ngenerating such examples rely on deterministic heuristic rules, which fail to\nproduce optimal adversarial examples. In response, this study proposes a new\nmethod called the Fraud's Bargain Attack (FBA), which uses a randomization\nmechanism to expand the search space and produce high-quality adversarial\nexamples with a higher probability of success. FBA uses the Metropolis-Hasting\nsampler, a type of Markov Chain Monte Carlo sampler, to improve the selection\nof adversarial examples from all candidates generated by a customized\nstochastic process called the Word Manipulation Process (WMP). The WMP method\nmodifies individual words in a contextually-aware manner through insertion,\nremoval, or substitution. Through extensive experiments, this study\ndemonstrates that FBA outperforms other methods in terms of attack success\nrate, imperceptibility and sentence quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_M/0/1/0/all/0/1\">Mingze Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhensu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identification of Negative Transfers in Multitask Learning Using Surrogate Models. (arXiv:2303.14582v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2303.14582","description":"<p>Multitask learning is widely used in practice to train a low-resource target\ntask by augmenting it with multiple related source tasks. Yet, naively\ncombining all the source tasks with a target task does not always improve the\nprediction performance for the target task due to negative transfers. Thus, a\ncritical problem in multitask learning is identifying subsets of source tasks\nthat would benefit the target task. This problem is computationally challenging\nsince the number of subsets grows exponentially with the number of source\ntasks; efficient heuristics for subset selection do not always capture the\nrelationship between task subsets and multitask learning performances. In this\npaper, we introduce an efficient procedure to address this problem via\nsurrogate modeling. In surrogate modeling, we sample (random) subsets of source\ntasks and precompute their multitask learning performances. Then, we\napproximate the precomputed performances with a linear regression model that\ncan also predict the multitask performance of unseen task subsets. We show\ntheoretically and empirically that fitting this model only requires sampling\nlinearly many subsets in the number of source tasks. The fitted model provides\na relevance score between each source and target task. We use the relevance\nscores to perform subset selection for multitask learning by thresholding.\nThrough extensive experiments, we show that our approach predicts negative\ntransfers from multiple source tasks to target tasks much more accurately than\nexisting task affinity measures. Additionally, we demonstrate that for several\nweak supervision datasets, our approach consistently improves upon existing\noptimization methods for multitask learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongyue Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huy L. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongyang R. Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Out-of-Distribution Detection in NLP. (arXiv:2305.03236v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03236","description":"<p>Out-of-distribution (OOD) detection is essential for the reliable and safe\ndeployment of machine learning systems in the real world. Great progress has\nbeen made over the past years. This paper presents the first review of recent\nadvances in OOD detection with a particular focus on natural language\nprocessing approaches. First, we provide a formal definition of OOD detection\nand discuss several related fields. We then categorize recent algorithms into\nthree classes according to the data they used: (1) OOD data available, (2) OOD\ndata unavailable + in-distribution (ID) label available, and (3) OOD data\nunavailable + ID label unavailable. Third, we introduce datasets, applications,\nand metrics. Finally, we summarize existing work and present potential future\nresearch topics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_H/0/1/0/all/0/1\">Hao Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Question-Answering System Extracts Information on Injection Drug Use from Clinical Notes. (arXiv:2305.08777v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2305.08777","description":"<p>Background: Injection drug use (IDU) is a dangerous health behavior that\nincreases mortality and morbidity. Identifying IDU early and initiating harm\nreduction interventions can benefit individuals at risk. However, extracting\nIDU behaviors from patients' electronic health records (EHR) is difficult\nbecause there is no International Classification of Disease (ICD) code and the\nonly place IDU information can be indicated is unstructured free-text clinical\nnotes. Although natural language processing can efficiently extract this\ninformation from unstructured data, there are no validated tools. Methods: To\naddress this gap in clinical information, we design and demonstrate a\nquestion-answering (QA) framework to extract information on IDU from clinical\nnotes. Our framework involves two main steps: (1) generating a gold-standard QA\ndataset and (2) developing and testing the QA model. We utilize 2323 clinical\nnotes of 1145 patients sourced from the VA Corporate Data Warehouse to\nconstruct the gold-standard dataset for developing and evaluating the QA model.\nWe also demonstrate the QA model's ability to extract IDU-related information\non temporally out-of-distribution data. Results: Here we show that for a strict\nmatch between gold-standard and predicted answers, the QA model achieves 51.65%\nF1 score. For a relaxed match between the gold-standard and predicted answers,\nthe QA model obtains 78.03% F1 score, along with 85.38% Precision and 79.02%\nRecall scores. Moreover, the QA model demonstrates consistent performance when\nsubjected to temporally out-of-distribution data. Conclusions: Our study\nintroduces a QA framework designed to extract IDU information from clinical\nnotes, aiming to enhance the accurate and efficient detection of people who\ninject drugs, extract relevant information, and ultimately facilitate informed\npatient care.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahbub_M/0/1/0/all/0/1\">Maria Mahbub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goethert_I/0/1/0/all/0/1\">Ian Goethert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danciu_I/0/1/0/all/0/1\">Ioana Danciu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knight_K/0/1/0/all/0/1\">Kathryn Knight</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Sudarshan Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamang_S/0/1/0/all/0/1\">Suzanne Tamang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozenberg_Ben_Dror_K/0/1/0/all/0/1\">Karine Rozenberg-Ben-Dror</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solares_H/0/1/0/all/0/1\">Hugo Solares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_S/0/1/0/all/0/1\">Susana Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trafton_J/0/1/0/all/0/1\">Jodie Trafton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Begoli_E/0/1/0/all/0/1\">Edmon Begoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peterson_G/0/1/0/all/0/1\">Gregory Peterson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks. (arXiv:2305.18365v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18365","description":"<p>Large Language Models (LLMs) with strong abilities in natural language\nprocessing tasks have emerged and have been applied in various kinds of areas\nsuch as science, finance and software engineering. However, the capability of\nLLMs to advance the field of chemistry remains unclear. In this paper, rather\nthan pursuing state-of-the-art performance, we aim to evaluate capabilities of\nLLMs in a wide range of tasks across the chemistry domain. We identify three\nkey chemistry-related capabilities including understanding, reasoning and\nexplaining to explore in LLMs and establish a benchmark containing eight\nchemistry tasks. Our analysis draws on widely recognized datasets facilitating\na broad exploration of the capacities of LLMs within the context of practical\nchemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are\nevaluated for each chemistry task in zero-shot and few-shot in-context learning\nsettings with carefully selected demonstration examples and specially crafted\nprompts. Our investigation found that GPT-4 outperformed other models and LLMs\nexhibit different competitive levels in eight chemistry tasks. In addition to\nthe key findings from the comprehensive benchmark analysis, our work provides\ninsights into the limitation of current LLMs and the impact of in-context\nlearning settings on LLMs' performance across various chemistry tasks. The code\nand datasets used in this study are available at\nhttps://github.com/ChemFoundationModels/ChemLLMBench.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Taicheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1\">Kehan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_B/0/1/0/all/0/1\">Bozhao Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhenwen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhichun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chawla_N/0/1/0/all/0/1\">Nitesh V. Chawla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiest_O/0/1/0/all/0/1\">Olaf Wiest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the Reliability of Psychological Scales on Large Language Models. (arXiv:2305.19926v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.19926","description":"<p>Recent research has extended beyond assessing the performance of Large\nLanguage Models (LLMs) to examining their characteristics from a psychological\nstandpoint, acknowledging the necessity of understanding their behavioral\ncharacteristics. The administration of personality tests to LLMs has emerged as\na noteworthy area in this context. However, the suitability of employing\npsychological scales, initially devised for humans, on LLMs is a matter of\nongoing debate. Our study aims to determine the reliability of applying\npersonality assessments to LLMs, explicitly investigating whether LLMs\ndemonstrate consistent personality traits. Analyzing responses under 2,500\nsettings reveals that gpt-3.5-turbo shows consistency in responses to the Big\nFive Inventory, indicating a high degree of reliability. Furthermore, our\nresearch explores the potential of gpt-3.5-turbo to emulate diverse\npersonalities and represent various groups, which is a capability increasingly\nsought after in social sciences for substituting human participants with LLMs\nto reduce costs. Our findings reveal that LLMs have the potential to represent\ndifferent personalities with specific prompt instructions. By shedding light on\nthe personalization of LLMs, our study endeavors to pave the way for future\nexplorations in this field. We have made our experimental results and the\ncorresponding code openly accessible via\nhttps://github.com/CUHK-ARISE/LLMPersonality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jen-tse Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1\">Man Ho Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Eric John Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual Transfer Learning for Low-Resource Speech Translation. (arXiv:2306.00789v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.00789","description":"<p>The paper presents a novel three-step transfer learning framework for\nenhancing cross-lingual transfer from high- to low-resource languages in the\ndownstream application of Automatic Speech Translation. The approach integrates\na semantic knowledge-distillation step into the existing two-step cross-lingual\ntransfer learning framework XLS-R. This extra step aims to encode semantic\nknowledge in the multilingual speech encoder pre-trained via Self-Supervised\nLearning using unlabeled speech. Our proposed three-step cross-lingual transfer\nlearning framework addresses the large cross-lingual transfer gap (TRFGap)\nobserved in the XLS-R framework between high-resource and low-resource\nlanguages. We validate our proposal through extensive experiments and\ncomparisons on the CoVoST-2 benchmark, showing significant improvements in\ntranslation performance, especially for low-resource languages, and a notable\nreduction in the TRFGap.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khurana_S/0/1/0/all/0/1\">Sameer Khurana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawalatabad_N/0/1/0/all/0/1\">Nauman Dawalatabad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurent_A/0/1/0/all/0/1\">Antoine Laurent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vicente_L/0/1/0/all/0/1\">Luis Vicente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimeno_P/0/1/0/all/0/1\">Pablo Gimeno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mingote_V/0/1/0/all/0/1\">Victoria Mingote</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_J/0/1/0/all/0/1\">Jonathan Le Roux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAVEN: An Embodied Conversational Agent for Efficient Audio-Visual Navigation in Noisy Environments. (arXiv:2306.04047v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2306.04047","description":"<p>Audio-visual navigation of an agent towards locating an audio goal is a\nchallenging task especially when the audio is sporadic or the environment is\nnoisy. In this paper, we present CAVEN, a Conversation-based Audio-Visual\nEmbodied Navigation framework in which the agent may interact with a\nhuman/oracle for solving the task of navigating to an audio goal. Specifically,\nCAVEN is modeled as a budget-aware partially observable semi-Markov decision\nprocess that implicitly learns the uncertainty in the audio-based navigation\npolicy to decide when and how the agent may interact with the oracle. Our CAVEN\nagent can engage in fully-bidirectional natural language conversations by\nproducing relevant questions and interpret free-form, potentially noisy\nresponses from the oracle based on the audio-visual context. To enable such a\ncapability, CAVEN is equipped with: (i) a trajectory forecasting network that\nis grounded in audio-visual cues to produce a potential trajectory to the\nestimated goal, and (ii) a natural language based question generation and\nreasoning network to pose an interactive question to the oracle or interpret\nthe oracle's response to produce navigation instructions. To train the\ninteractive modules, we present a large scale dataset: AVN-Instruct, based on\nthe Landmark-RxR dataset. To substantiate the usefulness of conversations, we\npresent experiments on the benchmark audio-goal task using the SoundSpaces\nsimulator under various noisy settings. Our results reveal that our\nfully-conversational approach leads to nearly an order-of-magnitude improvement\nin success rate, especially in localizing new sound sources and against methods\nthat only use uni-directional interaction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiulong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sudipta Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_M/0/1/0/all/0/1\">Moitreya Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1\">Anoop Cherian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT a Good Personality Recognizer? A Preliminary Study. (arXiv:2307.03952v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.03952","description":"<p>In recent years, personality has been regarded as a valuable personal factor\nbeing incorporated into numerous tasks such as sentiment analysis and product\nrecommendation. This has led to widespread attention to text-based personality\nrecognition task, which aims to identify an individual's personality based on\ngiven text. Considering that ChatGPT has recently exhibited remarkable\nabilities on various natural language processing tasks, we provide a\npreliminary evaluation of ChatGPT on text-based personality recognition task\nfor generating effective personality data. Concretely, we employ a variety of\nprompting strategies to explore ChatGPT's ability in recognizing personality\nfrom given text, especially the level-oriented prompting strategy we designed\nfor guiding ChatGPT in analyzing given text at a specified level. The\nexperimental results on two representative real-world datasets reveal that\nChatGPT with zero-shot chain-of-thought prompting exhibits impressive\npersonality recognition ability and is capable to provide natural language\nexplanations through text-based logical reasoning. Furthermore, by employing\nthe level-oriented prompting strategy to optimize zero-shot chain-of-thought\nprompting, the performance gap between ChatGPT and corresponding\nstate-of-the-art model has been narrowed even more. However, we observe that\nChatGPT shows unfairness towards certain sensitive demographic attributes such\nas gender and age. Additionally, we discover that eliciting the personality\nrecognition ability of ChatGPT helps improve its performance on\npersonality-related downstream tasks such as sentiment classification and\nstress prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yu Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Liang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Overview of Large Language Models. (arXiv:2307.06435v7 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.06435","description":"<p>Large Language Models (LLMs) have recently demonstrated remarkable\ncapabilities in natural language processing tasks and beyond. This success of\nLLMs has led to a large influx of research contributions in this direction.\nThese works encompass diverse topics such as architectural innovations, better\ntraining strategies, context length improvements, fine-tuning, multi-modal\nLLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid\ndevelopment of techniques and regular breakthroughs in LLM research, it has\nbecome considerably challenging to perceive the bigger picture of the advances\nin this direction. Considering the rapidly emerging plethora of literature on\nLLMs, it is imperative that the research community is able to benefit from a\nconcise yet comprehensive overview of the recent developments in this field.\nThis article provides an overview of the existing literature on a broad range\nof LLM-related concepts. Our self-contained comprehensive overview of LLMs\ndiscusses relevant background concepts along with covering the advanced topics\nat the frontier of research in LLMs. This review article is intended to not\nonly provide a systematic survey but also a quick comprehensive reference for\nthe researchers and practitioners to draw insights from extensive informative\nsummaries of the existing works to advance the LLM research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naveed_H/0/1/0/all/0/1\">Humza Naveed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Asad Ullah Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saqib_M/0/1/0/all/0/1\">Muhammad Saqib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usman_M/0/1/0/all/0/1\">Muhammad Usman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1\">Naveed Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction. (arXiv:2307.16082v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.16082","description":"<p>Social platforms have emerged as crucial platforms for disseminating\ninformation and discussing real-life social events, offering researchers an\nexcellent opportunity to design and implement novel event detection frameworks.\nHowever, most existing approaches only exploit keyword burstiness or network\nstructures to detect unspecified events. Thus, they often need help identifying\nunknown events regarding the challenging nature of events and social data.\nSocial data, e.g., tweets, is characterized by misspellings, incompleteness,\nword sense ambiguation, irregular language, and variation in aspects of\nopinions. Moreover, extracting discriminative features and patterns for\nevolving events by exploiting the limited structural knowledge is almost\ninfeasible. To address these challenges, in this paper, we propose a novel\nframework, namely EnrichEvent, that leverages the linguistic and contextual\nrepresentations of streaming social data. In particular, we leverage contextual\nand linguistic knowledge to detect semantically related tweets and enhance the\neffectiveness of the event detection approaches. Eventually, our proposed\nframework produces cluster chains for each event to show the evolving variation\nof the event through time. We conducted extensive experiments to evaluate our\nframework, validating its high performance and effectiveness in detecting and\ndistinguishing unspecified social events.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Esfahani_M/0/1/0/all/0/1\">Mohammadali Sefidi Esfahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbari_M/0/1/0/all/0/1\">Mohammad Akbari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue. (arXiv:2308.03549v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.03549","description":"<p>Recent advances in Large Language Models (LLMs) have achieved remarkable\nbreakthroughs in understanding and responding to user intents. However, their\nperformance lag behind general use cases in some expertise domains, such as\nChinese medicine. Existing efforts to incorporate Chinese medicine into LLMs\nrely on Supervised Fine-Tuning (SFT) with single-turn and distilled dialogue\ndata. These models lack the ability for doctor-like proactive inquiry and\nmulti-turn comprehension and cannot align responses with experts' intentions.\nIn this work, we introduce Zhongjing, the first Chinese medical LLaMA-based LLM\nthat implements an entire training pipeline from continuous pre-training, SFT,\nto Reinforcement Learning from Human Feedback (RLHF). Additionally, we\nconstruct a Chinese multi-turn medical dialogue dataset of 70,000 authentic\ndoctor-patient dialogues, CMtMedQA, which significantly enhances the model's\ncapability for complex dialogue and proactive inquiry initiation. We also\ndefine a refined annotation rule and evaluation criteria given the unique\ncharacteristics of the biomedical domain. Extensive experimental results show\nthat Zhongjing outperforms baselines in various capacities and matches the\nperformance of ChatGPT in some abilities, despite the 100x parameters. Ablation\nstudies also demonstrate the contributions of each component: pre-training\nenhances medical knowledge, and RLHF further improves instruction-following\nability and safety. Our code, datasets, and models are available at\nhttps://github.com/SupritYoung/Zhongjing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Songhua Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanjie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Senbin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guangyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yuxiang Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zan_H/0/1/0/all/0/1\">Hongying Zan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data. (arXiv:2308.10253v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2308.10253","description":"<p>The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have\nsparked significant interest in the development of multimodal Large Language\nModels (LLMs). A primary research objective of such models is to align visual\nand textual modalities effectively while comprehending human instructions.\nCurrent methodologies often rely on annotations derived from benchmark datasets\nto construct image-dialogue datasets for training purposes, akin to instruction\ntuning in LLMs. However, these datasets often exhibit domain bias, potentially\nconstraining the generative capabilities of the models. In an effort to\nmitigate these limitations, we propose a novel data collection methodology that\nsynchronously synthesizes images and dialogues for visual instruction tuning.\nThis approach harnesses the power of generative models, marrying the abilities\nof ChatGPT and text-to-image generative models to yield a diverse and\ncontrollable dataset with varied image content. Additionally, datasets can be\narbitrarily scaled. This not only provides greater flexibility compared to\nexisting methodologies but also significantly enhances several model\ncapabilities. Our research includes comprehensive experiments conducted on\nvarious datasets. The results emphasize substantial enhancements in more than\nten commonly assessed capabilities. Additionally, our model achieves\nstate-of-the-art results across multiple widely recognized multimodal\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Gang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhibin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1\">Bin Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Ling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio Generation with Multiple Conditional Diffusion Model. (arXiv:2308.11940v4 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2308.11940","description":"<p>Text-based audio generation models have limitations as they cannot encompass\nall the information in audio, leading to restricted controllability when\nrelying solely on text. To address this issue, we propose a novel model that\nenhances the controllability of existing pre-trained text-to-audio models by\nincorporating additional conditions including content (timestamp) and style\n(pitch contour and energy contour) as supplements to the text. This approach\nachieves fine-grained control over the temporal order, pitch, and energy of\ngenerated audio. To preserve the diversity of generation, we employ a trainable\ncontrol condition encoder that is enhanced by a large language model and a\ntrainable Fusion-Net to encode and fuse the additional conditions while keeping\nthe weights of the pre-trained text-to-audio model frozen. Due to the lack of\nsuitable datasets and evaluation metrics, we consolidate existing datasets into\na new dataset comprising the audio and corresponding conditions and use a\nseries of evaluation metrics to evaluate the controllability performance.\nExperimental results demonstrate that our model successfully achieves\nfine-grained control to accomplish controllable audio generation. Audio samples\nand our dataset are publicly available at\nhttps://conditionaudiogen.github.io/conditionaudiogen/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhifang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jianguo Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1\">Rui Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Long Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouchi_K/0/1/0/all/0/1\">Kazushige Ouchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiangdong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hot or Cold? Adaptive Temperature Sampling for Code Generation with Large Language Models. (arXiv:2309.02772v3 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2309.02772","description":"<p>Recently, Large Language Models (LLMs) have shown impressive abilities in\ncode generation. However, existing LLMs' decoding strategies are designed for\nNatural Language (NL) generation, overlooking the differences between NL and\nprogramming languages (PL). Due to this oversight, a better decoding strategy\nfor code generation remains an open question. In this paper, we conduct the\nfirst systematic study to explore a decoding strategy specialized in code\ngeneration. With an analysis of loss distributions of code tokens, we find that\ncode tokens can be divided into two categories: challenging tokens that are\ndifficult to predict and confident tokens that can be easily inferred. Among\nthem, the challenging tokens mainly appear at the beginning of a code block.\nInspired by the above findings, we propose a simple yet effective method:\nAdaptive Temperature (AdapT) sampling, which dynamically adjusts the\ntemperature coefficient when decoding different tokens. We apply a larger\ntemperature when sampling for challenging tokens, allowing LLMs to explore\ndiverse choices. We employ a smaller temperature for confident tokens avoiding\nthe influence of tail randomness noises. We apply AdapT sampling to LLMs with\ndifferent sizes and conduct evaluations on two popular datasets. Results show\nthat AdapT sampling significantly outperforms state-of-the-art decoding\nstrategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">YunFei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1\">Hong Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoLLD: Contrastive Layer-to-layer Distillation for Compressing Multilingual Pre-trained Speech Encoders. (arXiv:2309.07707v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.07707","description":"<p>Large-scale self-supervised pre-trained speech encoders outperform\nconventional approaches in speech recognition and translation tasks. Due to the\nhigh cost of developing these large models, building new encoders for new tasks\nand deploying them to on-device applications are infeasible. Prior studies\npropose model compression methods to address this issue, but those works focus\non smaller models and less realistic tasks. Thus, we propose Contrastive\nLayer-to-layer Distillation (CoLLD), a novel knowledge distillation method to\ncompress pre-trained speech encoders by leveraging masked prediction and\ncontrastive learning to train student models to copy the behavior of a large\nteacher model. CoLLD outperforms prior methods and closes the gap between small\nand large models on multilingual speech-to-text translation and recognition\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1\">Ning Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavlyutov_R/0/1/0/all/0/1\">Ruslan Mavlyutov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popuri_S/0/1/0/all/0/1\">Sravya Popuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1\">Yu-An Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptTTS++: Controlling Speaker Identity in Prompt-Based Text-to-Speech Using Natural Language Descriptions. (arXiv:2309.08140v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2309.08140","description":"<p>We propose PromptTTS++, a prompt-based text-to-speech (TTS) synthesis system\nthat allows control over speaker identity using natural language descriptions.\nTo control speaker identity within the prompt-based TTS framework, we introduce\nthe concept of speaker prompt, which describes voice characteristics (e.g.,\ngender-neutral, young, old, and muffled) designed to be approximately\nindependent of speaking style. Since there is no large-scale dataset containing\nspeaker prompts, we first construct a dataset based on the LibriTTS-R corpus\nwith manually annotated speaker prompts. We then employ a diffusion-based\nacoustic model with mixture density networks to model diverse speaker factors\nin the training data. Unlike previous studies that rely on style prompts\ndescribing only a limited aspect of speaker individuality, such as pitch,\nspeaking speed, and energy, our method utilizes an additional speaker prompt to\neffectively learn the mapping from natural language descriptions to the\nacoustic features of diverse speakers. Our subjective evaluation results show\nthat the proposed method can better control speaker characteristics than the\nmethods without the speaker prompt. Audio samples are available at\nhttps://reppy4620.github.io/demo.promptttspp/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shimizu_R/0/1/0/all/0/1\">Reo Shimizu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yamamoto_R/0/1/0/all/0/1\">Ryuichi Yamamoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawamura_M/0/1/0/all/0/1\">Masaya Kawamura</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shirahata_Y/0/1/0/all/0/1\">Yuma Shirahata</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Doi_H/0/1/0/all/0/1\">Hironori Doi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Komatsu_T/0/1/0/all/0/1\">Tatsuya Komatsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tachibana_K/0/1/0/all/0/1\">Kentaro Tachibana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting conformers with structured state-space sequence models for online speech recognition. (arXiv:2309.08551v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.08551","description":"<p>Online speech recognition, where the model only accesses context to the left,\nis an important and challenging use case for ASR systems. In this work, we\ninvestigate augmenting neural encoders for online ASR by incorporating\nstructured state-space sequence models (S4), a family of models that provide a\nparameter-efficient way of accessing arbitrarily long left context. We\nperformed systematic ablation studies to compare variants of S4 models and\npropose two novel approaches that combine them with convolutions. We found that\nthe most effective design is to stack a small S4 using real-valued recurrent\nweights with a local convolution, allowing them to work complementarily. Our\nbest model achieves WERs of 4.01%/8.53% on test sets from Librispeech,\noutperforming Conformers with extensively tuned convolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1\">Haozhe Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_A/0/1/0/all/0/1\">Albert Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1\">Krzysztof Choromanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara Sainath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.08475","description":"<p>In this paper, we focus on editing Multimodal Large Language Models (MLLMs).\nCompared to editing single-modal LLMs, multimodal model editing is more\nchallenging, which demands a higher level of scrutiny and careful consideration\nin the editing process. To facilitate research in this area, we construct a new\nbenchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite\nof innovative metrics for evaluation. We conduct comprehensive experiments\ninvolving various model editing baselines and analyze the impact of editing\ndifferent components for multimodal LLMs. Empirically, we notice that previous\nbaselines can implement editing multimodal LLMs to some extent, but the effect\nis still barely satisfactory, indicating the potential difficulty of this task.\nWe hope that our work can provide the NLP community with insights. Code and\ndataset are available in https://github.com/zjunlp/EasyEdit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1\">Bozhong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingbin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Relation Classification with Graph Meaning Representations. (arXiv:2310.09772v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.09772","description":"<p>In the field of natural language understanding, the intersection of neural\nmodels and graph meaning representations (GMRs) remains a compelling area of\nresearch. Despite the growing interest, a critical gap persists in\nunderstanding the exact influence of GMRs, particularly concerning relation\nextraction tasks. Addressing this, we introduce DAGNN-plus, a simple and\nparameter-efficient neural architecture designed to decouple contextual\nrepresentation learning from structural information propagation. Coupled with\nvarious sequence encoders and GMRs, this architecture provides a foundation for\nsystematic experimentation on two English and two Chinese datasets. Our\nempirical analysis utilizes four different graph formalisms and nine parsers.\nThe results yield a nuanced understanding of GMRs, showing improvements in\nthree out of the four datasets, particularly favoring English over Chinese due\nto highly accurate parsers. Interestingly, GMRs appear less effective in\nliterary-domain datasets compared to general-domain datasets. These findings\nlay the groundwork for better-informed design of GMRs and parsers to improve\nrelation classification, which is expected to tangibly impact the future\ntrajectory of natural language understanding research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Li Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Dingyi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Malu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Evaluation of Tool-Assisted Generation Strategies. (arXiv:2310.10062v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.10062","description":"<p>A growing area of research investigates augmenting language models with tools\n(e.g., search engines, calculators) to overcome their shortcomings (e.g.,\nmissing or incorrect knowledge, incorrect logical inferences). Various few-shot\ntool-usage strategies have been proposed. However, there is no systematic and\nfair comparison across different strategies, or between these strategies and\nstrong baselines that do not leverage tools. We conduct an extensive empirical\nanalysis, finding that (1) across various datasets, example difficulty levels,\nand models, strong no-tool baselines are competitive to tool-assisted\nstrategies, implying that effectively using tools with in-context\ndemonstrations is a difficult unsolved problem; (2) for knowledge-retrieval\ntasks, strategies that *refine* incorrect outputs with tools outperform\nstrategies that retrieve relevant information *ahead of* or *during\ngeneration*; (3) tool-assisted strategies are expensive in the number of tokens\nthey require to work -- incurring additional costs by orders of magnitude --\nwhich does not translate into significant improvement in performance. Overall,\nour findings suggest that few-shot tool integration is still an open challenge,\nemphasizing the need for comprehensive evaluations of future strategies to\naccurately assess their *benefits* and *costs*.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jacovi_A/0/1/0/all/0/1\">Alon Jacovi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1\">Avi Caciularu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1\">Jonathan Herzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1\">Roee Aharoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohnet_B/0/1/0/all/0/1\">Bernd Bohnet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.10477","description":"<p>The rapid development of large language models (LLMs) has not only provided\nnumerous opportunities but also presented significant challenges. This becomes\nparticularly evident when LLMs inadvertently generate harmful or toxic content,\neither unintentionally or because of intentional inducement. Existing alignment\nmethods usually direct LLMs toward favorable outcomes by utilizing\nhuman-annotated, flawless instruction-response pairs. Conversely, this study\nproposes a novel alignment technique based on mistake analysis, which\ndeliberately exposes LLMs to erroneous content to learn the reasons for\nmistakes and how to avoid them. In this case, mistakes are repurposed into\nvaluable data for alignment, effectively helping to avoid the production of\nerroneous responses. Without external models or human annotations, our method\nleverages a model's intrinsic ability to discern undesirable mistakes and\nimproves the safety of its generated responses. Experimental results reveal\nthat our method outperforms existing alignment approaches in enhancing model\nsafety while maintaining the overall utility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jianhua Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lanqing Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenyong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1\">Dit-Yan Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts. (arXiv:2310.14628v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.14628","description":"<p>As large language models (LLMs) have shown effectiveness with different\nprompting methods, such as Chain of Thought, Program of Thought, we find that\nthese methods have formed a great complementarity to each other on math\nreasoning tasks. In this work, we propose XoT, an integrated problem solving\nframework by prompting LLMs with diverse reasoning thoughts. For each question,\nXoT always begins with selecting the most suitable method then executes each\nmethod iteratively. Within each iteration, XoT actively checks the validity of\nthe generated answer and incorporates the feedback from external executors,\nallowing it to dynamically switch among different prompting methods. Through\nextensive experiments on 10 popular math reasoning datasets, we demonstrate the\neffectiveness of our proposed approach and thoroughly analyze the strengths of\neach module. Moreover, empirical results suggest that our framework is\northogonal to recent work that makes improvements on single reasoning methods\nand can further generalise to logical reasoning domain. By allowing method\nswitching, XoT provides a fresh perspective on the collaborative integration of\ndiverse reasoning thoughts in a unified framework. The code is available at\nhttps://github.com/tengxiaoliu/XoT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tengxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qipeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiangkun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Baseline Analysis of Reward Models' Ability To Accurately Analyze Foundation Models Under Distribution Shift. (arXiv:2311.14743v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.14743","description":"<p>Foundation models, specifically Large Language Models (LLM's), have lately\ngained wide-spread attention and adoption. Reinforcement Learning with Human\nFeedback (RLHF) involves training a reward model to capture desired behaviors,\nwhich is then used to align LLM's. These reward models are additionally used at\ninference-time to estimate LLM responses' adherence to those desired behaviors.\nHowever, there is little work measuring how robust these reward models are to\ndistribution shifts. In this work, we evaluate how reward model performance -\nmeasured via accuracy and calibration (i.e. alignment between accuracy and\nconfidence) - is affected by distribution shift. We show novel calibration\npatterns and accuracy drops due to OOD prompts and responses, and that the\nreward model is more sensitive to shifts in responses than prompts.\nAdditionally, we adapt an OOD detection technique commonly used in\nclassification to the reward model setting to detect these distribution shifts\nin prompts and responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+LeVine_W/0/1/0/all/0/1\">Will LeVine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pikus_B/0/1/0/all/0/1\">Ben Pikus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anthony Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendryx_S/0/1/0/all/0/1\">Sean Hendryx</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleCap: Automatic Speaking-Style Captioning from Speech Based on Speech and Language Self-supervised Learning Models. (arXiv:2311.16509v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.16509","description":"<p>We propose StyleCap, a method to generate natural language descriptions of\nspeaking styles appearing in speech. Although most of conventional techniques\nfor para-/non-linguistic information recognition focus on the category\nclassification or the intensity estimation of pre-defined labels, they cannot\nprovide the reasoning of the recognition result in an interpretable manner.\nStyleCap is a first step towards an end-to-end method for generating\nspeaking-style prompts from speech, i.e., automatic speaking-style captioning.\nStyleCap is trained with paired data of speech and natural language\ndescriptions. We train neural networks that convert a speech representation\nvector into prefix vectors that are fed into a large language model (LLM)-based\ntext decoder. We explore an appropriate text decoder and speech feature\nrepresentation suitable for this new task. The experimental results demonstrate\nthat our StyleCap leveraging richer LLMs for the text decoder, speech\nself-supervised learning (SSL) features, and sentence rephrasing augmentation\nimproves the accuracy and diversity of generated speaking-style captions.\nSamples of speaking-style captions generated by our StyleCap are publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamauchi_K/0/1/0/all/0/1\">Kazuki Yamauchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ijima_Y/0/1/0/all/0/1\">Yusuke Ijima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_Y/0/1/0/all/0/1\">Yuki Saito</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing AI Chatbots Performance in Comprehensive Standardized Test Preparation; A Case Study with GRE. (arXiv:2312.03719v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.03719","description":"<p>This research paper presents an analysis of how well three artificial\nintelligence chatbots, Bing, ChatGPT, and GPT-4, perform when answering\nquestions from standardized tests. The Graduate Record Examination (GRE) is\nused in this paper as a case study. A total of 137 questions with different\nforms of quantitative reasoning and 157 questions with verbal categories were\nused to assess their capabilities. This paper presents the performance of each\nchatbot across various skills and styles tested in the exam. This paper also\nexplores the proficiency of these chatbots in addressing image-based questions\nand illustrates the uncertainty level of each chatbot. The results show varying\ndegrees of success across the chatbots, where GPT-4 served as the most\nproficient, especially in complex language understanding tasks and image-based\nquestions. Results highlight the ability of these chatbots to pass the GRE with\na high score, which encourages the use of these chatbots in test preparation.\nThe results also show how important it is to ensure that, if the test is\nadministered online, as it was during COVID, the test taker is segregated from\nthese resources for a fair competition on higher education opportunities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abu_Haifa_M/0/1/0/all/0/1\">Mohammad Abu-Haifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etawi_B/0/1/0/all/0/1\">Bara&#x27;a Etawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alkhatatbeh_H/0/1/0/all/0/1\">Huthaifa Alkhatatbeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ababneh_A/0/1/0/all/0/1\">Ayman Ababneh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Steering Llama 2 via Contrastive Activation Addition. (arXiv:2312.06681v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.06681","description":"<p>We introduce Contrastive Activation Addition (CAA), an innovative method for\nsteering language models by modifying activations during their forward passes.\nCAA computes ``steering vectors'' by averaging the difference in residual\nstream activations between pairs of positive and negative examples of a\nparticular behavior such as factual versus hallucinatory responses. During\ninference, these steering vectors are added at all token positions after the\nuser's prompt with either a positive or negative coefficient, allowing precise\ncontrol over the degree of the targeted behavior. We evaluate CAA's\neffectiveness on Llama 2 Chat using both multiple-choice behavioral question\ndatasets and open-ended generation tasks. We demonstrate that CAA significantly\nalters model behavior, outperforms traditional methods like finetuning and\nfew-shot prompting, and minimally reduces capabilities. Moreover, by employing\nvarious activation space interpretation methods, we gain deeper insights into\nCAA's mechanisms. CAA both accurately steers model outputs and also sheds light\non how high-level concepts are represented in Large Language Models (LLMs).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rimsky_N/0/1/0/all/0/1\">Nina Rimsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabrieli_N/0/1/0/all/0/1\">Nick Gabrieli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_J/0/1/0/all/0/1\">Julian Schulz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1\">Meg Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubinger_E/0/1/0/all/0/1\">Evan Hubinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_A/0/1/0/all/0/1\">Alexander Matt Turner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models. (arXiv:2312.07492v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.07492","description":"<p>Current datasets for unwanted social bias auditing are limited to studying\nprotected demographic features such as race and gender. In this work, we\nintroduce a comprehensive benchmark that is meant to capture the amplification\nof social bias, via stigmas, in generative language models. Taking inspiration\nfrom social science research, we start with a documented list of 93 US-centric\nstigmas and curate a question-answering (QA) dataset which involves simple\nsocial situations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts,\nwith a variety of prompt styles, carefully constructed to systematically test\nfor both social bias and model robustness. We present results for\nSocialStigmaQA with two open source generative language models and we find that\nthe proportion of socially biased output ranges from 45% to 59% across a\nvariety of decoding strategies and prompting styles. We demonstrate that the\ndeliberate design of the templates in our benchmark (e.g., adding biasing text\nto the prompt or using different verbs that change the answer that indicates\nbias) impacts the model tendencies to generate socially biased output.\nAdditionally, through manual evaluation, we discover problematic patterns in\nthe generated chain-of-thought output that range from subtle bias to lack of\nreasoning.\n</p>\n<p>Warning: This paper contains examples of text which are toxic, biased, and\npotentially harmful.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagireddy_M/0/1/0/all/0/1\">Manish Nagireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiazor_L/0/1/0/all/0/1\">Lamogha Chiazor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Moninder Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldini_I/0/1/0/all/0/1\">Ioana Baldini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic Image-Report Generation. (arXiv:2312.08078v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2312.08078","description":"<p>To address these issues, we propose a novel Adaptive patch-word Matching\n(AdaMatch) model to correlate chest X-ray (CXR) image regions with words in\nmedical reports and apply it to CXR-report generation to provide explainability\nfor the generation process. AdaMatch exploits the fine-grained relation between\nadaptive patches and words to provide explanations of specific image regions\nwith corresponding words. To capture the abnormal regions of varying sizes and\npositions, we introduce the Adaptive Patch extraction (AdaPatch) module to\nacquire the adaptive patches for these regions adaptively. In order to provide\nexplicit explainability for CXR-report generation task, we propose an\nAdaMatch-based bidirectional large language model for Cyclic CXR-report\ngeneration (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords\nfor CXR images and `keypatches' for medical reports as hints to guide\nCXR-report generation. Extensive experiments on two publicly available CXR\ndatasets prove the effectiveness of our method and its superior performance to\nexisting methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yixuan Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations. (arXiv:2312.08935v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2312.08935","description":"<p>In this paper, we present an innovative process-oriented math process reward\nmodel called \\textbf{Math-Shepherd}, which assigns a reward score to each step\nof math problem solutions. The training of Math-Shepherd is achieved using\nautomatically constructed process-wise supervision data, breaking the\nbottleneck of heavy reliance on manual annotation in existing work. We explore\nthe effectiveness of Math-Shepherd in two scenarios: 1) \\textit{Verification}:\nMath-Shepherd is utilized for reranking multiple outputs generated by Large\nLanguage Models (LLMs); 2) \\textit{Reinforcement Learning}: Math-Shepherd is\nemployed to reinforce LLMs with step-by-step Proximal Policy Optimization\n(PPO). With Math-Shepherd, a series of open-source LLMs demonstrates\nexceptional performance. For instance, the step-by-step PPO with Math-Shepherd\nsignificantly improves the accuracy of Mistral-7B (77.9\\%$\\to$84.1\\% on GSM8K\nand 28.6\\%$\\to$33.0\\% on MATH). The accuracy can be further enhanced to 89.1\\%\nand 43.5\\% on GSM8K and MATH with the verification of Math-Shepherd,\nrespectively. We believe that automatic process supervision holds significant\npotential for the future evolution of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhihong Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">R.X. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Deli Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Y.Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RJUA-QA: A Comprehensive QA Dataset for Urology. (arXiv:2312.09785v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.09785","description":"<p>We introduce RJUA-QA, a novel medical dataset for question answering (QA) and\nreasoning with clinical evidence, contributing to bridge the gap between\ngeneral large language models (LLMs) and medical-specific LLM applications.\nRJUA-QA is derived from realistic clinical scenarios and aims to facilitate\nLLMs in generating reliable diagnostic and advice. The dataset contains 2,132\ncurated Question-Context-Answer pairs, corresponding about 25,000 diagnostic\nrecords and clinical cases. The dataset covers 67 common urological disease\ncategories, where the disease coverage exceeds 97.6\\% of the population seeking\nmedical services in urology. Each data instance in RJUA-QA comprises: (1) a\nquestion mirroring real patient to inquiry about clinical symptoms and medical\nconditions, (2) a context including comprehensive expert knowledge, serving as\na reference for medical examination and diagnosis, (3) a doctor response\noffering the diagnostic conclusion and suggested examination guidance, (4) a\ndiagnosed clinical disease as the recommended diagnostic outcome, and (5)\nclinical advice providing recommendations for medical examination. RJUA-QA is\nthe first medical QA dataset for clinical reasoning over the patient inquiries,\nwhere expert-level knowledge and experience are required for yielding\ndiagnostic conclusions and medical examination advice. A comprehensive\nevaluation is conducted to evaluate the performance of both medical-specific\nand general LLMs on the RJUA-QA dataset. Our data is are publicly available at\n\\url{https://github.com/alipay/RJU_Ant_QA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_C/0/1/0/all/0/1\">Chenfei Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hongbo Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1\">Lei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Deng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1\">Xianguo Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fangzhou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaowei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yue Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjie Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Wei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yiran Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graphs and Pre-trained Language Models enhanced Representation Learning for Conversational Recommender Systems. (arXiv:2312.10967v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.10967","description":"<p>Conversational recommender systems (CRS) utilize natural language\ninteractions and dialogue history to infer user preferences and provide\naccurate recommendations. Due to the limited conversation context and\nbackground knowledge, existing CRSs rely on external sources such as knowledge\ngraphs to enrich the context and model entities based on their inter-relations.\nHowever, these methods ignore the rich intrinsic information within entities.\nTo address this, we introduce the Knowledge-Enhanced Entity Representation\nLearning (KERL) framework, which leverages both the knowledge graph and a\npre-trained language model to improve the semantic understanding of entities\nfor CRS. In our KERL framework, entity textual descriptions are encoded via a\npre-trained language model, while a knowledge graph helps reinforce the\nrepresentation of these entities. We also employ positional encoding to\neffectively capture the temporal information of entities in a conversation. The\nenhanced entity representation is then used to develop a recommender component\nthat fuses both entity and contextual representations for more informed\nrecommendations, as well as a dialogue component that generates informative\nentity-related information in the response text. A high-quality knowledge graph\nwith aligned entity descriptions is constructed to facilitate our study, namely\nthe Wiki Movie Knowledge Graph (WikiMKG). The experimental results show that\nKERL achieves state-of-the-art results in both recommendation and response\ngeneration tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhangchi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1\">Ye Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liew_A/0/1/0/all/0/1\">Alan Wee-Chung Liew</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Paraphrasing The Original Text\" Makes High Accuracy Long-Context QA. (arXiv:2312.11193v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.11193","description":"<p>Most open-source LLMs still have a context window of no more than 4k,\nlimiting their ability to handle long-context problems. Meanwhile, even those\nwith a long context window still lack satisfactory accuracy. To address this\nissue, we explore from the perspective of training data and theoretically prove\ntraining the capability to handle long contexts requires \"effective\" rather\nthan \"long\" data. Based on this, we propose using the \"original text\nparaphrase\" task, and successfully extend the context window of the existing\nmodel to 32k by a low-cost and effective method, achieving the SOTA accuracy in\nmulti-document-QA among models of the same scale. The model and training data\nhave been open-sourced on\nHuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k) and\nWiseModel(https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yijiong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Instruction Quality: LIFT is What You Need. (arXiv:2312.11508v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.11508","description":"<p>Instruction tuning, a specialized technique to enhance large language model\n(LLM) performance via instruction datasets, relies heavily on the quality of\nemployed data. Existing quality improvement methods alter instruction data\nthrough dataset expansion or curation. However, the expansion method risks data\nredundancy, potentially compromising LLM performance, while the curation\napproach confines the LLM's potential to the original dataset. Our aim is to\nsurpass the original data quality without encountering these shortcomings. To\nachieve this, we propose LIFT (LLM Instruction Fusion Transfer), a novel and\nversatile paradigm designed to elevate the instruction quality to new heights.\nLIFT strategically broadens data distribution to encompass more high-quality\nsubspaces and eliminates redundancy, concentrating on high-quality segments\nacross overall data subspaces. Experimental results demonstrate that, even with\na limited quantity of high-quality instruction data selected by our paradigm,\nLLMs not only consistently uphold robust performance across various tasks but\nalso surpass some state-of-the-art results, highlighting the significant\nimprovement in instruction quality achieved by our paradigm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yongqiang Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_M/0/1/0/all/0/1\">Mengnan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Maoquan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1\">Bin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1\">Neel Sundaresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LatestEval: Addressing Data Contamination in Language Model Evaluation through Dynamic and Time-Sensitive Test Construction. (arXiv:2312.12343v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.12343","description":"<p>Data contamination in evaluation is getting increasingly prevalent with the\nemergence of language models pre-trained on super large, automatically crawled\ncorpora. This problem leads to significant challenges in the accurate\nassessment of model capabilities and generalisations. In this paper, we propose\nLatestEval, an automatic method that leverages the most recent texts to create\nuncontaminated reading comprehension evaluations. LatestEval avoids data\ncontamination by only using texts published within a recent time window,\nensuring no overlap with the training corpora of pre-trained language models.\nWe develop the LatestEval automated pipeline to 1) gather the latest texts; 2)\nidentify key information, and 3) construct questions targeting the information\nwhile removing the existing answers from the context. This encourages models to\ninfer the answers themselves based on the remaining context, rather than just\ncopy-paste. Our experiments demonstrate that language models exhibit negligible\nmemorisation behaviours on LatestEval as opposed to previous benchmarks,\nsuggesting a significantly reduced risk of data contamination and leading to a\nmore robust evaluation. Data and code are publicly available at:\nhttps://github.com/liyucheng09/LatestEval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yucheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1\">Frank Guerin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Early Detection of Hallucinations in Factual Question Answering. (arXiv:2312.14183v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.14183","description":"<p>While large language models (LLMs) have taken great strides towards helping\nhumans with a plethora of tasks like search and summarization, hallucinations\nremain a major impediment towards gaining user trust. The fluency and coherence\nof model generations even when hallucinating makes it difficult to detect\nwhether or not a model is hallucinating. In this work, we explore if the\nartifacts associated with the model generations can provide hints that the\ngeneration will contain hallucinations. Specifically, we probe LLMs at 1) the\ninputs via Integrated Gradients based token attribution, 2) the outputs via the\nSoftmax probabilities, and 3) the internal state via self-attention and\nfully-connected layer activations for signs of hallucinations on open-ended\nquestion answering tasks. Our results show that the distributions of these\nartifacts differ between hallucinated and non-hallucinated generations.\nBuilding on this insight, we train binary classifiers that use these artifacts\nas input features to classify model generations into hallucinations and\nnon-hallucinations. These hallucination classifiers achieve up to 0.80 AUROC.\nWe further show that tokens preceding a hallucination can predict the\nsubsequent hallucination before it occurs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Snyder_B/0/1/0/all/0/1\">Ben Snyder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moisescu_M/0/1/0/all/0/1\">Marius Moisescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafar_M/0/1/0/all/0/1\">Muhammad Bilal Zafar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reverse Multi-Choice Dialogue Commonsense Inference with Graph-of-Thought. (arXiv:2312.15291v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.15291","description":"<p>With the proliferation of dialogic data across the Internet, the Dialogue\nCommonsense Multi-choice Question Answering (DC-MCQ) task has emerged as a\nresponse to the challenge of comprehending user queries and intentions.\nAlthough prevailing methodologies exhibit effectiveness in addressing\nsingle-choice questions, they encounter difficulties in handling multi-choice\nqueries due to the heightened intricacy and informational density. In this\npaper, inspired by the human cognitive process of progressively excluding\noptions, we propose a three-step Reverse Exclusion Graph-of-Thought (ReX-GoT)\nframework, including Option Exclusion, Error Analysis, and Combine Information.\nSpecifically, our ReX-GoT mimics human reasoning by gradually excluding\nirrelevant options and learning the reasons for option errors to choose the\noptimal path of the GoT and ultimately infer the correct answer. By\nprogressively integrating intricate clues, our method effectively reduces the\ndifficulty of multi-choice reasoning and provides a novel solution for DC-MCQ.\nExtensive experiments on the CICERO and CICERO$_{v2}$ datasets validate the\nsignificant improvement of our approach on DC-MCQ task. On zero-shot setting,\nour model outperform the best baseline by 17.67% in terms of F1 score for the\nmulti-choice task. Most strikingly, our GPT3.5-based ReX-GoT framework achieves\na remarkable 39.44% increase in F1 score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Li Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hao Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bobo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Lizi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1\">Donghong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_C/0/1/0/all/0/1\">Chong Teng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Media Bias Taxonomy: A Systematic Literature Review on the Forms and Automated Detection of Media Bias. (arXiv:2312.16148v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.16148","description":"<p>The way the media presents events can significantly affect public perception,\nwhich in turn can alter people's beliefs and views. Media bias describes a\none-sided or polarizing perspective on a topic. This article summarizes the\nresearch on computational methods to detect media bias by systematically\nreviewing 3140 research papers published between 2019 and 2022. To structure\nour review and support a mutual understanding of bias across research domains,\nwe introduce the Media Bias Taxonomy, which provides a coherent overview of the\ncurrent state of research on media bias from different perspectives. We show\nthat media bias detection is a highly active research field, in which\ntransformer-based classification approaches have led to significant\nimprovements in recent years. These improvements include higher classification\naccuracy and the ability to detect more fine-granular types of bias. However,\nwe have identified a lack of interdisciplinarity in existing projects, and a\nneed for more awareness of the various types of media bias to support\nmethodologically thorough performance evaluations of media bias detection\nsystems. Concluding from our analysis, we see the integration of recent machine\nlearning advancements with reliable and diverse bias assessment strategies from\nother research areas as the most promising area for future research\ncontributions in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spinde_T/0/1/0/all/0/1\">Timo Spinde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinterreiter_S/0/1/0/all/0/1\">Smilla Hinterreiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haak_F/0/1/0/all/0/1\">Fabian Haak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giese_H/0/1/0/all/0/1\">Helge Giese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1\">Norman Meuschke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Multiple Intent Detection and Slot Filling with Supervised Contrastive Learning and Self-Distillation. (arXiv:2308.14654v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2308.14654","description":"<p>Multiple intent detection and slot filling are two fundamental and crucial\ntasks in spoken language understanding. Motivated by the fact that the two\ntasks are closely related, joint models that can detect intents and extract\nslots simultaneously are preferred to individual models that perform each task\nindependently. The accuracy of a joint model depends heavily on the ability of\nthe model to transfer information between the two tasks so that the result of\none task can correct the result of the other. In addition, since a joint model\nhas multiple outputs, how to train the model effectively is also challenging.\nIn this paper, we present a method for multiple intent detection and slot\nfilling by addressing these challenges. First, we propose a bidirectional joint\nmodel that explicitly employs intent information to recognize slots and slot\nfeatures to detect intents. Second, we introduce a novel method for training\nthe proposed joint model using supervised contrastive learning and\nself-distillation. Experimental results on two benchmark datasets MixATIS and\nMixSNIPS show that our method outperforms state-of-the-art models in both\ntasks. The results also demonstrate the contributions of both bidirectional\ndesign and the training method to the accuracy improvement. Our source code is\navailable at https://github.com/anhtunguyen98/BiSLU\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_N/0/1/0/all/0/1\">Nguyen Anh Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uyen_H/0/1/0/all/0/1\">Hoang Thi Thu Uyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phuong_T/0/1/0/all/0/1\">Tu Minh Phuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_N/0/1/0/all/0/1\">Ngo Xuan Bach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-12-28T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}