{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-03-14T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference. (arXiv:2303.06182v1 [cs.DC])","link":"http://arxiv.org/abs/2303.06182","description":"<p>Mixture-of-Experts (MoE) models have recently gained steam in achieving the\nstate-of-the-art performance in a wide range of tasks in computer vision and\nnatural language processing. They effectively expand the model capacity while\nincurring a minimal increase in computation cost during training. However,\ndeploying such models for inference is difficult due to their large model size\nand complex communication pattern. In this work, we provide a characterization\nof two MoE workloads, namely Language Modeling (LM) and Machine Translation\n(MT) and identify their sources of inefficiencies at deployment.\n</p>\n<p>We propose three optimization techniques to mitigate sources of\ninefficiencies, namely (1) Dynamic gating, (2) Expert Buffering, and (3) Expert\nload balancing. We show that dynamic gating improves execution time by\n1.25-4$\\times$ for LM, 2-5$\\times$ for MT Encoder and 1.09-1.5$\\times$ for MT\nDecoder. It also reduces memory usage by up to 1.36$\\times$ for LM and up to\n1.1$\\times$ for MT. We further propose Expert Buffering, a new caching\nmechanism that only keeps hot, active experts in GPU memory while buffering the\nrest in CPU memory. This reduces static memory allocation by 1.47$\\times$. We\nfinally propose a load balancing methodology that provides additional\nrobustness to the workload. The code will be open-sourced upon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haiyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardalani_N/0/1/0/all/0/1\">Newsha Ardalani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Anna Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1\">Liu Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsien-Hsin S. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1\">Anjali Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhosale_S/0/1/0/all/0/1\">Shruti Bhosale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Carole-Jean Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Benjamin Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Query Focused Summaries without Fine-tuning the Transformer-based Pre-trained Models. (arXiv:2303.06230v1 [cs.CL])","link":"http://arxiv.org/abs/2303.06230","description":"<p>Fine-tuning the Natural Language Processing (NLP) models for each new data\nset requires higher computational time associated with increased carbon\nfootprint and cost. However, fine-tuning helps the pre-trained models adapt to\nthe latest data sets; what if we avoid the fine-tuning steps and attempt to\ngenerate summaries using just the pre-trained models to reduce computational\ntime and cost. In this paper, we tried to omit the fine-tuning steps and\ninvestigate whether the Marginal Maximum Relevance (MMR)-based approach can\nhelp the pre-trained models to obtain query-focused summaries directly from a\nnew data set that was not used to pre-train the models. First, we used topic\nmodelling on Wikipedia Current Events Portal (WCEP) and Debatepedia datasets to\ngenerate queries for summarization tasks. Then, using MMR, we ranked the\nsentences of the documents according to the queries. Next, we passed the ranked\nsentences to seven transformer-based pre-trained models to perform the\nsummarization tasks. Finally, we used the MMR approach again to select the\nquery relevant sentences from the generated summaries of individual pre-trained\nmodels and constructed the final summary. As indicated by the experimental\nresults, our MMR-based approach successfully ranked and selected the most\nrelevant sentences as summaries and showed better performance than the\nindividual pre-trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdullah_D/0/1/0/all/0/1\">Deen Abdullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_S/0/1/0/all/0/1\">Shamanth Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suri_G/0/1/0/all/0/1\">Gandharv Suri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chali_Y/0/1/0/all/0/1\">Yllias Chali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AUTODIAL: Efficient Asynchronous Task-Oriented Dialogue Model. (arXiv:2303.06245v1 [cs.CL])","link":"http://arxiv.org/abs/2303.06245","description":"<p>As large dialogue models become commonplace in practice, the problems\nsurrounding high compute requirements for training, inference and larger memory\nfootprint still persists. In this work, we present AUTODIAL, a multi-task\ndialogue model that addresses the challenges of deploying dialogue model.\nAUTODIAL utilizes parallel decoders to perform tasks such as dialogue act\nprediction, domain prediction, intent prediction, and dialogue state tracking.\nUsing classification decoders over generative decoders allows AUTODIAL to\nsignificantly reduce memory footprint and achieve faster inference times\ncompared to existing generative approach namely SimpleTOD. We demonstrate that\nAUTODIAL provides 3-6x speedups during inference while having 11x fewer\nparameters on three dialogue tasks compared to SimpleTOD. Our results show that\nextending current dialogue models to have parallel decoders can be a viable\nalternative for deploying them in resource-constrained environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhargava_P/0/1/0/all/0/1\">Prajjwal Bhargava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_P/0/1/0/all/0/1\">Pooyan Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shayandeh_S/0/1/0/all/0/1\">Shahin Shayandeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1\">Chinnadhurai Sankar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Interactive UI to Support Sensemaking over Collections of Parallel Texts. (arXiv:2303.06264v1 [cs.HC])","link":"http://arxiv.org/abs/2303.06264","description":"<p>Scientists and science journalists, among others, often need to make sense of\na large number of papers and how they compare with each other in scope, focus,\nfindings, or any other important factors. However, with a large corpus of\npapers, it's cognitively demanding to pairwise compare and contrast them all\nwith each other. Fully automating this review process would be infeasible,\nbecause it often requires domain-specific knowledge, as well as understanding\nwhat the context and motivations for the review are. While there are existing\ntools to help with the process of organizing and annotating papers for\nliterature reviews, at the core they still rely on people to serially read\nthrough papers and manually make sense of relevant information.\n</p>\n<p>We present AVTALER, which combines peoples' unique skills, contextual\nawareness, and knowledge, together with the strength of automation. Given a set\nof comparable text excerpts from a paper corpus, it supports users in\nsensemaking and contrasting paper attributes by interactively aligning text\nexcerpts in a table so that comparable details are presented in a shared\ncolumn. AVTALER is based on a core alignment algorithm that makes use of modern\nNLP tools. Furthermore, AVTALER is a mixed-initiative system: users can\ninteractively give the system constraints which are integrated into the\nalignment construction process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joyce Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glassman_E/0/1/0/all/0/1\">Elena Glassman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel S. Weld</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistency Analysis of ChatGPT. (arXiv:2303.06273v1 [cs.CL])","link":"http://arxiv.org/abs/2303.06273","description":"<p>ChatGPT, a question-and-answer dialogue system based on a large language\nmodel, has gained huge popularity since its introduction. Its positive aspects\nhave been reported through many media platforms, and some analyses even showed\nthat ChatGPT achieved a decent grade in professional exams, including the law,\nmedical, and finance domains, adding extra support to the claim that AI now can\nassist and, even, replace humans in industrial fields. Others, however, doubt\nits reliability and trustworthiness. In this paper, we investigate ChatGPT's\ntrustworthiness regarding logically consistent behaviours. Our findings suggest\nthat, although ChatGPT seems to achieve an improved language understanding\nability, it still fails to generate logically correct predictions frequently.\nHence, while it is true that ChatGPT is an impressive and promising new\ntechnique, we conclude that its usage in real-world applications without\nthorough human inspection requires further consideration, especially for\nrisk-sensitive areas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_M/0/1/0/all/0/1\">Myeongjun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stabilizing Transformer Training by Preventing Attention Entropy Collapse. (arXiv:2303.06296v1 [cs.LG])","link":"http://arxiv.org/abs/2303.06296","description":"<p>Training stability is of great importance to Transformers. In this work, we\ninvestigate the training dynamics of Transformers by examining the evolution of\nthe attention layers. In particular, we track the attention entropy for each\nattention head during the course of training, which is a proxy for model\nsharpness. We identify a common pattern across different architectures and\ntasks, where low attention entropy is accompanied by high training instability,\nwhich can take the form of oscillating loss or divergence. We denote the\npathologically low attention entropy, corresponding to highly concentrated\nattention scores, as $\\textit{entropy collapse}$. As a remedy, we propose\n$\\sigma$Reparam, a simple and efficient solution where we reparametrize all\nlinear layers with spectral normalization and an additional learned scalar. We\ndemonstrate that the proposed reparameterization successfully prevents entropy\ncollapse in the attention layers, promoting more stable training. Additionally,\nwe prove a tight lower bound of the attention entropy, which decreases\nexponentially fast with the spectral norm of the attention logits, providing\nadditional motivation for our approach. We conduct experiments with\n$\\sigma$Reparam on image classification, image self-supervised learning,\nmachine translation, automatic speech recognition, and language modeling tasks,\nacross Transformer architectures. We show that $\\sigma$Reparam provides\nstability and robustness with respect to the choice of hyperparameters, going\nso far as enabling training (a) a Vision Transformer to competitive performance\nwithout warmup, weight decay, layer normalization or adaptive optimizers; (b)\ndeep architectures in machine translation and (c) speech recognition to\ncompetitive performance without warmup and adaptive optimizers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1\">Shuangfei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Littwin_E/0/1/0/all/0/1\">Etai Littwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busbridge_D/0/1/0/all/0/1\">Dan Busbridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramapuram_J/0/1/0/all/0/1\">Jason Ramapuram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiatao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1\">Josh Susskind</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parachute: Evaluating Interactive Human-LM Co-writing Systems. (arXiv:2303.06333v1 [cs.HC])","link":"http://arxiv.org/abs/2303.06333","description":"<p>A surge of advances in language models (LMs) has led to significant interest\nin using LMs to build co-writing systems, in which humans and LMs interactively\ncontribute to a shared writing artifact. However, there is a lack of studies\nassessing co-writing systems in interactive settings. We propose a\nhuman-centered evaluation framework, Parachute, for interactive co-writing\nsystems. Parachute showcases an integrative view of interaction evaluation,\nwhere each evaluation aspect consists of categorized practical metrics.\nFurthermore, we present Parachute with a use case to demonstrate how to\nevaluate and compare co-writing systems using Parachute.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and Multilingual Natural Language Generation. (arXiv:2303.06458v1 [cs.CL])","link":"http://arxiv.org/abs/2303.06458","description":"<p>Natural Language Generation (NLG) accepts input data in the form of images,\nvideos, or text and generates corresponding natural language text as output.\nExisting NLG methods mainly adopt a supervised approach and rely heavily on\ncoupled data-to-text pairs. However, for many targeted scenarios and for\nnon-English languages, sufficient quantities of labeled data are often not\navailable. To relax the dependency on labeled data of downstream tasks, we\npropose an intuitive and effective zero-shot learning framework, ZeroNLG, which\ncan deal with multiple NLG tasks, including image-to-text (image captioning),\nvideo-to-text (video captioning), and text-to-text (neural machine\ntranslation), across English, Chinese, German, and French within a unified\nframework. ZeroNLG does not require any labeled downstream pairs for training.\nDuring training, ZeroNLG (i) projects different domains (across modalities and\nlanguages) to corresponding coordinates in a shared common latent space; (ii)\nbridges different domains by aligning their corresponding coordinates in this\nspace; and (iii) builds an unsupervised multilingual auto-encoder to learn to\ngenerate text by reconstructing the input text given its coordinate in shared\nlatent space. Consequently, during inference, based on the data-to-text\npipeline, ZeroNLG can generate target sentences across different languages\ngiven the coordinate of input data in the common space. Within this unified\nframework, given visual (imaging or video) data as input, ZeroNLG can perform\nzero-shot visual captioning; given textual sentences as input, ZeroNLG can\nperform zero-shot machine translation. We present the results of extensive\nexperiments on twelve NLG tasks, showing that, without using any labeled\ndownstream pairs for training, ZeroNLG generates high-quality and believable\noutputs and significantly outperforms existing zero-shot methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1\">David A. Clifton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transcription free filler word detection with Neural semi-CRFs. (arXiv:2303.06475v1 [eess.AS])","link":"http://arxiv.org/abs/2303.06475","description":"<p>Non-linguistic filler words, such as \"uh\" or \"um\", are prevalent in\nspontaneous speech and serve as indicators for expressing hesitation or\nuncertainty. Previous works for detecting certain non-linguistic filler words\nare highly dependent on transcriptions from a well-established commercial\nautomatic speech recognition (ASR) system. However, certain ASR systems are not\nuniversally accessible from many aspects, e.g., budget, target languages, and\ncomputational power. In this work, we investigate filler word detection system\nthat does not depend on ASR systems. We show that, by using the structured\nstate space sequence model (S4) and neural semi-Markov conditional random\nfields (semi-CRFs), we achieve an absolute F1 improvement of 6.4% (segment\nlevel) and 3.1% (event level) on the PodcastFillers dataset. We also conduct a\nqualitative analysis on the detected results to analyze the limitations of our\nproposed system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhu_G/0/1/0/all/0/1\">Ge Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Yujia Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Caceres_J/0/1/0/all/0/1\">Juan-Pablo Caceres</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_Z/0/1/0/all/0/1\">Zhiyao Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Attention Networks Can Process Bounded Hierarchical Languages. (arXiv:2105.11115v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.11115","description":"<p>Despite their impressive performance in NLP, self-attention networks were\nrecently proved to be limited for processing formal languages with hierarchical\nstructure, such as $\\mathsf{Dyck}_k$, the language consisting of well-nested\nparentheses of $k$ types. This suggested that natural language can be\napproximated well with models that are too weak for formal languages, or that\nthe role of hierarchy and recursion in natural language might be limited. We\nqualify this implication by proving that self-attention networks can process\n$\\mathsf{Dyck}_{k, D}$, the subset of $\\mathsf{Dyck}_{k}$ with depth bounded by\n$D$, which arguably better captures the bounded hierarchical structure of\nnatural language. Specifically, we construct a hard-attention network with\n$D+1$ layers and $O(\\log k)$ memory size (per token per layer) that recognizes\n$\\mathsf{Dyck}_{k, D}$, and a soft-attention network with two layers and\n$O(\\log k)$ memory size that generates $\\mathsf{Dyck}_{k, D}$. Experiments show\nthat self-attention networks trained on $\\mathsf{Dyck}_{k, D}$ generalize to\nlonger inputs with near-perfect accuracy, and also verify the theoretical\nmemory advantage of self-attention networks over recurrent networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Shunyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Binghui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadimitriou_C/0/1/0/all/0/1\">Christos Papadimitriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-trained Language Models in Biomedical Domain: A Systematic Survey. (arXiv:2110.05006v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.05006","description":"<p>Pre-trained language models (PLMs) have been the de facto paradigm for most\nnatural language processing (NLP) tasks. This also benefits biomedical domain:\nresearchers from informatics, medicine, and computer science (CS) communities\npropose various PLMs trained on biomedical datasets, e.g., biomedical text,\nelectronic health records, protein, and DNA sequences for various biomedical\ntasks. However, the cross-discipline characteristics of biomedical PLMs hinder\ntheir spreading among communities; some existing works are isolated from each\nother without comprehensive comparison and discussions. It expects a survey\nthat not only systematically reviews recent advances of biomedical PLMs and\ntheir applications but also standardizes terminology and benchmarks. In this\npaper, we summarize the recent progress of pre-trained language models in the\nbiomedical domain and their applications in biomedical downstream tasks.\nParticularly, we discuss the motivations and propose a taxonomy of existing\nbiomedical PLMs. Their applications in biomedical downstream tasks are\nexhaustively discussed. At last, we illustrate various limitations and future\ntrends, which we hope can provide inspiration for the future research of the\nresearch community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benyou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jiahuan Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhihong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_P/0/1/0/all/0/1\">Prayag Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+fu_J/0/1/0/all/0/1\">Jie fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-view Graph Neural Networks for Knowledge Graph Completion. (arXiv:2112.09231v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.09231","description":"<p>We present an effective graph neural network (GNN)-based knowledge graph\nembedding model, which we name WGE, to capture entity- and relation-focused\ngraph structures. Given a knowledge graph, WGE builds a single undirected\nentity-focused graph that views entities as nodes. WGE also constructs another\nsingle undirected graph from relation-focused constraints, which views entities\nand relations as nodes. WGE then proposes a GNN-based architecture to better\nlearn vector representations of entities and relations from these two single\nentity- and relation-focused graphs. WGE feeds the learned entity and relation\nrepresentations into a weighted score function to return the triple scores for\nknowledge graph completion. Experimental results show that WGE outperforms\nstrong baselines on seven benchmark datasets for knowledge graph completion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tong_V/0/1/0/all/0/1\">Vinh Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dai Quoc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat Quoc Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Sentence Grounding in Videos: A Survey and Future Directions. (arXiv:2201.08071v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08071","description":"<p>Temporal sentence grounding in videos (TSGV), \\aka natural language video\nlocalization (NLVL) or video moment retrieval (VMR), aims to retrieve a\ntemporal moment that semantically corresponds to a language query from an\nuntrimmed video. Connecting computer vision and natural language, TSGV has\ndrawn significant attention from researchers in both communities. This survey\nattempts to provide a summary of fundamental concepts in TSGV and current\nresearch status, as well as future research directions. As the background, we\npresent a common structure of functional components in TSGV, in a tutorial\nstyle: from feature extraction from raw video and language query, to answer\nprediction of the target moment. Then we review the techniques for multimodal\nunderstanding and interaction, which is the key focus of TSGV for effective\nalignment between the two modalities. We construct a taxonomy of TSGV\ntechniques and elaborate the methods in different categories with their\nstrengths and weaknesses. Lastly, we discuss issues with the current TSGV\nresearch and share our insights about promising research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_W/0/1/0/all/0/1\">Wei Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image Captioning. (arXiv:2202.06574v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.06574","description":"<p>Image Captioning is a traditional vision-and-language task that aims to\ngenerate the language description of an image. Recent studies focus on scaling\nup the model size and the number of training data, which significantly increase\nthe cost of model training. Different to these heavy-cost models, we introduce\na lightweight image captioning framework (I-Tuning), which contains a small\nnumber of trainable parameters. We design a novel I-Tuning cross-attention\nmodule to connect the non-trainable pre-trained language decoder GPT2 and\nvision encoder CLIP-ViT. Since most parameters are not required to be updated\nduring training, our framework is lightweight and fast. Experimental results\nconducted on three image captioning benchmarks reveal that our framework\nachieves comparable or better performance than the large-scale baseline\nsystems. But our models contain up to 10 times fewer trainable parameters and\nrequire much fewer data for training compared with state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Ziyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1\">Yadong Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongsheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alternate Intermediate Conditioning with Syllable-level and Character-level Targets for Japanese ASR. (arXiv:2204.00175v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.00175","description":"<p>End-to-end automatic speech recognition directly maps input speech to\ncharacters. However, the mapping can be problematic when several different\npronunciations should be mapped into one character or when one pronunciation is\nshared among many different characters. Japanese ASR suffers the most from such\nmany-to-one and one-to-many mapping problems due to Japanese kanji characters.\nTo alleviate the problems, we introduce explicit interaction between characters\nand syllables using Self-conditioned connectionist temporal classification\n(CTC), in which the upper layers are ``self-conditioned'' on the intermediate\npredictions from the lower layers. The proposed method utilizes character-level\nand syllable-level intermediate predictions as conditioning features to deal\nwith mutual dependency between characters and syllables. Experimental results\non Corpus of Spontaneous Japanese show that the proposed method outperformed\nthe conventional multi-task and Self-conditioned CTC methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fujita_Y/0/1/0/all/0/1\">Yusuke Fujita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komatsu_T/0/1/0/all/0/1\">Tatsuya Komatsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kida_Y/0/1/0/all/0/1\">Yusuke Kida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Token-level Contrastive Framework for Sign Language Translation. (arXiv:2204.04916v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04916","description":"<p>Sign Language Translation (SLT) is a promising technology to bridge the\ncommunication gap between the deaf and the hearing people. Recently,\nresearchers have adopted Neural Machine Translation (NMT) methods, which\nusually require large-scale corpus for training, to achieve SLT. However, the\npublicly available SLT corpus is very limited, which causes the collapse of the\ntoken representations and the inaccuracy of the generated tokens. To alleviate\nthis issue, we propose ConSLT, a novel token-level \\textbf{Con}trastive\nlearning framework for \\textbf{S}ign \\textbf{L}anguage \\textbf{T}ranslation ,\nwhich learns effective token representations by incorporating token-level\ncontrastive learning into the SLT decoding process. Concretely, ConSLT treats\neach token and its counterpart generated by different dropout masks as positive\npairs during decoding, and then randomly samples $K$ tokens in the vocabulary\nthat are not in the current sentence to construct negative examples. We conduct\ncomprehensive experiments on two benchmarks (PHOENIX14T and CSL-Daily) for both\nend-to-end and cascaded settings. The experimental results demonstrate that\nConSLT can achieve better translation quality than the strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1\">Biao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1\">Peigen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Pei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Cong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yidong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaodong Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EasyNLP: A Comprehensive and Easy-to-use Toolkit for Natural Language Processing. (arXiv:2205.00258v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.00258","description":"<p>The success of Pre-Trained Models (PTMs) has reshaped the development of\nNatural Language Processing (NLP). Yet, it is not easy to obtain\nhigh-performing models and deploy them online for industrial practitioners. To\nbridge this gap, EasyNLP is designed to make it easy to build NLP applications,\nwhich supports a comprehensive suite of NLP algorithms. It further features\nknowledge-enhanced pre-training, knowledge distillation and few-shot learning\nfunctionalities for large-scale PTMs, and provides a unified framework of model\ntraining, inference and deployment for real-world applications. Currently,\nEasyNLP has powered over ten business units within Alibaba Group and is\nseamlessly integrated to the Platform of AI (PAI) products on Alibaba Cloud.\nThe source code of our EasyNLP toolkit is released at GitHub\n(https://github.com/alibaba/EasyNLP).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1\">Minghui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Taolin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tingting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Ming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech. (arXiv:2207.01063v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2207.01063","description":"<p>The majority of current Text-to-Speech (TTS) datasets, which are collections\nof individual utterances, contain few conversational aspects. In this paper, we\nintroduce DailyTalk, a high-quality conversational speech dataset designed for\nconversational TTS. We sampled, modified, and recorded 2,541 dialogues from the\nopen-domain dialogue dataset DailyDialog inheriting its annotated attributes.\nOn top of our dataset, we extend prior work as our baseline, where a\nnon-autoregressive TTS is conditioned on historical information in a dialogue.\nFrom the baseline experiment with both general and our novel metrics, we show\nthat DailyTalk can be used as a general TTS dataset, and more than that, our\nbaseline can represent contextual information from DailyTalk. The DailyTalk\ndataset and baseline code are freely available for academic use with CC-BY-SA\n4.0 license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lee_K/0/1/0/all/0/1\">Keon Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_K/0/1/0/all/0/1\">Kyumin Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning ASR pathways: A sparse multilingual ASR model. (arXiv:2209.05735v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2209.05735","description":"<p>Neural network pruning compresses automatic speech recognition (ASR) models\neffectively. However, in multilingual ASR, language-agnostic pruning may lead\nto severe performance drops on some languages because language-agnostic pruning\nmasks may not fit all languages and discard important language-specific\nparameters. In this work, we present ASR pathways, a sparse multilingual ASR\nmodel that activates language-specific sub-networks (\"pathways\"), such that the\nparameters for each language are learned explicitly. With the overlapping\nsub-networks, the shared parameters can also enable knowledge transfer for\nlower-resource languages via joint multilingual training. We propose a novel\nalgorithm to learn ASR pathways, and evaluate the proposed method on 4\nlanguages with a streaming RNN-T model. Our proposed ASR pathways outperform\nboth dense models and a language-agnostically pruned model, and provide better\nperformance on low-resource languages compared to the monolingual sparse\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_M/0/1/0/all/0/1\">Mu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tjandra_A/0/1/0/all/0/1\">Andros Tjandra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chunxi Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Le_D/0/1/0/all/0/1\">Duc Le</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing the impact of contextual information in hate speech detection. (arXiv:2210.00465v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.00465","description":"<p>In recent years, hate speech has gained great relevance in social networks\nand other virtual media because of its intensity and its relationship with\nviolent acts against members of protected groups. Due to the great amount of\ncontent generated by users, great effort has been made in the research and\ndevelopment of automatic tools to aid the analysis and moderation of this\nspeech, at least in its most threatening forms. One of the limitations of\ncurrent approaches to automatic hate speech detection is the lack of context.\nMost studies and resources are performed on data without context; that is,\nisolated messages without any type of conversational context or the topic being\ndiscussed. This restricts the available information to define if a post on a\nsocial network is hateful or not. In this work, we provide a novel corpus for\ncontextualized hate speech detection based on user responses to news posts from\nmedia outlets on Twitter. This corpus was collected in the Rioplatense\ndialectal variety of Spanish and focuses on hate speech associated with the\nCOVID-19 pandemic. Classification experiments using state-of-the-art techniques\nshow evidence that adding contextual information improves hate speech detection\nperformance for two proposed tasks (binary and multi-label prediction). We make\nour code, models, and corpus available for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1\">Juan Manuel P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luque_F/0/1/0/all/0/1\">Franco Luque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zayat_D/0/1/0/all/0/1\">Demian Zayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondratzky_M/0/1/0/all/0/1\">Mart&#xed;n Kondratzky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moro_A/0/1/0/all/0/1\">Agust&#xed;n Moro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serrati_P/0/1/0/all/0/1\">Pablo Serrati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zajac_J/0/1/0/all/0/1\">Joaqu&#xed;n Zajac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miguel_P/0/1/0/all/0/1\">Paula Miguel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Debandi_N/0/1/0/all/0/1\">Natalia Debandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gravano_A/0/1/0/all/0/1\">Agust&#xed;n Gravano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotik_V/0/1/0/all/0/1\">Viviana Cotik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Surprising Computational Power of Nondeterministic Stack RNNs. (arXiv:2210.01343v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.01343","description":"<p>Traditional recurrent neural networks (RNNs) have a fixed, finite number of\nmemory cells. In theory (assuming bounded range and precision), this limits\ntheir formal language recognition power to regular languages, and in practice,\nRNNs have been shown to be unable to learn many context-free languages (CFLs).\nIn order to expand the class of languages RNNs recognize, prior work has\naugmented RNNs with a nondeterministic stack data structure, putting them on\npar with pushdown automata and increasing their language recognition power to\nCFLs. Nondeterminism is needed for recognizing all CFLs (not just deterministic\nCFLs), but in this paper, we show that nondeterminism and the neural controller\ninteract to produce two more unexpected abilities. First, the nondeterministic\nstack RNN can recognize not only CFLs, but also many non-context-free\nlanguages. Second, it can recognize languages with much larger alphabet sizes\nthan one might expect given the size of its stack alphabet. Finally, to\nincrease the information capacity in the stack and allow it to solve more\ncomplicated tasks with large alphabet sizes, we propose a new version of the\nnondeterministic stack that simulates stacks of vectors rather than discrete\nsymbols. We demonstrate perplexity improvements with this new model on the Penn\nTreebank language modeling benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DuSell_B/0/1/0/all/0/1\">Brian DuSell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EDU-level Extractive Summarization with Varying Summary Lengths. (arXiv:2210.04029v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04029","description":"<p>Extractive models usually formulate text summarization as extracting fixed\ntop-$k$ salient sentences from the document as a summary. Few works exploited\nextracting finer-grained Elementary Discourse Unit (EDU) with little analysis\nand justification for the extractive unit selection. Further, the selection\nstrategy of the fixed top-$k$ salient sentences fits the summarization need\npoorly, as the number of salient sentences in different documents varies and\ntherefore a common or best $k$ does not exist in reality. To fill these gaps,\nthis paper first conducts the comparison analysis of oracle summaries based on\nEDUs and sentences, which provides evidence from both theoretical and\nexperimental perspectives to justify and quantify that EDUs make summaries with\nhigher automatic evaluation scores than sentences. Then, considering this merit\nof EDUs, this paper further proposes an EDU-level extractive model with Varying\nsummary Lengths and develops the corresponding learning algorithm. EDU-VL\nlearns to encode and predict probabilities of EDUs in the document, generate\nmultiple candidate summaries with varying lengths based on various $k$ values,\nand encode and score candidate summaries, in an end-to-end training manner.\nFinally, EDU-VL is experimented on single and multi-document benchmark datasets\nand shows improved performances on ROUGE scores in comparison with\nstate-of-the-art extractive models, and further human evaluation suggests that\nEDU-constituent summaries maintain good grammaticality and readability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuping Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_C/0/1/0/all/0/1\">Ching-Hsun Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jiayu Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shengzhong Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1\">Goran Nenadic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiao-Jun Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attribution and Obfuscation of Neural Text Authorship: A Data Mining Perspective. (arXiv:2210.10488v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10488","description":"<p>Two interlocking research questions of growing interest and importance in\nprivacy research are Authorship Attribution (AA) and Authorship Obfuscation\n(AO). Given an artifact, especially a text t in question, an AA solution aims\nto accurately attribute t to its true author out of many candidate authors\nwhile an AO solution aims to modify t to hide its true authorship.\nTraditionally, the notion of authorship and its accompanying privacy concern is\nonly toward human authors. However, in recent years, due to the explosive\nadvancements in Neural Text Generation (NTG) techniques in NLP, capable of\nsynthesizing human-quality open-ended texts (so-called \"neural texts\"), one has\nto now consider authorships by humans, machines, or their combination. Due to\nthe implications and potential threats of neural texts when used maliciously,\nit has become critical to understand the limitations of traditional AA/AO\nsolutions and develop novel AA/AO solutions in dealing with neural texts. In\nthis survey, therefore, we make a comprehensive review of recent literature on\nthe attribution and obfuscation of neural text authorship from a Data Mining\nperspective, and share our view on their limitations and promising research\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uchendu_A/0/1/0/all/0/1\">Adaku Uchendu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thai Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongwon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmentation with Projection: Towards an Effective and Efficient Data Augmentation Paradigm for Distillation. (arXiv:2210.11768v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.11768","description":"<p>Knowledge distillation is one of the primary methods of transferring\nknowledge from large to small models. However, it requires massive\ntask-specific data, which may not be plausible in many real-world applications.\nData augmentation methods such as representation interpolation, token\nreplacement, or augmentation with models are applied to tackle this problem.\nHowever, these data augmentation methods either potentially cause shifts in\ndecision boundaries (representation interpolation), are not expressive enough\n(token replacement), or introduce too much computational overhead (augmentation\nwith models). To this end, we propose AugPro (Augmentation with Projection), an\neffective and efficient data augmentation method for distillation. Our method\nbuilds on top of representation interpolation augmentation methods to maintain\nthe diversity of expressions and converts the augmented data to tokens to avoid\nshifting decision boundaries. It uses simple operations that come with little\ncomputational overhead. The results on multiple GLUE tasks show that our\nmethods can improve distillation performance by a large margin at a low time\ncost. Codes are available at\nhttps://github.com/google-research/google-research/tree/master/augpro.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuexin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Frederick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daogao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Le Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Named Entity Detection and Injection for Direct Speech Translation. (arXiv:2210.11981v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.11981","description":"<p>In a sentence, certain words are critical for its semantic. Among them, named\nentities (NEs) are notoriously challenging for neural models. Despite their\nimportance, their accurate handling has been neglected in speech-to-text (S2T)\ntranslation research, and recent work has shown that S2T models perform poorly\nfor locations and notably person names, whose spelling is challenging unless\nknown in advance. In this work, we explore how to leverage dictionaries of NEs\nknown to likely appear in a given context to improve S2T model outputs. Our\nexperiments show that we can reliably detect NEs likely present in an utterance\nstarting from S2T encoder outputs. Indeed, we demonstrate that the current\ndetection quality is sufficient to improve NE accuracy in the translation with\na 31% reduction in person name errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulikov_I/0/1/0/all/0/1\">Ilia Kulikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rongqing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Articulation GAN: Unsupervised modeling of articulatory learning. (arXiv:2210.15173v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2210.15173","description":"<p>Generative deep neural networks are widely used for speech synthesis, but\nmost existing models directly generate waveforms or spectral outputs. Humans,\nhowever, produce speech by controlling articulators, which results in the\nproduction of speech sounds through physical properties of sound propagation.\nWe introduce the Articulatory Generator to the Generative Adversarial Network\nparadigm, a new unsupervised generative model of speech production/synthesis.\nThe Articulatory Generator more closely mimics human speech production by\nlearning to generate articulatory representations (electromagnetic\narticulography or EMA) in a fully unsupervised manner. A separate pre-trained\nphysical model (ema2wav) then transforms the generated EMA representations to\nspeech waveforms, which get sent to the Discriminator for evaluation.\nArticulatory analysis suggests that the network learns to control articulators\nin a similar manner to humans during speech production. Acoustic analysis of\nthe outputs suggests that the network learns to generate words that are both\npresent and absent in the training distribution. We additionally discuss\nimplications of articulatory representations for cognitive models of human\nlanguage and speech technology in general.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Alan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peter Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anumanchipalli_G/0/1/0/all/0/1\">Gopala K Anumanchipalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Label Correlations in a Multi-label Setting: A Case Study in Emotion. (arXiv:2210.15842v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15842","description":"<p>Detecting emotions expressed in text has become critical to a range of\nfields. In this work, we investigate ways to exploit label correlations in\nmulti-label emotion recognition models to improve emotion detection. First, we\ndevelop two modeling approaches to the problem in order to capture word\nassociations of the emotion words themselves, by either including the emotions\nin the input, or by leveraging Masked Language Modeling (MLM). Second, we\nintegrate pairwise constraints of emotion representations as regularization\nterms alongside the classification loss of the models. We split these terms\ninto two categories, local and global. The former dynamically change based on\nthe gold labels, while the latter remain static during training. We demonstrate\nstate-of-the-art performance across Spanish, English, and Arabic in SemEval\n2018 Task 1 E-c using monolingual BERT-based models. On top of better\nperformance, we also demonstrate improved robustness. Code is available at\nhttps://github.com/gchochla/Demux-MEmo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chochlakis_G/0/1/0/all/0/1\">Georgios Chochlakis</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_G/0/1/0/all/0/1\">Gireesh Mahajan</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Baruah_S/0/1/0/all/0/1\">Sabyasachee Baruah</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Burghardt_K/0/1/0/all/0/1\">Keith Burghardt</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_K/0/1/0/all/0/1\">Kristina Lerman</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a> (1 and 2) ((1) Signal Analysis and Interpretation Lab, University of Southern California, (2) Information Science Institute, University of Southern California, (3) Microsoft Cognitive Services)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Emotion Embeddings to Transfer Knowledge Between Emotions, Languages, and Annotation Formats. (arXiv:2211.00171v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.00171","description":"<p>The need for emotional inference from text continues to diversify as more and\nmore disciplines integrate emotions into their theories and applications. These\nneeds include inferring different emotion types, handling multiple languages,\nand different annotation formats. A shared model between different\nconfigurations would enable the sharing of knowledge and a decrease in training\ncosts, and would simplify the process of deploying emotion recognition models\nin novel environments. In this work, we study how we can build a single model\nthat can transition between these different configurations by leveraging\nmultilingual models and Demux, a transformer-based model whose input includes\nthe emotions of interest, enabling us to dynamically change the emotions\npredicted by the model. Demux also produces emotion embeddings, and performing\noperations on them allows us to transition to clusters of emotions by pooling\nthe embeddings of each cluster. We show that Demux can simultaneously transfer\nknowledge in a zero-shot manner to a new language, to a novel annotation format\nand to unseen emotions. Code is available at\nhttps://github.com/gchochla/Demux-MEmo .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chochlakis_G/0/1/0/all/0/1\">Georgios Chochlakis</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_G/0/1/0/all/0/1\">Gireesh Mahajan</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Baruah_S/0/1/0/all/0/1\">Sabyasachee Baruah</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Burghardt_K/0/1/0/all/0/1\">Keith Burghardt</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_K/0/1/0/all/0/1\">Kristina Lerman</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a> (1 and 2) ((1) Signal Analysis and Interpretation Lab, University of Southern California, (2) Information Science Institute, University of Southern California, (3) Microsoft Cognitive Services)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. (arXiv:2211.05100v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05100","description":"<p>Large language models (LLMs) have been shown to be able to perform new tasks\nbased on a few demonstrations or natural language instructions. While these\ncapabilities have led to widespread adoption, most LLMs are developed by\nresource-rich organizations and are frequently kept from the public. As a step\ntowards democratizing this powerful technology, we present BLOOM, a\n176B-parameter open-access language model designed and built thanks to a\ncollaboration of hundreds of researchers. BLOOM is a decoder-only Transformer\nlanguage model that was trained on the ROOTS corpus, a dataset comprising\nhundreds of sources in 46 natural and 13 programming languages (59 in total).\nWe find that BLOOM achieves competitive performance on a wide variety of\nbenchmarks, with stronger results after undergoing multitask prompted\nfinetuning. To facilitate future research and applications using LLMs, we\npublicly release our models and code under the Responsible AI License.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Workshop_B/0/1/0/all/0/1\">BigScience Workshop</a>: <a href=\"http://arxiv.org/find/cs/1/au:+Scao_T/0/1/0/all/0/1\">Teven Le Scao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1\">Angela Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akiki_C/0/1/0/all/0/1\">Christopher Akiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilic_S/0/1/0/all/0/1\">Suzana Ili&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hesslow_D/0/1/0/all/0/1\">Daniel Hesslow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castagne_R/0/1/0/all/0/1\">Roman Castagn&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luccioni_A/0/1/0/all/0/1\">Alexandra Sasha Luccioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yvon_F/0/1/0/all/0/1\">Fran&#xe7;ois Yvon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galle_M/0/1/0/all/0/1\">Matthias Gall&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tow_J/0/1/0/all/0/1\">Jonathan Tow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammanamanchi_P/0/1/0/all/0/1\">Pawan Sasanka Ammanamanchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Thomas Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1\">Niklas Muennighoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moral_A/0/1/0/all/0/1\">Albert Villanova del Moral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruwase_O/0/1/0/all/0/1\">Olatunji Ruwase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bawden_R/0/1/0/all/0/1\">Rachel Bawden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekman_S/0/1/0/all/0/1\">Stas Bekman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMillan_Major_A/0/1/0/all/0/1\">Angelina McMillan-Major</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saulnier_L/0/1/0/all/0/1\">Lucile Saulnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Samson Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_P/0/1/0/all/0/1\">Pedro Ortiz Suarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanh_V/0/1/0/all/0/1\">Victor Sanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurencon_H/0/1/0/all/0/1\">Hugo Lauren&#xe7;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1\">Yacine Jernite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Launay_J/0/1/0/all/0/1\">Julien Launay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1\">Margaret Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokaslan_A/0/1/0/all/0/1\">Aaron Gokaslan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simhi_A/0/1/0/all/0/1\">Adi Simhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soroa_A/0/1/0/all/0/1\">Aitor Soroa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfassy_A/0/1/0/all/0/1\">Amit Alfassy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_A/0/1/0/all/0/1\">Anna Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nitzav_A/0/1/0/all/0/1\">Ariel Kreisberg Nitzav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_C/0/1/0/all/0/1\">Chenghao Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klamm_C/0/1/0/all/0/1\">Christopher Klamm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leong_C/0/1/0/all/0/1\">Colin Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strien_D/0/1/0/all/0/1\">Daniel van Strien</a>, et al. (344 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accidental Learners: Spoken Language Identification in Multilingual Self-Supervised Models. (arXiv:2211.05103v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2211.05103","description":"<p>In this paper, we extend previous self-supervised approaches for language\nidentification by experimenting with Conformer based architecture in a\nmultilingual pre-training paradigm. We find that pre-trained speech models\noptimally encode language discriminatory information in lower layers. Further,\nwe demonstrate that the embeddings obtained from these layers are significantly\nrobust to classify unseen languages and different acoustic environments without\nadditional training. After fine-tuning a pre-trained Conformer model on the\nVoxLingua107 dataset, we achieve results similar to current state-of-the-art\nsystems for language identification. More, our model accomplishes this with 5x\nless parameters. We open-source the model through the NVIDIA NeMo toolkit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bartley_T/0/1/0/all/0/1\">Travis M. Bartley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_F/0/1/0/all/0/1\">Fei Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Puvvada_K/0/1/0/all/0/1\">Krishna C. Puvvada</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kriman_S/0/1/0/all/0/1\">Samuel Kriman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatically Extracting Information in Medical Dialogue: Expert System And Attention for Labelling. (arXiv:2211.15544v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.15544","description":"<p>Medical dialogue information extraction is becoming an increasingly\nsignificant problem in modern medical care. It is difficult to extract key\ninformation from electronic medical records (EMRs) due to their large numbers.\nPreviously, researchers proposed attention-based models for retrieving features\nfrom EMRs, but their limitations were reflected in their inability to recognize\ndifferent categories in medical dialogues. In this paper, we propose a novel\nmodel, Expert System and Attention for Labelling (ESAL). We use mixture of\nexperts and pre-trained BERT to retrieve the semantics of different categories,\nenabling the model to fuse the differences between them. In our experiment,\nESAL was applied to a public dataset and the experimental results indicated\nthat ESAL significantly improved the performance of Medical Information\nClassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinshi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Daniel Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Transducer Training: Reduced Memory Consumption with Sample-wise Computation. (arXiv:2211.16270v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.16270","description":"<p>The neural transducer is an end-to-end model for automatic speech recognition\n(ASR). While the model is well-suited for streaming ASR, the training process\nremains challenging. During training, the memory requirements may quickly\nexceed the capacity of state-of-the-art GPUs, limiting batch size and sequence\nlengths. In this work, we analyze the time and space complexity of a typical\ntransducer training setup. We propose a memory-efficient training method that\ncomputes the transducer loss and gradients sample by sample. We present\noptimizations to increase the efficiency and parallelism of the sample-wise\nmethod. In a set of thorough benchmarks, we show that our sample-wise method\nsignificantly reduces memory usage, and performs at competitive speed when\ncompared to the default batched computation. As a highlight, we manage to\ncompute the transducer loss and gradients for a batch size of 1024, and audio\nlength of 40 seconds, using only 6 GB of memory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Braun_S/0/1/0/all/0/1\">Stefan Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDermott_E/0/1/0/all/0/1\">Erik McDermott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiao_R/0/1/0/all/0/1\">Roger Hsiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unifying Vision, Text, and Layout for Universal Document Processing. (arXiv:2212.02623v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2212.02623","description":"<p>We propose Universal Document Processing (UDOP), a foundation Document AI\nmodel which unifies text, image, and layout modalities together with varied\ntask formats, including document understanding and generation. UDOP leverages\nthe spatial correlation between textual content and document image to model\nimage, text, and layout modalities with one uniform representation. With a\nnovel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain\ndownstream tasks into a prompt-based sequence generation scheme. UDOP is\npretrained on both large-scale unlabeled document corpora using innovative\nself-supervised objectives and diverse labeled data. UDOP also learns to\ngenerate document images from text and layout modalities via masked image\nreconstruction. To the best of our knowledge, this is the first time in the\nfield of document AI that one model simultaneously achieves high-quality neural\ndocument editing and content customization. Our method sets the\nstate-of-the-art on 8 Document AI tasks, e.g., document understanding and QA,\nacross diverse data domains like finance reports, academic papers, and\nwebsites. UDOP ranks first on the leaderboard of the Document Understanding\nBenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zineng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cha Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Machine Translation with Large Language Models. (arXiv:2301.13294v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.13294","description":"<p>Consistency is a key requirement of high-quality translation. It is\nespecially important to adhere to pre-approved terminology and adapt to\ncorrected translations in domain-specific projects. Machine translation (MT)\nhas achieved significant progress in the area of domain adaptation. However,\nreal-time adaptation remains challenging. Large-scale language models (LLMs)\nhave recently shown interesting capabilities of in-context learning, where they\nlearn to replicate certain input-output text generation patterns, without\nfurther fine-tuning. By feeding an LLM at inference time with a prompt that\nconsists of a list of translation pairs, it can then simulate the domain and\nstyle characteristics. This work aims to investigate how we can utilize\nin-context learning to improve real-time adaptive MT. Our extensive experiments\nshow promising results at translation time. For example, GPT-3.5 can adapt to a\nset of in-domain sentence pairs and/or terminology while translating a new\nsentence. We observe that the translation quality with few-shot in-context\nlearning can surpass that of strong encoder-decoder MT systems, especially for\nhigh-resource languages. Moreover, we investigate whether we can combine MT\nfrom strong encoder-decoder models with fuzzy matches, which can further\nimprove translation quality, especially for less supported languages. We\nconduct our experiments across five diverse language pairs, namely\nEnglish-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French\n(EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moslem_Y/0/1/0/all/0/1\">Yasmin Moslem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haque_R/0/1/0/all/0/1\">Rejwanul Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelleher_J/0/1/0/all/0/1\">John D. Kelleher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Way_A/0/1/0/all/0/1\">Andy Way</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models. (arXiv:2302.07027v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.07027","description":"<p>Pretrained language models (PLMs) are trained on massive corpora, but often\nneed to specialize to specific domains. A parameter-efficient adaptation method\nsuggests training an adapter for each domain on the task of language modeling.\nThis leads to good in-domain scores but can be impractical for domain- or\nresource-restricted settings. A solution is to use a related-domain adapter for\nthe novel domain at test time. In this paper, we introduce AdapterSoup, an\napproach that performs weight-space averaging of adapters trained on different\ndomains. Our approach is embarrassingly parallel: first, we train a set of\ndomain-specific adapters; then, for each novel domain, we determine which\nadapters should be averaged at test time. We present extensive experiments\nshowing that AdapterSoup consistently improves performance to new domains\nwithout extra training. We also explore weight averaging of adapters trained on\nthe same domain with different hyper-parameters, and show that it preserves the\nperformance of a PLM on new domains while obtaining strong in-domain results.\nWe explore various approaches for choosing which adapters to combine, such as\ntext clustering and semantic similarity. We find that using clustering leads to\nthe most competitive results on novel domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chronopoulou_A/0/1/0/all/0/1\">Alexandra Chronopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1\">Matthew E. Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1\">Jesse Dodge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Correct answers\" from the psychology of artificial intelligence. (arXiv:2302.07267v3 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2302.07267","description":"<p>Large Language Models have vastly grown in capabilities. One proposed\napplication of such AI systems is to support data collection in the social and\ncognitive sciences, where perfect experimental control is currently unfeasible\nand the collection of large, representative datasets is generally expensive. In\nthis paper, we re-replicate 14 studies from the Many Labs 2 replication project\nwith OpenAI's text-davinci-003 model, colloquially known as GPT3.5. We\ncollected responses from the default setting of GPT3.5 by inputting each\nstudy's survey as text. Among the eight studies we could analyse, our GPT\nsample replicated 37.5% of the original results as well as 37.5% of the Many\nLabs 2 results. Unexpectedly, we could not analyse the remaining six studies as\nwe had planned in our pre-registration. This was because for each of these six\nstudies, GPT3.5 answered at least one of the survey questions (either a\ndependent variable or a condition variable) in an extremely predetermined way:\nan unexpected phenomenon we call the \"correct answer\" effect. Different runs of\nGPT3.5 answered nuanced questions probing political orientation, economic\npreference, judgement, and moral philosophy with zero or near-zero variation in\nresponses: with the supposedly \"correct answer.\" For example, our survey\nquestions found the default setting of GPT3.5 to almost always self-identify as\na maximally strong conservative (99.6%, N=1,030), and to always be morally\ndeontological in opposing the hypothetical pushing of a large man in front of\nan incoming trolley to save the lives of five people (100%, N=1,030). Since AI\nmodels of the future may be trained on much of the same data as GPT3.5,\ntraining data from which GPT3.5 may have learned its supposedly \"correct\nanswers,\" our results raise concerns that a hypothetical AI-led future may in\ncertain ways be subject to a diminished diversity of thought.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_P/0/1/0/all/0/1\">Peter S. Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoenegger_P/0/1/0/all/0/1\">Philipp Schoenegger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chongyang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Event-based News Narrative Extraction. (arXiv:2302.08351v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.08351","description":"<p>Narratives are fundamental to our understanding of the world, providing us\nwith a natural structure for knowledge representation over time. Computational\nnarrative extraction is a subfield of artificial intelligence that makes heavy\nuse of information retrieval and natural language processing techniques.\nDespite the importance of computational narrative extraction, relatively little\nscholarly work exists on synthesizing previous research and strategizing future\nresearch in the area. In particular, this article focuses on extracting news\nnarratives from an event-centric perspective. Extracting narratives from news\ndata has multiple applications in understanding the evolving information\nlandscape. This survey presents an extensive study of research in the area of\nevent-based news narrative extraction. In particular, we screened over 900\narticles that yielded 54 relevant articles. These articles are synthesized and\norganized by representation model, extraction criteria, and evaluation\napproaches. Based on the reviewed studies, we identify recent trends, open\nchallenges, and potential research lines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Norambuena_B/0/1/0/all/0/1\">Brian Keith Norambuena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_T/0/1/0/all/0/1\">Tanushree Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+North_C/0/1/0/all/0/1\">Chris North</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Reasonability of the Test Set for Simultaneous Machine Translation. (arXiv:2303.00969v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.00969","description":"<p>Simultaneous machine translation (SimulMT) models start translation before\nthe end of the source sentence, making the translation monotonically aligned\nwith the source sentence. However, the general full-sentence translation test\nset is acquired by offline translation of the entire source sentence, which is\nnot designed for SimulMT evaluation, making us rethink whether this will\nunderestimate the performance of SimulMT models. In this paper, we manually\nannotate a monotonic test set based on the MuST-C English-Chinese test set,\ndenoted as SiMuST-C. Our human evaluation confirms the acceptability of our\nannotated test set. Evaluations on three different SimulMT models verify that\nthe underestimation problem can be alleviated on our test set. Further\nexperiments show that finetuning on an automatically extracted monotonic\ntraining set improves SimulMT models by up to 3 BLEU points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengge Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_J/0/1/0/all/0/1\">Jian Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuhang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuoying Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FinXABSA: Explainable Finance through Aspect-Based Sentiment Analysis. (arXiv:2303.02563v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.02563","description":"<p>This paper presents a novel approach for explainability in financial analysis\nby utilizing the Pearson correlation coefficient to establish a relationship\nbetween aspect-based sentiment analysis and stock prices. The proposed\nmethodology involves constructing an aspect list from financial news articles\nand analyzing sentiment intensity scores for each aspect. These scores are then\ncompared to the stock prices for the relevant companies using the Pearson\ncoefficient to determine any significant correlations. The results indicate\nthat the proposed approach provides a more detailed and accurate understanding\nof the relationship between sentiment analysis and stock prices, which can be\nuseful for investors and financial analysts in making informed decisions.\nAdditionally, this methodology offers a transparent and interpretable way to\nexplain the sentiment analysis results and their impact on stock prices.\nOverall, the findings of this paper demonstrate the importance of\nexplainability in financial analysis and highlight the potential benefits of\nutilizing the Pearson coefficient for analyzing aspect-based sentiment analysis\nand stock prices. The proposed approach offers a valuable tool for\nunderstanding the complex relationships between financial news sentiment and\nstock prices, providing a new perspective on the financial market and aiding in\nmaking informed investment decisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ong_K/0/1/0/all/0/1\">Keane Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heever_W/0/1/0/all/0/1\">Wihan van der Heever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satapathy_R/0/1/0/all/0/1\">Ranjan Satapathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mengaldo_G/0/1/0/all/0/1\">Gianmarco Mengaldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing Spurious Correlations for Aspect-Based Sentiment Analysis with Variational Information Bottleneck and Contrastive Learning. (arXiv:2303.02846v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.02846","description":"<p>Deep learning techniques have dominated the literature on aspect-based\nsentiment analysis (ABSA), yielding state-of-the-art results. However, these\ndeep models generally suffer from spurious correlation problems between input\nfeatures and output labels, which creates significant barriers to robustness\nand generalization capability. In this paper, we propose a novel Contrastive\nVariational Information Bottleneck framework (called CVIB) to reduce spurious\ncorrelations for ABSA. The proposed CVIB framework is composed of an original\nnetwork and a self-pruned network, and these two networks are optimized\nsimultaneously via contrastive learning. Concretely, we employ the Variational\nInformation Bottleneck (VIB) principle to learn an informative and compressed\nnetwork (self-pruned network) from the original network, which discards the\nsuperfluous patterns or spurious correlations between input features and\nprediction labels. Then, self-pruning contrastive learning is devised to pull\ntogether semantically similar positive pairs and push away dissimilar pairs,\nwhere the representations of the anchor learned by the original and self-pruned\nnetworks respectively are regarded as a positive pair while the representations\nof two different sentences within a mini-batch are treated as a negative pair.\nTo verify the effectiveness of our CVIB method, we conduct extensive\nexperiments on five benchmark ABSA datasets and the experimental results show\nthat our approach achieves better performance than the strong competitors in\nterms of overall prediction performance, robustness, and generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Mingshan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qingshan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NASTyLinker: NIL-Aware Scalable Transformer-based Entity Linker. (arXiv:2303.04426v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.04426","description":"<p>Entity Linking (EL) is the task of detecting mentions of entities in text and\ndisambiguating them to a reference knowledge base. Most prevalent EL approaches\nassume that the reference knowledge base is complete. In practice, however, it\nis necessary to deal with the case of linking to an entity that is not\ncontained in the knowledge base (NIL entity). Recent works have shown that,\ninstead of focusing only on affinities between mentions and entities,\nconsidering inter-mention affinities can be used to represent NIL entities by\nproducing clusters of mentions. At the same time, inter-mention affinities can\nhelp to substantially improve linking performance for known entities. With\nNASTyLinker, we introduce an EL approach that is aware of NIL entities and\nproduces corresponding mention clusters while maintaining high linking\nperformance for known entities. The approach clusters mentions and entities\nbased on dense representations from Transformers and resolves conflicts (if\nmore than one entity is assigned to a cluster) by computing transitive\nmention-entity affinities. We show the effectiveness and scalability of\nNASTyLinker on NILK, a dataset that is explicitly constructed to evaluate EL\nwith respect to NIL entities. Further, we apply the presented approach to an\nactual EL task, namely to knowledge graph population by linking entities in\nWikipedia listings, and provide an analysis of the outcome.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heist_N/0/1/0/all/0/1\">Nicolas Heist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1\">Heiko Paulheim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning the Legibility of Visual Text Perturbations. (arXiv:2303.05077v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.05077","description":"<p>Many adversarial attacks in NLP perturb inputs to produce visually similar\nstrings ('ergo' $\\rightarrow$ '$\\epsilon$rgo') which are legible to humans but\ndegrade model performance. Although preserving legibility is a necessary\ncondition for text perturbation, little work has been done to systematically\ncharacterize it; instead, legibility is typically loosely enforced via\nintuitions around the nature and extent of perturbations. Particularly, it is\nunclear to what extent can inputs be perturbed while preserving legibility, or\nhow to quantify the legibility of a perturbed string. In this work, we address\nthis gap by learning models that predict the legibility of a perturbed string,\nand rank candidate perturbations based on their legibility. To do so, we\ncollect and release LEGIT, a human-annotated dataset comprising the legibility\nof visually perturbed text. Using this dataset, we build both text- and\nvision-based models which achieve up to $0.91$ F1 score in predicting whether\nan input is legible, and an accuracy of $0.86$ in predicting which of two given\nperturbations is more legible. Additionally, we discover that legible\nperturbations from the LEGIT dataset are more effective at lowering the\nperformance of NLP models than best-known attack strategies, suggesting that\ncurrent models may be vulnerable to a broad range of perturbations beyond what\nis captured by existing visual attacks. Data, code, and models are available at\nhttps://github.com/dvsth/learning-legibility-2023.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seth_D/0/1/0/all/0/1\">Dev Seth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stureborg_R/0/1/0/all/0/1\">Rickard Stureborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1\">Danish Pruthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1\">Bhuwan Dhingra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings. (arXiv:2303.05737v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2303.05737","description":"<p>Automatic Speech Recognition (ASR) in medical contexts has the potential to\nsave time, cut costs, increase report accuracy, and reduce physician burnout.\nHowever, the healthcare industry has been slower to adopt this technology, in\npart due to the importance of avoiding medically-relevant transcription\nmistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR\nmetric that penalizes clinically-relevant mistakes more than others. We\ndemonstrate that this metric more closely aligns with clinician preferences on\nmedical sentences as compared to other metrics (WER, BLUE, METEOR, etc),\nsometimes by wide margins. We collect a benchmark of 13 clinician preferences\non 149 realistic medical sentences called the Clinician Transcript Preference\nbenchmark (CTP), demonstrate that CBERTScore more closely matches what\nclinicians prefer, and release the benchmark for the community to further\ndevelop clinically-aware ASR metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shor_J/0/1/0/all/0/1\">Joel Shor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bi_R/0/1/0/all/0/1\">Ruyue Agnes Bi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Venugopalan_S/0/1/0/all/0/1\">Subhashini Venugopalan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ibara_S/0/1/0/all/0/1\">Steven Ibara</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goldenberg_R/0/1/0/all/0/1\">Roman Goldenberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rivlin_E/0/1/0/all/0/1\">Ehud Rivlin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-03-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}