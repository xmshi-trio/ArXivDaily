{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-09-12T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Retrieving Evidence from EHRs with LLMs: Possibilities and Challenges. (arXiv:2309.04550v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04550","description":"<p>Unstructured Electronic Health Record (EHR) data often contains critical\ninformation complementary to imaging data that would inform radiologists'\ndiagnoses. However, time constraints and the large volume of notes frequently\nassociated with individual patients renders manual perusal of such data to\nidentify relevant evidence infeasible in practice. Modern Large Language Models\n(LLMs) provide a flexible means of interacting with unstructured EHR data, and\nmay provide a mechanism to efficiently retrieve and summarize unstructured\nevidence relevant to a given query. In this work, we propose and evaluate an\nLLM (Flan-T5 XXL) for this purpose. Specifically, in a zero-shot setting we\ntask the LLM to infer whether a patient has or is at risk of a particular\ncondition; if so, we prompt the model to summarize the supporting evidence.\nEnlisting radiologists for manual evaluation, we find that this LLM-based\napproach provides outputs consistently preferred to a standard information\nretrieval baseline, but we also highlight the key outstanding challenge: LLMs\nare prone to hallucinating evidence. However, we provide results indicating\nthat model confidence in outputs might indicate when LLMs are hallucinating,\npotentially providing a means to address this.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahsan_H/0/1/0/all/0/1\">Hiba Ahsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McInerney_D/0/1/0/all/0/1\">Denis Jered McInerney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jisoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potter_C/0/1/0/all/0/1\">Christopher Potter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_G/0/1/0/all/0/1\">Geoffrey Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amir_S/0/1/0/all/0/1\">Silvio Amir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Three Ways to Improve Verbo-visual Fusion for Dense 3D Visual Grounding. (arXiv:2309.04561v1 [cs.CV])","link":"http://arxiv.org/abs/2309.04561","description":"<p>3D visual grounding is the task of localizing the object in a 3D scene which\nis referred by a description in natural language. With a wide range of\napplications ranging from autonomous indoor robotics to AR/VR, the task has\nrecently risen in popularity. A common formulation to tackle 3D visual\ngrounding is grounding-by-detection, where localization is done via bounding\nboxes. However, for real-life applications that require physical interactions,\na bounding box insufficiently describes the geometry of an object. We therefore\ntackle the problem of dense 3D visual grounding, i.e. referral-based 3D\ninstance segmentation. We propose a dense 3D grounding network ConcreteNet,\nfeaturing three novel stand-alone modules which aim to improve grounding\nperformance for challenging repetitive instances, i.e. instances with\ndistractors of the same semantic class. First, we introduce a bottom-up\nattentive fusion module that aims to disambiguate inter-instance relational\ncues, next we construct a contrastive training scheme to induce separation in\nthe latent space, and finally we resolve view-dependent utterances via a\nlearned global camera token. ConcreteNet ranks 1st on the challenging ScanRefer\nonline benchmark by a considerable +9.43% accuracy at 50% IoU and has won the\nICCV 3rd Workshop on Language for 3D Scenes \"3D Object Localization\" challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Unal_O/0/1/0/all/0/1\">Ozan Unal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaridis_C/0/1/0/all/0/1\">Christos Sakaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Suman Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale. (arXiv:2309.04564v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04564","description":"<p>Large volumes of text data have contributed significantly to the development\nof large language models (LLMs) in recent years. This data is typically\nacquired by scraping the internet, leading to pretraining datasets comprised of\nnoisy web text. To date, efforts to prune these datasets down to a higher\nquality subset have relied on hand-crafted heuristics encoded as rule-based\nfilters. In this work, we take a wider view and explore scalable estimates of\ndata quality that can be used to systematically measure the quality of\npretraining data. We perform a rigorous comparison at scale of the simple data\nquality estimator of perplexity, as well as more sophisticated and\ncomputationally intensive estimates of the Error L2-Norm and memorization.\nThese metrics are used to rank and prune pretraining corpora, and we\nsubsequently compare LLMs trained on these pruned datasets. Surprisingly, we\nfind that the simple technique of perplexity outperforms our more\ncomputationally expensive scoring methods. We improve over our no-pruning\nbaseline while training on as little as 30% of the original training dataset.\nOur work sets the foundation for unexplored strategies in automatically\ncurating high quality corpora and suggests the majority of pretraining data can\nbe removed while retaining performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marion_M/0/1/0/all/0/1\">Max Marion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ustun_A/0/1/0/all/0/1\">Ahmet &#xdc;st&#xfc;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pozzobon_L/0/1/0/all/0/1\">Luiza Pozzobon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alex Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fadaee_M/0/1/0/all/0/1\">Marzieh Fadaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linking Symptom Inventories using Semantic Textual Similarity. (arXiv:2309.04607v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04607","description":"<p>An extensive library of symptom inventories has been developed over time to\nmeasure clinical symptoms, but this variety has led to several long standing\nissues. Most notably, results drawn from different settings and studies are not\ncomparable, which limits reproducibility. Here, we present an artificial\nintelligence (AI) approach using semantic textual similarity (STS) to link\nsymptoms and scores across previously incongruous symptom inventories. We\ntested the ability of four pre-trained STS models to screen thousands of\nsymptom description pairs for related content - a challenging task typically\nrequiring expert panels. Models were tasked to predict symptom severity across\nfour different inventories for 6,607 participants drawn from 16 international\ndata sources. The STS approach achieved 74.8% accuracy across five tasks,\noutperforming other models tested. This work suggests that incorporating\ncontextual, semantic information can assist expert decision-making processes,\nyielding gains for both general and disease-specific clinical assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_E/0/1/0/all/0/1\">Eamonn Kennedy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vadlamani_S/0/1/0/all/0/1\">Shashank Vadlamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindsey_H/0/1/0/all/0/1\">Hannah M Lindsey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peterson_K/0/1/0/all/0/1\">Kelly S Peterson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_K/0/1/0/all/0/1\">Kristen Dams OConnor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_K/0/1/0/all/0/1\">Kenton Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1\">Ronak Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amiri_H/0/1/0/all/0/1\">Houshang H Amiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andersen_R/0/1/0/all/0/1\">Raeda K Andersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babikian_T/0/1/0/all/0/1\">Talin Babikian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baron_D/0/1/0/all/0/1\">David A Baron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigler_E/0/1/0/all/0/1\">Erin D Bigler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caeyenberghs_K/0/1/0/all/0/1\">Karen Caeyenberghs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delano_Wood_L/0/1/0/all/0/1\">Lisa Delano-Wood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Disner_S/0/1/0/all/0/1\">Seth G Disner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobryakova_E/0/1/0/all/0/1\">Ekaterina Dobryakova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eapen_B/0/1/0/all/0/1\">Blessen C Eapen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edelstein_R/0/1/0/all/0/1\">Rachel M Edelstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esopenko_C/0/1/0/all/0/1\">Carrie Esopenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genova_H/0/1/0/all/0/1\">Helen M Genova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geuze_E/0/1/0/all/0/1\">Elbert Geuze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodrich_Hunsaker_N/0/1/0/all/0/1\">Naomi J Goodrich-Hunsaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grafman_J/0/1/0/all/0/1\">Jordan Grafman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haberg_A/0/1/0/all/0/1\">Asta K Haberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodges_C/0/1/0/all/0/1\">Cooper B Hodges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoskinson_K/0/1/0/all/0/1\">Kristen R Hoskinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovenden_E/0/1/0/all/0/1\">Elizabeth S Hovenden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irimia_A/0/1/0/all/0/1\">Andrei Irimia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahanshad_N/0/1/0/all/0/1\">Neda Jahanshad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_R/0/1/0/all/0/1\">Ruchira M Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keleher_F/0/1/0/all/0/1\">Finian Keleher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kenney_K/0/1/0/all/0/1\">Kimbra Kenney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koerte_I/0/1/0/all/0/1\">Inga K Koerte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liebel_S/0/1/0/all/0/1\">Spencer W Liebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livny_A/0/1/0/all/0/1\">Abigail Livny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovstad_M/0/1/0/all/0/1\">Marianne Lovstad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martindale_S/0/1/0/all/0/1\">Sarah L Martindale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Max_J/0/1/0/all/0/1\">Jeffrey E Max</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayer_A/0/1/0/all/0/1\">Andrew R Mayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meier_T/0/1/0/all/0/1\">Timothy B Meier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menefee_D/0/1/0/all/0/1\">Deleene S Menefee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdalla Z Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondello_S/0/1/0/all/0/1\">Stefania Mondello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monti_M/0/1/0/all/0/1\">Martin M Monti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morey_R/0/1/0/all/0/1\">Rajendra A Morey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newcombe_V/0/1/0/all/0/1\">Virginia Newcombe</a>, et al. (36 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can NLP Models 'Identify', 'Distinguish', and 'Justify' Questions that Don't have a Definitive Answer?. (arXiv:2309.04635v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04635","description":"<p>Though state-of-the-art (SOTA) NLP systems have achieved remarkable\nperformance on a variety of language understanding tasks, they primarily focus\non questions that have a correct and a definitive answer. However, in\nreal-world applications, users often ask questions that don't have a definitive\nanswer. Incorrectly answering such questions certainly hampers a system's\nreliability and trustworthiness. Can SOTA models accurately identify such\nquestions and provide a reasonable response?\n</p>\n<p>To investigate the above question, we introduce QnotA, a dataset consisting\nof five different categories of questions that don't have definitive answers.\nFurthermore, for each QnotA instance, we also provide a corresponding QA\ninstance i.e. an alternate question that ''can be'' answered. With this data,\nwe formulate three evaluation tasks that test a system's ability to 'identify',\n'distinguish', and 'justify' QnotA questions. Through comprehensive\nexperiments, we show that even SOTA models including GPT-3 and Flan T5 do not\nfare well on these tasks and lack considerably behind the human performance\nbaseline. We conduct a thorough analysis which further leads to several\ninteresting findings. Overall, we believe our work and findings will encourage\nand facilitate further research in this important area and help develop more\nrobust models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Ayushi Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_N/0/1/0/all/0/1\">Nisarg Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Mihir Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallina_P/0/1/0/all/0/1\">Pavan Mallina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Aryan Bhavin Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangaraju_S/0/1/0/all/0/1\">Srihari Raju Sangaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_T/0/1/0/all/0/1\">Tirth Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_N/0/1/0/all/0/1\">Nihar Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Finetuning Large Language Models For Vietnamese Chatbot. (arXiv:2309.04646v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04646","description":"<p>Large language models (LLMs), such as GPT-4, PaLM, and LLaMa, have been shown\nto achieve remarkable performance across a variety of natural language tasks.\nRecent advancements in instruction tuning bring LLMs with ability in following\nuser's instructions and producing human-like responses. However, the high costs\nassociated with training and implementing LLMs pose challenges to academic\nresearch. Furthermore, the availability of pretrained LLMs and instruction-tune\ndatasets for Vietnamese language is limited. To tackle these concerns, we\nleverage large-scale instruction-following datasets from open-source projects,\nnamely Alpaca, GPT4All, and Chat-Doctor, which cover general domain and\nspecific medical domain. To the best of our knowledge, these are the first\ninstructional dataset for Vietnamese. Subsequently, we utilize\nparameter-efficient tuning through Low-Rank Adaptation (LoRA) on two open LLMs:\nBloomz (Multilingual) and GPTJ-6B (Vietnamese), resulting four models:\nBloomz-Chat, Bloomz-Doctor, GPTJ-Chat, GPTJ-Doctor.Finally, we assess the\neffectiveness of our methodology on a per-sample basis, taking into\nconsideration the helpfulness, relevance, accuracy, level of detail in their\nresponses. This evaluation process entails the utilization of GPT-4 as an\nautomated scoring mechanism. Despite utilizing a low-cost setup, our method\ndemonstrates about 20-30\\% improvement over the original models in our\nevaluation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doan_V/0/1/0/all/0/1\">Vu-Thuan Doan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_Q/0/1/0/all/0/1\">Quoc-Truong Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc-Vu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Vinh-Tiep Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_T/0/1/0/all/0/1\">Thuy-Ngan Nguyen Luu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf. (arXiv:2309.04658v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04658","description":"<p>Communication games, which we refer to as incomplete information games that\nheavily depend on natural language communication, hold significant research\nvalue in fields such as economics, social science, and artificial intelligence.\nIn this work, we explore the problem of how to engage large language models\n(LLMs) in communication games, and in response, propose a tuning-free\nframework. Our approach keeps LLMs frozen, and relies on the retrieval and\nreflection on past communications and experiences for improvement. An empirical\nstudy on the representative and widely-studied communication game,\n``Werewolf'', demonstrates that our framework can effectively play Werewolf\ngame without tuning the parameters of the LLMs. More importantly, strategic\nbehaviors begin to emerge in our experiments, suggesting that it will be a\nfruitful journey to engage LLMs in communication games and associated domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuzhuang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Fuwen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weidong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MADLAD-400: A Multilingual And Document-Level Large Audited Dataset. (arXiv:2309.04662v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04662","description":"<p>We introduce MADLAD-400, a manually audited, general domain 3T token\nmonolingual dataset based on CommonCrawl, spanning 419 languages. We discuss\nthe limitations revealed by self-auditing MADLAD-400, and the role data\nauditing had in the dataset creation process. We then train and release a\n10.7B-parameter multilingual machine translation model on 250 billion tokens\ncovering over 450 languages using publicly available data, and find that it is\ncompetitive with models that are significantly larger, and report the results\non different domains. In addition, we train a 8B-parameter language model, and\nassess the results on few-shot translation. We make the baseline models\navailable to the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kudugunta_S/0/1/0/all/0/1\">Sneha Kudugunta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caswell_I/0/1/0/all/0/1\">Isaac Caswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Biao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choquette_Choo_C/0/1/0/all/0/1\">Christopher A. Choquette-Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Katherine Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_D/0/1/0/all/0/1\">Derrick Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1\">Aditya Kusupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stella_R/0/1/0/all/0/1\">Romi Stella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FIAT: Fusing learning paradigms with Instruction-Accelerated Tuning. (arXiv:2309.04663v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04663","description":"<p>Learning paradigms for large language models (LLMs) currently tend to fall\nwithin either in-context learning (ICL) or full fine-tuning. Each of these\ncomes with their own trade-offs based on available data, model size, compute\ncost, ease-of-use, and final quality with neither solution performing well\nacross-the-board. In this article, we first describe ICL and fine-tuning\nparadigms in a way that highlights their natural connections. Based on these\nconnections, we propose a new learning paradigm called FIAT that fuses the best\nof these paradigms together, enabling prompt-engineered instructions and\nchain-of-thought reasoning with the very largest models while also using\nsimilar methods to perform parameter updates on a modestly-sized LLM with\nparameter-efficient tuning. We evaluate FIAT's effectiveness on a variety of\nmultilingual tasks and observe that FIAT performs better than both ICL and\nfine-tuning at scales ranging from 100-10,000 training examples. We hope that\nFIAT provides a practical way of harnessing the full potential of LLMs without\nneeding to make a hard choice between learning paradigms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieting_J/0/1/0/all/0/1\">John Wieting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jonathan H. Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embedding structure matters: Comparing methods to adapt multilingual vocabularies to new languages. (arXiv:2309.04679v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04679","description":"<p>Pre-trained multilingual language models underpin a large portion of modern\nNLP tools outside of English. A strong baseline for specializing these models\nfor specific languages is Language-Adaptive Pre-Training (LAPT). However,\nretaining a large cross-lingual vocabulary and embedding matrix comes at\nconsiderable excess computational cost during adaptation. In this study, we\npropose several simple techniques to replace a cross-lingual vocabulary with a\ncompact, language-specific one. Namely, we address strategies for\nre-initializing the token embedding matrix after vocabulary specialization. We\nthen provide a systematic experimental comparison of our techniques, in\naddition to the recently-proposed Focus method. We demonstrate that: 1)\nEmbedding-replacement techniques in the monolingual transfer literature are\ninadequate for adapting multilingual models. 2) Replacing cross-lingual\nvocabularies with smaller specialized ones provides an efficient method to\nimprove performance in low-resource languages. 3) Simple embedding\nre-initialization techniques based on script-wise sub-distributions rival\ntechniques such as Focus, which rely on similarity scores obtained from an\nauxiliary model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Downey_C/0/1/0/all/0/1\">C.M. Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blevins_T/0/1/0/all/0/1\">Terra Blevins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldfine_N/0/1/0/all/0/1\">Nora Goldfine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinert_Threlkeld_S/0/1/0/all/0/1\">Shane Steinert-Threlkeld</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code-Style In-Context Learning for Knowledge-Based Question Answering. (arXiv:2309.04695v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04695","description":"<p>Current methods for Knowledge-Based Question Answering (KBQA) usually rely on\ncomplex training techniques and model frameworks, leading to many limitations\nin practical applications. Recently, the emergence of In-Context Learning (ICL)\ncapabilities in Large Language Models (LLMs) provides a simple and\ntraining-free semantic parsing paradigm for KBQA: Given a small number of\nquestions and their labeled logical forms as demo examples, LLMs can understand\nthe task intent and generate the logic form for a new question. However,\ncurrent powerful LLMs have little exposure to logic forms during pre-training,\nresulting in a high format error rate. To solve this problem, we propose a\ncode-style in-context learning method for KBQA, which converts the generation\nprocess of unfamiliar logical form into the more familiar code generation\nprocess for LLMs. Experimental results on three mainstream datasets show that\nour method dramatically mitigated the formatting error problem in generating\nlogic forms while realizing a new SOTA on WebQSP, GrailQA, and GraphQ under the\nfew-shot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_Z/0/1/0/all/0/1\">Zhijie Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Richong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xudong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of Disinformation and Fake News Detection Using Fine-Tuned Large Language Model. (arXiv:2309.04704v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04704","description":"<p>The paper considers the possibility of fine-tuning Llama 2 large language\nmodel (LLM) for the disinformation analysis and fake news detection. For\nfine-tuning, the PEFT/LoRA based approach was used. In the study, the model was\nfine-tuned for the following tasks: analysing a text on revealing\ndisinformation and propaganda narratives, fact checking, fake news detection,\nmanipulation analytics, extracting named entities with their sentiments. The\nobtained results show that the fine-tuned Llama 2 model can perform a deep\nanalysis of texts and reveal complex styles and narratives. Extracted\nsentiments for named entities can be considered as predictive features in\nsupervised machine learning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pavlyshenko_B/0/1/0/all/0/1\">Bohdan M. Pavlyshenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Reproducing Network Research Results Using Large Language Models. (arXiv:2309.04716v1 [cs.LG])","link":"http://arxiv.org/abs/2309.04716","description":"<p>Reproducing research results in the networking community is important for\nboth academia and industry. The current best practice typically resorts to\nthree approaches: (1) looking for publicly available prototypes; (2) contacting\nthe authors to get a private prototype; and (3) manually implementing a\nprototype following the description of the publication. However, most published\nnetwork research does not have public prototypes and private prototypes are\nhard to get. As such, most reproducing efforts are spent on manual\nimplementation based on the publications, which is both time and labor\nconsuming and error-prone. In this paper, we boldly propose reproducing network\nresearch results using the emerging large language models (LLMs). In\nparticular, we first prove its feasibility with a small-scale experiment, in\nwhich four students with essential networking knowledge each reproduces a\ndifferent networking system published in prominent conferences and journals by\nprompt engineering ChatGPT. We report the experiment's observations and lessons\nand discuss future open research questions of this proposal. This work raises\nno ethical issue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Q/0/1/0/all/0/1\">Qiao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuling Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Mingjun Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Bang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_R/0/1/0/all/0/1\">Ridi Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_F/0/1/0/all/0/1\">Franck Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Linghe Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_J/0/1/0/all/0/1\">Jiwu Shu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EPA: Easy Prompt Augmentation on Large Language Models via Multiple Sources and Multiple Targets. (arXiv:2309.04725v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04725","description":"<p>Large language models (LLMs) have shown promising performance on various NLP\ntasks via task prompting. And their performance can be further improved by\nappending task demonstrations to the head of the prompt. And usually, a better\nperformance can be achieved with more demonstrations. However, asking the users\nto write the demonstrations can be cumbersome. As a simple yet cost-effective\nworkaround, this paper proposes a novel method called EPA (\\textbf{E}asy\n\\textbf{P}rompt \\textbf{A}ugmentation)\\footnote{While this paper considers\naugmenting prompts via demonstrations, we name it EPA as the name EDA is\nalready taken by a well-known NLP method \\citep{wei-zou-2019-eda}.} that\neffectively minimizes user efforts in writing demonstrations while improving\nthe model performance at the same time. EPA achieves these goals by\nautomatically augmenting the demonstrations with multiple sources/targets,\nwhere each of them paraphrases each other. This is well motivated as augmenting\ndata via paraphrasing effectively improves neural language models. EPA thus\nemploys paraphrasing as an augmentation method for in-context learning.\nExtensive experiments indicate that EPA effectively improves both NLU and NLG\ntasks, covering from natural language inference to machine translation in\ntranslating tens of languages.\\footnote{Code and data will be released upon\npublication.}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Better Multi-modal Keyphrase Generation via Visual Entity Enhancement and Multi-granularity Image Noise Filtering. (arXiv:2309.04734v1 [cs.CV])","link":"http://arxiv.org/abs/2309.04734","description":"<p>Multi-modal keyphrase generation aims to produce a set of keyphrases that\nrepresent the core points of the input text-image pair. In this regard,\ndominant methods mainly focus on multi-modal fusion for keyphrase generation.\nNevertheless, there are still two main drawbacks: 1) only a limited number of\nsources, such as image captions, can be utilized to provide auxiliary\ninformation. However, they may not be sufficient for the subsequent keyphrase\ngeneration. 2) the input text and image are often not perfectly matched, and\nthus the image may introduce noise into the model. To address these\nlimitations, in this paper, we propose a novel multi-modal keyphrase generation\nmodel, which not only enriches the model input with external knowledge, but\nalso effectively filters image noise. First, we introduce external visual\nentities of the image as the supplementary input to the model, which benefits\nthe cross-modal semantic alignment for keyphrase generation. Second, we\nsimultaneously calculate an image-text matching score and image region-text\ncorrelation scores to perform multi-granularity image noise filtering.\nParticularly, we introduce the correlation scores between image regions and\nground-truth keyphrases to refine the calculation of the previously-mentioned\ncorrelation scores. To demonstrate the effectiveness of our model, we conduct\nseveral groups of experiments on the benchmark dataset.\n</p>\n<p>Experimental results and in-depth analyses show that our model achieves the\nstate-of-the-art performance. Our code is available on\nhttps://github.com/DeepLearnXMU/MM-MKP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yifan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Suhang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jianxin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation for Conversational AI. (arXiv:2309.04739v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04739","description":"<p>Advancements in conversational systems have revolutionized information\naccess, surpassing the limitations of single queries. However, developing\ndialogue systems requires a large amount of training data, which is a challenge\nin low-resource domains and languages. Traditional data collection methods like\ncrowd-sourcing are labor-intensive and time-consuming, making them ineffective\nin this context. Data augmentation (DA) is an affective approach to alleviate\nthe data scarcity problem in conversational systems. This tutorial provides a\ncomprehensive and up-to-date overview of DA approaches in the context of\nconversational systems. It highlights recent advances in conversation\naugmentation, open domain and task-oriented conversation generation, and\ndifferent paradigms of evaluating these models. We also discuss current\nchallenges and future directions in order to help researchers and practitioners\nto further advance the field in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soudani_H/0/1/0/all/0/1\">Heydar Soudani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanoulas_E/0/1/0/all/0/1\">Evangelos Kanoulas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasibi_F/0/1/0/all/0/1\">Faegheh Hasibi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning. (arXiv:2309.04766v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04766","description":"<p>We present SeaEval, a benchmark for multilingual foundation models. In\naddition to characterizing how these models understand and reason with natural\nlanguage, we also investigate how well they comprehend cultural practices,\nnuances, and values. Alongside standard accuracy metrics, we investigate the\nbrittleness of foundation models in the dimensions of semantics and\nmultilinguality. Our analyses span both open-sourced and closed models, leading\nto empirical results across classic NLP tasks, reasoning, and cultural\ncomprehension. Key findings indicate (1) Most models exhibit varied behavior\nwhen given paraphrased instructions. (2) Many models still suffer from exposure\nbias (e.g., positional bias, majority label bias). (3) For questions rooted in\nfactual, scientific, and commonsense knowledge, consistent responses are\nexpected across multilingual queries that are semantically equivalent. Yet,\nmost models surprisingly demonstrate inconsistent performance on these queries.\n(4) Multilingually-trained models have not attained \"balanced multilingual\"\ncapabilities. Our endeavors underscore the need for more generalizable semantic\nrepresentations and enhanced multilingual contextualization. SeaEval can serve\nas a launchpad for more thorough investigations and evaluations for\nmultilingual and multicultural scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_F/0/1/0/all/0/1\">Fangkai Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aw_A/0/1/0/all/0/1\">Ai Ti Aw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMHQA-ICL: Multimodal In-context Learning for Hybrid Question Answering over Text, Tables and Images. (arXiv:2309.04790v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04790","description":"<p>In the real world, knowledge often exists in a multimodal and heterogeneous\nform. Addressing the task of question answering with hybrid data types,\nincluding text, tables, and images, is a challenging task (MMHQA). Recently,\nwith the rise of large language models (LLM), in-context learning (ICL) has\nbecome the most popular way to solve QA problems. We propose MMHQA-ICL\nframework for addressing this problems, which includes stronger heterogeneous\ndata retriever and an image caption module. Most importantly, we propose a\nType-specific In-context Learning Strategy for MMHQA, enabling LLMs to leverage\ntheir powerful performance in this task. We are the first to use end-to-end LLM\nprompting method for this task. Experimental results demonstrate that our\nframework outperforms all baselines and methods trained on the full dataset,\nachieving state-of-the-art results under the few-shot setting on the\nMultimodalQA dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_F/0/1/0/all/0/1\">Fangyu Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Tongxu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jiahe Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shizhu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaNS: a Facet-based Narrative Similarity Metric. (arXiv:2309.04823v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04823","description":"<p>Similar Narrative Retrieval is a crucial task since narratives are essential\nfor explaining and understanding events, and multiple related narratives often\nhelp to create a holistic view of the event of interest. To accurately identify\nsemantically similar narratives, this paper proposes a novel narrative\nsimilarity metric called Facet-based Narrative Similarity (FaNS), based on the\nclassic 5W1H facets (Who, What, When, Where, Why, and How), which are extracted\nby leveraging the state-of-the-art Large Language Models (LLMs). Unlike\nexisting similarity metrics that only focus on overall lexical/semantic match,\nFaNS provides a more granular matching along six different facets independently\nand then combines them. To evaluate FaNS, we created a comprehensive dataset by\ncollecting narratives from AllSides, a third-party news portal. Experimental\nresults demonstrate that the FaNS metric exhibits a higher correlation (37\\%\nhigher) than traditional text similarity metrics that directly measure the\nlexical/semantic match between narratives, demonstrating its effectiveness in\ncomparing the finer details between a pair of narratives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akter_M/0/1/0/all/0/1\">Mousumi Akter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santu_S/0/1/0/all/0/1\">Shubhra Kanti Karmaker Santu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neurons in Large Language Models: Dead, N-gram, Positional. (arXiv:2309.04827v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04827","description":"<p>We analyze a family of large language models in such a lightweight manner\nthat can be done on a single GPU. Specifically, we focus on the OPT family of\nmodels ranging from 125m to 66b parameters and rely only on whether an FFN\nneuron is activated or not. First, we find that the early part of the network\nis sparse and represents many discrete features. Here, many neurons (more than\n70% in some layers of the 66b model) are \"dead\", i.e. they never activate on a\nlarge collection of diverse data. At the same time, many of the alive neurons\nare reserved for discrete features and act as token and n-gram detectors.\nInterestingly, their corresponding FFN updates not only promote next token\ncandidates as could be expected, but also explicitly focus on removing the\ninformation about triggering them tokens, i.e., current input. To the best of\nour knowledge, this is the first example of mechanisms specialized at removing\n(rather than adding) information from the residual stream. With scale, models\nbecome more sparse in a sense that they have more dead neurons and token\ndetectors. Finally, some neurons are positional: them being activated or not\ndepends largely (or solely) on position and less so (or not at all) on textual\ndata. We find that smaller models have sets of neurons acting as position range\nindicators while larger models operate in a less explicit manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Voita_E/0/1/0/all/0/1\">Elena Voita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrando_J/0/1/0/all/0/1\">Javier Ferrando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nalmpantis_C/0/1/0/all/0/1\">Christoforos Nalmpantis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Large Language Models for Exploiting ASR Uncertainty. (arXiv:2309.04842v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04842","description":"<p>While large language models excel in a variety of natural language processing\n(NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they\nmust either rely on off-the-shelf automatic speech recognition (ASR) systems\nfor transcription, or be equipped with an in-built speech modality. This work\nfocuses on the former scenario, where LLM's accuracy on SLU tasks is\nconstrained by the accuracy of a fixed ASR system on the spoken input.\nSpecifically, we tackle speech-intent classification task, where a high\nword-error-rate can limit the LLM's ability to understand the spoken intent.\nInstead of chasing a high accuracy by designing complex or specialized\narchitectures regardless of deployment costs, we seek to answer how far we can\ngo without substantially changing the underlying ASR and LLM, which can\npotentially be shared by multiple unrelated tasks. To this end, we propose\nprompting the LLM with an n-best list of ASR hypotheses instead of only the\nerror-prone 1-best hypothesis. We explore prompt-engineering to explain the\nconcept of n-best lists to the LLM; followed by the finetuning of Low-Rank\nAdapters on the downstream tasks. Our approach using n-best lists proves to be\neffective on a device-directed speech detection task as well as on a keyword\nspotting task, where systems using n-best list prompts outperform those using\n1-best ASR hypothesis; thus paving the way for an efficient method to exploit\nASR uncertainty via LLMs for speech-based applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dighe_P/0/1/0/all/0/1\">Pranay Dighe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shangshang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunshu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_V/0/1/0/all/0/1\">Vineet Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xiaochuan Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewfik_A/0/1/0/all/0/1\">Ahmed Tewfik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Emotion Recognition with Distilled Prosodic and Linguistic Affect Representations. (arXiv:2309.04849v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04849","description":"<p>We propose EmoDistill, a novel speech emotion recognition (SER) framework\nthat leverages cross-modal knowledge distillation during training to learn\nstrong linguistic and prosodic representations of emotion from speech. During\ninference, our method only uses a stream of speech signals to perform unimodal\nSER thus reducing computation overhead and avoiding run-time transcription and\nprosodic feature extraction errors. During training, our method distills\ninformation at both embedding and logit levels from a pair of pre-trained\nProsodic and Linguistic teachers that are fine-tuned for SER. Experiments on\nthe IEMOCAP benchmark demonstrate that our method outperforms other unimodal\nand multimodal techniques by a considerable margin, and achieves\nstate-of-the-art performance of 77.49% unweighted accuracy and 78.91% weighted\naccuracy. Detailed ablation studies demonstrate the impact of each component of\nour method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shome_D/0/1/0/all/0/1\">Debaditya Shome</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System. (arXiv:2309.04858v1 [cs.LG])","link":"http://arxiv.org/abs/2309.04858","description":"<p>Neural language models are increasingly deployed into APIs and websites that\nallow a user to pass in a prompt and receive generated text. Many of these\nsystems do not reveal generation parameters. In this paper, we present methods\nto reverse-engineer the decoding method used to generate text (i.e., top-$k$ or\nnucleus sampling). Our ability to discover which decoding strategy was used has\nimplications for detecting generated text. Additionally, the process of\ndiscovering the decoding strategy can reveal biases caused by selecting\ndecoding settings which severely truncate a model's predicted distributions. We\nperform our attack on several families of open-source language models, as well\nas on production systems (e.g., ChatGPT).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Katherine Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasr_M/0/1/0/all/0/1\">Milad Nasr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yun William Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distributional Data Augmentation Methods for Low Resource Language. (arXiv:2309.04862v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04862","description":"<p>Text augmentation is a technique for constructing synthetic data from an\nunder-resourced corpus to improve predictive performance. Synthetic data\ngeneration is common in numerous domains. However, recently text augmentation\nhas emerged in natural language processing (NLP) to improve downstream tasks.\nOne of the current state-of-the-art text augmentation techniques is easy data\naugmentation (EDA), which augments the training data by injecting and replacing\nsynonyms and randomly permuting sentences. One major obstacle with EDA is the\nneed for versatile and complete synonym dictionaries, which cannot be easily\nfound in low-resource languages. To improve the utility of EDA, we propose two\nextensions, easy distributional data augmentation (EDDA) and type specific\nsimilar word replacement (TSSR), which uses semantic word context information\nand part-of-speech tags for word replacement and augmentation. In an extensive\nempirical evaluation, we show the utility of the proposed methods, measured by\nF1 score, on two representative datasets in Swedish as an example of a\nlow-resource language. With the proposed methods, we show that augmented data\nimprove classification performances in low-resource settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahamud_M/0/1/0/all/0/1\">Mosleh Mahamud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Z/0/1/0/all/0/1\">Zed Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samsten_I/0/1/0/all/0/1\">Isak Samsten</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Chunking with Hierarchical RNN. (arXiv:2309.04919v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04919","description":"<p>In Natural Language Processing (NLP), predicting linguistic structures, such\nas parsing and chunking, has mostly relied on manual annotations of syntactic\nstructures. This paper introduces an unsupervised approach to chunking, a\nsyntactic task that involves grouping words in a non-hierarchical manner. We\npresent a two-layer Hierarchical Recurrent Neural Network (HRNN) designed to\nmodel word-to-chunk and chunk-to-sentence compositions. Our approach involves a\ntwo-stage training process: pretraining with an unsupervised parser and\nfinetuning on downstream NLP tasks. Experiments on the CoNLL-2000 dataset\nreveal a notable improvement over existing unsupervised methods, enhancing\nphrase F1 score by up to 6 percentage points. Further, finetuning with\ndownstream tasks results in an additional performance improvement.\nInterestingly, we observe that the emergence of the chunking structure is\ntransient during the neural model's downstream-task training. This study\ncontributes to the advancement of unsupervised syntactic structure discovery\nand opens avenues for further research in linguistic theory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zijun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshmukh_A/0/1/0/all/0/1\">Anup Anand Deshmukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongkang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lili Mou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What's Hard in English RST Parsing? Predictive Models for Error Analysis. (arXiv:2309.04940v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04940","description":"<p>Despite recent advances in Natural Language Processing (NLP), hierarchical\ndiscourse parsing in the framework of Rhetorical Structure Theory remains\nchallenging, and our understanding of the reasons for this are as yet limited.\nIn this paper, we examine and model some of the factors associated with parsing\ndifficulties in previous work: the existence of implicit discourse relations,\nchallenges in identifying long-distance relations, out-of-vocabulary items, and\nmore. In order to assess the relative importance of these variables, we also\nrelease two annotated English test-sets with explicit correct and distracting\ndiscourse markers associated with gold standard RST relations. Our results show\nthat as in shallow discourse parsing, the explicit/implicit distinction plays a\nrole, but that long-distance dependencies are the main challenge, while lack of\nlexical overlap is less of a problem, at least for in-domain parsing. Our final\nmodel is able to predict where errors will occur with an accuracy of 76.3% for\nthe bottom-up parser and 76.6% for the top-down parser.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Janet Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aoyama_T/0/1/0/all/0/1\">Tatsuya Aoyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeldes_A/0/1/0/all/0/1\">Amir Zeldes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-document Summarization: A Comparative Evaluation. (arXiv:2309.04951v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04951","description":"<p>This paper is aimed at evaluating state-of-the-art models for Multi-document\nSummarization (MDS) on different types of datasets in various domains and\ninvestigating the limitations of existing models to determine future research\ndirections. To address this gap, we conducted an extensive literature review to\nidentify state-of-the-art models and datasets. We analyzed the performance of\nPRIMERA and PEGASUS models on BigSurvey-MDS and MS$^2$ datasets, which posed\nunique challenges due to their varied domains. Our findings show that the\nGeneral-Purpose Pre-trained Model LED outperforms PRIMERA and PEGASUS on the\nMS$^2$ dataset. We used the ROUGE score as a performance metric to evaluate the\nidentified models on different datasets. Our study provides valuable insights\ninto the models' strengths and weaknesses, as well as their applicability in\ndifferent domains. This work serves as a reference for future MDS research and\ncontributes to the development of accurate and robust models which can be\nutilized on demanding datasets with academically and/or scientifically complex\ndata as well as generalized, relatively simple datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hewapathirana_K/0/1/0/all/0/1\">Kushan Hewapathirana</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nisansa de Silva</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Athuraliya_C/0/1/0/all/0/1\">C.D. Athuraliya</a> (2) ((1) Department of Computer Science &amp; Engineering, University of Moratuwa, Sri Lanka, (2) ConscientAI, Sri Lanka)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prefix-diffusion: A Lightweight Diffusion Model for Diverse Image Captioning. (arXiv:2309.04965v1 [cs.CV])","link":"http://arxiv.org/abs/2309.04965","description":"<p>While impressive performance has been achieved in image captioning, the\nlimited diversity of the generated captions and the large parameter scale\nremain major barriers to the real-word application of these systems. In this\nwork, we propose a lightweight image captioning network in combination with\ncontinuous diffusion, called Prefix-diffusion. To achieve diversity, we design\nan efficient method that injects prefix image embeddings into the denoising\nprocess of the diffusion model. In order to reduce trainable parameters, we\nemploy a pre-trained model to extract image features and further design an\nextra mapping network. Prefix-diffusion is able to generate diverse captions\nwith relatively less parameters, while maintaining the fluency and relevance of\nthe captions benefiting from the generative capabilities of the diffusion\nmodel. Our work paves the way for scaling up diffusion models for image\ncaptioning, and achieves promising performance compared with recent approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guisheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Z/0/1/0/all/0/1\">Zhengcong Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Haiyan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiangyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanqing Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt Learning With Knowledge Memorizing Prototypes For Generalized Few-Shot Intent Detection. (arXiv:2309.04971v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04971","description":"<p>Generalized Few-Shot Intent Detection (GFSID) is challenging and realistic\nbecause it needs to categorize both seen and novel intents simultaneously.\nPrevious GFSID methods rely on the episodic learning paradigm, which makes it\nhard to extend to a generalized setup as they do not explicitly learn the\nclassification of seen categories and the knowledge of seen intents. To address\nthe dilemma, we propose to convert the GFSID task into the class incremental\nlearning paradigm. Specifically, we propose a two-stage learning framework,\nwhich sequentially learns the knowledge of different intents in various periods\nvia prompt learning. And then we exploit prototypes for categorizing both seen\nand novel intents. Furthermore, to achieve the transfer knowledge of intents in\ndifferent stages, for different scenarios we design two knowledge preservation\nmethods which close to realistic applications. Extensive experiments and\ndetailed analyses on two widely used datasets show that our framework based on\nthe class incremental learning paradigm achieves promising performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luoyiching_C/0/1/0/all/0/1\">Chaiyut Luoyiching</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_N/0/1/0/all/0/1\">Nannan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hanjing Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RGAT: A Deeper Look into Syntactic Dependency Information for Coreference Resolution. (arXiv:2309.04977v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04977","description":"<p>Although syntactic information is beneficial for many NLP tasks, combining it\nwith contextual information between words to solve the coreference resolution\nproblem needs to be further explored. In this paper, we propose an end-to-end\nparser that combines pre-trained BERT with a Syntactic Relation Graph Attention\nNetwork (RGAT) to take a deeper look into the role of syntactic dependency\ninformation for the coreference resolution task. In particular, the RGAT model\nis first proposed, then used to understand the syntactic dependency graph and\nlearn better task-specific syntactic embeddings. An integrated architecture\nincorporating BERT embeddings and syntactic embeddings is constructed to\ngenerate blending representations for the downstream task. Our experiments on a\npublic Gendered Ambiguous Pronouns (GAP) dataset show that with the supervision\nlearning of the syntactic dependency graph and without fine-tuning the entire\nBERT, we increased the F1-score of the previous best model (RGCN-with-BERT)\nfrom 80.3% to 82.5%, compared to the F1-score by single BERT embeddings from\n78.5% to 82.5%. Experimental results on another public dataset - OntoNotes 5.0\ndemonstrate that the performance of the model is also improved by incorporating\nsyntactic dependency information learned from RGAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuan Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xuhao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval-Augmented Meta Learning for Low-Resource Text Classification. (arXiv:2309.04979v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04979","description":"<p>Meta learning have achieved promising performance in low-resource text\nclassification which aims to identify target classes with knowledge transferred\nfrom source classes with sets of small tasks named episodes. However, due to\nthe limited training data in the meta-learning scenario and the inherent\nproperties of parameterized neural networks, poor generalization performance\nhas become a pressing problem that needs to be addressed. To deal with this\nissue, we propose a meta-learning based method called Retrieval-Augmented Meta\nLearning(RAML). It not only uses parameterization for inference but also\nretrieves non-parametric knowledge from an external corpus to make inferences,\nwhich greatly alleviates the problem of poor generalization performance caused\nby the lack of diverse training data in meta-learning. This method differs from\nprevious models that solely rely on parameters, as it explicitly emphasizes the\nimportance of non-parametric knowledge, aiming to strike a balance between\nparameterized neural networks and non-parametric knowledge. The model is\nrequired to determine which knowledge to access and utilize during inference.\nAdditionally, our multi-view passages fusion network module can effectively and\nefficiently integrate the retrieved information into low-resource\nclassification task. The extensive experiments demonstrate that RAML\nsignificantly outperforms current SOTA low-resource text classification models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luoyiching_C/0/1/0/all/0/1\">Chaiyut Luoyiching</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_N/0/1/0/all/0/1\">Nannan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hanjing Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Word Bias in Zero-shot Prompt-based Classifiers. (arXiv:2309.04992v1 [cs.CL])","link":"http://arxiv.org/abs/2309.04992","description":"<p>Prompt-based classifiers are an attractive approach for zero-shot\nclassification. However, the precise choice of the prompt template and label\nwords can largely influence performance, with semantically equivalent settings\noften showing notable performance difference. This discrepancy can be partly\nattributed to word biases, where the classifier may be biased towards classes.\nTo address this problem, it is possible to optimise classification thresholds\non a labelled data set, however, this mitigates some of the advantages of\nprompt-based classifiers. This paper instead approaches this problem by\nexamining the expected marginal probabilities of the classes. Here,\nprobabilities are reweighted to have a uniform prior over classes, in an\nunsupervised fashion. Further, we draw a theoretical connection between the\nclass priors and the language models' word prior, and offer the ability to set\na threshold in a zero-resource fashion. We show that matching class priors\ncorrelates strongly with the oracle upper bound performance and demonstrate\nlarge consistent performance gains for prompt settings over a range of NLP\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liusie_A/0/1/0/all/0/1\">Adian Liusie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manakul_P/0/1/0/all/0/1\">Potsawee Manakul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark J. F. Gales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FOLLOWUPQG: Towards Information-Seeking Follow-up Question Generation. (arXiv:2309.05007v1 [cs.CL])","link":"http://arxiv.org/abs/2309.05007","description":"<p>Humans ask follow-up questions driven by curiosity, which reflects a creative\nhuman cognitive process. We introduce the task of real-world\ninformation-seeking follow-up question generation (FQG), which aims to generate\nfollow-up questions seeking a more in-depth understanding of an initial\nquestion and answer. We construct FOLLOWUPQG, a dataset of over 3K real-world\n(initial question, answer, follow-up question) tuples collected from a Reddit\nforum providing layman-friendly explanations for open-ended questions. In\ncontrast to existing datasets, questions in FOLLOWUPQG use more diverse\npragmatic strategies to seek information, and they also show higher-order\ncognitive skills (such as applying and relating). We evaluate current question\ngeneration models on their efficacy for generating follow-up questions,\nexploring how to generate specific types of follow-up questions based on\nstep-by-step demonstrations. Our results validate FOLLOWUPQG as a challenging\nbenchmark, as model-generated questions are adequate but far from human-raised\nquestions in terms of informativeness and complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yan Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liangming Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Min-Yen Kan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chat2Brain: A Method for Mapping Open-Ended Semantic Queries to Brain Activation Maps. (arXiv:2309.05021v1 [cs.CL])","link":"http://arxiv.org/abs/2309.05021","description":"<p>Over decades, neuroscience has accumulated a wealth of research results in\nthe text modality that can be used to explore cognitive processes.\nMeta-analysis is a typical method that successfully establishes a link from\ntext queries to brain activation maps using these research results, but it\nstill relies on an ideal query environment. In practical applications, text\nqueries used for meta-analyses may encounter issues such as semantic redundancy\nand ambiguity, resulting in an inaccurate mapping to brain images. On the other\nhand, large language models (LLMs) like ChatGPT have shown great potential in\ntasks such as context understanding and reasoning, displaying a high degree of\nconsistency with human natural language. Hence, LLMs could improve the\nconnection between text modality and neuroscience, resolving existing\nchallenges of meta-analyses. In this study, we propose a method called\nChat2Brain that combines LLMs to basic text-2-image model, known as Text2Brain,\nto map open-ended semantic queries to brain activation maps in data-scarce and\ncomplex query environments. By utilizing the understanding and reasoning\ncapabilities of LLMs, the performance of the mapping model is optimized by\ntransferring text queries to semantic queries. We demonstrate that Chat2Brain\ncan synthesize anatomically plausible neural activation patterns for more\ncomplex tasks of text queries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yaonai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_T/0/1/0/all/0/1\">Tianyang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_M/0/1/0/all/0/1\">Muheng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unifying Framework of Bilinear LSTMs. (arXiv:1910.10294v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1910.10294","description":"<p>This paper presents a novel unifying framework of bilinear LSTMs that can\nrepresent and utilize the nonlinear interaction of the input features present\nin sequence datasets for achieving superior performance over a linear LSTM and\nyet not incur more parameters to be learned. To realize this, our unifying\nframework allows the expressivity of the linear vs. bilinear terms to be\nbalanced by correspondingly trading off between the hidden state vector size\nvs. approximation quality of the weight matrix in the bilinear term so as to\noptimize the performance of our bilinear LSTM, while not incurring more\nparameters to be learned. We empirically evaluate the performance of our\nbilinear LSTM in several language-based sequence learning tasks to demonstrate\nits general applicability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajpal_M/0/1/0/all/0/1\">Mohit Rajpal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Low_B/0/1/0/all/0/1\">Bryan Kian Hsiang Low</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Are People Asking About COVID-19? A Question Classification Dataset. (arXiv:2005.12522v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.12522","description":"<p>We present COVID-Q, a set of 1,690 questions about COVID-19 from 13 sources,\nwhich we annotate into 15 question categories and 207 question clusters. The\nmost common questions in our dataset asked about transmission, prevention, and\nsocietal effects of COVID, and we found that many questions that appeared in\nmultiple sources were not answered by any FAQ websites of reputable\norganizations such as the CDC and FDA. We post our dataset publicly at\nhttps://github.com/JerryWeiAI/COVID-Q. For classifying questions into 15\ncategories, a BERT baseline scored 58.1% accuracy when trained on 20 examples\nper category, and for a question clustering task, a BERT + triplet loss\nbaseline achieved 49.5% accuracy. We hope COVID-Q can help either for direct\nuse in developing applied systems or as a domain-specific resource for model\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jerry Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chengyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NewB: 200,000+ Sentences for Political Bias Detection. (arXiv:2006.03051v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2006.03051","description":"<p>We present the Newspaper Bias Dataset (NewB), a text corpus of more than\n200,000 sentences from eleven news sources regarding Donald Trump. While\nprevious datasets have labeled sentences as either liberal or conservative,\nNewB covers the political views of eleven popular media sources, capturing more\nnuanced political viewpoints than a traditional binary classification system\ndoes. We train two state-of-the-art deep learning models to predict the news\nsource of a given sentence from eleven newspapers and find that a recurrent\nneural network achieved top-1, top-3, and top-5 accuracies of 33.3%, 61.4%, and\n77.6%, respectively, significantly outperforming a baseline logistic regression\nmodel's accuracies of 18.3%, 42.6%, and 60.8%. Using the news source label of\nsentences, we analyze the top n-grams with our model to gain meaningful insight\ninto the portrayal of Trump by media sources.We hope that the public release of\nour dataset will encourage further research in using natural language\nprocessing to analyze more complex political biases.\n</p>\n<p>Our dataset is posted at https://github.com/JerryWeiAI/NewB .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jerry Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Deep Neural Networks Predict Data Correlations from Column Names?. (arXiv:2107.04553v2 [cs.DB] UPDATED)","link":"http://arxiv.org/abs/2107.04553","description":"<p>Recent publications suggest using natural language analysis on database\nschema elements to guide tuning and profiling efforts. The underlying\nhypothesis is that state-of-the-art language processing methods, so-called\nlanguage models, are able to extract information on data properties from schema\ntext.\n</p>\n<p>This paper examines that hypothesis in the context of data correlation\nanalysis: is it possible to find column pairs with correlated data by analyzing\ntheir names via language models? First, the paper introduces a novel benchmark\nfor data correlation analysis, created by analyzing thousands of Kaggle data\nsets (and available for download). Second, it uses that data to study the\nability of language models to predict correlation, based on column names. The\nanalysis covers different language models, various correlation metrics, and a\nmultitude of accuracy metrics. It pinpoints factors that contribute to\nsuccessful predictions, such as the length of column names as well as the ratio\nof words. Finally, \\rev{the study analyzes the impact of column types on\nprediction performance.} The results show that schema text can be a useful\nsource of information and inform future research efforts, targeted at\nNLP-enhanced database tuning and data profiling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trummer_I/0/1/0/all/0/1\">Immanuel Trummer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Knowledge Enhanced Pre-trained Models. (arXiv:2110.00269v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.00269","description":"<p>Pre-trained language models learn informative word representations on a\nlarge-scale text corpus through self-supervised learning, which has achieved\npromising performance in fields of natural language processing (NLP) after\nfine-tuning. These models, however, suffer from poor robustness and lack of\ninterpretability. We refer to pre-trained language models with knowledge\ninjection as knowledge-enhanced pre-trained language models (KEPLMs). These\nmodels demonstrate deep understanding and logical reasoning and introduce\ninterpretability. In this survey, we provide a comprehensive overview of KEPLMs\nin NLP. We first discuss the advancements in pre-trained language models and\nknowledge representation learning. Then we systematically categorize existing\nKEPLMs from three different perspectives. Finally, we outline some potential\ndirections of KEPLMs for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1\">Gang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yulong Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Word Learning in Children from the Performance of Computer Vision Systems. (arXiv:2207.09847v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.09847","description":"<p>For human children as well as machine learning systems, a key challenge in\nlearning a word is linking the word to the visual phenomena it describes. We\nexplore this aspect of word learning by using the performance of computer\nvision systems as a proxy for the difficulty of learning a word from visual\ncues. We show that the age at which children acquire different categories of\nwords is correlated with the performance of visual classification and\ncaptioning systems, over and above the expected effects of word frequency. The\nperformance of the computer vision systems is correlated with human judgments\nof the concreteness of words, which are in turn a predictor of children's word\nlearning, suggesting that these models are capturing the relationship between\nwords and visual phenomena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rane_S/0/1/0/all/0/1\">Sunayana Rane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nencheva_M/0/1/0/all/0/1\">Mira L. Nencheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zeyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lew_Williams_C/0/1/0/all/0/1\">Casey Lew-Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1\">Olga Russakovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What can we know about that which we cannot even imagine?. (arXiv:2208.03886v3 [physics.hist-ph] UPDATED)","link":"http://arxiv.org/abs/2208.03886","description":"<p>In this essay I will consider a sequence of questions. The first questions\nconcern the biological function of intelligence in general, and cognitive\nprostheses of human intelligence in particular. These will lead into questions\nconcerning human language, perhaps the most important cognitive prosthesis\nhumanity has ever developed. While it is traditional to rhapsodize about the\ncognitive power encapsulated in human language, I will emphasize how horribly\nlimited human language is -- and therefore how limited our cognitive abilities\nare, despite their being augmented with language. This will lead to questions\nof whether human mathematics, being ultimately formulated in terms of human\nlanguage, is also deeply limited. I will then combine these questions to pose a\npartial, sort-of, sideways answer to the guiding concern of this essay: what we\ncan ever discern about that we cannot even conceive?\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Wolpert_D/0/1/0/all/0/1\">David H. Wolpert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy. (arXiv:2210.17546v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.17546","description":"<p>Studying data memorization in neural language models helps us understand the\nrisks (e.g., to privacy or copyright) associated with models regurgitating\ntraining data and aids in the development of countermeasures. Many prior works\n-- and some recently deployed defenses -- focus on \"verbatim memorization\",\ndefined as a model generation that exactly matches a substring from the\ntraining set. We argue that verbatim memorization definitions are too\nrestrictive and fail to capture more subtle forms of memorization.\nSpecifically, we design and implement an efficient defense that perfectly\nprevents all verbatim memorization. And yet, we demonstrate that this \"perfect\"\nfilter does not prevent the leakage of training data. Indeed, it is easily\ncircumvented by plausible and minimally modified \"style-transfer\" prompts --\nand in some cases even the non-modified original prompts -- to extract\nmemorized information. We conclude by discussing potential alternative\ndefinitions and why defining memorization is a difficult yet crucial open\nquestion for neural language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1\">Florian Tram&#xe8;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasr_M/0/1/0/all/0/1\">Milad Nasr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagielski_M/0/1/0/all/0/1\">Matthew Jagielski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Katherine Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choquette_Choo_C/0/1/0/all/0/1\">Christopher A. Choquette-Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discover, Explanation, Improvement: An Automatic Slice Detection Framework for Natural Language Processing. (arXiv:2211.04476v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.04476","description":"<p>Pretrained natural language processing (NLP) models have achieved high\noverall performance, but they still make systematic errors. Instead of manual\nerror analysis, research on slice detection models (SDM), which automatically\nidentify underperforming groups of datapoints, has caught escalated attention\nin Computer Vision for both understanding model behaviors and providing\ninsights for future model training and designing. However, little research on\nSDM and quantitative evaluation of their effectiveness have been conducted on\nNLP tasks. Our paper fills the gap by proposing a benchmark named \"Discover,\nExplain, Improve (DEIM)\" for classification NLP tasks along with a new SDM\nEdisa. Edisa discovers coherent and underperforming groups of datapoints; DEIM\nthen unites them under human-understandable concepts and provides comprehensive\nevaluation tasks and corresponding quantitative metrics. The evaluation in DEIM\nshows that Edisa can accurately select error-prone datapoints with informative\nsemantic features that summarize error patterns. Detecting difficult datapoints\ndirectly boosts model performance without tuning any original model parameters,\nshowing that discovered slices are actionable for users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wenyue Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lifeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linfeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haitao Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Emotion Recognition in Textual Conversations: A Survey. (arXiv:2211.09172v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.09172","description":"<p>While Emotion Recognition in Conversations (ERC) has seen a tremendous\nadvancement in the last few years, new applications and implementation\nscenarios present novel challenges and opportunities. These range from\nleveraging the conversational context, speaker and emotion dynamics modelling,\nto interpreting common sense expressions, informal language and sarcasm,\naddressing challenges of real time ERC, recognizing emotion causes, different\ntaxonomies across datasets, multilingual ERC to interpretability. This survey\nstarts by introducing ERC, elaborating on the challenges and opportunities\npertaining to this task. It proceeds with a description of the emotion\ntaxonomies and a variety of ERC benchmark datasets employing such taxonomies.\nThis is followed by descriptions of the most prominent works in ERC with\nexplanations of the Deep Learning architectures employed. Then, it provides\nadvisable ERC practices towards better frameworks, elaborating on methods to\ndeal with subjectivity in annotations and modelling and methods to deal with\nthe typically unbalanced ERC datasets. Finally, it presents systematic review\ntables comparing several works regarding the methods used and their\nperformance. The survey highlights the advantage of leveraging techniques to\naddress unbalanced data, the exploration of mixed emotions and the benefits of\nincorporating annotation subjectivity in the learning phase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pereira_P/0/1/0/all/0/1\">Patr&#xed;cia Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moniz_H/0/1/0/all/0/1\">Helena Moniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_J/0/1/0/all/0/1\">Joao Paulo Carvalho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Human-Language Model Interaction. (arXiv:2212.09746v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09746","description":"<p>Many real-world applications of language models (LMs), such as writing\nassistance and code autocomplete, involve human-LM interaction. However, most\nbenchmarks are non-interactive in that a model produces output without human\ninvolvement. To evaluate human-LM interaction, we develop a new framework,\nHuman-AI Language-based Interaction Evaluation (HALIE), that defines the\ncomponents of interactive systems and dimensions to consider when designing\nevaluation metrics. Compared to standard, non-interactive evaluation, HALIE\ncaptures (i) the interactive process, not only the final output; (ii) the\nfirst-person subjective experience, not just a third-party assessment; and\n(iii) notions of preference beyond quality (e.g., enjoyment and ownership). We\nthen design five tasks to cover different forms of interaction: social\ndialogue, question answering, crossword puzzles, summarization, and metaphor\ngeneration. With four state-of-the-art LMs (three variants of OpenAI's GPT-3\nand AI21 Labs' Jurassic-1), we find that better non-interactive performance\ndoes not always translate to better human-LM interaction. In particular, we\nhighlight three cases where the results from non-interactive and interactive\nmetrics diverge and underscore the importance of human-LM interaction for LM\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Mina Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_M/0/1/0/all/0/1\">Megha Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardy_A/0/1/0/all/0/1\">Amelia Hardy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thickstun_J/0/1/0/all/0/1\">John Thickstun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paranjape_A/0/1/0/all/0/1\">Ashwin Paranjape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerard_Ursin_I/0/1/0/all/0/1\">Ines Gerard-Ursin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Lisa Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ladhak_F/0/1/0/all/0/1\">Faisal Ladhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_F/0/1/0/all/0/1\">Frieda Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rose E. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_M/0/1/0/all/0/1\">Minae Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Joon Sung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Hancheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tony Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bommasani_R/0/1/0/all/0/1\">Rishi Bommasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernstein_M/0/1/0/all/0/1\">Michael Bernstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language as a Latent Sequence: deep latent variable models for semi-supervised paraphrase generation. (arXiv:2301.02275v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.02275","description":"<p>This paper explores deep latent variable models for semi-supervised\nparaphrase generation, where the missing target pair for unlabelled data is\nmodelled as a latent paraphrase sequence. We present a novel unsupervised model\nnamed variational sequence auto-encoding reconstruction (VSAR), which performs\nlatent sequence inference given an observed text. To leverage information from\ntext pairs, we additionally introduce a novel supervised model we call dual\ndirectional learning (DDL), which is designed to integrate with our proposed\nVSAR model. Combining VSAR with DDL (DDL+VSAR) enables us to conduct\nsemi-supervised learning. Still, the combined model suffers from a cold-start\nproblem. To further combat this issue, we propose an improved weight\ninitialisation solution, leading to a novel two-stage training scheme we call\nknowledge-reinforced-learning (KRL). Our empirical evaluations suggest that the\ncombined model yields competitive performance against the state-of-the-art\nsupervised baselines on complete data. Furthermore, in scenarios where only a\nfraction of the labelled pairs are available, our combined model consistently\noutperforms the strong supervised model baseline (DDL) by a significant margin\n(p &lt;.05; Wilcoxon test). Our code is publicly available at\n\"https://github.com/jialin-yu/latent-sequence-paraphrase\".\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jialin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristea_A/0/1/0/all/0/1\">Alexandra I. Cristea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harit_A/0/1/0/all/0/1\">Anoushka Harit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhongtian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aduragba_O/0/1/0/all/0/1\">Olanrewaju Tahir Aduragba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1\">Lei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moubayed_N/0/1/0/all/0/1\">Noura Al Moubayed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Large Language Models. (arXiv:2303.18223v12 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.18223","description":"<p>Language is essentially a complex, intricate system of human expressions\ngoverned by grammatical rules. It poses a significant challenge to develop\ncapable AI algorithms for comprehending and grasping a language. As a major\napproach, language modeling has been widely studied for language understanding\nand generation in the past two decades, evolving from statistical language\nmodels to neural language models. Recently, pre-trained language models (PLMs)\nhave been proposed by pre-training Transformer models over large-scale corpora,\nshowing strong capabilities in solving various NLP tasks. Since researchers\nhave found that model scaling can lead to performance improvement, they further\nstudy the scaling effect by increasing the model size to an even larger size.\nInterestingly, when the parameter scale exceeds a certain level, these enlarged\nlanguage models not only achieve a significant performance improvement but also\nshow some special abilities that are not present in small-scale language\nmodels. To discriminate the difference in parameter scale, the research\ncommunity has coined the term large language models (LLM) for the PLMs of\nsignificant size. Recently, the research on LLMs has been largely advanced by\nboth academia and industry, and a remarkable progress is the launch of ChatGPT,\nwhich has attracted widespread attention from society. The technical evolution\nof LLMs has been making an important impact on the entire AI community, which\nwould revolutionize the way how we develop and use AI algorithms. In this\nsurvey, we review the recent advances of LLMs by introducing the background,\nkey findings, and mainstream techniques. In particular, we focus on four major\naspects of LLMs, namely pre-training, adaptation tuning, utilization, and\ncapacity evaluation. Besides, we also summarize the available resources for\ndeveloping LLMs and discuss the remaining issues for future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yupeng Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1\">Yingqian Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Beichen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zican Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yifan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yushuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhipeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jinhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1\">Ruiyang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xinyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zikang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes. (arXiv:2304.04321v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2304.04321","description":"<p>Understanding the continuous states of objects is essential for task learning\nand planning in the real world. However, most existing task learning benchmarks\nassume discrete (e.g., binary) object goal states, which poses challenges for\nthe learning of complex tasks and transferring learned policy from simulated\nenvironments to the real world. Furthermore, state discretization limits a\nrobot's ability to follow human instructions based on the grounding of actions\nand states. To tackle these challenges, we present ARNOLD, a benchmark that\nevaluates language-grounded task learning with continuous states in realistic\n3D scenes. ARNOLD is comprised of 8 language-conditioned tasks that involve\nunderstanding object states and learning policies for continuous goals. To\npromote language-instructed learning, we provide expert demonstrations with\ntemplate-generated language descriptions. We assess task performance by\nutilizing the latest language-conditioned policy learning models. Our results\nindicate that current models for language-conditioned manipulations continue to\nexperience significant challenges in novel goal-state generalizations, scene\ngeneralizations, and object generalizations. These findings highlight the need\nto develop new algorithms that address this gap and underscore the potential\nfor further research in this area. Project website:\nhttps://arnold-benchmark.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Ran Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiangyong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yizhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_H/0/1/0/all/0/1\">Haoran Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiaofeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qingyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_W/0/1/0/all/0/1\">Wensi Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Ziheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terzopoulos_D/0/1/0/all/0/1\">Demetri Terzopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_B/0/1/0/all/0/1\">Baoxiong Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models. (arXiv:2304.07619v4 [q-fin.ST] UPDATED)","link":"http://arxiv.org/abs/2304.07619","description":"<p>We examine the potential of ChatGPT and other large language models in\npredicting stock market returns using news headlines. We use ChatGPT to assess\nwhether each headline is good, bad, or neutral for firms' stock prices. We\ndocument a significantly positive correlation between ChatGPT scores and\nsubsequent daily stock returns. We find that ChatGPT outperforms traditional\nsentiment analysis methods. More basic models such as GPT-1, GPT-2, and BERT\ncannot accurately forecast returns, indicating return predictability is an\nemerging capacity of complex language models. Long-short strategies based on\nChatGPT-4 deliver the highest Sharpe ratio. Furthermore, we find predictability\nin both small and large stocks, suggesting market underreaction to company\nnews. Predictability is stronger among smaller stocks and stocks with bad news,\nconsistent with limits-to-arbitrage also playing an important role. Finally, we\npropose a new method to evaluate and understand the models' reasoning\ncapabilities. Overall, our results suggest that incorporating advanced language\nmodels into the investment decision-making process can yield more accurate\npredictions and enhance the performance of quantitative trading strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Lopez_Lira_A/0/1/0/all/0/1\">Alejandro Lopez-Lira</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Tang_Y/0/1/0/all/0/1\">Yuehua Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Edit: Fault-Aware Code Editor for Code Generation. (arXiv:2305.04087v5 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2305.04087","description":"<p>Large language models (LLMs) have demonstrated an impressive ability to\ngenerate codes on competitive programming tasks. However, with limited sample\nnumbers, LLMs still suffer from poor accuracy. Inspired by the process of human\nprogramming, we propose a generate-and-edit approach named Self-Edit that\nutilizes execution results of the generated code from LLMs to improve the code\nquality on the competitive programming task. We execute the generated code on\nthe example test case provided in the question and wrap execution results into\na supplementary comment. Utilizing this comment as guidance, our fault-aware\ncode editor is employed to correct errors in the generated code. We perform\nextensive evaluations across two competitive programming datasets with nine\ndifferent LLMs. Compared to directly generating from LLMs, our approach can\nimprove the average of pass@1 by 89\\% on APPS-dev, 31\\% on APPS-test, and 48\\%\non HumanEval over nine popular code generation LLMs with parameter sizes\nranging from 110M to 175B. Compared to other post-processing methods, our\nmethod demonstrates superior accuracy and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kechi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhi Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification. (arXiv:2305.04940v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2305.04940","description":"<p>The use of modern Natural Language Processing (NLP) techniques has shown to\nbe beneficial for software engineering tasks, such as vulnerability detection\nand type inference. However, training deep NLP models requires significant\ncomputational resources. This paper explores techniques that aim at achieving\nthe best usage of resources and available information in these models.\n</p>\n<p>We propose a generic approach, EarlyBIRD, to build composite representations\nof code from the early layers of a pre-trained transformer model. We\nempirically investigate the viability of this approach on the CodeBERT model by\ncomparing the performance of 12 strategies for creating composite\nrepresentations with the standard practice of only using the last encoder\nlayer.\n</p>\n<p>Our evaluation on four datasets shows that several early layer combinations\nyield better performance on defect detection, and some combinations improve\nmulti-class classification. More specifically, we obtain a +2 average\nimprovement of detection accuracy on Devign with only 3 out of 12 layers of\nCodeBERT and a 3.3x speed-up of fine-tuning. These findings show that early\nlayers can be used to obtain better results using the same resources, as well\nas to reduce resource usage during fine-tuning and inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grishina_A/0/1/0/all/0/1\">Anastasiia Grishina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hort_M/0/1/0/all/0/1\">Max Hort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moonen_L/0/1/0/all/0/1\">Leon Moonen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Document Understanding Dataset and Evaluation (DUDE). (arXiv:2305.08455v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.08455","description":"<p>We call on the Document AI (DocAI) community to reevaluate current\nmethodologies and embrace the challenge of creating more practically-oriented\nbenchmarks. Document Understanding Dataset and Evaluation (DUDE) seeks to\nremediate the halted research progress in understanding visually-rich documents\n(VRDs). We present a new dataset with novelties related to types of questions,\nanswers, and document layouts based on multi-industry, multi-domain, and\nmulti-page VRDs of various origins, and dates. Moreover, we are pushing the\nboundaries of current methods by creating multi-task and multi-domain\nevaluation setups that more accurately simulate real-world situations where\npowerful generalization and adaptation under low-resource settings are desired.\nDUDE aims to set a new standard as a more practical, long-standing benchmark\nfor the community, and we hope that it will lead to future extensions and\ncontributions that address real-world challenges. Finally, our work illustrates\nthe importance of finding more efficient ways to model language, images, and\nlayout in DocAI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Landeghem_J/0/1/0/all/0/1\">Jordy Van Landeghem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tito_R/0/1/0/all/0/1\">Rub&#xe9;n Tito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borchmann_L/0/1/0/all/0/1\">&#x141;ukasz Borchmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietruszka_M/0/1/0/all/0/1\">Micha&#x142; Pietruszka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joziak_P/0/1/0/all/0/1\">Pawe&#x142; J&#xf3;ziak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Powalski_R/0/1/0/all/0/1\">Rafa&#x142; Powalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurkiewicz_D/0/1/0/all/0/1\">Dawid Jurkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coustaty_M/0/1/0/all/0/1\">Micka&#xeb;l Coustaty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ackaert_B/0/1/0/all/0/1\">Bertrand Ackaert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valveny_E/0/1/0/all/0/1\">Ernest Valveny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blaschko_M/0/1/0/all/0/1\">Matthew Blaschko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_S/0/1/0/all/0/1\">Sien Moens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanislawek_T/0/1/0/all/0/1\">Tomasz Stanis&#x142;awek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Annotation Imputation to Individualize Predictions: Initial Studies on Distribution Dynamics and Model Predictions. (arXiv:2305.15070v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.15070","description":"<p>Annotating data via crowdsourcing is time-consuming and expensive. Due to\nthese costs, dataset creators often have each annotator label only a small\nsubset of the data. This leads to sparse datasets with examples that are marked\nby few annotators. The downside of this process is that if an annotator doesn't\nget to label a particular example, their perspective on it is missed. This is\nespecially concerning for subjective NLP datasets where there is no single\ncorrect label: people may have different valid opinions. Thus, we propose using\nimputation methods to generate the opinions of all annotators for all examples,\ncreating a dataset that does not leave out any annotator's view. We then train\nand prompt models, using data from the imputed dataset, to make predictions\nabout the distribution of responses and individual annotations.\n</p>\n<p>In our analysis of the results, we found that the choice of imputation method\nsignificantly impacts soft label changes and distribution. While the imputation\nintroduces noise in the prediction of the original dataset, it has shown\npotential in enhancing shots for prompts, particularly for low-response-rate\nannotators. We have made all of our code and data publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lowmanstone_L/0/1/0/all/0/1\">London Lowmanstone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_R/0/1/0/all/0/1\">Ruyuan Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owan_R/0/1/0/all/0/1\">Risako Owan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaehyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning. (arXiv:2305.17256v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17256","description":"<p>Large language models (LLMs) have recently shown great potential for\nin-context learning, where LLMs learn a new task simply by conditioning on a\nfew input-label pairs (prompts). Despite their potential, our understanding of\nthe factors influencing end-task performance and the robustness of in-context\nlearning remains limited. This paper aims to bridge this knowledge gap by\ninvestigating the reliance of LLMs on shortcuts or spurious correlations within\nprompts. Through comprehensive experiments on classification and extraction\ntasks, we reveal that LLMs are \"lazy learners\" that tend to exploit shortcuts\nin prompts for downstream tasks. Additionally, we uncover a surprising finding\nthat larger models are more likely to utilize shortcuts in prompts during\ninference. Our findings provide a new perspective on evaluating robustness in\nin-context learning and pose new challenges for detecting and mitigating the\nuse of shortcuts in prompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Ruixiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Dehan Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Longtao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks. (arXiv:2305.18365v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18365","description":"<p>Large Language Models (LLMs) with strong abilities in natural language\nprocessing tasks have emerged and have been applied in various kinds of areas\nsuch as science, finance and software engineering. However, the capability of\nLLMs to advance the field of chemistry remains unclear. In this paper, rather\nthan pursuing state-of-the-art performance, we aim to evaluate capabilities of\nLLMs in a wide range of tasks across the chemistry domain. We identify three\nkey chemistry-related capabilities including understanding, reasoning and\nexplaining to explore in LLMs and establish a benchmark containing eight\nchemistry tasks. Our analysis draws on widely recognized datasets facilitating\na broad exploration of the capacities of LLMs within the context of practical\nchemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are\nevaluated for each chemistry task in zero-shot and few-shot in-context learning\nsettings with carefully selected demonstration examples and specially crafted\nprompts. Our investigation found that GPT-4 outperformed other models and LLMs\nexhibit different competitive levels in eight chemistry tasks. In addition to\nthe key findings from the comprehensive benchmark analysis, our work provides\ninsights into the limitation of current LLMs and the impact of in-context\nlearning settings on LLMs' performance across various chemistry tasks. The code\nand datasets used in this study are available at\nhttps://github.com/ChemFoundationModels/ChemLLMBench.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Taicheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1\">Kehan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_B/0/1/0/all/0/1\">Bozhao Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhenwen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhichun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chawla_N/0/1/0/all/0/1\">Nitesh V. Chawla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiest_O/0/1/0/all/0/1\">Olaf Wiest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLMatic: Neural Architecture Search via Large Language Models and Quality Diversity Optimization. (arXiv:2306.01102v3 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2306.01102","description":"<p>Large Language Models (LLMs) have emerged as powerful tools capable of\naccomplishing a broad spectrum of tasks. Their abilities span numerous areas,\nand one area where they have made a significant impact is in the domain of code\ngeneration. In this context, we view LLMs as mutation and crossover tools.\nMeanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and\nrobust solutions. By merging the code-generating abilities of LLMs with the\ndiversity and robustness of QD solutions, we introduce LLMatic, a Neural\nArchitecture Search (NAS) algorithm. While LLMs struggle to conduct NAS\ndirectly through prompts, LLMatic uses a procedural approach, leveraging QD for\nprompts and network architecture to create diverse and highly performant\nnetworks. We test LLMatic on the CIFAR-10 image classification benchmark,\ndemonstrating that it can produce competitive networks with just $2,000$\nsearches, even without prior knowledge of the benchmark domain or exposure to\nany previous top-performing models for the benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nasir_M/0/1/0/all/0/1\">Muhammad U. Nasir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Earle_S/0/1/0/all/0/1\">Sam Earle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1\">Julian Togelius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Steven James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cleghorn_C/0/1/0/all/0/1\">Christopher Cleghorn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CamChoice: A Corpus of Multiple Choice Questions and Candidate Response Distributions. (arXiv:2306.13047v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.13047","description":"<p>Multiple choice exams are widely used to assess candidates across a diverse\nrange of domains and tasks. To moderate question quality, newly proposed\nquestions often pass through pre-test evaluation stages before being deployed\ninto real-world exams. Currently, this evaluation process is manually\nintensive, which can lead to time lags in the question development cycle.\nStreamlining this process via automation can significantly enhance efficiency,\nhowever, there's a current lack of datasets with adequate pre-test analysis\ninformation. In this paper we introduce CamChoice; a multiple-choice\ncomprehension dataset of questions at different target levels, with\ncorresponding candidate selection distributions. We introduce the task of\ncandidate distribution matching, propose several evaluation metrics for the\ntask, and demonstrate that automatic systems trained on RACE++ can be leveraged\nas baselines for our task. We further demonstrate that these automatic systems\ncan be used for practical pre-test evaluation tasks such as detecting\nunderperforming distractors, where our detection systems can automatically\nidentify poor distractors that few candidates select. We release the data\npublicly for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liusie_A/0/1/0/all/0/1\">Adian Liusie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vatsal Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullooly_A/0/1/0/all/0/1\">Andrew Mullooly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knill_K/0/1/0/all/0/1\">Kate Knill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark J. F. Gales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Trustworthy Explanation: On Causal Rationalization. (arXiv:2306.14115v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2306.14115","description":"<p>With recent advances in natural language processing, rationalization becomes\nan essential self-explaining diagram to disentangle the black box by selecting\na subset of input texts to account for the major variation in prediction. Yet,\nexisting association-based approaches on rationalization cannot identify true\nrationales when two or more snippets are highly inter-correlated and thus\nprovide a similar contribution to prediction accuracy, so-called spuriousness.\nTo address this limitation, we novelly leverage two causal desiderata,\nnon-spuriousness and efficiency, into rationalization from the causal inference\nperspective. We formally define a series of probabilities of causation based on\na newly proposed structural causal model of rationalization, with its\ntheoretical identification established as the main component of learning\nnecessary and sufficient rationales. The superior performance of the proposed\ncausal rationalization is demonstrated on real-world review and medical\ndatasets with extensive experiments compared to state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hengrui Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$\\alpha$-$\\beta$-Factorization and the Binary Case of Simon's Congruence. (arXiv:2306.14192v3 [math.CO] UPDATED)","link":"http://arxiv.org/abs/2306.14192","description":"<p>In 1991 H\\'ebrard introduced a factorization of words that turned out to be a\npowerful tool for the investigation of a word's scattered factors (also known\nas (scattered) subwords or subsequences). Based on this, first Karandikar and\nSchnoebelen introduced the notion of $k$-richness and later on Barker et al.\nthe notion of $k$-universality. In 2022 Fleischmann et al. presented a\ngeneralization of the arch factorization by intersecting the arch factorization\nof a word and its reverse. While the authors merely used this factorization for\nthe investigation of shortest absent scattered factors, in this work we\ninvestigate this new $\\alpha$-$\\beta$-factorization as such. We characterize\nthe famous Simon congruence of $k$-universal words in terms of $1$-universal\nwords. Moreover, we apply these results to binary words. In this special case,\nwe obtain a full characterization of the classes and calculate the index of the\ncongruence. Lastly, we start investigating the ternary case, present a full\nlist of possibilities for $\\alpha\\beta\\alpha$-factors, and characterize their\ncongruence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Fleischmann_P/0/1/0/all/0/1\">Pamela Fleischmann</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hofer_J/0/1/0/all/0/1\">Jonas H&#xf6;fer</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huch_A/0/1/0/all/0/1\">Annika Huch</a>, <a href=\"http://arxiv.org/find/math/1/au:+Nowotka_D/0/1/0/all/0/1\">Dirk Nowotka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions. (arXiv:2307.03941v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2307.03941","description":"<p>The Right to be Forgotten (RTBF) was first established as the result of the\nruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz\\'alez, and\nwas later included as the Right to Erasure under the General Data Protection\nRegulation (GDPR) of European Union to allow individuals the right to request\npersonal data be deleted by organizations. Specifically for search engines,\nindividuals can send requests to organizations to exclude their information\nfrom the query results. It was a significant emergent right as the result of\nthe evolution of technology. With the recent development of Large Language\nModels (LLMs) and their use in chatbots, LLM-enabled software systems have\nbecome popular. But they are not excluded from the RTBF. Compared with the\nindexing approach used by search engines, LLMs store, and process information\nin a completely different way. This poses new challenges for compliance with\nthe RTBF. In this paper, we explore these challenges and provide our insights\non how to implement technical solutions for the RTBF, including the use of\ndifferential privacy, machine unlearning, model editing, and prompt\nengineering. With the rapid advancement of AI and the increasing need of\nregulating this powerful technology, learning from the case of RTBF can provide\nvaluable lessons for technical practitioners, legal experts, organizations, and\nauthorities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dawen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finckenberg_Broman_P/0/1/0/all/0/1\">Pamela Finckenberg-Broman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1\">Thong Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shidong Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1\">Zhenchang Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staples_M/0/1/0/all/0/1\">Mark Staples</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiwei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deduplicating and Ranking Solution Programs for Suggesting Reference Solutions. (arXiv:2307.07940v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2307.07940","description":"<p>Referring to solution programs written by other users is helpful for learners\nin programming education. However, current online judge systems just list all\nsolution programs submitted by users for references, and the programs are\nsorted based on the submission date and time, execution time, or user rating,\nignoring to what extent the programs can be helpful to be referenced. In\naddition, users struggle to refer to a variety of solution approaches since\nthere are too many duplicated and near-duplicated programs. To motivate\nlearners to refer to various solutions to learn better solution approaches, in\nthis paper, we propose an approach to deduplicate and rank common solution\nprograms in each programming problem. Inspired by the nature that the\nmany-duplicated program adopts a more common approach and can be a general\nreference, we remove the near-duplicated solution programs and rank the unique\nprograms based on the duplicate count. The experiments on the solution programs\nsubmitted to a real-world online judge system demonstrate that the number of\nprograms is reduced by 60.20%, whereas the baseline only reduces by 29.59%\nafter the deduplication, meaning that users only need to refer to 39.80% of\nprograms on average. Furthermore, our analysis shows that top-10 ranked\nprograms cover 29.95% of programs on average, indicating that users can grasp\n29.95% of solution approaches by referring to only 10 programs. The proposed\napproach shows the potential of reducing the learners' burden of referring to\ntoo many solutions and motivating them to learn a variety of solution\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shirafuji_A/0/1/0/all/0/1\">Atsushi Shirafuji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanobe_Y/0/1/0/all/0/1\">Yutaka Watanobe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applying QNLP to sentiment analysis in finance. (arXiv:2307.11788v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.11788","description":"<p>As an application domain where the slightest qualitative improvements can\nyield immense value, finance is a promising candidate for early quantum\nadvantage. Focusing on the rapidly advancing field of Quantum Natural Language\nProcessing (QNLP), we explore the practical applicability of the two central\napproaches DisCoCat and Quantum-Enhanced Long Short-Term Memory (QLSTM) to the\nproblem of sentiment analysis in finance. Utilizing a novel ChatGPT-based data\ngeneration approach, we conduct a case study with more than 1000 realistic\nsentences and find that QLSTMs can be trained substantially faster than\nDisCoCat while also achieving close to classical results for their available\nsoftware implementations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stein_J/0/1/0/all/0/1\">Jonas Stein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christ_I/0/1/0/all/0/1\">Ivo Christ</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraus_N/0/1/0/all/0/1\">Nicolas Kraus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansky_M/0/1/0/all/0/1\">Maximilian Balthasar Mansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_R/0/1/0/all/0/1\">Robert M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linnhoff_Popien_C/0/1/0/all/0/1\">Claudia Linnhoff-Popien</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback. (arXiv:2307.15217v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2307.15217","description":"<p>Reinforcement learning from human feedback (RLHF) is a technique for training\nAI systems to align with human goals. RLHF has emerged as the central method\nused to finetune state-of-the-art large language models (LLMs). Despite this\npopularity, there has been relatively little public work systematizing its\nflaws. In this paper, we (1) survey open problems and fundamental limitations\nof RLHF and related methods; (2) overview techniques to understand, improve,\nand complement RLHF in practice; and (3) propose auditing and disclosure\nstandards to improve societal oversight of RLHF systems. Our work emphasizes\nthe limitations of RLHF and highlights the importance of a multi-faceted\napproach to the development of safer AI systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1\">Stephen Casper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davies_X/0/1/0/all/0/1\">Xander Davies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Claudia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilbert_T/0/1/0/all/0/1\">Thomas Krendl Gilbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheurer_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;my Scheurer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rando_J/0/1/0/all/0/1\">Javier Rando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freedman_R/0/1/0/all/0/1\">Rachel Freedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korbak_T/0/1/0/all/0/1\">Tomasz Korbak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindner_D/0/1/0/all/0/1\">David Lindner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freire_P/0/1/0/all/0/1\">Pedro Freire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tony Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marks_S/0/1/0/all/0/1\">Samuel Marks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segerie_C/0/1/0/all/0/1\">Charbel-Rapha&#xeb;l Segerie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carroll_M/0/1/0/all/0/1\">Micah Carroll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_A/0/1/0/all/0/1\">Andi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christoffersen_P/0/1/0/all/0/1\">Phillip Christoffersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damani_M/0/1/0/all/0/1\">Mehul Damani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slocum_S/0/1/0/all/0/1\">Stewart Slocum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_U/0/1/0/all/0/1\">Usman Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siththaranjan_A/0/1/0/all/0/1\">Anand Siththaranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeau_M/0/1/0/all/0/1\">Max Nadeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michaud_E/0/1/0/all/0/1\">Eric J. Michaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfau_J/0/1/0/all/0/1\">Jacob Pfau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krasheninnikov_D/0/1/0/all/0/1\">Dmitrii Krasheninnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langosco_L/0/1/0/all/0/1\">Lauro Langosco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1\">Peter Hase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biyik_E/0/1/0/all/0/1\">Erdem B&#x131;y&#x131;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1\">Anca Dragan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krueger_D/0/1/0/all/0/1\">David Krueger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1\">Dorsa Sadigh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1\">Dylan Hadfield-Menell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Probabilistic Programming to Complexity-based Programming. (arXiv:2307.15453v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2307.15453","description":"<p>The paper presents the main characteristics and a preliminary implementation\nof a novel computational framework named CompLog. Inspired by probabilistic\nprogramming systems like ProbLog, CompLog builds upon the inferential\nmechanisms proposed by Simplicity Theory, relying on the computation of two\nKolmogorov complexities (here implemented as min-path searches via ASP\nprograms) rather than probabilistic inference. The proposed system enables\nusers to compute ex-post and ex-ante measures of unexpectedness of a certain\nsituation, mapping respectively to posterior and prior subjective\nprobabilities. The computation is based on the specification of world and\nmental models by means of causal and descriptive relations between predicates\nweighted by complexity. The paper illustrates a few examples of application:\ngenerating relevant descriptions, and providing alternative approaches to\ndisjunction and to negation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sileno_G/0/1/0/all/0/1\">Giovanni Sileno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dessalles_J/0/1/0/all/0/1\">Jean-Louis Dessalles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenging the Machinery of Generative AI with Fact-Checking: Ontology-Driven Biological Graphs for Verifying Human Disease-Gene Links. (arXiv:2308.03929v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2308.03929","description":"<p>Methods: we adopted a biological networks approach that enables the\nsystematic interrogation of ChatGPT's linked entities. In particular, we\ndesigned an ontology-driven fact-checking algorithm that compares biological\ngraphs constructed from approximately 200,000 PubMed abstracts with\ncounterparts constructed from a dataset generated using the ChatGPT-3.5 Turbo\nmodel. The nodes refer to biological entities (genes and diseases) that occur\nin the text. The edges represent the co-occurrence relationships of two\nentities mentioned in the same document, weighted by the proximity distance\nbetween these two entities. This research assumes a ``closed-world\nassumption'', meaning that fact-checking is performed only using the literature\ndataset as our ground truth. Results: in ten samples of 250 randomly selected\nrecords from the ChatGPT dataset of 1000 ``simulated'' articles , the\nfact-checking link accuracy ranged from 70% to 86%, while the remainder of the\nlinks remained unverified. Given the closed world assumption, the fact-checking\nprecision is significant. When measuring and comparing the proximity distances\nof the edges of literature graphs against ChatGPT graphs we found that the\nChatGPT distances were significantly shorter (ranging from 90 to 153) character\ndistance. In contrast, the proximity distance of biological entities identified\nin the literature ranged from 236 to 765 character distance. This pattern held\ntrue for all the relationships among biological entities in the ten samples.\nConclusion: this study demonstrated a reasonably high percentage accuracy of\naggregate fact-checking of disease-gene relationships found in\nChatGPT-generated texts. The strikingly consistent pattern of short proximity\ndistances across all samples offers an illuminating feedback to the biological\nknowledge we possess in the literature today.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamed_A/0/1/0/all/0/1\">Ahmed Abdeen Hamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Byung Suk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crimi_A/0/1/0/all/0/1\">Alessandro Crimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misiak_M/0/1/0/all/0/1\">Magdalena M. Misiak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis. (arXiv:2308.11224v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2308.11224","description":"<p>Large Language Models (LLMs) have garnered considerable interest within both\nacademic and industrial. Yet, the application of LLMs to graph data remains\nunder-explored. In this study, we evaluate the capabilities of four LLMs in\naddressing several analytical problems with graph data. We employ four distinct\nevaluation metrics: Comprehension, Correctness, Fidelity, and Rectification.\nOur results show that: 1) LLMs effectively comprehend graph data in natural\nlanguage and reason with graph topology. 2) GPT models can generate logical and\ncoherent results, outperforming alternatives in correctness. 3) All examined\nLLMs face challenges in structural reasoning, with techniques like zero-shot\nchain-of-thought and few-shot prompting showing diminished efficacy. 4) GPT\nmodels often produce erroneous answers in multi-answer tasks, raising concerns\nin fidelity. 5) GPT models exhibit elevated confidence in their outputs,\npotentially hindering their rectification capacities. Notably, GPT-4 has\ndemonstrated the capacity to rectify responses from GPT-3.5-turbo and its own\nprevious iterations. The code is available at:\nhttps://github.com/Ayame1006/LLMtoGraph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HopPG: Self-Iterative Program Generation for Multi-Hop Question Answering over Heterogeneous Knowledge. (arXiv:2308.11257v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.11257","description":"<p>The semantic parsing-based method is an important research branch for\nknowledge-based question answering. It usually generates executable programs\nlean upon the question and then conduct them to reason answers over a knowledge\nbase. Benefit from this inherent mechanism, it has advantages in the\nperformance and the interpretability. However, traditional semantic parsing\nmethods usually generate a complete program before executing it, which\nstruggles with multi-hop question answering over heterogeneous knowledge. On\none hand, generating a complete multi-hop program relies on multiple\nheterogeneous supporting facts, and it is difficult for generators to\nunderstand these facts simultaneously. On the other hand, this way ignores the\nsemantic information of the intermediate answers at each hop, which is\nbeneficial for subsequent generation. To alleviate these challenges, we propose\na self-iterative framework for multi-hop program generation (HopPG) over\nheterogeneous knowledge, which leverages the previous execution results to\nretrieve supporting facts and generate subsequent programs hop by hop. We\nevaluate our model on MMQA-T^2, and the experimental results show that HopPG\noutperforms existing semantic-parsing-based baselines, especially on the\nmulti-hop questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yongwei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chaoqun Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Financial News Analytics Using Fine-Tuned Llama 2 GPT Model. (arXiv:2308.13032v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.13032","description":"<p>The paper considers the possibility to fine-tune Llama 2 GPT large language\nmodel (LLM) for the multitask analysis of financial news. For fine-tuning, the\nPEFT/LoRA based approach was used. In the study, the model was fine-tuned for\nthe following tasks: analysing a text from financial market perspectives,\nhighlighting main points of a text, summarizing a text and extracting named\nentities with appropriate sentiments. The obtained results show that the\nfine-tuned Llama 2 model can perform a multitask financial news analysis with a\nspecified structure of response, part of response can be a structured text and\nanother part of data can have JSON format for further processing. Extracted\nsentiments for named entities can be considered as predictive features in\nsupervised machine learning models with quantitative target variables.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pavlyshenko_B/0/1/0/all/0/1\">Bohdan M. Pavlyshenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MLLM-DataEngine: An Iterative Refinement Approach for MLLM. (arXiv:2308.13566v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2308.13566","description":"<p>Despite the great advance of Multimodal Large Language Models (MLLMs) in both\ninstruction dataset building and benchmarking, the independence of training and\nevaluation makes current MLLMs hard to further improve their capability under\nthe guidance of evaluation results with a relatively low human cost. In this\npaper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data\ngeneration, model training, and evaluation. Within each loop iteration, the\nMLLM-DataEngine first analyze the weakness of the model based on the evaluation\nresults, then generate a proper incremental dataset for the next training\niteration and enhance the model capability iteratively. Compared with previous\ndata collection methods which are separate from the benchmarking, the data\ngenerated by MLLM-DataEngine shows better targeting, quality, and correctness.\nFor targeting, we propose an Adaptive Bad-case Sampling module, which adjusts\nthe ratio of different types of data within each incremental dataset based on\nthe benchmarking results. For quality, we resort to GPT-4 to generate\nhigh-quality data with each given data type. For correctness, prompt design is\ncritical for the data generation results. Rather than previous hand-crafted\nprompt, we propose an Interactive Prompt Optimization strategy, which optimizes\nthe prompt with the multi-round interaction between human and GPT, and improve\nthe correctness of generated data greatly. Through extensive experiments, we\nfind our MLLM-DataEngine could boost the MLLM capability in a targeted and\nautomatic manner, with only a few human participation. We hope it could be a\ngeneral solution for the following MLLMs building. The MLLM-DataEngine has been\nopen-sourced and is now available at\nhttps://github.com/opendatalab/MLLM-DataEngine.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhiyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_L/0/1/0/all/0/1\">Linke Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Conghui He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Large Language Models for Knowledge Graph Completion. (arXiv:2308.13916v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.13916","description":"<p>Knowledge graphs play a vital role in numerous artificial intelligence tasks,\nyet they frequently face the issue of incompleteness. In this study, we explore\nutilizing Large Language Models (LLM) for knowledge graph completion. We\nconsider triples in knowledge graphs as text sequences and introduce an\ninnovative framework called Knowledge Graph LLM (KG-LLM) to model these\ntriples. Our technique employs entity and relation descriptions of a triple as\nprompts and utilizes the response for predictions. Experiments on various\nbenchmark knowledge graphs demonstrate that our method attains state-of-the-art\nperformance in tasks such as triple classification and relation prediction. We\nalso find that fine-tuning relatively smaller models (e.g., LLaMA-7B,\nChatGLM-6B) outperforms recent ChatGPT and GPT-4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Liang Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jiazhen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1\">Chengsheng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yuan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Based MoE for Multitask Multilingual Machine Translation. (arXiv:2308.15772v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.15772","description":"<p>Mixture-of-experts (MoE) architecture has been proven a powerful method for\ndiverse tasks in training deep models in many applications. However, current\nMoE implementations are task agnostic, treating all tokens from different tasks\nin the same manner. In this work, we instead design a novel method that\nincorporates task information into MoE models at different granular levels with\nshared dynamic task-based adapters. Our experiments and analysis show the\nadvantages of our approaches over the dense and canonical MoE models on\nmulti-task multilingual machine translations. With task-specific adapters, our\nmodels can additionally generalize to new tasks efficiently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hai Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1\">David P. Woodruff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poczos_B/0/1/0/all/0/1\">Barnabas Poczos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadalla_H/0/1/0/all/0/1\">Hany Hassan Awadalla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interdisciplinary Fairness in Imbalanced Research Proposal Topic Inference: A Hierarchical Transformer-based Method with Selective Interpolation. (arXiv:2309.01717v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.01717","description":"<p>The objective of topic inference in research proposals aims to obtain the\nmost suitable disciplinary division from the discipline system defined by a\nfunding agency. The agency will subsequently find appropriate peer review\nexperts from their database based on this division. Automated topic inference\ncan reduce human errors caused by manual topic filling, bridge the knowledge\ngap between funding agencies and project applicants, and improve system\nefficiency. Existing methods focus on modeling this as a hierarchical\nmulti-label classification problem, using generative models to iteratively\ninfer the most appropriate topic information. However, these methods overlook\nthe gap in scale between interdisciplinary research proposals and\nnon-interdisciplinary ones, leading to an unjust phenomenon where the automated\ninference system categorizes interdisciplinary proposals as\nnon-interdisciplinary, causing unfairness during the expert assignment. How can\nwe address this data imbalance issue under a complex discipline system and\nhence resolve this unfairness? In this paper, we implement a topic label\ninference system based on a Transformer encoder-decoder architecture.\nFurthermore, we utilize interpolation techniques to create a series of\npseudo-interdisciplinary proposals from non-interdisciplinary ones during\ntraining based on non-parametric indicators such as cross-topic probabilities\nand topic occurrence probabilities. This approach aims to reduce the bias of\nthe system during model training. Finally, we conduct extensive experiments on\na real-world dataset to verify the effectiveness of the proposed method. The\nexperimental results demonstrate that our training strategy can significantly\nmitigate the unfairness generated in the topic inference task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1\">Meng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Min Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1\">Ziyue Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanjie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Z/0/1/0/all/0/1\">Zhiyuan Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuanchun Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models. (arXiv:2309.01940v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.01940","description":"<p>With the emergence of Large Language Models (LLMs), there has been a\nsignificant improvement in the programming capabilities of models, attracting\ngrowing attention from researchers. We propose CodeApex, a bilingual benchmark\ndataset focusing on the programming comprehension and code generation abilities\nof LLMs. CodeApex comprises three types of multiple-choice questions:\nconceptual understanding, commonsense reasoning, and multi-hop reasoning,\ndesigned to evaluate LLMs on programming comprehension tasks. Additionally,\nCodeApex utilizes algorithmic questions and corresponding test cases to assess\nthe code quality generated by LLMs. We evaluate 14 state-of-the-art LLMs,\nincluding both general-purpose and specialized models. GPT exhibits the best\nprogramming capabilities, achieving approximate accuracies of 50% and 56% on\nthe two tasks, respectively. There is still significant room for improvement in\nprogramming tasks. We hope that CodeApex can serve as a reference for\nevaluating the coding capabilities of LLMs, further promoting their development\nand growth. Datasets are released at https://github.com/APEXLAB/CodeApex.git.\nCodeApex submission website is https://apex.sjtu.edu.cn/codeapex/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1\">Lingyue Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_H/0/1/0/all/0/1\">Huacan Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shuang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_K/0/1/0/all/0/1\">Kounianhua Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Longteng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jiayi Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rui_R/0/1/0/all/0/1\">Renting Rui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jianghao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuchen Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingkuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1\">Siyuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kangning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GRASS: Unified Generation Model for Speech-to-Semantic Tasks. (arXiv:2309.02780v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.02780","description":"<p>This paper explores the instruction fine-tuning technique for\nspeech-to-semantic tasks by introducing a unified end-to-end (E2E) framework\nthat generates target text conditioned on a task-related prompt for audio data.\nWe pre-train the model using large and diverse data, where instruction-speech\npairs are constructed via a text-to-speech (TTS) system. Extensive experiments\ndemonstrate that our proposed model achieves state-of-the-art (SOTA) results on\nmany benchmarks covering speech named entity recognition, speech sentiment\nanalysis, speech question answering, and more, after fine-tuning. Furthermore,\nthe proposed model achieves competitive performance in zero-shot and few-shot\nscenarios. To facilitate future work on instruction fine-tuning for\nspeech-to-semantic tasks, we release our instruction dataset and code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_A/0/1/0/all/0/1\">Aobo Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Shuyu Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yushu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_H/0/1/0/all/0/1\">Hua Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CSPRD: A Financial Policy Retrieval Dataset for Chinese Stock Market. (arXiv:2309.04389v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.04389","description":"<p>In recent years, great advances in pre-trained language models (PLMs) have\nsparked considerable research focus and achieved promising performance on the\napproach of dense passage retrieval, which aims at retrieving relative passages\nfrom massive corpus with given questions. However, most of existing datasets\nmainly benchmark the models with factoid queries of general commonsense, while\nspecialised fields such as finance and economics remain unexplored due to the\ndeficiency of large-scale and high-quality datasets with expert annotations. In\nthis work, we propose a new task, policy retrieval, by introducing the Chinese\nStock Policy Retrieval Dataset (CSPRD), which provides 700+ prospectus passages\nlabeled by experienced experts with relevant articles from 10k+ entries in our\ncollected Chinese policy corpus. Experiments on lexical, embedding and\nfine-tuned bi-encoder models show the effectiveness of our proposed CSPRD yet\nalso suggests ample potential for improvement. Our best performing baseline\nachieves 56.1% MRR@10, 28.5% NDCG@10, 37.5% Recall@10 and 80.6% Precision@10 on\ndev set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zeyang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jinhao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Y/0/1/0/all/0/1\">Yongjian Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1\">Dawei Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-09-11T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}