{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-06-01T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Large language models improve Alzheimer's disease diagnosis using multi-modality data. (arXiv:2305.19280v1 [cs.LG])","link":"http://arxiv.org/abs/2305.19280","description":"<p>In diagnosing challenging conditions such as Alzheimer's disease (AD),\nimaging is an important reference. Non-imaging patient data such as patient\ninformation, genetic data, medication information, cognitive and memory tests\nalso play a very important role in diagnosis. Effect. However, limited by the\nability of artificial intelligence models to mine such information, most of the\nexisting models only use multi-modal image data, and cannot make full use of\nnon-image data. We use a currently very popular pre-trained large language\nmodel (LLM) to enhance the model's ability to utilize non-image data, and\nachieved SOTA results on the ADNI dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yingjie Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xianfeng Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaoyin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models. (arXiv:2305.19308v1 [cs.SE])","link":"http://arxiv.org/abs/2305.19308","description":"<p>Computer end users have spent billions of hours completing daily tasks like\ntabular data processing and project timeline scheduling. Most of these tasks\nare repetitive and error-prone, yet most end users lack the skill of automating\naway these burdensome works. With the advent of large language models (LLMs),\ndirecting software with natural language user requests become a reachable goal.\nIn this work, we propose a SheetCopilot agent which takes natural language task\nand control spreadsheet to fulfill the requirements. We propose a set of atomic\nactions as an abstraction of spreadsheet software functionalities. We further\ndesign a state machine-based task planning framework for LLMs to robustly\ninteract with spreadsheets. We curate a representative dataset containing 221\nspreadsheet control tasks and establish a fully automated evaluation pipeline\nfor rigorously benchmarking the ability of LLMs in software control tasks. Our\nSheetCopilot correctly completes 44.3\\% of tasks for a single generation,\noutperforming the strong code generation baseline by a wide margin. Our project\npage:https://sheetcopilot-demo.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jingran Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuntao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoxiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breeding Machine Translations: Evolutionary approach to survive and thrive in the world of automated evaluation. (arXiv:2305.19330v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19330","description":"<p>We propose a genetic algorithm (GA) based method for modifying n-best lists\nproduced by a machine translation (MT) system. Our method offers an innovative\napproach to improving MT quality and identifying weaknesses in evaluation\nmetrics. Using common GA operations (mutation and crossover) on a list of\nhypotheses in combination with a fitness function (an arbitrary MT metric), we\nobtain novel and diverse outputs with high metric scores. With a combination of\nmultiple MT metrics as the fitness function, the proposed method leads to an\nincrease in translation quality as measured by other held-out automatic\nmetrics. With a single metric (including popular ones such as COMET) as the\nfitness function, we find blind spots and flaws in the metric. This allows for\nan automated search for adversarial examples in an arbitrary metric, without\nprior assumptions on the form of such example. As a demonstration of the\nmethod, we create datasets of adversarial examples and use them to show that\nreference-free COMET is substantially less robust than the reference-based\nversion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jon_J/0/1/0/all/0/1\">Josef Jon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses. (arXiv:2305.19339v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19339","description":"<p>A human decision-maker benefits the most from an AI assistant that corrects\nfor their biases. For problems such as generating interpretation of a radiology\nreport given findings, a system predicting only highly likely outcomes may be\nless useful, where such outcomes are already obvious to the user. To alleviate\nbiases in human decision-making, it is worth considering a broad differential\ndiagnosis, going beyond the most likely options. We introduce a new task, \"less\nlikely brainstorming,\" that asks a model to generate outputs that humans think\nare relevant but less likely to happen. We explore the task in two settings: a\nbrain MRI interpretation generation setting and an everyday commonsense\nreasoning setting. We found that a baseline approach of training with less\nlikely hypotheses as targets generates outputs that humans evaluate as either\nlikely or irrelevant nearly half of the time; standard MLE training is not\neffective. To tackle this problem, we propose a controlled text generation\nmethod that uses a novel contrastive learning strategy to encourage models to\ndifferentiate between generating likely and less likely outputs according to\nhumans. We compare our method with several state-of-the-art controlled text\ngeneration models via automatic and human evaluations and show that our models'\ncapability of generating less likely outputs is improved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Liyan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanshan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Ying Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rousseau_J/0/1/0/all/0/1\">Justin F. Rousseau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"infoVerse: A Universal Framework for Dataset Characterization with Multidimensional Meta-information. (arXiv:2305.19344v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19344","description":"<p>The success of NLP systems often relies on the availability of large,\nhigh-quality datasets. However, not all samples in these datasets are equally\nvaluable for learning, as some may be redundant or noisy. Several methods for\ncharacterizing datasets based on model-driven meta-information (e.g., model's\nconfidence) have been developed, but the relationship and complementary effects\nof these methods have received less attention. In this paper, we introduce\ninfoVerse, a universal framework for dataset characterization, which provides a\nnew feature space that effectively captures multidimensional characteristics of\ndatasets by incorporating various model-driven meta-information. infoVerse\nreveals distinctive regions of the dataset that are not apparent in the\noriginal semantic space, hence guiding users (or models) in identifying which\nsamples to focus on for exploration, assessment, or annotation. Additionally,\nwe propose a novel sampling method on infoVerse to select a set of data points\nthat maximizes informativeness. In three real-world applications (data pruning,\nactive learning, and data annotation), the samples chosen on infoVerse space\nconsistently outperform strong baselines in all applications. Our code and demo\nare publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaehyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yekyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langis_K/0/1/0/all/0/1\">Karin de Langis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stable Anisotropic Regularization. (arXiv:2305.19358v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19358","description":"<p>Given the success of Large Language Models (LLMs), there has been\nconsiderable interest in studying the properties of model activations. The\nliterature overwhelmingly agrees that LLM representations are dominated by a\nfew ``outlier dimensions'' with exceedingly high variance and magnitude.\nSeveral studies in Natural Language Processing (NLP) have sought to mitigate\nthe impact of such outlier dimensions and force LLMs to be isotropic (i.e.,\nhave uniform variance across all dimensions in embedding space). Isotropy is\nthought to be a desirable property for LLMs that improves model performance and\nmore closely aligns textual representations with human intuition. However, many\nof the claims regarding isotropy in NLP have been based on the average cosine\nsimilarity of embeddings, which has recently been shown to be a flawed measure\nof isotropy. In this paper, we propose I-STAR: IsoScore$^{\\star}$-based STable\nAnisotropic Regularization, a novel regularization method that can be used to\nincrease or decrease levels of isotropy in embedding space during training.\nI-STAR uses IsoScore$^{\\star}$, the first accurate measure of isotropy that is\nboth differentiable and stable on mini-batch computations. In contrast to\nseveral previous works, we find that \\textit{decreasing} isotropy in\ncontextualized embeddings improves performance on the majority of tasks and\nmodels considered in this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rudman_W/0/1/0/all/0/1\">William Rudman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blockwise Parallel Transformer for Long Context Large Models. (arXiv:2305.19370v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19370","description":"<p>Transformers have emerged as the cornerstone of state-of-the-art natural\nlanguage processing models, showcasing exceptional performance across a wide\nrange of AI applications. However, the memory demands posed by the\nself-attention mechanism and the large feedforward network in Transformers\nlimit their ability to handle long sequences, thereby creating challenges for\ntasks involving multiple long sequences or long-term dependencies. We present a\ndistinct approach, Blockwise Parallel Transformer (BPT), that leverages\nblockwise computation of self-attention and feedforward network fusion to\nminimize memory costs. By processing longer input sequences while maintaining\nmemory efficiency, BPT enables training sequences up to 32 times longer than\nvanilla Transformers and 2 to 4 times longer than previous memory-efficient\nmethods. Extensive experiments on language modeling and reinforcement learning\ntasks demonstrate the effectiveness of BPT in reducing memory requirements and\nimproving performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mining Themes in Clinical Notes to Identify Phenotypes and to Predict Length of Stay in Patients admitted with Heart Failure. (arXiv:2305.19373v1 [cs.LG])","link":"http://arxiv.org/abs/2305.19373","description":"<p>Heart failure is a syndrome which occurs when the heart is not able to pump\nblood and oxygen to support other organs in the body. Identifying the\nunderlying themes in the diagnostic codes and procedure reports of patients\nadmitted for heart failure could reveal the clinical phenotypes associated with\nheart failure and to group patients based on their similar characteristics\nwhich could also help in predicting patient outcomes like length of stay. These\nclinical phenotypes usually have a probabilistic latent structure and hence, as\nthere has been no previous work on identifying phenotypes in clinical notes of\nheart failure patients using a probabilistic framework and to predict length of\nstay of these patients using data-driven artificial intelligence-based methods,\nwe apply natural language processing technique, topic modeling, to identify the\nthemes present in diagnostic codes and in procedure reports of 1,200 patients\nadmitted for heart failure at the University of Illinois Hospital and Health\nSciences System (UI Health). Topic modeling identified twelve themes each in\ndiagnostic codes and procedure reports which revealed information about\ndifferent phenotypes related to various perspectives about heart failure, to\nstudy patients' profiles and to discover new relationships among medical\nconcepts. Each theme had a set of keywords and each clinical note was labeled\nwith two themes - one corresponding to its diagnostic code and the other\ncorresponding to its procedure reports along with their percentage\ncontribution. We used these themes and their percentage contribution to predict\nlength of stay. We found that the themes discovered in diagnostic codes and\nprocedure reports using topic modeling together were able to predict length of\nstay of the patients with an accuracy of 61.1% and an Area under the Receiver\nOperating Characteristic Curve (ROC AUC) value of 0.828.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Ankita Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_T/0/1/0/all/0/1\">Tanvi Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romine_W/0/1/0/all/0/1\">William L. Romine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thirunarayan_K/0/1/0/all/0/1\">Krishnaprasad Thirunarayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lingwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cajita_M/0/1/0/all/0/1\">Mia Cajita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantum Natural Language Processing based Sentiment Analysis using lambeq Toolkit. (arXiv:2305.19383v1 [quant-ph])","link":"http://arxiv.org/abs/2305.19383","description":"<p>Sentiment classification is one the best use case of classical natural\nlanguage processing (NLP) where we can witness its power in various daily life\ndomains such as banking, business and marketing industry. We already know how\nclassical AI and machine learning can change and improve technology. Quantum\nnatural language processing (QNLP) is a young and gradually emerging technology\nwhich has the potential to provide quantum advantage for NLP tasks. In this\npaper we show the first application of QNLP for sentiment analysis and achieve\nperfect test set accuracy for three different kinds of simulations and a decent\naccuracy for experiments ran on a noisy quantum device. We utilize the lambeq\nQNLP toolkit and $t|ket&gt;$ by Cambridge Quantum (Quantinuum) to bring out the\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Ganguly_S/0/1/0/all/0/1\">Srinjoy Ganguly</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Morapakula_S/0/1/0/all/0/1\">Sai Nandan Morapakula</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Coronado_L/0/1/0/all/0/1\">Luis Miguel Pozo Coronado</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DyGen: Learning from Noisy Labels via Dynamics-Enhanced Generative Modeling. (arXiv:2305.19395v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19395","description":"<p>Learning from noisy labels is a challenge that arises in many real-world\napplications where training data can contain incorrect or corrupted labels.\nWhen fine-tuning language models with noisy labels, models can easily overfit\nthe label noise, leading to decreased performance. Most existing methods for\nlearning from noisy labels use static input features for denoising, but these\nmethods are limited by the information they can provide on true label\ndistributions and can result in biased or incorrect predictions. In this work,\nwe propose the Dynamics-Enhanced Generative Model (DyGen), which uses dynamic\npatterns in the embedding space during the fine-tuning process of language\nmodels to improve noisy label predictions. DyGen uses the variational\nauto-encoding framework to infer the posterior distributions of true labels\nfrom noisy labels and training dynamics. Additionally, a co-regularization\nmechanism is used to minimize the impact of potentially noisy labels and\npriors. DyGen demonstrates an average accuracy improvement of 3.10% on two\nsynthetic noise datasets and 1.48% on three real-world noise datasets compared\nto the previous state-of-the-art. Extensive experiments and analyses show the\neffectiveness of each component in DyGen. Our code is available for\nreproducibility on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yuchen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingkai Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resource-Efficient Fine-Tuning Strategies for Automatic MOS Prediction in Text-to-Speech for Low-Resource Languages. (arXiv:2305.19396v1 [eess.AS])","link":"http://arxiv.org/abs/2305.19396","description":"<p>We train a MOS prediction model based on wav2vec 2.0 using the open-access\ndata sets BVCC and SOMOS. Our test with neural TTS data in the low-resource\nlanguage (LRL) West Frisian shows that pre-training on BVCC before fine-tuning\non SOMOS leads to the best accuracy for both fine-tuned and zero-shot\nprediction. Further fine-tuning experiments show that using more than 30\npercent of the total data does not lead to significant improvements. In\naddition, fine-tuning with data from a single listener shows promising\nsystem-level accuracy, supporting the viability of one-participant pilot tests.\nThese findings can all assist the resource-conscious development of TTS for\nLRLs by progressing towards better zero-shot MOS prediction and informing the\ndesign of listening tests, especially in early-stage evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Do_P/0/1/0/all/0/1\">Phat Do</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Coler_M/0/1/0/all/0/1\">Matt Coler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dijkstra_J/0/1/0/all/0/1\">Jelske Dijkstra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klabbers_E/0/1/0/all/0/1\">Esther Klabbers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Vision Transformers for Robust Representation Learning. (arXiv:2305.19402v1 [cs.CV])","link":"http://arxiv.org/abs/2305.19402","description":"<p>We present Contextual Vision Transformers (ContextViT), a method for\nproducing robust feature representations for images exhibiting grouped\nstructure such as covariates. ContextViT introduces an extra context token to\nencode group-specific information, allowing the model to explain away\ngroup-specific covariate structures while keeping core visual features shared\nacross groups. Specifically, given an input image, Context-ViT maps images that\nshare the same covariate into this context token appended to the input image\ntokens to capture the effects of conditioning the model on group membership. We\nfurthermore introduce a context inference network to predict such tokens on the\nfly given a few samples from a group distribution, enabling ContextViT to\ngeneralize to new testing distributions at inference time. We illustrate the\nperformance of ContextViT through a diverse range of applications. In\nsupervised fine-tuning, we demonstrate that augmenting pre-trained ViTs with\nadditional context conditioning leads to significant improvements in\nout-of-distribution generalization on iWildCam and FMoW. We also explored\nself-supervised representation learning with ContextViT. Our experiments on the\nCamelyon17 pathology imaging benchmark and the cpg-0000 microscopy imaging\nbenchmark demonstrate that ContextViT excels in learning stable image\nfeaturizations amidst covariate shift, consistently outperforming its ViT\ncounterpart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yujia Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karaletsos_T/0/1/0/all/0/1\">Theofanis Karaletsos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Examining risks of racial biases in NLP tools for child protective services. (arXiv:2305.19409v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19409","description":"<p>Although much literature has established the presence of demographic bias in\nnatural language processing (NLP) models, most work relies on curated bias\nmetrics that may not be reflective of real-world applications. At the same\ntime, practitioners are increasingly using algorithmic tools in high-stakes\nsettings, with particular recent interest in NLP. In this work, we focus on one\nsuch setting: child protective services (CPS). CPS workers often write copious\nfree-form text notes about families they are working with, and CPS agencies are\nactively seeking to deploy NLP models to leverage these data. Given\nwell-established racial bias in this setting, we investigate possible ways\ndeployed NLP is liable to increase racial disparities. We specifically examine\nword statistics within notes and algorithmic fairness in risk prediction,\ncoreference resolution, and named entity recognition (NER). We document\nconsistent algorithmic unfairness in NER models, possible algorithmic\nunfairness in coreference resolution models, and little evidence of exacerbated\nracial bias in risk prediction. While there is existing pronounced criticism of\nrisk prediction, our results expose previously undocumented risks of racial\nbias in realistic information extraction systems, highlighting potential\nconcerns in deploying them, even though they may appear more benign. Our work\nserves as a rare realistic examination of NLP algorithmic fairness in a\npotential deployed setting and a timely investigation of a specific risk\nassociated with deploying NLP in CPS settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Field_A/0/1/0/all/0/1\">Anjalie Field</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coston_A/0/1/0/all/0/1\">Amanda Coston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_N/0/1/0/all/0/1\">Nupoor Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chouldechova_A/0/1/0/all/0/1\">Alexandra Chouldechova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putnam_Hornstein_E/0/1/0/all/0/1\">Emily Putnam-Hornstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steier_D/0/1/0/all/0/1\">David Steier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Multi-Instance Multi-Label Learning for Detecting Propaganda Techniques. (arXiv:2305.19419v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19419","description":"<p>Since the introduction of the SemEval 2020 Task 11 (Martino et al., 2020a),\nseveral approaches have been proposed in the literature for classifying\npropaganda based on the rhetorical techniques used to influence readers. These\nmethods, however, classify one span at a time, ignoring dependencies from the\nlabels of other spans within the same context. In this paper, we approach\npropaganda technique classification as a Multi-Instance Multi-Label (MIML)\nlearning problem (Zhou et al., 2012) and propose a simple RoBERTa-based model\n(Zhuang et al., 2021) for classifying all spans in an article simultaneously.\nFurther, we note that, due to the annotation process where annotators\nclassified the spans by following a decision tree, there is an inherent\nhierarchical relationship among the different techniques, which existing\napproaches ignore. We incorporate these hierarchical label dependencies by\nadding an auxiliary classifier for each node in the decision tree to the\ntraining objective and ensembling the predictions from the original and\nauxiliary classifiers at test time. Overall, our model leads to an absolute\nimprovement of 2.47% micro-F1 over the model from the shared task winning team\nin a cross-validation setup and is the best performing non-ensemble model on\nthe shared task leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anni Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1\">Bhuwan Dhingra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning. (arXiv:2305.19426v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19426","description":"<p>A number of recent benchmarks seek to assess how well models handle natural\nlanguage negation. However, these benchmarks lack the controlled example\nparadigms that would allow us to infer whether a model had learned how negation\nmorphemes semantically scope. To fill these analytical gaps, we present the\nScoped Negation NLI (ScoNe-NLI) benchmark, which contains contrast sets of six\nexamples with up to two negations where either zero, one, or both negative\nmorphemes affect the NLI label. We use ScoNe-NLI to assess fine-tuning and\nin-context learning strategies. We find that RoBERTa and DeBERTa models solve\nScoNe-NLI after many shot fine-tuning. For in-context learning, we test\nInstructGPT models and find that most prompt strategies are not successful,\nincluding those using step-by-step reasoning. To better understand this result,\nwe extend ScoNe with ScoNe-NLG, a sentence completion test set that embeds\nnegation reasoning in short narratives. Here, InstructGPT is successful, which\nreveals the model can correctly reason about negation, but struggles to do so\non prompt-adapted NLI examples outside of its core pretraining regime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+She_J/0/1/0/all/0/1\">Jingyuan Selena She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Atticus Geiger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Impact of Positional Encoding on Length Generalization in Transformers. (arXiv:2305.19466v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19466","description":"<p>Length generalization, the ability to generalize from small training context\nsizes to larger ones, is a critical challenge in the development of\nTransformer-based language models. Positional encoding (PE) has been identified\nas a major factor influencing length generalization, but the exact impact of\ndifferent PE schemes on extrapolation in downstream tasks remains unclear. In\nthis paper, we conduct a systematic empirical study comparing the length\ngeneralization performance of decoder-only Transformers with five different\nposition encoding approaches including Absolute Position Embedding (APE), T5's\nRelative PE, ALiBi, and Rotary, in addition to Transformers without positional\nencoding (NoPE). Our evaluation encompasses a battery of reasoning and\nmathematical tasks. Our findings reveal that the most commonly used positional\nencoding methods, such as ALiBi, Rotary, and APE, are not well suited for\nlength generalization in downstream tasks. More importantly, NoPE outperforms\nother explicit positional encoding methods while requiring no additional\ncomputation. We theoretically demonstrate that NoPE can represent both absolute\nand relative PEs, but when trained with SGD, it mostly resembles T5's relative\nPE attention patterns. Finally, we find that scratchpad is not always helpful\nto solve length generalization and its format highly impacts the model's\nperformance. Overall, our work suggests that explicit position embeddings are\nnot essential for decoder-only Transformers to generalize well to longer\nsequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kazemnejad_A/0/1/0/all/0/1\">Amirhossein Kazemnejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padhi_I/0/1/0/all/0/1\">Inkit Padhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamurthy_K/0/1/0/all/0/1\">Karthikeyan Natesan Ramamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Payel Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning. (arXiv:2305.19472v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19472","description":"<p>Procedural planning, which entails decomposing a high-level goal into a\nsequence of temporally ordered steps, is an important yet intricate task for\nmachines. It involves integrating common-sense knowledge to reason about\ncomplex contextualized situations that are often counterfactual, e.g.\n\"scheduling a doctor's appointment without a phone\". While current approaches\nshow encouraging results using large language models (LLMs), they are hindered\nby drawbacks such as costly API calls and reproducibility issues. In this\npaper, we advocate planning using smaller language models. We present PlaSma, a\nnovel two-pronged approach to endow small language models with procedural\nknowledge and (counterfactual) planning capabilities. More concretely, we\ndevelop symbolic procedural knowledge distillation to enhance the implicit\nknowledge in small language models and an inference-time algorithm to\nfacilitate more structured and accurate reasoning. In addition, we introduce a\nnovel task, Counterfactual Planning, that requires a revision of a plan to cope\nwith a counterfactual situation. In both the original and counterfactual\nsetting, we show that orders-of-magnitude smaller models (770M-11B parameters)\ncan compete and often surpass their larger teacher models' capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1\">Valentina Pyatkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Lorraine Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arai_H/0/1/0/all/0/1\">Hirona J. Arai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1\">Soumya Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1\">Keisuke Sakaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ethical Considerations for Machine Translation of Indigenous Languages: Giving a Voice to the Speakers. (arXiv:2305.19474v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19474","description":"<p>In recent years machine translation has become very successful for\nhigh-resource language pairs. This has also sparked new interest in research on\nthe automatic translation of low-resource languages, including Indigenous\nlanguages. However, the latter are deeply related to the ethnic and cultural\ngroups that speak (or used to speak) them. The data collection, modeling and\ndeploying machine translation systems thus result in new ethical questions that\nmust be addressed. Motivated by this, we first survey the existing literature\non ethical considerations for the documentation, translation, and general\nnatural language processing for Indigenous languages. Afterward, we conduct and\nanalyze an interview study to shed light on the positions of community leaders,\nteachers, and language activists regarding ethical concerns for the automatic\ntranslation of their languages. Our results show that the inclusion, at\ndifferent degrees, of native speakers and community members is vital to\nperforming better and more ethical research on Indigenous languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mager_M/0/1/0/all/0/1\">Manuel Mager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mager_E/0/1/0/all/0/1\">Elisabeth Mager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1\">Katharina Kann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Flow Graph Prediction of Open-Domain Procedural Texts. (arXiv:2305.19497v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19497","description":"<p>Machine comprehension of procedural texts is essential for reasoning about\nthe steps and automating the procedures. However, this requires identifying\nentities within a text and resolving the relationships between the entities.\nPrevious work focused on the cooking domain and proposed a framework to convert\na recipe text into a flow graph (FG) representation. In this work, we propose a\nframework based on the recipe FG for flow graph prediction of open-domain\nprocedural texts. To investigate flow graph prediction performance in\nnon-cooking domains, we introduce the wikiHow-FG corpus from articles on\nwikiHow, a website of how-to instruction articles. In experiments, we consider\nusing the existing recipe corpus and performing domain adaptation from the\ncooking to the target domain. Experimental results show that the domain\nadaptation models achieve higher performance than those trained only on the\ncooking or target domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shirai_K/0/1/0/all/0/1\">Keisuke Shirai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kameko_H/0/1/0/all/0/1\">Hirotaka Kameko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_S/0/1/0/all/0/1\">Shinsuke Mori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Lottery Prompts for Pre-trained Language Models. (arXiv:2305.19500v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19500","description":"<p>Consistently scaling pre-trained language models (PLMs) imposes substantial\nburdens on model adaptation, necessitating more efficient alternatives to\nconventional fine-tuning. Given the advantage of prompting in the zero-shot\nsetting and the observed performance fluctuation among different prompts, we\nexplore the instance-level prompt and their generalizability. By searching\nthrough the prompt space, we first validate the assumption that for every\ninstance, there is almost always a lottery prompt that induces the correct\nprediction from the PLM, and such prompt can be obtained at a low cost thanks\nto the inherent ability of PLMs. Meanwhile, we find that some strong lottery\nprompts have high performance over the whole training set, and they are\nequipped with distinguishable linguistic features. Lastly, we attempt to\ngeneralize the searched strong lottery prompts to unseen data with prompt\nensembling method without any parameter tuning. Experiments are conducted on\nvarious types of NLP classification tasks and demonstrate that the proposed\nmethod can achieve comparable results with other gradient-free and\noptimization-free baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaobin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengding Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Text Style Transfer with Diffusion-Based Language Models. (arXiv:2305.19512v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19512","description":"<p>Diffusion probabilistic models have shown great success in generating\nhigh-quality images controllably, and researchers have tried to utilize this\ncontrollability into text generation domain. Previous works on diffusion-based\nlanguage models have shown that they can be trained without external knowledge\n(such as pre-trained weights) and still achieve stable performance and\ncontrollability. In this paper, we trained a diffusion-based model on StylePTB\ndataset, the standard benchmark for fine-grained text style transfers. The\ntasks in StylePTB requires much more refined control over the output text\ncompared to tasks evaluated in previous works, and our model was able to\nachieve state-of-the-art performance on StylePTB on both individual and\ncompositional transfers. Moreover, our model, trained on limited data from\nStylePTB without external knowledge, outperforms previous works that utilized\npretrained weights, embeddings, and external grammar parsers, and this may\nindicate that diffusion-based language models have great potential under\nlow-resource settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Tiange Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiacheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollon_T/0/1/0/all/0/1\">Todd C. Hollon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate and Structured Pruning for Efficient Automatic Speech Recognition. (arXiv:2305.19549v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19549","description":"<p>Automatic Speech Recognition (ASR) has seen remarkable advancements with deep\nneural networks, such as Transformer and Conformer. However, these models\ntypically have large model sizes and high inference costs, posing a challenge\nto deploy on resource-limited devices. In this paper, we propose a novel\ncompression strategy that leverages structured pruning and knowledge\ndistillation to reduce the model size and inference cost of the Conformer model\nwhile preserving high recognition performance. Our approach utilizes a set of\nbinary masks to indicate whether to retain or prune each Conformer module, and\nemploys L0 regularization to learn the optimal mask values. To further enhance\npruning performance, we use a layerwise distillation strategy to transfer\nknowledge from unpruned to pruned models. Our method outperforms all pruning\nbaselines on the widely used LibriSpeech benchmark, achieving a 50% reduction\nin model size and a 28% reduction in inference cost with minimal performance\nloss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Huiqiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Lyna Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shijie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Ting Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Lili Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Are Not Abstract Reasoners. (arXiv:2305.19555v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19555","description":"<p>Large Language Models have shown tremendous performance on a large variety of\nnatural language processing tasks, ranging from text comprehension to common\nsense reasoning. However, the mechanisms responsible for this success remain\nunknown, and it is unclear whether LLMs can achieve human-like cognitive\ncapabilities or whether these models are still fundamentally limited. Abstract\nreasoning is a fundamental task for cognition, consisting of finding and\napplying a general pattern from few data. Evaluating deep neural architectures\non this task could give insight into their potential limitations regarding\nreasoning and their broad generalisation abilities, yet this is currently an\nunder-explored area. In this paper, we perform extensive evaluations of\nstate-of-the-art LLMs on abstract reasoning tasks, showing that they achieve\nvery limited performance in contrast with other natural language tasks, and we\ninvestigate the reasons for this difference. We apply techniques that have been\nshown to improve performance on other NLP tasks and show that in most cases\ntheir impact on abstract reasoning performance is limited. In the course of\nthis work, we have generated a new benchmark for evaluating language models on\nabstract reasoning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gendron_G/0/1/0/all/0/1\">Ga&#xeb;l Gendron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Q/0/1/0/all/0/1\">Qiming Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witbrock_M/0/1/0/all/0/1\">Michael Witbrock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobbie_G/0/1/0/all/0/1\">Gillian Dobbie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Automatic Pronunciation Assessment. (arXiv:2305.19563v1 [cs.SD])","link":"http://arxiv.org/abs/2305.19563","description":"<p>Automatic Pronunciation Assessment (APA) is vital for computer-assisted\nlanguage learning. Prior methods rely on annotated speech-text data to train\nAutomatic Speech Recognition (ASR) models or speech-score data to train\nregression models. In this work, we propose a novel zero-shot APA method based\non the pre-trained acoustic model, HuBERT. Our method involves encoding speech\ninput and corrupting them via a masking module. We then employ the Transformer\nencoder and apply k-means clustering to obtain token sequences. Finally, a\nscoring module is designed to measure the number of wrongly recovered tokens.\nExperimental results on speechocean762 demonstrate that the proposed method\nachieves comparable performance to supervised regression baselines and\noutperforms non-regression baselines in terms of Pearson Correlation\nCoefficient (PCC). Additionally, we analyze how masking strategies affect the\nperformance of APA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Mingqian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ye Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DC CoMix TTS: An End-to-End Expressive TTS with Discrete Code Collaborated with Mixer. (arXiv:2305.19567v1 [cs.SD])","link":"http://arxiv.org/abs/2305.19567","description":"<p>Despite the huge successes made in neutral TTS, content-leakage remains a\nchallenge. In this paper, we propose a new input representation and simple\narchitecture to achieve improved prosody modeling. Inspired by the recent\nsuccess in the use of discrete code in TTS, we introduce discrete code to the\ninput of the reference encoder. Specifically, we leverage the vector quantizer\nfrom the audio compression model to exploit the diverse acoustic information it\nhas already been trained on. In addition, we apply the modified MLP-Mixer to\nthe reference encoder, making the architecture lighter. As a result, we train\nthe prosody transfer TTS in an end-to-end manner. We prove the effectiveness of\nour method through both subjective and objective evaluations. We demonstrate\nthat the reference encoder learns better speaker-independent prosody when\ndiscrete code is utilized as input in the experiments. In addition, we obtain\ncomparable results even when fewer parameters are inputted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yerin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_M/0/1/0/all/0/1\">Myoung-Wan Koo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Tag-Team Approach: Leveraging CLS and Language Tagging for Enhancing Multilingual ASR. (arXiv:2305.19584v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19584","description":"<p>Building a multilingual Automated Speech Recognition (ASR) system in a\nlinguistically diverse country like India can be a challenging task due to the\ndifferences in scripts and the limited availability of speech data. This\nproblem can be solved by exploiting the fact that many of these languages are\nphonetically similar. These languages can be converted into a Common Label Set\n(CLS) by mapping similar sounds to common labels. In this paper, new approaches\nare explored and compared to improve the performance of CLS based multilingual\nASR model. Specific language information is infused in the ASR model by giving\nLanguage ID or using CLS to Native script converter on top of the CLS\nMultilingual model. These methods give a significant improvement in Word Error\nRate (WER) compared to the CLS baseline. These methods are further tried on\nout-of-distribution data to check their robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jayakumar_K/0/1/0/all/0/1\">Kaousheik Jayakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhadia_V/0/1/0/all/0/1\">Vrunda N. Sukhadia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arunkumar_A/0/1/0/all/0/1\">A Arunkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAIT: Efficient Multi-Segment Encoding in Transformers with Layer-Adjustable Interaction. (arXiv:2305.19585v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19585","description":"<p>Transformer encoders contextualize token representations by attending to all\nother tokens at each layer, leading to quadratic increase in compute effort\nwith the input length. In practice, however, the input text of many NLP tasks\ncan be seen as a sequence of related segments (e.g., the sequence of sentences\nwithin a passage, or the hypothesis and premise in NLI). While attending across\nthese segments is highly beneficial for many tasks, we hypothesize that this\ninteraction can be delayed until later encoding stages.\n</p>\n<p>To this end, we introduce Layer-Adjustable Interactions in Transformers\n(LAIT). Within LAIT, segmented inputs are first encoded independently, and then\njointly. This partial two-tower architecture bridges the gap between a Dual\nEncoder's ability to pre-compute representations for segments and a fully\nself-attentive Transformer's capacity to model cross-segment attention. The\nLAIT framework effectively leverages existing pretrained Transformers and\nconverts them into the hybrid of the two aforementioned architectures, allowing\nfor easy and intuitive control over the performance-efficiency tradeoff.\nExperimenting on a wide range of NLP tasks, we find LAIT able to reduce 30-50%\nof the attention FLOPs on many tasks, while preserving high accuracy; in some\npractical settings, LAIT could reduce actual latency by orders of magnitude.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Milbauer_J/0/1/0/all/0/1\">Jeremiah Milbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Louis_A/0/1/0/all/0/1\">Annie Louis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1\">Mohammad Javad Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabrikant_A/0/1/0/all/0/1\">Alex Fabrikant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLABERT Talk Pretty One Day: Modeling Second Language Acquisition with BERT. (arXiv:2305.19589v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19589","description":"<p>Second language acquisition (SLA) research has extensively studied\ncross-linguistic transfer, the influence of linguistic structure of a speaker's\nnative language [L1] on the successful acquisition of a foreign language [L2].\nEffects of such transfer can be positive (facilitating acquisition) or negative\n(impeding acquisition). We find that NLP literature has not given enough\nattention to the phenomenon of negative transfer. To understand patterns of\nboth positive and negative transfer between L1 and L2, we model sequential\nsecond language acquisition in LMs. Further, we build a Mutlilingual Age\nOrdered CHILDES (MAO-CHILDES) -- a dataset consisting of 5 typologically\ndiverse languages, i.e., German, French, Polish, Indonesian, and Japanese -- to\nunderstand the degree to which native Child-Directed Speech (CDS) [L1] can help\nor conflict with English language acquisition [L2]. To examine the impact of\nnative CDS, we use the TILT-based cross lingual transfer learning approach\nestablished by Papadimitriou and Jurafsky (2020) and find that, as in human\nSLA, language family distance predicts more negative transfer. Additionally, we\nfind that conversational speech data shows greater facilitation for language\nacquisition than scripted speech data. Our findings call for further research\nusing our novel Transformer-based SLA models and we would like to encourage it\nby releasing our code, data, and models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadavalli_A/0/1/0/all/0/1\">Aditya Yadavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadavalli_A/0/1/0/all/0/1\">Alekhya Yadavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tobin_V/0/1/0/all/0/1\">Vera Tobin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What does the Failure to Reason with \"Respectively\" in Zero/Few-Shot Settings Tell Us about Language Models?. (arXiv:2305.19597v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19597","description":"<p>Humans can effortlessly understand the coordinate structure of sentences such\nas \"Niels Bohr and Kurt Cobain were born in Copenhagen and Seattle,\nrespectively\". In the context of natural language inference (NLI), we examine\nhow language models (LMs) reason with respective readings (Gawron and Kehler,\n2004) from two perspectives: syntactic-semantic and commonsense-world\nknowledge. We propose a controlled synthetic dataset WikiResNLI and a naturally\noccurring dataset NatResNLI to encompass various explicit and implicit\nrealizations of \"respectively\". We show that fine-tuned NLI models struggle\nwith understanding such readings without explicit supervision. While few-shot\nlearning is easy in the presence of explicit cues, longer training is required\nwhen the reading is evoked implicitly, leaving models to rely on common sense\ninferences. Furthermore, our fine-grained analysis indicates models fail to\ngeneralize across different constructions. To conclude, we demonstrate that LMs\nstill lag behind humans in generalizing to the long tail of linguistic\nconstructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1\">Ruixiang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seolhwa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Clean Label Backdoor Attacks and Defenses on Text Classification Systems. (arXiv:2305.19607v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19607","description":"<p>Clean-label (CL) attack is a form of data poisoning attack where an adversary\nmodifies only the textual input of the training data, without requiring access\nto the labeling function. CL attacks are relatively unexplored in NLP, as\ncompared to label flipping (LF) attacks, where the latter additionally requires\naccess to the labeling function as well. While CL attacks are more resilient to\ndata sanitization and manual relabeling methods than LF attacks, they often\ndemand as high as ten times the poisoning budget than LF attacks. In this work,\nwe first introduce an Adversarial Clean Label attack which can adversarially\nperturb in-class training examples for poisoning the training set. We then show\nthat an adversary can significantly bring down the data requirements for a CL\nattack, using the aforementioned approach, to as low as 20% of the data\notherwise required. We then systematically benchmark and analyze a number of\ndefense methods, for both LF and CL attacks, some previously employed solely\nfor LF attacks in the textual domain and others adapted from computer vision.\nWe find that text-specific defenses greatly vary in their effectiveness\ndepending on their properties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ashim Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_A/0/1/0/all/0/1\">Amrith Krishna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adverbs, Surprisingly. (arXiv:2305.19650v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19650","description":"<p>This paper begins with the premise that adverbs are neglected in\ncomputational linguistics. This view derives from two analyses: a literature\nreview and a novel adverb dataset to probe a state-of-the-art language model,\nthereby uncovering systematic gaps in accounts for adverb meaning. We suggest\nthat using Frame Semantics for characterizing word meaning, as in FrameNet,\nprovides a promising approach to adverb analysis, given its ability to describe\nambiguity, semantic roles, and null instantiation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nikolaev_D/0/1/0/all/0/1\">Dmitry Nikolaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baker_C/0/1/0/all/0/1\">Collin F. Baker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petruck_M/0/1/0/all/0/1\">Miriam R.L. Petruck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1\">Sebastian Pad&#xf3;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unveiling Cross Modality Bias in Visual Question Answering: A Causal View with Possible Worlds VQA. (arXiv:2305.19664v1 [cs.CV])","link":"http://arxiv.org/abs/2305.19664","description":"<p>To increase the generalization capability of VQA systems, many recent studies\nhave tried to de-bias spurious language or vision associations that shortcut\nthe question or image to the answer. Despite these efforts, the literature\nfails to address the confounding effect of vision and language simultaneously.\nAs a result, when they reduce bias learned from one modality, they usually\nincrease bias from the other. In this paper, we first model a confounding\neffect that causes language and vision bias simultaneously, then propose a\ncounterfactual inference to remove the influence of this effect. The model\ntrained in this strategy can concurrently and efficiently reduce vision and\nlanguage bias. To the best of our knowledge, this is the first work to reduce\nbiases resulting from confounding effects of vision and language in VQA,\nleveraging causal explain-away relations. We accompany our method with an\nexplain-away strategy, pushing the accuracy of the questions with numerical\nanswers results compared to existing methods that have been an open problem.\nThe proposed method outperforms the state-of-the-art methods in VQA-CP v2\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_A/0/1/0/all/0/1\">Ali Vosoughi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shijian Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yapeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Word Importance Using Models Trained for Semantic Tasks. (arXiv:2305.19689v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19689","description":"<p>Many NLP tasks require to automatically identify the most significant words\nin a text. In this work, we derive word significance from models trained to\nsolve semantic task: Natural Language Inference and Paraphrase Identification.\nUsing an attribution method aimed to explain the predictions of these models,\nwe derive importance scores for each input token. We evaluate their relevance\nusing a so-called cross-task evaluation: Analyzing the performance of one model\non an input masked according to the other model's weight, we show that our\nmethod is robust with respect to the choice of the initial task. Additionally,\nwe investigate the scores from the syntax point of view and observe interesting\npatterns, e.g. words closer to the root of a syntactic tree receive higher\nimportance scores. Altogether, these observations suggest that our method can\nbe used to identify important words in sentences without any explicit word\nimportance labeling in training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Javorsky_D/0/1/0/all/0/1\">D&#xe1;vid Javorsk&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yvon_F/0/1/0/all/0/1\">Fran&#xe7;ois Yvon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Extractive Question Answering System to Support Human-AI Health Coaching Model for Sleep Domain. (arXiv:2305.19707v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19707","description":"<p>Non-communicable diseases (NCDs) are a leading cause of global deaths,\nnecessitating a focus on primary prevention and lifestyle behavior change.\nHealth coaching, coupled with Question Answering (QA) systems, has the\npotential to transform preventive healthcare. This paper presents a\nhuman-Artificial Intelligence (AI) health coaching model incorporating a\ndomain-specific extractive QA system. A sleep-focused dataset, SleepQA, was\nmanually assembled and used to fine-tune domain-specific BERT models. The QA\nsystem was evaluated using automatic and human methods. A data-centric\nframework enhanced the system's performance by improving passage retrieval and\nquestion reformulation. Although the system did not outperform the baseline in\nautomatic evaluation, it excelled in the human evaluation of real-world\nquestions. Integration into a Human-AI health coaching model was tested in a\npilot Randomized Controlled Trial (RCT).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bojic_I/0/1/0/all/0/1\">Iva Bojic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_Q/0/1/0/all/0/1\">Qi Chwen Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Car_J/0/1/0/all/0/1\">Josip Car</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XPhoneBERT: A Pre-trained Multilingual Model for Phoneme Representations for Text-to-Speech. (arXiv:2305.19709v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19709","description":"<p>We present XPhoneBERT, the first multilingual model pre-trained to learn\nphoneme representations for the downstream text-to-speech (TTS) task. Our\nXPhoneBERT has the same model architecture as BERT-base, trained using the\nRoBERTa pre-training approach on 330M phoneme-level sentences from nearly 100\nlanguages and locales. Experimental results show that employing XPhoneBERT as\nan input phoneme encoder significantly boosts the performance of a strong\nneural TTS model in terms of naturalness and prosody and also helps produce\nfairly high-quality speech with limited training data. We publicly release our\npre-trained XPhoneBERT with the hope that it would facilitate future research\nand downstream TTS applications for multiple languages. Our XPhoneBERT model is\navailable at https://github.com/VinAIResearch/XPhoneBERT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Linh The Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1\">Thinh Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat Quoc Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Red Teaming Language Model Detectors with Language Models. (arXiv:2305.19713v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19713","description":"<p>The prevalence and high capacity of large language models (LLMs) present\nsignificant safety and ethical risks when malicious users exploit them for\nautomated content generation. To prevent the potentially deceptive usage of\nLLMs, recent works have proposed several algorithms to detect machine-generated\ntext. In this paper, we systematically test the reliability of the existing\ndetectors, by designing two types of attack strategies to fool the detectors:\n1) replacing words with their synonyms based on the context; 2) altering the\nwriting style of generated text. These strategies are implemented by\ninstructing LLMs to generate synonymous word substitutions or writing\ndirectives that modify the style without human involvement, and the LLMs\nleveraged in the attack can also be protected by detectors. Our research\nreveals that our attacks effectively compromise the performance of all tested\ndetectors, thereby underscoring the urgent need for the development of more\nrobust machine-generated text detection systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhouxing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1\">Fan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangning Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Base Question Answering for Space Debris Queries. (arXiv:2305.19734v1 [cs.AI])","link":"http://arxiv.org/abs/2305.19734","description":"<p>Space agencies execute complex satellite operations that need to be supported\nby the technical knowledge contained in their extensive information systems.\nKnowledge bases (KB) are an effective way of storing and accessing such\ninformation at scale. In this work we present a system, developed for the\nEuropean Space Agency (ESA), that can answer complex natural language queries,\nto support engineers in accessing the information contained in a KB that models\nthe orbital space debris environment. Our system is based on a pipeline which\nfirst generates a sequence of basic database operations, called a %program\nsketch, from a natural language question, then specializes the sketch into a\nconcrete query program with mentions of entities, attributes and relations, and\nfinally executes the program against the database. This pipeline decomposition\napproach enables us to train the system by leveraging out-of-domain data and\nsemi-synthetic data generated by GPT-3, thus reducing overfitting and shortcut\nlearning even with limited amount of in-domain training data. Our code can be\nfound at \\url{https://github.com/PaulDrm/DISCOSQA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Darm_P/0/1/0/all/0/1\">Paul Darm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miceli_Barone_A/0/1/0/all/0/1\">Antonio Valerio Miceli-Barone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Shay B. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riccardi_A/0/1/0/all/0/1\">Annalisa Riccardi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Text Representations by Measuring Task Alignment. (arXiv:2305.19747v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19747","description":"<p>Textual representations based on pre-trained language models are key,\nespecially in few-shot learning scenarios. What makes a representation good for\ntext classification? Is it due to the geometric properties of the space or\nbecause it is well aligned with the task? We hypothesize the second claim. To\ntest it, we develop a task alignment score based on hierarchical clustering\nthat measures alignment at different levels of granularity. Our experiments on\ntext classification validate our hypothesis by showing that task alignment can\nexplain the classification performance of a given representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Gutierrez_C/0/1/0/all/0/1\">Cesar Gonzalez-Gutierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Primadhanty_A/0/1/0/all/0/1\">Audi Primadhanty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cazzaro_F/0/1/0/all/0/1\">Francesco Cazzaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quattoni_A/0/1/0/all/0/1\">Ariadna Quattoni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UKP-SQuARE: An Interactive Tool for Teaching Question Answering. (arXiv:2305.19748v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19748","description":"<p>The exponential growth of question answering (QA) has made it an\nindispensable topic in any Natural Language Processing (NLP) course.\nAdditionally, the breadth of QA derived from this exponential growth makes it\nan ideal scenario for teaching related NLP topics such as information\nretrieval, explainability, and adversarial attacks among others. In this paper,\nwe introduce UKP-SQuARE as a platform for QA education. This platform provides\nan interactive environment where students can run, compare, and analyze various\nQA models from different perspectives, such as general behavior,\nexplainability, and robustness. Therefore, students can get a first-hand\nexperience in different QA techniques during the class. Thanks to this, we\npropose a learner-centered approach for QA education in which students\nproactively learn theoretical concepts and acquire problem-solving skills\nthrough interactive exploration, experimentation, and practical assignments,\nrather than solely relying on traditional lectures. To evaluate the\neffectiveness of UKP-SQuARE in teaching scenarios, we adopted it in a\npostgraduate NLP course and surveyed the students after the course. Their\npositive feedback shows the platform's effectiveness in their course and\ninvites a wider adoption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Haishuo Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puerto_H/0/1/0/all/0/1\">Haritz Puerto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-to-Speech Pipeline for Swiss German -- A comparison. (arXiv:2305.19750v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19750","description":"<p>In this work, we studied the synthesis of Swiss German speech using different\nText-to-Speech (TTS) models. We evaluated the TTS models on three corpora, and\nwe found, that VITS models performed best, hence, using them for further\ntesting. We also introduce a new method to evaluate TTS models by letting the\ndiscriminator of a trained vocoder GAN model predict whether a given waveform\nis human or synthesized. In summary, our best model delivers speech synthesis\nfor different Swiss German dialects with previously unachieved quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bollinger_T/0/1/0/all/0/1\">Tobias Bollinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deriu_J/0/1/0/all/0/1\">Jan Deriu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogel_M/0/1/0/all/0/1\">Manfred Vogel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence Simplification Using Paraphrase Corpus for Initialization. (arXiv:2305.19754v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19754","description":"<p>Neural sentence simplification method based on sequence-to-sequence framework\nhas become the mainstream method for sentence simplification (SS) task.\nUnfortunately, these methods are currently limited by the scarcity of parallel\nSS corpus. In this paper, we focus on how to reduce the dependence on parallel\ncorpus by leveraging a careful initialization for neural SS methods from\nparaphrase corpus. Our work is motivated by the following two findings: (1)\nParaphrase corpus includes a large proportion of sentence pairs belonging to SS\ncorpus. (2) We can construct large-scale pseudo parallel SS data by keeping\nthese sentence pairs with a higher complexity difference. Therefore, we propose\ntwo strategies to initialize neural SS methods using paraphrase corpus. We\ntrain three different neural SS methods with our initialization, which can\nobtain substantial improvements on the available WikiLarge data compared with\nthemselves without initialization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiang_J/0/1/0/all/0/1\">Jipeng Qiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Discrimination of Human and Neural Machine Translation in Multilingual Scenarios. (arXiv:2305.19757v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19757","description":"<p>We tackle the task of automatically discriminating between human and machine\ntranslations. As opposed to most previous work, we perform experiments in a\nmultilingual setting, considering multiple languages and multilingual\npretrained language models. We show that a classifier trained on parallel data\nwith a single source language (in our case German-English) can still perform\nwell on English translations that come from different source languages, even\nwhen the machine translations were produced by other systems than the one it\nwas trained on. Additionally, we demonstrate that incorporating the source text\nin the input of a multilingual classifier improves (i) its accuracy and (ii)\nits robustness on cross-system evaluation, compared to a monolingual\nclassifier. Furthermore, we find that using training data from multiple source\nlanguages (German, Russian, and Chinese) tends to improve the accuracy of both\nmonolingual and multilingual classifiers. Finally, we show that bilingual\nclassifiers and classifiers trained on multiple source languages benefit from\nbeing trained on longer text sequences, rather than on sentences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chichirau_M/0/1/0/all/0/1\">Malina Chichirau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noord_R/0/1/0/all/0/1\">Rik van Noord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toral_A/0/1/0/all/0/1\">Antonio Toral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple yet Effective Code-Switching Language Identification with Multitask Pre-Training and Transfer Learning. (arXiv:2305.19759v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19759","description":"<p>Code-switching, also called code-mixing, is the linguistics phenomenon where\nin casual settings, multilingual speakers mix words from different languages in\none utterance. Due to its spontaneous nature, code-switching is extremely\nlow-resource, which makes it a challenging problem for language and speech\nprocessing tasks. In such contexts, Code-Switching Language Identification\n(CSLID) becomes a difficult but necessary task if we want to maximally leverage\nexisting monolingual tools for other tasks. In this work, we propose two novel\napproaches toward improving language identification accuracy on an\nEnglish-Mandarin child-directed speech dataset. Our methods include a stacked\nResidual CNN+GRU model and a multitask pre-training approach to use Automatic\nSpeech Recognition (ASR) as an auxiliary task for CSLID. Due to the\nlow-resource nature of code-switching, we also employ careful silver data\ncreation using monolingual corpora in both languages and up-sampling as data\naugmentation. We focus on English-Mandarin code-switched data, but our method\nworks on any language pair. Our best model achieves a balanced accuracy of\n0.781 on a real English-Mandarin code-switching child-directed speech corpus\nand outperforms the previous baseline by 55.3%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuyue Stella Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Cihan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianjian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odoom_B/0/1/0/all/0/1\">Bismarck Odoom</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recursive Metropolis-Hastings Naming Game: Symbol Emergence in a Multi-agent System based on Probabilistic Generative Models. (arXiv:2305.19761v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19761","description":"<p>In the studies on symbol emergence and emergent communication in a population\nof agents, a computational model was employed in which agents participate in\nvarious language games. Among these, the Metropolis-Hastings naming game (MHNG)\npossesses a notable mathematical property: symbol emergence through MHNG is\nproven to be a decentralized Bayesian inference of representations shared by\nthe agents. However, the previously proposed MHNG is limited to a two-agent\nscenario. This paper extends MHNG to an N-agent scenario. The main\ncontributions of this paper are twofold: (1) we propose the recursive\nMetropolis-Hastings naming game (RMHNG) as an N-agent version of MHNG and\ndemonstrate that RMHNG is an approximate Bayesian inference method for the\nposterior distribution over a latent variable shared by agents, similar to\nMHNG; and (2) we empirically evaluate the performance of RMHNG on synthetic and\nreal image data, enabling multiple agents to develop and share a symbol system.\nFurthermore, we introduce two types of approximations -- one-sample and\nlimited-length -- to reduce computational complexity while maintaining the\nability to explain communication in a population of agents. The experimental\nfindings showcased the efficacy of RMHNG as a decentralized Bayesian inference\nfor approximating the posterior distribution concerning latent variables, which\nare jointly shared among agents, akin to MHNG. Moreover, the utilization of\nRMHNG elucidated the agents' capacity to exchange symbols. Furthermore, the\nstudy discovered that even the computationally simplified version of RMHNG\ncould enable symbols to emerge among the agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inukai_J/0/1/0/all/0/1\">Jun Inukai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taniguchi_T/0/1/0/all/0/1\">Tadahiro Taniguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taniguchi_A/0/1/0/all/0/1\">Akira Taniguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagiwara_Y/0/1/0/all/0/1\">Yoshinobu Hagiwara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-Based Methods For Audio Question Answering. (arXiv:2305.19769v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19769","description":"<p>Audio question answering (AQA) is the task of producing natural language\nanswers when a system is provided with audio and natural language questions. In\nthis paper, we propose neural network architectures based on self-attention and\ncross-attention for the AQA task. The self-attention layers extract powerful\naudio and textual representations. The cross-attention maps audio features that\nare relevant to the textual features to produce answers. All our models are\ntrained on the recently proposed Clotho-AQA dataset for both binary yes/no\nquestions and single-word answer questions. Our results clearly show\nimprovement over the reference method reported in the original paper. On the\nyes/no binary classification task, our proposed model achieves an accuracy of\n68.3% compared to 62.7% in the reference model. For the single-word answers\nmulticlass classifier, our model produces a top-1 and top-5 accuracy of 57.9%\nand 99.8% compared to 54.2% and 93.7% in the reference model respectively. We\nfurther discuss some of the challenges in the Clotho-AQA dataset such as the\npresence of the same answer word in multiple tenses, singular and plural forms,\nand the presence of specific and generic answers to the same question. We\naddress these issues and present a revised version of the dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sudarsanam_P/0/1/0/all/0/1\">Parthasaarathy Sudarsanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virtanen_T/0/1/0/all/0/1\">Tuomas Virtanen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IDAS: Intent Discovery with Abstractive Summarization. (arXiv:2305.19783v1 [cs.CL])","link":"http://arxiv.org/abs/2305.19783","description":"<p>Intent discovery is the task of inferring latent intents from a set of\nunlabeled utterances, and is a useful step towards the efficient creation of\nnew conversational agents. We show that recent competitive methods in intent\ndiscovery can be outperformed by clustering utterances based on abstractive\nsummaries, i.e., \"labels\", that retain the core elements while removing\nnon-essential information. We contribute the IDAS approach, which collects a\nset of descriptive utterance labels by prompting a Large Language Model,\nstarting from a well-chosen seed set of prototypical utterances, to bootstrap\nan In-Context Learning procedure to generate labels for non-prototypical\nutterances. The utterances and their resulting noisy labels are then encoded by\na frozen pre-trained encoder, and subsequently clustered to recover the latent\nintents. For the unsupervised task (without any intent labels) IDAS outperforms\nthe state-of-the-art by up to +7.42% in standard cluster metrics for the\nBanking, StackOverflow, and Transport datasets. For the semi-supervised task\n(with labels for a subset of intents) IDAS surpasses 2 recent methods on the\nCLINC benchmark without even using labeled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raedt_M/0/1/0/all/0/1\">Maarten De Raedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godin_F/0/1/0/all/0/1\">Fr&#xe9;deric Godin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1\">Thomas Demeester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Develder_C/0/1/0/all/0/1\">Chris Develder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Question Answering Modeling Improvements Hold Across Benchmarks?. (arXiv:2102.01065v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.01065","description":"<p>Do question answering (QA) modeling improvements (e.g., choice of\narchitecture and training procedure) hold consistently across the diverse\nlandscape of QA benchmarks? To study this question, we introduce the notion of\nconcurrence -- two benchmarks have high concurrence on a set of modeling\napproaches if they rank the modeling approaches similarly. We measure the\nconcurrence between 32 QA benchmarks on a set of 20 diverse modeling approaches\nand find that human-constructed benchmarks have high concurrence amongst\nthemselves, even if their passage and question distributions are very\ndifferent. Surprisingly, even downsampled human-constructed benchmarks (i.e.,\ncollecting less data) and programmatically-generated benchmarks (e.g.,\ncloze-formatted examples) have high concurrence with human-constructed\nbenchmarks. These results indicate that, despite years of intense community\nfocus on a small number of benchmarks, the modeling improvements studied hold\nbroadly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nelson F. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tony Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AmbiFC: Fact-Checking Ambiguous Claims with Evidence. (arXiv:2104.00640v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.00640","description":"<p>Automated fact-checking systems in real-world scenarios must compare claims\nwith retrieved evidence to predict the veracity. The retrieved evidence may not\nunambiguously support or refute the claim and yield diverse valid\ninterpretations. Existing fact-checking datasets necessitate that models\npredict a single veracity label for each claim and lack the ability to manage\nsuch ambiguity. We present AmbiFC, a large-scale fact-checking dataset with\nrealistic claims derived from real-world information needs. Our dataset\ncontains fine-grained evidence annotations of passages from complete Wikipedia\npages. We thoroughly analyze disagreements arising from ambiguous claims in\nAmbiFC, observing a strong correlation of annotator disagreement with their\nself-assessment and expert-annotated linguistic phenomena. We introduce the\ntask of evidence-based fact-checking for ambiguous claims with soft labels, and\ncompare three methodologies incorporating annotation signals with a\nsingle-label classification approach. We find that a pipeline with annotation\ndistillation for sentence-level evidence selection and veracity prediction\nyields the best performance. Models trained on ambiguous instances exhibit\nimproved performance dealing with the identified linguistic categories, and\nacquire an understanding of nuanced differences among evidence sentences\nassociated with diverse veracity interpretations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glockner_M/0/1/0/all/0/1\">Max Glockner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staliunaite_I/0/1/0/all/0/1\">Ieva Stali&#x16b;nait&#x117;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1\">James Thorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vallejo_G/0/1/0/all/0/1\">Gisela Vallejo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models. (arXiv:2201.12675v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12675","description":"<p>A central tenet of Federated learning (FL), which trains models without\ncentralizing user data, is privacy. However, previous work has shown that the\ngradient updates used in FL can leak user information. While the most\nindustrial uses of FL are for text applications (e.g. keystroke prediction),\nnearly all attacks on FL privacy have focused on simple image classifiers. We\npropose a novel attack that reveals private user text by deploying malicious\nparameter vectors, and which succeeds even with mini-batches, multiple users,\nand long sequences. Unlike previous attacks on FL, the attack exploits\ncharacteristics of both the Transformer architecture and the token embedding,\nseparately extracting tokens and positional embeddings to retrieve\nhigh-fidelity text. This work suggests that FL on text, which has historically\nbeen resistant to privacy attacks, is far more vulnerable than previously\nthought.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fowl_L/0/1/0/all/0/1\">Liam Fowl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1\">Jonas Geiping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reich_S/0/1/0/all/0/1\">Steven Reich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czaja_W/0/1/0/all/0/1\">Wojtek Czaja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Document Summarization with Centroid-Based Pretraining. (arXiv:2208.01006v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.01006","description":"<p>In Multi-Document Summarization (MDS), the input can be modeled as a set of\ndocuments, and the output is its summary. In this paper, we focus on\npretraining objectives for MDS. Specifically, we introduce a novel pretraining\nobjective, which involves selecting the ROUGE-based centroid of each document\ncluster as a proxy for its summary. Our objective thus does not require human\nwritten summaries and can be utilized for pretraining on a dataset consisting\nsolely of document sets. Through zero-shot, few-shot, and fully supervised\nexperiments on multiple MDS datasets, we show that our model Centrum is better\nor comparable to a state-of-the-art model. We make the pretrained and\nfine-tuned models freely available to the research community\nhttps://github.com/ratishsp/centrum.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Puduppully_R/0/1/0/all/0/1\">Ratish Puduppully</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Parag Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1\">Mark Steedman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ILLUME: Rationalizing Vision-Language Models through Human Interactions. (arXiv:2208.08241v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2208.08241","description":"<p>Bootstrapping from pre-trained language models has been proven to be an\nefficient approach for building vision-language models (VLM) for tasks such as\nimage captioning or visual question answering. However, outputs of these models\nrarely align with user's rationales for specific answers. In order to improve\nthis alignment and reinforce commonsense reasons, we propose a tuning paradigm\nbased on human interactions with machine-generated data. Our ILLUME executes\nthe following loop: Given an image-question-answer prompt, the VLM samples\nmultiple candidate rationales, and a human critic provides feedback via\npreference selection, used for fine-tuning. This loop increases the training\ndata and gradually carves out the VLM's rationalization capabilities that are\naligned with human intent. Our exhaustive experiments demonstrate that ILLUME\nis competitive with standard supervised finetuning while using significantly\nfewer training data and only requiring minimal feedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brack_M/0/1/0/all/0/1\">Manuel Brack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1\">Patrick Schramowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deiseroth_B/0/1/0/all/0/1\">Bj&#xf6;rn Deiseroth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Open-Domain Question Answering. (arXiv:2210.06345v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06345","description":"<p>Retrieval-augmented models have proven to be effective in natural language\nprocessing tasks, yet there remains a lack of research on their optimization\nusing variational inference. We introduce the Variational Open-Domain (VOD)\nframework for end-to-end training and evaluation of retrieval-augmented models,\nfocusing on open-domain question answering and language modelling. The VOD\nobjective, a self-normalized estimate of the R\\'enyi variational bound,\napproximates the task marginal likelihood and is evaluated under samples drawn\nfrom an auxiliary sampling distribution (cached retriever and/or approximate\nposterior). It remains tractable, even for retriever distributions defined on\nlarge corpora. We demonstrate VOD's versatility by training reader-retriever\nBERT-sized models on multiple-choice medical exam questions. On the MedMCQA\ndataset, we outperform the domain-tuned Med-PaLM by +5.3% despite using\n2.500$\\times$ fewer parameters. Our retrieval-augmented BioLinkBERT model\nscored 62.9% on the MedMCQA and 55.0% on the MedQA-USMLE. Last, we show the\neffectiveness of our learned retriever component in the context of medical\nsemantic search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lievin_V/0/1/0/all/0/1\">Valentin Li&#xe9;vin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motzfeldt_A/0/1/0/all/0/1\">Andreas Geert Motzfeldt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jensen_I/0/1/0/all/0/1\">Ida Riis Jensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1\">Ole Winther</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Sample-Efficient NLP Models More Robust?. (arXiv:2210.06456v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06456","description":"<p>Recent results in image classification and extractive question answering have\nobserved that pre-trained models trained on less in-distribution data have\nbetter out-of-distribution performance. However, it is unclear how broadly\nthese trends hold. We conduct a large empirical study across three tasks, three\nbroadly-applicable modeling interventions (increasing model size, using a\ndifferent adaptation method, and pre-training on more data), and 14 diverse\ndatasets to investigate the relationship between sample efficiency (amount of\ndata needed to reach a given ID accuracy) and robustness (how models fare on\nOOD evaluation). We find that higher sample efficiency is only correlated with\nbetter average OOD robustness on some modeling interventions and tasks, but not\nothers. On individual datasets, models with lower sample efficiency can even be\nmore robust. These results suggest that general-purpose methods for improving\nsample efficiency are unlikely to yield universal OOD robustness improvements,\nsince such improvements are highly dataset- and task-dependent. Even in an era\nof large, multi-purpose pretrained models, task-specific decisions may often be\nnecessary for OOD generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nelson F. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ananya Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mention Annotations Alone Enable Efficient Domain Adaptation for Coreference Resolution. (arXiv:2210.07602v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07602","description":"<p>Although recent neural models for coreference resolution have led to\nsubstantial improvements on benchmark datasets, transferring these models to\nnew target domains containing out-of-vocabulary spans and requiring differing\nannotation schemes remains challenging. Typical approaches involve continued\ntraining on annotated target-domain data, but obtaining annotations is costly\nand time-consuming. We show that annotating mentions alone is nearly twice as\nfast as annotating full coreference chains. Accordingly, we propose a method\nfor efficiently adapting coreference models, which includes a high-precision\nmention detection objective and requires annotating only mentions in the target\ndomain. Extensive evaluation across three English coreference datasets:\nCoNLL-2012 (news/conversation), i2b2/VA (medical notes), and previously\nunstudied child welfare notes, reveals that our approach facilitates\nannotation-efficient transfer and results in a 7-14% improvement in average F1\nwithout increasing annotator time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_N/0/1/0/all/0/1\">Nupoor Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Field_A/0/1/0/all/0/1\">Anjalie Field</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1\">Emma Strubell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RARR: Researching and Revising What Language Models Say, Using Language Models. (arXiv:2210.08726v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08726","description":"<p>Language models (LMs) now excel at many tasks such as few-shot learning,\nquestion answering, reasoning, and dialog. However, they sometimes generate\nunsupported or misleading content. A user cannot easily determine whether their\noutputs are trustworthy or not, because most LMs do not have any built-in\nmechanism for attribution to external evidence. To enable attribution while\nstill preserving all the powerful advantages of recent generation models, we\npropose RARR (Retrofit Attribution using Research and Revision), a system that\n1) automatically finds attribution for the output of any text generation model\nand 2) post-edits the output to fix unsupported content while preserving the\noriginal output as much as possible. When applied to the output of several\nstate-of-the-art LMs on a diverse set of generation tasks, we find that RARR\nsignificantly improves attribution while otherwise preserving the original\ninput to a much greater degree than previously explored edit models.\nFurthermore, the implementation of RARR requires only a handful of training\nexamples, a large language model, and standard web search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Luyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zhuyun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasupat_P/0/1/0/all/0/1\">Panupong Pasupat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anthony Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaganty_A/0/1/0/all/0/1\">Arun Tejasvi Chaganty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yicheng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_V/0/1/0/all/0/1\">Vincent Y. Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lao_N/0/1/0/all/0/1\">Ni Lao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hongrae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juan_D/0/1/0/all/0/1\">Da-Cheng Juan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guu_K/0/1/0/all/0/1\">Kelvin Guu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating context-invariance in unsupervised speech representations. (arXiv:2210.15775v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15775","description":"<p>Unsupervised speech representations have taken off, with benchmarks (SUPERB,\nZeroSpeech) demonstrating major progress on semi-supervised speech recognition,\nspeech synthesis, and speech-only language modelling. Inspiration comes from\nthe promise of ``discovering the phonemes'' of a language or a similar\nlow-bitrate encoding. However, one of the critical properties of phoneme\ntranscriptions is context-invariance: the phonetic context of a speech sound\ncan have massive influence on the way it is pronounced, while the text remains\nstable. This is what allows tokens of the same word to have the same\ntranscriptions -- key to language understanding. Current benchmarks do not\nmeasure context-invariance. We develop a new version of the ZeroSpeech ABX\nbenchmark that measures context-invariance, and apply it to recent\nself-supervised representations. We demonstrate that the context-independence\nof representations is predictive of the stability of word-level\nrepresentations. We suggest research concentrate on improving\ncontext-independence of self-supervised and unsupervised representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hallap_M/0/1/0/all/0/1\">Mark Hallap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunbar_E/0/1/0/all/0/1\">Ewan Dunbar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets. (arXiv:2211.07321v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07321","description":"<p>In this paper, we provide a new perspective on self-supervised speech models\nfrom how the training targets are obtained. We generalize the targets extractor\ninto Offline Targets Extractor (Off-TE) and Online Targets Extractor (On-TE).\nBased on this, we propose a new multi-tasking learning framework for\nself-supervised learning, MT4SSL, which stands for Boosting Self-Supervised\nSpeech Representation Learning by Integrating Multiple Targets. MT4SSL uses the\nK-means algorithm as an Off-TE and a teacher network without gradients as an\nOn-TE, respectively. Our model outperforms previous SSL methods by nontrivial\nmargins on the LibriSpeech benchmark, and is comparable to or even better than\nthe best-performing models with fewer data. Furthermore, we find that using\nboth Off-TE and On-TE results in better convergence in the pre-training phase.\nWith both effectiveness and efficiency, we think doing multi-task learning on\nself-supervised speech models from our perspective is a promising trend.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Ziyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhisheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Changli Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xie Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers learn in-context by gradient descent. (arXiv:2212.07677v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2212.07677","description":"<p>At present, the mechanisms of in-context learning in Transformers are not\nwell understood and remain mostly an intuition. In this paper, we suggest that\ntraining Transformers on auto-regressive objectives is closely related to\ngradient-based meta-learning formulations. We start by providing a simple\nweight construction that shows the equivalence of data transformations induced\nby 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a\nregression loss. Motivated by that construction, we show empirically that when\ntraining self-attention-only Transformers on simple regression tasks either the\nmodels learned by GD and Transformers show great similarity or, remarkably, the\nweights found by optimization match the construction. Thus we show how trained\nTransformers become mesa-optimizers i.e. learn models by gradient descent in\ntheir forward pass. This allows us, at least in the domain of regression\nproblems, to mechanistically understand the inner workings of in-context\nlearning in optimized Transformers. Building on this insight, we furthermore\nidentify how Transformers surpass the performance of plain gradient descent by\nlearning an iterative curvature correction and learn linear models on deep data\nrepresentations to solve non-linear regression tasks. Finally, we discuss\nintriguing parallels to a mechanism identified to be crucial for in-context\nlearning termed induction-head (Olsson et al., 2022) and show how it could be\nunderstood as a specific case of in-context learning by gradient descent\nlearning within Transformers. Code to reproduce the experiments can be found at\nhttps://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oswald_J/0/1/0/all/0/1\">Johannes von Oswald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niklasson_E/0/1/0/all/0/1\">Eyvind Niklasson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Randazzo_E/0/1/0/all/0/1\">Ettore Randazzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sacramento_J/0/1/0/all/0/1\">Jo&#xe3;o Sacramento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordvintsev_A/0/1/0/all/0/1\">Alexander Mordvintsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhmoginov_A/0/1/0/all/0/1\">Andrey Zhmoginov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vladymyrov_M/0/1/0/all/0/1\">Max Vladymyrov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DuNST: Dual Noisy Self Training for Semi-Supervised Controllable Text Generation. (arXiv:2212.08724v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08724","description":"<p>Self-training (ST) has prospered again in language understanding by\naugmenting the fine-tuning of pre-trained language models when labeled data is\ninsufficient. However, it remains challenging to incorporate ST into\nattribute-controllable language generation. Augmented by only self-generated\npseudo text, generation models over-emphasize exploitation of the previously\nlearned space, suffering from a constrained generalization boundary. We revisit\nST and propose a novel method, DuNST to alleviate this problem. DuNST jointly\nmodels text generation and classification with a shared Variational AutoEncoder\nand corrupts the generated pseudo text by two kinds of flexible noise to\ndisturb the space. In this way, our model could construct and utilize both\npseudo text from given labels and pseudo labels from available unlabeled text,\nwhich are gradually refined during the ST process. We theoretically demonstrate\nthat DuNST can be regarded as enhancing exploration towards the potential real\ntext space, providing a guarantee of improved performance. Experiments on three\ncontrollable generation tasks show that DuNST could significantly boost control\naccuracy while maintaining comparable generation fluency and diversity against\nseveral strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yuxi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1\">Xiaoyuan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshmanan_L/0/1/0/all/0/1\">Laks V.S. Lakshmanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages. (arXiv:2212.09651v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09651","description":"<p>Multilingual Pretrained Language Models (MPLMs) have shown their strong\nmultilinguality in recent empirical cross-lingual transfer studies. In this\npaper, we propose the Prompts Augmented by Retrieval Crosslingually (PARC)\npipeline to improve the zero-shot performance on low-resource languages (LRLs)\nby augmenting the context with semantically similar sentences retrieved from a\nhigh-resource language (HRL) as prompts. PARC improves the zero-shot\nperformance on three downstream tasks (binary sentiment classification, topic\ncategorization and natural language inference) with multilingual parallel test\nsets across 10 LRLs covering 6 language families in both unlabeled settings\n(+5.1%) and labeled settings (+16.3%). PARC-labeled also outperforms the\nfinetuning baseline by 3.7%. We find a significant positive correlation between\ncross-lingual transfer performance on one side, and the similarity between the\nhigh- and low-resource languages as well as the amount of low-resource\npretraining data on the other side. A robustness analysis suggests that PARC\nhas the potential to achieve even stronger performance with more powerful\nMPLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_E/0/1/0/all/0/1\">Ercong Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Sheng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_H/0/1/0/all/0/1\">Helmut Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Pre-Training Tasks for Neural Machine Translation. (arXiv:2212.09864v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09864","description":"<p>Pre-training models with large crawled corpora can lead to issues such as\ntoxicity and bias, as well as copyright and privacy concerns. A promising way\nof alleviating such concerns is to conduct pre-training with synthetic tasks\nand data, since no real-world information is ingested by the model. Our goal in\nthis paper is to understand the factors that contribute to the effectiveness of\npre-training models when using synthetic resources, particularly in the context\nof neural machine translation. We propose several novel approaches to\npre-training translation models that involve different levels of lexical and\nstructural knowledge, including: 1) generating obfuscated data from a large\nparallel corpus 2) concatenating phrase pairs extracted from a small\nword-aligned corpus, and 3) generating synthetic parallel data without real\nhuman language corpora. Our experiments on multiple language pairs reveal that\npre-training benefits can be realized even with high levels of obfuscation or\npurely synthetic parallel data. We hope the findings from our comprehensive\nempirical analysis will shed light on understanding what matters for NMT\npre-training, as well as pave the way for the development of more efficient and\nless toxic models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zexue He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blackwood_G/0/1/0/all/0/1\">Graeme Blackwood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I Cast Detect Thoughts: Learning to Converse and Guide with Intents and Theory-of-Mind in Dungeons and Dragons. (arXiv:2212.10060v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10060","description":"<p>We propose a novel task, G4C, to study teacher-student natural language\ninteractions in a goal-driven and grounded environment. Dungeons and Dragons\n(D&amp;D), a role-playing game, provides an ideal setting to investigate such\ninteractions. Here, the Dungeon Master (DM), i.e., the teacher, guides the\nactions of several players -- students, each with their own personas and\nabilities -- to achieve shared goals grounded in a fantasy world. Our approach\nis to decompose and model these interactions into (1) the DM's intent to guide\nplayers toward a given goal; (2) the DM's guidance utterance to the players\nexpressing this intent; and (3) a theory-of-mind (ToM) model that anticipates\nthe players' reaction to the guidance one turn into the future. We develop a\nnovel reinforcement learning (RL) method for training a DM that generates\nguidance for players by rewarding utterances where the intent matches the\nToM-anticipated player actions. Human and automated evaluations show that a DM\ntrained to explicitly model intents and incorporate ToM of the players using RL\ngenerates better-quality guidance that is 3x more likely to fulfill the DM's\nintent than a vanilla natural language generation (NLG) approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_A/0/1/0/all/0/1\">Andrew Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jennifer Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClarifyDelphi: Reinforced Clarification Questions with Defeasibility Rewards for Social and Moral Situations. (arXiv:2212.10409v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10409","description":"<p>Context is everything, even in commonsense moral reasoning. Changing contexts\ncan flip the moral judgment of an action; \"Lying to a friend\" is wrong in\ngeneral, but may be morally acceptable if it is intended to protect their life.\n</p>\n<p>We present ClarifyDelphi, an interactive system that learns to ask\nclarification questions (e.g., why did you lie to your friend?) in order to\nelicit additional salient contexts of a social or moral situation. We posit\nthat questions whose potential answers lead to diverging moral judgments are\nthe most informative. Thus, we propose a reinforcement learning framework with\na defeasibility reward that aims to maximize the divergence between moral\njudgments of hypothetical answers to a question. Human evaluation demonstrates\nthat our system generates more relevant, informative and defeasible questions\ncompared to competitive baselines. Our work is ultimately inspired by studies\nin cognitive science that have investigated the flexibility in moral cognition\n(i.e., the diverse contexts in which moral rules can be bent), and we hope that\nresearch in this direction can assist both cognitive and computational\ninvestigations of moral judgments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1\">Valentina Pyatkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generic Temporal Reasoning with Differential Analysis and Explanation. (arXiv:2212.10467v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10467","description":"<p>Temporal reasoning is the task of predicting temporal relations of event\npairs. While temporal reasoning models can perform reasonably well on in-domain\nbenchmarks, we have little idea of these systems' generalizability due to\nexisting datasets' limitations. In this work, we introduce a novel task named\nTODAY that bridges this gap with temporal differential analysis, which as the\nname suggests, evaluates whether systems can correctly understand the effect of\nincremental changes. Specifically, TODAY introduces slight contextual changes\nfor given event pairs, and systems are asked to tell how this subtle contextual\nchange would affect relevant temporal relation distributions. To facilitate\nlearning, TODAY also annotates human explanations. We show that existing\nmodels, including GPT-3.5, drop to random guessing on TODAY, suggesting that\nthey heavily rely on spurious information rather than proper reasoning for\ntemporal predictions. On the other hand, we show that TODAY's supervision style\nand explanation annotations can be used in joint learning, encouraging models\nto use more appropriate signals during training and thus outperform across\nseveral benchmarks. TODAY can also be used to train models to solicit\nincidental supervision from noisy sources such as GPT-3.5, thus moving us more\ntoward the goal of generic temporal reasoning systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Ben Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Helen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Contrastive Finetuning Improves Low-Resource Relation Extraction. (arXiv:2212.10823v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10823","description":"<p>Relation extraction (RE), which has relied on structurally annotated corpora\nfor model training, has been particularly challenging in low-resource scenarios\nand domains. Recent literature has tackled low-resource RE by self-supervised\nlearning, where the solution involves pretraining the entity pair embedding by\nRE-based objective and finetuning on labeled data by classification-based\nobjective. However, a critical challenge to this approach is the gap in\nobjectives, which prevents the RE model from fully utilizing the knowledge in\npretrained representations. In this paper, we aim at bridging the gap and\npropose to pretrain and finetune the RE model using consistent objectives of\ncontrastive learning. Since in this kind of representation learning paradigm,\none relation may easily form multiple clusters in the representation space, we\nfurther propose a multi-center contrastive loss that allows one relation to\nform multiple clusters to better align with pretraining. Experiments on two\ndocument-level RE datasets, BioRED and Re-DocRED, demonstrate the effectiveness\nof our method. Particularly, when using 1% end-task training data, our method\noutperforms PLM-based RE classifier by 10.5% and 6.1% on the two datasets,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matching Exemplar as Next Sentence Prediction (MeNSP): Zero-shot Prompt Learning for Automatic Scoring in Science Education. (arXiv:2301.08771v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.08771","description":"<p>Developing models to automatically score students' written responses to\nscience problems is critical for science education. However, collecting and\nlabeling sufficient student responses for training models is time and\ncost-consuming. Recent studies suggest that pre-trained language models can be\nadapted to downstream tasks without fine-tuning with prompts. However, no\nresearch has employed such a prompt approach in science education. As student\nresponses are presented with natural language, aligning the scoring procedure\nas the next sentence prediction task using prompts can skip the costly\nfine-tuning stage. In this study, we developed a zero-shot approach to\nautomatically score student responses via Matching Exemplars as Next Sentence\nPrediction (MeNSP). This approach employs no training samples. We first apply\nMeNSP in scoring three assessment tasks of scientific argumentation and found\nmachine-human scoring agreements, Cohen's Kappa ranges from 0.30 to 0.57, and\nF1 score ranges from 0.54 to 0.81. To improve the performance, we extend our\nresearch to the few-shots setting, either randomly selecting labeled student\nresponses or manually constructing responses to fine-tune the models. We find\nthat one task's performance is improved with more samples, Cohen's Kappa from\n0.30 to 0.38, and F1 score from 0.54 to 0.59; for the two others, scoring\nperformance is not improved. We also find that randomly selected few-shots\nperform better than the human expert-crafted approach. This study suggests that\nMeNSP can yield referable automatic scoring for student responses while\nsignificantly reducing the cost of model training. This method can benefit\nlow-stakes classroom assessment practices in science education. Future research\nshould further explore the applicability of the MeNSP in different types of\nassessment tasks in science education and improve the model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xuansheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xinyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ninghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaoming Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases. (arXiv:2301.12017v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.12017","description":"<p>Improving the deployment efficiency of transformer-based language models has\nbeen challenging given their high computation and memory cost. While INT8\nquantization has recently been shown to be effective in reducing both the\nmemory cost and latency while preserving model accuracy, it remains unclear\nwhether we can leverage INT4 (which doubles peak hardware throughput) to\nachieve further latency improvement. In this study, we explore the feasibility\nof employing INT4 weight and activation (W4A4) quantization for language\nmodels. Our findings indicate that W4A4 quantization introduces no to\nnegligible accuracy degradation for encoder-only and encoder-decoder models,\nbut causes a significant accuracy drop for decoder-only models. To materialize\nthe performance gain using W4A4, we develop a highly optimized end-to-end W4A4\nencoder inference pipeline supporting different quantization strategies. Our\nINT4 pipeline is $8.5\\times$ faster for latency-oriented scenarios and up to\n$3\\times$ for throughput-oriented scenarios compared to the inference of FP16,\nand improves the SOTA BERT INT8 performance from FasterTransformer by up to\n$1.7\\times$. We provide insights into the failure cases when applying W4A4 to\ndecoder-only models, and further explore the compatibility of INT4 quantization\nwith other compression methods, like pruning and layer reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaoxia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aminabadi_R/0/1/0/all/0/1\">Reza Yazdani Aminabadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxiong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers. (arXiv:2301.13741v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2301.13741","description":"<p>Real-world data contains a vast amount of multimodal information, among which\nvision and language are the two most representative modalities. Moreover,\nincreasingly heavier models, \\textit{e}.\\textit{g}., Transformers, have\nattracted the attention of researchers to model compression. However, how to\ncompress multimodal models, especially vison-language Transformers, is still\nunder-explored. This paper proposes the \\textbf{U}nified and\n\\textbf{P}r\\textbf{o}gressive \\textbf{P}runing (\\textbf{\\emph{UPop}}) as a\nuniversal vison-language Transformer compression framework, which incorporates\n1) unifiedly searching multimodal subnets in a continuous optimization space\nfrom the original model, which enables automatic assignment of pruning ratios\namong compressible modalities and structures; 2) progressively searching and\nretraining the subnet, which maintains convergence between the search and\nretrain to attain higher compression ratios. Experiments on various tasks,\ndatasets, and model architectures demonstrate the effectiveness and versatility\nof the proposed UPop framework. The code is available at\nhttps://github.com/sdc17/UPop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Dachuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chaofan Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Ying Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhendong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models. (arXiv:2301.13826v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2301.13826","description":"<p>Recent text-to-image generative models have demonstrated an unparalleled\nability to generate diverse and creative imagery guided by a target text\nprompt. While revolutionary, current state-of-the-art diffusion models may\nstill fail in generating images that fully convey the semantics in the given\ntext prompt. We analyze the publicly available Stable Diffusion model and\nassess the existence of catastrophic neglect, where the model fails to generate\none or more of the subjects from the input prompt. Moreover, we find that in\nsome cases the model also fails to correctly bind attributes (e.g., colors) to\ntheir corresponding subjects. To help mitigate these failure cases, we\nintroduce the concept of Generative Semantic Nursing (GSN), where we seek to\nintervene in the generative process on the fly during inference time to improve\nthe faithfulness of the generated images. Using an attention-based formulation\nof GSN, dubbed Attend-and-Excite, we guide the model to refine the\ncross-attention units to attend to all subject tokens in the text prompt and\nstrengthen - or excite - their activations, encouraging the model to generate\nall subjects described in the text prompt. We compare our approach to\nalternative approaches and demonstrate that it conveys the desired concepts\nmore faithfully across a range of text prompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chefer_H/0/1/0/all/0/1\">Hila Chefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alaluf_Y/0/1/0/all/0/1\">Yuval Alaluf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinker_Y/0/1/0/all/0/1\">Yael Vinker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query-Utterance Attention with Joint modeling for Query-Focused Meeting Summarization. (arXiv:2303.04487v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.04487","description":"<p>Query-focused meeting summarization (QFMS) aims to generate summaries from\nmeeting transcripts in response to a given query. Previous works typically\nconcatenate the query with meeting transcripts and implicitly model the query\nrelevance only at the token level with attention mechanism. However, due to the\ndilution of key query-relevant information caused by long meeting transcripts,\nthe original transformer-based model is insufficient to highlight the key parts\nrelated to the query. In this paper, we propose a query-aware framework with\njoint modeling token and utterance based on Query-Utterance Attention. It\ncalculates the utterance-level relevance to the query with a dense retrieval\nmodule. Then both token-level query relevance and utterance-level query\nrelevance are combined and incorporated into the generation process with\nattention mechanism explicitly. We show that the query relevance of different\ngranularities contributes to generating a summary more related to the query.\nExperimental results on the QMSum dataset show that the proposed model achieves\nnew state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingxian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_B/0/1/0/all/0/1\">Bin Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bo Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yajing Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Partial Knowledge Base Inference in Biomedical Entity Linking. (arXiv:2303.10330v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.10330","description":"<p>Biomedical entity linking (EL) consists of named entity recognition (NER) and\nnamed entity disambiguation (NED). EL models are trained on corpora labeled by\na predefined KB. However, it is a common scenario that only entities within a\nsubset of the KB are precious to stakeholders. We name this scenario partial\nknowledge base inference: training an EL model with one KB and inferring on the\npart of it without further training. In this work, we give a detailed\ndefinition and evaluation procedures for this practically valuable but\nsignificantly understudied scenario and evaluate methods from three\nrepresentative EL paradigms. We construct partial KB inference benchmarks and\nwitness a catastrophic degradation in EL performance due to dramatically\nprecision drop. Our findings reveal these EL paradigms can not correctly handle\nunlinkable mentions (NIL), so they are not robust to partial KB inference. We\nalso propose two simple-and-effective redemption methods to combat the NIL\nissue with little computational overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongyi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Keming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"cTBLS: Augmenting Large Language Models with Conversational Tables. (arXiv:2303.12024v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.12024","description":"<p>Optimizing accuracy and performance while eliminating hallucinations of\nopen-domain conversational large language models (LLMs) is an open research\nchallenge. A particularly promising direction is to augment and ground LLMs\nwith information from structured sources. This paper introduces Conversational\nTables (cTBLS), a three-step architecture to retrieve and generate dialogue\nresponses grounded on retrieved tabular information. cTBLS uses Transformer\nencoder embeddings for Dense Table Retrieval and obtains up to 125% relative\nimprovement over the retriever in the previous state-of-the-art system on the\nHyrbiDialogue dataset. cTBLS then uses a shared process between encoder and\ndecoder models to perform a coarse+fine tabular knowledge (e.g., cell) ranking\ncombined with a GPT-3.5 LLM response generator to yield a 2x relative\nimprovement in ROUGE scores. Finally, human evaluators prefer cTBLs +80% of the\ntime (coherency, fluency) and judge informativeness to be 4x better than the\nprevious state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sundar_A/0/1/0/all/0/1\">Anirudh S Sundar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heck_L/0/1/0/all/0/1\">Larry Heck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aligning a medium-size GPT model in English to a small closed domain in Spanish. (arXiv:2303.17649v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.17649","description":"<p>In this paper, we propose a methodology to align a medium-sized GPT model,\noriginally trained in English for an open domain, to a small closed domain in\nSpanish. The application for which the model is finely tuned is the question\nanswering task. To achieve this we also needed to train and implement another\nneural network (which we called the reward model) that could score and\ndetermine whether an answer is appropriate for a given question. This component\nserved to improve the decoding and generation of the answers of the system.\nNumerical metrics such as BLEU and perplexity were used to evaluate the model,\nand human judgment was also used to compare the decoding technique with others.\nFinally, the results favored the proposed method, and it was determined that it\nis feasible to use a reward model to align the generation of responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Navarrete_Parra_O/0/1/0/all/0/1\">Oscar R. Navarrete-Parra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uc_Cetina_V/0/1/0/all/0/1\">Victor Uc-Cetina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_Magana_J/0/1/0/all/0/1\">Jorge Reyes-Magana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling. (arXiv:2304.01373v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01373","description":"<p>How do large language models (LLMs) develop and evolve over the course of\ntraining? How do these patterns change as models scale? To answer these\nquestions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on\npublic data seen in the exact same order and ranging in size from 70M to 12B\nparameters. We provide public access to 154 checkpoints for each one of the 16\nmodels, alongside tools to download and reconstruct their exact training\ndataloaders for further study. We intend \\textit{Pythia} to facilitate research\nin many areas, and we present several case studies including novel results in\nmemorization, term frequency effects on few-shot performance, and reducing\ngender bias. We demonstrate that this highly controlled setup can be used to\nyield novel insights toward LLMs and their training dynamics. Trained models,\nanalysis code, training code, and training data can be found at\n\\url{https://github.com/EleutherAI/pythia}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoelkopf_H/0/1/0/all/0/1\">Hailey Schoelkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anthony_Q/0/1/0/all/0/1\">Quentin Anthony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradley_H/0/1/0/all/0/1\">Herbie Bradley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OBrien_K/0/1/0/all/0/1\">Kyle O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hallahan_E/0/1/0/all/0/1\">Eric Hallahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Aflah Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purohit_S/0/1/0/all/0/1\">Shivanshu Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prashanth_U/0/1/0/all/0/1\">USVSN Sai Prashanth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1\">Edward Raff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skowron_A/0/1/0/all/0/1\">Aviya Skowron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutawika_L/0/1/0/all/0/1\">Lintang Sutawika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wal_O/0/1/0/all/0/1\">Oskar van der Wal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Optimized Adapters for an End-to-End Task-Oriented Dialogue System. (arXiv:2305.02468v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.02468","description":"<p>Task-Oriented Dialogue (TOD) systems are designed to carry out specific tasks\nby tracking dialogue states and generating appropriate responses to help users\nachieve defined goals. Recently, end-to-end dialogue models pre-trained based\non large datasets have shown promising performance in the conversational\nsystem. However, they share the same parameters to train tasks of the dialogue\nsystem (NLU, DST, NLG), so debugging each task is challenging. Also, they\nrequire a lot of effort to fine-tune large parameters to create a task-oriented\nchatbot, making it difficult for non-experts to handle. Therefore, we intend to\ntrain relatively lightweight and fast models compared to PLM. In this paper, we\npropose an End-to-end TOD system with Task-Optimized Adapters which learn\nindependently per task, adding only small number of parameters after fixed\nlayers of pre-trained network. We also enhance the performance of the DST and\nNLG modules through reinforcement learning, overcoming the learning curve that\nhas lacked at the adapter learning and enabling the natural and consistent\nresponse generation that is appropriate for the goal. Our method is a\nmodel-agnostic approach and does not require prompt-tuning as only input data\nwithout a prompt. As results of the experiment, our method shows competitive\nperformance on the MultiWOZ benchmark compared to the existing end-to-end\nmodels. In particular, we attain state-of-the-art performance on the DST task\nof 2.2 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bang_N/0/1/0/all/0/1\">Namo Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jeehyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_M/0/1/0/all/0/1\">Myoung-Wan Koo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accuracy on the Curve: On the Nonlinear Correlation of ML Performance Between Data Subpopulations. (arXiv:2305.02995v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.02995","description":"<p>Understanding the performance of machine learning (ML) models across diverse\ndata distributions is critically important for reliable applications. Despite\nrecent empirical studies positing a near-perfect linear correlation between\nin-distribution (ID) and out-of-distribution (OOD) accuracies, we empirically\ndemonstrate that this correlation is more nuanced under subpopulation shifts.\nThrough rigorous experimentation and analysis across a variety of datasets,\nmodels, and training epochs, we demonstrate that OOD performance often has a\nnonlinear correlation with ID performance in subpopulation shifts. Our\nfindings, which contrast previous studies that have posited a linear\ncorrelation in model performance during distribution shifts, reveal a \"moon\nshape\" correlation (parabolic uptrend curve) between the test performance on\nthe majority subpopulation and the minority subpopulation. This non-trivial\nnonlinear correlation holds across model architectures, hyperparameters,\ntraining durations, and the imbalance between subpopulations. Furthermore, we\nfound that the nonlinearity of this \"moon shape\" is causally influenced by the\ndegree of spurious correlations in the training data. Our controlled\nexperiments show that stronger spurious correlation in the training data\ncreates more nonlinear performance correlation. We provide complementary\nexperimental and theoretical analyses for this phenomenon, and discuss its\nimplications for ML reliability and fairness. Our work highlights the\nimportance of understanding the nonlinear effects of model improvement on\nperformance in different subpopulations, and has the potential to inform the\ndevelopment of more equitable and responsible machine learning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1\">Weixin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yining Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Yongchan Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Role of Global and Local Context in Named Entity Recognition. (arXiv:2305.03132v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03132","description":"<p>Pre-trained transformer-based models have recently shown great performance\nwhen applied to Named Entity Recognition (NER). As the complexity of their\nself-attention mechanism prevents them from processing long documents at once,\nthese models are usually applied in a sequential fashion. Such an approach\nunfortunately only incorporates local context and prevents leveraging global\ndocument context in long documents such as novels, which might hinder\nperformance. In this article, we explore the impact of global document context,\nand its relationships with local context. We find that correctly retrieving\nglobal document context has a greater impact on performance than only\nleveraging local context, prompting for further research on how to better\nretrieve that context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amalvy_A/0/1/0/all/0/1\">Arthur Amalvy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labatut_V/0/1/0/all/0/1\">Vincent Labatut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufour_R/0/1/0/all/0/1\">Richard Dufour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Hidden Mystery of OCR in Large Multimodal Models. (arXiv:2305.07895v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.07895","description":"<p>Large models have recently played a dominant role in natural language\nprocessing and multimodal vision-language learning. It remains less explored\nabout their efficacy in text-related visual tasks. We conducted a comprehensive\nstudy of existing publicly available multimodal models, evaluating their\nperformance in text recognition (document text, artistic text, handwritten\ntext, scene text), text-based visual question answering (document text, scene\ntext, and bilingual text), key information extraction (receipts, documents, and\nnutrition facts) and handwritten mathematical expression recognition. Our\nfindings reveal strengths and weaknesses in these models, which primarily rely\non semantic understanding for word recognition and exhibit inferior perception\nof individual character shapes. They also display indifference towards text\nlength and have limited capabilities in detecting fine-grained features in\nimages. Consequently, these results demonstrate that even the current most\npowerful large multimodal models cannot match domain-specific methods in\ntraditional text tasks and face greater challenges in more complex tasks. Most\nimportantly, the baseline results showcased in this study could provide a\nfoundational framework for the conception and assessment of innovative\nstrategies targeted at enhancing zero-shot multimodal techniques. Evaluation\npipeline will be available at https://github.com/Yuliang-Liu/MultimodalOCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenwen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Mingxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Dezhi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingrui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations. (arXiv:2305.11694v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11694","description":"<p>Formulating selective information needs results in queries that implicitly\nspecify set operations, such as intersection, union, and difference. For\ninstance, one might search for \"shorebirds that are not sandpipers\" or\n\"science-fiction films shot in England\". To study the ability of retrieval\nsystems to meet such information needs, we construct QUEST, a dataset of 3357\nnatural language queries with implicit set operations, that map to a set of\nentities corresponding to Wikipedia documents. The dataset challenges models to\nmatch multiple constraints mentioned in queries with corresponding evidence in\ndocuments and correctly perform various set operations. The dataset is\nconstructed semi-automatically using Wikipedia category names. Queries are\nautomatically composed from individual categories, then paraphrased and further\nvalidated for naturalness and fluency by crowdworkers. Crowdworkers also assess\nthe relevance of entities based on their documents and highlight attribution of\nquery constraints to spans of document text. We analyze several modern\nretrieval systems, finding that they often struggle on such queries. Queries\ninvolving negation and conjunction are particularly challenging and systems are\nfurther challenged with combinations of these operations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malaviya_C/0/1/0/all/0/1\">Chaitanya Malaviya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_P/0/1/0/all/0/1\">Peter Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kenton Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1\">Kristina Toutanova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparison of Multilingual Self-Supervised and Weakly-Supervised Speech Pre-Training for Adaptation to Unseen Languages. (arXiv:2305.12606v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12606","description":"<p>Recent models such as XLS-R and Whisper have made multilingual speech\ntechnologies more accessible by pre-training on audio from around 100 spoken\nlanguages each. However, there are thousands of spoken languages worldwide, and\nadapting to new languages is an important problem. In this work, we aim to\nunderstand which model adapts better to languages unseen during pre-training.\nWe fine-tune both models on 13 unseen languages and 18 seen languages. Our\nresults show that the number of hours seen per language and language family\nduring pre-training is predictive of how the models compare, despite the\nsignificant differences in the pre-training methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rouditchenko_A/0/1/0/all/0/1\">Andrew Rouditchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khurana_S/0/1/0/all/0/1\">Sameer Khurana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Samuel Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1\">Leonid Karlinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Bias and Fairness in NLP: How to have a fairer text classification?. (arXiv:2305.12829v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12829","description":"<p>In this paper, we provide a holistic analysis of the different sources of\nbias, Upstream, Sample and Overampflication biases, in NLP models. We\ninvestigate how they impact the fairness of the task of text classification. We\nalso investigate the impact of removing these biases using different debiasing\ntechniques on the fairness of text classification. We found that\noveramplification bias is the most impactful bias on the fairness of text\nclassification. And that removing overamplification bias by fine-tuning the LM\nmodels on a dataset with balanced representations of the different identity\ngroups leads to fairer text classification models. Finally, we build on our\nfindings and introduce practical guidelines on how to have a fairer text\nclassification model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elsafoury_F/0/1/0/all/0/1\">Fatma Elsafoury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsigiannis_S/0/1/0/all/0/1\">Stamos Katsigiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramzan_N/0/1/0/all/0/1\">Naeem Ramzan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion. (arXiv:2305.14652v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14652","description":"<p>Video multimodal fusion aims to integrate multimodal signals in videos, such\nas visual, audio and text, to make a complementary prediction with multiple\nmodalities contents. However, unlike other image-text multimodal tasks, video\nhas longer multimodal sequences with more redundancy and noise in both visual\nand audio modalities. Prior denoising methods like forget gate are coarse in\nthe granularity of noise filtering. They often suppress the redundant and noisy\ninformation at the risk of losing critical information. Therefore, we propose a\ndenoising bottleneck fusion (DBF) model for fine-grained video multimodal\nfusion. On the one hand, we employ a bottleneck mechanism to filter out noise\nand redundancy with a restrained receptive field. On the other hand, we use a\nmutual information maximization module to regulate the filter-out module to\npreserve key information within different modalities. Our DBF model achieves\nsignificant improvement over current state-of-the-art baselines on multiple\nbenchmarks covering multimodal sentiment analysis and multimodal summarization\ntasks. It proves that our model can effectively capture salient features from\nnoisy and redundant video, audio, and text inputs. The code for this paper is\npublicly available at https://github.com/WSXRHFG/DBF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shaoxiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Ziwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Binghuai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do GPTs Produce Less Literal Translations?. (arXiv:2305.16806v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16806","description":"<p>Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose\nlanguage models capable of addressing many natural language generation or\nunderstanding tasks. On the task of Machine Translation (MT), multiple works\nhave investigated few-shot prompting mechanisms to elicit better translations\nfrom LLMs. However, there has been relatively little investigation on how such\ntranslations differ qualitatively from the translations generated by standard\nNeural Machine Translation (NMT) models. In this work, we investigate these\ndifferences in terms of the literalness of translations produced by the two\nsystems. Using literalness measures involving word alignment and monotonicity,\nwe find that translations out of English (E-X) from GPTs tend to be less\nliteral, while exhibiting similar or better scores on MT quality metrics. We\ndemonstrate that this finding is borne out in human evaluations as well. We\nthen show that these differences are especially pronounced when translating\nsentences that contain idiomatic expressions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raunak_V/0/1/0/all/0/1\">Vikas Raunak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menezes_A/0/1/0/all/0/1\">Arul Menezes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1\">Matt Post</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadalla_H/0/1/0/all/0/1\">Hany Hassan Awadalla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Computational Power of Decoder-Only Transformer Language Models. (arXiv:2305.17026v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17026","description":"<p>This article presents a theoretical evaluation of the computational\nuniversality of decoder-only transformer models. We extend the theoretical\nliterature on transformer models and show that decoder-only transformer\narchitectures (even with only a single layer and single attention head) are\nTuring complete under reasonable assumptions. From the theoretical analysis, we\nshow sparsity/compressibility of the word embedding to be a necessary condition\nfor Turing completeness to hold.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roberts_J/0/1/0/all/0/1\">Jesse Roberts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Honey, I Shrunk the Language: Language Model Behavior at Reduced Scale. (arXiv:2305.17266v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17266","description":"<p>In recent years, language models have drastically grown in size, and the\nabilities of these models have been shown to improve with scale. The majority\nof recent scaling laws studies focused on high-compute high-parameter count\nsettings, leaving the question of when these abilities begin to emerge largely\nunanswered. In this paper, we investigate whether the effects of pre-training\ncan be observed when the problem size is reduced, modeling a smaller,\nreduced-vocabulary language. We show the benefits of pre-training with masked\nlanguage modeling (MLM) objective in models as small as 1.25M parameters, and\nestablish a strong correlation between pre-training perplexity and downstream\nperformance (GLUE benchmark). We examine downscaling effects, extending scaling\nlaws to models as small as ~1M parameters. At this scale, we observe a break of\nthe power law for compute-optimal models and show that the MLM loss does not\nscale smoothly with compute-cost (FLOPs) below $2.2 \\times 10^{15}$ FLOPs. We\nalso find that adding layers does not always benefit downstream performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_V/0/1/0/all/0/1\">Vijeta Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechi_D/0/1/0/all/0/1\">Dan Pechi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thatte_S/0/1/0/all/0/1\">Shree Thatte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lialin_V/0/1/0/all/0/1\">Vladislav Lialin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1\">Anna Rumshisky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Framework For Refining Text Classification and Object Recognition from Academic Articles. (arXiv:2305.17401v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.17401","description":"<p>With the widespread use of the internet, it has become increasingly crucial\nto extract specific information from vast amounts of academic articles\nefficiently. Data mining techniques are generally employed to solve this issue.\nHowever, data mining for academic articles is challenging since it requires\nautomatically extracting specific patterns in complex and unstructured layout\ndocuments. Current data mining methods for academic articles employ\nrule-based(RB) or machine learning(ML) approaches. However, using rule-based\nmethods incurs a high coding cost for complex typesetting articles. On the\nother hand, simply using machine learning methods requires annotation work for\ncomplex content types within the paper, which can be costly. Furthermore, only\nusing machine learning can lead to cases where patterns easily recognized by\nrule-based methods are mistakenly extracted. To overcome these issues, from the\nperspective of analyzing the standard layout and typesetting used in the\nspecified publication, we emphasize implementing specific methods for specific\ncharacteristics in academic articles. We have developed a novel Text Block\nRefinement Framework (TBRF), a machine learning and rule-based scheme hybrid.\nWe used the well-known ACL proceeding articles as experimental data for the\nvalidation experiment. The experiment shows that our approach achieved over 95%\nclassification accuracy and 90% detection accuracy for tables and figures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinghong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ota_K/0/1/0/all/0/1\">Koichi Ota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1\">Wen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_S/0/1/0/all/0/1\">Shinobu Hasegawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Curse of Recursion: Training on Generated Data Makes Models Forget. (arXiv:2305.17493v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.17493","description":"<p>Stable Diffusion revolutionised image creation from descriptive text. GPT-2,\nGPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of\nlanguage tasks. ChatGPT introduced such language models to the general public.\nIt is now clear that large language models (LLMs) are here to stay, and will\nbring about drastic change in the whole ecosystem of online text and images. In\nthis paper we consider what the future might hold. What will happen to GPT-{n}\nonce LLMs contribute much of the language found online? We find that use of\nmodel-generated content in training causes irreversible defects in the\nresulting models, where tails of the original content distribution disappear.\nWe refer to this effect as Model Collapse and show that it can occur in\nVariational Autoencoders, Gaussian Mixture Models and LLMs. We build\ntheoretical intuition behind the phenomenon and portray its ubiquity amongst\nall learned generative models. We demonstrate that it has to be taken seriously\nif we are to sustain the benefits of training from large-scale data scraped\nfrom the web. Indeed, the value of data collected about genuine human\ninteractions with systems will be increasingly valuable in the presence of\ncontent generated by LLMs in data crawled from the Internet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1\">Ilia Shumailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shumaylov_Z/0/1/0/all/0/1\">Zakhar Shumaylov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yiren Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1\">Nicolas Papernot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_R/0/1/0/all/0/1\">Ross Anderson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical PCG Through Large Language Models. (arXiv:2305.18243v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18243","description":"<p>Large Language Models (LLMs) have proven to be useful tools in various\ndomains outside of the field of their inception, which was natural language\nprocessing. In this study, we provide practical directions on how to use LLMs\nto generate 2D-game rooms for an under-development game, named Metavoidal. Our\ntechnique can harness the power of GPT-3 by Human-in-the-loop fine-tuning which\nallows our method to create 37% Playable-Novel levels from as scarce data as\nonly 60 hand-designed rooms under a scenario of the non-trivial game, with\nrespect to (Procedural Content Generation) PCG, that has a good amount of local\nand global constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nasir_M/0/1/0/all/0/1\">Muhammad U Nasir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1\">Julian Togelius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models. (arXiv:2305.18703v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18703","description":"<p>Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing (NLP), providing a highly useful, task-agnostic foundation\nfor a wide range of applications. The great promise of LLMs as general task\nsolvers motivated people to extend their functionality largely beyond just a\n``chatbot'', and use it as an assistant or even replacement for domain experts\nand tools in specific domains such as healthcare, finance, and education.\nHowever, directly applying LLMs to solve sophisticated problems in specific\ndomains meets many hurdles, caused by the heterogeneity of domain data, the\nsophistication of domain knowledge, the uniqueness of domain objectives, and\nthe diversity of the constraints (e.g., various social norms, cultural\nconformity, religious beliefs, and ethical standards in the domain\napplications). To fill such a gap, explosively-increase research, and practices\nhave been conducted in very recent years on the domain specialization of LLMs,\nwhich, however, calls for a comprehensive and systematic review to better\nsummarizes and guide this promising domain. In this survey paper, first, we\npropose a systematic taxonomy that categorizes the LLM domain-specialization\ntechniques based on the accessibility to LLMs and summarizes the framework for\nall the subcategories as well as their relations and differences to each other.\nWe also present a comprehensive taxonomy of critical application domains that\ncan benefit from specialized LLMs, discussing their practical significance and\nopen challenges. Furthermore, we offer insights into the current research\nstatus and future trends in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1\">Chen Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xujiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiaying Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chengyuan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Can Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_T/0/1/0/all/0/1\">Tanmoy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianjiao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panalkar_A/0/1/0/all/0/1\">Amit Panalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengzhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1\">Chris White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable Performance Analysis for Vision-Language Models. (arXiv:2305.18786v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.18786","description":"<p>Joint vision-language models have shown great performance over a diverse set\nof tasks. However, little is known about their limitations, as the high\ndimensional space learned by these models makes it difficult to identify\nsemantic errors. Recent work has addressed this problem by designing highly\ncontrolled probing task benchmarks. Our paper introduces a more scalable\nsolution that relies on already annotated benchmarks. Our method consists of\nextracting a large set of diverse features from a vision-language benchmark and\nmeasuring their correlation with the output of the target model. We confirm\nprevious findings that CLIP behaves like a bag of words model and performs\nbetter with nouns and verbs; we also uncover novel insights such as CLIP\ngetting confused by concrete words. Our framework is available at\nhttps://github.com/MichiganNLP/Scalable-VLM-Probing and can be used with other\nmultimodal models and benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Castro_S/0/1/0/all/0/1\">Santiago Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1\">Oana Ignat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross Encoding as Augmentation: Towards Effective Educational Text Classification. (arXiv:2305.18977v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18977","description":"<p>Text classification in education, usually called auto-tagging, is the\nautomated process of assigning relevant tags to educational content, such as\nquestions and textbooks. However, auto-tagging suffers from a data scarcity\nproblem, which stems from two major challenges: 1) it possesses a large tag\nspace and 2) it is multi-label. Though a retrieval approach is reportedly good\nat low-resource scenarios, there have been fewer efforts to directly address\nthe data scarcity problem. To mitigate these issues, here we propose a novel\nretrieval approach CEAA that provides effective learning in educational text\nclassification. Our main contributions are as follows: 1) we leverage transfer\nlearning from question-answering datasets, and 2) we propose a simple but\neffective data augmentation method introducing cross-encoder style texts to a\nbi-encoder architecture for more efficient inference. An extensive set of\nexperiments shows that our proposed method is effective in multi-label\nscenarios and low-resource tags compared to state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyun Seung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Seungtaek Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yunsung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_H/0/1/0/all/0/1\">Hyeongdon Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Shinhyeok Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1\">Myeongho Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Go_H/0/1/0/all/0/1\">Hyojun Go</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallraven_C/0/1/0/all/0/1\">Christian Wallraven</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlled Text Generation with Hidden Representation Transformations. (arXiv:2305.19230v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.19230","description":"<p>We propose CHRT (Control Hidden Representation Transformation) - a controlled\nlanguage generation framework that steers large language models to generate\ntext pertaining to certain attributes (such as toxicity). CHRT gains attribute\ncontrol by modifying the hidden representation of the base model through\nlearned transformations. We employ a contrastive-learning framework to learn\nthese transformations that can be combined to gain multi-attribute control. The\neffectiveness of CHRT is experimentally shown by comparing it with seven\nbaselines over three attributes. CHRT outperforms all the baselines in the task\nof detoxification, positive sentiment steering, and text simplification while\nminimizing the loss in linguistic qualities. Further, our approach has the\nlowest inference latency of only 0.01 seconds more than the base model, making\nit the most suitable for high-performance production environments. We\nopen-source our code and release two novel datasets to further propel\ncontrolled language generation research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vaibhav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koorehdavoudi_H/0/1/0/all/0/1\">Hana Koorehdavoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moshtaghi_M/0/1/0/all/0/1\">Masud Moshtaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_A/0/1/0/all/0/1\">Amita Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Ankit Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrara_E/0/1/0/all/0/1\">Emilio Ferrara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grammar Prompting for Domain-Specific Language Generation with Large Language Models. (arXiv:2305.19234v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.19234","description":"<p>Large language models (LLMs) can learn to perform a wide range of natural\nlanguage tasks from just a handful of in-context examples. However, for\ngenerating strings from highly structured languages (e.g., semantic parsing to\ncomplex domain-specific languages), it is challenging for the LLM to generalize\nfrom just a few exemplars. We explore $\\textbf{grammar prompting}$ as a simple\napproach for enabling LLMs to use external knowledge and domain-specific\nconstraints, expressed through a grammar expressed in Backus--Naur Form (BNF),\nduring in-context learning. Grammar prompting augments each demonstration\nexample with a specialized grammar that is minimally sufficient for generating\nthe particular output example, where the specialized grammar is a subset of the\nfull DSL grammar. For inference, the LLM first predicts a BNF grammar given a\ntest input, and then generates the output according to the rules of the\ngrammar. Experiments demonstrate that grammar prompting can enable LLMs to\nperform competitively on a diverse set of DSL generation tasks, including\nsemantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and even\nmolecule generation (SMILES).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bailin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saurous_R/0/1/0/all/0/1\">Rif A. Saurous</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CONE: An Efficient COarse-to-fiNE Alignment Framework for Long Video Temporal Grounding. (arXiv:2209.10918v2 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2209.10918","description":"<p>This paper tackles an emerging and challenging problem of long video temporal\ngrounding~(VTG) that localizes video moments related to a natural language (NL)\nquery. Compared with short videos, long videos are also highly demanded but\nless explored, which brings new challenges in higher inference computation cost\nand weaker multi-modal alignment. To address these challenges, we propose CONE,\nan efficient COarse-to-fiNE alignment framework. CONE is a plug-and-play\nframework on top of existing VTG models to handle long videos through a sliding\nwindow mechanism. Specifically, CONE (1) introduces a query-guided window\nselection strategy to speed up inference, and (2) proposes a coarse-to-fine\nmechanism via a novel incorporation of contrastive learning to enhance\nmulti-modal alignment for long videos. Extensive experiments on two large-scale\nlong VTG benchmarks consistently show both substantial performance gains (e.g.,\nfrom 3.13% to 6.87% on MAD) and state-of-the-art results. Analyses also reveal\nhigher efficiency as the query-guided window selection mechanism accelerates\ninference time by 2x on Ego4D-NLQ and 15x on MAD while keeping SOTA results.\nCodes have been released at https://github.com/houzhijian/CONE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zhijian Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wanjun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1\">Lei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Difei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Kun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1\">Wing-Kwong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1\">Chong-Wah Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_Z/0/1/0/all/0/1\">Zheng Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-training for Speech Translation: CTC Meets Optimal Transport. (arXiv:2301.11716v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2301.11716","description":"<p>The gap between speech and text modalities is a major challenge in\nspeech-to-text translation (ST). Different methods have been proposed to reduce\nthis gap, but most of them require architectural changes in ST training. In\nthis work, we propose to mitigate this issue at the pre-training stage,\nrequiring no change in the ST model. First, we show that the connectionist\ntemporal classification (CTC) loss can reduce the modality gap by design. We\nprovide a quantitative comparison with the more common cross-entropy loss,\nshowing that pre-training with CTC consistently achieves better final ST\naccuracy. Nevertheless, CTC is only a partial solution and thus, in our second\ncontribution, we propose a novel pre-training method combining CTC and optimal\ntransport to further reduce this gap. Our method pre-trains a Siamese-like\nmodel composed of two encoders, one for acoustic inputs and the other for\ntextual inputs, such that they produce representations that are close to each\nother in the Wasserstein space. Extensive experiments on the standard CoVoST-2\nand MuST-C datasets show that our pre-training method applied to the vanilla\nencoder-decoder Transformer achieves state-of-the-art performance under the\nno-external-data setting, and performs on par with recent strong multi-task\nlearning systems trained with external data. Finally, our method can also be\napplied on top of these multi-task systems, leading to further improvements for\nthese models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_P/0/1/0/all/0/1\">Phuong-Hang Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lecouteux_B/0/1/0/all/0/1\">Benjamin Lecouteux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwab_D/0/1/0/all/0/1\">Didier Schwab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaPlanner: Adaptive Planning from Feedback with Language Models. (arXiv:2305.16653v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2305.16653","description":"<p>Large language models (LLMs) have recently demonstrated the potential in\nacting as autonomous agents for sequential decision-making tasks. However, most\nexisting methods either take actions greedily without planning or rely on\nstatic plans that are not adaptable to environmental feedback. Consequently,\nthe sequential decision-making performance of LLM agents degenerates with\nproblem complexity and plan horizons increase. We propose a closed-loop\napproach, AdaPlanner, which allows the LLM agent to refine its self-generated\nplan adaptively in response to environmental feedback. In AdaPlanner, the LLM\nagent adaptively refines its plan from feedback with both in-plan and\nout-of-plan refinement strategies. To mitigate hallucination, we develop a\ncode-style LLM prompt structure that facilitates plan generation across a\nvariety of tasks, environments, and agent capabilities. Furthermore, we propose\na skill discovery mechanism that leverages successful plans as few-shot\nexemplars, enabling the agent to plan and refine with fewer task\ndemonstrations. Our experiments in the ALFWorld and MiniWoB++ environments\ndemonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and\n4.11% while utilizing 2x and 600x fewer samples, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haotian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yuchen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingkai Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-05-31T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}