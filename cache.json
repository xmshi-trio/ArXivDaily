{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-10-19T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Deep Bidirectional Language-Knowledge Graph Pretraining. (arXiv:2210.09338v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09338","description":"<p>Pretraining a language model (LM) on text has been shown to help various\ndownstream NLP tasks. Recent works show that a knowledge graph (KG) can\ncomplement text data, offering structured background knowledge that provides a\nuseful scaffold for reasoning. However, these works are not pretrained to learn\na deep fusion of the two modalities at scale, limiting the potential to acquire\nfully joint representations of text and KG. Here we propose DRAGON (Deep\nBidirectional Language-Knowledge Graph Pretraining), a self-supervised approach\nto pretraining a deeply joint language-knowledge foundation model from text and\nKG at scale. Specifically, our model takes pairs of text segments and relevant\nKG subgraphs as input and bidirectionally fuses information from both\nmodalities. We pretrain this model by unifying two self-supervised reasoning\ntasks, masked language modeling and KG link prediction. DRAGON outperforms\nexisting LM and LM+KG models on diverse downstream tasks including question\nanswering across general and biomedical domains, with +5% absolute gain on\naverage. In particular, DRAGON achieves notable performance on complex\nreasoning about language and knowledge (+10% on questions involving long\ncontexts or multi-step reasoning) and low-resource QA (+8% on OBQA and\nRiddleSense), and new state-of-the-art results on various BioNLP tasks. Our\ncode and trained models are available at\nhttps://github.com/michiyasunaga/dragon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xikun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D Manning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferring Knowledge via Neighborhood-Aware Optimal Transport for Low-Resource Hate Speech Detection. (arXiv:2210.09340v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09340","description":"<p>The concerning rise of hateful content on online platforms has increased the\nattention towards automatic hate speech detection, commonly formulated as a\nsupervised classification task. State-of-the-art deep learning-based approaches\nusually require a substantial amount of labeled resources for training.\nHowever, annotating hate speech resources is expensive, time-consuming, and\noften harmful to the annotators. This creates a pressing need to transfer\nknowledge from the existing labeled resources to low-resource hate speech\ncorpora with the goal of improving system performance. For this,\nneighborhood-based frameworks have been shown to be effective. However, they\nhave limited flexibility. In our paper, we propose a novel training strategy\nthat allows flexible modeling of the relative proximity of neighbors retrieved\nfrom a resource-rich corpus to learn the amount of transfer. In particular, we\nincorporate neighborhood information with Optimal Transport, which permits\nexploiting the geometry of the data embedding space. By aligning the joint\nembedding and label distributions of neighbors, we demonstrate substantial\nimprovements over strong baselines, in low-resource scenarios, on different\npublicly available hate speech corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bose_T/0/1/0/all/0/1\">Tulika Bose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Illina_I/0/1/0/all/0/1\">Irina Illina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fohr_D/0/1/0/all/0/1\">Dominique Fohr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossRE: A Cross-Domain Dataset for Relation Extraction. (arXiv:2210.09345v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09345","description":"<p>Relation Extraction (RE) has attracted increasing attention, but current RE\nevaluation is limited to in-domain evaluation setups. Little is known on how\nwell a RE system fares in challenging, but realistic out-of-distribution\nevaluation setups. To address this gap, we propose CrossRE, a new,\nfreely-available cross-domain benchmark for RE, which comprises six distinct\ntext domains and includes multi-label annotations. An additional innovation is\nthat we release meta-data collected during annotation, to include explanations\nand flags of difficult instances. We provide an empirical evaluation with a\nstate-of-the-art model for relation classification. As the meta-data enables us\nto shed new light on the state-of-the-art model, we provide a comprehensive\nanalysis on the impact of difficult cases and find correlations between model\nand human annotations. Overall, our empirical investigation highlights the\ndifficulty of cross-domain RE. We release our dataset, to spur more research in\nthis direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bassignana_E/0/1/0/all/0/1\">Elisa Bassignana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Potrika: Raw and Balanced Newspaper Datasets in the Bangla Language with Eight Topics and Five Attributes. (arXiv:2210.09389v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09389","description":"<p>Knowledge is central to human and scientific developments. Natural Language\nProcessing (NLP) allows automated analysis and creation of knowledge. Data is a\ncrucial NLP and machine learning ingredient. The scarcity of open datasets is a\nwell-known problem in machine and deep learning research. This is very much the\ncase for textual NLP datasets in English and other major world languages. For\nthe Bangla language, the situation is even more challenging and the number of\nlarge datasets for NLP research is practically nil. We hereby present Potrika,\na large single-label Bangla news article textual dataset curated for NLP\nresearch from six popular online news portals in Bangladesh (Jugantor,\nJaijaidin, Ittefaq, Kaler Kontho, Inqilab, and Somoyer Alo) for the period\n2014-2020. The articles are classified into eight distinct categories\n(National, Sports, International, Entertainment, Economy, Education, Politics,\nand Science \\&amp; Technology) providing five attributes (News Article, Category,\nHeadline, Publication Date, and Newspaper Source). The raw dataset contains\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles. Moreover, using NLP augmentation techniques, we create from the raw\n(unbalanced) dataset another (balanced) dataset comprising 320,000 news\narticles with 40,000 articles in each of the eight news categories. Potrika\ncontains both the datasets (raw and balanced) to suit a wide range of NLP\nresearch. By far, to the best of our knowledge, Potrika is the largest and the\nmost extensive dataset for news classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_I/0/1/0/all/0/1\">Istiak Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AlQurashi_F/0/1/0/all/0/1\">Fahad AlQurashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehmood_R/0/1/0/all/0/1\">Rashid Mehmood</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Affective Idiosyncratic Responses to Music. (arXiv:2210.09396v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09396","description":"<p>Affective responses to music are highly personal. Despite consensus that\nidiosyncratic factors play a key role in regulating how listeners emotionally\nrespond to music, precisely measuring the marginal effects of these variables\nhas proved challenging. To address this gap, we develop computational methods\nto measure affective responses to music from over 403M listener comments on a\nChinese social music platform. Building on studies from music psychology in\nsystematic and quasi-causal analyses, we test for musical, lyrical, contextual,\ndemographic, and mental health effects that drive listener affective responses.\nFinally, motivated by the social phenomenon known as w\\v{a}ng-y\\`i-y\\'un, we\nidentify influencing factors of platform user self-disclosures, the social\nsupport they receive, and notable differences in discloser user activity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+CH_Wang_S/0/1/0/all/0/1\">Sky CH-Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Evan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_O/0/1/0/all/0/1\">Oliver Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deepfake Text Detection: Limitations and Opportunities. (arXiv:2210.09421v1 [cs.CR])","link":"http://arxiv.org/abs/2210.09421","description":"<p>Recent advances in generative models for language have enabled the creation\nof convincing synthetic text or deepfake text. Prior work has demonstrated the\npotential for misuse of deepfake text to mislead content consumers. Therefore,\ndeepfake text detection, the task of discriminating between human and\nmachine-generated text, is becoming increasingly critical. Several defenses\nhave been proposed for deepfake text detection. However, we lack a thorough\nunderstanding of their real-world applicability. In this paper, we collect\ndeepfake text from 4 online services powered by Transformer-based tools to\nevaluate the generalization ability of the defenses on content in the wild. We\ndevelop several low-cost adversarial attacks, and investigate the robustness of\nexisting defenses against an adaptive attacker. We find that many defenses show\nsignificant degradation in performance under our evaluation scenarios compared\nto their original claimed performance. Our evaluation shows that tapping into\nthe semantic information in the text content is a promising approach for\nimproving the robustness and generalization performance of deepfake text\ndetection schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pu_J/0/1/0/all/0/1\">Jiameng Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarwar_Z/0/1/0/all/0/1\">Zain Sarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdullah_S/0/1/0/all/0/1\">Sifat Muhammad Abdullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehman_A/0/1/0/all/0/1\">Abdullah Rehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoonjin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_P/0/1/0/all/0/1\">Parantapa Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1\">Mobin Javed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viswanath_B/0/1/0/all/0/1\">Bimal Viswanath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Low-Resource Cross-lingual Parsing with Expected Statistic Regularization. (arXiv:2210.09428v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09428","description":"<p>We present Expected Statistic Regularization (ESR), a novel regularization\ntechnique that utilizes low-order multi-task structural statistics to shape\nmodel distributions for semi-supervised learning on low-resource datasets. We\nstudy ESR in the context of cross-lingual transfer for syntactic analysis (POS\ntagging and labeled dependency parsing) and present several classes of\nlow-order statistic functions that bear on model behavior. Experimentally, we\nevaluate the proposed statistics with ESR for unsupervised transfer on 5\ndiverse target languages and show that all statistics, when estimated\naccurately, yield improvements to both POS and LAS, with the best statistic\nimproving POS by +7.0 and LAS by +8.5 on average. We also present\nsemi-supervised transfer and learning curve experiments that show ESR provides\nsignificant gains over strong cross-lingual-transfer-plus-fine-tuning baselines\nfor modest amounts of label data. These results indicate that ESR is a\npromising and complementary approach to model-transfer approaches for\ncross-lingual parsing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Effland_T/0/1/0/all/0/1\">Thomas Effland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1\">Michael Collins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modelling Emotion Dynamics in Song Lyrics with State Space Models. (arXiv:2210.09434v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09434","description":"<p>Most previous work in music emotion recognition assumes a single or a few\nsong-level labels for the whole song. While it is known that different emotions\ncan vary in intensity within a song, annotated data for this setup is scarce\nand difficult to obtain. In this work, we propose a method to predict emotion\ndynamics in song lyrics without song-level supervision. We frame each song as a\ntime series and employ a State Space Model (SSM), combining a sentence-level\nemotion predictor with an Expectation-Maximization (EM) procedure to generate\nthe full emotion dynamics. Our experiments show that applying our method\nconsistently improves the performance of sentence-level baselines without\nrequiring any annotated songs, making it ideal for limited training data\nscenarios. Further analysis through case studies shows the benefits of our\nmethod while also indicating the limitations and pointing to future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yingjin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beck_D/0/1/0/all/0/1\">Daniel Beck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Bottleneck Adapters to Identify Cancer in Clinical Notes under Low-Resource Constraints. (arXiv:2210.09440v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09440","description":"<p>Processing information locked within clinical health records is a challenging\ntask that remains an active area of research in biomedical NLP. In this work,\nwe evaluate a broad set of machine learning techniques ranging from simple RNNs\nto specialised transformers such as BioBERT on a dataset containing clinical\nnotes along with a set of annotations indicating whether a sample is\ncancer-related or not.\n</p>\n<p>Furthermore, we specifically employ efficient fine-tuning methods from NLP,\nnamely, bottleneck adapters and prompt tuning, to adapt the models to our\nspecialised task. Our evaluations suggest that fine-tuning a frozen BERT model\npre-trained on natural language and with bottleneck adapters outperforms all\nother strategies, including full fine-tuning of the specialised BioBERT model.\nBased on our findings, we suggest that using bottleneck adapters in\nlow-resource situations with limited access to labelled data or processing\ncapacity could be a viable strategy in biomedical text mining. The code used in\nthe experiments are going to be made available at\nhttps://github.com/omidrohanian/bottleneck-adapters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rohanian_O/0/1/0/all/0/1\">Omid Rohanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jauncey_H/0/1/0/all/0/1\">Hannah Jauncey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nouriborji_M/0/1/0/all/0/1\">Mohammadmahdi Nouriborji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goncalves_B/0/1/0/all/0/1\">Bronner P. Gon&#xe7;alves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kartsonaki_C/0/1/0/all/0/1\">Christiana Kartsonaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Group_ISARIC_Clinical_Characterisation/0/1/0/all/0/1\">ISARIC Clinical Characterisation Group</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merson_L/0/1/0/all/0/1\">Laura Merson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1\">David Clifton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-granularity Argument Mining in Legal Texts. (arXiv:2210.09472v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09472","description":"<p>In this paper, we explore legal argument mining using multiple levels of\ngranularity. Argument mining has usually been conceptualized as a sentence\nclassification problem. In this work, we conceptualize argument mining as a\ntoken-level (i.e., word-level) classification problem. We use a Longformer\nmodel to classify the tokens. Results show that token-level text classification\nidentifies certain legal argument elements more accurately than sentence-level\ntext classification. Token-level classification also provides greater\nflexibility to analyze legal texts and to gain more insight into what the model\nfocuses on when processing a large amount of input data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huihui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashley_K/0/1/0/all/0/1\">Kevin Ashley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Non-dialogue Summaries for Dialogue Summarization. (arXiv:2210.09474v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09474","description":"<p>To mitigate the lack of diverse dialogue summarization datasets in academia,\nwe present methods to utilize non-dialogue summarization data for enhancing\ndialogue summarization systems. We apply transformations to document\nsummarization data pairs to create training data that better befit dialogue\nsummarization. The suggested transformations also retain desirable properties\nof non-dialogue datasets, such as improved faithfulness to the source text. We\nconduct extensive experiments across both English and Korean to verify our\napproach. Although absolute gains in ROUGE naturally plateau as more dialogue\nsummarization samples are introduced, utilizing non-dialogue data for training\nsignificantly improves summarization performance in zero- and few-shot settings\nand enhances faithfulness across all training regimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seongmin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1\">Dongchan Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jihwa Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Systematicity in GPT-3's Interpretation of Novel English Noun Compounds. (arXiv:2210.09492v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09492","description":"<p>Levin et al. (2019) show experimentally that the interpretations of novel\nEnglish noun compounds (e.g., stew skillet), while not fully compositional, are\nhighly predictable based on whether the modifier and head refer to artifacts or\nnatural kinds. Is the large language model GPT-3 governed by the same\ninterpretive principles? To address this question, we first compare Levin et\nal.'s experimental data with GPT-3 generations, finding a high degree of\nsimilarity. However, this evidence is consistent with GPT3 reasoning only about\nspecific lexical items rather than the more abstract conceptual categories of\nLevin et al.'s theory. To probe more deeply, we construct prompts that require\nthe relevant kind of conceptual reasoning. Here, we fail to find convincing\nevidence that GPT-3 is reasoning about more than just individual lexical items.\nThese results highlight the importance of controlling for low-level\ndistributional regularities when assessing whether a large language model\nlatently encodes a deeper theory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlson_R/0/1/0/all/0/1\">Riley Carlson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalization of CTC Speech Recognition Models. (arXiv:2210.09510v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09510","description":"<p>End-to-end speech recognition models trained using joint Connectionist\nTemporal Classification (CTC)-Attention loss have gained popularity recently.\nIn these models, a non-autoregressive CTC decoder is often used at inference\ntime due to its speed and simplicity. However, such models are hard to\npersonalize because of their conditional independence assumption that prevents\noutput tokens from previous time steps to influence future predictions. To\ntackle this, we propose a novel two-way approach that first biases the encoder\nwith attention over a predefined list of rare long-tail and out-of-vocabulary\n(OOV) words and then uses dynamic boosting and phone alignment network during\ndecoding to further bias the subword predictions. We evaluate our approach on\nopen-source VoxPopuli and in-house medical datasets to showcase a 60%\nimprovement in F1 score on domain-specific rare words over a strong CTC\nbaseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dingliwal_S/0/1/0/all/0/1\">Saket Dingliwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkara_M/0/1/0/all/0/1\">Monica Sunkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ronanki_S/0/1/0/all/0/1\">Srikanth Ronanki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farris_J/0/1/0/all/0/1\">Jeff Farris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1\">Katrin Kirchhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodapati_S/0/1/0/all/0/1\">Sravan Bodapati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Team Flow at DRC2022: Pipeline System for Travel Destination Recommendation Task in Spoken Dialogue. (arXiv:2210.09518v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09518","description":"<p>To improve the interactive capabilities of a dialogue system, e.g., to adapt\nto different customers, the Dialogue Robot Competition (DRC2022) was held. As\none of the teams, we built a dialogue system with a pipeline structure\ncontaining four modules. The natural language understanding (NLU) and natural\nlanguage generation (NLG) modules were GPT-2 based models, and the dialogue\nstate tracking (DST) and policy modules were designed on the basis of\nhand-crafted rules. After the preliminary round of the competition, we found\nthat the low variation in training examples for the NLU and failed\nrecommendation due to the policy used were probably the main reasons for the\nlimited performance of the system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hirai_R/0/1/0/all/0/1\">Ryu Hirai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohashi_A/0/1/0/all/0/1\">Atsumoto Ohashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_A/0/1/0/all/0/1\">Ao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiroma_H/0/1/0/all/0/1\">Hideki Shiroma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xulin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tone_Y/0/1/0/all/0/1\">Yukihiko Tone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iizuka_S/0/1/0/all/0/1\">Shinya Iizuka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Higashinaka_R/0/1/0/all/0/1\">Ryuichiro Higashinaka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less is More: Simplifying Feature Extractors Prevents Overfitting for Neural Discourse Parsing Models. (arXiv:2210.09537v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09537","description":"<p>Complex feature extractors are widely employed for text representation\nbuilding. However, these complex feature extractors can lead to severe\noverfitting problems especially when the training datasets are small, which is\nespecially the case for several discourse parsing tasks. Thus, we propose to\nremove additional feature extractors and only utilize self-attention mechanism\nto exploit pretrained neural language models in order to mitigate the\noverfitting problem. Experiments on three common discourse parsing tasks (News\nDiscourse Profiling, Rhetorical Structure Theory based Discourse Parsing and\nPenn Discourse Treebank based Discourse Parsing) show that powered by recent\npretrained language models, our simplied feature extractors obtain better\ngeneralizabilities and meanwhile achieve comparable or even better system\nperformance. The simplified feature extractors have fewer learnable parameters\nand less processing time. Codes will be released and this simple yet effective\nmodel can serve as a better baseline for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Ming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sijing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Ruihong Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models. (arXiv:2210.09545v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09545","description":"<p>Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks.\nIn Natural Language Processing (NLP), DNNs are often backdoored during the\nfine-tuning process of a large-scale Pre-trained Language Model (PLM) with\npoisoned samples. Although the clean weights of PLMs are readily available,\nexisting methods have ignored this information in defending NLP models against\nbackdoor attacks. In this work, we take the first step to exploit the\npre-trained (unfine-tuned) weights to mitigate backdoors in fine-tuned language\nmodels. Specifically, we leverage the clean pre-trained weights via two\ncomplementary techniques: (1) a two-step Fine-mixing technique, which first\nmixes the backdoored weights (fine-tuned on poisoned data) with the pre-trained\nweights, then fine-tunes the mixed weights on a small subset of clean data; (2)\nan Embedding Purification (E-PUR) technique, which mitigates potential\nbackdoors existing in the word embeddings. We compare Fine-mixing with typical\nbackdoor mitigation methods on three single-sentence sentiment classification\ntasks and two sentence-pair classification tasks and show that it outperforms\nthe baselines by a considerable margin in all scenarios. We also show that our\nE-PUR method can benefit existing mitigation methods. Our work establishes a\nsimple but strong baseline defense for secure fine-tuned NLP models against\nbackdoor attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xingjun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Cross-modal Semantics Alignment Capability from the Textual Perspective. (arXiv:2210.09550v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09550","description":"<p>In recent years, vision and language pre-training (VLP) models have advanced\nthe state-of-the-art results in a variety of cross-modal downstream tasks.\nAligning cross-modal semantics is claimed to be one of the essential\ncapabilities of VLP models. However, it still remains unclear about the inner\nworking mechanism of alignment in VLP models. In this paper, we propose a new\nprobing method that is based on image captioning to first empirically study the\ncross-modal semantics alignment of VLP models. Our probing method is built upon\nthe fact that given an image-caption pair, the VLP models will give a score,\nindicating how well two modalities are aligned; maximizing such scores will\ngenerate sentences that VLP models believe are of good alignment. Analyzing\nthese sentences thus will reveal in what way different modalities are aligned\nand how well these alignments are in VLP models. We apply our probing method to\nfive popular VLP models, including UNITER, ROSITA, ViLBERT, CLIP, and LXMERT,\nand provide a comprehensive analysis of the generated captions guided by these\nmodels. Our results show that VLP models (1) focus more on just aligning\nobjects with visual words, while neglecting global semantics; (2) prefer fixed\nsentence patterns, thus ignoring more important textual information including\nfluency and grammar; and (3) deem the captions with more visual words are\nbetter aligned with images. These findings indicate that VLP models still have\nweaknesses in cross-modal semantics alignment and we hope this work will draw\nresearchers' attention to such problems when designing a new VLP model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_S/0/1/0/all/0/1\">Shi Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1\">Mianzhi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianbing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xinyu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DisCup: Discriminator Cooperative Unlikelihood Prompt-tuning for Controllable Text Generation. (arXiv:2210.09551v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09551","description":"<p>Prompt learning with immensely large Casual Language Models (CLMs) has been\nshown promising for attribute-controllable text generation (CTG). However,\nvanilla prompt tuning tends to imitate training corpus characteristics beyond\nthe control attributes, resulting in a poor generalization ability. Moreover,\nit is less able to capture the relationship between different attributes,\nfurther limiting the control performance. In this paper, we propose a new CTG\napproach, namely DisCup, which incorporates the attribute knowledge of\ndiscriminator to optimize the control-prompts, steering a frozen CLM to produce\nattribute-specific texts. Specifically, the frozen CLM model, capable of\nproducing multitudinous texts, is first used to generate the next-token\ncandidates based on the context, so as to ensure the diversity of tokens to be\npredicted. Then, we leverage an attribute-discriminator to select\ndesired/undesired tokens from those candidates, providing the inter-attribute\nknowledge. Finally, we bridge the above two traits by an unlikelihood objective\nfor prompt-tuning. Extensive experimental results show that DisCup can achieve\na new state-of-the-art control performance while maintaining an efficient and\nhigh-quality text generation, only relying on around 10 virtual tokens.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discrete Cross-Modal Alignment Enables Zero-Shot Speech Translation. (arXiv:2210.09556v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09556","description":"<p>End-to-end Speech Translation (ST) aims at translating the source language\nspeech into target language text without generating the intermediate\ntranscriptions. However, the training of end-to-end methods relies on parallel\nST data, which are difficult and expensive to obtain. Fortunately, the\nsupervised data for automatic speech recognition (ASR) and machine translation\n(MT) are usually more accessible, making zero-shot speech translation a\npotential direction. Existing zero-shot methods fail to align the two\nmodalities of speech and text into a shared semantic space, resulting in much\nworse performance compared to the supervised ST methods. In order to enable\nzero-shot ST, we propose a novel Discrete Cross-Modal Alignment (DCMA) method\nthat employs a shared discrete vocabulary space to accommodate and match both\nmodalities of speech and text. Specifically, we introduce a vector quantization\nmodule to discretize the continuous representations of speech and text into a\nfinite set of virtual tokens, and use ASR data to map corresponding speech and\ntext to the same virtual token in a shared codebook. This way, source language\nspeech can be embedded in the same semantic space as the source language text,\nwhich can be then transformed into target language text with an MT module.\nExperiments on multiple language pairs demonstrate that our zero-shot ST method\nsignificantly improves the SOTA, and even performers on par with the strong\nsupervised ST baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_C/0/1/0/all/0/1\">Chengqing Zong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Inference of Data-Driven Discourse Structures using a Tree Auto-Encoder. (arXiv:2210.09559v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09559","description":"<p>With a growing need for robust and general discourse structures in many\ndownstream tasks and real-world applications, the current lack of high-quality,\nhigh-quantity discourse trees poses a severe shortcoming. In order the\nalleviate this limitation, we propose a new strategy to generate tree\nstructures in a task-agnostic, unsupervised fashion by extending a latent tree\ninduction framework with an auto-encoding objective. The proposed approach can\nbe applied to any tree-structured objective, such as syntactic parsing,\ndiscourse parsing and others. However, due to the especially difficult\nannotation process to generate discourse trees, we initially develop such\nmethod to complement task-specific models in generating much larger and more\ndiverse discourse treebanks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huber_P/0/1/0/all/0/1\">Patrick Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1\">Giuseppe Carenini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Domain-Independent Supervised Discourse Parsing Through Gradient Boosting. (arXiv:2210.09565v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09565","description":"<p>Discourse analysis and discourse parsing have shown great impact on many\nimportant problems in the field of Natural Language Processing (NLP). Given the\ndirect impact of discourse annotations on model performance and\ninterpretability, robustly extracting discourse structures from arbitrary\ndocuments is a key task to further improve computational models in NLP. To this\nend, we present a new, supervised paradigm directly tackling the domain\nadaptation issue in discourse parsing. Specifically, we introduce the first\nfully supervised discourse parser designed to alleviate the domain dependency\nthrough a staged model of weak classifiers by introducing the gradient boosting\nframework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huber_P/0/1/0/all/0/1\">Patrick Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1\">Giuseppe Carenini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NADI 2022: The Third Nuanced Arabic Dialect Identification Shared Task. (arXiv:2210.09582v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09582","description":"<p>We describe findings of the third Nuanced Arabic Dialect Identification\nShared Task (NADI 2022). NADI aims at advancing state of the art Arabic NLP,\nincluding on Arabic dialects. It does so by affording diverse datasets and\nmodeling opportunities in a standardized context where meaningful comparisons\nbetween models and approaches are possible. NADI 2022 targeted both dialect\nidentification (Subtask 1) and dialectal sentiment analysis (Subtask 2) at the\ncountry level. A total of 41 unique teams registered for the shared task, of\nwhom 21 teams have actually participated (with 105 valid submissions). Among\nthese, 19 teams participated in Subtask 1 and 10 participated in Subtask 2. The\nwinning team achieved 27.06 F1 on Subtask 1 and F1=75.16 on Subtask 2,\nreflecting that the two subtasks remain challenging and motivating future work\nin this area. We describe methods employed by participating teams and offer an\noutlook for NADI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1\">AbdelRahim Elmadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouamor_H/0/1/0/all/0/1\">Houda Bouamor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summary Workbench: Unifying Application and Evaluation of Text Summarization Models. (arXiv:2210.09587v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09587","description":"<p>This paper presents Summary Workbench, a new tool for developing and\nevaluating text summarization models. New models and evaluation measures can be\neasily integrated as Docker-based plugins, allowing to examine the quality of\ntheir summaries against any input and to evaluate them using various evaluation\nmeasures. Visual analyses combining multiple measures provide insights into the\nmodels' strengths and weaknesses. The tool is hosted at\n\\url{https://tldr.demo.webis.de} and also supports local deployment for private\nresources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1\">Shahbaz Syed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwabe_D/0/1/0/all/0/1\">Dominik Schwabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synergy with Translation Artifacts for Training and Inference in Multilingual Tasks. (arXiv:2210.09588v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09588","description":"<p>Translation has played a crucial role in improving the performance on\nmultilingual tasks: (1) to generate the target language data from the source\nlanguage data for training and (2) to generate the source language data from\nthe target language data for inference. However, prior works have not\nconsidered the use of both translations simultaneously. This paper shows that\ncombining them can synergize the results on various multilingual sentence\nclassification tasks. We empirically find that translation artifacts stylized\nby translators are the main factor of the performance gain. Based on this\nanalysis, we adopt two training methods, SupCon and MixUp, considering\ntranslation artifacts. Furthermore, we propose a cross-lingual fine-tuning\nalgorithm called MUSC, which uses SupCon and MixUp jointly and improves the\nperformance. Our code is available at https://github.com/jongwooko/MUSC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jaehoon Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1\">Jongwoo Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Se-Young Yun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Soft-Labeled Contrastive Pre-training for Function-level Code Representation. (arXiv:2210.09597v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09597","description":"<p>Code contrastive pre-training has recently achieved significant progress on\ncode-related tasks. In this paper, we present \\textbf{SCodeR}, a\n\\textbf{S}oft-labeled contrastive pre-training framework with two positive\nsample construction methods to learn functional-level \\textbf{Code}\n\\textbf{R}epresentation. Considering the relevance between codes in a\nlarge-scale code corpus, the soft-labeled contrastive pre-training can obtain\nfine-grained soft-labels through an iterative adversarial manner and use them\nto learn better code representation. The positive sample construction is\nanother key for contrastive pre-training. Previous works use\ntransformation-based methods like variable renaming to generate semantically\nequal positive codes. However, they usually result in the generated code with a\nhighly similar surface form, and thus mislead the model to focus on superficial\ncode structure instead of code semantics. To encourage SCodeR to capture\nsemantic information from the code, we utilize code comments and abstract\nsyntax sub-trees of the code to build positive samples. We conduct experiments\non four code-related tasks over seven datasets. Extensive experimental results\nshow that SCodeR achieves new state-of-the-art performance on all of them,\nwhich illustrates the effectiveness of the proposed pre-training method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaonan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Daya Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Denoising Enhanced Distantly Supervised Ultrafine Entity Typing. (arXiv:2210.09599v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09599","description":"<p>Recently, the task of distantly supervised (DS) ultra-fine entity typing has\nreceived significant attention. However, DS data is noisy and often suffers\nfrom missing or wrong labeling issues resulting in low precision and low\nrecall. This paper proposes a novel ultra-fine entity typing model with\ndenoising capability. Specifically, we build a noise model to estimate the\nunknown labeling noise distribution over input contexts and noisy type labels.\nWith the noise model, more trustworthy labels can be recovered by subtracting\nthe estimated noise from the input. Furthermore, we propose an entity typing\nmodel, which adopts a bi-encoder architecture, is trained on the denoised data.\nFinally, the noise model and entity typing model are trained iteratively to\nenhance each other. We conduct extensive experiments on the Ultra-Fine entity\ntyping dataset as well as OntoNotes dataset and demonstrate that our approach\nsignificantly outperforms other baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hongliang Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tencent's Multilingual Machine Translation System for WMT22 Large-Scale African Languages. (arXiv:2210.09644v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09644","description":"<p>This paper describes Tencent's multilingual machine translation systems for\nthe WMT22 shared task on Large-Scale Machine Translation Evaluation for African\nLanguages. We participated in the $\\mathbf{constrained}$ translation track in\nwhich only the data and pretrained models provided by the organizer are\nallowed. The task is challenging due to three problems, including the absence\nof training data for some to-be-evaluated language pairs, the uneven\noptimization of language pairs caused by data imbalance, and the curse of\nmultilinguality. To address these problems, we adopt data augmentation,\ndistributionally robust optimization, and language family grouping,\nrespectively, to develop our multilingual neural machine translation (MNMT)\nmodels. Our submissions won the $\\mathbf{1st\\ place}$ on the blind test sets in\nterms of the automatic evaluation metrics. Codes, models, and detailed\ncompetition results are available at\nhttps://github.com/wxjiao/WMT2022-Large-Scale-African.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiarui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jen-tse Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ROSE: Robust Selective Fine-tuning for Pre-trained Language Models. (arXiv:2210.09658v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09658","description":"<p>Even though the large-scale language models have achieved excellent\nperformances, they suffer from various adversarial attacks. A large body of\ndefense methods has been proposed. However, they are still limited due to\nredundant attack search spaces and the inability to defend against various\ntypes of attacks. In this work, we present a novel fine-tuning approach called\n\\textbf{RO}bust \\textbf{SE}letive fine-tuning (\\textbf{ROSE}) to address this\nissue. ROSE conducts selective updates when adapting pre-trained models to\ndownstream tasks, filtering out invaluable and unrobust updates of parameters.\nSpecifically, we propose two strategies: the first-order and second-order ROSE\nfor selecting target robust parameters. The experimental results show that ROSE\nachieves significant improvements in adversarial robustness on various\ndownstream NLP tasks, and the ensemble method even surpasses both variants\nabove. Furthermore, ROSE can be easily incorporated into existing fine-tuning\nmethods to improve their adversarial robustness further. The empirical analysis\nconfirms that ROSE eliminates unrobust spurious updates during fine-tuning,\nleading to solutions corresponding to flatter and wider optima than the\nconventional method. Code is available at\n\\url{https://github.com/jiangllan/ROSE}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Rui Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alibaba-Translate China's Submission for WMT 2022 Metrics Shared Task. (arXiv:2210.09683v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09683","description":"<p>In this report, we present our submission to the WMT 2022 Metrics Shared\nTask. We build our system based on the core idea of UNITE (Unified Translation\nEvaluation), which unifies source-only, reference-only, and\nsource-reference-combined evaluation scenarios into one single model.\nSpecifically, during the model pre-training phase, we first apply the\npseudo-labeled data examples to continuously pre-train UNITE. Notably, to\nreduce the gap between pre-training and fine-tuning, we use data cropping and a\nranking-based score normalization strategy. During the fine-tuning phase, we\nuse both Direct Assessment (DA) and Multidimensional Quality Metrics (MQM) data\nfrom past years' WMT competitions. Specially, we collect the results from\nmodels with different pre-trained language model backbones, and use different\nensembling strategies for involved translation directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yu Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_K/0/1/0/all/0/1\">Keqin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dayiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baosong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Lidia S. Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Analysis of Acknowledgement Texts in Web of Science: a case study on four scientific domains. (arXiv:2210.09716v1 [cs.DL])","link":"http://arxiv.org/abs/2210.09716","description":"<p>Analysis of acknowledgments is particularly interesting as acknowledgments\nmay give information not only about funding, but they are also able to reveal\nhidden contributions to authorship and the researcher's collaboration patterns,\ncontext in which research was conducted, and specific aspects of the academic\nwork. The focus of the present research is the analysis of a large sample of\nacknowledgement texts indexed in the Web of Science (WoS) Core Collection.\nRecord types 'article' and 'review' from four different scientific domains,\nnamely social sciences, economics, oceanography and computer science, published\nfrom 2014 to 2019 in a scientific journal in English were considered. Six types\nof acknowledged entities, i.e., funding agency, grant number, individuals,\nuniversity, corporation and miscellaneous, were extracted from the\nacknowledgement texts using a Named Entity Recognition (NER) tagger and\nsubsequently examined. A general analysis of the acknowledgement texts showed\nthat indexing of funding information in WoS is incomplete. The analysis of the\nautomatically extracted entities revealed differences and distinct patterns in\nthe distribution of acknowledged entities of different types between different\nscientific domains. A strong association was found between acknowledged entity\nand scientific domain and acknowledged entity and entity type. Only negligible\ncorrelation was found between the number of citations and the number of\nacknowledged entities. Generally, the number of words in the acknowledgement\ntexts positively correlates with the number of acknowledged funding\norganizations, universities, individuals and miscellaneous entities. At the\nsame time, acknowledgement texts with the larger number of sentences have more\nacknowledged individuals and miscellaneous categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smirnova_N/0/1/0/all/0/1\">Nina Smirnova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayr_P/0/1/0/all/0/1\">Philipp Mayr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Textual Entailment Recognition with Semantic Features from Empirical Text Representation. (arXiv:2210.09723v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09723","description":"<p>Textual entailment recognition is one of the basic natural language\nunderstanding(NLU) tasks. Understanding the meaning of sentences is a\nprerequisite before applying any natural language processing(NLP) techniques to\nautomatically recognize the textual entailment. A text entails a hypothesis if\nand only if the true value of the hypothesis follows the text. Classical\napproaches generally utilize the feature value of each word from word embedding\nto represent the sentences. In this paper, we propose a novel approach to\nidentifying the textual entailment relationship between text and hypothesis,\nthereby introducing a new semantic feature focusing on empirical\nthreshold-based semantic text representation. We employ an element-wise\nManhattan distance vector-based feature that can identify the semantic\nentailment relationship between the text-hypothesis pair. We carried out\nseveral experiments on a benchmark entailment classification(SICK-RTE) dataset.\nWe train several machine learning(ML) algorithms applying both semantic and\nlexical features to classify the text-hypothesis pair as entailment, neutral,\nor contradiction. Our empirical sentence representation technique enriches the\nsemantic information of the texts and hypotheses found to be more efficient\nthan the classical ones. In the end, our approach significantly outperforms\nknown methods in understanding the meaning of the sentences for the textual\nentailment classification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atabuzzaman_M/0/1/0/all/0/1\">Md Atabuzzaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shajalal_M/0/1/0/all/0/1\">Md Shajalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baby_M/0/1/0/all/0/1\">Maksuda Bilkis Baby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Md Rezaul Karim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simultaneous Translation for Unsegmented Input: A Sliding Window Approach. (arXiv:2210.09754v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09754","description":"<p>In the cascaded approach to spoken language translation (SLT), the ASR output\nis typically punctuated and segmented into sentences before being passed to MT,\nsince the latter is typically trained on written text. However, erroneous\nsegmentation, due to poor sentence-final punctuation by the ASR system, leads\nto degradation in translation quality, especially in the simultaneous (online)\nsetting where the input is continuously updated. To reduce the influence of\nautomatic segmentation, we present a sliding window approach to translate raw\nASR outputs (online or offline) without needing to rely on an automatic\nsegmenter. We train translation models using parallel windows (instead of\nparallel sentences) extracted from the original training data. At test time, we\ntranslate at the window level and join the translated windows using a simple\napproach to generate the final translation. Experiments on English-to-German\nand English-to-Czech show that our approach improves 1.3--2.0 BLEU points over\nthe usual ASR-segmenter pipeline, and the fixed-length window considerably\nreduces flicker compared to a baseline retranslation-based online SLT system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sen_S/0/1/0/all/0/1\">Sukanta Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddow_B/0/1/0/all/0/1\">Barry Haddow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EventGraph at CASE 2021 Task 1: A General Graph-based Approach to Protest Event Extraction. (arXiv:2210.09770v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09770","description":"<p>This paper presents our submission to the 2022 edition of the CASE 2021\nshared task 1, subtask 4. The EventGraph system adapts an end-to-end,\ngraph-based semantic parser to the task of Protest Event Extraction and more\nspecifically subtask 4 on event trigger and argument extraction. We experiment\nwith various graphs, encoding the events as either \"labeled-edge\" or\n\"node-centric\" graphs. We show that the \"node-centric\" approach yields best\nresults overall, performing well across the three languages of the task, namely\nEnglish, Spanish, and Portuguese. EventGraph is ranked 3rd for English and\nPortuguese, and 4th for Spanish. Our code is available at:\nhttps://github.com/huiling-y/eventgraph_at_case\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1\">Huiling You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samuel_D/0/1/0/all/0/1\">David Samuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Touileb_S/0/1/0/all/0/1\">Samia Touileb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovrelid_L/0/1/0/all/0/1\">Lilja &#xd8;vrelid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrofitting Multilingual Sentence Embeddings with Abstract Meaning Representation. (arXiv:2210.09773v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09773","description":"<p>We introduce a new method to improve existing multilingual sentence\nembeddings with Abstract Meaning Representation (AMR). Compared with the\noriginal textual input, AMR is a structured semantic representation that\npresents the core concepts and relations in a sentence explicitly and\nunambiguously. It also helps reduce surface variations across different\nexpressions and languages. Unlike most prior work that only evaluates the\nability to measure semantic similarity, we present a thorough evaluation of\nexisting multilingual sentence embeddings and our improved versions, which\ninclude a collection of five transfer tasks in different downstream\napplications. Experiment results show that retrofitting multilingual sentence\nembeddings with AMR leads to better state-of-the-art performance on both\nsemantic textual similarity and transfer tasks. Our codebase and evaluation\nscripts can be found at \\url{https://github.com/jcyk/MSE-AMR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jackie Chun-Sing Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment-Aware Word and Sentence Level Pre-training for Sentiment Analysis. (arXiv:2210.09803v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09803","description":"<p>Most existing pre-trained language representation models (PLMs) are\nsub-optimal in sentiment analysis tasks, as they capture the sentiment\ninformation from word-level while under-considering sentence-level information.\nIn this paper, we propose SentiWSP, a novel Sentiment-aware pre-trained\nlanguage model with combined Word-level and Sentence-level Pre-training tasks.\nThe word level pre-training task detects replaced sentiment words, via a\ngenerator-discriminator framework, to enhance the PLM's knowledge about\nsentiment words. The sentence level pre-training task further strengthens the\ndiscriminator via a contrastive learning framework, with similar sentences as\nnegative samples, to encode sentiments in a sentence. Extensive experimental\nresults show that SentiWSP achieves new state-of-the-art performance on various\nsentence-level and aspect-level sentiment classification benchmarks. We have\nmade our code and model publicly available at\nhttps://github.com/XMUDM/SentiWSP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1\">Shuai Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haonan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhenghao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Specific Sub-network for Multi-Domain Neural Machine Translation. (arXiv:2210.09805v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09805","description":"<p>This paper presents Domain-Specific Sub-network (DoSS). It uses a set of\nmasks obtained through pruning to define a sub-network for each domain and\nfinetunes the sub-network parameters on domain data. This performs very closely\nand drastically reduces the number of parameters compared to finetuning the\nwhole network on each domain. Also a method to make masks unique per domain is\nproposed and shown to greatly improve the generalization to unseen domains. In\nour experiments on German to English machine translation the proposed method\noutperforms the strong baseline of continue training on multi-domain (medical,\ntech and religion) data by 1.47 BLEU points. Also continue training DoSS on new\ndomain (legal) outperforms the multi-domain (medical, tech, religion, legal)\nbaseline by 1.52 BLEU points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hendy_A/0/1/0/all/0/1\">Amr Hendy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelghaffar_M/0/1/0/all/0/1\">Mohamed Abdelghaffar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afify_M/0/1/0/all/0/1\">Mohamed Afify</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tawfik_A/0/1/0/all/0/1\">Ahmed Y. Tawfik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eye-tracking based classification of Mandarin Chinese readers with and without dyslexia using neural sequence models. (arXiv:2210.09819v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09819","description":"<p>Eye movements are known to reflect cognitive processes in reading, and\npsychological reading research has shown that eye gaze patterns differ between\nreaders with and without dyslexia. In recent years, researchers have attempted\nto classify readers with dyslexia based on their eye movements using Support\nVector Machines (SVMs). However, these approaches (i) are based on highly\naggregated features averaged over all words read by a participant, thus\ndisregarding the sequential nature of the eye movements, and (ii) do not\nconsider the linguistic stimulus and its interaction with the reader's eye\nmovements. In the present work, we propose two simple sequence models that\nprocess eye movements on the entire stimulus without the need of aggregating\nfeatures across the sentence. Additionally, we incorporate the linguistic\nstimulus into the model in two ways -- contextualized word embeddings and\nmanually extracted linguistic features. The models are evaluated on a Mandarin\nChinese dataset containing eye movements from children with and without\ndyslexia. Our results show that (i) even for a logographic script such as\nChinese, sequence models are able to classify dyslexia on eye gaze sequences,\nreaching state-of-the-art performance, and (ii) incorporating the linguistic\nstimulus does not help to improve classification performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haller_P/0/1/0/all/0/1\">Patrick Haller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sauberli_A/0/1/0/all/0/1\">Andreas S&#xe4;uberli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiener_S/0/1/0/all/0/1\">Sarah Elisabeth Kiener</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jinger Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jager_L/0/1/0/all/0/1\">Lena J&#xe4;ger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph-Based Multilingual Label Propagation for Low-Resource Part-of-Speech Tagging. (arXiv:2210.09840v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09840","description":"<p>Part-of-Speech (POS) tagging is an important component of the NLP pipeline,\nbut many low-resource languages lack labeled data for training. An established\nmethod for training a POS tagger in such a scenario is to create a labeled\ntraining set by transferring from high-resource languages. In this paper, we\npropose a novel method for transferring labels from multiple high-resource\nsource to low-resource target languages. We formalize POS tag projection as\ngraph-based label propagation. Given translations of a sentence in multiple\nlanguages, we create a graph with words as nodes and alignment links as edges\nby aligning words for all language pairs. We then propagate node labels from\nsource to target using a Graph Neural Network augmented with transformer\nlayers. We show that our propagation creates training sets that allow us to\ntrain POS taggers for a diverse set of languages. When combined with enhanced\ncontextualized embeddings, our method achieves a new state-of-the-art for\nunsupervised POS tagging of low-resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imani_A/0/1/0/all/0/1\">Ayyoob Imani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Severini_S/0/1/0/all/0/1\">Silvia Severini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabet_M/0/1/0/all/0/1\">Masoud Jalili Sabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yvon_F/0/1/0/all/0/1\">Fran&#xe7;ois Yvon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taxonomy of Abstractive Dialogue Summarization: Scenarios, Approaches and Future Directions. (arXiv:2210.09894v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09894","description":"<p>Abstractive dialogue summarization is to generate a concise and fluent\nsummary covering the salient information in a dialogue among two or more\ninterlocutors. It has attracted great attention in recent years based on the\nmassive emergence of social communication platforms and an urgent requirement\nfor efficient dialogue information understanding and digestion. Different from\nnews or articles in traditional document summarization, dialogues bring unique\ncharacteristics and additional challenges, including different language styles\nand formats, scattered information, flexible discourse structures and unclear\ntopic boundaries. This survey provides a comprehensive investigation on\nexisting work for abstractive dialogue summarization from scenarios, approaches\nto evaluations. It categorizes the task into two broad categories according to\nthe type of input dialogues, i.e., open-domain and task-oriented, and presents\na taxonomy of existing techniques in three directions, namely, injecting\ndialogue features, designing auxiliary training tasks and using additional\ndata.A list of datasets under different scenarios and widely-accepted\nevaluation metrics are summarized for completeness. After that, the trends of\nscenarios and techniques are summarized, together with deep insights on\ncorrelations between extensively exploited features and different scenarios.\nBased on these analyses, we recommend future directions including more\ncontrolled and complicated scenarios, technical innovations and comparisons,\npublicly available datasets in special domains, etc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Q/0/1/0/all/0/1\">Qi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Siyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yizhu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kenny Q. Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAPO: An Adaptive Ranking Paradigm for Bilingual Lexicon Induction. (arXiv:2210.09926v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09926","description":"<p>Bilingual lexicon induction induces the word translations by aligning\nindependently trained word embeddings in two languages. Existing approaches\ngenerally focus on minimizing the distances between words in the aligned pairs,\nwhile suffering from low discriminative capability to distinguish the relative\norders between positive and negative candidates. In addition, the mapping\nfunction is globally shared by all words, whose performance might be hindered\nby the deviations in the distributions of different languages. In this work, we\npropose a novel ranking-oriented induction model RAPO to learn personalized\nmapping function for each word. RAPO is capable of enjoying the merits from the\nunique characteristics of a single word and the cross-language isomorphism\nsimultaneously. Extensive experimental results on public datasets including\nboth rich-resource and low-resource languages demonstrate the superiority of\nour proposal. Our code is publicly available in\n\\url{https://github.com/Jlfj345wf/RAPO}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhoujin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chaozhuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuo Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_Z/0/1/0/all/0/1\">Zhiqiang Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zengxuan Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinyue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haizhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1\">Denvy Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making Science Simple: Corpora for the Lay Summarisation of Scientific Literature. (arXiv:2210.09932v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09932","description":"<p>Lay summarisation aims to jointly summarise and simplify a given text, thus\nmaking its content more comprehensible to non-experts. Automatic approaches for\nlay summarisation can provide significant value in broadening access to\nscientific literature, enabling a greater degree of both interdisciplinary\nknowledge sharing and public understanding when it comes to research findings.\nHowever, current corpora for this task are limited in their size and scope,\nhindering the development of broadly applicable data-driven approaches. Aiming\nto rectify these issues, we present two novel lay summarisation datasets, PLOS\n(large-scale) and eLife (medium-scale), each of which contains biomedical\njournal articles alongside expert-written lay summaries. We provide a thorough\ncharacterisation of our lay summaries, highlighting differing levels of\nreadability and abstractiveness between datasets that can be leveraged to\nsupport the needs of different applications. Finally, we benchmark our datasets\nusing mainstream summarisation approaches and perform a manual evaluation with\ndomain experts, demonstrating their utility and casting light on the key\nchallenges of this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goldsack_T/0/1/0/all/0/1\">Tomas Goldsack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple and Effective Method to Improve Zero-Shot Cross-Lingual Transfer Learning. (arXiv:2210.09934v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09934","description":"<p>Existing zero-shot cross-lingual transfer methods rely on parallel corpora or\nbilingual dictionaries, which are expensive and impractical for low-resource\nlanguages. To disengage from these dependencies, researchers have explored\ntraining multilingual models on English-only resources and transferring them to\nlow-resource languages. However, its effect is limited by the gap between\nembedding clusters of different languages. To address this issue, we propose\nEmbedding-Push, Attention-Pull, and Robust targets to transfer English\nembeddings to virtual multilingual embeddings without semantic loss, thereby\nimproving cross-lingual transferability. Experimental results on mBERT and\nXLM-R demonstrate that our method significantly outperforms previous works on\nthe zero-shot cross-lingual text classification task and can obtain a better\nmultilingual alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1\">Kunbo Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weijie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuejian Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Weiquan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haoyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_R/0/1/0/all/0/1\">Rong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiren Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer-learning for video classification: Video Swin Transformer on multiple domains. (arXiv:2210.09969v1 [cs.CV])","link":"http://arxiv.org/abs/2210.09969","description":"<p>The computer vision community has seen a shift from convolutional-based to\npure transformer architectures for both image and video tasks. Training a\ntransformer from zero for these tasks usually requires a lot of data and\ncomputational resources. Video Swin Transformer (VST) is a pure-transformer\nmodel developed for video classification which achieves state-of-the-art\nresults in accuracy and efficiency on several datasets. In this paper, we aim\nto understand if VST generalizes well enough to be used in an out-of-domain\nsetting. We study the performance of VST on two large-scale datasets, namely\nFCVID and Something-Something using a transfer learning approach from\nKinetics-400, which requires around 4x less memory than training from scratch.\nWe then break down the results to understand where VST fails the most and in\nwhich scenarios the transfer-learning approach is viable. Our experiments show\nan 85\\% top-1 accuracy on FCVID without retraining the whole model which is\nequal to the state-of-the-art for the dataset and a 21\\% accuracy on\nSomething-Something. The experiments also suggest that the performance of the\nVST decreases on average when the video duration increases which seems to be a\nconsequence of a design choice of the model. From the results, we conclude that\nVST generalizes well enough to classify out-of-domain videos without retraining\nwhen the target classes are from the same type as the classes used to train the\nmodel. We observed this effect when we performed transfer-learning from\nKinetics-400 to FCVID, where most datasets target mostly objects. On the other\nhand, if the classes are not from the same type, then the accuracy after the\ntransfer-learning approach is expected to be poor. We observed this effect when\nwe performed transfer-learning from Kinetics-400, where the classes represent\nmostly objects, to Something-Something, where the classes represent mostly\nactions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1\">Daniel Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matos_D/0/1/0/all/0/1\">David Martins de Matos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Information Content of Predictions in Word Analogy Tests. (arXiv:2210.09972v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09972","description":"<p>An approach is proposed to quantify, in bits of information, the actual\nrelevance of analogies in analogy tests. The main component of this approach is\na softaccuracy estimator that also yields entropy estimates with compensated\nbiases. Experimental results obtained with pre-trained GloVe 300-D vectors and\ntwo public analogy test sets show that proximity hints are much more relevant\nthan analogies in analogy tests, from an information content perspective.\nAccordingly, a simple word embedding model is used to predict that analogies\ncarry about one bit of information, which is experimentally corroborated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Montalvao_J/0/1/0/all/0/1\">Jugurta Montalv&#xe3;o</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making a MIRACL: Multilingual Information Retrieval Across a Continuum of Languages. (arXiv:2210.09984v1 [cs.IR])","link":"http://arxiv.org/abs/2210.09984","description":"<p>MIRACL (Multilingual Information Retrieval Across a Continuum of Languages)\nis a multilingual dataset we have built for the WSDM 2023 Cup challenge that\nfocuses on ad hoc retrieval across 18 different languages, which collectively\nencompass over three billion native speakers around the world. These languages\nhave diverse typologies, originate from many different language families, and\nare associated with varying amounts of available resources -- including what\nresearchers typically characterize as high-resource as well as low-resource\nlanguages. Our dataset is designed to support the creation and evaluation of\nmodels for monolingual retrieval, where the queries and the corpora are in the\nsame language. In total, we have gathered over 700k high-quality relevance\njudgments for around 77k queries over Wikipedia in these 18 languages, where\nall assessments have been performed by native speakers hired by our team. Our\ngoal is to spur research that will improve retrieval across a continuum of\nlanguages, thus enhancing information access capabilities for diverse\npopulations around the world, particularly those that have been traditionally\nunderserved. This overview paper describes the dataset and baselines that we\nshare with the community. The MIRACL website is live at <a href=\"http://miracl.ai/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nandan Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogundepo_O/0/1/0/all/0/1\">Odunayo Ogundepo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamalloo_E/0/1/0/all/0/1\">Ehsan Kamalloo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfonso_Hermelo_D/0/1/0/all/0/1\">David Alfonso-Hermelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Post-hoc analysis of Arabic transformer models. (arXiv:2210.09990v1 [cs.CL])","link":"http://arxiv.org/abs/2210.09990","description":"<p>Arabic is a Semitic language which is widely spoken with many dialects. Given\nthe success of pre-trained language models, many transformer models trained on\nArabic and its dialects have surfaced. While there have been an extrinsic\nevaluation of these models with respect to downstream NLP tasks, no work has\nbeen carried out to analyze and compare their internal representations. We\nprobe how linguistic information is encoded in the transformer models, trained\non different Arabic dialects. We perform a layer and neuron analysis on the\nmodels using morphological tagging tasks for different dialects of Arabic and a\ndialectal identification task. Our analysis enlightens interesting findings\nsuch as: i) word morphology is learned at the lower and middle layers, ii)\nwhile syntactic dependencies are predominantly captured at the higher layers,\niii) despite a large overlap in their vocabulary, the MSA-based models fail to\ncapture the nuances of Arabic dialects, iv) we found that neurons in embedding\nlayers are polysemous in nature, while the neurons in middle layers are\nexclusive to specific properties\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelali_A/0/1/0/all/0/1\">Ahmed Abdelali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1\">Nadir Durrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_F/0/1/0/all/0/1\">Fahim Dalvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sajjad_H/0/1/0/all/0/1\">Hassan Sajjad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear Guardedness and its Implications. (arXiv:2210.10012v1 [cs.LG])","link":"http://arxiv.org/abs/2210.10012","description":"<p>Previous work on concept identification in neural representations has focused\non linear concept subspaces and their neutralization. In this work, we\nformulate the notion of linear guardedness -- the inability to directly predict\na given concept from the representation -- and study its implications. We show\nthat, in the binary case, the neutralized concept cannot be recovered by an\nadditional linear layer. However, we point out that -- contrary to what was\nimplicitly argued in previous works -- multiclass softmax classifiers can be\nconstructed that indirectly recover the concept. Thus, linear guardedness does\nnot guarantee that linear classifiers do not utilize the neutralized concepts,\nshedding light on theoretical limitations of linear information removal\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ULN: Towards Underspecified Vision-and-Language Navigation. (arXiv:2210.10020v1 [cs.CV])","link":"http://arxiv.org/abs/2210.10020","description":"<p>Vision-and-Language Navigation (VLN) is a task to guide an embodied agent\nmoving to a target position using language instructions. Despite the\nsignificant performance improvement, the wide use of fine-grained instructions\nfails to characterize more practical linguistic variations in reality. To fill\nin this gap, we introduce a new setting, namely Underspecified\nvision-and-Language Navigation (ULN), and associated evaluation datasets. ULN\nevaluates agents using multi-level underspecified instructions instead of\npurely fine-grained or coarse-grained, which is a more realistic and general\nsetting. As a primary step toward ULN, we propose a VLN framework that consists\nof a classification module, a navigation agent, and an\nExploitation-to-Exploration (E2E) module. Specifically, we propose to learn\nGranularity Specific Sub-networks (GSS) for the agent to ground multi-level\ninstructions with minimal additional parameters. Then, our E2E module estimates\ngrounding uncertainty and conducts multi-step lookahead exploration to improve\nthe success rate further. Experimental results show that existing VLN models\nare still brittle to multi-level language underspecification. Our framework is\nmore robust and outperforms the baselines on ULN by ~10% relative success rate\nacross all levels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Weixi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1\">Tsu-Jui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Maestro-U: Leveraging joint speech-text representation learning for zero supervised speech ASR. (arXiv:2210.10027v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10027","description":"<p>Training state-of-the-art Automated Speech Recognition (ASR) models typically\nrequires a substantial amount of transcribed speech. In this work, we\ndemonstrate that a modality-matched joint speech and text model can be\nleveraged to train a massively multilingual ASR model without any supervised\n(manually transcribed) speech for some languages. This paper explores the use\nof jointly learnt speech and text representations in a massively multilingual,\nzero supervised speech, real-world setting to expand the set of languages\ncovered by ASR with only unlabeled speech and text in the target languages.\nUsing the FLEURS dataset, we define the task to cover $102$ languages, where\ntranscribed speech is available in $52$ of these languages and can be used to\nimprove end-to-end ASR quality on the remaining $50$. First, we show that by\ncombining speech representations with byte-level text representations and use\nof language embeddings, we can dramatically reduce the Character Error Rate\n(CER) on languages with no supervised speech from 64.8\\% to 30.8\\%, a relative\nreduction of 53\\%. Second, using a subset of South Asian languages we show that\nMaestro-U can promote knowledge transfer from languages with supervised speech\neven when there is limited to no graphemic overlap. Overall, Maestro-U closes\nthe gap to oracle performance by 68.5\\% relative and reduces the CER of 19\nlanguages below 15\\%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhehuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_A/0/1/0/all/0/1\">Andrew Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramabhadran_B/0/1/0/all/0/1\">Bhuvana Ramabhadran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_P/0/1/0/all/0/1\">Pedro Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nanxin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding COVID-19 Vaccine Campaign on Facebook using Minimal Supervision. (arXiv:2210.10031v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10031","description":"<p>In the age of social media, where billions of internet users share\ninformation and opinions, the negative impact of pandemics is not limited to\nthe physical world. It provokes a surge of incomplete, biased, and incorrect\ninformation, also known as an infodemic. This global infodemic jeopardizes\nmeasures to control the pandemic by creating panic, vaccine hesitancy, and\nfragmented social response. Platforms like Facebook allow advertisers to adapt\ntheir messaging to target different demographics and help alleviate or\nexacerbate the infodemic problem depending on their content. In this paper, we\npropose a minimally supervised multi-task learning framework for understanding\nmessaging on Facebook related to the covid vaccine by identifying ad themes and\nmoral foundations. Furthermore, we perform a more nuanced thematic analysis of\nmessaging tactics of vaccine campaigns on social media so that policymakers can\nmake better decisions on pandemic control.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tunazzina Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks. (arXiv:2210.10040v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10040","description":"<p>How reliably can we trust the scores obtained from social bias benchmarks as\nfaithful indicators of problematic social biases in a given language model? In\nthis work, we study this question by contrasting social biases with non-social\nbiases stemming from choices made during dataset construction that might not\neven be discernible to the human eye. To do so, we empirically simulate various\nalternative constructions for a given benchmark based on innocuous\nmodifications (such as paraphrasing or random-sampling) that maintain the\nessence of their social bias. On two well-known social bias benchmarks\n(Winogender and BiasNLI) we observe that these shallow modifications have a\nsurprising effect on the resulting degree of bias across various models. We\nhope these troubling observations motivate more robust measures of social\nbiases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Selvam_N/0/1/0/all/0/1\">Nikil Roashan Selvam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1\">Tushar Khot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hidden State Variability of Pretrained Language Models Can Guide Computation Reduction for Transfer Learning. (arXiv:2210.10041v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10041","description":"<p>While transferring a pretrained language model, common approaches\nconventionally attach their task-specific classifiers to the top layer and\nadapt all the pretrained layers. We investigate whether one could make a\ntask-specific selection on which subset of the layers to adapt and where to\nplace the classifier. The goal is to reduce the computation cost of transfer\nlearning methods (e.g. fine-tuning or adapter-tuning) without sacrificing its\nperformance.\n</p>\n<p>We propose to select layers based on the variability of their hidden states\ngiven a task-specific corpus. We say a layer is already ``well-specialized'' in\na task if the within-class variability of its hidden states is low relative to\nthe between-class variability. Our variability metric is cheap to compute and\ndoesn't need any training or hyperparameter tuning. It is robust to data\nimbalance and data scarcity. Extensive experiments on the GLUE benchmark\ndemonstrate that selecting layers based on our metric can yield significantly\nstronger performance than using the same number of top layers and often match\nthe performance of fine-tuning or adapter-tuning the entire language model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shuo Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jiahao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasad_A/0/1/0/all/0/1\">Ankita Pasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Li Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Q/0/1/0/all/0/1\">Qing Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1\">Hongyuan Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SafeText: A Benchmark for Exploring Physical Safety in Language Models. (arXiv:2210.10045v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10045","description":"<p>Understanding what constitutes safe text is an important issue in natural\nlanguage processing and can often prevent the deployment of models deemed\nharmful and unsafe. One such type of safety that has been scarcely studied is\ncommonsense physical safety, i.e. text that is not explicitly violent and\nrequires additional commonsense knowledge to comprehend that it leads to\nphysical harm. We create the first benchmark dataset, SafeText, comprising\nreal-life scenarios with paired safe and physically unsafe pieces of advice. We\nutilize SafeText to empirically study commonsense physical safety across\nvarious models designed for text generation and commonsense reasoning tasks. We\nfind that state-of-the-art large language models are susceptible to the\ngeneration of unsafe text and have difficulty rejecting unsafe advice. As a\nresult, we argue for further studies of safety and the assessment of\ncommonsense physical safety in models before release.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Sharon Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allaway_E/0/1/0/all/0/1\">Emily Allaway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subbiah_M/0/1/0/all/0/1\">Melanie Subbiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chilton_L/0/1/0/all/0/1\">Lydia Chilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patton_D/0/1/0/all/0/1\">Desmond Patton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisit Systematic Generalization via Meaningful Learning. (arXiv:2003.06658v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2003.06658","description":"<p>Humans can systematically generalize to novel compositions of existing\nconcepts. Recent studies argue that neural networks appear inherently\nineffective in such cognitive capacity, leading to a pessimistic view and a\nlack of attention to optimistic results. We revisit this controversial topic\nfrom the perspective of meaningful learning, an exceptional capability of\nhumans to learn novel concepts by connecting them with known ones. We reassess\nthe compositional skills of sequence-to-sequence models conditioned on the\nsemantic links between new and old concepts. Our observations suggest that\nmodels can successfully one-shot generalize to novel concepts and compositions\nthrough semantic linking, either inductively or deductively. We demonstrate\nthat prior knowledge plays a key role as well. In addition to synthetic tests,\nwe further conduct proof-of-concept experiments in machine translation and\nsemantic parsing, showing the benefits of meaningful learning in applications.\nWe hope our positive findings will encourage excavating modern neural networks'\npotential in systematic generalization through more advanced learning schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_N/0/1/0/all/0/1\">Ning Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntactic structures and the general Markov models. (arXiv:2104.08462v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08462","description":"<p>We study phylogenetic signal present in syntactic information by considering\nthe syntactic structures data from Longobardi (2017b), Collins (2010), Ceolin\net al. (2020) and Koopman (2011). Focusing first on the general Markov models,\nwe explore how well the the syntactic structures data conform to the hypothesis\nrequired by these models. We do this by comparing derived phylogenetic trees\nagainst trees agreed on by the linguistics community. We then interpret the\nmethods of Ceolin et al. (2020) as an infinite sites evolutionary model and\ncompare the consistency of the data with this alternative. The ideas and\nmethods discussed in the present paper are more generally applicable than to\nthe specific setting of syntactic structures, and can be used in other\ncontexts, when analyzing consistency of data with against hypothesized\nevolutionary models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gakkhar_S/0/1/0/all/0/1\">Sitanshu Gakkhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcolli_M/0/1/0/all/0/1\">Matilde Marcolli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NSP-BERT: A Prompt-based Few-Shot Learner Through an Original Pre-training Task--Next Sentence Prediction. (arXiv:2109.03564v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03564","description":"<p>Using prompts to utilize language models to perform various downstream tasks,\nalso known as prompt-based learning or prompt-learning, has lately gained\nsignificant success in comparison to the pre-train and fine-tune paradigm.\nNonetheless, virtually all prompt-based methods are token-level, meaning they\nall utilize GPT's left-to-right language model or BERT's masked language model\nto perform cloze-style tasks. In this paper, we attempt to accomplish several\nNLP tasks in the zero-shot scenario using a BERT original pre-training task\nabandoned by RoBERTa and other models--Next Sentence Prediction (NSP). Unlike\ntoken-level techniques, our sentence-level prompt-based method NSP-BERT does\nnot need to fix the length of the prompt or the position to be predicted,\nallowing it to handle tasks such as entity linking with ease. Based on the\ncharacteristics of NSP-BERT, we offer several quick building templates for\nvarious downstream tasks. We suggest a two-stage prompt method for word sense\ndisambiguation tasks in particular. Our strategies for mapping the labels\nsignificantly enhance the model's performance on sentence pair tasks. On the\nFewCLUE benchmark, our NSP-BERT outperforms other zero-shot methods on most of\nthese tasks and comes close to the few-shot methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1\">Chao Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Hangping Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Contextual Toxicity Detection in Conversations. (arXiv:2111.12447v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.12447","description":"<p>Understanding toxicity in user conversations is undoubtedly an important\nproblem. Addressing \"covert\" or implicit cases of toxicity is particularly hard\nand requires context. Very few previous studies have analysed the influence of\nconversational context in human perception or in automated detection models. We\ndive deeper into both these directions. We start by analysing existing\ncontextual datasets and come to the conclusion that toxicity labelling by\nhumans is in general influenced by the conversational structure, polarity and\ntopic of the context. We then propose to bring these findings into\ncomputational detection models by introducing and evaluating (a) neural\narchitectures for contextual toxicity detection that are aware of the\nconversational structure, and (b) data augmentation strategies that can help\nmodel contextual toxicity detection. Our results have shown the encouraging\npotential of neural architectures that are aware of the conversation structure.\nWe have also demonstrated that such models can benefit from synthetic data,\nespecially in the social media domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anuchitanukul_A/0/1/0/all/0/1\">Atijit Anuchitanukul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ive_J/0/1/0/all/0/1\">Julia Ive</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models. (arXiv:2201.05966v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05966","description":"<p>Structured knowledge grounding (SKG) leverages structured knowledge to\ncomplete user requests, such as semantic parsing over databases and question\nanswering over knowledge bases. Since the inputs and outputs of SKG tasks are\nheterogeneous, they have been studied separately by different communities,\nwhich limits systematic and compatible research on SKG. In this paper, we\novercome this limitation by proposing the UnifiedSKG framework, which unifies\n21 SKG tasks into a text-to-text format, aiming to promote systematic SKG\nresearch, instead of being exclusive to a single task, domain, or dataset. We\nuse UnifiedSKG to benchmark T5 with different sizes and show that T5, with\nsimple modifications when necessary, achieves state-of-the-art performance on\nalmost all of the 21 tasks. We further demonstrate that multi-task\nprefix-tuning improves the performance on most tasks, largely improving the\noverall performance. UnifiedSKG also facilitates the investigation of zero-shot\nand few-shot learning, and we show that T0, GPT-3, and Codex struggle in\nzero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct a\nseries of controlled experiments on structured knowledge encoding variants\nacross SKG tasks. UnifiedSKG is easily extensible to more tasks, and it is\nopen-sourced at https://github.com/hkunlp/unifiedskg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tianbao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Henry Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Peng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholak_T/0/1/0/all/0/1\">Torsten Scholak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Pengcheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sida I. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_V/0/1/0/all/0/1\">Victor Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bailin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengzu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyle_C/0/1/0/all/0/1\">Connor Boyle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1\">Ansong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Ziyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments. (arXiv:2202.06387v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.06387","description":"<p>Neural scaling laws define a predictable relationship between a model's\nparameter count and its performance after training in the form of a power law.\nHowever, most research to date has not explicitly investigated whether scaling\nlaws can be used to accelerate model development. In this work, we perform such\nan empirical investigation across a wide range of language understanding tasks,\nstarting from models with as few as 10K parameters, and evaluate downstream\nperformance across 9 language understanding tasks. We find that scaling laws\nemerge at finetuning time in some NLP tasks, and that they can also be\nexploited for debugging convergence when training large models. Moreover, for\ntasks where scaling laws exist, they can be used to predict the performance of\nlarger models, which enables effective model selection. However, revealing\nscaling laws requires careful hyperparameter tuning and multiple runs for the\npurpose of uncertainty estimation, which incurs additional overhead, partially\noffsetting the computational benefits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivgi_M/0/1/0/all/0/1\">Maor Ivgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmon_Y/0/1/0/all/0/1\">Yair Carmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiteTransformerSearch: Training-free Neural Architecture Search for Efficient Language Models. (arXiv:2203.02094v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.02094","description":"<p>The Transformer architecture is ubiquitously used as the building block of\nlarge-scale autoregressive language models. However, finding architectures with\nthe optimal trade-off between task performance (perplexity) and hardware\nconstraints like peak memory utilization and latency is non-trivial. This is\nexacerbated by the proliferation of various hardware. We leverage the somewhat\nsurprising empirical observation that the number of decoder parameters in\nautoregressive Transformers has a high rank correlation with task performance,\nirrespective of the architecture topology. This observation organically induces\na simple Neural Architecture Search (NAS) algorithm that uses decoder\nparameters as a proxy for perplexity without need for any model training. The\nsearch phase of our training-free algorithm, dubbed Lightweight Transformer\nSearch (LTS), can be run directly on target devices since it does not require\nGPUs. Using on-target-device measurements, LTS extracts the Pareto-frontier of\nperplexity versus any hardware performance cost. We evaluate LTS on diverse\ndevices from ARM CPUs to NVIDIA GPUs and two popular autoregressive Transformer\nbackbones: GPT-2 and Transformer-XL. Results show that the perplexity of\n16-layer GPT-2 and Transformer-XL can be achieved with up to 1.5x, 2.5x faster\nruntime and 1.2x, 2.0x lower peak memory utilization. When evaluated in zero\nand one-shot settings, LTS Pareto-frontier models achieve higher average\naccuracy compared to the 350M parameter OPT across 14 tasks, with up to 1.6x\nlower latency. LTS extracts the Pareto-frontier in under 3 hours while running\non a commodity laptop. We effectively remove the carbon footprint of hundreds\nof GPU hours of training during search, offering a strong simple baseline for\nfuture NAS methods in autoregressive language modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Javaheripi_M/0/1/0/all/0/1\">Mojan Javaheripi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosa_G/0/1/0/all/0/1\">Gustavo H. de Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Shital Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Religa_T/0/1/0/all/0/1\">Tomasz L. Religa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendes_C/0/1/0/all/0/1\">Caio C. T. Mendes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1\">Sebastien Bubeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koushanfar_F/0/1/0/all/0/1\">Farinaz Koushanfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_D/0/1/0/all/0/1\">Debadeepta Dey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models. (arXiv:2203.07259v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07259","description":"<p>Transformer-based language models have become a key building block for\nnatural language processing. While these models are extremely accurate, they\ncan be too large and computationally intensive to run on standard deployments.\nA variety of compression methods, including distillation, quantization,\nstructured and unstructured pruning are known to decrease model size and\nincrease inference speed, with low accuracy loss. In this context, this paper's\ncontributions are two-fold. We perform an in-depth study of the\naccuracy-compression trade-off for unstructured weight pruning of BERT models.\nWe introduce Optimal BERT Surgeon (oBERT), an efficient and accurate weight\npruning method based on approximate second-order information, which we show to\nyield state-of-the-art results in both stages of language tasks: pre-training\nand fine-tuning. Specifically, oBERT extends existing work on unstructured\nsecond-order pruning by allowing for pruning blocks of weights, and by being\napplicable at the BERT scale. Second, we investigate the impact of this pruning\nmethod when compounding compression approaches to obtain highly compressed but\naccurate models for deployment on edge devices. These models significantly push\nboundaries of the current state-of-the-art sparse BERT models with respect to\nall metrics: model size, inference speed and task accuracy. For example,\nrelative to the dense BERT-base, we obtain 10x model size compression (in MB)\nwith &lt; 1% accuracy drop, 10x CPU-inference speedup with &lt; 2% accuracy drop, and\n29x CPU-inference speedup with &lt; 7.5% accuracy drop. Our code, fully integrated\nwith Transformers and SparseML, is available at\nhttps://github.com/neuralmagic/sparseml/tree/main/research/optimal_BERT_surgeon_oBERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurtic_E/0/1/0/all/0/1\">Eldar Kurtic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_D/0/1/0/all/0/1\">Daniel Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frantar_E/0/1/0/all/0/1\">Elias Frantar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtz_M/0/1/0/all/0/1\">Mark Kurtz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fineran_B/0/1/0/all/0/1\">Benjamin Fineran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goin_M/0/1/0/all/0/1\">Michael Goin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1\">Dan Alistarh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperdecoders: Instance-specific decoders for multi-task NLP. (arXiv:2203.08304v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08304","description":"<p>We investigate input-conditioned hypernetworks for multi-tasking in NLP,\ngenerating parameter-efficient adaptations for a decoder using a hypernetwork\nconditioned on the output of an encoder. This approach produces a unique\ndecoder adaptation for every input instance, allowing the network a larger\ndegree of flexibility than prior work that only produces one decoder adaptation\nper task. We apply our method to sequence classification tasks, extractive QA,\nand summarisation and find that it surpasses previous parameter efficient\nfine-tuning methods and often outperforms fully finetuning the underlying\nmodel. An analysis of the embeddings used by our hypernetwork shows that they\nare sensitive to output label and type, suggesting that our approach better\nmaps from encoder representations to output labels. Our code is publicly\navailable at https://github.com/allenai/hyperdecoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivison_H/0/1/0/all/0/1\">Hamish Ivison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1\">Matthew E. Peters</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning. (arXiv:2204.06487v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06487","description":"<p>Multilingual pre-trained language models (PLMs) have demonstrated impressive\nperformance on several downstream tasks for both high-resourced and\nlow-resourced languages. However, there is still a large performance drop for\nlanguages unseen during pre-training, especially African languages. One of the\nmost effective approaches to adapt to a new language is \\textit{language\nadaptive fine-tuning} (LAFT) -- fine-tuning a multilingual PLM on monolingual\ntexts of a language using the pre-training objective. However, adapting to a\ntarget language individually takes a large disk space and limits the\ncross-lingual transfer abilities of the resulting models because they have been\nspecialized for a single language. In this paper, we perform\n\\textit{multilingual adaptive fine-tuning} on 17 most-resourced African\nlanguages and three other high-resource languages widely spoken on the African\ncontinent to encourage cross-lingual transfer learning. To further specialize\nthe multilingual PLM, we removed vocabulary tokens from the embedding layer\nthat corresponds to non-African writing scripts before MAFT, thus reducing the\nmodel size by around 50%. Our evaluation on two multilingual PLMs (AfriBERTa\nand XLM-R) and three NLP tasks (NER, news topic classification, and sentiment\nclassification) shows that our approach is competitive to applying LAFT on\nindividual languages while requiring significantly less disk space.\nAdditionally, we show that our adapted PLM also improves the zero-shot\ncross-lingual transfer abilities of parameter efficient fine-tuning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba O. Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosbach_M/0/1/0/all/0/1\">Marius Mosbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Cross-Task Generalization via Retrieval Augmentation. (arXiv:2204.07937v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07937","description":"<p>Humans can perform unseen tasks by recalling relevant skills acquired\npreviously and then generalizing them to the target tasks, even if there is no\nsupervision at all. In this paper, we aim to improve this kind of cross-task\ngeneralization ability of massive multi-task language models, such as T0 and\nFLAN, in an unsupervised setting. We propose a retrieval-augmentation method\nnamed ReCross that takes a few unlabelled examples as queries to retrieve a\nsmall subset of upstream data and uses them to update the multi-task model for\nbetter generalization. ReCross is a straightforward yet effective retrieval\nmethod that combines both efficient dense retrieval and effective pair-wise\nreranking. Our results and analysis show that it significantly outperforms both\nnon-retrieval methods and other baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1\">Kangmin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_C/0/1/0/all/0/1\">Chris Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1\">Beiwen Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Climate Awareness in NLP Research. (arXiv:2205.05071v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.05071","description":"<p>The climate impact of AI, and NLP research in particular, has become a\nserious issue given the enormous amount of energy that is increasingly being\nused for training and running computational models. Consequently, increasing\nfocus is placed on efficient NLP. However, this important initiative lacks\nsimple guidelines that would allow for systematic climate reporting of NLP\nresearch. We argue that this deficiency is one of the reasons why very few\npublications in NLP report key figures that would allow a more thorough\nexamination of environmental impact. As a remedy, we propose a climate\nperformance model card with the primary purpose of being practically usable\nwith only limited information about experiments and the underlying computer\nhardware. We describe why this step is essential to increase awareness about\nthe environmental impact of NLP research and, thereby, paving the way for more\nthorough discussions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webersinke_N/0/1/0/all/0/1\">Nicolas Webersinke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraus_M/0/1/0/all/0/1\">Mathias Kraus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bingler_J/0/1/0/all/0/1\">Julia Anna Bingler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leippold_M/0/1/0/all/0/1\">Markus Leippold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recovering Private Text in Federated Learning of Language Models. (arXiv:2205.08514v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.08514","description":"<p>Federated learning allows distributed users to collaboratively train a model\nwhile keeping each user's data private. Recently, a growing body of work has\ndemonstrated that an eavesdropping attacker can effectively recover image data\nfrom gradients transmitted during federated learning. However, little progress\nhas been made in recovering text data. In this paper, we present a novel attack\nmethod FILM for federated learning of language models (LMs). For the first\ntime, we show the feasibility of recovering text from large batch sizes of up\nto 128 sentences. Unlike image-recovery methods that are optimized to match\ngradients, we take a distinct approach that first identifies a set of words\nfrom gradients and then directly reconstructs sentences based on beam search\nand a prior-based reordering strategy. We conduct the FILM attack on several\nlarge-scale datasets and show that it can successfully reconstruct single\nsentences with high fidelity for large batch sizes and even multiple sentences\nif applied iteratively. We evaluate three defense methods: gradient pruning,\nDPSGD, and a simple approach to freeze word embeddings that we propose. We show\nthat both gradient pruning and DPSGD lead to a significant drop in utility.\nHowever, if we fine-tune a public pre-trained LM on private text without\nupdating word embeddings, it can effectively defend the attack with minimal\ndata utility loss. Together, we hope that our results can encourage the\ncommunity to rethink the privacy concerns of LM training and its standard\npractices in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Samyak Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yangsibo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zexuan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Level Modeling Units for End-to-End Mandarin Speech Recognition. (arXiv:2205.11998v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11998","description":"<p>The choice of modeling units is crucial for automatic speech recognition\n(ASR) tasks. In mandarin scenarios, the Chinese characters represent meaning\nbut are not directly related to the pronunciation. Thus only considering the\nwriting of Chinese characters as modeling units is insufficient to capture\nspeech features. In this paper, we present a novel method involves with\nmulti-level modeling units, which integrates multi-level information for\nmandarin speech recognition. Specifically, the encoder block considers\nsyllables as modeling units and the decoder block deals with character-level\nmodeling units. To facilitate the incremental conversion from syllable features\nto character features, we design an auxiliary task that applies cross-entropy\n(CE) loss to intermediate decoder layers. During inference, the input feature\nsequences are converted into syllable sequences by the encoder block and then\nconverted into Chinese characters by the decoder block. Experiments on the\nwidely used AISHELL-1 corpus demonstrate that our method achieves promising\nresults with CER of 4.1%/4.6% and 4.6%/5.2%, using the Conformer and the\nTransformer backbones respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuting Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Binbin Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuke Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DivEMT: Neural Machine Translation Post-Editing Effort Across Typologically Diverse Languages. (arXiv:2205.12215v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12215","description":"<p>We introduce DivEMT, the first publicly available post-editing study of\nNeural Machine Translation (NMT) over a typologically diverse set of target\nlanguages. Using a strictly controlled setup, 18 professional translators were\ninstructed to translate or post-edit the same set of English documents into\nArabic, Dutch, Italian, Turkish, Ukrainian, and Vietnamese. During the process,\ntheir edits, keystrokes, editing times and pauses were recorded, enabling an\nin-depth, cross-lingual evaluation of NMT quality and post-editing\neffectiveness. Using this new dataset, we assess the impact of two\nstate-of-the-art NMT systems, Google Translate and the multilingual mBART-50\nmodel, on translation productivity. We find that post-editing is consistently\nfaster than translation from scratch. However, the magnitude of productivity\ngains varies widely across systems and languages, highlighting major\ndisparities in post-editing effectiveness for languages at different degrees of\ntypological relatedness to English, even when controlling for system\narchitecture and training data size. We publicly release the complete dataset\nincluding all collected behavioral data, to foster new research on the\ntranslation capabilities of NMT systems for typologically diverse languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarti_G/0/1/0/all/0/1\">Gabriele Sarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisazza_A/0/1/0/all/0/1\">Arianna Bisazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arenas_A/0/1/0/all/0/1\">Ana Guerberof Arenas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toral_A/0/1/0/all/0/1\">Antonio Toral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Natural Language Proofs with Verifier-Guided Search. (arXiv:2205.12443v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12443","description":"<p>Deductive reasoning over natural language is a challenging problem in NLP. In\nthis work, we focus on proof generation: Given a hypothesis and a set of\nsupporting facts, the model generates a proof tree indicating how to deduce the\nhypothesis from supporting facts. Compared to generating the entire proof in\none shot, stepwise generation can better exploit the compositionality and\ngeneralize to longer proofs but has achieved limited success on real-world\ndata. Existing stepwise methods struggle to generate proof steps that are both\nlogically valid and relevant to the hypothesis. Instead, they tend to\nhallucinate invalid steps given the hypothesis. In this paper, we present a\nnovel stepwise method, NLProofS (Natural Language Proof Search), which learns\nto generate relevant steps conditioning on the hypothesis. At the core of our\napproach, we train an independent verifier to check the validity of the proof\nsteps to prevent hallucination. Instead of generating steps greedily, we search\nfor proofs maximizing a global proof score judged by the verifier. NLProofS\nachieves state-of-the-art performance on EntailmentBank and RuleTaker.\nSpecifically, it improves the correctness of predicted proofs from 27.7% to\n33.3% in the distractor setting of EntailmentBank, demonstrating the\neffectiveness of NLProofS in generating challenging human-authored proofs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kaiyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jia Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Burst2Vec: An Adversarial Multi-Task Approach for Predicting Emotion, Age, and Origin from Vocal Bursts. (arXiv:2206.12469v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2206.12469","description":"<p>We present Burst2Vec, our multi-task learning approach to predict emotion,\nage, and origin (i.e., native country/language) from vocal bursts. Burst2Vec\nutilises pre-trained speech representations to capture acoustic information\nfrom raw waveforms and incorporates the concept of model debiasing via\nadversarial training. Our models achieve a relative 30 % performance gain over\nbaselines using pre-extracted features and score the highest amongst all\nparticipants in the ICML ExVo 2022 Multi-Task Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anuchitanukul_A/0/1/0/all/0/1\">Atijit Anuchitanukul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code Translation with Compiler Representations. (arXiv:2207.03578v3 [cs.PL] UPDATED)","link":"http://arxiv.org/abs/2207.03578","description":"<p>In this paper, we leverage low-level compiler intermediate representations\n(IR) to improve code translation. Traditional transpilers rely on syntactic\ninformation and handcrafted rules, which limits their applicability and\nproduces unnatural-looking code. Applying neural machine translation (NMT)\napproaches to code has successfully broadened the set of programs on which one\ncan get a natural-looking translation. However, they treat the code as\nsequences of text tokens, and still do not differentiate well enough between\nsimilar pieces of code which have different semantics in different languages.\nThe consequence is low quality translation, reducing the practicality of NMT,\nand stressing the need for an approach significantly increasing its accuracy.\nHere we propose to augment code translation with IRs, specifically LLVM IR,\nwith results on the C++, Java, Rust, and Go languages. Our method improves upon\nthe state of the art for unsupervised code translation, increasing the number\nof correct translations by 11% on average, and up to 79% for the Java -&gt; Rust\npair with greedy decoding. With beam search, it increases the number of correct\ntranslations by 5.5% in average. We extend previous test sets for code\ntranslation, by adding hundreds of Go and Rust functions. Additionally, we\ntrain models with high performance on the problem of IR decompilation,\ngenerating programming source code from IR, and study using IRs as intermediary\npivot for translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Szafraniec_M/0/1/0/all/0/1\">Marc Szafraniec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roziere_B/0/1/0/all/0/1\">Baptiste Roziere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leather_H/0/1/0/all/0/1\">Hugh Leather</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charton_F/0/1/0/all/0/1\">Francois Charton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labatut_P/0/1/0/all/0/1\">Patrick Labatut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STOP: A dataset for Spoken Task Oriented Semantic Parsing. (arXiv:2207.10643v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.10643","description":"<p>End-to-end spoken language understanding (SLU) predicts intent directly from\naudio using a single model. It promises to improve the performance of assistant\nsystems by leveraging acoustic information lost in the intermediate textual\nrepresentation and preventing cascading errors from Automatic Speech\nRecognition (ASR). Further, having one unified model has efficiency advantages\nwhen deploying assistant systems on-device. However, the limited number of\npublic audio datasets with semantic parse labels hinders the research progress\nin this area. In this paper, we release the Spoken Task-Oriented semantic\nParsing (STOP) dataset, the largest and most complex SLU dataset to be publicly\navailable. Additionally, we define low-resource splits to establish a benchmark\nfor improving SLU when limited labeled data is available. Furthermore, in\naddition to the human-recorded audio, we are releasing a TTS-generated version\nto benchmark the performance for low-resource domain adaptation of end-to-end\nSLU systems. Initial experimentation show end-to-end SLU models performing\nslightly worse than their cascaded counterparts, which we hope encourages\nfuture work in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tomasello_P/0/1/0/all/0/1\">Paden Tomasello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Akshat Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazar_D/0/1/0/all/0/1\">Daniel Lazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_P/0/1/0/all/0/1\">Po-Chun Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Adithya Sagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elkahky_A/0/1/0/all/0/1\">Ali Elkahky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Algayres_R/0/1/0/all/0/1\">Robin Algayres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu Ahn Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Where's the Learning in Representation Learning for Compositional Semantics and the Case of Thematic Fit. (arXiv:2208.04749v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.04749","description":"<p>Observing that for certain NLP tasks, such as semantic role prediction or\nthematic fit estimation, random embeddings perform as well as pretrained\nembeddings, we explore what settings allow for this and examine where most of\nthe learning is encoded: the word embeddings, the semantic role embeddings, or\n``the network''. We find nuanced answers, depending on the task and its\nrelation to the training objective. We examine these representation learning\naspects in multi-task learning, where role prediction and role-filling are\nsupervised tasks, while several thematic fit tasks are outside the models'\ndirect supervision. We observe a non-monotonous relation between some tasks'\nquality score and the training data size. In order to better understand this\nobservation, we analyze these results using easier, per-verb versions of these\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muthupari_M/0/1/0/all/0/1\">Mughilan Muthupari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halder_S/0/1/0/all/0/1\">Samrat Halder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayeed_A/0/1/0/all/0/1\">Asad Sayeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marton_Y/0/1/0/all/0/1\">Yuval Marton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Complementarity between Pre-Training and Random-Initialization for Resource-Rich Machine Translation. (arXiv:2209.03316v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.03316","description":"<p>Pre-Training (PT) of text representations has been successfully applied to\nlow-resource Neural Machine Translation (NMT). However, it usually fails to\nachieve notable gains (sometimes, even worse) on resource-rich NMT on par with\nits Random-Initialization (RI) counterpart. We take the first step to\ninvestigate the complementarity between PT and RI in resource-rich scenarios\nvia two probing analyses, and find that: 1) PT improves NOT the accuracy, but\nthe generalization by achieving flatter loss landscapes than that of RI; 2) PT\nimproves NOT the confidence of lexical choice, but the negative diversity by\nassigning smoother lexical probability distributions than that of RI. Based on\nthese insights, we propose to combine their complementarities with a model\nfusion algorithm that utilizes optimal transport to align neurons between PT\nand RI. Experiments on two resource-rich translation benchmarks, WMT'17\nEnglish-Chinese (20M) and WMT'19 English-German (36M), show that PT and RI\ncould be nicely complementary to each other, achieving substantial improvements\nconsidering both translation accuracy, generalization, and negative diversity.\nProbing tools and code are released at: https://github.com/zanchangtong/PTvsRI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zan_C/0/1/0/all/0/1\">Changtong Zan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information Extraction and Human-Robot Dialogue towards Real-life Tasks: A Baseline Study with the MobileCS Dataset. (arXiv:2209.13464v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.13464","description":"<p>Recently, there have merged a class of task-oriented dialogue (TOD) datasets\ncollected through Wizard-of-Oz simulated games. However, the Wizard-of-Oz data\nare in fact simulated data and thus are fundamentally different from real-life\nconversations, which are more noisy and casual. Recently, the SereTOD challenge\nis organized and releases the MobileCS dataset, which consists of real-world\ndialog transcripts between real users and customer-service staffs from China\nMobile. Based on the MobileCS dataset, the SereTOD challenge has two tasks, not\nonly evaluating the construction of the dialogue system itself, but also\nexamining information extraction from dialog transcripts, which is crucial for\nbuilding the knowledge base for TOD. This paper mainly presents a baseline\nstudy of the two tasks with the MobileCS dataset. We introduce how the two\nbaselines are constructed, the problems encountered, and the results. We\nanticipate that the baselines can facilitate exciting future research to build\nhuman-robot dialogue systems for real-life tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zhijian Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Junlan Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment. (arXiv:2210.01478v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.01478","description":"<p>AI systems are becoming increasingly intertwined with human life. In order to\neffectively collaborate with humans and ensure safety, AI systems need to be\nable to understand, interpret and predict human moral judgments and decisions.\nHuman moral judgments are often guided by rules, but not always. A central\nchallenge for AI safety is capturing the flexibility of the human moral mind --\nthe ability to determine when a rule should be broken, especially in novel or\nunusual situations. In this paper, we present a novel challenge set consisting\nof rule-breaking question answering (RBQA) of cases that involve potentially\npermissible rule-breaking -- inspired by recent moral psychology studies. Using\na state-of-the-art large language model (LLM) as a basis, we propose a novel\nmoral chain of thought (MORALCOT) prompting strategy that combines the\nstrengths of LLMs with theories of moral reasoning developed in cognitive\nscience to predict human moral judgments. MORALCOT outperforms seven existing\nLLMs by 6.2% F1, suggesting that modeling human reasoning might be necessary to\ncapture the flexibility of the human moral mind. We also conduct a detailed\nerror analysis to suggest directions for future work to improve AI safety using\nRBQA. Our data and code are available at https://github.com/feradauto/MoralCoT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sydney Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_F/0/1/0/all/0/1\">Fernando Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamal_O/0/1/0/all/0/1\">Ojasv Kamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Josh Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Entity Typing with Curriculum Learning. (arXiv:2210.02914v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.02914","description":"<p>Entity typing aims to assign types to the entity mentions in given texts. The\ntraditional classification-based entity typing paradigm has two unignorable\ndrawbacks: 1) it fails to assign an entity to the types beyond the predefined\ntype set, and 2) it can hardly handle few-shot and zero-shot situations where\nmany long-tail types only have few or even no training instances. To overcome\nthese drawbacks, we propose a novel generative entity typing (GET) paradigm:\ngiven a text with an entity mention, the multiple types for the role that the\nentity plays in the text are generated with a pre-trained language model (PLM).\nHowever, PLMs tend to generate coarse-grained types after fine-tuning upon the\nentity typing dataset. Besides, we only have heterogeneous training data\nconsisting of a small portion of human-annotated data and a large portion of\nauto-generated but low-quality data. To tackle these problems, we employ\ncurriculum learning (CL) to train our GET model upon the heterogeneous data,\nwhere the curriculum could be self-adjusted with the self-paced learning\naccording to its comprehension of the type granularity and data heterogeneity.\nOur extensive experiments upon the datasets of different languages and\ndownstream tasks justify the superiority of our GET model over the\nstate-of-the-art entity typing models. The code has been released on\nhttps://github.com/siyuyuan/GET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Siyu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Deqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiaqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinxi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jingyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PQLM -- Multilingual Decentralized Portable Quantum Language Model for Privacy Protection. (arXiv:2210.03221v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.03221","description":"<p>With careful manipulation, malicious agents can reverse engineer private\ninformation encoded in pre-trained language models. Security concerns motivate\nthe development of quantum pre-training. In this work, we propose a highly\nportable quantum language model (PQLM) that can be easily transferred to\ndownstream tasks on classical machines. The framework consists of a cloud PQLM\nbuilt with random Variational Quantum Classifiers (VQC) and local models for\ndownstream applications. We demonstrate the portability of the quantum model by\nextracting only the word embeddings and effectively applying them to downstream\ntasks on classical machines. Our PQLM exhibits comparable performance to its\nclassical counterpart on both intrinsic evaluation (loss, perplexity) and\nextrinsic evaluation (multilingual sentiment analysis accuracy) metrics and\nachieves an accuracy of 93.4%, outperforming the classical model. We also\nperform ablation studies on the factors affecting PQLM performance to analyze\nmodel stability. Our work establishes a theoretical foundation for a portable\nquantum pre-trained language model that could be trained on private data and\nmade available for public use with privacy protection guarantees.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuyue Stella Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_H/0/1/0/all/0/1\">Hongchao Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1\">Ruixing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hexin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_L/0/1/0/all/0/1\">Leibny Paola Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrating Factual Knowledge in Pretrained Language Models. (arXiv:2210.03329v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03329","description":"<p>Previous literature has proved that Pretrained Language Models (PLMs) can\nstore factual knowledge. However, we find that facts stored in the PLMs are not\nalways correct. It motivates us to explore a fundamental question: How do we\ncalibrate factual knowledge in PLMs without re-training from scratch? In this\nwork, we propose a simple and lightweight method CaliNet to achieve this goal.\nTo be specific, we first detect whether PLMs can learn the right facts via a\ncontrastive score between right and fake facts. If not, we then use a\nlightweight method to add and adapt new parameters to specific factual texts.\nExperiments on the knowledge probing task show the calibration effectiveness\nand efficiency. In addition, through closed-book question answering, we find\nthat the calibrated PLM possesses knowledge generalization ability after\nfine-tuning. Beyond the calibration performance, we further investigate and\nvisualize the knowledge calibration mechanism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qingxiu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yifan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters. (arXiv:2210.04284v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04284","description":"<p>Adapter Tuning, which freezes the pretrained language models (PLMs) and only\nfine-tunes a few extra modules, becomes an appealing efficient alternative to\nthe full model fine-tuning. Although computationally efficient, the recent\nAdapters often increase parameters (e.g. bottleneck dimension) for matching the\nperformance of full model fine-tuning, which we argue goes against their\noriginal intention. In this work, we re-examine the parameter-efficiency of\nAdapters through the lens of network pruning (we name such plug-in concept as\n\\texttt{SparseAdapter}) and find that SparseAdapter can achieve comparable or\nbetter performance than standard Adapters when the sparse ratio reaches up to\n80\\%. Based on our findings, we introduce an easy but effective setting\n``\\textit{Large-Sparse}'' to improve the model capacity of Adapters under the\nsame parameter budget. Experiments on five competitive Adapters upon three\nadvanced PLMs show that with proper sparse method (e.g. SNIP) and ratio (e.g.\n40\\%) SparseAdapter can consistently outperform their corresponding\ncounterpart. Encouragingly, with the \\textit{Large-Sparse} setting, we can\nobtain further appealing gains, even outperforming the full fine-tuning by a\nlarge margin. Our code will be released at:\nhttps://github.com/Shwai-He/SparseAdapter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shwai He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1\">Daize Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Distillation Transfer Sets and their Impact on Downstream NLU Tasks. (arXiv:2210.04834v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04834","description":"<p>Teacher-student knowledge distillation is a popular technique for compressing\ntoday's prevailing large language models into manageable sizes that fit\nlow-latency downstream applications. Both the teacher and the choice of\ntransfer set used for distillation are crucial ingredients in creating a high\nquality student. Yet, the generic corpora used to pretrain the teacher and the\ncorpora associated with the downstream target domain are often significantly\ndifferent, which raises a natural question: should the student be distilled\nover the generic corpora, so as to learn from high-quality teacher predictions,\nor over the downstream task corpora to align with finetuning? Our study\ninvestigates this trade-off using Domain Classification (DC) and Intent\nClassification/Named Entity Recognition (ICNER) as downstream tasks. We distill\nseveral multilingual students from a larger multilingual LM with varying\nproportions of generic and task-specific datasets, and report their performance\nafter finetuning on DC and ICNER. We observe significant improvements across\ntasks and test sets when only task-specific corpora is used. We also report on\nhow the impact of adding task-specific data to the transfer set correlates with\nthe similarity between generic and task-specific data. Our results clearly\nindicate that, while distillation from a generic LM benefits downstream tasks,\nstudents learn better using target domain data even if it comes at the price of\nnoisier teacher predictions. In other words, target domain data still trumps\nteacher knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peris_C/0/1/0/all/0/1\">Charith Peris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Lizhen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gueudre_T/0/1/0/all/0/1\">Thomas Gueudre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gojayev_T/0/1/0/all/0/1\">Turan Gojayev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1\">Pan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oz_G/0/1/0/all/0/1\">Gokmen Oz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Confidence estimation of classification based on the distribution of the neural network output layer. (arXiv:2210.07745v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07745","description":"<p>One of the most common problems preventing the application of prediction\nmodels in the real world is lack of generalization: The accuracy of models,\nmeasured in the benchmark does repeat itself on future data, e.g. in the\nsettings of real business. There is relatively little methods exist that\nestimate the confidence of prediction models. In this paper, we propose novel\nmethods that, given a neural network classification model, estimate uncertainty\nof particular predictions generated by this model. Furthermore, we propose a\nmethod that, given a model and a confidence level, calculates a threshold that\nseparates prediction generated by this model into two subsets, one of them\nmeets the given confidence level. In contrast to other methods, the proposed\nmethods do not require any changes on existing neural networks, because they\nsimply build on the output logit layer of a common neural network. In\nparticular, the methods infer the confidence of a particular prediction based\non the distribution of the logit values corresponding to this prediction. The\nproposed methods constitute a tool that is recommended for filtering\npredictions in the process of knowledge extraction, e.g. based on web\nscrapping, where predictions subsets are identified that maximize the precision\non cost of the recall, which is less important due to the availability of data.\nThe method has been tested on different tasks including relation extraction,\nnamed entity recognition and image classification to show the significant\nincrease of accuracy achieved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taha_A/0/1/0/all/0/1\">Abdel Aziz Taha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennig_L/0/1/0/all/0/1\">Leonhard Hennig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoth_P/0/1/0/all/0/1\">Petr Knoth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Second Wave of UD Hebrew Treebanking and Cross-Domain Parsing. (arXiv:2210.07873v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07873","description":"<p>Foundational Hebrew NLP tasks such as segmentation, tagging and parsing, have\nrelied to date on various versions of the Hebrew Treebank (HTB, Sima'an et al.\n2001). However, the data in HTB, a single-source newswire corpus, is now over\n30 years old, and does not cover many aspects of contemporary Hebrew on the\nweb. This paper presents a new, freely available UD treebank of Hebrew\nstratified from a range of topics selected from Hebrew Wikipedia. In addition\nto introducing the corpus and evaluating the quality of its annotations, we\ndeploy automatic validation tools based on grew (Guillaume, 2021), and conduct\nthe first cross domain parsing experiments in Hebrew. We obtain new\nstate-of-the-art (SOTA) results on UD NLP tasks, using a combination of the\nlatest language modelling and some incremental improvements to existing\ntransformer based approaches. We also release a new version of the UD HTB\nmatching annotation scheme updates from our new corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeldes_A/0/1/0/all/0/1\">Amir Zeldes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howell_N/0/1/0/all/0/1\">Nick Howell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ordan_N/0/1/0/all/0/1\">Noam Ordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moshe_Y/0/1/0/all/0/1\">Yifat Ben Moshe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Learners for Natural Language Understanding via a Unified Multiple Choice Perspective. (arXiv:2210.08590v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08590","description":"<p>We propose a new paradigm for zero-shot learners that is format agnostic,\ni.e., it is compatible with any format and applicable to a list of language\ntasks, such as text classification, commonsense reasoning, coreference\nresolution, and sentiment analysis. Zero-shot learning aims to train a model on\na given task such that it can address new learning tasks without any additional\ntraining. Our approach converts zero-shot learning into multiple-choice tasks,\navoiding problems in commonly used large-scale generative models such as FLAN.\nIt not only adds generalization ability to models but also significantly\nreduces the number of parameters. Our method shares the merits of efficient\ntraining and deployment. Our approach shows state-of-the-art performance on\nseveral benchmarks and produces satisfactory results on tasks such as natural\nlanguage inference and text classification. Our model achieves this success\nwith only 235M parameters, which is substantially smaller than state-of-the-art\nmodels with billions of parameters. The code and pre-trained models are\navailable at https://github.com/IDEA-CCNL/Fengshenbang-LM .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Ping Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_R/0/1/0/all/0/1\">Ruyi Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakai_T/0/1/0/all/0/1\">Tetsuya Sakai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Generative User Simulator with GPT-based Architecture and Goal State Tracking for Reinforced Multi-Domain Dialog Systems. (arXiv:2210.08692v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08692","description":"<p>Building user simulators (USs) for reinforcement learning (RL) of\ntask-oriented dialog systems (DSs) has gained more and more attention, which,\nhowever, still faces several fundamental challenges. First, it is unclear\nwhether we can leverage pretrained language models to design, for example,\nGPT-2 based USs, to catch up and interact with the recently advanced GPT-2\nbased DSs. Second, an important ingredient in a US is that the user goal can be\neffectively incorporated and tracked; but how to flexibly integrate goal state\ntracking and develop an end-to-end trainable US for multi-domains has remained\nto be a challenge. In this work, we propose a generative user simulator (GUS)\nwith GPT-2 based architecture and goal state tracking towards addressing the\nabove two challenges. Extensive experiments are conducted on MultiWOZ2.1.\nDifferent DSs are trained via RL with GUS, the classic agenda-based user\nsimulator (ABUS) and other ablation simulators respectively, and are compared\nfor cross-model evaluation, corpus-based evaluation and human evaluation. The\nGUS achieves superior results in all three evaluation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yucheng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zhijian Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Junlan Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Multilingual Knowledge Graph Completion and Alignment. (arXiv:2210.08922v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08922","description":"<p>Knowledge graph (KG) alignment and completion are usually treated as two\nindependent tasks. While recent work has leveraged entity and relation\nalignments from multiple KGs, such as alignments between multilingual KGs with\ncommon entities and relations, a deeper understanding of the ways in which\nmultilingual KG completion (MKGC) can aid the creation of multilingual KG\nalignments (MKGA) is still limited. Motivated by the observation that\nstructural inconsistencies -- the main challenge for MKGA models -- can be\nmitigated through KG completion methods, we propose a novel model for jointly\ncompleting and aligning knowledge graphs. The proposed model combines two\ncomponents that jointly accomplish KG completion and alignment. These two\ncomponents employ relation-aware graph neural networks that we propose to\nencode multi-hop neighborhood structures into entity and relation\nrepresentations. Moreover, we also propose (i) a structural inconsistency\nreduction mechanism to incorporate information from the completion into the\nalignment component, and (ii) an alignment seed enlargement and triple\ntransferring mechanism to enlarge alignment seeds and transfer triples during\nKGs alignment. Extensive experiments on a public multilingual benchmark show\nthat our proposed model outperforms existing competitive baselines, obtaining\nnew state-of-the-art results on both MKGC and MKGA tasks. We publicly release\nthe implementation of our model at https://github.com/vinhsuhi/JMAC\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tong_V/0/1/0/all/0/1\">Vinh Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat Quoc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_T/0/1/0/all/0/1\">Trung Thanh Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tam Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1\">Quoc Viet Hung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1\">Mathias Niepert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Covertly Unsafe Text within Natural Language Systems. (arXiv:2210.09306v1 [cs.AI] CROSS LISTED)","link":"http://arxiv.org/abs/2210.09306","description":"<p>An increasingly prevalent problem for intelligent technologies is text\nsafety, as uncontrolled systems may generate recommendations to their users\nthat lead to injury or life-threatening consequences. However, the degree of\nexplicitness of a generated statement that can cause physical harm varies. In\nthis paper, we distinguish types of text that can lead to physical harm and\nestablish one particularly underexplored category: covertly unsafe text. Then,\nwe further break down this category with respect to the system's information\nand discuss solutions to mitigate the generation of text in each of these\nsubcategories. Ultimately, our work defines the problem of covertly unsafe\nlanguage that causes physical harm and argues that this subtle yet dangerous\nissue needs to be prioritized by stakeholders and regulators. We highlight\nmitigation strategies to inspire future researchers to tackle this challenging\nproblem and help improve safety within smart systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mei_A/0/1/0/all/0/1\">Alex Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_A/0/1/0/all/0/1\">Anisha Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Sharon Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subbiah_M/0/1/0/all/0/1\">Melanie Subbiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allaway_E/0/1/0/all/0/1\">Emily Allaway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Judge_J/0/1/0/all/0/1\">John Judge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patton_D/0/1/0/all/0/1\">Desmond Patton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bimber_B/0/1/0/all/0/1\">Bruce Bimber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-10-18T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}