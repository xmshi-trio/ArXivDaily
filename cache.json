{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-12-18T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Acoustic models of Brazilian Portuguese Speech based on Neural Transformers. (arXiv:2312.09265v1 [cs.SD])","link":"http://arxiv.org/abs/2312.09265","description":"<p>An acoustic model, trained on a significant amount of unlabeled data,\nconsists of a self-supervised learned speech representation useful for solving\ndownstream tasks, perhaps after a fine-tuning of the model in the respective\ndownstream task. In this work, we build an acoustic model of Brazilian\nPortuguese Speech through a Transformer neural network. This model was\npretrained on more than $800$ hours of Brazilian Portuguese Speech, using a\ncombination of pretraining techniques. Using a labeled dataset collected for\nthe detection of respiratory insufficiency in Brazilian Portuguese speakers, we\nfine-tune the pretrained Transformer neural network on the following tasks:\nrespiratory insufficiency detection, gender recognition and age group\nclassification. We compare the performance of pretrained Transformers on these\ntasks with that of Transformers without previous pretraining, noting a\nsignificant improvement. In particular, the performance of respiratory\ninsufficiency detection obtains the best reported results so far, indicating\nthis kind of acoustic model as a promising tool for speech-as-biomarker\napproach. Moreover, the performance of gender recognition is comparable to the\nstate of the art models in English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gauy_M/0/1/0/all/0/1\">Marcelo Matheus Gauy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finger_M/0/1/0/all/0/1\">Marcelo Finger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weight subcloning: direct initialization of transformers using larger pretrained ones. (arXiv:2312.09299v1 [cs.LG])","link":"http://arxiv.org/abs/2312.09299","description":"<p>Training large transformer models from scratch for a target task requires\nlots of data and is computationally demanding. The usual practice of transfer\nlearning overcomes this challenge by initializing the model with weights of a\npretrained model of the same size and specification to increase the convergence\nand training speed. However, what if no pretrained model of the required size\nis available? In this paper, we introduce a simple yet effective technique to\ntransfer the knowledge of a pretrained model to smaller variants. Our approach\ncalled weight subcloning expedites the training of scaled-down transformers by\ninitializing their weights from larger pretrained models.\n</p>\n<p>Weight subcloning involves an operation on the pretrained model to obtain the\nequivalent initialized scaled-down model. It consists of two key steps: first,\nwe introduce neuron importance ranking to decrease the embedding dimension per\nlayer in the pretrained model. Then, we remove blocks from the transformer\nmodel to match the number of layers in the scaled-down network. The result is a\nnetwork ready to undergo training, which gains significant improvements in\ntraining speed compared to random initialization. For instance, we achieve 4x\nfaster training for vision transformers in image classification and language\nmodels designed for next token prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Samragh_M/0/1/0/all/0/1\">Mohammad Samragh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farajtabar_M/0/1/0/all/0/1\">Mehrdad Farajtabar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Sachin Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vemulapalli_R/0/1/0/all/0/1\">Raviteja Vemulapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faghri_F/0/1/0/all/0/1\">Fartash Faghri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_D/0/1/0/all/0/1\">Devang Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuzel_O/0/1/0/all/0/1\">Oncel Tuzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1\">Mohammad Rastegari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Evaluation Improves Selective Generation in Large Language Models. (arXiv:2312.09300v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09300","description":"<p>Safe deployment of large language models (LLMs) may benefit from a reliable\nmethod for assessing their generated content to determine when to abstain or to\nselectively generate. While likelihood-based metrics such as perplexity are\nwidely employed, recent research has demonstrated the limitations of using\nsequence-level probability estimates given by LLMs as reliable indicators of\ngeneration quality. Conversely, LLMs have demonstrated strong calibration at\nthe token level, particularly when it comes to choosing correct answers in\nmultiple-choice questions or evaluating true/false statements. In this work, we\nreformulate open-ended generation tasks into token-level prediction tasks, and\nleverage LLMs' superior calibration at the token level. We instruct an LLM to\nself-evaluate its answers, employing either a multi-way comparison or a\npoint-wise evaluation approach, with the option to include a ``None of the\nabove'' option to express the model's uncertainty explicitly. We benchmark a\nrange of scoring methods based on self-evaluation and evaluate their\nperformance in selective generation using TruthfulQA and TL;DR. Through\nexperiments with PaLM-2 and GPT-3, we demonstrate that self-evaluation based\nscores not only improve accuracy, but also correlate better with the overall\nquality of generated content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tu Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peter J. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Well-calibrated Confidence Measures for Multi-label Text Classification with a Large Number of Labels. (arXiv:2312.09304v1 [cs.LG])","link":"http://arxiv.org/abs/2312.09304","description":"<p>We extend our previous work on Inductive Conformal Prediction (ICP) for\nmulti-label text classification and present a novel approach for addressing the\ncomputational inefficiency of the Label Powerset (LP) ICP, arrising when\ndealing with a high number of unique labels. We present experimental results\nusing the original and the proposed efficient LP-ICP on two English and one\nCzech language data-sets. Specifically, we apply the LP-ICP on three deep\nArtificial Neural Network (ANN) classifiers of two types: one based on\ncontextualised (bert) and two on non-contextualised (word2vec) word-embeddings.\nIn the LP-ICP setting we assign nonconformity scores to label-sets from which\nthe corresponding p-values and prediction-sets are determined. Our approach\ndeals with the increased computational burden of LP by eliminating from\nconsideration a significant number of label-sets that will surely have p-values\nbelow the specified significance level. This reduces dramatically the\ncomputational complexity of the approach while fully respecting the standard CP\nguarantees. Our experimental results show that the contextualised-based\nclassifier surpasses the non-contextualised-based ones and obtains\nstate-of-the-art performance for all data-sets examined. The good performance\nof the underlying classifiers is carried on to their ICP counterparts without\nany significant accuracy loss, but with the added benefits of ICP, i.e. the\nconfidence information encapsulated in the prediction sets. We experimentally\ndemonstrate that the resulting prediction sets can be tight enough to be\npractically useful even though the set of all possible label-sets contains more\nthan $1e+16$ combinations. Additionally, the empirical error rates of the\nobtained prediction-sets confirm that our outputs are well-calibrated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maltoudoglou_L/0/1/0/all/0/1\">Lysimachos Maltoudoglou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paisios_A/0/1/0/all/0/1\">Andreas Paisios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenc_L/0/1/0/all/0/1\">Ladislav Lenc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinek_J/0/1/0/all/0/1\">Ji&#x159;&#xed; Mart&#xed;nek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kral_P/0/1/0/all/0/1\">Pavel Kr&#xe1;l</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_H/0/1/0/all/0/1\">Harris Papadopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arabic Mini-ClimateGPT : A Climate Change and Sustainability Tailored Arabic LLM. (arXiv:2312.09366v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09366","description":"<p>Climate change is one of the most significant challenges we face together as\na society. Creating awareness and educating policy makers the wide-ranging\nimpact of climate change is an essential step towards a sustainable future.\nRecently, Large Language Models (LLMs) like ChatGPT and Bard have shown\nimpressive conversational abilities and excel in a wide variety of NLP tasks.\nWhile these models are close-source, recently alternative open-source LLMs such\nas Stanford Alpaca and Vicuna have shown promising results. However, these\nopen-source models are not specifically tailored for climate related domain\nspecific information and also struggle to generate meaningful responses in\nother languages such as, Arabic. To this end, we propose a light-weight Arabic\nMini-ClimateGPT that is built on an open-source LLM and is specifically\nfine-tuned on a conversational-style instruction tuning curated Arabic dataset\nClima500-Instruct with over 500k instructions about climate change and\nsustainability. Further, our model also utilizes a vector embedding based\nretrieval mechanism during inference. We validate our proposed model through\nquantitative and qualitative evaluations on climate-related queries. Our model\nsurpasses the baseline LLM in 88.3% of cases during ChatGPT-based evaluation.\nFurthermore, our human expert evaluation reveals an 81.6% preference for our\nmodel's responses over multiple popular open-source models. Our open-source\ndemos, code-base and models are available here\nhttps://github.com/mbzuai-oryx/ClimateGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mullappilly_S/0/1/0/all/0/1\">Sahal Shaji Mullappilly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaker_A/0/1/0/all/0/1\">Abdelrahman Shaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thawakar_O/0/1/0/all/0/1\">Omkar Thawakar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cholakkal_H/0/1/0/all/0/1\">Hisham Cholakkal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwer_R/0/1/0/all/0/1\">Rao Muhammad Anwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision. (arXiv:2312.09390v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09390","description":"<p>Widely used alignment techniques, such as reinforcement learning from human\nfeedback (RLHF), rely on the ability of humans to supervise model behavior -\nfor example, to evaluate whether a model faithfully followed instructions or\ngenerated safe outputs. However, future superhuman models will behave in\ncomplex ways too difficult for humans to reliably evaluate; humans will only be\nable to weakly supervise superhuman models. We study an analogy to this\nproblem: can weak model supervision elicit the full capabilities of a much\nstronger model? We test this using a range of pretrained language models in the\nGPT-4 family on natural language processing (NLP), chess, and reward modeling\ntasks. We find that when we naively finetune strong pretrained models on labels\ngenerated by a weak model, they consistently perform better than their weak\nsupervisors, a phenomenon we call weak-to-strong generalization. However, we\nare still far from recovering the full capabilities of strong models with naive\nfinetuning alone, suggesting that techniques like RLHF may scale poorly to\nsuperhuman models without further work. We find that simple methods can often\nsignificantly improve weak-to-strong generalization: for example, when\nfinetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence\nloss, we can recover close to GPT-3.5-level performance on NLP tasks. Our\nresults suggest that it is feasible to make empirical progress today on a\nfundamental challenge of aligning superhuman models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burns_C/0/1/0/all/0/1\">Collin Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izmailov_P/0/1/0/all/0/1\">Pavel Izmailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchner_J/0/1/0/all/0/1\">Jan Hendrik Kirchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baker_B/0/1/0/all/0/1\">Bowen Baker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Leo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aschenbrenner_L/0/1/0/all/0/1\">Leopold Aschenbrenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yining Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ecoffet_A/0/1/0/all/0/1\">Adrien Ecoffet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joglekar_M/0/1/0/all/0/1\">Manas Joglekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leike_J/0/1/0/all/0/1\">Jan Leike</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutskever_I/0/1/0/all/0/1\">Ilya Sutskever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jeff Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OTOv3: Automatic Architecture-Agnostic Neural Network Training and Compression from Structured Pruning to Erasing Operators. (arXiv:2312.09411v1 [cs.LG])","link":"http://arxiv.org/abs/2312.09411","description":"<p>Compressing a predefined deep neural network (DNN) into a compact sub-network\nwith competitive performance is crucial in the efficient machine learning\nrealm. This topic spans various techniques, from structured pruning to neural\narchitecture search, encompassing both pruning and erasing operators\nperspectives. Despite advancements, existing methods suffers from complex,\nmulti-stage processes that demand substantial engineering and domain knowledge,\nlimiting their broader applications. We introduce the third-generation\nOnly-Train-Once (OTOv3), which first automatically trains and compresses a\ngeneral DNN through pruning and erasing operations, creating a compact and\ncompetitive sub-network without the need of fine-tuning. OTOv3 simplifies and\nautomates the training and compression process, minimizes the engineering\nefforts required from users. It offers key technological advancements: (i)\nautomatic search space construction for general DNNs based on dependency graph\nanalysis; (ii) Dual Half-Space Projected Gradient (DHSPG) and its enhanced\nversion with hierarchical search (H2SPG) to reliably solve (hierarchical)\nstructured sparsity problems and ensure sub-network validity; and (iii)\nautomated sub-network construction using solutions from DHSPG/H2SPG and\ndependency graphs. Our empirical results demonstrate the efficacy of OTOv3\nacross various benchmarks in structured pruning and neural architecture search.\nOTOv3 produces sub-networks that match or exceed the state-of-the-arts. The\nsource code will be available at https://github.com/tianyic/only_train_once.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1\">Tianyu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhihui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">HsiangTao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zharkov_I/0/1/0/all/0/1\">Ilya Zharkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Luming Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Domain Knowledge Extraction for Knowledge Graphs. (arXiv:2312.09424v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09424","description":"<p>The quality of a knowledge graph directly impacts the quality of downstream\napplications (e.g. the number of answerable questions using the graph). One\nongoing challenge when building a knowledge graph is to ensure completeness and\nfreshness of the graph's entities and facts. In this paper, we introduce ODKE,\na scalable and extensible framework that sources high-quality entities and\nfacts from open web at scale. ODKE utilizes a wide range of extraction models\nand supports both streaming and batch processing at different latency. We\nreflect on the challenges and design decisions made and share lessons learned\nwhen building and deploying ODKE to grow an industry-scale open domain\nknowledge graph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1\">Kun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belyi_A/0/1/0/all/0/1\">Anton Belyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khorshidi_S/0/1/0/all/0/1\">Samira Khorshidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikfarjam_A/0/1/0/all/0/1\">Azadeh Nikfarjam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khot_R/0/1/0/all/0/1\">Rahul Khot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_Y/0/1/0/all/0/1\">Yisi Sang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luna_K/0/1/0/all/0/1\">Katherine Luna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xianqi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eric Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Govind_Y/0/1/0/all/0/1\">Yash Govind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seivwright_C/0/1/0/all/0/1\">Chloe Seivwright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yiwen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhry_A/0/1/0/all/0/1\">Ahmed Fakhry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekatsinas_T/0/1/0/all/0/1\">Theo Rekatsinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilyas_I/0/1/0/all/0/1\">Ihab Ilyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaoguang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunyao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Representation Learning for Open Vocabulary Electroencephalography-to-Text Decoding. (arXiv:2312.09430v1 [eess.SP])","link":"http://arxiv.org/abs/2312.09430","description":"<p>Previous research has demonstrated the potential of using pre-trained\nlanguage models for decoding open vocabulary Electroencephalography (EEG)\nsignals captured through a non-invasive Brain-Computer Interface (BCI).\nHowever, the impact of embedding EEG signals in the context of language models\nand the effect of subjectivity, remain unexplored, leading to uncertainty about\nthe best approach to enhance decoding performance. Additionally, current\nevaluation metrics used to assess decoding effectiveness are predominantly\nsyntactic and do not provide insights into the comprehensibility of the decoded\noutput for human understanding. We present an end-to-end deep learning\nframework for non-invasive brain recordings that brings modern representational\nlearning approaches to neuroscience. Our proposal introduces the following\ninnovations: 1) an end-to-end deep learning architecture for open vocabulary\nEEG decoding, incorporating a subject-dependent representation learning module\nfor raw EEG encoding, a BART language model, and a GPT-4 sentence refinement\nmodule; 2) a more comprehensive sentence-level evaluation metric based on the\nBERTScore; 3) an ablation study that analyses the contributions of each module\nwithin our proposal, providing valuable insights for future research. We\nevaluate our approach on two publicly available datasets, ZuCo v1.0 and v2.0,\ncomprising EEG recordings of 30 subjects engaged in natural reading tasks. Our\nmodel achieves a BLEU-1 score of 42.75%, a ROUGE-1-F of 33.28%, and a\nBERTScore-F of 53.86%, outperforming the previous state-of-the-art methods by\n3.38%, 8.43%, and 6.31%, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Amrani_H/0/1/0/all/0/1\">Hamza Amrani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Micucci_D/0/1/0/all/0/1\">Daniela Micucci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Napoletano_P/0/1/0/all/0/1\">Paolo Napoletano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MANTIS at #SMM4H 2023: Leveraging Hybrid and Ensemble Models for Detection of Social Anxiety Disorder on Reddit. (arXiv:2312.09451v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09451","description":"<p>This paper presents our system employed for the Social Media Mining for\nHealth 2023 Shared Task 4: Binary classification of English Reddit posts\nself-reporting a social anxiety disorder diagnosis. We systematically\ninvestigate and contrast the efficacy of hybrid and ensemble models that\nharness specialized medical domain-adapted transformers in conjunction with\nBiLSTM neural networks. The evaluation results outline that our best performing\nmodel obtained 89.31% F1 on the validation set and 83.76% F1 on the test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zanwar_S/0/1/0/all/0/1\">Sourabh Zanwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiechmann_D/0/1/0/all/0/1\">Daniel Wiechmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerz_E/0/1/0/all/0/1\">Elma Kerz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Functional Analytics for Document Ordering for Curriculum Development and Comprehension. (arXiv:2312.09457v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09457","description":"<p>We propose multiple techniques for automatic document order generation for\n(1) curriculum development and for (2) creation of optimal reading order for\nuse in learning, training, and other content-sequencing applications. Such\ntechniques could potentially be used to improve comprehension, identify areas\nthat need expounding, generate curricula, and improve search engine results. We\nadvance two main techniques: The first uses document similarities through\nvarious methods. The second uses entropy against the backdrop of topics\ngenerated through Latent Dirichlet Allocation (LDA). In addition, we try the\nsame methods on the summarized documents and compare them against the results\nobtained using the complete documents. Our results showed that while the\ndocument orders for our control document sets (biographies, novels, and\nWikipedia articles) could not be predicted using our methods, our test\ndocuments (textbooks, courses, journal papers, dissertations) provided more\nreliability. We also demonstrated that summarized documents were good stand-ins\nfor the complete documents for the purposes of ordering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Villanueva_A/0/1/0/all/0/1\">Arturo N. Villanueva Jr.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simske_S/0/1/0/all/0/1\">Steven J. Simske</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Partial Rewriting for Multi-Stage ASR. (arXiv:2312.09463v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09463","description":"<p>For many streaming automatic speech recognition tasks, it is important to\nprovide timely intermediate streaming results, while refining a high quality\nfinal result. This can be done using a multi-stage architecture, where a small\nleft-context only model creates streaming results and a larger left- and\nright-context model produces a final result at the end. While this\nsignificantly improves the quality of the final results without compromising\nthe streaming emission latency of the system, streaming results do not benefit\nfrom the quality improvements. Here, we propose using a text manipulation\nalgorithm that merges the streaming outputs of both models. We improve the\nquality of streaming results by around 10%, without altering the final results.\nOur approach introduces no additional latency and reduces flickering. It is\nalso lightweight, does not require retraining the model, and it can be applied\nto a wide variety of multi-stage architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bruguier_A/0/1/0/all/0/1\">Antoine Bruguier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_D/0/1/0/all/0/1\">David Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yanzhang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clinical Text Deduplication Practices for Efficient Pretraining and Improved Clinical Tasks. (arXiv:2312.09469v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09469","description":"<p>Despite being a unique source of information on patients' status and disease\nprogression, clinical notes are characterized by high levels of duplication and\ninformation redundancy. In general domain text, it has been shown that\ndeduplication does not harm language model (LM) pretraining, thus helping\nreduce the training cost. Although large LMs have proven to learn medical\nknowledge, they still require specialized domain adaptation for improved\ndownstream clinical tasks. By leveraging large real-world clinical corpora, we\nfirst provided a fine-grained characterization of duplicates stemming from\ncommon writing practices and clinical relevancy. Second, we demonstrated that\ndeduplicating clinical text can help clinical LMs encode less redundant\ninformation in a more efficient manner and do not harm classification tasks via\nprompt-based learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Landi_I/0/1/0/all/0/1\">Isotta Landi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alleva_E/0/1/0/all/0/1\">Eugenia Alleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valentine_A/0/1/0/all/0/1\">Alissa A. Valentine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepow_L/0/1/0/all/0/1\">Lauren A. Lepow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charney_A/0/1/0/all/0/1\">Alexander W. Charney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No-Skim: Towards Efficiency Robustness Evaluation on Skimming-based Language Models. (arXiv:2312.09494v1 [cs.CR])","link":"http://arxiv.org/abs/2312.09494","description":"<p>To reduce the computation cost and the energy consumption in large language\nmodels (LLM), skimming-based acceleration dynamically drops unimportant tokens\nof the input sequence progressively along layers of the LLM while preserving\nthe tokens of semantic importance. However, our work for the first time reveals\nthe acceleration may be vulnerable to Denial-of-Service (DoS) attacks. In this\npaper, we propose No-Skim, a general framework to help the owners of\nskimming-based LLM to understand and measure the robustness of their\nacceleration scheme. Specifically, our framework searches minimal and\nunnoticeable perturbations at character-level and token-level to generate\nadversarial inputs that sufficiently increase the remaining token ratio, thus\nincreasing the computation cost and energy consumption. We systematically\nevaluate the vulnerability of the skimming acceleration in various LLM\narchitectures including BERT and RoBERTa on the GLUE benchmark. In the worst\ncase, the perturbation found by No-Skim substantially increases the running\ncost of LLM by over 145% on average. Moreover, No-Skim extends the evaluation\nframework to various scenarios, making the evaluation conductible with\ndifferent level of knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xudong Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IndicIRSuite: Multilingual Dataset and Neural Information Models for Indian Languages. (arXiv:2312.09508v1 [cs.IR])","link":"http://arxiv.org/abs/2312.09508","description":"<p>In this paper, we introduce Neural Information Retrieval resources for 11\nwidely spoken Indian Languages (Assamese, Bengali, Gujarati, Hindi, Kannada,\nMalayalam, Marathi, Oriya, Punjabi, Tamil, and Telugu) from two major Indian\nlanguage families (Indo-Aryan and Dravidian). These resources include (a)\nINDIC-MARCO, a multilingual version of the MSMARCO dataset in 11 Indian\nLanguages created using Machine Translation, and (b) Indic-ColBERT, a\ncollection of 11 distinct Monolingual Neural Information Retrieval models, each\ntrained on one of the 11 languages in the INDIC-MARCO dataset. To the best of\nour knowledge, IndicIRSuite is the first attempt at building large-scale Neural\nInformation Retrieval resources for a large number of Indian languages, and we\nhope that it will help accelerate research in Neural IR for Indian Languages.\nExperiments demonstrate that Indic-ColBERT achieves 47.47% improvement in the\nMRR@10 score averaged over the INDIC-MARCO baselines for all 11 Indian\nlanguages except Oriya, 12.26% improvement in the NDCG@10 score averaged over\nthe MIRACL Bengali and Hindi Language baselines, and 20% improvement in the\nMRR@100 Score over the Mr.Tydi Bengali Language baseline. IndicIRSuite is\navailable at https://github.com/saifulhaq95/IndicIRSuite\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haq_S/0/1/0/all/0/1\">Saiful Haq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Ashutosh Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Riveter: Measuring Power and Social Dynamics Between Entities. (arXiv:2312.09536v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09536","description":"<p>Riveter provides a complete easy-to-use pipeline for analyzing verb\nconnotations associated with entities in text corpora. We prepopulate the\npackage with connotation frames of sentiment, power, and agency, which have\ndemonstrated usefulness for capturing social phenomena, such as gender bias, in\na broad range of corpora. For decades, lexical frameworks have been\nfoundational tools in computational social science, digital humanities, and\nnatural language processing, facilitating multifaceted analysis of text\ncorpora. But working with verb-centric lexica specifically requires natural\nlanguage processing skills, reducing their accessibility to other researchers.\nBy organizing the language processing pipeline, providing complete lexicon\nscores and visualizations for all entities in a corpus, and providing\nfunctionality for users to target specific research questions, Riveter greatly\nimproves the accessibility of verb lexica and can facilitate a broad range of\nfuture research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antoniak_M/0/1/0/all/0/1\">Maria Antoniak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Field_A/0/1/0/all/0/1\">Anjalie Field</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mun_J/0/1/0/all/0/1\">Jimin Mun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walsh_M/0/1/0/all/0/1\">Melanie Walsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_L/0/1/0/all/0/1\">Lauren F. Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Picking the Underused Heads: A Network Pruning Perspective of Attention Head Selection for Fusing Dialogue Coreference Information. (arXiv:2312.09541v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09541","description":"<p>The Transformer-based models with the multi-head self-attention mechanism are\nwidely used in natural language processing, and provide state-of-the-art\nresults. While the pre-trained language backbones are shown to implicitly\ncapture certain linguistic knowledge, explicitly incorporating structure-aware\nfeatures can bring about further improvement on the downstream tasks. However,\nsuch enhancement often requires additional neural components and increases\ntraining parameter size. In this work, we investigate the attention head\nselection and manipulation strategy for feature injection from a network\npruning perspective, and conduct a case study on dialogue summarization. We\nfirst rank attention heads in a Transformer-based summarizer with layer-wise\nimportance. We then select the underused heads through extensive analysis, and\ninject structure-aware features by manipulating the selected heads.\nExperimental results show that the importance-based head selection is effective\nfor feature injection, and dialogue summarization can be improved by\nincorporating coreference information via head manipulation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Marathon: A Race Through the Realm of Long Context with Large Language Models. (arXiv:2312.09542v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09542","description":"<p>Although there are currently many benchmarks available for evaluating the\nlong context understanding and reasoning capability of large language models,\nwith the expansion of the context window in these models, the existing long\ncontext benchmarks are no longer sufficient for evaluating the long context\nunderstanding and reasoning capability of large language models. In this paper,\nwe have developed a fresh long context evaluation benchmark, which we name it\nMarathon in the form of multiple choice questions, inspired by benchmarks such\nas MMLU, for assessing the long context comprehension capability of large\nlanguage models quickly, accurately, and objectively. We have evaluated several\nof the latest and most popular large language models, as well as three recent\nand effective long context optimization methods, on our benchmark. This\nshowcases the long context reasoning and comprehension capabilities of these\nlarge language models and validates the effectiveness of these optimization\nmethods. Marathon is available at\nhttps://huggingface.co/datasets/Lemoncoke/Marathon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunshui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziqiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+yang_J/0/1/0/all/0/1\">Jiaxi yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-4 Surpassing Human Performance in Linguistic Pragmatics. (arXiv:2312.09545v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09545","description":"<p>As Large Language Models (LLMs) become increasingly integrated into everyday\nlife, their capabilities to understand and emulate human cognition are under\nsteady examination. This study investigates the ability of LLMs to comprehend\nand interpret linguistic pragmatics, an aspect of communication that considers\ncontext and implied meanings. Using Grice's communication principles, LLMs and\nhuman subjects (N=76) were evaluated based on their responses to various\ndialogue-based tasks. The findings revealed the superior performance and speed\nof LLMs, particularly GPT4, over human subjects in interpreting pragmatics.\nGPT4 also demonstrated accuracy in the pre-testing of human-written samples,\nindicating its potential in text analysis. In a comparative analysis of LLMs\nusing human individual and average scores, the models exhibited significant\nchronological improvement. The models were ranked from lowest to highest score,\nwith GPT2 positioned at 78th place, GPT3 ranking at 23rd, Bard at 10th, GPT3.5\nplacing 5th, Best Human scoring 2nd, and GPT4 achieving the top spot. The\nfindings highlight the remarkable progress made in the development and\nperformance of these LLMs. Future studies should consider diverse subjects,\nmultiple languages, and other cognitive aspects to fully comprehend the\ncapabilities of LLMs. This research holds significant implications for the\ndevelopment and application of AI-based models in communication-centered\nsectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bojic_L/0/1/0/all/0/1\">Ljubisa Bojic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovacevic_P/0/1/0/all/0/1\">Predrag Kovacevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabarkapa_M/0/1/0/all/0/1\">Milan Cabarkapa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extending Context Window of Large Language Models via Semantic Compression. (arXiv:2312.09571v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09571","description":"<p>Transformer-based Large Language Models (LLMs) often impose limitations on\nthe length of the text input to ensure the generation of fluent and relevant\nresponses. This constraint restricts their applicability in scenarios involving\nlong texts. We propose a novel semantic compression method that enables\ngeneralization to texts that are 6-8 times longer, without incurring\nsignificant computational costs or requiring fine-tuning. Our proposed\nframework draws inspiration from source coding in information theory and\nemploys a pre-trained model to reduce the semantic redundancy of long inputs\nbefore passing them to the LLMs for downstream tasks. Experimental results\ndemonstrate that our method effectively extends the context window of LLMs\nacross a range of tasks including question answering, summarization, few-shot\nlearning, and information retrieval. Furthermore, the proposed semantic\ncompression method exhibits consistent fluency in text generation while\nreducing the associated computational overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fei_W/0/1/0/all/0/1\">Weizhi Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xueyan Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pingyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_B/0/1/0/all/0/1\">Bo Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Lei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IR-UWB Radar-Based Contactless Silent Speech Recognition of Vowels, Consonants, Words, and Phrases. (arXiv:2312.09572v1 [eess.AS])","link":"http://arxiv.org/abs/2312.09572","description":"<p>Several sensing techniques have been proposed for silent speech recognition\n(SSR); however, many of these methods require invasive processes or sensor\nattachment to the skin using adhesive tape or glue, rendering them unsuitable\nfor frequent use in daily life. By contrast, impulse radio ultra-wideband\n(IR-UWB) radar can operate without physical contact with users' articulators\nand related body parts, offering several advantages for SSR. These advantages\ninclude high range resolution, high penetrability, low power consumption,\nrobustness to external light or sound interference, and the ability to be\nembedded in space-constrained handheld devices. This study demonstrated IR-UWB\nradar-based contactless SSR using four types of speech stimuli (vowels,\nconsonants, words, and phrases). To achieve this, a novel speech feature\nextraction algorithm specifically designed for IR-UWB radar-based SSR is\nproposed. Each speech stimulus is recognized by applying a classification\nalgorithm to the extracted speech features. Two different algorithms,\nmultidimensional dynamic time warping (MD-DTW) and deep neural network-hidden\nMarkov model (DNN-HMM), were compared for the classification task.\nAdditionally, a favorable radar antenna position, either in front of the user's\nlips or below the user's chin, was determined to achieve higher recognition\naccuracy. Experimental results demonstrated the efficacy of the proposed speech\nfeature extraction algorithm combined with DNN-HMM for classifying vowels,\nconsonants, words, and phrases. Notably, this study represents the first\ndemonstration of phoneme-level SSR using contactless radar.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1\">Sunghwa Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shin_Y/0/1/0/all/0/1\">Younghoon Shin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_M/0/1/0/all/0/1\">Myungjong Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seo_J/0/1/0/all/0/1\">Jiwon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phoneme-aware Encoding for Prefix-tree-based Contextual ASR. (arXiv:2312.09582v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09582","description":"<p>In speech recognition applications, it is important to recognize\ncontext-specific rare words, such as proper nouns. Tree-constrained Pointer\nGenerator (TCPGen) has shown promise for this purpose, which efficiently biases\nsuch words with a prefix tree. While the original TCPGen relies on\ngrapheme-based encoding, we propose extending it with phoneme-aware encoding to\nbetter recognize words of unusual pronunciations. As TCPGen handles biasing\nwords as subword units, we propose obtaining subword-level phoneme-aware\nencoding by using alignment between phonemes and subwords. Furthermore, we\npropose injecting phoneme-level predictions from CTC into queries of TCPGen so\nthat the model better interprets the phoneme-aware encodings. We conducted ASR\nexperiments with TCPGen for RNN transducer. We observed that proposed\nphoneme-aware encoding outperformed ordinary grapheme-based encoding on both\nthe English LibriSpeech and Japanese CSJ datasets, demonstrating the robustness\nof our approach across linguistically diverse languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Futami_H/0/1/0/all/0/1\">Hayato Futami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsunoo_E/0/1/0/all/0/1\">Emiru Tsunoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashiwagi_Y/0/1/0/all/0/1\">Yosuke Kashiwagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogawa_H/0/1/0/all/0/1\">Hiroaki Ogawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Siddhant Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Language ID to Calculate Intermediate CTC Loss for Enhanced Code-Switching Speech Recognition. (arXiv:2312.09583v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09583","description":"<p>In recent years, end-to-end speech recognition has emerged as a technology\nthat integrates the acoustic, pronunciation dictionary, and language model\ncomponents of the traditional Automatic Speech Recognition model. It is\npossible to achieve human-like recognition without the need to build a\npronunciation dictionary in advance. However, due to the relative scarcity of\ntraining data on code-switching, the performance of ASR models tends to degrade\ndrastically when encountering this phenomenon. Most past studies have\nsimplified the learning complexity of the model by splitting the code-switching\ntask into multiple tasks dealing with a single language and then learning the\ndomain-specific knowledge of each language separately. Therefore, in this\npaper, we attempt to introduce language identification information into the\nmiddle layer of the ASR model's encoder. We aim to generate acoustic features\nthat imply language distinctions in a more implicit way, reducing the model's\nconfusion when dealing with language switching.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tzu-Ting Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Berlin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Binary Code Summarization: Benchmarking ChatGPT/GPT-4 and Other Large Language Models. (arXiv:2312.09601v1 [cs.CR])","link":"http://arxiv.org/abs/2312.09601","description":"<p>Binary code summarization, while invaluable for understanding code semantics,\nis challenging due to its labor-intensive nature. This study delves into the\npotential of large language models (LLMs) for binary code comprehension. To\nthis end, we present BinSum, a comprehensive benchmark and dataset of over 557K\nbinary functions and introduce a novel method for prompt synthesis and\noptimization. To more accurately gauge LLM performance, we also propose a new\nsemantic similarity metric that surpasses traditional exact-match approaches.\nOur extensive evaluation of prominent LLMs, including ChatGPT, GPT-4, Llama 2,\nand Code Llama, reveals 10 pivotal insights. This evaluation generates 4\nbillion inference tokens, incurred a total expense of 11,418 US dollars and 873\nNVIDIA A100 GPU hours. Our findings highlight both the transformative potential\nof LLMs in this field and the challenges yet to be overcome.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larson_J/0/1/0/all/0/1\">Jonathan Larson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Weiwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhiqiang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-Supervised 3D Visual Grounding based on Visual Linguistic Alignment. (arXiv:2312.09625v1 [cs.CV])","link":"http://arxiv.org/abs/2312.09625","description":"<p>Learning to ground natural language queries to target objects or regions in\n3D point clouds is quite essential for 3D scene understanding. Nevertheless,\nexisting 3D visual grounding approaches require a substantial number of\nbounding box annotations for text queries, which is time-consuming and\nlabor-intensive to obtain. In this paper, we propose \\textbf{3D-VLA}, a weakly\nsupervised approach for \\textbf{3D} visual grounding based on \\textbf{V}isual\n\\textbf{L}inguistic \\textbf{A}lignment. Our 3D-VLA exploits the superior\nability of current large-scale vision-language models (VLMs) on aligning the\nsemantics between texts and 2D images, as well as the naturally existing\ncorrespondences between 2D images and 3D point clouds, and thus implicitly\nconstructs correspondences between texts and 3D point clouds with no need for\nfine-grained box annotations in the training procedure. During the inference\nstage, the learned text-3D correspondence will help us ground the text queries\nto the 3D target objects even without 2D images. To the best of our knowledge,\nthis is the first work to investigate 3D visual grounding in a weakly\nsupervised manner by involving large scale vision-language models, and\nextensive experiments on ReferIt3D and ScanRefer datasets demonstrate that our\n3D-VLA achieves comparable and even superior results over the fully supervised\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaoxu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yitian Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiudan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1\">Zequn Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Pretrained Language Models with Hierarchy Properties. (arXiv:2312.09670v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09670","description":"<p>Since Pretrained Language Models (PLMs) are the cornerstone of the most\nrecent Information Retrieval (IR) models, the way they encode semantic\nknowledge is particularly important. However, little attention has been given\nto studying the PLMs' capability to capture hierarchical semantic knowledge.\nTraditionally, evaluating such knowledge encoded in PLMs relies on their\nperformance on a task-dependent evaluation approach based on proxy tasks, such\nas hypernymy detection. Unfortunately, this approach potentially ignores other\nimplicit and complex taxonomic relations. In this work, we propose a\ntask-agnostic evaluation method able to evaluate to what extent PLMs can\ncapture complex taxonomy relations, such as ancestors and siblings. The\nevaluation is based on intrinsic properties that capture the hierarchical\nnature of taxonomies. Our experimental evaluation shows that the\nlexico-semantic knowledge implicitly encoded in PLMs does not always capture\nhierarchical relations. We further demonstrate that the proposed properties can\nbe injected into PLMs to improve their understanding of hierarchy. Through\nevaluations on taxonomy reconstruction, hypernym discovery and reading\ncomprehension tasks, we show that the knowledge about hierarchy is moderately\nbut not systematically transferable across tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lovon_Melgarejo_J/0/1/0/all/0/1\">Jes&#xfa;s Lov&#xf3;n-Melgarejo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_J/0/1/0/all/0/1\">Jose G. Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besancon_R/0/1/0/all/0/1\">Romaric Besan&#xe7;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferret_O/0/1/0/all/0/1\">Olivier Ferret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamine_L/0/1/0/all/0/1\">Lynda Tamine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Highly Influential Shortcut Reasoning: An Automated Template-Free Approach. (arXiv:2312.09718v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09718","description":"<p>Shortcut reasoning is an irrational process of inference, which degrades the\nrobustness of an NLP model. While a number of previous work has tackled the\nidentification of shortcut reasoning, there are still two major limitations:\n(i) a method for quantifying the severity of the discovered shortcut reasoning\nis not provided; (ii) certain types of shortcut reasoning may be missed. To\naddress these issues, we propose a novel method for identifying shortcut\nreasoning. The proposed method quantifies the severity of the shortcut\nreasoning by leveraging out-of-distribution data and does not make any\nassumptions about the type of tokens triggering the shortcut reasoning. Our\nexperiments on Natural Language Inference and Sentiment Analysis demonstrate\nthat our framework successfully discovers known and unknown shortcut reasoning\nin the previous work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haraguchi_D/0/1/0/all/0/1\">Daichi Haraguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shirai_K/0/1/0/all/0/1\">Kiyoaki Shirai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inoue_N/0/1/0/all/0/1\">Naoya Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kertkeidkachorn_N/0/1/0/all/0/1\">Natthawut Kertkeidkachorn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HEAR: Hearing Enhanced Audio Response for Video-grounded Dialogue. (arXiv:2312.09736v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09736","description":"<p>Video-grounded Dialogue (VGD) aims to answer questions regarding a given\nmulti-modal input comprising video, audio, and dialogue history. Although there\nhave been numerous efforts in developing VGD systems to improve the quality of\ntheir responses, existing systems are competent only to incorporate the\ninformation in the video and text and tend to struggle in extracting the\nnecessary information from the audio when generating appropriate responses to\nthe question. The VGD system seems to be deaf, and thus, we coin this symptom\nof current systems' ignoring audio data as a deaf response. To overcome the\ndeaf response problem, Hearing Enhanced Audio Response (HEAR) framework is\nproposed to perform sensible listening by selectively attending to audio\nwhenever the question requires it. The HEAR framework enhances the accuracy and\naudibility of VGD systems in a model-agnostic manner. HEAR is validated on VGD\ndatasets (i.e., AVSD@DSTC7 and AVSD@DSTC8) and shows effectiveness with various\nVGD systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sunjae Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dahyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_E/0/1/0/all/0/1\">Eunseop Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1\">Hee Suk Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junyeong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1\">Chnag D. Yoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GSQA: An End-to-End Model for Generative Spoken Question Answering. (arXiv:2312.09781v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09781","description":"<p>In recent advancements in spoken question answering (QA), end-to-end models\nhave made significant strides. However, previous research has primarily focused\non extractive span selection. While this extractive-based approach is effective\nwhen answers are present directly within the input, it falls short in\naddressing abstractive questions, where answers are not directly extracted but\ninferred from the given information. To bridge this gap, we introduce the first\nend-to-end Generative Spoken Question Answering (GSQA) model that empowers the\nsystem to engage in abstractive reasoning. The challenge in training our GSQA\nmodel lies in the absence of a spoken abstractive QA dataset. We propose using\ntext models for initialization and leveraging the extractive QA dataset to\ntransfer knowledge from the text generative model to the spoken generative\nmodel. Experimental results indicate that our model surpasses the previous\nextractive model by 3% on extractive QA datasets. Furthermore, the GSQA model\nhas only been fine-tuned on the spoken extractive QA dataset. Despite not\nhaving seen any spoken abstractive QA data, it can still closely match the\nperformance of the cascade model. In conclusion, our GSQA model shows the\npotential to generalize to a broad spectrum of questions, thus further\nexpanding spoken question answering capabilities of abstractive QA. Our code is\navailable at\n\\href{https://voidful.github.io/GSQA}{https://voidful.github.io/GSQA}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shih_M/0/1/0/all/0/1\">Min-Han Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Ho-Lam Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pai_Y/0/1/0/all/0/1\">Yu-Chi Pai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_M/0/1/0/all/0/1\">Ming-Hao Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guan-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RJUA-QA: A Comprehensive QA Dataset for Urology. (arXiv:2312.09785v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09785","description":"<p>We introduce RJUA-QA, a novel medical dataset for question answering (QA) and\nreasoning with clinical evidence, contributing to bridge the gap between\ngeneral large language models (LLMs) and medical-specific LLM applications.\nRJUA-QA is derived from realistic clinical scenarios and aims to facilitate\nLLMs in generating reliable diagnostic and advice. The dataset contains 2,132\ncurated Question-Context-Answer pairs, corresponding about 25,000 diagnostic\nrecords and clinical cases. The dataset covers 67 common urological disease\ncategories, where the disease coverage exceeds 97.6\\% of the population seeking\nmedical services in urology. Each data instance in RJUA-QA comprises: (1) a\nquestion mirroring real patient to inquiry about clinical symptoms and medical\nconditions, (2) a context including comprehensive expert knowledge, serving as\na reference for medical examination and diagnosis, (3) a doctor response\noffering the diagnostic conclusion and suggested examination guidance, (4) a\ndiagnosed clinical disease as the recommended diagnostic outcome, and (5)\nclinical advice providing recommendations for medical examination. RJUA-QA is\nthe first medical QA dataset for clinical reasoning over the patient inquiries,\nwhere expert-level knowledge and experience are required for yielding\ndiagnostic conclusions and medical examination advice. A comprehensive\nevaluation is conducted to evaluate the performance of both medical-specific\nand general LLMs on the RJUA-QA dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_C/0/1/0/all/0/1\">Chenfei Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hongbo Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1\">Lei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Deng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1\">Xianguo Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fangzhou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaowei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yue Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjie Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Wei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yiran Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProCoT: Stimulating Critical Thinking and Writing of Students through Engagement with Large Language Models (LLMs). (arXiv:2312.09801v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09801","description":"<p>We introduce a novel writing method called Probing Chain of Thought (ProCoT),\nwhich prevents students from cheating using a Large Language Model (LLM), such\nas ChatGPT, while enhancing their active learning through such models. LLMs\nhave disrupted education and many other feilds. For fear of students cheating,\nmany educationists have resorted to banning their use, as their outputs can be\nhuman-like and hard to detect in some cases. These LLMs are also known for\nhallucinations (i.e. fake facts). We conduct studies with ProCoT in two\ndifferent courses with a combined total of about 66 students. The students in\neach course were asked to prompt an LLM of their choice with one question from\na set of four and required to affirm or refute statements in the LLM output by\nusing peer reviewed references. The results show two things: (1) ProCoT\nstimulates creative/critical thinking and writing of students through\nengagement with LLMs when we compare the LLM solely output to ProCoT output and\n(2) ProCoT can prevent cheating because of clear limitations in existing LLMs\nwhen we compare students ProCoT output to LLM ProCoT output. We also discover\nthat most students prefer to give answers in fewer words than LLMs, which are\ntypically verbose. The average word counts for students, ChatGPT (v3.5) and\nPhind (v8) are 208, 391 and 383, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1\">Tosin Adewumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alkhaled_L/0/1/0/all/0/1\">Lama Alkhaled</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buck_C/0/1/0/all/0/1\">Claudia Buck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_S/0/1/0/all/0/1\">Sergio Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brilioth_S/0/1/0/all/0/1\">Saga Brilioth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kekung_M/0/1/0/all/0/1\">Mkpe Kekung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragimov_Y/0/1/0/all/0/1\">Yelvin Ragimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barney_E/0/1/0/all/0/1\">Elisa Barney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Biomedical Entity Linking with Retrieval-enhanced Learning. (arXiv:2312.09806v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09806","description":"<p>Biomedical entity linking (BioEL) has achieved remarkable progress with the\nhelp of pre-trained language models. However, existing BioEL methods usually\nstruggle to handle rare and difficult entities due to long-tailed distribution.\nTo address this limitation, we introduce a new scheme $k$NN-BioEL, which\nprovides a BioEL model with the ability to reference similar instances from the\nentire training corpus as clues for prediction, thus improving the\ngeneralization capabilities. Moreover, we design a contrastive learning\nobjective with dynamic hard negative sampling (DHNS) that improves the quality\nof the retrieved neighbors during inference. Extensive experimental results\nshow that $k$NN-BioEL outperforms state-of-the-art baselines on several\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhenxi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMILE: Multimodal Dataset for Understanding Laughter in Video with Language Models. (arXiv:2312.09818v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09818","description":"<p>Despite the recent advances of the artificial intelligence, building social\nintelligence remains a challenge. Among social signals, laughter is one of the\ndistinctive expressions that occurs during social interactions between humans.\nIn this work, we tackle a new challenge for machines to understand the\nrationale behind laughter in video, Video Laugh Reasoning. We introduce this\nnew task to explain why people laugh in a particular video and a dataset for\nthis task. Our proposed dataset, SMILE, comprises video clips and language\ndescriptions of why people laugh. We propose a baseline by leveraging the\nreasoning capacity of large language models (LLMs) with textual video\nrepresentation. Experiments show that our baseline can generate plausible\nexplanations for laughter. We further investigate the scalability of our\nbaseline by probing other video understanding tasks and in-the-wild videos. We\nrelease our dataset, code, and model checkpoints on\nhttps://github.com/SMILE-data/SMILE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hyun_L/0/1/0/all/0/1\">Lee Hyun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_Bin_K/0/1/0/all/0/1\">Kim Sung-Bin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Seungju Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1\">Tae-Hyun Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grammatical information in BERT sentence embeddings as two-dimensional arrays. (arXiv:2312.09890v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09890","description":"<p>Sentence embeddings induced with various transformer architectures encode\nmuch semantic and syntactic information in a distributed manner in a\none-dimensional array. We investigate whether specific grammatical information\ncan be accessed in these distributed representations. Using data from a task\ndeveloped to test rule-like generalizations, our experiments on detecting\nsubject-verb agreement yield several promising results. First, we show that\nwhile the usual sentence representations encoded as one-dimensional arrays do\nnot easily support extraction of rule-like regularities, a two-dimensional\nreshaping of these vectors allows various learning architectures to access such\ninformation. Next, we show that various architectures can detect patterns in\nthese two-dimensional reshaped sentence embeddings and successfully learn a\nmodel based on smaller amounts of simpler training data, which performs well on\nmore complex test data. This indicates that current sentence embeddings contain\ninformation that is regularly distributed, and which can be captured when the\nembeddings are reshaped into higher dimensional arrays. Our results cast light\non representations produced by language models and help move towards developing\nfew-shot learning approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nastase_V/0/1/0/all/0/1\">Vivi Nastase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merlo_P/0/1/0/all/0/1\">Paola Merlo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Context-aware Fine-tuning of Self-supervised Speech Models. (arXiv:2312.09895v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09895","description":"<p>When performing tasks like automatic speech recognition or spoken language\nunderstanding for a given utterance, access to preceding text or audio provides\ncontextual information can improve performance. Considering the recent advances\nin generative large language models (LLM), we hypothesize that an LLM could\ngenerate useful context information using the preceding text. With appropriate\nprompts, LLM could generate a prediction of the next sentence or abstractive\ntext like titles or topics. In this paper, we study the use of LLM-generated\ncontext information and propose an approach to distill the generated\ninformation during fine-tuning of self-supervised speech models, which we refer\nto as generative context-aware fine-tuning. This approach allows the fine-tuned\nmodel to make improved predictions without access to the true surrounding\nsegments or to the LLM at inference time, while requiring only a very small\nadditional context module. We evaluate the proposed approach using the SLUE and\nLibri-light benchmarks for several downstream tasks: automatic speech\nrecognition, named entity recognition, and sentiment analysis. The results show\nthat generative context-aware fine-tuning outperforms a context injection\nfine-tuning approach that accesses the ground-truth previous text, and is\ncompetitive with a generative context injection fine-tuning approach that\nrequires the LLM at inference time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shon_S/0/1/0/all/0/1\">Suwon Shon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kwangyoun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_P/0/1/0/all/0/1\">Prashant Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yi-Te Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Automatic Text Simplification of German Narrative Documents. (arXiv:2312.09907v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09907","description":"<p>In this paper, we apply transformer-based Natural Language Generation (NLG)\ntechniques to the problem of text simplification. Currently, there are only a\nfew German datasets available for text simplification, even fewer with larger\nand aligned documents, and not a single one with narrative texts. In this\npaper, we explore to which degree modern NLG techniques can be applied to\nGerman narrative text simplifications. We use Longformer attention and a\npre-trained mBART model. Our findings indicate that the existing approaches for\nGerman are not able to solve the task properly. We conclude on a few directions\nfor future research to address this problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schomacker_T/0/1/0/all/0/1\">Thorben Schomacker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donicke_T/0/1/0/all/0/1\">Tillmann D&#xf6;nicke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tropmann_Frick_M/0/1/0/all/0/1\">Marina Tropmann-Frick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Red AI? Inconsistent Responses from GPT3.5 Models on Political Issues in the US and China. (arXiv:2312.09917v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09917","description":"<p>The rising popularity of ChatGPT and other AI-powered large language models\n(LLMs) has led to increasing studies highlighting their susceptibility to\nmistakes and biases. However, most of these studies focus on models trained on\nEnglish texts. Taking an innovative approach, this study investigates political\nbiases in GPT's multilingual models. We posed the same question about\nhigh-profile political issues in the United States and China to GPT in both\nEnglish and simplified Chinese, and our analysis of the bilingual responses\nrevealed that GPT's bilingual models' political \"knowledge\" (content) and the\npolitical \"attitude\" (sentiment) are significantly more inconsistent on\npolitical issues in China. The simplified Chinese GPT models not only tended to\nprovide pro-China information but also presented the least negative sentiment\ntowards China's problems, whereas the English GPT was significantly more\nnegative towards China. This disparity may stem from Chinese state censorship\nand US-China geopolitical tensions, which influence the training corpora of GPT\nbilingual models. Moreover, both Chinese and English models tended to be less\ncritical towards the issues of \"their own\" represented by the language used,\nthan the issues of \"the other.\" This suggests that GPT multilingual models\ncould potentially develop a \"political identity\" and an associated sentiment\nbias based on their training language. We discussed the implications of our\nfindings for information transmission and communication in an increasingly\ndivided world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Di Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinxian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RDR: the Recap, Deliberate, and Respond Method for Enhanced Language Understanding. (arXiv:2312.09932v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09932","description":"<p>Natural language understanding (NLU) using neural network pipelines often\nrequires additional context that is not solely present in the input data.\nThrough Prior research, it has been evident that NLU benchmarks are susceptible\nto manipulation by neural models, wherein these models exploit statistical\nartifacts within the encoded external knowledge to artificially inflate\nperformance metrics for downstream tasks. Our proposed approach, known as the\nRecap, Deliberate, and Respond (RDR) paradigm, addresses this issue by\nincorporating three distinct objectives within the neural network pipeline.\nFirstly, the Recap objective involves paraphrasing the input text using a\nparaphrasing model in order to summarize and encapsulate its essence. Secondly,\nthe Deliberation objective entails encoding external graph information related\nto entities mentioned in the input text, utilizing a graph embedding model.\nFinally, the Respond objective employs a classification head model that\nutilizes representations from the Recap and Deliberation modules to generate\nthe final prediction. By cascading these three models and minimizing a combined\nloss, we mitigate the potential for gaming the benchmark and establish a robust\nmethod for capturing the underlying semantic patterns, thus enabling accurate\npredictions. To evaluate the effectiveness of the RDR method, we conduct tests\non multiple GLUE benchmark tasks. Our results demonstrate improved performance\ncompared to competitive baselines, with an enhancement of up to 2\\% on standard\nmetrics. Furthermore, we analyze the observed evidence for semantic\nunderstanding exhibited by RDR models, emphasizing their ability to avoid\ngaming the benchmark and instead accurately capture the true underlying\nsemantic patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zi_Y/0/1/0/all/0/1\">Yuxin Zi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veeramani_H/0/1/0/all/0/1\">Hariram Veeramani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data and Approaches for German Text simplification -- towards an Accessibility-enhanced Communication. (arXiv:2312.09966v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09966","description":"<p>This paper examines the current state-of-the-art of German text\nsimplification, focusing on parallel and monolingual German corpora. It reviews\nneural language models for simplifying German texts and assesses their\nsuitability for legal texts and accessibility requirements. Our findings\nhighlight the need for additional training data and more appropriate approaches\nthat consider the specific linguistic characteristics of German, as well as the\nimportance of the needs and preferences of target groups with cognitive or\nlanguage impairments. The authors launched the interdisciplinary OPEN-LS\nproject in April 2023 to address these research gaps. The project aims to\ndevelop a framework for text formats tailored to individuals with low literacy\nlevels, integrate legal texts, and enhance comprehensibility for those with\nlinguistic or cognitive impairments. It will also explore cost-effective ways\nto enhance the data with audience-specific illustrations using image-generating\nAI.\n</p>\n<p>For more and up-to-date information, please visit our project homepage\nhttps://open-ls.entavis.com\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schomacker_T/0/1/0/all/0/1\">Thorben Schomacker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gille_M/0/1/0/all/0/1\">Michael Gille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hulls_J/0/1/0/all/0/1\">J&#xf6;rg von der H&#xfc;lls</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tropmann_Frick_M/0/1/0/all/0/1\">Marina Tropmann-Frick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Art of Balancing: Revolutionizing Mixture of Experts for Maintaining World Knowledge in Language Model Alignment. (arXiv:2312.09979v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09979","description":"<p>Supervised fine-tuning (SFT) is a crucial step for large language models\n(LLMs), enabling them to align with human instructions and enhance their\ncapabilities in downstream tasks. When the models are required to align with a\nbroader range of downstream tasks, or there is a desire to notably improve the\nperformance on a specific task, a substantial increase in fine-tuning data\noften emerges as the solution. However, we find that large-scale increases in\ninstruction data can disrupt the world knowledge previously stored in the LLMs,\ni.e., world knowledge forgetting. In this paper, we introduce LoRAMoE to\naddress above challenge. The LoRAMoE is a plugin version of Mixture of Experts\n(MoE). The plugin-form ensures the integrity of world knowledge by freezing the\nbackbone model during the training phase. And we propose the use of localized\nbalancing constraints to coordinate parts of experts for task utilization,\nmeanwhile enables other experts to to fully leverage the world knowledge stored\nin the models. Experimental results demonstrate that LoRAMoE can reasonly\ncoordinate experts based on data type during inference, and even dramatically\nincreasing instruction data does not result in knowledge forgetting. Moreover,\nLoRAMoE provides additional benefits for the performance of downstream tasks,\nindicating the potential of our approach for multi-task learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1\">Shihan Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1\">Enyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Songyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1\">Zhiheng Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiaoran Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Rui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLaMAntino: LLaMA 2 Models for Effective Text Generation in Italian Language. (arXiv:2312.09993v1 [cs.CL])","link":"http://arxiv.org/abs/2312.09993","description":"<p>Large Language Models represent state-of-the-art linguistic models designed\nto equip computers with the ability to comprehend natural language. With its\nexceptional capacity to capture complex contextual relationships, the LLaMA\n(Large Language Model Meta AI) family represents a novel advancement in the\nfield of natural language processing by releasing foundational models designed\nto improve the natural language understanding abilities of the transformer\narchitecture thanks to their large amount of trainable parameters (7, 13, and\n70 billion parameters). In many natural language understanding tasks, these\nmodels obtain the same performances as private company models such as OpenAI\nChat-GPT with the advantage to make publicly available weights and code for\nresearch and commercial uses. In this work, we investigate the possibility of\nLanguage Adaptation for LLaMA models, explicitly focusing on addressing the\nchallenge of Italian Language coverage. Adopting an open science approach, we\nexplore various tuning approaches to ensure a high-quality text generated in\nItalian suitable for common tasks in this underrepresented language in the\noriginal models' datasets. We aim to release effective text generation models\nwith strong linguistic properties for many tasks that seem challenging using\nmultilingual or general-purpose LLMs. By leveraging an open science philosophy,\nthis study contributes to Language Adaptation strategies for the Italian\nlanguage by introducing the novel LLaMAntino family of Italian LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basile_P/0/1/0/all/0/1\">Pierpaolo Basile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musacchio_E/0/1/0/all/0/1\">Elio Musacchio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polignano_M/0/1/0/all/0/1\">Marco Polignano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siciliani_L/0/1/0/all/0/1\">Lucia Siciliani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1\">Giuseppe Fiameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semeraro_G/0/1/0/all/0/1\">Giovanni Semeraro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent. (arXiv:2312.10003v1 [cs.CL])","link":"http://arxiv.org/abs/2312.10003","description":"<p>Answering complex natural language questions often necessitates multi-step\nreasoning and integrating external information. Several systems have combined\nknowledge retrieval with a large language model (LLM) to answer such questions.\nThese systems, however, suffer from various failure cases, and we cannot\ndirectly train them end-to-end to fix such failures, as interaction with\nexternal knowledge is non-differentiable. To address these deficiencies, we\ndefine a ReAct-style LLM agent with the ability to reason and act upon external\nknowledge. We further refine the agent through a ReST-like method that\niteratively trains on previous trajectories, employing growing-batch\nreinforcement learning with AI feedback for continuous self-improvement and\nself-distillation. Starting from a prompted large model and after just two\niterations of the algorithm, we can produce a fine-tuned small model that\nachieves comparable performance on challenging compositional question-answering\nbenchmarks with two orders of magnitude fewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aksitov_R/0/1/0/all/0/1\">Renat Aksitov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miryoosefi_S/0/1/0/all/0/1\">Sobhan Miryoosefi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zonglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Daliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babayan_S/0/1/0/all/0/1\">Sheila Babayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopparapu_K/0/1/0/all/0/1\">Kavya Kopparapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_Z/0/1/0/all/0/1\">Zachary Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Ruiqi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1\">Sushant Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_P/0/1/0/all/0/1\">Pranesh Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Manzil Zaheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Felix Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faithful Persona-based Conversational Dataset Generation with Large Language Models. (arXiv:2312.10007v1 [cs.CL])","link":"http://arxiv.org/abs/2312.10007","description":"<p>High-quality conversational datasets are essential for developing AI models\nthat can communicate with users. One way to foster deeper interactions between\na chatbot and its user is through personas, aspects of the user's character\nthat provide insights into their personality, motivations, and behaviors.\nTraining Natural Language Processing (NLP) models on a diverse and\ncomprehensive persona-based dataset can lead to conversational models that\ncreate a deeper connection with the user, and maintain their engagement. In\nthis paper, we leverage the power of Large Language Models (LLMs) to create a\nlarge, high-quality conversational dataset from a seed dataset. We propose a\nGenerator-Critic architecture framework to expand the initial dataset, while\nimproving the quality of its conversations. The Generator is an LLM prompted to\noutput conversations. The Critic consists of a mixture of expert LLMs that\ncontrol the quality of the generated conversations. These experts select the\nbest generated conversations, which we then use to improve the Generator. We\nrelease Synthetic-Persona-Chat, consisting of 20k conversations seeded from\nPersona-Chat. We evaluate the quality of Synthetic-Persona-Chat and our\ngeneration framework on different dimensions through extensive experiments, and\nobserve that the losing rate of Synthetic-Persona-Chat against Persona-Chat\nduring Turing test decreases from 17.2% to 8.8% over three iterations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jandaghi_P/0/1/0/all/0/1\">Pegah Jandaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_X/0/1/0/all/0/1\">XiangHai Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xinyi Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidahmed_H/0/1/0/all/0/1\">Hakim Sidahmed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective and Imperceptible Adversarial Textual Attack via Multi-objectivization. (arXiv:2111.01528v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.01528","description":"<p>The field of adversarial textual attack has significantly grown over the last\nfew years, where the commonly considered objective is to craft adversarial\nexamples (AEs) that can successfully fool the target model. However, the\nimperceptibility of attacks, which is also essential for practical attackers,\nis often left out by previous studies. In consequence, the crafted AEs tend to\nhave obvious structural and semantic differences from the original\nhuman-written text, making them easily perceptible. In this work, we advocate\nleveraging multi-objectivization to address such issue. Specifically, we\nreformulate the problem of crafting AEs as a multi-objective optimization\nproblem, where the attack imperceptibility is considered as an auxiliary\nobjective. Then, we propose a simple yet effective evolutionary algorithm,\ndubbed HydraText, to solve this problem. To the best of our knowledge,\nHydraText is currently the only approach that can be effectively applied to\nboth score-based and decision-based attack settings. Exhaustive experiments\ninvolving 44237 instances demonstrate that HydraText consistently achieves\ncompetitive attack success rates and better attack imperceptibility than the\nrecently proposed attack approaches. A human evaluation study also shows that\nthe AEs crafted by HydraText are more indistinguishable from human-written\ntext. Finally, these AEs exhibit good transferability and can bring notable\nrobustness improvement to the target model by adversarial training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengcai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_N/0/1/0/all/0/1\">Ning Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_W/0/1/0/all/0/1\">Wenjing Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chao Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of semantic relations impact in query expansion-based retrieval systems. (arXiv:2203.16230v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16230","description":"<p>With the increasing demand of intelligent systems capable of operating in\ndifferent contexts (e.g. users on the move) the correct interpretation of the\nuser-need by such systems has become crucial to give consistent answers to the\nuser questions. The most effective applications addressing such task are in the\nfields of natural language processing and semantic expansion of terms. These\ntechniques are aimed at estimating the goal of an input query reformulating it\nas an intent, commonly relying on textual resources built exploiting different\nsemantic relations like \\emph{synonymy}, \\emph{antonymy} and many others. The\naim of this paper is to generate such resources using the labels of a given\ntaxonomy as source of information. The obtained resources are integrated into a\nplain classifier for reformulating a set of input queries as intents and\ntracking the effect of each relation, in order to quantify the impact of each\nsemantic relation on the classification. As an extension to this, the best\ntradeoff between improvement and noise introduction when combining such\nrelations is evaluated. The assessment is made generating the resources and\ntheir combinations and using them for tuning the classifier which is used to\nreformulate the user questions as labels. The evaluation employs a wide and\nvaried taxonomy as a use-case, exploiting its labels as basis for the semantic\nexpansion and producing several corpora with the purpose of enhancing the\npseudo-queries estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Massai_L/0/1/0/all/0/1\">Lorenzo Massai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning. (arXiv:2305.14160v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14160","description":"<p>In-context learning (ICL) emerges as a promising capability of large language\nmodels (LLMs) by providing them with demonstration examples to perform diverse\ntasks. However, the underlying mechanism of how LLMs learn from the provided\ncontext remains under-explored. In this paper, we investigate the working\nmechanism of ICL through an information flow lens. Our findings reveal that\nlabel words in the demonstration examples function as anchors: (1) semantic\ninformation aggregates into label word representations during the shallow\ncomputation layers' processing; (2) the consolidated information in label words\nserves as a reference for LLMs' final predictions. Based on these insights, we\nintroduce an anchor re-weighting method to improve ICL performance, a\ndemonstration compression technique to expedite inference, and an analysis\nframework for diagnosing ICL errors in GPT2-XL. The promising applications of\nour findings again validate the uncovered ICL working mechanism and pave the\nway for future studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lean Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Deli Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers. (arXiv:2305.18396v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.18396","description":"<p>The community explored to build private inference frameworks for\ntransformer-based large language models (LLMs) in a server-client setting,\nwhere the server holds the model parameters and the client inputs its private\ndata (or prompt) for inference. However, these frameworks impose significant\noverhead when the private inputs are forward propagated through the original\nLLMs. In this paper, we show that substituting the computation- and\ncommunication-heavy operators in the transformer architecture with\nprivacy-computing friendly approximations can greatly reduce the private\ninference costs while incurring very minor impact on model performance.\nCompared to state-of-the-art Iron (NeurIPS 2022), our privacy-computing\nfriendly model inference pipeline achieves a $5\\times$ acceleration in\ncomputation and an 80% reduction in communication overhead, while retaining\nnearly identical accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuanqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhuotao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3's personality instruments results. (arXiv:2306.04308v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2306.04308","description":"<p>As AI-bots continue to gain popularity due to their human-like traits and the\nintimacy they offer to users, their societal impact inevitably expands. This\nleads to the rising necessity for comprehensive studies to fully understand\nAI-bots and reveal their potential opportunities, drawbacks, and overall\nsocietal impact. With that in mind, this research conducted an extensive\ninvestigation into ChatGPT3, a renowned AI bot, aiming to assess the temporal\nreliability of its personality profile. Psychological questionnaires were\nadministered to the chatbot on two separate occasions, followed by a comparison\nof the responses to human normative data. The findings revealed varying levels\nof agreement in chatbot's responses over time, with some scales displaying\nexcellent agreement while others demonstrated poor agreement. Overall,\nDavinci-003 displayed a socially desirable and pro-social personality profile,\nparticularly in the domain of communion. However, the underlying basis of the\nchatbot's responses-whether driven by conscious self reflection or\npredetermined algorithms-remains uncertain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bodroza_B/0/1/0/all/0/1\">Bojana Bodroza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinic_B/0/1/0/all/0/1\">Bojana M. Dinic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojic_L/0/1/0/all/0/1\">Ljubisa Bojic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CIF-T: A Novel CIF-based Transducer Architecture for Automatic Speech Recognition. (arXiv:2307.14132v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2307.14132","description":"<p>RNN-T models are widely used in ASR, which rely on the RNN-T loss to achieve\nlength alignment between input audio and target sequence. However, the\nimplementation complexity and the alignment-based optimization target of RNN-T\nloss lead to computational redundancy and a reduced role for predictor network,\nrespectively. In this paper, we propose a novel model named CIF-Transducer\n(CIF-T) which incorporates the Continuous Integrate-and-Fire (CIF) mechanism\nwith the RNN-T model to achieve efficient alignment. In this way, the RNN-T\nloss is abandoned, thus bringing a computational reduction and allowing the\npredictor network a more significant role. We also introduce Funnel-CIF,\nContext Blocks, Unified Gating and Bilinear Pooling joint network, and\nauxiliary training strategy to further improve performance. Experiments on the\n178-hour AISHELL-1 and 10000-hour WenetSpeech datasets show that CIF-T achieves\nstate-of-the-art results with lower computational overhead compared to RNN-T\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tian-Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dinghao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_G/0/1/0/all/0/1\">Guiping Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiaming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baoxiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Ehanced Move Recognition Algorithm Based on Pre-trained Models with Positional Embeddings. (arXiv:2308.10822v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.10822","description":"<p>The recognition of abstracts is crucial for effectively locating the content\nand clarifying the article. Existing move recognition algorithms lack the\nability to learn word position information to obtain contextual semantics. This\npaper proposes a novel enhanced move recognition algorithm with an improved\npre-trained model and a gated network with attention mechanism for unstructured\nabstracts of Chinese scientific and technological papers. The proposed\nalgorithm first performs summary data segmentation and vocabulary training. The\nEP-ERNIE$\\_$AT-GRU framework is leveraged to incorporate word positional\ninformation, facilitating deep semantic learning and targeted feature\nextraction. Experimental results demonstrate that the proposed algorithm\nachieves 13.37$\\%$ higher accuracy on the split dataset than on the original\ndataset and a 7.55$\\%$ improvement in accuracy over the basic comparison model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1\">Hao Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_X/0/1/0/all/0/1\">Xiaodong Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"C-Pack: Packaged Resources To Advance General Chinese Embedding. (arXiv:2309.07597v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.07597","description":"<p>We introduce C-Pack, a package of resources that significantly advance the\nfield of general Chinese embeddings. C-Pack includes three critical resources.\n1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6\ntasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated\nfrom labeled and unlabeled Chinese corpora for training embedding models. 3)\nC-TEM is a family of embedding models covering multiple sizes. Our models\noutperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the\ntime of the release. We also integrate and optimize the entire suite of\ntraining methods for C-TEM. Along with our resources on general Chinese\nembedding, we release our data and models for English text embeddings. The\nEnglish models achieve state-of-the-art performance on MTEB benchmark;\nmeanwhile, our released English data is 2 times larger than the Chinese data.\nAll these resources are made publicly available at\nhttps://github.com/FlagOpen/FlagEmbedding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shitao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peitian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1\">Niklas Muennighoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dementia Assessment Using Mandarin Speech with an Attention-based Speech Recognition Encoder. (arXiv:2310.03985v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.03985","description":"<p>Dementia diagnosis requires a series of different testing methods, which is\ncomplex and time-consuming. Early detection of dementia is crucial as it can\nprevent further deterioration of the condition. This paper utilizes a speech\nrecognition model to construct a dementia assessment system tailored for\nMandarin speakers during the picture description task. By training an\nattention-based speech recognition model on voice data closely resembling\nreal-world scenarios, we have significantly enhanced the model's recognition\ncapabilities. Subsequently, we extracted the encoder from the speech\nrecognition model and added a linear layer for dementia assessment. We\ncollected Mandarin speech data from 99 subjects and acquired their clinical\nassessments from a local hospital. We achieved an accuracy of 92.04% in\nAlzheimer's disease detection and a mean absolute error of 9% in clinical\ndementia rating score prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zih-Jyun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Ju Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_P/0/1/0/all/0/1\">Po-Chih Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Likai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chaur-Jong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng-Yu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeqXGPT: Sentence-Level AI-Generated Text Detection. (arXiv:2310.08903v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.08903","description":"<p>Widely applied large language models (LLMs) can generate human-like content,\nraising concerns about the abuse of LLMs. Therefore, it is important to build\nstrong AI-generated text (AIGT) detectors. Current works only consider\ndocument-level AIGT detection, therefore, in this paper, we first introduce a\nsentence-level detection challenge by synthesizing a dataset that contains\ndocuments that are polished with LLMs, that is, the documents contain sentences\nwritten by humans and sentences modified by LLMs. Then we propose\n\\textbf{Seq}uence \\textbf{X} (Check) \\textbf{GPT}, a novel method that utilizes\nlog probability lists from white-box LLMs as features for sentence-level AIGT\ndetection. These features are composed like \\textit{waves} in speech processing\nand cannot be studied by LLMs. Therefore, we build SeqXGPT based on convolution\nand self-attention networks. We test it in both sentence and document-level\ndetection challenges. Experimental results show that previous methods struggle\nin solving sentence-level AIGT detection, while our method not only\nsignificantly surpasses baseline methods in both sentence and document-level\ndetection challenges but also exhibits strong generalization capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1\">Ke Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Botian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not all Fake News is Written: A Dataset and Analysis of Misleading Video Headlines. (arXiv:2310.13859v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.13859","description":"<p>Polarization and the marketplace for impressions have conspired to make\nnavigating information online difficult for users, and while there has been a\nsignificant effort to detect false or misleading text, multimodal datasets have\nreceived considerably less attention. To complement existing resources, we\npresent multimodal Video Misleading Headline (VMH), a dataset that consists of\nvideos and whether annotators believe the headline is representative of the\nvideo's contents. After collecting and annotating this dataset, we analyze\nmultimodal baselines for detecting misleading headlines. Our annotation process\nalso focuses on why annotators view a video as misleading, allowing us to\nbetter understand the interplay of annotators' background and the content of\nthe videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yoo Yeon Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1\">Jordan Boyd-Graber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_N/0/1/0/all/0/1\">Naeemul Hassan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT a game changer for geocoding -- a benchmark for geocoding address parsing techniques. (arXiv:2310.14360v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.14360","description":"<p>The remarkable success of GPT models across various tasks, including toponymy\nrecognition motivates us to assess the performance of the GPT-3 model in the\ngeocoding address parsing task. To ensure that the evaluation more accurately\nmirrors performance in real-world scenarios with diverse user input qualities\nand resolve the pressing need for a 'gold standard' evaluation dataset for\ngeocoding systems, we introduce a benchmark dataset of low-quality address\ndescriptions synthesized based on human input patterns mining from actual input\nlogs of a geocoding system in production. This dataset has 21 different input\nerrors and variations; contains over 239,000 address records that are uniquely\nselected from streets across all U.S. 50 states and D.C.; and consists of three\nsubsets to be used as training, validation, and testing sets. Building on this,\nwe train and gauge the performance of the GPT-3 model in extracting address\ncomponents, contrasting its performance with transformer-based and LSTM-based\nmodels. The evaluation results indicate that Bidirectional LSTM-CRF model has\nachieved the best performance over these transformer-based models and GPT-3\nmodel. Transformer-based models demonstrate very comparable results compared to\nthe Bidirectional LSTM-CRF model. The GPT-3 model, though trailing in\nperformance, showcases potential in the address parsing task with few-shot\nexamples, exhibiting room for improvement with additional fine-tuning. We open\nsource the code and data of this presented benchmark so that researchers can\nutilize it for future model development or extend it to evaluate similar tasks,\nsuch as document geocoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhengcong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Diya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_D/0/1/0/all/0/1\">Daniel W. Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation. (arXiv:2310.15539v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.15539","description":"<p>With the recent focus on Large Language Models (LLMs), both StarCoder (Li et\nal., 2023) and Code Llama (Rozi\\`ere et al., 2023) have demonstrated remarkable\nperformance in code generation. However, there is still a need for improvement\nin code translation functionality with efficient training techniques. In\nresponse to this, we introduce SteloCoder, a decoder-only StarCoder-based LLM\ndesigned specifically for multi-programming language-to-Python code\ntranslation. In particular, SteloCoder achieves C++, C#, JavaScript, Java, or\nPHP-to-Python code translation without specifying the input programming\nlanguage. We modified StarCoder model architecture by incorporating a\nMixture-of-Experts (MoE) technique featuring five experts and a gating network\nfor multi-task handling. Experts are obtained by StarCoder fine-tuning.\nSpecifically, we use a Low-Rank Adaptive Method (LoRA) technique, limiting each\nexpert size as only 0.06% of number of StarCoder's parameters. At the same\ntime, to enhance training efficiency in terms of time, we adopt curriculum\nlearning strategy and use self-instruct data for efficient fine-tuning. As a\nresult, each expert takes only 6 hours to train on one single 80Gb A100 HBM.\nWith experiments on XLCoST datasets, SteloCoder achieves an average of 73.76\nCodeBLEU score in multi-programming language-to-Python translation, surpassing\nthe top performance from the leaderboard by at least 3.5. This accomplishment\nis attributed to only 45M extra parameters with StarCoder as the backbone and\n32 hours of valid training on one 80GB A100 HBM. The source code is release\nhere: https://github.com/sade-adrien/SteloCoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jialing Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sade_A/0/1/0/all/0/1\">Adrien Sad&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soriano_E/0/1/0/all/0/1\">Eric Soriano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sole_G/0/1/0/all/0/1\">Guillem Sole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flamant_S/0/1/0/all/0/1\">Sylvain Flamant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Editing for Large Language Models: A Survey. (arXiv:2310.16218v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.16218","description":"<p>Large language models (LLMs) have recently transformed both the academic and\nindustrial landscapes due to their remarkable capacity to understand, analyze,\nand generate texts based on their vast knowledge and reasoning ability.\nNevertheless, one major drawback of LLMs is their substantial computational\ncost for pre-training due to their unprecedented amounts of parameters. The\ndisadvantage is exacerbated when new knowledge frequently needs to be\nintroduced into the pre-trained model. Therefore, it is imperative to develop\neffective and efficient techniques to update pre-trained LLMs. Traditional\nmethods encode new knowledge in pre-trained LLMs through direct fine-tuning.\nHowever, naively re-training LLMs can be computationally intensive and risks\ndegenerating valuable pre-trained knowledge irrelevant to the update in the\nmodel. Recently, Knowledge-based Model Editing (KME) has attracted increasing\nattention, which aims to precisely modify the LLMs to incorporate specific\nknowledge, without negatively influencing other irrelevant knowledge. In this\nsurvey, we aim to provide a comprehensive and in-depth overview of recent\nadvances in the field of KME. We first introduce a general formulation of KME\nto encompass different KME strategies. Afterward, we provide an innovative\ntaxonomy of KME techniques based on how the new knowledge is introduced into\npre-trained LLMs, and investigate existing KME strategies while analyzing key\ninsights, advantages, and limitations of methods from each category. Moreover,\nrepresentative metrics, datasets, and applications of KME are introduced\naccordingly. Finally, we provide an in-depth analysis regarding the\npracticality and remaining challenges of KME and suggest promising research\ndirections for further advancement in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaochen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haochen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zaiyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jundong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"She had Cobalt Blue Eyes: Prompt Testing to Create Aligned and Sustainable Language Models. (arXiv:2310.18333v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.18333","description":"<p>As the use of large language models (LLMs) increases within society, as does\nthe risk of their misuse. Appropriate safeguards must be in place to ensure LLM\noutputs uphold the ethical standards of society, highlighting the positive role\nthat artificial intelligence technologies can have. Recent events indicate\nethical concerns around conventionally trained LLMs, leading to overall unsafe\nuser experiences. This motivates our research question: how do we ensure LLM\nalignment? In this work, we introduce a test suite of unique prompts to foster\nthe development of aligned LLMs that are fair, safe, and robust. We show that\nprompting LLMs at every step of the development pipeline, including data\ncuration, pre-training, and fine-tuning, will result in an overall more\nresponsible model. Our test suite evaluates outputs from four state-of-the-art\nlanguage models: GPT-3.5, GPT-4, OPT, and LLaMA-2. The assessment presented in\nthis paper highlights a gap between societal alignment and the capabilities of\ncurrent LLMs. Additionally, implementing a test suite such as ours lowers the\nenvironmental overhead of making models safe and fair.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chatrath_V/0/1/0/all/0/1\">Veronica Chatrath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamgbose_O/0/1/0/all/0/1\">Oluwanifemi Bamgbose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raza_S/0/1/0/all/0/1\">Shaina Raza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficiently Adapting Pretrained Language Models To New Languages. (arXiv:2311.05741v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.05741","description":"<p>Recent large language models (LLM) exhibit sub-optimal performance on\nlow-resource languages, as the training data of these models is usually\ndominated by English and other high-resource languages. Furthermore, it is\nchallenging to train models for low-resource languages, especially from\nscratch, due to a lack of high quality training data. Adapting pretrained LLMs\nreduces the need for data in the new language while also providing cross\nlingual transfer capabilities. However, naively adapting to new languages leads\nto catastrophic forgetting and poor tokenizer efficiency. In this work, we\nstudy how to efficiently adapt any existing pretrained LLM to a new language\nwithout running into these issues. In particular, we improve the encoding\nefficiency of the tokenizer by adding new tokens from the target language and\nstudy the data mixing recipe to mitigate forgetting. Our experiments on\nadapting an English LLM to Hungarian and Thai show that our recipe can reach\nbetter performance than open source models on the target language, with minimal\nregressions on English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Csaki_Z/0/1/0/all/0/1\">Zoltan Csaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pawakapan_P/0/1/0/all/0/1\">Pian Pawakapan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakker_U/0/1/0/all/0/1\">Urmish Thakker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Use Large Language Models for Text Coding: The Case of Fatherhood Roles in Public Policy Documents. (arXiv:2311.11844v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.11844","description":"<p>Recent advances in large language models (LLMs) like GPT-3 and GPT-4 have\nopened up new opportunities for text analysis in political science. They\npromise automation with better results and less programming. In this study, we\nevaluate LLMs on three original coding tasks of non-English political science\ntexts, and we provide a detailed description of a general workflow for using\nLLMs for text coding in political science research. Our use case offers a\npractical guide for researchers looking to incorporate LLMs into their research\non text analysis. We find that, when provided with detailed label definitions\nand coding examples, an LLM can be as good as or even better than a human\nannotator while being much faster (up to hundreds of times), considerably\ncheaper (costing up to 60% less than human coding), and much easier to scale to\nlarge amounts of text. Overall, LLMs present a viable option for most text\ncoding projects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lupo_L/0/1/0/all/0/1\">Lorenzo Lupo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magnusson_O/0/1/0/all/0/1\">Oscar Magnusson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naurin_E/0/1/0/all/0/1\">Elin Naurin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wangnerud_L/0/1/0/all/0/1\">Lena W&#xe4;ngnerud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YUAN 2.0: A Large Language Model with Localized Filtering-based Attention. (arXiv:2311.15786v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.15786","description":"<p>In this work, we develop and release Yuan 2.0, a series of large language\nmodels with parameters ranging from 2.1 billion to 102.6 billion. The Localized\nFiltering-based Attention (LFA) is introduced to incorporate prior knowledge of\nlocal dependencies of natural language into Attention. A data filtering and\ngenerating system is presented to build pre-training and fine-tuning dataset in\nhigh quality. A distributed training method with non-uniform pipeline parallel,\ndata parallel, and optimizer parallel is proposed, which greatly reduces the\nbandwidth requirements of intra-node communication, and achieves good\nperformance in large-scale distributed training. Yuan 2.0 models display\nimpressive ability in code generation, math problem-solving, and chatting\ncompared with existing models. The latest version of YUAN 2.0, including model\nweights and source code, is accessible at Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shaohua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xudong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shenling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiangang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lingjun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing the Rationale-Input Alignment for Self-explaining Rationalization. (arXiv:2312.04103v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2312.04103","description":"<p>Rationalization empowers deep learning models with self-explaining\ncapabilities through a cooperative game, where a generator selects a\nsemantically consistent subset of the input as a rationale, and a subsequent\npredictor makes predictions based on the selected rationale. In this paper, we\ndiscover that rationalization is prone to a problem named \\emph{rationale\nshift}, which arises from the algorithmic bias of the cooperative game.\nRationale shift refers to a situation where the semantics of the selected\nrationale may deviate from the original input, but the predictor still produces\naccurate predictions based on the deviation, resulting in a compromised\ngenerator with misleading feedback.\n</p>\n<p>To address this issue, we first demonstrate the importance of the alignment\nbetween the rationale and the full input through both empirical observations\nand theoretical analysis. Subsequently, we introduce a novel approach called\nDAR (\\textbf{D}iscriminatively \\textbf{A}ligned \\textbf{R}ationalization),\nwhich utilizes an auxiliary module pretrained on the full input to\ndiscriminatively align the selected rationale and the original input. We\ntheoretically illustrate how DAR accomplishes the desired alignment, thereby\novercoming the rationale shift problem. The experiments on two widely used\nreal-world benchmarks show that the proposed method significantly improves the\nexplanation quality (measured by the overlap between the model-selected\nexplanation and the human-annotated rationale) as compared to state-of-the-art\ntechniques. Additionally, results on two synthetic settings further validate\nthe effectiveness of DAR in addressing the rationale shift problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haozhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhiying Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">YuanKai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruixuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PaperQA: Retrieval-Augmented Generative Agent for Scientific Research. (arXiv:2312.07559v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.07559","description":"<p>Large Language Models (LLMs) generalize well across language tasks, but\nsuffer from hallucinations and uninterpretability, making it difficult to\nassess their accuracy without ground-truth. Retrieval-Augmented Generation\n(RAG) models have been proposed to reduce hallucinations and provide provenance\nfor how an answer was generated. Applying such models to the scientific\nliterature may enable large-scale, systematic processing of scientific\nknowledge. We present PaperQA, a RAG agent for answering questions over the\nscientific literature. PaperQA is an agent that performs information retrieval\nacross full-text scientific articles, assesses the relevance of sources and\npassages, and uses RAG to provide answers. Viewing this agent as a question\nanswering model, we find it exceeds performance of existing LLMs and LLM agents\non current science QA benchmarks. To push the field closer to how humans\nperform research on scientific literature, we also introduce LitQA, a more\ncomplex benchmark that requires retrieval and synthesis of information from\nfull-text scientific papers across the literature. Finally, we demonstrate\nPaperQA's matches expert human researchers on LitQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lala_J/0/1/0/all/0/1\">Jakub L&#xe1;la</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODonoghue_O/0/1/0/all/0/1\">Odhran O&#x27;Donoghue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shtedritski_A/0/1/0/all/0/1\">Aleksandar Shtedritski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cox_S/0/1/0/all/0/1\">Sam Cox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriques_S/0/1/0/all/0/1\">Samuel G. Rodriques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1\">Andrew D. White</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic Image-Report Generation. (arXiv:2312.08078v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2312.08078","description":"<p>To address these issues, we propose a novel Adaptive patch-word Matching\n(AdaMatch) model to correlate chest X-ray (CXR) image regions with words in\nmedical reports and apply it to CXR-report generation to provide explainability\nfor the generation process. AdaMatch exploits the fine-grained relation between\nadaptive patches and words to provide explanations of specific image regions\nwith corresponding words. To capture the abnormal regions of varying sizes and\npositions, we introduce the Adaptive Patch extraction (AdaPatch) module to\nacquire the adaptive patches for these regions adaptively. In order to provide\nexplicit explainability for CXR-report generation task, we propose an\nAdaMatch-based bidirectional large language model for Cyclic CXR-report\ngeneration (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords\nfor CXR images and `keypatches' for medical reports as hints to guide\nCXR-report generation. Extensive experiments on two publicly available CXR\ndatasets prove the effectiveness of our method and its superior performance to\nexisting methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yixuan Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-throughput Biomedical Relation Extraction for Semi-Structured Web Articles Empowered by Large Language Models. (arXiv:2312.08274v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.08274","description":"<p>Objective: To develop a high-throughput biomedical relation extraction system\nthat takes advantage of the large language models' (LLMs) reading comprehension\nability and biomedical world knowledge in a scalable and evidential manner.\nMethods: We formulate the relation extraction task as a simple binary\nclassification problem for large language models such as ChatGPT. Specifically,\nLLMs make the decision based on the external corpus and its world knowledge,\ngiving the reason for the judgment to factual verification. This method is\ntailored for semi-structured web articles, wherein we designate the main title\nas the tail entity and explicitly incorporate it into the context, and the\npotential head entities are matched based on a biomedical thesaurus. Moreover,\nlengthy contents are sliced into text chunks, embedded, and retrieved with\nadditional embedding models, ensuring compatibility with the context window\nsize constraints of available open-source LLMs. Results: Using an open-source\nLLM, we extracted 304315 relation triplets of three distinct relation types\nfrom four reputable biomedical websites. To assess the efficacy of the basic\npipeline employed for biomedical relation extraction, we curated a benchmark\ndataset annotated by a medical expert. Evaluation results indicate that the\npipeline exhibits performance comparable to that of GPT-4. Case studies further\nilluminate challenges faced by contemporary LLMs in the context of biomedical\nrelation extraction for semi-structured web articles. Conclusion: The proposed\nmethod has demonstrated its effectiveness in leveraging the strengths of LLMs\nfor high-throughput biomedical relation extraction. Its adaptability is\nevident, as it can be seamlessly extended to diverse semi-structured biomedical\nwebsites, facilitating the extraction of various types of biomedical relations\nwith ease.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Songchi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sheng Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting LLMs with content plans to enhance the summarization of scientific articles. (arXiv:2312.08282v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.08282","description":"<p>This paper presents novel prompting techniques to improve the performance of\nautomatic summarization systems for scientific articles. Scientific article\nsummarization is highly challenging due to the length and complexity of these\ndocuments. We conceive, implement, and evaluate prompting techniques that\nprovide additional contextual information to guide summarization systems.\nSpecifically, we feed summarizers with lists of key terms extracted from\narticles, such as author keywords or automatically generated keywords. Our\ntechniques are tested with various summarization models and input texts.\nResults show performance gains, especially for smaller models summarizing\nsections separately. This evidences that prompting is a promising approach to\novercoming the limitations of less powerful systems. Our findings introduce a\nnew research direction of using prompts to aid smaller models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Creo_A/0/1/0/all/0/1\">Aldan Creo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lama_M/0/1/0/all/0/1\">Manuel Lama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_J/0/1/0/all/0/1\">Juan C. Vidal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TigerBot: An Open Multilingual Multitask LLM. (arXiv:2312.08688v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.08688","description":"<p>We release and introduce the TigerBot family of large language models (LLMs),\nconsisting of base and chat models, sized from 7, 13, 70 and 180 billion\nparameters. We develop our models embarking from Llama-2 and BLOOM, and push\nthe boundary further in data, training algorithm, infrastructure, and\napplication tools. Our models yield meaningful performance gain over SOTA\nopen-source models, e.g., Llama-2, specifically 6% gain in English and 20% gain\nin Chinese. TigerBot model family also achieves leading performance in major\nacademic and industrial benchmarks and leaderboards. We believe that TigerBot\nrepresents just a snapshot of lightning-fast progression in LLM open-source\ncommunity. Therefore, we are thrilled to give back by publicly releasing our\nmodels and reporting our approach behind, with additional emphases on building\nSOTA LLMs in a democratized way and making LLMs of use in real-world\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ye Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Wei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liangmin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaowei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_Z/0/1/0/all/0/1\">Zhanxuan Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Cong Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Outlier Activations in Low-Precision Fine-Tuning of Language Models. (arXiv:2312.09211v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.09211","description":"<p>Low-precision fine-tuning of language models has gained prominence as a\ncost-effective and energy-efficient approach to deploying large-scale models in\nvarious applications. However, this approach is susceptible to the existence of\noutlier values in activation. The outlier values in the activation can\nnegatively affect the performance of fine-tuning language models in the\nlow-precision regime since they affect the scaling factor and thus make\nrepresenting smaller values harder. This paper investigates techniques for\nmitigating outlier activation in low-precision integer fine-tuning of the\nlanguage models. Our proposed novel approach enables us to represent the\noutlier activation values in 8-bit integers instead of floating-point (FP16)\nvalues. The benefit of using integers for outlier values is that it enables us\nto use operator tiling to avoid performing 16-bit integer matrix multiplication\nto address this problem effectively. We provide theoretical analysis and\nsupporting experiments to demonstrate the effectiveness of our approach in\nimproving the robustness and performance of low-precision fine-tuned language\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghaffari_A/0/1/0/all/0/1\">Alireza Ghaffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Justin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nejad_M/0/1/0/all/0/1\">Mahsa Ghazvini Nejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asgharian_M/0/1/0/all/0/1\">Masoud Asgharian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nia_V/0/1/0/all/0/1\">Vahid Partovi Nia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-12-17T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/"}}]}]}