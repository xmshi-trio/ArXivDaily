{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-11-21T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Large Language Models in Finance: A Survey. (arXiv:2311.10723v1 [q-fin.GN])","link":"http://arxiv.org/abs/2311.10723","description":"<p>Recent advances in large language models (LLMs) have opened new possibilities\nfor artificial intelligence applications in finance. In this paper, we provide\na practical survey focused on two key aspects of utilizing LLMs for financial\ntasks: existing solutions and guidance for adoption.\n</p>\n<p>First, we review current approaches employing LLMs in finance, including\nleveraging pretrained models via zero-shot or few-shot learning, fine-tuning on\ndomain-specific data, and training custom LLMs from scratch. We summarize key\nmodels and evaluate their performance improvements on financial natural\nlanguage processing tasks.\n</p>\n<p>Second, we propose a decision framework to guide financial professionals in\nselecting the appropriate LLM solution based on their use case constraints\naround data, compute, and performance needs. The framework provides a pathway\nfrom lightweight experimentation to heavy investment in customized LLMs.\n</p>\n<p>Lastly, we discuss limitations and challenges around leveraging LLMs in\nfinancial applications. Overall, this survey aims to synthesize the\nstate-of-the-art and provide a roadmap for responsibly applying LLMs to advance\nfinancial AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Li_Y/0/1/0/all/0/1\">Yinheng Li</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Wang_S/0/1/0/all/0/1\">Shaofei Wang</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Ding_H/0/1/0/all/0/1\">Han Ding</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Chen_H/0/1/0/all/0/1\">Hang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chatbot-supported Thesis Writing: An Autoethnographic Report. (arXiv:2311.10729v1 [cs.CY])","link":"http://arxiv.org/abs/2311.10729","description":"<p>The release of the large language model based chatbot ChatGPT in November\n2022 has brought considerable attention to the subject of artificial\nintelligence, not only in the public. From the perspective of higher education,\nChatGPT challenges various learning and assessment formats as it significantly\nreduces the effectiveness of their learning and assessment functionalities. In\nparticular, ChatGPT might be applied to formats that require learners to\ngenerate text, such as bachelor theses or student research papers. Accordingly,\nthe research question arises to what extent writing of bachelor theses is still\na valid learning and assessment format. Correspondingly, in this study, the\nfirst author was asked to write his bachelor's thesis exploiting ChatGPT. For\ntracing the impact of ChatGPT, methodically an autoethnographic approach was\nused. First, all considerations on the potential use of ChatGPT were documented\nin logs and secondly, all ChatGPT chats were logged. Both logs and chat\nhistories were analyzed and are presented along to the recommendations for\nstudents regarding the use of ChatGPT suggested by Gimpel et al. (2023). In\nconclusion, ChatGPT is beneficial in thesis writing during various activities,\nsuch as brainstorming, structuring and text revision. However, there arise\nlimitations, e.g., in referencing. Thus, ChatGPT requires a continuous\nvalidation of the outcomes generated fostering learning. Currently, ChatGPT is\nto be valued as a beneficial tool in thesis writing. However, writing a\nconclusive thesis still requires the learner's meaningful engagement.\nAccordingly, writing a thesis is still a valid learning and assessment format.\nWith further releases of ChatGPT, an increase in capabilities is to be expected\nand the research question needs to be reevaluated from time to time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwenke_N/0/1/0/all/0/1\">Nicolas Schwenke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sobke_H/0/1/0/all/0/1\">Heinrich S&#xf6;bke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraft_E/0/1/0/all/0/1\">Eckhard Kraft</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proceedings of the 3rd International Workshop on Mining and Learning in the Legal Domain (MLLD-23). (arXiv:2311.10733v1 [cs.CY])","link":"http://arxiv.org/abs/2311.10733","description":"<p>This is the Proceedings of the 3rd International Workshop on Mining and\nLearning in the Legal Domain (MLLD-23) which took place in conjunction with the\n32nd ACM International Conference on Information and Knowledge Management\n(CIKM-2023) at the University of Birmingham, Birmingham, UK on Sunday 22nd\nOctober 2023.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Makrehchi_M/0/1/0/all/0/1\">Masoud Makrehchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dell Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrova_A/0/1/0/all/0/1\">Alina Petrova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armour_J/0/1/0/all/0/1\">John Armour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EIT: Earnest Insight Toolkit for Evaluating Students' Earnestness in Interactive Lecture Participation Exercises. (arXiv:2311.10746v1 [cs.CY])","link":"http://arxiv.org/abs/2311.10746","description":"<p>In today's rapidly evolving educational landscape, traditional modes of\npassive information delivery are giving way to transformative pedagogical\napproaches that prioritize active student engagement. Within the context of\nlarge-scale hybrid classrooms, the challenge lies in fostering meaningful and\nactive interaction between students and course content. This study delves into\nthe significance of measuring students' earnestness during interactive lecture\nparticipation exercises. By analyzing students' responses to interactive\nlecture poll questions, establishing a clear rubric for evaluating earnestness,\nand conducting a comprehensive assessment, we introduce EIT (Earnest Insight\nToolkit), a tool designed to assess students' engagement within interactive\nlecture participation exercises - particularly in the context of large-scale\nhybrid classrooms. Through the utilization of EIT, our objective is to equip\neducators with valuable means of identifying at-risk students for enhancing\nintervention and support strategies, as well as measuring students' levels of\nengagement with course content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miroyan_M/0/1/0/all/0/1\">Mihran Miroyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_S/0/1/0/all/0/1\">Shiny Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rahul Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Lisa Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norouzi_N/0/1/0/all/0/1\">Narges Norouzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Five Accountable Talk Moves to Improve Instruction at Scale. (arXiv:2311.10749v1 [cs.CY])","link":"http://arxiv.org/abs/2311.10749","description":"<p>Providing consistent, individualized feedback to teachers on their\ninstruction can improve student learning outcomes. Such feedback can especially\nbenefit novice instructors who teach on online platforms and have limited\naccess to instructional training. To build scalable measures of instruction, we\nfine-tune RoBERTa and GPT models to identify five instructional talk moves\ninspired by accountable talk theory: adding on, connecting, eliciting, probing\nand revoicing students' ideas. We fine-tune these models on a newly annotated\ndataset of 2500 instructor utterances derived from transcripts of small group\ninstruction in an online computer science course, Code in Place. Although we\nfind that GPT-3 consistently outperforms RoBERTa in terms of precision, its\nrecall varies significantly. We correlate the instructors' use of each talk\nmove with indicators of student engagement and satisfaction, including\nstudents' section attendance, section ratings, and assignment completion rates.\nWe find that using talk moves generally correlates positively with student\noutcomes, and connecting student ideas has the largest positive impact. These\nresults corroborate previous research on the effectiveness of accountable talk\nmoves and provide exciting avenues for using these models to provide\ninstructors with useful, scalable feedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kupor_A/0/1/0/all/0/1\">Ashlee Kupor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_C/0/1/0/all/0/1\">Candice Morgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demszky_D/0/1/0/all/0/1\">Dorottya Demszky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProAgent: From Robotic Process Automation to Agentic Process Automation. (arXiv:2311.10751v1 [cs.RO])","link":"http://arxiv.org/abs/2311.10751","description":"<p>From ancient water wheels to robotic process automation (RPA), automation\ntechnology has evolved throughout history to liberate human beings from arduous\ntasks. Yet, RPA struggles with tasks needing human-like intelligence,\nespecially in elaborate design of workflow construction and dynamic\ndecision-making in workflow execution. As Large Language Models (LLMs) have\nemerged human-like intelligence, this paper introduces Agentic Process\nAutomation (APA), a groundbreaking automation paradigm using LLM-based agents\nfor advanced automation by offloading the human labor to agents associated with\nconstruction and execution. We then instantiate ProAgent, an LLM-based agent\ndesigned to craft workflows from human instructions and make intricate\ndecisions by coordinating specialized agents. Empirical experiments are\nconducted to detail its construction and execution procedure of workflow,\nshowcasing the feasibility of APA, unveiling the possibility of a new paradigm\nof automation driven by agents. Our code is public at\nhttps://github.com/OpenBMB/ProAgent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yining Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1\">Xin Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shizuo Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiannan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yaxi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Heyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huadong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Contentious Terms About People and Cultures are Used in Linked Open Data. (arXiv:2311.10757v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10757","description":"<p>Web resources in linked open data (LOD) are comprehensible to humans through\nliteral textual values attached to them, such as labels, notes, or comments.\nWord choices in literals may not always be neutral. When outdated and\nculturally stereotyping terminology is used in literals, they may appear as\noffensive to users in interfaces and propagate stereotypes to algorithms\ntrained on them. We study how frequently and in which literals contentious\nterms about people and cultures occur in LOD and whether there are attempts to\nmark the usage of such terms. For our analysis, we reuse English and Dutch\nterms from a knowledge graph that provides opinions of experts from the\ncultural heritage domain about terms' contentiousness. We inspect occurrences\nof these terms in four widely used datasets: Wikidata, The Getty Art &amp;\nArchitecture Thesaurus, Princeton WordNet, and Open Dutch WordNet. Some terms\nare ambiguous and contentious only in particular senses. Applying word sense\ndisambiguation, we generate a set of literals relevant to our analysis. We\nfound that outdated, derogatory, stereotyping terms frequently appear in\ndescriptive and labelling literals, such as preferred labels that are usually\ndisplayed in interfaces and used for indexing. In some cases, LOD contributors\nmark contentious terms with words and phrases in literals (implicit markers) or\nproperties linked to resources (explicit markers). However, such marking is\nrare and non-consistent in all datasets. Our quantitative and qualitative\ninsights could be helpful in developing more systematic approaches to address\nthe propagation of stereotypes via LOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nesterov_A/0/1/0/all/0/1\">Andrei Nesterov</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hollink_L/0/1/0/all/0/1\">Laura Hollink</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ossenbruggen_J/0/1/0/all/0/1\">Jacco van Ossenbruggen</a> (2) ((1) Centrum Wiskunde &amp; Informatica, (2) VU University Amsterdam)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Parametric Memory Guidance for Multi-Document Summarization. (arXiv:2311.10760v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10760","description":"<p>Multi-document summarization (MDS) is a difficult task in Natural Language\nProcessing, aiming to summarize information from several documents. However,\nthe source documents are often insufficient to obtain a qualitative summary. We\npropose a retriever-guided model combined with non-parametric memory for\nsummary generation. This model retrieves relevant candidates from a database\nand then generates the summary considering the candidates with a copy mechanism\nand the source documents. The retriever is implemented with Approximate Nearest\nNeighbor Search (ANN) to search large databases. Our method is evaluated on the\nMultiXScience dataset which includes scientific articles. Finally, we discuss\nour results and possible directions for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baud_F/0/1/0/all/0/1\">Florian Baud</a> (LIRIS), <a href=\"http://arxiv.org/find/cs/1/au:+Aussem_A/0/1/0/all/0/1\">Alex Aussem</a> (LIRIS)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Generalization in Learning with Limited Numbers of Exemplars: Transformer vs. RNN in Attractor Dynamics. (arXiv:2311.10763v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10763","description":"<p>ChatGPT, a widely-recognized large language model (LLM), has recently gained\nsubstantial attention for its performance scaling, attributed to the billions\nof web-sourced natural language sentences used for training. Its underlying\narchitecture, Transformer, has found applications across diverse fields,\nincluding video, audio signals, and robotic movement. %The crucial question\nthis raises concerns the Transformer's generalization-in-learning (GIL)\ncapacity. However, this raises a crucial question about Transformer's\ngeneralization in learning (GIL) capacity. Is ChatGPT's success chiefly due to\nthe vast dataset used for training, or is there more to the story? To\ninvestigate this, we compared Transformer's GIL capabilities with those of a\ntraditional Recurrent Neural Network (RNN) in tasks involving attractor\ndynamics learning. For performance evaluation, the Dynamic Time Warping (DTW)\nmethod has been employed. Our simulation results suggest that under conditions\nof limited data availability, Transformer's GIL abilities are markedly inferior\nto those of RNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fukushima_R/0/1/0/all/0/1\">Rui Fukushima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tani_J/0/1/0/all/0/1\">Jun Tani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Machine Translation through Advanced In-Context Learning: A Methodological Strategy for GPT-4 Improvement. (arXiv:2311.10765v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10765","description":"<p>The challenge of improving translation accuracy in GPT-4 is being addressed\nby harnessing a method known as in-context learning. This paper introduces a\nstrategic approach to utilize in-context learning specifically for machine\ntranslation, aiming to significantly boost accuracy. The crux of this method\nlies in the judicious selection of demonstrations that are most effective for\nin-context learning. By selecting these examples carefully, GPT-4 can utilize\nthem to achieve remarkably accurate machine translations, eliminating the need\nfor task-specific fine-tuning. This technique is anchored in the semantic\nsimilarities between the user's prompt and the chosen dataset. Sentences from\nthis dataset, carefully picked for their relevance and clarity, serve as potent\ndemonstrations for in-context learning. This approach not only enhances\ntranslation accuracy but also enriches the understanding of nuanced linguistic\nstructures. It represents a significant step forward in machine learning,\nleveraging the inherent capabilities of GPT-4 to provide translations that are\nnot only accurate but also contextually rich and linguistically sophisticated.\nThis method demonstrates the potential of in-context learning in overcoming\nlanguage barriers, opening new avenues for cross-cultural communication and\nglobal collaboration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Value FULCRA: Mapping Large Language Models to the Multidimensional Spectrum of Basic Human Values. (arXiv:2311.10766v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10766","description":"<p>The rapid advancement of Large Language Models (LLMs) has attracted much\nattention to value alignment for their responsible development. However, how to\ndefine values in this context remains a largely unexplored question. Existing\nwork mainly follows the Helpful, Honest, Harmless principle and specifies\nvalues as risk criteria formulated in the AI community, e.g., fairness and\nprivacy protection, suffering from poor clarity, adaptability and transparency.\nInspired by basic values in humanity and social science across cultures, this\nwork proposes a novel basic value alignment paradigm and introduces a value\nspace spanned by basic value dimensions. All LLMs' behaviors can be mapped into\nthe space by identifying the underlying values, possessing the potential to\naddress the three challenges. To foster future research, we apply the\nrepresentative Schwartz's Theory of Basic Values as an initialized example and\nconstruct FULCRA, a dataset consisting of 5k (LLM output, value vector) pairs.\nOur extensive analysis of FULCRA reveals the underlying relation between basic\nvalues and LLMs' behaviors, demonstrating that our approach not only covers\nexisting mainstream risks but also anticipates possibly unidentified ones.\nAdditionally, we present an initial implementation of the basic value\nevaluation and alignment, paving the way for future research in this line.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jing Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1\">Xiaoyuan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory Augmented Language Models through Mixture of Word Experts. (arXiv:2311.10768v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10768","description":"<p>Scaling up the number of parameters of language models has proven to be an\neffective approach to improve performance. For dense models, increasing model\nsize proportionally increases the model's computation footprint. In this work,\nwe seek to aggressively decouple learning capacity and FLOPs through\nMixture-of-Experts (MoE) style models with large knowledge-rich vocabulary\nbased routing functions and experts. Our proposed approach, dubbed Mixture of\nWord Experts (MoWE), can be seen as a memory augmented model, where a large set\nof word-specific experts play the role of a sparse memory. We demonstrate that\nMoWE performs significantly better than the T5 family of models with similar\nnumber of FLOPs in a variety of NLP tasks. Additionally, MoWE outperforms\nregular MoE models on knowledge intensive tasks and has similar performance to\nmore complex memory augmented approaches that often require to invoke custom\nmechanisms to search the sparse memory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santos_C/0/1/0/all/0/1\">Cicero Nogueira dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Thorp_J/0/1/0/all/0/1\">James Lee-Thorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noble_I/0/1/0/all/0/1\">Isaac Noble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chung-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uthus_D/0/1/0/all/0/1\">David Uthus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exponentially Faster Language Modelling. (arXiv:2311.10770v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10770","description":"<p>Language models only really need to use an exponential fraction of their\nneurons for individual inferences. As proof, we present FastBERT, a BERT\nvariant that uses 0.3\\% of its neurons during inference while performing on par\nwith similar BERT models. FastBERT selectively engages just 12 out of 4095\nneurons for each layer inference. This is achieved by replacing feedforward\nnetworks with fast feedforward networks (FFFs). While no truly efficient\nimplementation currently exists to unlock the full acceleration potential of\nconditional neural execution, we provide high-level CPU code achieving 78x\nspeedup over the optimized baseline feedforward implementation, and a PyTorch\nimplementation delivering 40x speedup over the equivalent batched feedforward\ninference. We publish our training code, benchmarking setup, and model weights.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belcak_P/0/1/0/all/0/1\">Peter Belcak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1\">Roger Wattenhofer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Restoration of Diacritics for Speech Data Sets. (arXiv:2311.10771v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10771","description":"<p>Automatic text-based diacritic restoration models generally have high\ndiacritic error rates when applied to speech transcripts as a result of domain\nand style shifts in spoken language. In this work, we explore the possibility\nof improving the performance of automatic diacritic restoration when applied to\nspeech data by utilizing the parallel spoken utterances. In particular, we use\nthe pre-trained Whisper ASR model fine-tuned on relatively small amounts of\ndiacritized Arabic speech data to produce rough diacritized transcripts for the\nspeech utterances, which we then use as an additional input for a\ntransformer-based diacritic restoration model. The proposed model consistently\nimprove diacritic restoration performance compared to an equivalent text-only\nmodel, with at least 5\\% absolute reduction in diacritic error rate within the\nsame domain and on two out-of-domain test sets. Our results underscore the\ninadequacy of current text-based diacritic restoration models for speech data\nsets and provide a new baseline for speech-based diacritic restoration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shatnawi_S/0/1/0/all/0/1\">Sara Shatnawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alqahtani_S/0/1/0/all/0/1\">Sawsan Alqahtani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldarmaki_H/0/1/0/all/0/1\">Hanan Aldarmaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning. (arXiv:2311.10774v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10774","description":"<p>With the rapid development of large language models (LLMs) and their\nintegration into large multimodal models (LMMs), there has been impressive\nprogress in zero-shot completion of user-oriented vision-language tasks.\nHowever, a gap remains in the domain of chart image understanding due to the\ndistinct abstract components in charts. To address this, we introduce a\nlarge-scale MultiModal Chart Instruction (MMC-Instruction) dataset comprising\n600k instances supporting diverse tasks and chart types. Leveraging this data,\nwe develop MultiModal Chart Assistant (MMCA), an LMM that achieves\nstate-of-the-art performance on existing chart QA benchmarks. Recognizing the\nneed for a comprehensive evaluation of LMM chart understanding, we also propose\na MultiModal Chart Benchmark (MMC-Benchmark), a comprehensive human-annotated\nbenchmark with 9 distinct tasks evaluating reasoning capabilities over charts.\nExtensive experiments on MMC-Benchmark reveal the limitations of existing LMMs\non correctly interpreting charts, even for the most recent GPT-4V model. Our\nwork provides an instruction-tuning methodology and benchmark to advance\nmultimodal understanding of charts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fuxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wenlin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianshu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaiqiang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sangwoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yacoob_Y/0/1/0/all/0/1\">Yaser Yacoob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ToolTalk: Evaluating Tool-Usage in a Conversational Setting. (arXiv:2311.10775v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10775","description":"<p>Large language models (LLMs) have displayed massive improvements in reasoning\nand decision-making skills and can hold natural conversations with users. Many\nrecent works seek to augment LLM-based assistants with external tools so they\ncan access private or up-to-date information and carry out actions on behalf of\nusers. To better measure the performance of these assistants, this paper\nintroduces ToolTalk, a benchmark consisting of complex user intents requiring\nmulti-step tool usage specified through dialogue. ToolTalk contains 28 tools\ngrouped into 7 plugins, and includes a complete simulated implementation of\neach tool, allowing for fully automated evaluation of assistants that rely on\nexecution feedback. ToolTalk also emphasizes tools that externally affect the\nworld rather than only tools for referencing or searching information. We\nevaluate GPT-3.5 and GPT-4 on ToolTalk resulting in success rates of 26% and\n50% respectively. Our analysis of the errors reveals three major categories and\nsuggests some future directions for improvement. We release ToolTalk at\nhttps://github.com/microsoft/ToolTalk.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farn_N/0/1/0/all/0/1\">Nicholas Farn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_R/0/1/0/all/0/1\">Richard Shin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Systematic Review of Aspect-based Sentiment Analysis (ABSA): Domains, Methods, and Trends. (arXiv:2311.10777v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10777","description":"<p>Aspect-based Sentiment Analysis (ABSA) is a type of fine-grained sentiment\nanalysis (SA) that identifies aspects and the associated opinions from a given\ntext. In the digital era, ABSA gained increasing popularity and applications in\nmining opinionated text data to obtain insights and support decisions. ABSA\nresearch employs linguistic, statistical, and machine-learning approaches and\nutilises resources such as labelled datasets, aspect and sentiment lexicons and\nontology. By its nature, ABSA is domain-dependent and can be sensitive to the\nimpact of misalignment between the resource and application domains. However,\nto our knowledge, this topic has not been explored by the existing ABSA\nliterature reviews. In this paper, we present a Systematic Literature Review\n(SLR) of ABSA studies with a focus on the research application domain, dataset\ndomain, and the research methods to examine their relationships and identify\ntrends over time. Our results suggest a number of potential systemic issues in\nthe ABSA research literature, including the predominance of the\n``product/service review'' dataset domain among the majority of studies that\ndid not have a specific research application domain, coupled with the\nprevalence of dataset-reliant methods such as supervised machine learning. This\nreview makes a number of unique contributions to the ABSA research field: 1) To\nour knowledge, it is the first SLR that links the research domain, dataset\ndomain, and research method through a systematic perspective; 2) it is one of\nthe largest scoped SLR on ABSA, with 519 eligible studies filtered from 4191\nsearch results without time constraint; and 3) our review methodology adopted\nan innovative automatic filtering process based on PDF-mining, which enhanced\nscreening quality and reliability. Suggestions and our review limitations are\nalso discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yan Cathy Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1\">Paul Denny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taskova_K/0/1/0/all/0/1\">Katerina Taskova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wicker_J/0/1/0/all/0/1\">J&#xf6;erg Wicker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Language Model Moderators Improve the Health of Online Discourse?. (arXiv:2311.10781v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10781","description":"<p>Human moderation of online conversation is essential to maintaining civility\nand focus in a dialogue, but is challenging to scale and harmful to moderators.\nThe inclusion of sophisticated natural language generation modules as a force\nmultiplier aid moderators is a tantalizing prospect, but adequate evaluation\napproaches have so far been elusive. In this paper, we establish a systematic\ndefinition of conversational moderation effectiveness through a\nmultidisciplinary lens that incorporates insights from social science. We then\npropose a comprehensive evaluation framework that uses this definition to asses\nmodels' moderation capabilities independently of human intervention. With our\nframework, we conduct the first known study of conversational dialogue models\nas moderators, finding that appropriately prompted models can provide specific\nand fair feedback on toxic behavior but struggle to influence users to increase\ntheir levels of respect and cooperation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyundong Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1\">Taiwei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1\">Darpan Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizk_B/0/1/0/all/0/1\">Basem Rizk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zixun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_N/0/1/0/all/0/1\">Nuan Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gratch_J/0/1/0/all/0/1\">Jonathan Gratch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrara_E/0/1/0/all/0/1\">Emilio Ferrara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A BERT based Ensemble Approach for Sentiment Classification of Customer Reviews and its Application to Nudge Marketing in e-Commerce. (arXiv:2311.10782v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10782","description":"<p>According to the literature, Product reviews are an important source of\ninformation for customers to support their buying decision. Product reviews\nimprove customer trust and loyalty. Reviews help customers in understanding\nwhat other customers think about a particular product and helps in driving\npurchase decisions. Therefore, for an e-commerce platform it is important to\nunderstand the sentiments in customer reviews to understand their products and\nservices, and it also allows them to potentially create positive consumer\ninteraction as well as long lasting relationships. Reviews also provide\ninnovative ways to market the products for an ecommerce company. One such\napproach is Nudge Marketing. Nudge marketing is a subtle way for an ecommerce\ncompany to help their customers make better decisions without hesitation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Putatunda_S/0/1/0/all/0/1\">Sayan Putatunda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhowmik_A/0/1/0/all/0/1\">Anwesha Bhowmik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiruvenkadam_G/0/1/0/all/0/1\">Girish Thiruvenkadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1\">Rahul Ghosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ExFake: Towards an Explainable Fake News Detection Based on Content and Social Context Information. (arXiv:2311.10784v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10784","description":"<p>ExFake is an explainable fake news detection system based on content and\ncontext-level information. It is concerned with the veracity analysis of online\nposts based on their content, social context (i.e., online users' credibility\nand historical behaviour), and data coming from trusted entities such as\nfact-checking websites and named entities. Unlike state-of-the-art systems, an\nExplainable AI (XAI) assistant is also adopted to help online social networks\n(OSN) users develop good reflexes when faced with any doubted information that\nspreads on social networks. The trustworthiness of OSN users is also addressed\nby assigning a credibility score to OSN users, as OSN users are one of the main\nculprits for spreading fake news. Experimental analysis on a real-world dataset\ndemonstrates that ExFake significantly outperforms other baseline methods for\nfake news detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amri_S/0/1/0/all/0/1\">Sabrine Amri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boleilanga_H/0/1/0/all/0/1\">Henri-Cedric Mputu Boleilanga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aimeur_E/0/1/0/all/0/1\">Esma A&#xef;meur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Sanitization Beyond Specific Domains: Zero-Shot Redaction & Substitution with Large Language Models. (arXiv:2311.10785v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10785","description":"<p>In the context of information systems, text sanitization techniques are used\nto identify and remove sensitive data to comply with security and regulatory\nrequirements. Even though many methods for privacy preservation have been\nproposed, most of them are focused on the detection of entities from specific\ndomains (e.g., credit card numbers, social security numbers), lacking\ngenerality and requiring customization for each desirable domain. Moreover,\nremoving words is, in general, a drastic measure, as it can degrade text\ncoherence and contextual information. Less severe measures include substituting\na word for a safe alternative, yet it can be challenging to automatically find\nmeaningful substitutions. We present a zero-shot text sanitization technique\nthat detects and substitutes potentially sensitive information using Large\nLanguage Models. Our evaluation shows that our method excels at protecting\nprivacy while maintaining text coherence and contextual information, preserving\ndata utility for downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albanese_F/0/1/0/all/0/1\">Federico Albanese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciolek_D/0/1/0/all/0/1\">Daniel Ciolek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DIppolito_N/0/1/0/all/0/1\">Nicolas D&#x27;Ippolito</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TaCo: Enhancing Cross-Lingual Transfer for Low-Resource Languages in LLMs through Translation-Assisted Chain-of-Thought Processes. (arXiv:2311.10797v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10797","description":"<p>LLMs such as ChatGPT and PaLM can be utilized to train on a new language and\nrevitalize low-resource languages. However, it is evidently very costly to\npretrain pr fine-tune LLMs to adopt new languages. Another challenge is the\nlimitation of benchmark datasets and the metrics used to measure the\nperformance of models in multilingual settings. This paper proposes\ncost-effective solutions to both of the aforementioned challenges. We introduce\nthe Multilingual Instruction-Tuning Dataset (MITS), which is comprised of the\ntranslation of Alpaca-52K, Dolly-15K, and Vicuna Benchmark in 132 languages.\nAlso, we propose a new method called \\emph{TaCo: Translation-Assisted\nCross-Linguality}, which make uses of translation in a chain-of-thought process\nto instruction-tune LLMs on a new languages through a curriculum learning\nprocess. As a proof of concept, we experimented with the instruction-tuned\nGuanaco-33B model and performed further instruction tuning using the TaCo\nmethod in three low-resource languages and one high-resource language. Our\nresults show that the TaCo method impresses the GPT-4 with 82% for a\nlow-resource language in the Vicuna Benchmark dataset, and boosts performance\nby double in contrast to the performance of instruction tuning only. Our\nresults show that TaCo is a promising method for creating multilingual LLMs,\neven for low-resource languages. We have released our datasets and the model\nadapters, and encourage the research community to make use of these resources\ntowards advancing work on multilingual LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Upadhayay_B/0/1/0/all/0/1\">Bibek Upadhayay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behzadan_V/0/1/0/all/0/1\">Vahid Behzadan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study on Altering the Latent Space of Pretrained Text to Speech Models for Improved Expressiveness. (arXiv:2311.10804v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10804","description":"<p>This report explores the challenge of enhancing expressiveness control in\nText-to-Speech (TTS) models by augmenting a frozen pretrained model with a\nDiffusion Model that is conditioned on joint semantic audio/text embeddings.\nThe paper identifies the challenges encountered when working with a VAE-based\nTTS model and evaluates different image-to-image methods for altering latent\nspeech features. Our results offer valuable insights into the complexities of\nadding expressiveness control to TTS systems and open avenues for future\nresearch in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vogel_M/0/1/0/all/0/1\">Mathias Vogel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Use GPT-J Prompt Generation with RoBERTa for NER Models on Diagnosis Extraction of Periodontal Diagnosis from Electronic Dental Records. (arXiv:2311.10810v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10810","description":"<p>This study explored the usability of prompt generation on named entity\nrecognition (NER) tasks and the performance in different settings of the\nprompt. The prompt generation by GPT-J models was utilized to directly test the\ngold standard as well as to generate the seed and further fed to the RoBERTa\nmodel with the spaCy package. In the direct test, a lower ratio of negative\nexamples with higher numbers of examples in prompt achieved the best results\nwith a F1 score of 0.72. The performance revealed consistency, 0.92-0.97 in the\nF1 score, in all settings after training with the RoBERTa model. The study\nhighlighted the importance of seed quality rather than quantity in feeding NER\nmodels. This research reports on an efficient and accurate way to mine clinical\nnotes for periodontal diagnoses, allowing researchers to easily and quickly\nbuild a NER model with the prompt generation approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yao-Shun Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chun-Teh Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandon_R/0/1/0/all/0/1\">Ryan Brandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1\">Duong Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tokede_O/0/1/0/all/0/1\">Oluwabunmi Tokede</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walji_M/0/1/0/all/0/1\">Muhammad F. Walji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Language Agent for Autonomous Driving. (arXiv:2311.10813v1 [cs.CV])","link":"http://arxiv.org/abs/2311.10813","description":"<p>Human-level driving is an ultimate goal of autonomous driving. Conventional\napproaches formulate autonomous driving as a perception-prediction-planning\nframework, yet their systems do not capitalize on the inherent reasoning\nability and experiential knowledge of humans. In this paper, we propose a\nfundamental paradigm shift from current pipelines, exploiting Large Language\nModels (LLMs) as a cognitive agent to integrate human-like intelligence into\nautonomous driving systems. Our approach, termed Agent-Driver, transforms the\ntraditional autonomous driving pipeline by introducing a versatile tool library\naccessible via function calls, a cognitive memory of common sense and\nexperiential knowledge for decision-making, and a reasoning engine capable of\nchain-of-thought reasoning, task planning, motion planning, and\nself-reflection. Powered by LLMs, our Agent-Driver is endowed with intuitive\ncommon sense and robust reasoning capabilities, thus enabling a more nuanced,\nhuman-like approach to autonomous driving. We evaluate our approach on the\nlarge-scale nuScenes benchmark, and extensive experiments substantiate that our\nAgent-Driver significantly outperforms the state-of-the-art driving methods by\na large margin. Our approach also demonstrates superior interpretability and\nfew-shot learning ability to these methods. Project page:\n\\href{https://github.com/USC-GVL/Agent-Driver/blob/main/index.html}{here}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiageng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yuxi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Token-level Adaptation of LoRA Adapters for Downstream Task Generalization. (arXiv:2311.10847v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10847","description":"<p>This paper introduces a method for adapting LoRA adapters in smaller-sized\nlanguage models to arbitrary downstream tasks. Unlike standard\nmixture-of-expert architectures, our method employs a gradient-free routing\nfunction to choose a weighted combination of experts without increasing the\ncompute requirements for training or inference. The results show that\ntoken-level adaptation of LoRA adapters outperforms the base Llama-2-7b model\nacross mathematical (GSM8K), scientific (ARC-Challenge), reading comprehension\n(SQuAD), and coding (CodeAlpaca-20k) tasks. Further evaluations also show that\nthe average performance of token-level adaptation outperforms individual models\nfine-tuned for each of the tasks with the best performance observed in\nadaptation of every-other token during inference. The code for this study is\nmade available through a public repository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belofsky_J/0/1/0/all/0/1\">Joshua Belofsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Formal concept analysis for evaluating intrinsic dimension of a natural language. (arXiv:2311.10862v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10862","description":"<p>Some results of a computational experiment for determining the intrinsic\ndimension of linguistic varieties for the Bengali and Russian languages are\npresented. At the same time, both sets of words and sets of bigrams in these\nlanguages were considered separately. The method used to solve this problem was\nbased on formal concept analysis algorithms. It was found that the intrinsic\ndimensions of these languages are significantly less than the dimensions used\nin popular neural network models in natural language processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_S/0/1/0/all/0/1\">Sergei O. Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gromov_V/0/1/0/all/0/1\">Vasilii A. Gromov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borodin_N/0/1/0/all/0/1\">Nikita S. Borodin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Divavin_A/0/1/0/all/0/1\">Andrei M. Divavin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Labeling Indoor Scenes with Fusion of Out-of-the-Box Perception Models. (arXiv:2311.10883v1 [cs.CV])","link":"http://arxiv.org/abs/2311.10883","description":"<p>The image annotation stage is a critical and often the most time-consuming\npart required for training and evaluating object detection and semantic\nsegmentation models. Deployment of the existing models in novel environments\noften requires detecting novel semantic classes not present in the training\ndata. Furthermore, indoor scenes contain significant viewpoint variations,\nwhich need to be handled properly by trained perception models. We propose to\nleverage the recent advancements in state-of-the-art models for bottom-up\nsegmentation (SAM), object detection (Detic), and semantic segmentation\n(MaskFormer), all trained on large-scale datasets. We aim to develop a\ncost-effective labeling approach to obtain pseudo-labels for semantic\nsegmentation and object instance detection in indoor environments, with the\nultimate goal of facilitating the training of lightweight models for various\ndownstream tasks. We also propose a multi-view labeling fusion stage, which\nconsiders the setting where multiple views of the scenes are available and can\nbe used to identify and rectify single-view inconsistencies. We demonstrate the\neffectiveness of the proposed approach on the Active Vision dataset and the\nADE20K dataset. We evaluate the quality of our labeling process by comparing it\nwith human annotations. Also, we demonstrate the effectiveness of the obtained\nlabels in downstream tasks such as object goal navigation and part discovery.\nIn the context of object goal navigation, we depict enhanced performance using\nthis fusion approach compared to a zero-shot baseline that utilizes large\nmonolithic vision-language pre-trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yimeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajabi_N/0/1/0/all/0/1\">Navid Rajabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrestha_S/0/1/0/all/0/1\">Sulabh Shrestha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reza_M/0/1/0/all/0/1\">Md Alimoor Reza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosecka_J/0/1/0/all/0/1\">Jana Kosecka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extraction and Summarization of Explicit Video Content using Multi-Modal Deep Learning. (arXiv:2311.10899v1 [cs.CV])","link":"http://arxiv.org/abs/2311.10899","description":"<p>With the increase in video-sharing platforms across the internet, it is\ndifficult for humans to moderate the data for explicit content. Hence, an\nautomated pipeline to scan through video data for explicit content has become\nthe need of the hour. We propose a novel pipeline that uses multi-modal deep\nlearning to first extract the explicit segments of input videos and then\nsummarize their content using text to determine its age appropriateness and age\nrating. We also evaluate our pipeline's effectiveness in the end using standard\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Shaunak Joshi</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Gaggar_R/0/1/0/all/0/1\">Raghav Gaggar</a> (1) ((1) University of Southern California)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flexible Model Interpretability through Natural Language Model Editing. (arXiv:2311.10905v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10905","description":"<p>Model interpretability and model editing are crucial goals in the age of\nlarge language models. Interestingly, there exists a link between these two\ngoals: if a method is able to systematically edit model behavior with regard to\na human concept of interest, this editor method can help make internal\nrepresentations more interpretable by pointing towards relevant representations\nand systematically manipulating them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DOosterlinck_K/0/1/0/all/0/1\">Karel D&#x27;Oosterlinck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1\">Thomas Demeester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Develder_C/0/1/0/all/0/1\">Chris Develder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding and Mitigating Classification Errors Through Interpretable Token Patterns. (arXiv:2311.10920v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10920","description":"<p>State-of-the-art NLP methods achieve human-like performance on many tasks,\nbut make errors nevertheless. Characterizing these errors in easily\ninterpretable terms gives insight into whether a classifier is prone to making\nsystematic errors, but also gives a way to act and improve the classifier. We\npropose to discover those patterns of tokens that distinguish correct and\nerroneous predictions as to obtain global and interpretable descriptions for\narbitrary NLP classifiers. We formulate the problem of finding a succinct and\nnon-redundant set of such patterns in terms of the Minimum Description Length\nprinciple. Through an extensive set of experiments, we show that our method,\nPremise, performs well in practice. Unlike existing solutions, it recovers\nground truth, even on highly imbalanced data over large vocabularies. In VQA\nand NER case studies, we confirm that it gives clear and actionable insight\ninto the systematic errors made by NLP classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hedderich_M/0/1/0/all/0/1\">Michael A. Hedderich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_J/0/1/0/all/0/1\">Jonas Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vreeken_J/0/1/0/all/0/1\">Jilles Vreeken</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable Product Classification for Customs. (arXiv:2311.10922v1 [cs.AI])","link":"http://arxiv.org/abs/2311.10922","description":"<p>The task of assigning internationally accepted commodity codes (aka HS codes)\nto traded goods is a critical function of customs offices. Like court decisions\nmade by judges, this task follows the doctrine of precedent and can be\nnontrivial even for experienced officers. Together with the Korea Customs\nService (KCS), we propose a first-ever explainable decision supporting model\nthat suggests the most likely subheadings (i.e., the first six digits) of the\nHS code. The model also provides reasoning for its suggestion in the form of a\ndocument that is interpretable by customs officers. We evaluated the model\nusing 5,000 cases that recently received a classification request. The results\nshowed that the top-3 suggestions made by our model had an accuracy of 93.9\\%\nwhen classifying 925 challenging subheadings. A user study with 32 customs\nexperts further confirmed that our algorithmic suggestions accompanied by\nexplainable reasonings, can substantially reduce the time and effort taken by\ncustoms officers for classification reviews.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">Eunji Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sihyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sundong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Soyeon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Heeja Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_M/0/1/0/all/0/1\">Meeyoung Cha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAMRA: Copilot for AMR Annotation. (arXiv:2311.10928v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10928","description":"<p>In this paper, we introduce CAMRA (Copilot for AMR Annotatations), a\ncutting-edge web-based tool designed for constructing Abstract Meaning\nRepresentation (AMR) from natural language text. CAMRA offers a novel approach\nto deep lexical semantics annotation such as AMR, treating AMR annotation akin\nto coding in programming languages. Leveraging the familiarity of programming\nparadigms, CAMRA encompasses all essential features of existing AMR editors,\nincluding example lookup, while going a step further by integrating Propbank\nroleset lookup as an autocomplete feature within the tool. Notably, CAMRA\nincorporates AMR parser models as coding co-pilots, greatly enhancing the\nefficiency and accuracy of AMR annotators. To demonstrate the tool's\ncapabilities, we provide a live demo accessible at: https://camra.colorado.edu\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jon Z. Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Shafiuddin Rehan Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonn_J/0/1/0/all/0/1\">Julia Bonn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wright_Bettner_K/0/1/0/all/0/1\">Kristin Wright-Bettner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palmer_M/0/1/0/all/0/1\">Martha Palmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1\">James H. Martin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representing visual classification as a linear combination of words. (arXiv:2311.10933v1 [cs.AI])","link":"http://arxiv.org/abs/2311.10933","description":"<p>Explainability is a longstanding challenge in deep learning, especially in\nhigh-stakes domains like healthcare. Common explainability methods highlight\nimage regions that drive an AI model's decision. Humans, however, heavily rely\non language to convey explanations of not only \"where\" but \"what\".\nAdditionally, most explainability approaches focus on explaining individual AI\npredictions, rather than describing the features used by an AI model in\ngeneral. The latter would be especially useful for model and dataset auditing,\nand potentially even knowledge generation as AI is increasingly being used in\nnovel tasks. Here, we present an explainability strategy that uses a\nvision-language model to identify language-based descriptors of a visual\nclassification task. By leveraging a pre-trained joint embedding space between\nimages and text, our approach estimates a new classification task as a linear\ncombination of words, resulting in a weight for each word that indicates its\nalignment with the vision-based classifier. We assess our approach using two\nmedical imaging classification tasks, where we find that the resulting\ndescriptors largely align with clinical knowledge despite a lack of\ndomain-specific language training. However, our approach also identifies the\npotential for 'shortcut connections' in the public datasets used. Towards a\nfunctional measure of explainability, we perform a pilot reader study where we\nfind that the AI-identified words can enable non-expert humans to perform a\nspecialized medical task at a non-trivial level. Altogether, our results\nemphasize the potential of using multimodal foundational models to deliver\nintuitive, language-based explanations of visual tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Shobhit Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semenov_Y/0/1/0/all/0/1\">Yevgeniy R. Semenov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotter_W/0/1/0/all/0/1\">William Lotter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Partially Randomizing Transformer Weights for Dialogue Response Diversity. (arXiv:2311.10943v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10943","description":"<p>Despite recent progress in generative open-domain dialogue, the issue of low\nresponse diversity persists. Prior works have addressed this issue via either\nnovel objective functions, alternative learning approaches such as variational\nframeworks, or architectural extensions such as the Randomized Link (RL)\nTransformer. However, these approaches typically entail either additional\ndifficulties during training/inference, or a significant increase in model size\nand complexity. Hence, we propose the \\underline{Pa}rtially\n\\underline{Ra}ndomized trans\\underline{Former} (PaRaFormer), a simple extension\nof the transformer which involves freezing the weights of selected layers after\nrandom initialization. Experimental results reveal that the performance of the\nPaRaformer is comparable to that of the aforementioned approaches, despite not\nentailing any additional training difficulty or increase in model complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jing Yang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kong Aik Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1\">Woon-Seng Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deception Detection from Linguistic and Physiological Data Streams Using Bimodal Convolutional Neural Networks. (arXiv:2311.10944v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10944","description":"<p>Deception detection is gaining increasing interest due to ethical and\nsecurity concerns. This paper explores the application of convolutional neural\nnetworks for the purpose of multimodal deception detection. We use a dataset\nbuilt by interviewing 104 subjects about two topics, with one truthful and one\nfalsified response from each subject about each topic. In particular, we make\nthree main contributions. First, we extract linguistic and physiological\nfeatures from this data to train and construct the neural network models.\nSecond, we propose a fused convolutional neural network model using both\nmodalities in order to achieve an improved overall performance. Third, we\ncompare our new approach with earlier methods designed for multimodal deception\ndetection. We find that our system outperforms regular classification methods;\nour results indicate the feasibility of using neural networks for deception\ndetection even in the presence of limited amounts of data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Panfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abouelenien_M/0/1/0/all/0/1\">Mohamed Abouelenien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Bayes Framework for Open-Domain Dialogue Generation. (arXiv:2311.10945v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10945","description":"<p>To engage human users in meaningful conversation, open-domain dialogue agents\nare required to generate diverse and contextually coherent dialogue. Despite\nrecent advancements, which can be attributed to the usage of pretrained\nlanguage models, the generation of diverse and coherent dialogue remains an\nopen research problem. A popular approach to address this issue involves the\nadaptation of variational frameworks. However, while these approaches\nsuccessfully improve diversity, they tend to compromise on contextual\ncoherence. Hence, we propose the Bayesian Open-domain Dialogue with Empirical\nBayes (BODEB) framework, an empirical bayes framework for constructing an\nBayesian open-domain dialogue agent by leveraging pretrained parameters to\ninform the prior and posterior parameter distributions. Empirical results show\nthat BODEB achieves better results in terms of both diversity and coherence\ncompared to variational frameworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jing Yang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kong Aik Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1\">Woon-Seng Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Journey of Hallucination-minimized Generative AI Solutions for Financial Decision Makers. (arXiv:2311.10961v1 [cs.CL])","link":"http://arxiv.org/abs/2311.10961","description":"<p>Generative AI has significantly reduced the entry barrier to the domain of AI\nowing to the ease of use and core capabilities of automation, translation, and\nintelligent actions in our day to day lives. Currently, Large language models\n(LLMs) that power such chatbots are being utilized primarily for their\nautomation capabilities for software monitoring, report generation etc. and for\nspecific personalized question answering capabilities, on a limited scope and\nscale. One major limitation of the currently evolving family of LLMs is\n'hallucinations', wherein inaccurate responses are reported as factual.\nHallucinations are primarily caused by biased training data, ambiguous prompts\nand inaccurate LLM parameters, and they majorly occur while combining\nmathematical facts with language-based context. Thus, monitoring and\ncontrolling for hallucinations becomes necessary when designing solutions that\nare meant for decision makers. In this work we present the three major stages\nin the journey of designing hallucination-minimized LLM-based solutions that\nare specialized for the decision makers of the financial domain, namely:\nprototyping, scaling and LLM evolution using human feedback. These three stages\nand the novel data to answer generation modules presented in this work are\nnecessary to ensure that the Generative AI chatbots, autonomous reports and\nalerts are reliable and high-quality to aid key decision-making processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roychowdhury_S/0/1/0/all/0/1\">Sohini Roychowdhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Behavior Optimized Image Generation. (arXiv:2311.10995v1 [cs.CV])","link":"http://arxiv.org/abs/2311.10995","description":"<p>The last few years have witnessed great success on image generation, which\nhas crossed the acceptance thresholds of aesthetics, making it directly\napplicable to personal and commercial applications. However, images, especially\nin marketing and advertising applications, are often created as a means to an\nend as opposed to just aesthetic concerns. The goal can be increasing sales,\ngetting more clicks, likes, or image sales (in the case of stock businesses).\nTherefore, the generated images need to perform well on these key performance\nindicators (KPIs), in addition to being aesthetically good. In this paper, we\nmake the first endeavor to answer the question of \"How can one infuse the\nknowledge of the end-goal within the image generation process itself to create\nnot just better-looking images but also \"better-performing'' images?''. We\npropose BoigLLM, an LLM that understands both image content and user behavior.\nBoigLLM knows how an image should look to get a certain required KPI. We show\nthat BoigLLM outperforms 13x larger models such as GPT-3.5 and GPT-4 in this\ntask, demonstrating that while these state-of-the-art models can understand\nimages, they lack information on how these images perform in the real world. To\ngenerate actual pixels of behavior-conditioned images, we train a\ndiffusion-based model (BoigSD) to align with a proposed BoigLLM-defined reward.\nWe show the performance of the overall pipeline on two datasets covering two\ndifferent behaviors: a stock dataset with the number of forward actions as the\nKPI and a dataset containing tweets with the total likes as the KPI, denoted as\nBoigBench. To advance research in the direction of utility-driven image\ngeneration and understanding, we release BoigBench, a benchmark dataset\ncontaining 168 million enterprise tweets with their media, brand account names,\ntime of post, and total likes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khurana_V/0/1/0/all/0/1\">Varun Khurana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1\">Yaman K Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_J/0/1/0/all/0/1\">Jayakumar Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiqiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1\">Balaji Krishnamurthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gendec: A Machine Learning-based Framework for Gender Detection from Japanese Names. (arXiv:2311.11001v1 [cs.CL])","link":"http://arxiv.org/abs/2311.11001","description":"<p>Every human has their own name, a fundamental aspect of their identity and\ncultural heritage. The name often conveys a wealth of information, including\ndetails about an individual's background, ethnicity, and, especially, their\ngender. By detecting gender through the analysis of names, researchers can\nunlock valuable insights into linguistic patterns and cultural norms, which can\nbe applied to practical applications. Hence, this work presents a novel dataset\nfor Japanese name gender detection comprising 64,139 full names in romaji,\nhiragana, and kanji forms, along with their biological genders. Moreover, we\npropose Gendec, a framework for gender detection from Japanese names that\nleverages diverse approaches, including traditional machine learning techniques\nor cutting-edge transfer learning models, to predict the gender associated with\nJapanese names accurately. Through a thorough investigation, the proposed\nframework is expected to be effective and serve potential applications in\nvarious domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pham_D/0/1/0/all/0/1\">Duong Tien Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Luan Thanh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joyful: Joint Modality Fusion and Graph Contrastive Learning for Multimodal Emotion Recognition. (arXiv:2311.11009v1 [cs.CL])","link":"http://arxiv.org/abs/2311.11009","description":"<p>Multimodal emotion recognition aims to recognize emotions for each utterance\nof multiple modalities, which has received increasing attention for its\napplication in human-machine interaction. Current graph-based methods fail to\nsimultaneously depict global contextual features and local diverse uni-modal\nfeatures in a dialogue. Furthermore, with the number of graph layers\nincreasing, they easily fall into over-smoothing. In this paper, we propose a\nmethod for joint modality fusion and graph contrastive learning for multimodal\nemotion recognition (Joyful), where multimodality fusion, contrastive learning,\nand emotion recognition are jointly optimized. Specifically, we first design a\nnew multimodal fusion mechanism that can provide deep interaction and fusion\nbetween the global contextual and uni-modal specific features. Then, we\nintroduce a graph contrastive learning framework with inter-view and intra-view\ncontrastive losses to learn more distinguishable representations for samples\nwith different sentiments. Extensive experiments on three benchmark datasets\nindicate that Joyful achieved state-of-the-art (SOTA) performance compared to\nall baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yusong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funakoshi_K/0/1/0/all/0/1\">Kotaro Funakoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okumura_M/0/1/0/all/0/1\">Manabu Okumura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bit Cipher -- A Simple yet Powerful Word Representation System that Integrates Efficiently with Language Models. (arXiv:2311.11012v1 [cs.CL])","link":"http://arxiv.org/abs/2311.11012","description":"<p>While Large Language Models (LLMs) become ever more dominant, classic\npre-trained word embeddings sustain their relevance through computational\nefficiency and nuanced linguistic interpretation. Drawing from recent studies\ndemonstrating that the convergence of GloVe and word2vec optimizations all tend\ntowards log-co-occurrence matrix variants, we construct a novel word\nrepresentation system called Bit-cipher that eliminates the need of\nbackpropagation while leveraging contextual information and hyper-efficient\ndimensionality reduction techniques based on unigram frequency, providing\nstrong interpretability, alongside efficiency. We use the bit-cipher algorithm\nto train word vectors via a two-step process that critically relies on a\nhyperparameter -- bits -- that controls the vector dimension. While the first\nstep trains the bit-cipher, the second utilizes it under two different\naggregation modes -- summation or concatenation -- to produce contextually rich\nrepresentations from word co-occurrences. We extend our investigation into\nbit-cipher's efficacy, performing probing experiments on part-of-speech (POS)\ntagging and named entity recognition (NER) to assess its competitiveness with\nclassic embeddings like word2vec and GloVe. Additionally, we explore its\napplicability in LM training and fine-tuning. By replacing embedding layers\nwith cipher embeddings, our experiments illustrate the notable efficiency of\ncipher in accelerating the training process and attaining better optima\ncompared to conventional training paradigms. Experiments on the integration of\nbit-cipher embedding layers with Roberta, T5, and OPT, prior to or as a\nsubstitute for fine-tuning, showcase a promising enhancement to transfer\nlearning, allowing rapid model convergence while preserving competitive\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haoran Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1\">Jake Ryland Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoPaSul Manual -- Contour-based parametric and superpositional intonation stylization. (arXiv:1612.04765v13 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1612.04765","description":"<p>The purposes of the CoPaSul toolkit are (1) automatic prosodic annotation and\n(2) prosodic feature extraction from syllable to utterance level. CoPaSul\nstands for contour-based, parametric, superpositional intonation stylization.\nIn this framework intonation is represented as a superposition of global and\nlocal contours that are described parametrically in terms of polynomial\ncoefficients. On the global level (usually associated but not necessarily\nrestricted to intonation phrases) the stylization serves to represent register\nin terms of time-varying F0 level and range. On the local level (e.g. accent\ngroups), local contour shapes are described. From this parameterization several\nfeatures related to prosodic boundaries and prominence can be derived.\nFurthermore, by coefficient clustering prosodic contour classes can be obtained\nin a bottom-up way. Next to the stylization-based feature extraction also\nstandard F0 and energy measures (e.g. mean and variance) as well as rhythmic\naspects can be calculated. At the current state automatic annotation comprises:\nsegmentation into interpausal chunks, syllable nucleus extraction, and\nunsupervised localization of prosodic phrase boundaries and prominent\nsyllables. F0 and partly also energy feature sets can be derived for: standard\nmeasurements (as median and IQR), register in terms of F0 level and range,\nprosodic boundaries, local contour shapes, bottom-up derived contour classes,\nGestalt of accent groups in terms of their deviation from higher level prosodic\nunits, as well as for rhythmic aspects quantifying the relation between F0 and\nenergy contours and prosodic event rates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reichel_U/0/1/0/all/0/1\">Uwe D. Reichel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inferring the Reader: Guiding Automated Story Generation with Commonsense Reasoning. (arXiv:2105.01311v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.01311","description":"<p>Transformer-based language model approaches to automated story generation\ncurrently provide state-of-the-art results. However, they still suffer from\nplot incoherence when generating narratives over time, and critically lack\nbasic commonsense reasoning. Furthermore, existing methods generally focus only\non single-character stories, or fail to track characters at all. To improve the\ncoherence of generated narratives and to expand the scope of character-centric\nnarrative generation, we introduce Commonsense-inference Augmented neural\nStoryTelling (CAST), a framework for introducing commonsense reasoning into the\ngeneration process with the option to model the interaction between multiple\ncharacters. We find that our CAST method produces significantly more coherent,\non-topic, enjoyable and fluent stories than existing models in both the\nsingle-character and two-character settings in three storytelling domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiangyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiegreffe_S/0/1/0/all/0/1\">Sarah Wiegreffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark Riedl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Influenza A Viral Host Using PSSM and Word Embeddings. (arXiv:2201.01140v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.01140","description":"<p>The rapid mutation of the influenza virus threatens public health.\nReassortment among viruses with different hosts can lead to a fatal pandemic.\nHowever, it is difficult to detect the original host of the virus during or\nafter an outbreak as influenza viruses can circulate between different species.\nTherefore, early and rapid detection of the viral host would help reduce the\nfurther spread of the virus. We use various machine learning models with\nfeatures derived from the position-specific scoring matrix (PSSM) and features\nlearned from word embedding and word encoding to infer the origin host of\nviruses. The results show that the performance of the PSSM-based model reaches\nthe MCC around 95%, and the F1 around 96%. The MCC obtained using the model\nwith word embedding is around 96%, and the F1 is around 97%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanhua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wojtczak_D/0/1/0/all/0/1\">Dominik Wojtczak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleTTS: A Style-Based Generative Model for Natural and Diverse Text-to-Speech Synthesis. (arXiv:2205.15439v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2205.15439","description":"<p>Text-to-Speech (TTS) has recently seen great progress in synthesizing\nhigh-quality speech owing to the rapid development of parallel TTS systems, but\nproducing speech with naturalistic prosodic variations, speaking styles and\nemotional tones remains challenging. Moreover, since duration and speech are\ngenerated separately, parallel TTS models still have problems finding the best\nmonotonic alignments that are crucial for naturalistic speech synthesis. Here,\nwe propose StyleTTS, a style-based generative model for parallel TTS that can\nsynthesize diverse speech with natural prosody from a reference speech\nutterance. With novel Transferable Monotonic Aligner (TMA) and\nduration-invariant data augmentation schemes, our method significantly\noutperforms state-of-the-art models on both single and multi-speaker datasets\nin subjective tests of speech naturalness and speaker similarity. Through\nself-supervised learning of the speaking styles, our model can synthesize\nspeech with the same prosodic and emotional tone as any given reference speech\nwithout the need for explicitly labeling these categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yinghao Aaron Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1\">Cong Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mesgarani_N/0/1/0/all/0/1\">Nima Mesgarani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Opinion Summarization Using Approximate Geodesics. (arXiv:2209.07496v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.07496","description":"<p>Opinion summarization is the task of creating summaries capturing popular\nopinions from user reviews. In this paper, we introduce Geodesic Summarizer\n(GeoSumm), a novel system to perform unsupervised extractive opinion\nsummarization. GeoSumm involves an encoder-decoder based representation\nlearning model, that generates representations of text as a distribution over\nlatent semantic units. GeoSumm generates these representations by performing\ndictionary learning over pre-trained text representations at multiple decoder\nlayers. We then use these representations to quantify the relevance of review\nsentences using a novel approximate geodesic distance based scoring mechanism.\nWe use the relevance scores to identify popular opinions in order to compose\ngeneral and aspect-specific summaries. Our proposed model, GeoSumm, achieves\nstate-of-the-art performance on three opinion summarization datasets. We\nperform additional experiments to analyze the functioning of our model and\nshowcase the generalization ability of {\\X} across different domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Somnath Basu Roy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monath_N/0/1/0/all/0/1\">Nicholas Monath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1\">Avinava Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1\">Amr Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Varieties of Italy: Technology Challenges and Opportunities. (arXiv:2209.09757v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.09757","description":"<p>Italy is characterized by a one-of-a-kind linguistic diversity landscape in\nEurope, which implicitly encodes local knowledge, cultural traditions, artistic\nexpressions and history of its speakers. However, most local languages and\ndialects in Italy are at risk of disappearing within few generations. The NLP\ncommunity has recently begun to engage with endangered languages, including\nthose of Italy. Yet, most efforts assume that these varieties are\nunder-resourced language monoliths with an established written form and\nhomogeneous functions and needs, and thus highly interchangeable with each\nother and with high-resource, standardized languages. In this paper, we\nintroduce the linguistic context of Italy and challenge the default\nmachine-centric assumptions of NLP for Italy's language varieties. We advocate\nfor a shift in the paradigm from machine-centric to speaker-centric NLP, and\nprovide recommendations and opportunities for work that prioritizes languages\nand their speakers over technological advances. To facilitate the process, we\nfinally propose building a local community towards responsible, participatory\nefforts aimed at supporting vitality of languages and dialects of Italy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramponi_A/0/1/0/all/0/1\">Alan Ramponi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEAL: Stable and Active Learning for Few-Shot Prompting. (arXiv:2211.08358v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08358","description":"<p>Few-shot classification has made great strides due to foundation models that,\nthrough priming and prompting, are highly effective few-shot learners. However,\nthis approach has high variance both across different sets of few shots (data\nselection) and across different finetuning runs (run variability). This is\nproblematic not only because it impedes the fair comparison of different\napproaches, but especially because it makes few-shot learning too unreliable\nfor many real-world applications. To alleviate these issues, we make two\ncontributions for more stable and effective few-shot learning: First, we\npropose novel ensembling methods and show that they substantially reduce run\nvariability. Second, we introduce a new active learning (AL) criterion for data\nselection and present the first AL-based approach specifically tailored towards\nprompt-based learning. In our experiments, we show that our combined method,\nMEAL (Multiprompt finetuning and prediction Ensembling with Active Learning),\nimproves overall performance of prompt-based finetuning by 2.3 points on five\ndiverse tasks. We publicly share our code and data splits in\nhttps://github.com/akoksal/MEAL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koksal_A/0/1/0/all/0/1\">Abdullatif K&#xf6;ksal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schick_T/0/1/0/all/0/1\">Timo Schick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Talk the Walk: Synthetic Data Generation for Conversational Music Recommendation. (arXiv:2301.11489v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2301.11489","description":"<p>Recommender systems are ubiquitous yet often difficult for users to control,\nand adjust if recommendation quality is poor. This has motivated conversational\nrecommender systems (CRSs), with control provided through natural language\nfeedback. However, as with most application domains, building robust CRSs\nrequires training data that reflects system usage$\\unicode{x2014}$here\nconversations with user utterances paired with items that cover a wide range of\npreferences. This has proved challenging to collect scalably using conventional\nmethods. We address the question of whether it can be generated synthetically,\nbuilding on recent advances in natural language. We evaluate in the setting of\nitem set recommendation, noting the increasing attention to this task motivated\nby use cases like music, news, and recipe recommendation. We present\nTalkTheWalk, which synthesizes realistic high-quality conversational data by\nleveraging domain expertise encoded in widely available curated item\ncollections, generating a sequence of hypothetical yet plausible item sets,\nthen using a language model to produce corresponding user utterances. We\ngenerate over one million diverse playlist curation conversations in the music\ndomain, and show these contain consistent utterances with relevant item sets\nnearly matching the quality of an existing but small human-collected dataset\nfor this task. We demonstrate the utility of the generated synthetic dataset on\na conversational item retrieval task and show that it improves over both\nunsupervised baselines and systems trained on a real dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leszczynski_M/0/1/0/all/0/1\">Megan Leszczynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganti_R/0/1/0/all/0/1\">Ravi Ganti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balog_K/0/1/0/all/0/1\">Krisztian Balog</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radlinski_F/0/1/0/all/0/1\">Filip Radlinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_F/0/1/0/all/0/1\">Fernando Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaganty_A/0/1/0/all/0/1\">Arun Tejasvi Chaganty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Selection for Language Models via Importance Resampling. (arXiv:2302.03169v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.03169","description":"<p>Selecting a suitable pretraining dataset is crucial for both general-domain\n(e.g., GPT-3) and domain-specific (e.g., Codex) language models (LMs). We\nformalize this problem as selecting a subset of a large raw unlabeled dataset\nto match a desired target distribution given unlabeled target samples. Due to\nthe scale and dimensionality of the raw text data, existing methods use simple\nheuristics or require human experts to manually curate data. Instead, we extend\nthe classic importance resampling approach used in low-dimensions for LM data\nselection. We propose Data Selection with Importance Resampling (DSIR), an\nefficient and scalable framework that estimates importance weights in a reduced\nfeature space for tractability and selects data with importance resampling\naccording to these weights. We instantiate the DSIR framework with hashed\nn-gram features for efficiency, enabling the selection of 100M documents from\nthe full Pile dataset in 4.5 hours. To measure whether hashed n-gram features\npreserve the aspects of the data that are relevant to the target, we define KL\nreduction, a data metric that measures the proximity between the selected\npretraining data and the target on some feature space. Across 8 data selection\nmethods (including expert selection), KL reduction on hashed n-gram features\nhighly correlates with average downstream accuracy (r=0.82). When selecting\ndata for continued pretraining on a specific domain, DSIR performs comparably\nto expert curation across 8 target distributions. When pretraining\ngeneral-domain models (target is Wikipedia and books), DSIR improves over\nrandom selection and heuristic filtering baselines by 2-2.5% on the GLUE\nbenchmark. Code is available at https://github.com/p-lambda/dsir.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sang Michael Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santurkar_S/0/1/0/all/0/1\">Shibani Santurkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Decision Transformers with Exponential Tilt for Interactive Text Environments. (arXiv:2302.05507v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.05507","description":"<p>Text-based game environments are challenging because agents must deal with\nlong sequences of text, execute compositional actions using text and learn from\nsparse rewards. We address these challenges by proposing Language Decision\nTransformers (LDTs), a framework that is based on transformer language models\nand decision transformers (DTs). Our LDTs extend DTs with 3 components: (1)\nexponential tilt to guide the agent towards high obtainable goals, (2) novel\ngoal conditioning methods yielding better results than the traditional\nreturn-to-go (sum of all future rewards), and (3) a model of future\nobservations that improves agent performance. LDTs are the first to address\noffline RL with DTs on these challenging games. Our experiments show that LDTs\nachieve the highest scores among many different types of agents on some of the\nmost challenging Jericho games, such as Enchanter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gontier_N/0/1/0/all/0/1\">Nicolas Gontier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1\">Pau Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laradji_I/0/1/0/all/0/1\">Issam Laradji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1\">David Vazquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT a General-Purpose Natural Language Processing Task Solver?. (arXiv:2302.06476v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.06476","description":"<p>Spurred by advancements in scale, large language models (LLMs) have\ndemonstrated the ability to perform a variety of natural language processing\n(NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently,\nthe debut of ChatGPT has drawn a great deal of attention from the natural\nlanguage processing (NLP) community due to the fact that it can generate\nhigh-quality responses to human input and self-correct previous mistakes based\non subsequent conversations. However, it is not yet known whether ChatGPT can\nserve as a generalist model that can perform many NLP tasks zero-shot. In this\nwork, we empirically analyze the zero-shot learning ability of ChatGPT by\nevaluating it on 20 popular NLP datasets covering 7 representative task\ncategories. With extensive empirical studies, we demonstrate both the\neffectiveness and limitations of the current version of ChatGPT. We find that\nChatGPT performs well on many tasks favoring reasoning capabilities (e.g.,\narithmetic reasoning) while it still faces challenges when solving specific\ntasks such as sequence tagging. We additionally provide in-depth analysis\nthrough qualitative case studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chengwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?. (arXiv:2302.08143v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.08143","description":"<p>Prompt tuning (PT) which only tunes the embeddings of an additional sequence\nof tokens per task, keeping the pre-trained language model (PLM) frozen, has\nshown remarkable performance in few-shot learning. Despite this, PT has been\nshown to rely heavily on good initialization of the prompt embeddings. In this\nwork, we study meta prompt tuning (MPT) to systematically explore how\nmeta-learning can help improve (if it can) cross-task generalization in PT\nthrough learning to initialize the prompt embeddings from other relevant tasks.\nWe empirically analyze a representative set of meta learning algorithms in a\nwide range of adaptation settings with different source/target task\nconfigurations on a large set of few-shot tasks. With extensive experiments and\nanalysis, we demonstrate the effectiveness of MPT. We find the improvement to\nbe significant particularly on classification tasks. For other kinds of tasks\nsuch as question answering, we observe that while MPT can outperform PT in most\ncases, it does not always outperform multi-task learning. We further provide an\nin-depth analysis from the perspective of task similarity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chengwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruochen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IRFL: Image Recognition of Figurative Language. (arXiv:2303.15445v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.15445","description":"<p>Figures of speech such as metaphors, similes, and idioms are integral parts\nof human communication. They are ubiquitous in many forms of discourse,\nallowing people to convey complex, abstract ideas and evoke emotion. As\nfigurative forms are often conveyed through multiple modalities (e.g., both\ntext and images), understanding multimodal figurative language is an important\nAI challenge, weaving together profound vision, language, commonsense and\ncultural knowledge.\n</p>\n<p>In this work, we develop the Image Recognition of Figurative Language (IRFL)\ndataset. We leverage human annotation and an automatic pipeline we created to\ngenerate a multimodal dataset, and introduce two novel tasks as a benchmark for\nmultimodal figurative language understanding. We experimented with\nstate-of-the-art vision and language models and found that the best (22%)\nperformed substantially worse than humans (97%). We release our dataset,\nbenchmark, and code, in hopes of driving the development of models that can\nbetter understand figurative language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yosef_R/0/1/0/all/0/1\">Ron Yosef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1\">Yonatan Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1\">Dafna Shahaf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferring Procedural Knowledge across Commonsense Tasks. (arXiv:2304.13867v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.13867","description":"<p>Stories about everyday situations are an essential part of human\ncommunication, motivating the need to develop AI agents that can reliably\nunderstand these stories. Despite the long list of supervised methods for story\ncompletion and procedural understanding, current AI has no mechanisms to\nautomatically track and explain procedures in unseen stories. To bridge this\ngap, we study the ability of AI models to transfer procedural knowledge to\nnovel narrative tasks in a transparent manner. We design LEAP: a comprehensive\nframework that integrates state-of-the-art modeling architectures, training\nregimes, and augmentation strategies based on both natural and synthetic\nstories. To address the lack of densely annotated training data, we devise a\nrobust automatic labeler based on few-shot prompting to enhance the augmented\ndata. Our experiments with in- and out-of-domain tasks reveal insights into the\ninterplay of different architectures, training regimes, and augmentation\nstrategies. LEAP's labeler has a clear positive impact on out-of-domain\ndatasets, while the resulting dense annotation provides native explainability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaixin Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-Agnostic Bias Detection in Language Models with Bias Probing. (arXiv:2305.13302v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13302","description":"<p>Pretrained language models (PLMs) are key components in NLP, but they contain\nstrong social biases. Quantifying these biases is challenging because current\nmethods focusing on fill-the-mask objectives are sensitive to slight changes in\ninput. To address this, we propose a bias probing technique called LABDet, for\nevaluating social bias in PLMs with a robust and language-agnostic method. For\nnationality as a case study, we show that LABDet `surfaces' nationality bias by\ntraining a classifier on top of a frozen PLM on non-nationality sentiment\ndetection. We find consistent patterns of nationality bias across monolingual\nPLMs in six languages that align with historical and political context. We also\nshow for English BERT that bias surfaced by LABDet correlates well with bias in\nthe pretraining data; thus, our work is one of the few studies that directly\nlinks pretraining data to PLM behavior. Finally, we verify LABDet's reliability\nand applicability to different templates and languages through an extensive set\nof robustness checks. We publicly share our code and dataset in\nhttps://github.com/akoksal/LABDet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koksal_A/0/1/0/all/0/1\">Abdullatif K&#xf6;ksal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yalcin_O/0/1/0/all/0/1\">Omer Faruk Yalcin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbiyik_A/0/1/0/all/0/1\">Ahmet Akbiyik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilavuz_M/0/1/0/all/0/1\">M. Tahir Kilavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent inabilities? Inverse scaling over the course of pretraining. (arXiv:2305.14681v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14681","description":"<p>Does inverse scaling only occur as a function of model size, or can it also\noccur over the course of training? We carry out an exploratory study\ninvestigating whether the performance of language models on specific tasks can\ndecrease (while general performance remains high) during training on the\nlanguage modeling task. We find 8 tasks on which Pythia 12B (Biderman et al.,\n2023) shows decreased performance over the course of training. Five of these\ntasks (TruthfulQA-MC1, TruthfulQA-MC2, Hindsight Neglect, Memo Trap, and\nPattern Match Suppression) additionally show a consistent relationship whereby\nlarger language models show a greater decrease in performance the more they are\ntrained, despite showing standard (positive) scaling overall. This highlights\nthe importance of testing performance at all relevant benchmarks any time\nmodels are trained on additional data, even if their overall performance\nimproves\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michaelov_J/0/1/0/all/0/1\">James A. Michaelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1\">Benjamin K. Bergen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How To Train Your (Compressed) Large Language Model. (arXiv:2305.14864v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14864","description":"<p>With the increase in the size of large language models (LLMs), we need\ncompression methods that can reduce the model size while preserving the\ngenerality and zero-shot promptability of the model. This goal is more\nambitious than the typical compression setup, which reduces the model's size at\nthe expense of specializing it to a specific end-task. To study this, we\ndevelop a task-agnostic compression pipeline with a large-scale evaluation\ncomprising language modeling perplexity and 12 zero-shot end-tasks. Our results\nshow that a simple layer-wise pruning followed by continued language model\npretraining matches or outperforms three existing state-of-the-art baselines\nwhile being 1.5x more computationally efficient. However, unlike typical\ntask-specialized compression, our best-compressed model significantly\nunderperforms a similar-sized model trained from scratch. We posit the\nhalf-sized pretrained model as an upper bound for task-agnostic compression and\ncall for future work to bridge this gap under a reasonable token budget. Our\nfindings highlight the inadequacy of existing compression methods for LLMs and\nestablish a requirement for new methods that preserve a model's generality and\nzero-shot promptability under compression. We release our code and evaluation\nsetup to facilitate reproducibility and help iterate on method design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1\">Ananya Harsh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sherborne_T/0/1/0/all/0/1\">Tom Sherborne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walsh_E/0/1/0/all/0/1\">Evan Pete Walsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groeneveld_D/0/1/0/all/0/1\">Dirk Groeneveld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1\">Emma Strubell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Landmark Attention: Random-Access Infinite Context Length for Transformers. (arXiv:2305.16300v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16300","description":"<p>While Transformers have shown remarkable success in natural language\nprocessing, their attention mechanism's large memory requirements have limited\ntheir ability to handle longer contexts. Prior approaches, such as recurrent\nmemory or retrieval-based augmentation, have either compromised the\nrandom-access flexibility of attention (i.e., the capability to select any\ntoken in the entire context) or relied on separate mechanisms for relevant\ncontext retrieval, which may not be compatible with the model's attention. In\nthis paper, we present a novel approach that allows access to the complete\ncontext while retaining random-access flexibility, closely resembling running\nattention on the entire context. Our method uses a landmark token to represent\neach block of the input and trains the attention to use it for selecting\nrelevant blocks, enabling retrieval of blocks directly through the attention\nmechanism instead of by relying on a separate mechanism. Our approach\nseamlessly integrates with specialized data structures and the system's memory\nhierarchy, enabling processing of arbitrarily long context lengths. We\ndemonstrate that our method can obtain comparable performance with\nTransformer-XL while significantly reducing the number of retrieved tokens in\neach step. Finally, we show that fine-tuning LLaMA 7B with our method\nsuccessfully extends its context length capacity to over 32k tokens, allowing\nfor inference at the context lengths of GPT-4. We release the implementation of\nlandmark attention and the code to reproduce our experiments at\nhttps://github.com/epfml/landmark-attention/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohtashami_A/0/1/0/all/0/1\">Amirkeivan Mohtashami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models. (arXiv:2306.02080v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2306.02080","description":"<p>Various adaptation methods, such as LoRA, prompts, and adapters, have been\nproposed to enhance the performance of pre-trained vision-language models in\nspecific domains. The robustness of these adaptation methods against\ndistribution shifts have not been studied. In this study, we assess the\nrobustness of 11 widely-used adaptation methods across 4 vision-language\ndatasets under multimodal corruptions. Concretely, we introduce 7 benchmark\ndatasets, including 96 visual and 87 textual corruptions, to investigate the\nrobustness of different adaptation methods, the impact of available adaptation\nexamples, and the influence of trainable parameter size during adaptation. Our\nanalysis reveals that: 1) Adaptation methods are more sensitive to text\ncorruptions than visual corruptions. 2) Full fine-tuning does not consistently\nprovide the highest robustness; instead, adapters can achieve better robustness\nwith comparable clean performance. 3) Contrary to expectations, our findings\nindicate that increasing the number of adaptation data and parameters does not\nguarantee enhanced robustness; instead it results in even lower robustness. We\nhope this study could benefit future research in the development of robust\nmultimodal adaptation methods. The benchmark, code, and dataset used in this\nstudy can be accessed at https://adarobustness.github.io .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jindong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhen Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yunpu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models. (arXiv:2306.07691v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2306.07691","description":"<p>In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that\nleverages style diffusion and adversarial training with large speech language\nmodels (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its\npredecessor by modeling styles as a latent random variable through diffusion\nmodels to generate the most suitable style for the text without requiring\nreference speech, achieving efficient latent diffusion while benefiting from\nthe diverse speech synthesis offered by diffusion models. Furthermore, we\nemploy large pre-trained SLMs, such as WavLM, as discriminators with our novel\ndifferentiable duration modeling for end-to-end training, resulting in improved\nspeech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker\nLJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by\nnative English speakers. Moreover, when trained on the LibriTTS dataset, our\nmodel outperforms previous publicly available models for zero-shot speaker\nadaptation. This work achieves the first human-level TTS on both single and\nmultispeaker datasets, showcasing the potential of style diffusion and\nadversarial training with large SLMs. The audio demos and source code are\navailable at https://styletts2.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yinghao Aaron Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1\">Cong Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raghavan_V/0/1/0/all/0/1\">Vinay S. Raghavan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mischler_G/0/1/0/all/0/1\">Gavin Mischler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mesgarani_N/0/1/0/all/0/1\">Nima Mesgarani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Generation Capabilities of Large Chinese Language Models. (arXiv:2308.04823v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.04823","description":"<p>This paper presents CG-Eval, the first comprehensive evaluation of the\ngeneration capabilities of large Chinese language models across a wide range of\nacademic disciplines. The models' performance was assessed based on their\nability to generate accurate and relevant responses to different types of\nquestions in six disciplines, namely, Science and Engineering, Humanities and\nSocial Sciences, Mathematical Calculations, Medical Practitioner Qualification\nExamination, Judicial Examination, and Certified Public Accountant Examination.\nThis paper also presents Gscore, a composite index derived from the weighted\nsum of multiple metrics to measure the quality of model's generation against a\nreference. The test data and test results can be found at\n<a href=\"http://cgeval.besteasy.com/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Hui Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jingyuan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_M/0/1/0/all/0/1\">Meng Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_B/0/1/0/all/0/1\">Bin Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Na Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation. (arXiv:2308.15363v4 [cs.DB] UPDATED)","link":"http://arxiv.org/abs/2308.15363","description":"<p>Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL\ntask. However, the absence of a systematical benchmark inhibits the development\nof designing effective, efficient and economic LLM-based Text-to-SQL solutions.\nTo address this challenge, in this paper, we first conduct a systematical and\nextensive comparison over existing prompt engineering methods, including\nquestion representation, example selection and example organization, and with\nthese experimental results, we elaborate their pros and cons. Based on these\nfindings, we propose a new integrated solution, named DAIL-SQL, which refreshes\nthe Spider leaderboard with 86.6% execution accuracy and sets a new bar. To\nexplore the potential of open-source LLM, we investigate them in various\nscenarios, and further enhance their performance with supervised fine-tuning.\nOur explorations highlight open-source LLMs' potential in Text-to-SQL, as well\nas the advantages and disadvantages of the supervised fine-tuning.\nAdditionally, towards an efficient and economic LLM-based Text-to-SQL solution,\nwe emphasize the token efficiency in prompt engineering and compare the prior\nstudies under this metric. We hope that our work provides a deeper\nunderstanding of Text-to-SQL with LLMs, and inspires further investigations and\nbroad applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Dawei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haibin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiuyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yichen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bolin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Proxy for Human Labeling: Ensemble Disagreement Scores in Large Language Models for Industrial NLP. (arXiv:2309.05619v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.05619","description":"<p>Large language models (LLMs) have demonstrated significant capability to\ngeneralize across a large number of NLP tasks. For industry applications, it is\nimperative to assess the performance of the LLM on unlabeled production data\nfrom time to time to validate for a real-world setting. Human labeling to\nassess model error requires considerable expense and time delay. Here we\ndemonstrate that ensemble disagreement scores work well as a proxy for human\nlabeling for language models in zero-shot, few-shot, and fine-tuned settings,\nper our evaluation on keyphrase extraction (KPE) task. We measure fidelity of\nthe results by comparing to true error measured from human labeled ground\ntruth. We contrast with the alternative of using another LLM as a source of\nmachine labels, or silver labels. Results across various languages and domains\nshow disagreement scores provide a better estimation of model performance with\nmean average error (MAE) as low as 0.4% and on average 13.8% better than using\nsilver labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Advani_L/0/1/0/all/0/1\">Laksh Advani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gambhir_Y/0/1/0/all/0/1\">Yashmeet Gambhir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perry_D/0/1/0/all/0/1\">Daniel J Perry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiralkar_P/0/1/0/all/0/1\">Prashant Shiralkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1\">Zhengzheng Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colak_A/0/1/0/all/0/1\">Aaron Colak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models. (arXiv:2309.10003v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.10003","description":"<p>This work proposes to measure the scope of a patent claim as the reciprocal\nof the self-information contained in this claim. A probability of occurrence of\nthe claim is obtained from a language model and this probability is used to\ncompute the self-information. Grounded in information theory, this approach is\nbased on the assumption that an unlikely concept is more informative than a\nusual concept, insofar as it is more surprising. In turn, the more surprising\nthe information required to defined the claim, the narrower its scope. Five\nlanguage models are considered, ranging from simplest models (each word or\ncharacter is assigned an identical probability) to intermediate models (using\naverage word or character frequencies), to a large language model (GPT2).\nInterestingly, the scope resulting from the simplest language models is\nproportional to the reciprocal of the number of words or characters involved in\nthe claim, a metric already used in previous works. Application is made to\nmultiple series of patent claims directed to distinct inventions, where each\nseries consists of claims devised to have a gradually decreasing scope. The\nperformance of the language models is assessed with respect to several ad hoc\ntests. The more sophisticated the model, the better the results. I.e., the GPT2\nprobability model outperforms models based on word and character frequencies,\nwhich themselves outdo the simplest models based on word or character counts.\nStill, the character count appears to be a more reliable indicator than the\nword count.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ragot_S/0/1/0/all/0/1\">S&#xe9;bastien Ragot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Large Language Models to Knowledge Graphs for Biomarker Discovery in Cancer. (arXiv:2310.08365v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.08365","description":"<p>Domain experts often rely on most recent knowledge for apprehending and\ndisseminating specific biological processes that help them design strategies\nfor developing prevention and therapeutic decision-making in various disease\nscenarios. A challenging scenarios for artificial intelligence (AI) is using\nbiomedical data (e.g., texts, imaging, omics, and clinical) to provide\ndiagnosis and treatment recommendations for cancerous conditions.~Data and\nknowledge about biomedical entities like cancer, drugs, genes, proteins, and\ntheir mechanism is spread across structured (knowledge bases (KBs)) and\nunstructured (e.g., scientific articles) sources. A large-scale knowledge graph\n(KG) can be constructed by integrating and extracting facts about semantically\ninterrelated entities and relations. Such a KG not only allows exploration and\nquestion answering (QA) but also enables domain experts to deduce new\nknowledge. However, exploring and querying large-scale KGs is tedious for\nnon-domain users due to their lack of understanding of the data assets and\nsemantic technologies. In this paper, we develop a domain KG to leverage\ncancer-specific biomarker discovery and interactive QA. For this, we\nconstructed a domain ontology called OncoNet Ontology (ONO), which enables\nsemantic reasoning for validating gene-disease (different types of cancer)\nrelations. The KG is further enriched by harmonizing the ONO, metadata,\ncontrolled vocabularies, and biomedical concepts from scientific articles by\nemploying BioBERT- and SciBERT-based information extractors. Further, since the\nbiomedical domain is evolving, where new findings often replace old ones,\nwithout having access to up-to-date scientific findings, there is a high chance\nan AI system exhibits concept drift while providing diagnosis and treatment.\nTherefore, we fine-tune the KG using large language models (LLMs) based on more\nrecent articles and KBs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Md. Rezaul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comet_L/0/1/0/all/0/1\">Lina Molinas Comet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shajalal_M/0/1/0/all/0/1\">Md Shajalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyan_O/0/1/0/all/0/1\">Oya Deniz Beyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rebholz_Schuhmann_D/0/1/0/all/0/1\">Dietrich Rebholz-Schuhmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decker_S/0/1/0/all/0/1\">Stefan Decker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving Math Word Problems with Reexamination. (arXiv:2310.09590v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.09590","description":"<p>Math word problem (MWP) solving aims to understand the descriptive math\nproblem and calculate the result, for which previous efforts are mostly devoted\nto upgrade different technical modules. This paper brings a different\nperspective of \\textit{reexamination process} during training by introducing a\npseudo-dual task to enhance the MWP solving. We propose a pseudo-dual (PseDual)\nlearning scheme to model such process, which is model-agnostic thus can be\nadapted to any existing MWP solvers. The pseudo-dual task is specifically\ndefined as filling the numbers in the expression back into the original word\nproblem with numbers masked. To facilitate the effective joint learning of the\ntwo tasks, we further design a scheduled fusion strategy for the number\ninfilling task, which smoothly switches the input from the ground-truth math\nexpressions to the predicted ones. Our pseudo-dual learning scheme has been\ntested and proven effective when being equipped in several representative MWP\nsolvers through empirical studies. \\textit{The codes and trained models are\navailable at:} \\url{https://github.com/steven640pixel/PsedualMWP}.\n\\end{abstract}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bin_Y/0/1/0/all/0/1\">Yi Bin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Wenhao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yujuan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1\">See-Kiong Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation. (arXiv:2310.09886v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.09886","description":"<p>Lifelong sequence generation (LSG), a problem in continual learning, aims to\ncontinually train a model on a sequence of generation tasks to learn constantly\nemerging new generation patterns while avoiding the forgetting of previous\nknowledge. Existing LSG methods mainly focus on maintaining old knowledge while\npaying little attention to knowledge transfer across tasks. In contrast, humans\ncan better learn new tasks by leveraging previously acquired knowledge from\nsimilar tasks. Inspired by the learning paradigm of humans, we propose Dynamic\nModule Expansion and Adaptation (DMEA), which enables the model to dynamically\ndetermine the architecture for acquiring new knowledge based on task\ncorrelation and select the most similar previous tasks to facilitate adaptation\nto new tasks. In addition, as the learning process can easily be biased towards\nthe current task which might cause more severe forgetting of previously learned\nknowledge, we propose dynamic gradient scaling to balance the learning of the\ncurrent task and replayed tasks. With extensive experiments, we demonstrate\nthat DMEA can consistently outperform existing methods in different LSG\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chengwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attribution Patching Outperforms Automated Circuit Discovery. (arXiv:2310.10348v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2310.10348","description":"<p>Automated interpretability research has recently attracted attention as a\npotential research direction that could scale explanations of neural network\nbehavior to large models. Existing automated circuit discovery work applies\nactivation patching to identify subnetworks responsible for solving specific\ntasks (circuits). In this work, we show that a simple method based on\nattribution patching outperforms all existing methods while requiring just two\nforward passes and a backward pass. We apply a linear approximation to\nactivation patching to estimate the importance of each edge in the\ncomputational subgraph. Using this approximation, we prune the least important\nedges of the network. We survey the performance and limitations of this method,\nfinding that averaged over all tasks our method has greater AUC from circuit\nrecovery than other methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Syed_A/0/1/0/all/0/1\">Aaquib Syed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rager_C/0/1/0/all/0/1\">Can Rager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conmy_A/0/1/0/all/0/1\">Arthur Conmy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting. (arXiv:2310.11732v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2310.11732","description":"<p>Despite the significant progress made in practical applications of aligned\nlanguage models (LMs), they tend to be overconfident in output answers compared\nto the corresponding pre-trained LMs. In this work, we systematically evaluate\nthe impact of the alignment process on logit-based uncertainty calibration of\nLMs under the multiple-choice setting. We first conduct a thoughtful empirical\nstudy on how aligned LMs differ in calibration from their pre-trained\ncounterparts. Experimental results reveal that there are two distinct\nuncertainties in LMs under the multiple-choice setting, which are responsible\nfor the answer decision and the format preference of the LMs, respectively.\nThen, we investigate the role of these two uncertainties on aligned LM's\ncalibration through fine-tuning in simple synthetic alignment schemes and\nconclude that one reason for aligned LMs' overconfidence is the conflation of\nthese two types of uncertainty. Furthermore, we examine the utility of common\npost-hoc calibration methods for aligned LMs and propose an easy-to-implement\nand sample-efficient method to calibrate aligned LMs. We hope our findings\ncould provide insights into the design of more reliable alignment processes for\nLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1\">Guande He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianfei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT a game changer for geocoding -- a benchmark for geocoding address parsing techniques. (arXiv:2310.14360v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.14360","description":"<p>The remarkable success of GPT models across various tasks, including toponymy\nrecognition motivates us to assess the performance of the GPT-3 model in the\ngeocoding address parsing task. To ensure that the evaluation more accurately\nmirrors performance in real-world scenarios with diverse user input qualities\nand resolve the pressing need for a 'gold standard' evaluation dataset for\ngeocoding systems, we introduce a benchmark dataset of low-quality address\ndescriptions synthesized based on human input patterns mining from actual input\nlogs of a geocoding system in production. This dataset has 21 different input\nerrors and variations; contains over 239,000 address records that are uniquely\nselected from streets across all U.S. 50 states and D.C.; and consists of three\nsubsets to be used as training, validation, and testing sets. Building on this,\nwe train and gauge the performance of the GPT-3 model in extracting address\ncomponents, contrasting its performance with transformer-based and LSTM-based\nmodels. The evaluation results indicate that Bidirectional LSTM-CRF model has\nachieved the best performance over these transformer-based models and GPT-3\nmodel. Transformer-based models demonstrate very comparable results compared to\nthe Bidirectional LSTM-CRF model. The GPT-3 model, though trailing in\nperformance, showcases potential in the address parsing task with few-shot\nexamples, exhibiting room for improvement with additional fine-tuning. We open\nsource the code and data of this presented benchmark so that researchers can\nutilize it for future model development or extend it to evaluate similar tasks,\nsuch as document geocoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhengcong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Diya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_D/0/1/0/all/0/1\">Daniel W. Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models. (arXiv:2310.15127v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2310.15127","description":"<p>Pre-trained and frozen large language models (LLMs) can effectively map\nsimple scene rearrangement instructions to programs over a robot's visuomotor\nfunctions through appropriate few-shot example prompting. To parse open-domain\nnatural language and adapt to a user's idiosyncratic procedures, not known\nduring prompt engineering time, fixed prompts fall short. In this paper, we\nintroduce HELPER, an embodied agent equipped with an external memory of\nlanguage-program pairs that parses free-form human-robot dialogue into action\nprograms through retrieval-augmented LLM prompting: relevant memories are\nretrieved based on the current dialogue, instruction, correction, or VLM\ndescription, and used as in-context prompt examples for LLM querying. The\nmemory is expanded during deployment to include pairs of user's language and\naction plans, to assist future inferences and personalize them to the user's\nlanguage and routines. HELPER sets a new state-of-the-art in the TEACh\nbenchmark in both Execution from Dialog History (EDH) and Trajectory from\nDialogue (TfD), with a 1.7x improvement over the previous state-of-the-art for\nTfD. Our models, code, and video results can be found in our project's website:\nhttps://helper-agent-llm.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarch_G/0/1/0/all/0/1\">Gabriel Sarch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarr_M/0/1/0/all/0/1\">Michael J. Tarr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1\">Katerina Fragkiadaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SSLCL: An Efficient Model-Agnostic Supervised Contrastive Learning Framework for Emotion Recognition in Conversations. (arXiv:2310.16676v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.16676","description":"<p>Emotion recognition in conversations (ERC) is a rapidly evolving task within\nthe natural language processing community, which aims to detect the emotions\nexpressed by speakers during a conversation. Recently, a growing number of ERC\nmethods have focused on leveraging supervised contrastive learning (SCL) to\nenhance the robustness and generalizability of learned features. However,\ncurrent SCL-based approaches in ERC are impeded by the constraint of large\nbatch sizes and the lack of compatibility with most existing ERC models. To\naddress these challenges, we propose an efficient and model-agnostic SCL\nframework named Supervised Sample-Label Contrastive Learning with Soft-HGR\nMaximal Correlation (SSLCL), which eliminates the need for a large batch size\nand can be seamlessly integrated with existing ERC models without introducing\nany model-specific assumptions. Specifically, we introduce a novel perspective\non utilizing label representations by projecting discrete labels into dense\nembeddings through a shallow multilayer perceptron, and formulate the training\nobjective to maximize the similarity between sample features and their\ncorresponding ground-truth label embeddings, while minimizing the similarity\nbetween sample features and label embeddings of disparate classes. Moreover, we\ninnovatively adopt the Soft-HGR maximal correlation as a measure of similarity\nbetween sample features and label embeddings, leading to significant\nperformance improvements over conventional similarity measures. Additionally,\nmultimodal cues of utterances are effectively leveraged by SSLCL as data\naugmentations to boost model performances. Extensive experiments on two ERC\nbenchmark datasets, IEMOCAP and MELD, demonstrate the compatibility and\nsuperiority of our proposed SSLCL framework compared to existing\nstate-of-the-art SCL methods. Our code is available at\n\\url{https://github.com/TaoShi1998/SSLCL}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1\">Tao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yaoyuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1\">Xinyi Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shao-Lun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using GPT-4 to Augment Unbalanced Data for Automatic Scoring. (arXiv:2310.18365v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.18365","description":"<p>Machine learning-based automatic scoring can be challenging if students'\nresponses are unbalanced across scoring categories, as it introduces\nuncertainty in the machine training process. To meet this challenge, we\nintroduce a novel text data augmentation framework using GPT-4, a generative\nlarge language model, specifically tailored for unbalanced datasets in\nautomatic scoring. Our experimental dataset comprised student-written responses\nto two science items. We crafted prompts for GPT-4 to generate responses\nresembling student-written answers, particularly for the minority scoring\nclasses, to augment the data. We then finetuned DistillBERT for automatic\nscoring based on the augmented and original datasets. Model performance was\nassessed using accuracy, precision, recall, and F1 score. We incorporate varied\namounts of augmented data to examine scoring performance, and our findings\nrevealed remarkedly improved model performance. The average maximum increase\nobserved across two items is: 3.5% for accuracy, 30.6% for precision, 21.1% for\nrecall, and 24.2% for F1 score. Notably, using just 5% of the augmented data\nled to substantial improvements: 2.6%, 29.2%, 15.1%, and 19.6%. Interestingly,\nthe extent of improvement varied depending on specific datasets. Moreover, we\nfound that a varying amount of augmented data (5%-40%) was needed to obtain a\nstable improvement. We also compare models trained with GPT-4 augmented data\nand those trained with additional student-written responses. The findings\nindicate that former ones match or even exceed the performance of the latter.\nSpecifically, there is an average difference of 1.7%, 1.9%, 11.0%, and 7.8% for\nfour metrics separately. This research underscores the potential and\neffectiveness of data augmentation techniques utilizing GPT-4 in addressing\nunbalanced datasets within automated assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Luyang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gyeong-Geon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaoming Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Women Wearing Lipstick: Measuring the Bias Between an Object and Its Related Gender. (arXiv:2310.19130v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.19130","description":"<p>In this paper, we investigate the impact of objects on gender bias in image\ncaptioning systems. Our results show that only gender-specific objects have a\nstrong gender bias (e.g., women-lipstick). In addition, we propose a visual\nsemantic-based gender score that measures the degree of bias and can be used as\na plug-in for any image captioning system. Our experiments demonstrate the\nutility of the gender score, since we observe that our score can measure the\nbias relation between a caption and its related gender; therefore, our score\ncan be used as an additional metric to the existing Object Gender Co-Occ\napproach. Code and data are publicly available at\n\\url{https://github.com/ahmedssabir/GenderScore}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sabir_A/0/1/0/all/0/1\">Ahmed Sabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padro_L/0/1/0/all/0/1\">Llu&#xed;s Padr&#xf3;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Medical Prescriptions with Conditional Transformer. (arXiv:2310.19727v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.19727","description":"<p>Access to real-world medication prescriptions is essential for medical\nresearch and healthcare quality improvement. However, access to real medication\nprescriptions is often limited due to the sensitive nature of the information\nexpressed. Additionally, manually labelling these instructions for training and\nfine-tuning Natural Language Processing (NLP) models can be tedious and\nexpensive. We introduce a novel task-specific model architecture,\nLabel-To-Text-Transformer (\\textbf{LT3}), tailored to generate synthetic\nmedication prescriptions based on provided labels, such as a vocabulary list of\nmedications and their attributes. LT3 is trained on a set of around 2K lines of\nmedication prescriptions extracted from the MIMIC-III database, allowing the\nmodel to produce valuable synthetic medication prescriptions. We evaluate LT3's\nperformance by contrasting it with a state-of-the-art Pre-trained Language\nModel (PLM), T5, analysing the quality and diversity of generated texts. We\ndeploy the generated synthetic data to train the SpacyNER model for the Named\nEntity Recognition (NER) task over the n2c2-2018 dataset. The experiments show\nthat the model trained on synthetic data can achieve a 96-98\\% F1 score at\nLabel Recognition on Drug, Frequency, Route, Strength, and Form. LT3 codes and\ndata will be shared at\n\\url{https://github.com/HECTA-UoM/Label-To-Text-Transformer}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belkadi_S/0/1/0/all/0/1\">Samuel Belkadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Micheletti_N/0/1/0/all/0/1\">Nicolo Micheletti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Del_Pinto_W/0/1/0/all/0/1\">Warren Del-Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1\">Goran Nenadic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-training LLMs using human-like development data corpus. (arXiv:2311.04666v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.04666","description":"<p>Pre-trained Large Language Models (LLMs) have shown success in a diverse set\nof language inference and understanding tasks. The pre-training stage of LLMs\nlooks at a large corpus of raw textual data. The BabyLM shared task compares\nLLM pre-training to human language acquisition, where the number of tokens seen\nby 13-year-old kids is magnitudes smaller than the number of tokens seen by\nLLMs. In this work, we pre-train and evaluate LLMs on their ability to learn\ncontextual word representations using roughly the same number of tokens as seen\nby children. We provide a strong set of baselines; with different\narchitectures, evaluation of changes in performance across epochs, and reported\npre-training metrics for the strict small and strict tracks of the task. We\nalso try to loosely replicate the RoBERTa baseline given by the task organizers\nto observe the training robustness to hyperparameter selection and\nreplicability. We provide the submission details to the strict and strict-small\ntracks in this report.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_K/0/1/0/all/0/1\">Khushi Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Raj Sanjay Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_S/0/1/0/all/0/1\">Sashank Varma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DocGen: Generating Detailed Parameter Docstrings in Python. (arXiv:2311.06453v3 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2311.06453","description":"<p>Documentation debt hinders the effective utilization of open-source software.\nAlthough code summarization tools have been helpful for developers, most would\nprefer a detailed account of each parameter in a function rather than a\nhigh-level summary. However, generating such a summary is too intricate for a\nsingle generative model to produce reliably due to the lack of high-quality\ntraining data. Thus, we propose a multi-step approach that combines multiple\ntask-specific models, each adept at producing a specific section of a\ndocstring. The combination of these models ensures the inclusion of each\nsection in the final docstring. We compared the results from our approach with\nexisting generative models using both automatic metrics and a human-centred\nevaluation with 17 participating developers, which proves the superiority of\nour approach over existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venkatkrishna_V/0/1/0/all/0/1\">Vatsal Venkatkrishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagabushanam_D/0/1/0/all/0/1\">Durga Shree Nagabushanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_E/0/1/0/all/0/1\">Emmanuel Iko-Ojo Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidoni_M/0/1/0/all/0/1\">Melina Vidoni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalization Analogies: A Testbed for Generalizing AI Oversight to Hard-To-Measure Domains. (arXiv:2311.07723v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2311.07723","description":"<p>As AI systems become more intelligent and their behavior becomes more\nchallenging to assess, they may learn to game the flaws of human feedback\ninstead of genuinely striving to follow instructions; however, this risk can be\nmitigated by controlling how LLMs generalize human feedback to situations where\nit is unreliable. To better understand how reward models generalize, we craft\n69 distribution shifts spanning 8 categories. We find that reward models do not\nlearn to evaluate `instruction-following' by default and instead favor personas\nthat resemble internet text. Techniques for interpreting reward models'\ninternal representations achieve better generalization than standard\nfine-tuning, but still frequently fail to distinguish instruction-following\nfrom conflated behaviors. We consolidate the 15 most challenging distribution\nshifts into the GENeralization analogIES (GENIES) benchmark, which we hope will\nenable progress toward controlling reward model generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clymer_J/0/1/0/all/0/1\">Joshua Clymer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baker_G/0/1/0/all/0/1\">Garrett Baker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramani_R/0/1/0/all/0/1\">Rohan Subramani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sam Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-context Learning and Gradient Descent Revisited. (arXiv:2311.07772v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.07772","description":"<p>In-context learning (ICL) has shown impressive results in few-shot learning\ntasks, yet its underlying mechanism is still not fully understood. Recent works\nsuggest that ICL can be thought of as a gradient descent (GD) based\noptimization process. While promising, these results mainly focus on simplified\nsettings of ICL and provide only a preliminary evaluation of the similarities\nbetween the two methods. In this work, we revisit the comparison between ICL\nand GD-based finetuning and study what properties of ICL an equivalent process\nmust follow. We highlight a major difference in the flow of information between\nICL and standard finetuning. Namely, ICL can only rely on information from\nlower layers at every point, while finetuning depends on loss gradients from\ndeeper layers. We refer to this discrepancy as Layer Causality and show that a\nlayer causal variant of the finetuning process aligns with ICL on par with\nvanilla finetuning and is even better in most cases across relevant metrics. To\nthe best of our knowledge, this is the first work to discuss this discrepancy\nexplicitly and suggest a solution that tackles this problem with minimal\nchanges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deutch_G/0/1/0/all/0/1\">Gilad Deutch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magar_N/0/1/0/all/0/1\">Nadav Magar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natan_T/0/1/0/all/0/1\">Tomer Bar Natan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dar_G/0/1/0/all/0/1\">Guy Dar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Language Models for Code. (arXiv:2311.07989v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.07989","description":"<p>In this work we systematically review the recent advancements in code\nprocessing with language models, covering 50+ models, 30+ evaluation tasks,\n150+ datasets, and 550 related works. We break down code processing models into\ngeneral language models represented by the GPT family and specialized models\nthat are specifically pretrained on code, often with tailored objectives. We\ndiscuss the relations and differences between these models, and highlight the\nhistorical transition of code modeling from statistical models and RNNs to\npretrained Transformers and LLMs, which is exactly the same course that had\nbeen taken by NLP. We also discuss code-specific features such as AST, CFG, and\nunit tests, along with their application in training code language models, and\nidentify key challenges and potential future directions in this domain. We keep\nthe survey open and updated on GitHub repository at\nhttps://github.com/codefuse-ai/Awesome-Code-LLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziyin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bingchang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1\">Cong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zi Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Generation from Human Brain Activities. (arXiv:2311.09889v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.09889","description":"<p>Generating human language through non-invasive brain-computer interfaces\n(BCIs) has the potential to unlock many applications, such as serving disabled\npatients and improving communication. Currently, however, generating language\nvia BCIs has been previously successful only within a classification setup for\nselecting pre-generated sentence continuation candidates with the most likely\ncortical semantic representation. Inspired by recent research that revealed\nassociations between the brain and the large computational language models, we\npropose a generative language BCI that utilizes the capacity of a large\nlanguage model (LLM) jointly with a semantic brain decoder to directly generate\nlanguage from functional magnetic resonance imaging (fMRI) input. The proposed\nmodel can generate coherent language sequences aligned with the semantic\ncontent of visual or auditory language stimuli perceived, without prior\nknowledge of any pre-generated candidates. We compare the language generated\nfrom the presented model with a random control, pre-generated language\nselection approach, and a standard LLM, which generates common coherent text\nsolely based on the next word likelihood according to statistical language\ntraining data. The proposed model is found to generate language that is more\naligned with semantic stimulus in response to which brain input is sampled. Our\nfindings demonstrate the potential and feasibility of employing BCIs in direct\nlanguage generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Ziyi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingyao Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiqun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lioma_C/0/1/0/all/0/1\">Christina Lioma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruotsalo_T/0/1/0/all/0/1\">Tuukka Ruotsalo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Song Describer Dataset: a Corpus of Audio Captions for Music-and-Language Evaluation. (arXiv:2311.10057v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2311.10057","description":"<p>We introduce the Song Describer dataset (SDD), a new crowdsourced corpus of\nhigh-quality audio-caption pairs, designed for the evaluation of\nmusic-and-language models. The dataset consists of 1.1k human-written natural\nlanguage descriptions of 706 music recordings, all publicly accessible and\nreleased under Creative Common licenses. To showcase the use of our dataset, we\nbenchmark popular models on three key music-and-language tasks (music\ncaptioning, text-to-music generation and music-language retrieval). Our\nexperiments highlight the importance of cross-dataset evaluation and offer\ninsights into how researchers can use SDD to gain a broader understanding of\nmodel performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manco_I/0/1/0/all/0/1\">Ilaria Manco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weck_B/0/1/0/all/0/1\">Benno Weck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doh_S/0/1/0/all/0/1\">SeungHeon Doh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Won_M/0/1/0/all/0/1\">Minz Won</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yixiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodganov_D/0/1/0/all/0/1\">Dmitry Bodganov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yusong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Ke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tovstogan_P/0/1/0/all/0/1\">Philip Tovstogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1\">Emmanouil Benetos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quinton_E/0/1/0/all/0/1\">Elio Quinton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gy&#xf6;rgy Fazekas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_J/0/1/0/all/0/1\">Juhan Nam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Language and Its Dimensions: Intrinsic Dimensions of Language Fractal Structures. (arXiv:2311.10217v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.10217","description":"<p>The present paper introduces a novel object of study - a language fractal\nstructure. We hypothesize that a set of embeddings of all $n$-grams of a\nnatural language constitutes a representative sample of this fractal set. (We\nuse the term Hailonakea to refer to the sum total of all language fractal\nstructures, over all $n$). The paper estimates intrinsic (genuine) dimensions\nof language fractal structures for the Russian and English languages. To this\nend, we employ methods based on (1) topological data analysis and (2) a minimum\nspanning tree of a data graph for a cloud of points considered (Steele\ntheorem). For both languages, for all $n$, the intrinsic dimensions appear to\nbe non-integer values (typical for fractal sets), close to 9 for both of the\nRussian and English language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gromov_V/0/1/0/all/0/1\">Vasilii A. Gromov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borodin_N/0/1/0/all/0/1\">Nikita S. Borodin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yerbolova_A/0/1/0/all/0/1\">Asel S. Yerbolova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2. (arXiv:2311.10702v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.10702","description":"<p>Since the release of T\\\"ULU [Wang et al., 2023b], open resources for\ninstruction tuning have developed quickly, from better base models to new\nfinetuning techniques. We test and incorporate a number of these advances into\nT\\\"ULU, resulting in T\\\"ULU 2, a suite of improved T\\\"ULU models for advancing\nthe understanding and best practices of adapting pretrained language models to\ndownstream tasks and user preferences. Concretely, we release: (1)\nT\\\"ULU-V2-mix, an improved collection of high-quality instruction datasets; (2)\nT\\\"ULU 2, LLAMA-2 models finetuned on the V2 mixture; (3) T\\\"ULU 2+DPO, T\\\"ULU\n2 models trained with direct preference optimization (DPO), including the\nlargest DPO-trained model to date (T\\\"ULU 2+DPO 70B); (4) CODE T\\\"ULU 2, CODE\nLLAMA models finetuned on our V2 mix that outperform CODE LLAMA and its\ninstruction-tuned variant, CODE LLAMA-Instruct. Our evaluation from multiple\nperspectives shows that the T\\\"ULU 2 suite achieves state-of-the-art\nperformance among open models and matches or exceeds the performance of\nGPT-3.5-turbo-0301 on several benchmarks. We release all the checkpoints, data,\ntraining and evaluation code to facilitate future open efforts on adapting\nlarge language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivison_H/0/1/0/all/0/1\">Hamish Ivison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1\">Valentina Pyatkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambert_N/0/1/0/all/0/1\">Nathan Lambert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1\">Matthew Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasigi_P/0/1/0/all/0/1\">Pradeep Dasigi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Joel Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadden_D/0/1/0/all/0/1\">David Wadden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-11-20T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/"}}]}]}