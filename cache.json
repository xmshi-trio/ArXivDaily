{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-05-22T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt. (arXiv:2305.11186v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11186","description":"<p>Large Language Models (LLMs), armed with billions of parameters, exhibit\nexceptional performance across a wide range of Natural Language Processing\n(NLP) tasks. However, they present a significant computational challenge during\ninference, especially when deploying on common hardware such as single GPUs. As\nsuch, minimizing the latency of LLM inference by curtailing computational and\nmemory requirements, though achieved through compression, becomes critically\nimportant. However, this process inevitably instigates a trade-off between\nefficiency and accuracy, as compressed LLMs typically experience a reduction in\npredictive precision. In this research, we introduce an innovative perspective:\nto optimize this trade-off, compressed LLMs require a unique input format that\nvaries from that of the original models. Our findings indicate that the\ngeneration quality in a compressed LLM can be markedly improved for specific\nqueries by selecting prompts with precision. Capitalizing on this insight, we\nintroduce a prompt learning paradigm that cultivates an additive prompt over a\ncompressed LLM to bolster their accuracy. Our empirical results imply that\nthrough our strategic prompt utilization, compressed LLMs can match, and\noccasionally even exceed, the accuracy of the original models. Moreover, we\ndemonstrated that these learned prompts have a certain degree of\ntransferability across various datasets, tasks, and compression levels. These\ninsights shine a light on new possibilities for enhancing the balance between\naccuracy and efficiency in LLM inference. Specifically, they underscore the\nimportance of judicious input editing to a compressed large model, hinting at\npotential advancements in scaling LLMs on common hardware.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhaozhuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zirui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Beidi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yuxin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaixiong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Anshumali Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LIMA: Less Is More for Alignment. (arXiv:2305.11206v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11206","description":"<p>Large language models are trained in two stages: (1) unsupervised pretraining\nfrom raw text, to learn general-purpose representations, and (2) large scale\ninstruction tuning and reinforcement learning, to better align to end tasks and\nuser preferences. We measure the relative importance of these two stages by\ntraining LIMA, a 65B parameter LLaMa language model fine-tuned with the\nstandard supervised loss on only 1,000 carefully curated prompts and responses,\nwithout any reinforcement learning or human preference modeling. LIMA\ndemonstrates remarkably strong performance, learning to follow specific\nresponse formats from only a handful of examples in the training data,\nincluding complex queries that range from planning trip itineraries to\nspeculating about alternate history. Moreover, the model tends to generalize\nwell to unseen tasks that did not appear in the training data. In a controlled\nhuman study, responses from LIMA are either equivalent or strictly preferred to\nGPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard\nand 65% versus DaVinci003, which was trained with human feedback. Taken\ntogether, these results strongly suggest that almost all knowledge in large\nlanguage models is learned during pretraining, and only limited instruction\ntuning data is necessary to teach models to produce high quality output.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chunting Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Puxin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_S/0/1/0/all/0/1\">Srini Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuezhe Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efrat_A/0/1/0/all/0/1\">Avia Efrat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Ping Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lili Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Susan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent Trends in Unsupervised Summarization. (arXiv:2305.11231v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11231","description":"<p>Unsupervised summarization is a powerful technique that enables training\nsummarizing models without requiring labeled datasets. This survey covers\ndifferent recent techniques and models used for unsupervised summarization. We\ncover extractive, abstractive, and hybrid models and strategies used to achieve\nunsupervised summarization. While the main focus of this survey is on recent\nresearch, we also cover some of the important previous research. We\nadditionally introduce a taxonomy, classifying different research based on\ntheir approach to unsupervised training. Finally, we discuss the current\napproaches and mention some datasets and evaluation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khosravani_M/0/1/0/all/0/1\">Mohammad Khosravani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trabelsi_A/0/1/0/all/0/1\">Amine Trabelsi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Biases and the Impact of Multilingual Training across Multiple Languages. (arXiv:2305.11242v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11242","description":"<p>Studies in bias and fairness in natural language processing have primarily\nexamined social biases within a single language and/or across few attributes\n(e.g. gender, race). However, biases can manifest differently across various\nlanguages for individual attributes. As a result, it is critical to examine\nbiases within each language and attribute. Of equal importance is to study how\nthese biases compare across languages and how the biases are affected when\ntraining a model on multilingual data versus monolingual data. We present a\nbias analysis across Italian, Chinese, English, Hebrew, and Spanish on the\ndownstream sentiment analysis task to observe whether specific demographics are\nviewed more positively. We study bias similarities and differences across these\nlanguages and investigate the impact of multilingual vs. monolingual training\ndata. We adapt existing sentiment bias templates in English to Italian,\nChinese, Hebrew, and Spanish for four attributes: race, religion, nationality,\nand gender. Our results reveal similarities in bias expression such as\nfavoritism of groups that are dominant in each language's culture (e.g.\nmajority religions and nationalities). Additionally, we find an increased\nvariation in predictions across protected groups, indicating bias\namplification, after multilingual finetuning in comparison to multilingual\npretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Sharon Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+John_N/0/1/0/all/0/1\">Neha Anna John</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyas_Y/0/1/0/all/0/1\">Yogarshi Vyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jie Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujinuma_Y/0/1/0/all/0/1\">Yoshinari Fujinuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballesteros_M/0/1/0/all/0/1\">Miguel Ballesteros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castelli_V/0/1/0/all/0/1\">Vittorio Castelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses. (arXiv:2305.11243v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11243","description":"<p>Developmental psychologists have spent decades devising experiments to test\nthe intelligence and knowledge of infants and children, tracing the origin of\ncrucial concepts and capacities. Moreover, experimental techniques in\ndevelopmental psychology have been carefully designed to discriminate the\ncognitive capacities that underlie particular behaviors. We propose that using\nclassical experiments from child development is a particularly effective way to\nprobe the computational abilities of AI models, in general, and LLMs in\nparticular. First, the methodological techniques of developmental psychology,\nsuch as the use of novel stimuli to control for past experience or control\nconditions to determine whether children are using simple associations, can be\nequally helpful for assessing the capacities of LLMs. In parallel, testing LLMs\nin this way can tell us whether the information that is encoded in text is\nsufficient to enable particular responses, or whether those responses depend on\nother kinds of information, such as information from exploration of the\nphysical world. In this work we adapt classical developmental experiments to\nevaluate the capabilities of LaMDA, a large language model from Google. We\npropose a novel LLM Response Score (LRS) metric which can be used to evaluate\nother language models, such as GPT. We find that LaMDA generates appropriate\nresponses that are similar to those of children in experiments involving social\nunderstanding, perhaps providing evidence that knowledge of these domains is\ndiscovered through language. On the other hand, LaMDA's responses in early\nobject and action understanding, theory of mind, and especially causal\nreasoning tasks are very different from those of young children, perhaps\nshowing that these domains require more real-world, self-initiated exploration\nand cannot simply be learned from patterns in language input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kosoy_E/0/1/0/all/0/1\">Eliza Kosoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reagan_E/0/1/0/all/0/1\">Emily Rose Reagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1\">Leslie Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopnik_A/0/1/0/all/0/1\">Alison Gopnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobb_D/0/1/0/all/0/1\">Danielle Krettek Cobb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Parameter-Efficient Learning Approach to Arabic Dialect Identification with Pre-Trained General-Purpose Speech Model. (arXiv:2305.11244v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11244","description":"<p>In this work, we explore Parameter-Efficient-Learning (PEL) techniques to\nrepurpose a General-Purpose-Speech (GSM) model for Arabic dialect\nidentification (ADI). Specifically, we investigate different setups to\nincorporate trainable features into a multi-layer encoder-decoder GSM\nformulation under frozen pre-trained settings. Our architecture includes\nresidual adapter and model reprogramming (input-prompting). We design a\ntoken-level label mapping to condition the GSM for Arabic Dialect\nIdentification (ADI). This is challenging due to the high variation in\nvocabulary and pronunciation among the numerous regional dialects. We achieve\nnew state-of-the-art accuracy on the ADI-17 dataset by vanilla fine-tuning. We\nfurther reduce the training budgets with the PEL method, which performs within\n1.86% accuracy to fine-tuning using only 2.5% of (extra) network trainable\nparameters. Our study demonstrates how to identify Arabic dialects using a\nsmall dataset and limited computation with open source code and pre-trained\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Radhakrishnan_S/0/1/0/all/0/1\">Srijith Radhakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Sumeer Ahmad Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiani_N/0/1/0/all/0/1\">Narsis A. Kiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Cabrero_D/0/1/0/all/0/1\">David Gomez-Cabrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tegner_J/0/1/0/all/0/1\">Jesper N. Tegner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computational thematics: Comparing algorithms for clustering the genres of literary fiction. (arXiv:2305.11251v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11251","description":"<p>What are the best methods of capturing thematic similarity between literary\ntexts? Knowing the answer to this question would be useful for automatic\nclustering of book genres, or any other thematic grouping. This paper compares\na variety of algorithms for unsupervised learning of thematic similarities\nbetween texts, which we call \"computational thematics\". These algorithms belong\nto three steps of analysis: text preprocessing, extraction of text features,\nand measuring distances between the lists of features. Each of these steps\nincludes a variety of options. We test all the possible combinations of these\noptions: every combination of algorithms is given a task to cluster a corpus of\nbooks belonging to four pre-tagged genres of fiction. This clustering is then\nvalidated against the \"ground truth\" genre labels. Such comparison of\nalgorithms allows us to learn the best and the worst combinations for\ncomputational thematic analysis. To illustrate the sharp difference between the\nbest and the worst methods, we then cluster 5000 random novels from the\nHathiTrust corpus of fiction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sobchuk_O/0/1/0/all/0/1\">Oleg Sobchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sela_A/0/1/0/all/0/1\">Artjoms &#x160;e&#x13c;a</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning Implicit Sentiment with Chain-of-Thought Prompting. (arXiv:2305.11255v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11255","description":"<p>While sentiment analysis systems try to determine the sentiment polarities of\ngiven targets based on the key opinion expressions in input texts, in implicit\nsentiment analysis (ISA) the opinion cues come in an implicit and obscure\nmanner. Thus detecting implicit sentiment requires the common-sense and\nmulti-hop reasoning ability to infer the latent intent of opinion. Inspired by\nthe recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop\nReasoning (THOR) CoT framework to mimic the human-like reasoning process for\nISA. We design a three-step prompting principle for THOR to step-by-step induce\nthe implicit aspect, opinion, and finally the sentiment polarity. Our\nTHOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on\nsupervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50%\nF1 on zero-shot setting. Our code is at\nhttps://github.com/scofield7419/THOR-ISA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hao Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bobo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models. (arXiv:2305.11262v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11262","description":"<p>\\textit{\\textbf{\\textcolor{red}{Warning}:} This paper contains content that\nmay be offensive or upsetting.} Pretrained conversational agents have been\nexposed to safety issues, exhibiting a range of stereotypical human biases such\nas gender bias. However, there are still limited bias categories in current\nresearch, and most of them only focus on English. In this paper, we introduce a\nnew Chinese dataset, CHBias, for bias evaluation and mitigation of Chinese\nconversational language models. Apart from those previous well-explored bias\ncategories, CHBias includes under-explored bias categories, such as ageism and\nappearance biases, which received less attention. We evaluate two popular\npretrained Chinese conversational models, CDial-GPT and EVA2.0, using CHBias.\nFurthermore, to mitigate different biases, we apply several debiasing methods\nto the Chinese pretrained models. Experimental results show that these Chinese\npretrained models are potentially risky for generating texts that contain\nsocial biases, and debiasing methods using the proposed dataset can make\nresponse generation less biased while preserving the models' conversational\ncapabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiaxu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zijing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Ling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Collaborative Plan Acquisition through Theory of Mind Modeling in Situated Dialogue. (arXiv:2305.11271v1 [cs.AI])","link":"http://arxiv.org/abs/2305.11271","description":"<p>Collaborative tasks often begin with partial task knowledge and incomplete\ninitial plans from each partner. To complete these tasks, agents need to engage\nin situated communication with their partners and coordinate their partial\nplans towards a complete plan to achieve a joint task goal. While such\ncollaboration seems effortless in a human-human team, it is highly challenging\nfor human-AI collaboration. To address this limitation, this paper takes a step\ntowards collaborative plan acquisition, where humans and agents strive to learn\nand communicate with each other to acquire a complete plan for joint tasks.\nSpecifically, we formulate a novel problem for agents to predict the missing\ntask knowledge for themselves and for their partners based on rich perceptual\nand dialogue history. We extend a situated dialogue benchmark for symmetric\ncollaborative tasks in a 3D blocks world and investigate computational\nstrategies for plan acquisition. Our empirical results suggest that predicting\nthe partner's missing knowledge is a more viable approach than predicting one's\nown. We show that explicit modeling of the partner's dialogue moves and mental\nstates produces improved and more stable results than without. These results\nprovide insight for future AI agents that can predict what knowledge their\npartner is missing and, therefore, can proactively communicate such information\nto help their partner acquire such missing knowledge toward a common\nunderstanding of joint tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bara_C/0/1/0/all/0/1\">Cristian-Paul Bara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Ziqiao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yingzhuo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1\">Julie Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Toponym Resolution with Better Candidate Generation, Transformer-based Reranking, and Two-Stage Resolution. (arXiv:2305.11315v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11315","description":"<p>Geocoding is the task of converting location mentions in text into structured\ndata that encodes the geospatial semantics. We propose a new architecture for\ngeocoding, GeoNorm. GeoNorm first uses information retrieval techniques to\ngenerate a list of candidate entries from the geospatial ontology. Then it\nreranks the candidate entries using a transformer-based neural network that\nincorporates information from the ontology such as the entry's population. This\ngenerate-and-rerank process is applied twice: first to resolve the less\nambiguous countries, states, and counties, and second to resolve the remaining\nlocation mentions, using the identified countries, states, and counties as\ncontext. Our proposed toponym resolution framework achieves state-of-the-art\nperformance on multiple datasets. Code and models are available at\n\\url{https://github.com/clulab/geonorm}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zeyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethard_S/0/1/0/all/0/1\">Steven Bethard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation. (arXiv:2305.11317v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11317","description":"<p>The field of text-to-image (T2I) generation has garnered significant\nattention both within the research community and among everyday users. Despite\nthe advancements of T2I models, a common issue encountered by users is the need\nfor repetitive editing of input prompts in order to receive a satisfactory\nimage, which is time-consuming and labor-intensive. Given the demonstrated text\ngeneration power of large-scale language models, such as GPT-k, we investigate\nthe potential of utilizing such models to improve the prompt editing process\nfor T2I generation. We conduct a series of experiments to compare the common\nedits made by humans and GPT-k, evaluate the performance of GPT-k in prompting\nT2I, and examine factors that may influence this process. We found that GPT-k\nmodels focus more on inserting modifiers while humans tend to replace words and\nphrases, which includes changes to the subject matter. Experimental results\nshow that GPT-k are more effective in adjusting modifiers rather than\npredicting spontaneous changes in the primary subject matters. Adopting the\nedit suggested by GPT-k models may reduce the percentage of remaining edits by\n20-30%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1\">Tsu-Jui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_M/0/1/0/all/0/1\">Miguel Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data. (arXiv:2305.11326v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11326","description":"<p>Tabular data is the most common format to publish and exchange structured\ndata online. A clear example is the growing number of open data portals\npublished by all types of public administrations. However, exploitation of\nthese data sources is currently limited to technical people able to\nprogrammatically manipulate and digest such data. As an alternative, we propose\nthe use of chatbots to offer a conversational interface to facilitate the\nexploration of tabular data sources. With our approach, any regular citizen can\nbenefit and leverage them. Moreover, our chatbots are not manually created:\ninstead, they are automatically generated from the data source itself thanks to\nthe instantiation of a configurable collection of conversation patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_M/0/1/0/all/0/1\">Marcos Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabot_J/0/1/0/all/0/1\">Jordi Cabot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clariso_R/0/1/0/all/0/1\">Robert Claris&#xf3;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Writing your own book: A method for going from closed to open book QA to improve robustness and performance of smaller LLMs. (arXiv:2305.11334v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11334","description":"<p>We introduce two novel methods, Tree-Search and Self-contextualizing QA,\ndesigned to enhance the performance of large language models (LLMs) in\nquestion-answering tasks. Tree-Search is a sampling technique specifically\ncreated to extract diverse information from an LLM for a given prompt.\nSelf-contextualizing QA leverages Tree-Search to enable the model to create its\nown context using a wide range of information relevant to the prompt, evaluate\nit explicitly and return a open book answer to the initial prompt . We\ndemonstrate that the quality of generated answers improves according to various\nmetrics, including accuracy, informativeness, coherence, and consistency, as\nevaluated by GPT3.5(text-davinci-003). Furthermore, we show that our methods\nresult in increased robustness and that performance is positively correlated\nwith tree size, benefiting both answer quality and robustness. Finally, we\ndiscuss other promising applications of Tree-Search, highlighting its potential\nto enhance a broad range of tasks beyond question-answering.\n</p>\n<p>\\noindent We also discuss several areas for future work, including refining\nthe Tree-Search and Self-Contextualizing QA methods, improving the coherence of\nthe generated context, and investigating the impact of bootstrapping on model\nrobustness\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kokaia_G/0/1/0/all/0/1\">Giorgi Kokaia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_P/0/1/0/all/0/1\">Pratyush Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yutong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boujemaa_N/0/1/0/all/0/1\">Nozha Boujemaa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In the Name of Fairness: Assessing the Bias in Clinical Record De-identification. (arXiv:2305.11348v1 [cs.LG])","link":"http://arxiv.org/abs/2305.11348","description":"<p>Data sharing is crucial for open science and reproducible research, but the\nlegal sharing of clinical data requires the removal of protected health\ninformation from electronic health records. This process, known as\nde-identification, is often achieved through the use of machine learning\nalgorithms by many commercial and open-source systems. While these systems have\nshown compelling results on average, the variation in their performance across\ndifferent demographic groups has not been thoroughly examined. In this work, we\ninvestigate the bias of de-identification systems on names in clinical notes\nvia a large-scale empirical analysis. To achieve this, we create 16 name sets\nthat vary along four demographic dimensions: gender, race, name popularity, and\nthe decade of popularity. We insert these names into 100 manually curated\nclinical templates and evaluate the performance of nine public and private\nde-identification methods. Our findings reveal that there are statistically\nsignificant performance gaps along a majority of the demographic dimensions in\nmost methods. We further illustrate that de-identification quality is affected\nby polysemy in names, gender context, and clinical note characteristics. To\nmitigate the identified gaps, we propose a simple and method-agnostic solution\nby fine-tuning de-identification methods with clinical context and diverse\nnames. Overall, it is imperative to address the bias in existing methods\nimmediately so that downstream stakeholders can build high-quality systems to\nserve all demographic parties fairly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yuxin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Shulammite Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollard_T/0/1/0/all/0/1\">Tom Joseph Pollard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1\">Marzyeh Ghassemi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain-agnostic Fake News Detection using Multi-modal Weak Signals. (arXiv:2305.11349v1 [cs.LG])","link":"http://arxiv.org/abs/2305.11349","description":"<p>The emergence of social media as one of the main platforms for people to\naccess news has enabled the wide dissemination of fake news. This has motivated\nnumerous studies on automating fake news detection. Although there have been\nlimited attempts at unsupervised fake news detection, their performance suffers\ndue to not exploiting the knowledge from various modalities related to news\nrecords and due to the presence of various latent biases in the existing news\ndatasets. To address these limitations, this work proposes an effective\nframework for unsupervised fake news detection, which first embeds the\nknowledge available in four modalities in news records and then proposes a\nnovel noise-robust self-supervised learning technique to identify the veracity\nof news records from the multi-modal embeddings. Also, we propose a novel\ntechnique to construct news datasets minimizing the latent biases in existing\nnews datasets. Following the proposed approach for dataset construction, we\nproduce a Large-scale Unlabelled News Dataset consisting 419,351 news articles\nrelated to COVID-19, acronymed as LUND-COVID. We trained the proposed\nunsupervised framework using LUND-COVID to exploit the potential of large\ndatasets, and evaluate it using a set of existing labelled datasets. Our\nresults show that the proposed unsupervised framework largely outperforms\nexisting unsupervised baselines for different tasks such as multi-modal fake\nnews detection, fake news early detection and few-shot fake news detection,\nwhile yielding notable improvements for unseen domains during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1\">Amila Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Ling Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karunasekera_S/0/1/0/all/0/1\">Shanika Karunasekera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leckie_C/0/1/0/all/0/1\">Christopher Leckie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Redaction from Conditional Generative Models. (arXiv:2305.11351v1 [cs.LG])","link":"http://arxiv.org/abs/2305.11351","description":"<p>Deep generative models are known to produce undesirable samples such as\nharmful content. Traditional mitigation methods include re-training from\nscratch, filtering, or editing; however, these are either computationally\nexpensive or can be circumvented by third parties. In this paper, we take a\ndifferent approach and study how to post-edit an already-trained conditional\ngenerative model so that it redacts certain conditionals that will, with high\nprobability, lead to undesirable content. This is done by distilling the\nconditioning network in the models, giving a solution that is effective,\nefficient, controllable, and universal for a class of deep generative models.\nWe conduct experiments on redacting prompts in text-to-image models and\nredacting voices in text-to-speech models. Our method is computationally light,\nleads to better redaction quality and robustness than baseline methods while\nstill retaining high generation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1\">Zhifeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1\">Kamalika Chaudhuri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MD3: The Multi-Dialect Dataset of Dialogues. (arXiv:2305.11355v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11355","description":"<p>We introduce a new dataset of conversational speech representing English from\nIndia, Nigeria, and the United States. The Multi-Dialect Dataset of Dialogues\n(MD3) strikes a new balance between open-ended conversational speech and\ntask-oriented dialogue by prompting participants to perform a series of short\ninformation-sharing tasks. This facilitates quantitative cross-dialectal\ncomparison, while avoiding the imposition of a restrictive task structure that\nmight inhibit the expression of dialect features. Preliminary analysis of the\ndataset reveals significant differences in syntax and in the use of discourse\nmarkers. The dataset, which will be made publicly available with the\npublication of this paper, includes more than 20 hours of audio and more than\n200,000 orthographically-transcribed tokens.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1\">Vinodkumar Prabhakaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivera_C/0/1/0/all/0/1\">Clara Rivera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demszky_D/0/1/0/all/0/1\">Dorottya Demszky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Devyani Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models. (arXiv:2305.11364v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11364","description":"<p>Large language models (LLMs) can be used to generate smaller, more refined\ndatasets via few-shot prompting for benchmarking, fine-tuning or other use\ncases. However, understanding and evaluating these datasets is difficult, and\nthe failure modes of LLM-generated data are still not well understood.\nSpecifically, the data can be repetitive in surprising ways, not only\nsemantically but also syntactically and lexically. We present LinguisticLens, a\nnovel inter-active visualization tool for making sense of and analyzing\nsyntactic diversity of LLM-generated datasets. LinguisticLens clusters text\nalong syntactic, lexical, and semantic axes. It supports hierarchical\nvisualization of a text dataset, allowing users to quickly scan for an overview\nand inspect individual examples. The live demo is available at\nshorturl.at/zHOUV.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1\">Emily Reif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahng_M/0/1/0/all/0/1\">Minsuk Kahng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petridis_S/0/1/0/all/0/1\">Savvas Petridis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoTrial: Prompting Language Models for Clinical Trial Design. (arXiv:2305.11366v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11366","description":"<p>Clinical trials are critical for drug development. Constructing the\nappropriate eligibility criteria (i.e., the inclusion/exclusion criteria for\npatient recruitment) is essential for the trial's success. Proper design of\nclinical trial protocols should consider similar precedent trials and their\neligibility criteria to ensure sufficient patient coverage. In this paper, we\npresent a method named AutoTrial to aid the design of clinical eligibility\ncriteria using language models. It allows (1) controllable generation under\ninstructions via a hybrid of discrete and neural prompting, (2) scalable\nknowledge incorporation via in-context learning, and (3) explicit reasoning\nchains to provide rationales for understanding the outputs. Experiments on over\n70K clinical trials verify that AutoTrial generates high-quality criteria texts\nthat are fluent and coherent and with high accuracy in capturing the relevant\nclinical concepts to the target trial. It is noteworthy that our method, with a\nmuch smaller parameter size, gains around 60\\% winning rate against the GPT-3.5\nbaselines via human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Cao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jimeng Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing tradeoffs between teaching via language and demonstrations in multi-agent systems. (arXiv:2305.11374v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11374","description":"<p>Humans teach others about the world through language and demonstration. When\nmight one of these modalities be more effective than the other? In this work,\nwe study the factors that modulate the effectiveness of language vs.\ndemonstration using multi-agent systems to model human communication.\nSpecifically, we train neural network agents to teach via language or\ndemonstration in a grounded communication task, manipulating 1) the inherent\ndifficulty of the task and 2) the competence of the teacher. We find that\nteaching by demonstration is more effective in the simplest settings, but\nlanguage is more effective as task difficulty increases, due to its ability to\ngeneralize more effectively to unseen scenarios. Overall, these results provide\nconverging evidence for a tradeoff between language and demonstration as\nteaching modalities in humans, and make the novel predictions that\ndemonstration may be optimal for easy tasks, while language enables\ngeneralization in more challenging settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dhara Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah D. Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1\">Jesse Mu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast-StrucTexT: An Efficient Hourglass Transformer with Modality-guided Dynamic Token Merge for Document Understanding. (arXiv:2305.11392v1 [cs.CV])","link":"http://arxiv.org/abs/2305.11392","description":"<p>Transformers achieve promising performance in document understanding because\nof their high effectiveness and still suffer from quadratic computational\ncomplexity dependency on the sequence length. General efficient transformers\nare challenging to be directly adapted to model document. They are unable to\nhandle the layout representation in documents, e.g. word, line and paragraph,\non different granularity levels and seem hard to achieve a good trade-off\nbetween efficiency and performance. To tackle the concerns, we propose\nFast-StrucTexT, an efficient multi-modal framework based on the StrucTexT\nalgorithm with an hourglass transformer architecture, for visual document\nunderstanding. Specifically, we design a modality-guided dynamic token merging\nblock to make the model learn multi-granularity representation and prunes\nredundant tokens. Additionally, we present a multi-modal interaction module\ncalled Symmetry Cross Attention (SCA) to consider multi-modal fusion and\nefficiently guide the token mergence. The SCA allows one modality input as\nquery to calculate cross attention with another modality in a dual phase.\nExtensive experiments on FUNSD, SROIE, and CORD datasets demonstrate that our\nmodel achieves the state-of-the-art performance and almost 1.9X faster\ninference time than the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_M/0/1/0/all/0/1\">Mingliang Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yulin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiameng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_C/0/1/0/all/0/1\">Chen Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qunyi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1\">Kun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yunde Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comfort Foods and Community Connectedness: Investigating Diet Change during COVID-19 Using YouTube Videos on Twitter. (arXiv:2305.11398v1 [cs.SI])","link":"http://arxiv.org/abs/2305.11398","description":"<p>Unprecedented lockdowns at the start of the COVID-19 pandemic have\ndrastically changed the routines of millions of people, potentially impacting\nimportant health-related behaviors. In this study, we use YouTube videos\nembedded in tweets about diet, exercise and fitness posted before and during\nCOVID-19 to investigate the influence of the pandemic lockdowns on diet and\nnutrition. In particular, we examine the nutritional profile of the foods\nmentioned in the transcript, description and title of each video in terms of\nsix macronutrients (protein, energy, fat, sodium, sugar, and saturated fat).\nThese macronutrient values were further linked to demographics to assess if\nthere are specific effects on those potentially having insufficient access to\nhealthy sources of food. Interrupted time series analysis revealed a\nconsiderable shift in the aggregated macronutrient scores before and during\nCOVID-19. In particular, whereas areas with lower incomes showed decrease in\nenergy, fat, and saturated fat, those with higher percentage of African\nAmericans showed an elevation in sodium. Word2Vec word similarities and odds\nratio analysis suggested a shift from popular diets and lifestyle bloggers\nbefore the lockdowns to the interest in a variety of healthy foods, communal\nsharing of quick and easy recipes, as well as a new emphasis on comfort foods.\nTo the best of our knowledge, this work is novel in terms of linking attention\nsignals in tweets, content of videos, their nutrients profile, and aggregate\ndemographics of the users. The insights made possible by this combination of\nresources are important for monitoring the secondary health effects of social\ndistancing, and informing social programs designed to alleviate these effects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mejova_Y/0/1/0/all/0/1\">Yelena Mejova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manikonda_L/0/1/0/all/0/1\">Lydia Manikonda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide for Simultaneous Speech Translation. (arXiv:2305.11408v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11408","description":"<p>Attention is the core mechanism of today's most used architectures for\nnatural language processing and has been analyzed from many perspectives,\nincluding its effectiveness for machine translation-related tasks. Among these\nstudies, attention resulted to be a useful source of information to get\ninsights about word alignment also when the input text is substituted with\naudio segments, as in the case of the speech translation (ST) task. In this\npaper, we propose AlignAtt, a novel policy for simultaneous ST (SimulST) that\nexploits the attention information to generate source-target alignments that\nguide the model during inference. Through experiments on the 8 language pairs\nof MuST-C v1.0, we show that AlignAtt outperforms previous state-of-the-art\nSimulST policies applied to offline-trained models with gains in terms of BLEU\nof 2 points and latency reductions ranging from 0.5s to 0.8s across the 8\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DUB: Discrete Unit Back-translation for Speech Translation. (arXiv:2305.11411v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11411","description":"<p>How can speech-to-text translation (ST) perform as well as machine\ntranslation (MT)? The key point is to bridge the modality gap between speech\nand text so that useful MT techniques can be applied to ST. Recently, the\napproach of representing speech with unsupervised discrete units yields a new\nway to ease the modality problem. This motivates us to propose Discrete Unit\nBack-translation (DUB) to answer two questions: (1) Is it better to represent\nspeech with discrete units than with continuous features in direct ST? (2) How\nmuch benefit can useful MT techniques bring to ST? With DUB, the\nback-translation technique can successfully be applied on direct ST and obtains\nan average boost of 5.5 BLEU on MuST-C En-De/Fr/Es. In the low-resource\nlanguage scenario, our method achieves comparable performance to existing\nmethods that rely on large-scale external data. Code and models are available\nat https://github.com/0nutation/DUB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Rong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_T/0/1/0/all/0/1\">Tom Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yaqian Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Post Hoc Explanations of Language Models Can Improve Language Models. (arXiv:2305.11426v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11426","description":"<p>Large Language Models (LLMs) have demonstrated remarkable capabilities in\nperforming complex tasks. Moreover, recent research has shown that\nincorporating human-annotated rationales (e.g., Chain-of- Thought prompting)\nduring in-context learning can significantly enhance the performance of these\nmodels, particularly on tasks that require reasoning capabilities. However,\nincorporating such rationales poses challenges in terms of scalability as this\nrequires a high degree of human involvement. In this work, we present a novel\nframework, Amplifying Model Performance by Leveraging In-Context Learning with\nPost Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges\nby automating the process of rationale generation. To this end, we leverage\npost hoc explanation methods which output attribution scores (explanations)\ncapturing the influence of each of the input features on model predictions.\nMore specifically, we construct automated natural language rationales that\nembed insights from post hoc explanations to provide corrective signals to\nLLMs. Extensive experimentation with real-world datasets demonstrates that our\nframework, AMPLIFY, leads to prediction accuracy improvements of about 10-25%\nover a wide range of tasks, including those where prior approaches which rely\non human-annotated rationales such as Chain-of-Thought prompting fall short.\nOur work makes one of the first attempts at highlighting the potential of post\nhoc explanations as valuable tools for enhancing the effectiveness of LLMs.\nFurthermore, we conduct additional empirical analyses and ablation studies to\ndemonstrate the impact of each of the components of AMPLIFY, which, in turn,\nlead to critical insights for refining in-context learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Satyapriya/0/1/0/all/0/1\">Satyapriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna/0/1/0/all/0/1\">Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiaqi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slack_D/0/1/0/all/0/1\">Dylan Slack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghandeharioun_A/0/1/0/all/0/1\">Asma Ghandeharioun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1\">Himabindu Lakkaraju</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks. (arXiv:2305.11430v1 [cs.AI])","link":"http://arxiv.org/abs/2305.11430","description":"<p>While LLMs have shown great success in understanding and generating text in\ntraditional conversational settings, their potential for performing ill-defined\ncomplex tasks is largely under-studied. Indeed, we are yet to conduct\ncomprehensive benchmarking studies with multiple LLMs that are exclusively\nfocused on a complex task. However, conducting such benchmarking studies is\nchallenging because of the large variations in LLMs' performance when different\nprompt types/styles are used and different degrees of detail are provided in\nthe prompts. To address this issue, the paper proposes a general taxonomy that\ncan be used to design prompts with specific properties in order to perform a\nwide range of complex tasks. This taxonomy will allow future benchmarking\nstudies to report the specific categories of prompts used as part of the study,\nenabling meaningful comparisons across different studies. Also, by establishing\na common standard through this taxonomy, researchers will be able to draw more\naccurate conclusions about LLMs' performance on a specific complex task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santu_S/0/1/0/all/0/1\">Shubhra Kanti Karmaker Santu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">Dongji Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Mode. (arXiv:2305.11435v1 [eess.AS])","link":"http://arxiv.org/abs/2305.11435","description":"<p>In this paper, we show that representations capturing syllabic units emerge\nwhen training a self-supervised speech model with a visually-grounded training\nobjective. We demonstrate that a nearly identical model architecture (HuBERT)\ntrained with a masked language modeling loss does not exhibit this same\nability, suggesting that the visual grounding objective is responsible for the\nemergence of this phenomenon. We propose the use of a minimum cut algorithm to\nautomatically predict syllable boundaries in speech, followed by a 2-stage\nclustering method to group identical syllables together. We show that our model\nnot only outperforms a state-of-the-art syllabic segmentation method on the\nlanguage it was trained on (English), but also generalizes in a zero-shot\nfashion to Estonian. Finally, we show that the same model is capable of\nzero-shot generalization for a word segmentation task on 4 other languages from\nthe Zerospeech Challenge, in some cases beating the previous state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_P/0/1/0/all/0/1\">Puyuan Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phonetic and Prosody-aware Self-supervised Learning Approach for Non-native Fluency Scoring. (arXiv:2305.11438v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11438","description":"<p>Speech fluency/disfluency can be evaluated by analyzing a range of phonetic\nand prosodic features. Deep neural networks are commonly trained to map\nfluency-related features into the human scores. However, the effectiveness of\ndeep learning-based models is constrained by the limited amount of labeled\ntraining samples. To address this, we introduce a self-supervised learning\n(SSL) approach that takes into account phonetic and prosody awareness for\nfluency scoring. Specifically, we first pre-train the model using a\nreconstruction loss function, by masking phones and their durations jointly on\na large amount of unlabeled speech and text prompts. We then fine-tune the\npre-trained model using human-annotated scoring data. Our experimental results,\nconducted on datasets such as Speechocean762 and our non-native datasets, show\nthat our proposed method outperforms the baseline systems in terms of Pearson\ncorrelation coefficients (PCC). Moreover, we also conduct an ablation study to\nbetter understand the contribution of phonetic and prosody factors during the\npre-training stage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Kaiqi Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shaojun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuju Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xiaohai Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zejun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Text Classification via Self-Supervised Tuning. (arXiv:2305.11442v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11442","description":"<p>Existing solutions to zero-shot text classification either conduct prompting\nwith pre-trained language models, which is sensitive to the choices of\ntemplates, or rely on large-scale annotated data of relevant tasks for\nmeta-tuning. In this work, we propose a new paradigm based on self-supervised\nlearning to solve zero-shot text classification tasks by tuning the language\nmodels with unlabeled data, called self-supervised tuning. By exploring the\ninherent structure of free texts, we propose a new learning objective called\nfirst sentence prediction to bridge the gap between unlabeled data and text\nclassification tasks. After tuning the model to learn to predict the first\nsentence in a paragraph based on the rest, the model is able to conduct\nzero-shot inference on unseen tasks such as topic classification and sentiment\nanalysis. Experimental results show that our model outperforms the\nstate-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis reveals\nthat our model is less sensitive to the prompt design. Our code and pre-trained\nmodels are publicly available at https://github.com/DAMO-NLP-SG/SSTuning .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chaoqun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guizhen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaobao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1\">Anh Tuan Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chip Hong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arukikata Travelogue Dataset. (arXiv:2305.11444v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11444","description":"<p>We have constructed Arukikata Travelogue Dataset and released it free of\ncharge for academic research. This dataset is a Japanese text dataset with a\ntotal of over 31 million words, comprising 4,672 Japanese domestic travelogues\nand 9,607 overseas travelogues. Before providing our dataset, there was a\nscarcity of widely available travelogue data for research purposes, and each\nresearcher had to prepare their own data. This hinders the replication of\nexisting studies and fair comparative analysis of experimental results. Our\ndataset enables any researchers to conduct investigation on the same data and\nto ensure transparency and reproducibility in research. In this paper, we\ndescribe the academic significance, characteristics, and prospects of our\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouchi_H/0/1/0/all/0/1\">Hiroki Ouchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shindo_H/0/1/0/all/0/1\">Hiroyuki Shindo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wakamiya_S/0/1/0/all/0/1\">Shoko Wakamiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuda_Y/0/1/0/all/0/1\">Yuki Matsuda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inoue_N/0/1/0/all/0/1\">Naoya Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Higashiyama_S/0/1/0/all/0/1\">Shohei Higashiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1\">Satoshi Nakamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1\">Taro Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-tuning Slow and Fast. (arXiv:2305.11449v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11449","description":"<p>Existing research has shown that a multilingual pre-trained language model\nfine-tuned with one (source) language also performs well on downstream tasks\nfor non-source languages, even though no fine-tuning is done on these\nlanguages. However, there is a clear gap between the performance of the source\nlanguage and that of the non-source languages. This paper analyzes the\nfine-tuning process, discovers when the performance gap changes and identifies\nwhich network weights affect the overall performance most. Additionally, the\npaper seeks to answer to what extent the gap can be reduced by reducing\nforgetting. Based on the analysis results, a method named Fine-tuning slow and\nfast with four training policies is proposed to address these issues.\nExperimental results show the proposed method outperforms baselines by a clear\nmargin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yiduo Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yaobo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_D/0/1/0/all/0/1\">Duan Nan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shattering the Agent-Environment Interface for Fine-Tuning Inclusive Language Models. (arXiv:2305.11455v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11455","description":"<p>A centerpiece of the ever-popular reinforcement learning from human feedback\n(RLHF) approach to fine-tuning autoregressive language models is the explicit\ntraining of a reward model to emulate human feedback, distinct from the\nlanguage model itself. This reward model is then coupled with policy-gradient\nmethods to dramatically improve the alignment between language model outputs\nand desired responses. In this work, we adopt a novel perspective wherein a\npre-trained language model is itself simultaneously a policy, reward function,\nand transition function. An immediate consequence of this is that reward\nlearning and language model fine-tuning can be performed jointly and directly,\nwithout requiring any further downstream policy optimization. While this\nperspective does indeed break the traditional agent-environment interface, we\nnevertheless maintain that there can be enormous statistical benefits afforded\nby bringing to bear traditional algorithmic concepts from reinforcement\nlearning. Our experiments demonstrate one concrete instance of this through\nefficient exploration based on the representation and resolution of epistemic\nuncertainty. In order to illustrate these ideas in a transparent manner, we\nrestrict attention to a simple didactic data generating process and leave for\nfuture work extension to systems of practical scale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wanqiao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Shi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arumugam_D/0/1/0/all/0/1\">Dilip Arumugam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1\">Benjamin Van Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Agreement: A Framework for Fine-tuning Language Models to Find Agreement among Diverse Opinions. (arXiv:2305.11460v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11460","description":"<p>Finding an agreement among diverse opinions is a challenging topic in\nmultiagent systems. Recently, large language models (LLMs) have shown great\npotential in addressing this challenge due to their remarkable capabilities in\ncomprehending human opinions and generating human-like text. However, they\ntypically rely on extensive human-annotated data. In this paper, we propose\nSelf-Agreement, a novel framework for fine-tuning LLMs to autonomously find\nagreement using data generated by LLM itself. Specifically, our approach\nemploys the generative pre-trained transformer-3 (GPT-3) to generate multiple\nopinions for each question in a question dataset and create several agreement\ncandidates among these opinions. Then, a bidirectional encoder representations\nfrom transformers (BERT)-based model evaluates the agreement score of each\nagreement candidate and selects the one with the highest agreement score. This\nprocess yields a dataset of question-opinion-agreements, which we use to\nfine-tune a pre-trained LLM for discovering agreements among diverse opinions.\nRemarkably, a pre-trained LLM fine-tuned by our Self-Agreement framework\nachieves comparable performance to GPT-3 with only 1/25 of its parameters,\nshowcasing its ability to identify agreement among various opinions without the\nneed for human-annotated data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shiyao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ito_T/0/1/0/all/0/1\">Takayuki Ito</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extending Memory for Language Modelling. (arXiv:2305.11462v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11462","description":"<p>Breakthroughs in deep learning and memory networks have made major advances\nin natural language understanding. Language is sequential and information\ncarried through the sequence can be captured through memory networks. Learning\nthe sequence is one of the key aspects in learning the language. However,\nmemory networks are not capable of holding infinitely long sequences in their\nmemories and are limited by various constraints such as the vanishing or\nexploding gradient problem. Therefore, natural language understanding models\nare affected when presented with long sequential text. We introduce Long Term\nMemory network (LTM) to learn from infinitely long sequences. LTM gives\npriority to the current inputs to allow it to have a high impact. Language\nmodeling is an important factor in natural language understanding. LTM was\ntested in language modeling, which requires long term memory. LTM is tested on\nPenn Tree bank dataset, Google Billion Word dataset and WikiText-2 dataset. We\ncompare LTM with other language models which require long term memory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nugaliyadde_A/0/1/0/all/0/1\">Anupiya Nugaliyadde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graphologue: Exploring Large Language Model Responses with Interactive Diagrams. (arXiv:2305.11473v1 [cs.HC])","link":"http://arxiv.org/abs/2305.11473","description":"<p>Large language models (LLMs) have recently soared in popularity due to their\nease of access and the unprecedented intelligence exhibited on diverse\napplications. However, LLMs like ChatGPT present significant limitations in\nsupporting complex information tasks due to the insufficient affordances of the\ntext-based medium and linear conversational structure. Through a formative\nstudy with ten participants, we found that LLM interfaces often present\nlong-winded responses, making it difficult for people to quickly comprehend and\ninteract flexibly with various pieces of information, particularly during more\ncomplex tasks. We present Graphologue, an interactive system that converts\ntext-based responses from LLMs into graphical diagrams to facilitate\ninformation-seeking and question-answering tasks. Graphologue employs novel\nprompting strategies and interface designs to extract entities and\nrelationships from LLM responses and constructs node-link diagrams in\nreal-time. Further, users can interact with the diagrams to flexibly adjust the\ngraphical presentation and to submit context-specific prompts to obtain more\ninformation. Utilizing diagrams, Graphologue enables graphical, non-linear\ndialogues between humans and LLMs, facilitating information exploration,\norganization, and comprehension.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peiling Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayan_J/0/1/0/all/0/1\">Jude Rayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dow_S/0/1/0/all/0/1\">Steven P. Dow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1\">Haijun Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CCGen: Explainable Complementary Concept Generation in E-Commerce. (arXiv:2305.11480v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11480","description":"<p>We propose and study Complementary Concept Generation (CCGen): given a\nconcept of interest, e.g., \"Digital Cameras\", generating a list of\ncomplementary concepts, e.g., 1) Camera Lenses 2) Batteries 3) Camera Cases 4)\nMemory Cards 5) Battery Chargers. CCGen is beneficial for various applications\nlike query suggestion and item recommendation, especially in the e-commerce\ndomain. To solve CCGen, we propose to train language models to generate ranked\nlists of concepts with a two-step training strategy. We also teach the models\nto generate explanations by incorporating explanations distilled from large\nteacher models. Extensive experiments and analysis demonstrate that our model\ncan generate high-quality concepts complementary to the input concept while\nproducing explanations to justify the predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yifan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingfeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zining Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Personalized Dialogue Generation with Contrastive Latent Variables: Combining Sparse and Dense Persona. (arXiv:2305.11482v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11482","description":"<p>The personalized dialogue explores the consistent relationship between\ndialogue generation and personality. Existing personalized dialogue agents\nmodel persona profiles from three resources: sparse or dense persona\ndescriptions and dialogue histories. However, sparse structured persona\nattributes are explicit but uninformative, dense persona texts contain rich\npersona descriptions with much noise, and dialogue history query is both noisy\nand uninformative for persona modeling. In this work, we combine the advantages\nof the three resources to obtain a richer and more accurate persona. We design\na Contrastive Latent Variable-based model (CLV) that clusters the dense persona\ndescriptions into sparse categories, which are combined with the history query\nto generate personalized responses. Experimental results on Chinese and English\ndatasets demonstrate our model's superiority in personalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yihong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Miao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruifang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yuexian Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])","link":"http://arxiv.org/abs/2305.11490","description":"<p>Building on the recent remarkable development of large language models\n(LLMs), active attempts are being made to extend the utility of LLMs to\nmultimodal tasks. There have been previous efforts to link language and visual\ninformation, and attempts to add visual capabilities to LLMs are ongoing as\nwell. However, existing attempts use LLMs only as image decoders and no attempt\nhas been made to generate images in the same line as the natural language. By\nadopting a VQ-GAN framework in which latent representations of images are\ntreated as a kind of text tokens, we present a novel method to fine-tune a\npre-trained LLM to read and generate images like text without any structural\nchanges, extra training objectives, or the need for training an ad-hoc network\nwhile still preserving the of the instruction-following capability of the LLM.\nWe apply this framework to chest X-ray (CXR) image and report generation tasks\nas it is a domain in which translation of complex information between visual\nand language domains is important. The code will soon be made publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Suhyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1\">Won Jun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding. (arXiv:2305.11497v1 [cs.CV])","link":"http://arxiv.org/abs/2305.11497","description":"<p>Prompt tuning has achieved great success in transferring the knowledge from\nlarge pretrained vision-language models into downstream tasks, and has\ndominated the performance on visual grounding (VG). However, almost all\nexisting prompt tuning paradigms suffer from poor interpretability. In this\npaper, we argue that their poor interpretability is attributed to the holistic\nprompt generation and inference process. By \"holistic\", we mean that they\nusually directly learn a set of vectors as the prompt (i.e., prompt\ngeneration), and use the learned global prompt to augment the textual input for\nthe VG model (i.e., prompt inference). To this end, we propose a new prompt\nconstruction paradigm with explicit explainable ability, named TreePrompt.\nSpecifically, we first deconstruct a complex sentence into a tree, that is\nconsistent with human reasoning. Then, following the syntax tree, we compose a\nstructured prompt in a bottom-up manner. Thanks to this step-by-step prompt\nconstruction process, each intermediate prompt (i.e., tree node) permits us to\nunderstand the reasoning process. Extensive ablations on various backbones and\nbenchmarks consistently demonstrate the effectiveness and interpretability of\nour TreePrompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenchi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jian Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recouple Event Field via Probabilistic Bias for Event Extraction. (arXiv:2305.11498v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11498","description":"<p>Event Extraction (EE), aiming to identify and classify event triggers and\narguments from event mentions, has benefited from pre-trained language models\n(PLMs). However, existing PLM-based methods ignore the information of\ntrigger/argument fields, which is crucial for understanding event schemas. To\nthis end, we propose a Probabilistic reCoupling model enhanced Event extraction\nframework (ProCE). Specifically, we first model the syntactic-related event\nfields as probabilistic biases, to clarify the event fields from ambiguous\nentanglement. Furthermore, considering multiple occurrences of the same\ntriggers/arguments in EE, we explore probabilistic interaction strategies among\nmultiple fields of the same triggers/arguments, to recouple the corresponding\nclarified distributions and capture more latent information fields. Experiments\non EE datasets demonstrate the effectiveness and generalization of our proposed\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xingyu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Taiqiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Han Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuefeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiayi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weijie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_Q/0/1/0/all/0/1\">Qi Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Weigang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought. (arXiv:2305.11499v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11499","description":"<p>Large language Models (LLMs) have achieved promising performance on\narithmetic reasoning tasks by incorporating step-by-step chain-of-thought (CoT)\nprompting. However, LLMs face challenges in maintaining factual consistency\nduring reasoning, exhibiting tendencies to condition overlooking, question\nmisinterpretation, and condition hallucination over given problems. Existing\nmethods use coarse-grained feedback (e.g., whether the answer is correct) to\nimprove factual consistency. In this work, we propose RCoT (Reversing\nChain-of-Thought), a novel method to improve LLMs' reasoning abilities by\nautomatically detecting and rectifying factual inconsistency in LLMs' generated\nsolutions. To detect factual inconsistency, RCoT first asks LLMs to reconstruct\nthe problem based on generated solutions. Then fine-grained comparisons between\nthe original problem and the reconstructed problem expose the factual\ninconsistency in the original solutions. To rectify the solution, RCoT\nformulates detected factual inconsistency into fine-grained feedback to guide\nLLMs in revising solutions. Experimental results demonstrate consistent\nimprovements of RCoT over standard CoT across seven arithmetic datasets.\nMoreover, we find that manually written fine-grained feedback can dramatically\nimprove LLMs' reasoning abilities (e.g., ChatGPT reaches 94.6% accuracy on\nGSM8K), encouraging the community to further explore the fine-grained feedback\ngeneration methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1\">Tianci Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhailong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Pengfei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Alignment to Entailment: A Unified Textual Entailment Framework for Entity Alignment. (arXiv:2305.11501v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11501","description":"<p>Entity Alignment (EA) aims to find the equivalent entities between two\nKnowledge Graphs (KGs). Existing methods usually encode the triples of entities\nas embeddings and learn to align the embeddings, which prevents the direct\ninteraction between the original information of the cross-KG entities.\nMoreover, they encode the relational triples and attribute triples of an entity\nin heterogeneous embedding spaces, which prevents them from helping each other.\nIn this paper, we transform both triples into unified textual sequences, and\nmodel the EA task as a bi-directional textual entailment task between the\nsequences of cross-KG entities. Specifically, we feed the sequences of two\nentities simultaneously into a pre-trained language model (PLM) and propose two\nkinds of PLM-based entity aligners that model the entailment probability\nbetween sequences as the similarity between entities. Our approach captures the\nunified correlation pattern of two kinds of information between entities, and\nexplicitly models the fine-grained interaction between original entity\ninformation. The experiments on five cross-lingual EA datasets show that our\napproach outperforms the state-of-the-art EA methods and enables the mutual\nenhancement of the heterogeneous information. Codes are available at\nhttps://github.com/OreOZhao/TEA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yike Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xiangrui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaojie Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Topic-aware Summarization Framework with Different Modal Side Information. (arXiv:2305.11503v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11503","description":"<p>Automatic summarization plays an important role in the exponential document\ngrowth on the Web. On content websites such as CNN.com and WikiHow.com, there\noften exist various kinds of side information along with the main document for\nattention attraction and easier understanding, such as videos, images, and\nqueries. Such information can be used for better summarization, as they often\nexplicitly or implicitly mention the essence of the article. However, most of\nthe existing side-aware summarization methods are designed to incorporate\neither single-modal or multi-modal side information, and cannot effectively\nadapt to each other. In this paper, we propose a general summarization\nframework, which can flexibly incorporate various modalities of side\ninformation. The main challenges in designing a flexible summarization model\nwith side information include: (1) the side information can be in textual or\nvisual format, and the model needs to align and unify it with the document into\nthe same semantic space, (2) the side inputs can contain information from\nvarious aspects, and the model should recognize the aspects useful for\nsummarization. To address these two challenges, we first propose a unified\ntopic encoder, which jointly discovers latent topics from the document and\nvarious kinds of side information. The learned topics flexibly bridge and guide\nthe information flow between multiple inputs in a graph encoder through a\ntopic-aware interaction. We secondly propose a triplet contrastive learning\nmechanism to align the single-modal or multi-modal information into a unified\nsemantic space, where the summary quality is enhanced by better understanding\nthe document and side information. Results show that our model significantly\nsurpasses strong baselines on three public single-modal or multi-modal\nbenchmark summarization datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qishen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plug-and-Play Medical Dialogue System. (arXiv:2305.11508v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11508","description":"<p>Medical dialogue systems aim to provide accurate answers to patients,\nnecessitating specific domain knowledge. Recent advancements in Large Language\nModels (LLMs) have demonstrated their exceptional capabilities in the medical\nQ&amp;A domain, indicating a rich understanding of common sense. However, LLMs are\ninsufficient for direct diagnosis due to the absence of diagnostic strategies.\nThe conventional approach to address this challenge involves expensive\nfine-tuning of LLMs. Alternatively, a more appealing solution is the\ndevelopment of a plugin that empowers LLMs to perform medical conversation\ntasks. Drawing inspiration from in-context learning, we propose PlugMed, a\nPlug-and-Play Medical Dialogue System that facilitates appropriate dialogue\nactions by LLMs through two modules: the prompt generation (PG) module and the\nresponse ranking (RR) module. The PG module is designed to capture dialogue\ninformation from both global and local perspectives. It selects suitable\nprompts by assessing their similarity to the entire dialogue history and recent\nutterances grouped by patient symptoms, respectively. Additionally, the RR\nmodule incorporates fine-tuned SLMs as response filters and selects appropriate\nresponses generated by LLMs. Moreover, we devise a novel evaluation method\nbased on intent and medical entities matching to assess the efficacy of\ndialogue strategies in medical conversations more effectively. Experimental\nevaluations conducted on three unlabeled medical dialogue datasets, including\nboth automatic and manual assessments, demonstrate that our model surpasses the\nstrong fine-tuning baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_C/0/1/0/all/0/1\">Chengfeng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenping Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haiyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1\">Zhenwei Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yongqiang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextualized Word Vector-based Methods for Discovering Semantic Differences with No Training nor Word Alignment. (arXiv:2305.11516v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11516","description":"<p>In this paper, we propose methods for discovering semantic differences in\nwords appearing in two corpora based on the norms of contextualized word\nvectors. The key idea is that the coverage of meanings is reflected in the norm\nof its mean word vector. The proposed methods do not require the assumptions\nconcerning words and corpora for comparison that the previous methods do. All\nthey require are to compute the mean vector of contextualized word vectors and\nits norm for each word type. Nevertheless, they are (i) robust for the skew in\ncorpus size; (ii) capable of detecting semantic differences in infrequent\nwords; and (iii) effective in pinpointing word instances that have a meaning\nmissing in one of the two corpora for comparison. We show these advantages for\nnative and non-native English corpora and also for historical corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagata_R/0/1/0/all/0/1\">Ryo Nagata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takamura_H/0/1/0/all/0/1\">Hiroya Takamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otani_N/0/1/0/all/0/1\">Naoki Otani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawasaki_Y/0/1/0/all/0/1\">Yoshifumi Kawasaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffuSIA: A Spiral Interaction Architecture for Encoder-Decoder Text Diffusion. (arXiv:2305.11517v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11517","description":"<p>Diffusion models have emerged as the new state-of-the-art family of deep\ngenerative models, and their promising potentials for text generation have\nrecently attracted increasing attention. Existing studies mostly adopt a single\nencoder architecture with partially noising processes for conditional text\ngeneration, but its degree of flexibility for conditional modeling is limited.\nIn fact, the encoder-decoder architecture is naturally more flexible for its\ndetachable encoder and decoder modules, which is extensible to multilingual and\nmultimodal generation tasks for conditions and target texts. However, the\nencoding process of conditional texts lacks the understanding of target texts.\nTo this end, a spiral interaction architecture for encoder-decoder text\ndiffusion (DiffuSIA) is proposed. Concretely, the conditional information from\nencoder is designed to be captured by the diffusion decoder, while the target\ninformation from decoder is designed to be captured by the conditional encoder.\nThese two types of information flow run through multilayer interaction spirally\nfor deep fusion and understanding. DiffuSIA is evaluated on four text\ngeneration tasks, including paraphrase, text simplification, question\ngeneration, and open-domain dialogue generation. Experimental results show that\nDiffuSIA achieves competitive performance among previous methods on all four\ntasks, demonstrating the effectiveness and generalization ability of the\nproposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chao-Hong Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jia-Chen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhen-Hua Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InstructIE: A Chinese Instruction-based Information Extraction Dataset. (arXiv:2305.11527v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11527","description":"<p>We introduce a new Information Extraction (IE) task dubbed Instruction-based\nIE, which aims to ask the system to follow specific instructions or guidelines\nto extract information. To facilitate research in this area, we construct a\ndataset called InstructIE, consisting of 270,000 weakly supervised data from\nChinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We\nfurther evaluate the performance of various baseline models on the InstructIE\ndataset. The results reveal that although current models exhibit promising\nperformance, there is still room for improvement. Furthermore, we conduct a\ncomprehensive case study analysis, underlining the challenges inherent in the\nInstruction-based IE task. Code and dataset are available at\nhttps://github.com/zjunlp/DeepKE/tree/main/example/llm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gui_H/0/1/0/all/0/1\">Honghao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jintian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Sequence-to-Sequence Approach for Arabic Pronoun Resolution. (arXiv:2305.11529v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11529","description":"<p>This paper proposes a sequence-to-sequence learning approach for Arabic\npronoun resolution, which explores the effectiveness of using advanced natural\nlanguage processing (NLP) techniques, specifically Bi-LSTM and the BERT\npre-trained Language Model, in solving the pronoun resolution problem in\nArabic. The proposed approach is evaluated on the AnATAr dataset, and its\nperformance is compared to several baseline models, including traditional\nmachine learning models and handcrafted feature-based models. Our results\ndemonstrate that the proposed model outperforms the baseline models, which\ninclude KNN, logistic regression, and SVM, across all metrics. In addition, we\nexplore the effectiveness of various modifications to the model, including\nconcatenating the anaphor text beside the paragraph text as input, adding a\nmask to focus on candidate scores, and filtering candidates based on gender and\nnumber agreement with the anaphor. Our results show that these modifications\nsignificantly improve the model's performance, achieving up to 81% on MRR and\n71% for F1 score while also demonstrating higher precision, recall, and\naccuracy. These findings suggest that the proposed model is an effective\napproach to Arabic pronoun resolution and highlights the potential benefits of\nleveraging advanced NLP neural models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murayshid_H/0/1/0/all/0/1\">Hanan S. Murayshid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benhidour_H/0/1/0/all/0/1\">Hafida Benhidour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerrache_S/0/1/0/all/0/1\">Said Kerrache</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PORTRAIT: a hybrid aPproach tO cReate extractive ground-TRuth summAry for dIsaster evenT. (arXiv:2305.11536v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11536","description":"<p>Disaster summarization approaches provide an overview of the important\ninformation posted during disaster events on social media platforms, such as,\nTwitter. However, the type of information posted significantly varies across\ndisasters depending on several factors like the location, type, severity, etc.\nVerification of the effectiveness of disaster summarization approaches still\nsuffer due to the lack of availability of good spectrum of datasets along with\nthe ground-truth summary. Existing approaches for ground-truth summary\ngeneration (ground-truth for extractive summarization) relies on the wisdom and\nintuition of the annotators. Annotators are provided with a complete set of\ninput tweets from which a subset of tweets is selected by the annotators for\nthe summary. This process requires immense human effort and significant time.\nAdditionally, this intuition-based selection of the tweets might lead to a high\nvariance in summaries generated across annotators. Therefore, to handle these\nchallenges, we propose a hybrid (semi-automated) approach (PORTRAIT) where we\npartly automate the ground-truth summary generation procedure. This approach\nreduces the effort and time of the annotators while ensuring the quality of the\ncreated ground-truth summary. We validate the effectiveness of PORTRAIT on 5\ndisaster events through quantitative and qualitative comparisons of\nground-truth summaries generated by existing intuitive approaches, a\nsemi-automated approach, and PORTRAIT. We prepare and release the ground-truth\nsummaries for 5 disaster events which consist of both natural and man-made\ndisaster events belonging to 4 different countries. Finally, we provide a study\nabout the performance of various state-of-the-art summarization approaches on\nthe ground-truth summaries generated by PORTRAIT using ROUGE-N F1-scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_P/0/1/0/all/0/1\">Piyush Kumar Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_R/0/1/0/all/0/1\">Roshni Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandapat_S/0/1/0/all/0/1\">Sourav Kumar Dandapat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Cross-Lingual Transfer for Chinese Stable Diffusion with Images as Pivots. (arXiv:2305.11540v1 [cs.CV])","link":"http://arxiv.org/abs/2305.11540","description":"<p>Diffusion models have made impressive progress in text-to-image synthesis.\nHowever, training such large-scale models (e.g. Stable Diffusion), from scratch\nrequires high computational costs and massive high-quality text-image pairs,\nwhich becomes unaffordable in other languages. To handle this challenge, we\npropose IAP, a simple but effective method to transfer English Stable Diffusion\ninto Chinese. IAP optimizes only a separate Chinese text encoder with all other\nparameters fixed to align Chinese semantics space to the English one in CLIP.\nTo achieve this, we innovatively treat images as pivots and minimize the\ndistance of attentive features produced from cross-attention between images and\neach language respectively. In this way, IAP establishes connections of\nChinese, English and visual semantics in CLIP's embedding space efficiently,\nadvancing the quality of the generated image with direct Chinese prompts.\nExperimental results show that our method outperforms several strong Chinese\ndiffusion models with only 5%~10% training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1\">Xiaoyuan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yutong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering. (arXiv:2305.11541v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11541","description":"<p>Large Language Model (LLM) has gained popularity and achieved remarkable\nresults in open-domain tasks, but its performance in real industrial\ndomain-specific scenarios is average since there is no specific knowledge in\nit. This issue has attracted widespread attention, but there are few relevant\nbenchmarks available. In this paper, we provide a benchmark Question Answering\n(QA) dataset named MSQA, which is about Microsoft products and IT technical\nproblems encountered by customers. This dataset contains industry\ncloud-specific QA knowledge, which is not available for general LLM, so it is\nwell suited for evaluating methods aimed at improving domain-specific\ncapabilities of LLM. In addition, we propose a new model interaction paradigm\nthat can empower LLM to achieve better performance on domain-specific tasks\nwhere it is not proficient. Extensive experiments demonstrate that the approach\nfollowing our model fusion framework outperforms the commonly used LLM with\nretrieval methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zezhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fangkai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1\">Mohit Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qingwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing Word-Context-Coupled Space Aligned with Associative Knowledge Relations for Interpretable Language Modeling. (arXiv:2305.11543v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11543","description":"<p>As the foundation of current natural language processing methods, pre-trained\nlanguage model has achieved excellent performance. However, the black-box\nstructure of the deep neural network in pre-trained language models seriously\nlimits the interpretability of the language modeling process. After revisiting\nthe coupled requirement of deep neural representation and semantics logic of\nlanguage modeling, a Word-Context-Coupled Space (W2CSpace) is proposed by\nintroducing the alignment processing between uninterpretable neural\nrepresentation and interpretable statistical logic. Moreover, a clustering\nprocess is also designed to connect the word- and context-level semantics.\nSpecifically, an associative knowledge network (AKN), considered interpretable\nstatistical logic, is introduced in the alignment process for word-level\nsemantics. Furthermore, the context-relative distance is employed as the\nsemantic feature for the downstream classifier, which is greatly different from\nthe current uninterpretable semantic representations of pre-trained models. Our\nexperiments for performance evaluation and interpretable analysis are executed\non several types of datasets, including SIGHAN, Weibo, and ChnSenti. Wherein a\nnovel evaluation strategy for the interpretability of machine learning models\nis first proposed. According to the experimental results, our language model\ncan achieve better performance and highly credible interpretable ability\ncompared to related state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fanyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhenping Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Viewing Knowledge Transfer in Multilingual Machine Translation Through a Representational Lens. (arXiv:2305.11550v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11550","description":"<p>We argue that translation quality alone is not a sufficient metric for\nmeasuring knowledge transfer in multilingual neural machine translation. To\nsupport this claim, we introduce Representational Transfer Potential (RTP),\nwhich measures representational similarities between languages. We show that\nRTP can measure both positive and negative transfer (interference), and find\nthat RTP is strongly correlated with changes in translation quality, indicating\nthat transfer does occur. Furthermore, we investigate data and language\ncharacteristics that are relevant for transfer, and find that multi-parallel\noverlap is an important yet under-explored feature. Based on this, we develop a\nnovel training scheme, which uses an auxiliary similarity loss that encourages\nrepresentations to be more invariant across languages by taking advantage of\nmulti-parallel data. We show that our method yields increased translation\nquality for low- and mid-resource languages across multiple data and model\nsetups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stap_D/0/1/0/all/0/1\">David Stap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niculae_V/0/1/0/all/0/1\">Vlad Niculae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monz_C/0/1/0/all/0/1\">Christof Monz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Scientific Abstract Segmentation with Normalized Mutual Information. (arXiv:2305.11553v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11553","description":"<p>The abstracts of scientific papers consist of premises and conclusions.\nStructured abstracts explicitly highlight the conclusion sentences, whereas\nnon-structured abstracts may have conclusion sentences at uncertain positions.\nThis implicit nature of conclusion positions makes the automatic segmentation\nof scientific abstracts into premises and conclusions a challenging task. In\nthis work, we empirically explore using Normalized Mutual Information (NMI) for\nabstract segmentation. We consider each abstract as a recurrent cycle of\nsentences and place segmentation boundaries by greedily optimizing the NMI\nscore between premises and conclusions. On non-structured abstracts, our\nproposed unsupervised approach GreedyCAS achieves the best performance across\nall evaluation metrics; on structured abstracts, GreedyCAS outperforms all\nbaseline methods measured by $P_k$. The strong correlation of NMI to our\nevaluation metrics reveals the effectiveness of NMI for abstract segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yingqiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_J/0/1/0/all/0/1\">Jessica Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1\">Nianlong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahnloser_R/0/1/0/all/0/1\">Richard H.R. Hahnloser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings. (arXiv:2305.11554v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11554","description":"<p>Augmenting large language models (LLMs) with external tools has emerged as a\npromising approach to solving complex problems. However, traditional methods,\nwhich finetune LLMs with tool demonstration data, can be both costly and\nrestricted to a predefined set of tools. Recent in-context learning paradigm\nalleviates these issues, but the limited context length only allows for a few\nshots of demonstrations, leading to suboptimal understandings of the tools.\nMoreover, when there are numerous tools to choose from, in-context learning\ncould completely fail to work. In this paper, we propose an alternative\napproach, $\\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our\napproach represents each $\\underline{tool}$ as a to$\\underline{ken}$\n($\\textit{toolken}$) and learns an embedding for it, enabling tool calls in the\nsame way as generating a regular word token. Once a toolken is triggered, the\nLLM is prompted to complete arguments for the tool to execute. ToolkenGPT\noffers the flexibility to plug in an arbitrary number of tools by expanding the\nset of toolkens on the fly. In addition, it improves tool use by allowing\nextensive demonstration data for learning the toolken embeddings. In diverse\ndomains, including numerical reasoning, knowledge-based question answering, and\nembodied plan generation, our approach effectively augments LLMs with tools and\nsubstantially outperforms various latest baselines. ToolkenGPT demonstrates the\npromising ability to use relevant tools from a large tool set in complex\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1\">Shibo Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blank-regularized CTC for Frame Skipping in Neural Transducer. (arXiv:2305.11558v1 [eess.AS])","link":"http://arxiv.org/abs/2305.11558","description":"<p>Neural Transducer and connectionist temporal classification (CTC) are popular\nend-to-end automatic speech recognition systems. Due to their frame-synchronous\ndesign, blank symbols are introduced to address the length mismatch between\nacoustic frames and output tokens, which might bring redundant computation.\nPrevious studies managed to accelerate the training and inference of neural\nTransducers by discarding frames based on the blank symbols predicted by a\nco-trained CTC. However, there is no guarantee that the co-trained CTC can\nmaximize the ratio of blank symbols. This paper proposes two novel\nregularization methods to explicitly encourage more blanks by constraining the\nself-loop of non-blank symbols in the CTC. It is interesting to find that the\nframe reduction ratio of the neural Transducer can approach the theoretical\nboundary. Experiments on LibriSpeech corpus show that our proposed method\naccelerates the inference of neural Transducer by 4 times without sacrificing\nperformance. Our work is open-sourced and publicly available\nhttps://github.com/k2-fsa/icefall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yifan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_L/0/1/0/all/0/1\">Liyong Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_Z/0/1/0/all/0/1\">Zengwei Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_W/0/1/0/all/0/1\">Wei Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuang_F/0/1/0/all/0/1\">Fangjun Kuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Long Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xie Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Povey_D/0/1/0/all/0/1\">Daniel Povey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decouple knowledge from paramters for plug-and-play language modeling. (arXiv:2305.11564v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11564","description":"<p>Pre-trained language models(PLM) have made impressive results in various NLP\ntasks. It has been revealed that one of the key factors to their success is the\nparameters of these models implicitly learn all kinds of knowledge during\npre-training. However, encoding knowledge implicitly in the model parameters\nhas two fundamental drawbacks. First, the knowledge is neither editable nor\nscalable once the model is trained, which is especially problematic in that\nknowledge is consistently evolving. Second, it lacks interpretability and\nprevents humans from understanding which knowledge PLM requires for a certain\nproblem. In this paper, we introduce PlugLM, a pre-training model with\ndifferentiable plug-in memory(DPM). The key intuition is to decouple the\nknowledge storage from model parameters with an editable and scalable key-value\nmemory and leverage knowledge in an explainable manner by knowledge retrieval\nin the DPM. To justify this design choice, we conduct evaluations in three\nsettings including: (1) domain adaptation. PlugLM obtains 3.95 F1 improvements\nacross four domains on average without any in-domain pre-training. (2)\nknowledge update. PlugLM could absorb new knowledge in a training-free way\nafter pre-training is done. (3) in-task knowledge learning. PlugLM could be\nfurther improved by incorporating training samples into DPM with knowledge\nprompting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-Universal Phonetic Representation in Multilingual Speech Pretraining for Low-Resource Speech Recognition. (arXiv:2305.11569v1 [eess.AS])","link":"http://arxiv.org/abs/2305.11569","description":"<p>We improve low-resource ASR by integrating the ideas of multilingual training\nand self-supervised learning. Concretely, we leverage an International Phonetic\nAlphabet (IPA) multilingual model to create frame-level pseudo labels for\nunlabeled speech, and use these pseudo labels to guide hidden-unit BERT\n(HuBERT) based speech pretraining in a phonetically-informed manner. The\nexperiments on the Multilingual Speech (MLS) Corpus show that the proposed\napproach consistently outperforms the standard HuBERT on all the target\nlanguages. Moreover, on 3 of the 4 languages, comparing to the standard HuBERT,\nthe approach performs better, meanwhile is able to save supervised training\ndata by 1.5k hours (75%) at most. Our approach outperforms most of the state of\nthe arts, with much less pretraining data in terms of hours and language\ndiversity. Compared to XLSR-53 and a retraining based multilingual method, our\napproach performs better with full and limited finetuning data scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Feng_S/0/1/0/all/0/1\">Siyuan Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tu_M/0/1/0/all/0/1\">Ming Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_R/0/1/0/all/0/1\">Rui Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1\">Chuanzeng Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-universal phonetic encoder for low-resource speech recognition. (arXiv:2305.11576v1 [eess.AS])","link":"http://arxiv.org/abs/2305.11576","description":"<p>Multilingual training is effective in improving low-resource ASR, which may\npartially be explained by phonetic representation sharing between languages. In\nend-to-end (E2E) ASR systems, graphemes are often used as basic modeling units,\nhowever graphemes may not be ideal for multilingual phonetic sharing. In this\npaper, we leverage International Phonetic Alphabet (IPA) based\nlanguage-universal phonetic model to improve low-resource ASR performances, for\nthe first time within the attention encoder-decoder architecture. We propose an\nadaptation method on the phonetic IPA model to further improve the proposed\napproach on extreme low-resource languages. Experiments carried out on the\nopen-source MLS corpus and our internal databases show our approach outperforms\nbaseline monolingual models and most state-of-the-art works. Our main approach\nand adaptation are effective on extremely low-resource languages, even within\ndomain- and language-mismatched scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Feng_S/0/1/0/all/0/1\">Siyuan Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tu_M/0/1/0/all/0/1\">Ming Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_R/0/1/0/all/0/1\">Rui Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1\">Chuanzeng Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech-Text Dialog Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment. (arXiv:2305.11579v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11579","description":"<p>Recently, speech-text pre-training methods have shown remarkable success in\nmany speech and natural language processing tasks. However, most previous\npre-trained models are usually tailored for one or two specific tasks, but fail\nto conquer a wide range of speech-text tasks. In addition, existing speech-text\npre-training methods fail to explore the contextual information within a\ndialogue to enrich utterance representations. In this paper, we propose\nSpeech-text dialog Pre-training for spoken dialog understanding with ExpliCiT\ncRoss-Modal Alignment (SPECTRA), which is the first-ever speech-text dialog\npre-training model. Concretely, to consider the temporality of speech modality,\nwe design a novel temporal position prediction task to capture the speech-text\nalignment. This pre-training task aims to predict the start and end time of\neach textual word in the corresponding speech waveform. In addition, to learn\nthe characteristics of spoken dialogs, we generalize a response selection task\nfrom textual dialog pre-training to speech-text dialog pre-training scenarios.\nExperimental results on four different downstream speech-text tasks demonstrate\nthe superiority of SPECTRA in learning speech-text alignment and multi-turn\ndialog context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianshu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Haoyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting-En Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuchuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wentao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IKDSumm: Incorporating Key-phrases into BERT for extractive Disaster Tweet Summarization. (arXiv:2305.11592v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11592","description":"<p>Online social media platforms, such as Twitter, are one of the most valuable\nsources of information during disaster events. Therefore, humanitarian\norganizations, government agencies, and volunteers rely on a summary of this\ninformation, i.e., tweets, for effective disaster management. Although there\nare several existing supervised and unsupervised approaches for automated tweet\nsummary approaches, these approaches either require extensive labeled\ninformation or do not incorporate specific domain knowledge of disasters.\nAdditionally, the most recent approaches to disaster summarization have\nproposed BERT-based models to enhance the summary quality. However, for further\nimproved performance, we introduce the utilization of domain-specific knowledge\nwithout any human efforts to understand the importance (salience) of a tweet\nwhich further aids in summary creation and improves summary quality. In this\npaper, we propose a disaster-specific tweet summarization framework, IKDSumm,\nwhich initially identifies the crucial and important information from each\ntweet related to a disaster through key-phrases of that tweet. We identify\nthese key-phrases by utilizing the domain knowledge (using existing ontology)\nof disasters without any human intervention. Further, we utilize these\nkey-phrases to automatically generate a summary of the tweets. Therefore, given\ntweets related to a disaster, IKDSumm ensures fulfillment of the summarization\nkey objectives, such as information coverage, relevance, and diversity in\nsummary without any human intervention. We evaluate the performance of IKDSumm\nwith 8 state-of-the-art techniques on 12 disaster datasets. The evaluation\nresults show that IKDSumm outperforms existing techniques by approximately\n2-79% in terms of ROUGE-N F1-score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_P/0/1/0/all/0/1\">Piyush Kumar Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_R/0/1/0/all/0/1\">Roshni Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Srishti Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandapat_S/0/1/0/all/0/1\">Sourav Kumar Dandapat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate. (arXiv:2305.11595v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11595","description":"<p>Large language models (LLMs) have demonstrated impressive zero-shot or\nfew-shot commonsense reasoning performance on various natural language\nprocessing (NLP) tasks. However, despite their strong commonsense reasoning\nabilities, LLMs still exhibit various kinds of inconsistency problems. While\nprevious researches mainly focused on the self-consistency within a single LLM,\nwe propose to explore the inter-consistency issue between two or more LLMs,\nwhich is critical for diverse and precise decision-making processes. Since the\nLLMs possess human-like intelligence after instruction tuning and reinforcement\nlearning with human feedback (RLHF), we design a formal debate framework to\ndelve into the inter-consistency problem among LLMs with three-stage debate:\nfair debate, mismatched debate, and roundtable debate. Through extensive\nexperiments on 7 commonsense reasoning datasets, LLMs not only become more\ninter-consistent by compromising and refuting but also achieve higher\nperformance and stronger interpretability. Furthermore, we find a much stronger\nLLM would be dominant in mismatched debates, while it will be easily misled by\nrelatively weaker LLMs in a more complex debate scenario such as roundtable\ndebate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_K/0/1/0/all/0/1\">Kai Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation. (arXiv:2305.11596v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11596","description":"<p>Modern NLP models are often trained over large untrusted datasets, raising\nthe potential for a malicious adversary to compromise model behaviour. For\ninstance, backdoors can be implanted through crafting training instances with a\nspecific textual trigger and a target label. This paper posits that backdoor\npoisoning attacks exhibit spurious correlation between simple text features and\nclassification labels, and accordingly, proposes methods for mitigating\nspurious correlation as means of defence. Our empirical study reveals that the\nmalicious triggers are highly correlated to their target labels; therefore such\ncorrelations are extremely distinguishable compared to those scores of benign\nfeatures, and can be used to filter out potentially problematic instances.\nCompared with several existing defences, our defence method significantly\nreduces attack success rates across backdoor attacks, and in the case of\ninsertion based attacks, our method provides a near-perfect defence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuanli He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiongkai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1\">Benjamin Rubinstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Introspective Tips: Large Language Model for In-Context Decision Making. (arXiv:2305.11598v1 [cs.AI])","link":"http://arxiv.org/abs/2305.11598","description":"<p>The emergence of large language models (LLMs) has substantially influenced\nnatural language processing, demonstrating exceptional results across various\ntasks. In this study, we employ ``Introspective Tips\" to facilitate LLMs in\nself-optimizing their decision-making. By introspectively examining\ntrajectories, LLM refines its policy by generating succinct and valuable tips.\nOur method enhances the agent's performance in both few-shot and zero-shot\nlearning situations by considering three essential scenarios: learning from the\nagent's past experiences, integrating expert demonstrations, and generalizing\nacross diverse games. Importantly, we accomplish these improvements without\nfine-tuning the LLM parameters; rather, we adjust the prompt to generalize\ninsights from the three aforementioned situations. Our framework not only\nsupports but also emphasizes the advantage of employing LLM in in-contxt\ndecision-making. Experiments involving over 100 games in TextWorld illustrate\nthe superior performance of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yali Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fangkai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1\">Si Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajmohan_S/0/1/0/all/0/1\">Saravan Rajmohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qingwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attributable and Scalable Opinion Summarization. (arXiv:2305.11603v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11603","description":"<p>We propose a method for unsupervised opinion summarization that encodes\nsentences from customer reviews into a hierarchical discrete latent space, then\nidentifies common opinions based on the frequency of their encodings. We are\nable to generate both abstractive summaries by decoding these frequent\nencodings, and extractive summaries by selecting the sentences assigned to the\nsame frequent encodings. Our method is attributable, because the model\nidentifies sentences used to generate the summary as part of the summarization\nprocess. It scales easily to many hundreds of input reviews, because\naggregation is performed in the latent space rather than over long sequences of\ntokens. We also demonstrate that our appraoch enables a degree of control,\ngenerating aspect-specific summaries by restricting the model to parts of the\nencoding space that correspond to desired aspects (e.g., location or food).\nAutomatic and human evaluation on two datasets from different domains\ndemonstrates that our method generates summaries that are more informative than\nprior work and better grounded in the input reviews.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hosking_T/0/1/0/all/0/1\">Tom Hosking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching by Code: a New SearchBySnippet Dataset and SnippeR Retrieval Model for Searching by Code Snippets. (arXiv:2305.11625v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11625","description":"<p>Code search is an important task that has seen many developments in recent\nyears. However, previous attempts have mostly considered the problem of\nsearching for code by a text query. We argue that using a code snippet (and\npossibly an associated traceback) as a query and looking for answers with\nbugfixing instructions and code samples is a natural use case that is not\ncovered by existing approaches. Moreover, existing datasets use comments\nextracted from code rather than full-text descriptions as text, making them\nunsuitable for this use case. We present a new SearchBySnippet dataset\nimplementing the search-by-code use case based on StackOverflow data; it turns\nout that in this setting, existing architectures fall short of the simplest\nBM25 baseline even after fine-tuning. We present a new single encoder model\nSnippeR that outperforms several strong baselines on the SearchBySnippet\ndataset with a result of 0.451 Recall@10; we propose the SearchBySnippet\ndataset and SnippeR as a new important benchmark for code search evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sedykh_I/0/1/0/all/0/1\">Ivan Sedykh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abulkhanov_D/0/1/0/all/0/1\">Dmitry Abulkhanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorokin_N/0/1/0/all/0/1\">Nikita Sorokin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolenko_S/0/1/0/all/0/1\">Sergey Nikolenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malykh_V/0/1/0/all/0/1\">Valentin Malykh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CCT-Code: Cross-Consistency Training for Multilingual Clone Detection and Code Search. (arXiv:2305.11626v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11626","description":"<p>We consider the clone detection and information retrieval problems for source\ncode, well-known tasks important for any programming language. Although it is\nalso an important and interesting problem to find code snippets that operate\nidentically but are written in different programming languages, to the best of\nour knowledge multilingual clone detection has not been studied in literature.\nIn this work, we formulate the multilingual clone detection problem and present\nXCD, a new benchmark dataset produced from the CodeForces submissions dataset.\nMoreover, we present a novel training procedure, called cross-consistency\ntraining (CCT), that we apply to train language models on source code in\ndifferent programming languages. The resulting CCT-LM model, initialized with\nGraphCodeBERT and fine-tuned with CCT, achieves new state of the art,\noutperforming existing approaches on the POJ-104 clone detection benchmark with\n95.67\\% MAP and AdvTest code search benchmark with 47.18\\% MRR; it also shows\nthe best results on the newly created multilingual clone detection benchmark\nXCD across all programming languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sorokin_N/0/1/0/all/0/1\">Nikita Sorokin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abulkhanov_D/0/1/0/all/0/1\">Dmitry Abulkhanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolenko_S/0/1/0/all/0/1\">Sergey Nikolenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malykh_V/0/1/0/all/0/1\">Valentin Malykh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM-Pruner: On the Structural Pruning of Large Language Models. (arXiv:2305.11627v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11627","description":"<p>Large language models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. However, such impressive capability typically\ncomes with a substantial model size, which presents significant challenges in\nboth the deployment, inference, and training stages. With LLM being a\ngeneral-purpose task solver, we explore its compression in a task-agnostic\nmanner, which aims to preserve the multi-task solving and language generation\nability of the original LLM. One challenge to achieving this is the enormous\nsize of the training corpus of LLM, which makes both data transfer and model\npost-training over-burdensome. Thus, we tackle the compression of LLMs within\nthe bound of two constraints: being task-agnostic and minimizing the reliance\non the original training dataset. Our method, named LLM-Pruner, adopts\nstructural pruning that selectively removes non-critical coupled structures\nbased on gradient information, maximally preserving the majority of the LLM's\nfunctionality. To this end, the performance of pruned models can be efficiently\nrecovered through tuning techniques, LoRA, in merely 3 hours, requiring only\n50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna,\nand ChatGLM, and demonstrate that the compressed models still exhibit\nsatisfactory capabilities in zero-shot classification and generation. The code\nis available at: https://github.com/horseee/LLM-Pruner\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinyin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_G/0/1/0/all/0/1\">Gongfan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating task understanding through multilingual consistency: A ChatGPT case study. (arXiv:2305.11662v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11662","description":"<p>At the staggering pace with which the capabilities of large language models\n(LLMs) are increasing, creating future-proof evaluation sets to assess their\nunderstanding becomes more and more challenging. In this paper, we propose a\nnovel paradigm for evaluating LLMs which leverages the idea that correct world\nunderstanding should be consistent across different (Fregean) senses of the\nsame meaning. Accordingly, we measure understanding not in terms of correctness\nbut by evaluating consistency across multiple senses that are generated by the\nmodel itself. We showcase our approach by instantiating a test where the\ndifferent senses are different languages, hence using multilingual\nself-consistency as a litmus test for the model's understanding and\nsimultaneously addressing the important topic of multilingualism. Taking one of\nthe latest versions of ChatGPT as our object of study, we evaluate multilingual\nconsistency for two different tasks across three different languages. We show\nthat its multilingual consistency is still lacking, and that its task and world\nunderstanding are thus not language-independent. As our approach does not\nrequire any static evaluation corpora in languages other than English, it can\neasily and cheaply be extended to different languages and tasks and could\nbecome an integral part of future benchmarking efforts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ohmer_X/0/1/0/all/0/1\">Xenia Ohmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruni_E/0/1/0/all/0/1\">Elia Bruni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1\">Dieuwke Hupkes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Algorithmic failure as a humanities methodology: machine learning's mispredictions identify rich cases for qualitative analysis. (arXiv:2305.11663v1 [cs.LG])","link":"http://arxiv.org/abs/2305.11663","description":"<p>This commentary tests a methodology proposed by Munk et al. (2022) for using\nfailed predictions in machine learning as a method to identify ambiguous and\nrich cases for qualitative analysis. Using a dataset describing actions\nperformed by fictional characters interacting with machine vision technologies\nin 500 artworks, movies, novels and videogames, I trained a simple machine\nlearning algorithm (using the kNN algorithm in R) to predict whether or not an\naction was active or passive using only information about the fictional\ncharacters. Predictable actions were generally unemotional and unambiguous\nactivities where machine vision technologies were treated as simple tools.\nUnpredictable actions, that is, actions that the algorithm could not correctly\npredict, were more ambivalent and emotionally loaded, with more complex power\nrelationships between characters and technologies. The results thus support\nMunk et al.'s theory that failed predictions can be productively used to\nidentify rich cases for qualitative analysis. This test goes beyond simply\nreplicating Munk et al.'s results by demonstrating that the method can be\napplied to a broader humanities domain, and that it does not require complex\nneural networks but can also work with a simpler machine learning algorithm.\nFurther research is needed to develop an understanding of what kinds of data\nthe method is useful for and which kinds of machine learning are most\ngenerative. To support this, the R code required to produce the results is\nincluded so the test can be replicated. The code can also be reused or adapted\nto test the method on other datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rettberg_J/0/1/0/all/0/1\">Jill Walker Rettberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bias Beyond English: Counterfactual Tests for Bias in Sentiment Analysis in Four Languages. (arXiv:2305.11673v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11673","description":"<p>Sentiment analysis (SA) systems are used in many products and hundreds of\nlanguages. Gender and racial biases are well-studied in English SA systems, but\nunderstudied in other languages, with few resources for such studies. To remedy\nthis, we build a counterfactual evaluation corpus for gender and racial/migrant\nbias in four languages. We demonstrate its usefulness by answering a simple but\nimportant question that an engineer might need to answer when deploying a\nsystem: What biases do systems import from pre-trained models when compared to\na baseline with no pre-training? Our evaluation corpus, by virtue of being\ncounterfactual, not only reveals which models have less bias, but also\npinpoints changes in model bias behaviour, which enables more targeted\nmitigation strategies. We release our code and evaluation corpora to facilitate\nfuture research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goldfarb_Tarrant_S/0/1/0/all/0/1\">Seraphina Goldfarb-Tarrant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1\">Adam Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanco_R/0/1/0/all/0/1\">Roi Blanco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcheggiani_D/0/1/0/all/0/1\">Diego Marcheggiani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sensing of inspiration events from speech: comparison of deep learning and linguistic methods. (arXiv:2305.11683v1 [cs.SD])","link":"http://arxiv.org/abs/2305.11683","description":"<p>Respiratory chest belt sensor can be used to measure the respiratory rate and\nother respiratory health parameters. Virtual Respiratory Belt, VRB, algorithms\nestimate the belt sensor waveform from speech audio. In this paper we compare\nthe detection of inspiration events (IE) from respiratory belt sensor data\nusing a novel neural VRB algorithm and the detections based on time-aligned\nlinguistic content. The results show the superiority of the VRB method over\nword pause detection or grammatical content segmentation. The comparison of the\nmethods show that both read and spontaneous speech content has a significant\namount of ungrammatical breathing, that is, breathing events that are not\naligned with grammatically appropriate places in language. This study gives new\ninsights into the development of VRB methods and adds to the general\nunderstanding of speech breathing behavior. Moreover, a new VRB method, VRBOLA,\nfor the reconstruction of the continuous breathing waveform is demonstrated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harma_A/0/1/0/all/0/1\">Aki H&#xe4;rm&#xe4;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grossekathofer_U/0/1/0/all/0/1\">Ulf Grossekath&#xf6;fer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouweltjes_O/0/1/0/all/0/1\">Okke Ouweltjes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nallanthighal_V/0/1/0/all/0/1\">Venkata Srikanth Nallanthighal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation. (arXiv:2305.11685v1 [eess.AS])","link":"http://arxiv.org/abs/2305.11685","description":"<p>Transformer-based speech self-supervised learning (SSL) models, such as\nHuBERT, show surprising performance in various speech processing tasks.\nHowever, huge number of parameters in speech SSL models necessitate the\ncompression to a more compact model for wider usage in academia or small\ncompanies. In this study, we suggest to reuse attention maps across the\nTransformer layers, so as to remove key and query parameters while retaining\nthe number of layers. Furthermore, we propose a novel masking distillation\nstrategy to improve the student model's speech representation quality. We\nextend the distillation loss to utilize both masked and unmasked speech frames\nto fully leverage the teacher model's high-quality representation. Our\nuniversal compression strategy yields the student model that achieves phoneme\nerror rate (PER) of 7.72% and word error rate (WER) of 9.96% on the SUPERB\nbenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jang_K/0/1/0/all/0/1\">Kangwook Jang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Sungnyun Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yun_S/0/1/0/all/0/1\">Se-Young Yun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1\">Hoirin Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surgical-VQLA: Transformer with Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery. (arXiv:2305.11692v1 [cs.CV])","link":"http://arxiv.org/abs/2305.11692","description":"<p>Despite the availability of computer-aided simulators and recorded videos of\nsurgical procedures, junior residents still heavily rely on experts to answer\ntheir queries. However, expert surgeons are often overloaded with clinical and\nacademic workloads and limit their time in answering. For this purpose, we\ndevelop a surgical question-answering system to facilitate robot-assisted\nsurgical scene and activity understanding from recorded videos. Most of the\nexisting VQA methods require an object detector and regions based feature\nextractor to extract visual features and fuse them with the embedded text of\nthe question for answer generation. However, (1) surgical object detection\nmodel is scarce due to smaller datasets and lack of bounding box annotation;\n(2) current fusion strategy of heterogeneous modalities like text and image is\nnaive; (3) the localized answering is missing, which is crucial in complex\nsurgical scenarios. In this paper, we propose Visual Question\nLocalized-Answering in Robotic Surgery (Surgical-VQLA) to localize the specific\nsurgical area during the answer prediction. To deal with the fusion of the\nheterogeneous modalities, we design gated vision-language embedding (GVLE) to\nbuild input patches for the Language Vision Transformer (LViT) to predict the\nanswer. To get localization, we add the detection head in parallel with the\nprediction head of the LViT. We also integrate GIoU loss to boost localization\nperformance by preserving the accuracy of the question-answering model. We\nannotate two datasets of VQLA by utilizing publicly available surgical videos\nfrom MICCAI challenges EndoVis-17 and 18. Our validation results suggest that\nSurgical-VQLA can better understand the surgical scene and localize the\nspecific area related to the question-answering. GVLE presents an efficient\nlanguage-vision embedding technique by showing superior performance over the\nexisting benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Long Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mobarakol Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seenivasan_L/0/1/0/all/0/1\">Lalithkumar Seenivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongliang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations. (arXiv:2305.11694v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11694","description":"<p>Formulating selective information needs results in queries that implicitly\nspecify set operations, such as intersection, union, and difference. For\ninstance, one might search for \"shorebirds that are not sandpipers\" or\n\"science-fiction films shot in England\". To study the ability of retrieval\nsystems to meet such information needs, we construct QUEST, a dataset of 3357\nnatural language queries with implicit set operations, that map to a set of\nentities corresponding to Wikipedia documents. The dataset challenges models to\nmatch multiple constraints mentioned in queries with corresponding evidence in\ndocuments and correctly perform various set operations. The dataset is\nconstructed semi-automatically using Wikipedia category names. Queries are\nautomatically composed from individual categories, then paraphrased and further\nvalidated for naturalness and fluency by crowdworkers. Crowdworkers also assess\nthe relevance of entities based on their documents and highlight attribution of\nquery constraints to spans of document text. We analyze several modern\nretrieval systems, finding that they often struggle on such queries. Queries\ninvolving negation and conjunction are particularly challenging and systems are\nfurther challenged with combinations of these operations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malaviya_C/0/1/0/all/0/1\">Chaitanya Malaviya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_P/0/1/0/all/0/1\">Peter Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kenton Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1\">Kristina Toutanova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Comes Next? Evaluating Uncertainty in Neural Text Generators Against Human Production Variability. (arXiv:2305.11707v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11707","description":"<p>In Natural Language Generation (NLG) tasks, for any input, multiple\ncommunicative goals are plausible, and any goal can be put into words, or\nproduced, in multiple ways. We characterise the extent to which human\nproduction varies lexically, syntactically, and semantically across four NLG\ntasks, connecting human production variability to aleatoric or data\nuncertainty. We then inspect the space of output strings shaped by a generation\nsystem's predicted probability distribution and decoding algorithm to probe its\nuncertainty. For each test input, we measure the generator's calibration to\nhuman production variability. Following this instance-level approach, we\nanalyse NLG models and decoding strategies, demonstrating that probing a\ngenerator with multiple samples and, when possible, multiple references,\nprovides the level of detail necessary to gain understanding of a model's\nrepresentation of uncertainty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giulianelli_M/0/1/0/all/0/1\">Mario Giulianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baan_J/0/1/0/all/0/1\">Joris Baan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1\">Wilker Aziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_R/0/1/0/all/0/1\">Raquel Fern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information Screening whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling. (arXiv:2305.11719v1 [cs.CV])","link":"http://arxiv.org/abs/2305.11719","description":"<p>Existing research on multimodal relation extraction (MRE) faces two\nco-existing challenges, internal-information over-utilization and\nexternal-information under-exploitation. To combat that, we propose a novel\nframework that simultaneously implements the idea of internal-information\nscreening and external-information exploiting. First, we represent the\nfine-grained semantic structures of the input image and text with the visual\nand textual scene graphs, which are further fused into a unified cross-modal\ngraph (CMG). Based on CMG, we perform structure refinement with the guidance of\nthe graph information bottleneck principle, actively denoising the\nless-informative features. Next, we perform topic modeling over the input image\nand text, incorporating latent multimodal topic features to enrich the\ncontexts. On the benchmark MRE dataset, our system outperforms the current best\nmodel significantly. With further in-depth analyses, we reveal the great\npotential of our method for the MRE task. Our codes are open at\nhttps://github.com/ChocoWu/MRE-ISE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shengqiong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hao Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S$^3$HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering. (arXiv:2305.11725v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11725","description":"<p>Answering multi-hop questions over hybrid factual knowledge from the given\ntext and table (TextTableQA) is a challenging task. Existing models mainly\nadopt a retriever-reader framework, which have several deficiencies, such as\nnoisy labeling in training retriever, insufficient utilization of heterogeneous\ninformation over text and table, and deficient ability for different reasoning\noperations. In this paper, we propose a three-stage TextTableQA framework\nS3HQA, which comprises of retriever, selector, and reasoner. We use a retriever\nwith refinement training to solve the noisy labeling problem. Then, a hybrid\nselector considers the linked relationships between heterogeneous data to\nselect the most relevant factual knowledge. For the final stage, instead of\nadapting a reading comprehension module like in previous methods, we employ a\ngeneration-based reasoner to obtain answers. This includes two approaches: a\nrow-wise generator and an LLM prompting generator~(first time used in this\ntask). The experimental results demonstrate that our method achieves\ncompetitive results in the few-shot setting. When trained on the full dataset,\nour approach outperforms all baseline methods, ranking first on the HybridQA\nleaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_F/0/1/0/all/0/1\">Fangyu Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yifan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shizhu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yiming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Persian Typographical Error Type Detection using Many-to-Many Deep Neural Networks on Algorithmically-Generated Misspellings. (arXiv:2305.11731v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11731","description":"<p>Digital technologies have led to an influx of text created daily in a variety\nof languages, styles, and formats. A great deal of the popularity of\nspell-checking systems can be attributed to this phenomenon since they are\ncrucial to polishing the digitally conceived text. In this study, we tackle\nTypographical Error Type Detection in Persian, which has been relatively\nunderstudied. In this paper, we present a public dataset named FarsTypo,\ncontaining 3.4 million chronologically ordered and part-of-speech tagged words\nof diverse topics and linguistic styles. An algorithm for applying\nPersian-specific errors is developed and applied to a scalable size of these\nwords, forming a parallel dataset of correct and incorrect words. Using\nFarsTypo, we establish a firm baseline and compare different methodologies\nusing various architectures. In addition, we present a novel Many-to-Many Deep\nSequential Neural Network to perform token classification using both word and\ncharacter embeddings in combination with bidirectional LSTM layers to detect\ntypographical errors across 51 classes. We compare our approach with\nhighly-advanced industrial systems that, unlike this study, have been developed\nutilizing a variety of resources. The results of our final method were\ncompetitive in that we achieved an accuracy of 97.62%, a precision of 98.83%, a\nrecall of 98.61%, and outperformed the rest in terms of speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mohammad Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faili_H/0/1/0/all/0/1\">Heshaam Faili</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing. (arXiv:2305.11738v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11738","description":"<p>Recent developments in large language models (LLMs) have been impressive.\nHowever, these models sometimes show inconsistencies and problematic behavior,\nsuch as hallucinating facts, generating flawed code, or creating offensive and\ntoxic content. Unlike these models, humans typically utilize external tools to\ncross-check and refine their initial content, like using a search engine for\nfact-checking, or a code interpreter for debugging. Inspired by this\nobservation, we introduce a framework called CRITIC that allows LLMs, which are\nessentially \"black boxes\" to validate and progressively amend their own outputs\nin a manner similar to human interaction with tools. More specifically,\nstarting with an initial output, CRITIC interacts with appropriate tools to\nevaluate certain aspects of the text, and then revises the output based on the\nfeedback obtained during this validation process. Comprehensive evaluations\ninvolving free-form question answering, mathematical program synthesis, and\ntoxicity reduction demonstrate that CRITIC consistently enhances the\nperformance of LLMs. Meanwhile, our research highlights the crucial importance\nof external feedback in promoting the ongoing self-improvement of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gou_Z/0/1/0/all/0/1\">Zhibin Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhihong Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inference-time Re-ranker Relevance Feedback for Neural Information Retrieval. (arXiv:2305.11744v1 [cs.IR])","link":"http://arxiv.org/abs/2305.11744","description":"<p>Neural information retrieval often adopts a retrieve-and-rerank framework: a\nbi-encoder network first retrieves K (e.g., 100) candidates that are then\nre-ranked using a more powerful cross-encoder model to rank the better\ncandidates higher. The re-ranker generally produces better candidate scores\nthan the retriever, but is limited to seeing only the top K retrieved\ncandidates, thus providing no improvements in retrieval performance as measured\nby Recall@K. In this work, we leverage the re-ranker to also improve retrieval\nby providing inference-time relevance feedback to the retriever. Concretely, we\nupdate the retriever's query representation for a test instance using a\nlightweight inference-time distillation of the re-ranker's prediction for that\ninstance. The distillation loss is designed to bring the retriever's candidate\nscores closer to those of the re-ranker. A second retrieval step is then\nperformed with the updated query vector. We empirically show that our approach,\nwhich can serve arbitrary retrieve-and-rerank pipelines, significantly improves\nretrieval recall in multiple domains, languages, and modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1\">Revanth Gangi Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasigi_P/0/1/0/all/0/1\">Pradeep Dasigi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sultan_M/0/1/0/all/0/1\">Md Arafat Sultan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1\">Avirup Sil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HalOmi: A Manually Annotated Benchmark for Multilingual Hallucination and Omission Detection in Machine Translation. (arXiv:2305.11746v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11746","description":"<p>Hallucinations in machine translation are translations that contain\ninformation completely unrelated to the input. Omissions are translations that\ndo not include some of the input information. While both cases tend to be\ncatastrophic errors undermining user trust, annotated data with these types of\npathologies is extremely scarce and is limited to a few high-resource\nlanguages. In this work, we release an annotated dataset for the hallucination\nand omission phenomena covering 18 translation directions with varying resource\nlevels and scripts. Our annotation covers different levels of partial and full\nhallucinations as well as omissions both at the sentence and at the word level.\nAdditionally, we revisit previous methods for hallucination and omission\ndetection, show that conclusions made based on a single language pair largely\ndo not hold for a large-scale evaluation, and establish new solid baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dale_D/0/1/0/all/0/1\">David Dale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voita_E/0/1/0/all/0/1\">Elena Voita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_J/0/1/0/all/0/1\">Janice Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansanti_P/0/1/0/all/0/1\">Prangthip Hansanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ropers_C/0/1/0/all/0/1\">Christophe Ropers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalbassi_E/0/1/0/all/0/1\">Elahe Kalbassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Cynthia Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrault_L/0/1/0/all/0/1\">Lo&#xef;c Barrault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models. (arXiv:2305.11747v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11747","description":"<p>Large language models (LLMs), such as ChatGPT, are prone to generate\nhallucinations, \\ie content that conflicts with the source or cannot be\nverified by the factual knowledge. To understand what types of content and to\nwhich extent LLMs are apt to hallucinate, we introduce the Hallucination\nEvaluation for Large Language Models (HELMA) benchmark, a large collection of\ngenerated and human-annotated hallucinated samples for evaluating the\nperformance of LLMs in recognizing and alleviating hallucination. To generate\nthese samples, we propose a ChatGPT-based two-step framework, \\ie\nsampling-then-filtering. Specifically, we first adopt two different sampling\nmethods to generate hallucinated samples based on instructions, and then use an\nexample-enhanced filtering method to select the best one. Furthermore, we also\nhire some human labelers to annotate the hallucinations in ChatGPT responses.\nThe empirical results suggest that ChatGPT has some probabilities to generate\nhallucinations and existing LLMs face great challenges in recognizing the\nhallucinations in text. In addition, the performance can be improved by\nproviding external knowledge or adding reasoning steps. Our benchmark can be\naccessed at https://github.com/RUCAIBox/HELMA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiaoxue Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning. (arXiv:2305.11759v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11759","description":"<p>Large Language Models (LLMs) are known to memorize significant portions of\ntheir training data. Parts of this memorized content have been shown to be\nextractable by simply querying the model, which poses a privacy risk. We\npresent a novel approach which uses prompt-tuning to control the extraction\nrates of memorized content in LLMs. We present two prompt training strategies\nto increase and decrease extraction rates, which correspond to an attack and a\ndefense, respectively. We demonstrate the effectiveness of our techniques by\nusing models from the GPT-Neo family on a public benchmark. For the 1.3B\nparameter GPT-Neo model, our attack yields a 9.3 percentage point increase in\nextraction rate compared to our baseline. Our defense can be tuned to achieve\ndifferent privacy-utility trade-offs by a user-specified hyperparameter. We\nachieve an extraction rate reduction of up to 97.7% relative to our baseline,\nwith a perplexity increase of 16.9%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ozdayi_M/0/1/0/all/0/1\">Mustafa Safa Ozdayi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peris_C/0/1/0/all/0/1\">Charith Peris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+FitzGerald_J/0/1/0/all/0/1\">Jack FitzGerald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupuy_C/0/1/0/all/0/1\">Christophe Dupuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majmudar_J/0/1/0/all/0/1\">Jimit Majmudar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_H/0/1/0/all/0/1\">Haidar Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_R/0/1/0/all/0/1\">Rahil Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReSeTOX: Re-learning attention weights for toxicity mitigation in machine translation. (arXiv:2305.11761v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11761","description":"<p>Our proposed method, ReSeTOX (REdo SEarch if TOXic), addresses the issue of\nNeural Machine Translation (NMT) generating translation outputs that contain\ntoxic words not present in the input. The objective is to mitigate the\nintroduction of toxic language without the need for re-training. In the case of\nidentified added toxicity during the inference process, ReSeTOX dynamically\nadjusts the key-value self-attention weights and re-evaluates the beam search\nhypotheses. Experimental results demonstrate that ReSeTOX achieves a remarkable\n57% reduction in added toxicity while maintaining an average translation\nquality of 99.5% across 164 languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gilabert_J/0/1/0/all/0/1\">Javier Garc&#xed;a Gilabert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escolano_C/0/1/0/all/0/1\">Carlos Escolano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_Jussa_M/0/1/0/all/0/1\">Marta R. Costa-Juss&#xe0;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Visual Spatial Description via Holistic 3D Scene Understanding. (arXiv:2305.11768v1 [cs.CV])","link":"http://arxiv.org/abs/2305.11768","description":"<p>Visual spatial description (VSD) aims to generate texts that describe the\nspatial relations of the given objects within images. Existing VSD work merely\nmodels the 2D geometrical vision features, thus inevitably falling prey to the\nproblem of skewed spatial understanding of target objects. In this work, we\ninvestigate the incorporation of 3D scene features for VSD. With an external 3D\nscene extractor, we obtain the 3D objects and scene features for input images,\nbased on which we construct a target object-centered 3D spatial scene graph\n(Go3D-S2G), such that we model the spatial semantics of target objects within\nthe holistic 3D scenes. Besides, we propose a scene subgraph selecting\nmechanism, sampling topologically-diverse subgraphs from Go3D-S2G, where the\ndiverse local structure features are navigated to yield spatially-diversified\ntext generation. Experimental results on two VSD datasets demonstrate that our\nframework outperforms the baselines significantly, especially improving on the\ncases with complex visual spatial relations. Meanwhile, our method can produce\nmore spatially-diversified generation. Code is available at\nhttps://github.com/zhaoyucs/VSD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hao Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jianguo Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Vision-Language Pre-Training with Jointly Learned Questioner and Dense Captioner. (arXiv:2305.11769v1 [cs.CV])","link":"http://arxiv.org/abs/2305.11769","description":"<p>Large pre-trained multimodal models have demonstrated significant success in\na range of downstream tasks, including image captioning, image-text retrieval,\nvisual question answering (VQA), etc. However, many of these methods rely on\nimage-text pairs collected from the web as pre-training data and unfortunately\noverlook the need for fine-grained feature alignment between vision and\nlanguage modalities, which requires detailed understanding of images and\nlanguage expressions. While integrating VQA and dense captioning (DC) into\npre-training can address this issue, acquiring image-question-answer as well as\nimage-location-caption triplets is challenging and time-consuming.\nAdditionally, publicly available datasets for VQA and dense captioning are\ntypically limited in scale due to manual data collection and labeling efforts.\nIn this paper, we propose a novel method called Joint QA and DC GEneration\n(JADE), which utilizes a pre-trained multimodal model and easily-crawled\nimage-text pairs to automatically generate and filter large-scale VQA and dense\ncaptioning datasets. We apply this method to the Conceptual Caption (CC3M)\ndataset to generate a new dataset called CC3M-QA-DC. Experiments show that when\nused for pre-training in a multi-task manner, CC3M-QA-DC can improve the\nperformance with various backbones on various downstream tasks. Furthermore,\nour generated CC3M-QA-DC can be combined with larger image-text datasets (e.g.,\nCC15M) and achieve competitive results compared with models using much more\ndata. Code and dataset will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zikang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sihan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Longteng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Handong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xingjian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual Supervision improves Large Language Models Pre-training. (arXiv:2305.11778v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11778","description":"<p>The recent rapid progress in pre-training Large Language Models has relied on\nusing self-supervised language modeling objectives like next token prediction\nor span corruption. On the other hand, Machine Translation Systems are mostly\ntrained using cross-lingual supervision that requires aligned data between\nsource and target languages. We demonstrate that pre-training Large Language\nModels on a mixture of a self-supervised Language Modeling objective and the\nsupervised Machine Translation objective, therefore including cross-lingual\nparallel data during pre-training, yields models with better in-context\nlearning abilities. As pre-training is a very resource-intensive process and a\ngrid search on the best mixing ratio between the two objectives is\nprohibitively expensive, we propose a simple yet effective strategy to learn it\nduring pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schioppa_A/0/1/0/all/0/1\">Andrea Schioppa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DMDD: A Large-Scale Dataset for Dataset Mentions Detection. (arXiv:2305.11779v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11779","description":"<p>The recognition of dataset names is a critical task for automatic information\nextraction in scientific literature, enabling researchers to understand and\nidentify research opportunities. However, existing corpora for dataset mention\ndetection are limited in size and naming diversity. In this paper, we introduce\nthe Dataset Mentions Detection Dataset (DMDD), the largest publicly available\ncorpus for this task. DMDD consists of the DMDD main corpus, comprising 31,219\nscientific articles with over 449,000 dataset mentions weakly annotated in the\nformat of in-text spans, and an evaluation set, which comprises of 450\nscientific articles manually annotated for evaluation purposes. We use DMDD to\nestablish baseline performance for dataset mention detection and linking. By\nanalyzing the performance of various models on DMDD, we are able to identify\nopen problems in dataset mention detection. We invite the community to use our\ndataset as a challenge to develop novel dataset mention detection models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Huitong Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragut_E/0/1/0/all/0/1\">Eduard Dragut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Latecki_L/0/1/0/all/0/1\">Longin Jan Latecki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach. (arXiv:2305.11789v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11789","description":"<p>Humans work together to solve common problems by having discussions,\nexplaining, and agreeing or disagreeing with each other. Similarly, if a system\ncan have discussions with humans when solving tasks, it can improve the\nsystem's performance and reliability. In previous research on explainability,\nit has only been possible for the system to make predictions and for humans to\nask questions about them rather than having a mutual exchange of opinions. This\nresearch aims to create a dataset and computational framework for systems that\ndiscuss and refine their predictions through dialogue. Through experiments, we\nshow that the proposed system can have beneficial discussions with humans\nimproving the accuracy by up to 25 points in the natural language inference\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_M/0/1/0/all/0/1\">Masahiro Kaneko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting with Pseudo-Code Instructions. (arXiv:2305.11790v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11790","description":"<p>Prompting with natural language instructions has recently emerged as a\npopular method of harnessing the capabilities of large language models. Given\nthe inherent ambiguity present in natural language, it is intuitive to consider\nthe possible advantages of prompting with less ambiguous prompt styles, such as\nthe use of pseudo-code.\n</p>\n<p>In this paper we explore if prompting via pseudo-code instructions helps\nimprove the performance of pre-trained language models. We manually create a\ndataset of pseudo-code prompts for 132 different tasks spanning classification,\nQA and generative language tasks, sourced from the Super-NaturalInstructions\ndataset. Using these prompts along with their counterparts in natural language,\nwe study their performance on two LLM families - BLOOM and CodeGen. Our\nexperiments show that using pseudo-code instructions leads to better results,\nwith an average increase (absolute) of 7-16 points in F1 scores for\nclassification tasks and an improvement (relative) of 12-38% in aggregate ROUGE\nscores across all tasks. We include detailed ablation studies which indicate\nthat code comments, docstrings, and the structural clues encoded in pseudo-code\nall contribute towards the improvement in performance.\n</p>\n<p>To the best of our knowledge our work is the first to demonstrate how\npseudo-code prompts can be helpful in improving the performance of pre-trained\nLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_M/0/1/0/all/0/1\">Mayank Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Prince Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1\">Riyaz Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V_R/0/1/0/all/0/1\">Rudra Murthy V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Contractor_D/0/1/0/all/0/1\">Danish Contractor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamilselvam_S/0/1/0/all/0/1\">Srikanth Tamilselvam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Few-shot NER with Prompt Ordering based Data Augmentation. (arXiv:2305.11791v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11791","description":"<p>Recently, data augmentation (DA) methods have been proven to be effective for\npre-trained language models (PLMs) in low-resource settings, including few-shot\nnamed entity recognition (NER). However, conventional NER DA methods are mostly\naimed at sequence labeling models, i.e., token-level classification, and few\nare compatible with unified autoregressive generation frameworks, which can\nhandle a wider range of NER tasks, such as nested NER. Furthermore, these\ngeneration frameworks have a strong assumption that the entities will appear in\nthe target sequence with the same left-to-right order as the source sequence.\nIn this paper, we claim that there is no need to keep this strict order, and\nmore diversified but reasonable target entity sequences can be provided during\nthe training stage as a novel DA method. Nevertheless, a naive mixture of\naugmented data can confuse the model since one source sequence will then be\npaired with different target sequences. Therefore, we propose a simple but\neffective Prompt Ordering based Data Augmentation (PODA) method to improve the\ntraining of unified autoregressive generation frameworks under few-shot NER\nscenarios. Experimental results on three public NER datasets and further\nanalyses demonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soh_D/0/1/0/all/0/1\">De Wen Soh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain-of-thought prompting for responding to in-depth dialogue questions with LLM. (arXiv:2305.11792v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11792","description":"<p>The way and content in which users ask questions can provide insight into\ntheir current status, including their personality, emotions, and psychology.\nInstead of directly prompting the large language models (LLMs), we explore how\nchain-of-thought prompting helps in this scenario to perform reasoning and\nplanning according to user status, aiming to provide a more personalized and\nengaging experience for the user query. To this end, we first construct a\nbenchmark of 6 dialogue or question-answering datasets in both English and\nChinese, covering 3 different aspects of user status (\\textit{including}\n\\textit{personality}, \\textit{emotion}, and \\textit{psychology}). Then we\nprompt the LLMs to generate the response regarding the user status as\nintermediate reasoning processing. We propose a novel demonstration selection\nstrategy using the semantic similarity of intermediate reasoning instead of\ntest queries. To evaluate the effectiveness and robustness of our approach, we\nconduct extensive experiments with 7 LLMs under zero-shot and one-shot\nsettings. The experimental results show that our approach consistently\noutperforms standard prompting in terms of both \\textit{helpfulness} and\n\\textit{acceptness} across all datasets, regardless of the LLMs used. The code\nand dataset can be found at\n\\url{https://github.com/ruleGreen/Dialogue\\_CoT.git}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zezhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics. (arXiv:2305.11806v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11806","description":"<p>Neural metrics for machine translation evaluation, such as COMET, exhibit\nsignificant improvements in their correlation with human judgments, as compared\nto traditional metrics based on lexical overlap, such as BLEU. Yet, neural\nmetrics are, to a great extent, \"black boxes\" returning a single sentence-level\nscore without transparency about the decision-making process. In this work, we\ndevelop and compare several neural explainability methods and demonstrate their\neffectiveness for interpreting state-of-the-art fine-tuned neural metrics. Our\nstudy reveals that these metrics leverage token-level information that can be\ndirectly attributed to translation errors, as assessed through comparison of\ntoken-level neural saliency maps with Multidimensional Quality Metrics (MQM)\nannotations and with synthetically-generated critical translation errors. To\nease future research, we release our code at:\nhttps://github.com/Unbabel/COMET/tree/explainable-metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rei_R/0/1/0/all/0/1\">Ricardo Rei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerreiro_N/0/1/0/all/0/1\">Nuno M. Guerreiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treviso_M/0/1/0/all/0/1\">Marcos Treviso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coheur_L/0/1/0/all/0/1\">Luisa Coheur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavie_A/0/1/0/all/0/1\">Alon Lavie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F.T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo-Label Training and Model Inertia in Neural Machine Translation. (arXiv:2305.11808v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11808","description":"<p>Like many other machine learning applications, neural machine translation\n(NMT) benefits from over-parameterized deep neural models. However, these\nmodels have been observed to be brittle: NMT model predictions are sensitive to\nsmall input changes and can show significant variation across re-training or\nincremental model updates. This work studies a frequently used method in NMT,\npseudo-label training (PLT), which is common to the related techniques of\nforward-translation (or self-training) and sequence-level knowledge\ndistillation. While the effect of PLT on quality is well-documented, we\nhighlight a lesser-known effect: PLT can enhance a model's stability to model\nupdates and input perturbations, a set of properties we call model inertia. We\nstudy inertia effects under different training settings and we identify\ndistribution simplification as a mechanism behind the observed results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_B/0/1/0/all/0/1\">Benjamin Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Currey_A/0/1/0/all/0/1\">Anna Currey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadejde_M/0/1/0/all/0/1\">Maria N&#x103;dejde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinu_G/0/1/0/all/0/1\">Georgiana Dinu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STOAT: Structured Data to Analytical Text With Controls. (arXiv:2305.11826v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11826","description":"<p>Recent language models have made tremendous progress in the structured data\nto text generation task. However, these models still give sub-optimal\nperformance where logical inference is required to generate the descriptions.\nIn this work, we specifically focus on analytical text generation from\nstructured data such as tables. Building on the taxonomy proposed in (Gupta et\nal., 2020) we focus on controllable table to text generation for the following\nreasoning categories: numerical reasoning, commonsense reasoning, temporal\nreasoning, table knowledge, and entity knowledge. We propose STOAT model, which\nis table and reasoning aware, with vector-quantization to infuse the given\nreasoning categories in the output. We observe that our model provides 10.19%,\n1.13% improvement on the PARENT metric in iToTTo and Infotabs for the\nanalytical sentence task. We also found that our model generates 15.3% more\nfaithful and analytical descriptions as compared to the baseline models in\nhuman evaluation. We curate and release two reasoning category annotated\ntable-to-interesting text generation datasets based on the ToTTo (Parikh et\nal., 2020) and InfoTabs datasets (Gupta et al.,2020).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nema_P/0/1/0/all/0/1\">Preksha Nema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghuveer_A/0/1/0/all/0/1\">Aravindan Raghuveer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews. (arXiv:2305.11828v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11828","description":"<p>Medical systematic reviews are crucial for informing clinical decision making\nand healthcare policy. But producing such reviews is onerous and\ntime-consuming. Thus, high-quality evidence synopses are not available for many\nquestions and may be outdated even when they are available. Large language\nmodels (LLMs) are now capable of generating long-form texts, suggesting the\ntantalizing possibility of automatically generating literature reviews on\ndemand. However, LLMs sometimes generate inaccurate (and potentially\nmisleading) texts by hallucinating or omitting important information. In the\nhealthcare context, this may render LLMs unusable at best and dangerous at\nworst. Most discussion surrounding the benefits and risks of LLMs have been\ndivorced from specific applications. In this work, we seek to qualitatively\ncharacterize the potential utility and risks of LLMs for assisting in\nproduction of medical evidence reviews. We conducted 16 semi-structured\ninterviews with international experts in systematic reviews, grounding\ndiscussion in the context of generating evidence reviews. Domain experts\nindicated that LLMs could aid writing reviews, as a tool for drafting or\ncreating plain language summaries, generating templates or suggestions,\ndistilling information, crosschecking, and synthesizing or interpreting text\ninputs. But they also identified issues with model outputs and expressed\nconcerns about potential downstream harms of confidently composed but\ninaccurate LLM outputs which might mislead. Other anticipated potential\ndownstream harms included lessened accountability and proliferation of\nautomatically generated reviews that might be of low quality. Informed by this\nqualitative analysis, we identify criteria for rigorous evaluation of\nbiomedical LLMs aligned with domain expert views.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yun_H/0/1/0/all/0/1\">Hye Sun Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshall_I/0/1/0/all/0/1\">Iain J. Marshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trikalinos_T/0/1/0/all/0/1\">Thomas Trikalinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models. (arXiv:2305.11840v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11840","description":"<p>Stereotype benchmark datasets are crucial to detect and mitigate social\nstereotypes about groups of people in NLP models. However, existing datasets\nare limited in size and coverage, and are largely restricted to stereotypes\nprevalent in the Western society. This is especially problematic as language\ntechnologies gain hold across the globe. To address this gap, we present\nSeeGULL, a broad-coverage stereotype dataset, built by utilizing generative\ncapabilities of large language models such as PaLM, and GPT-3, and leveraging a\nglobally diverse rater pool to validate the prevalence of those stereotypes in\nsociety. SeeGULL is in English, and contains stereotypes about identity groups\nspanning 178 countries across 8 different geo-political regions across 6\ncontinents, as well as state-level identities within the US and India. We also\ninclude fine-grained offensiveness scores for different stereotypes and\ndemonstrate their global disparities. Furthermore, we include comparative\nannotations about the same groups by annotators living in the region vs. those\nthat are based in North America, and demonstrate that within-region stereotypes\nabout groups differ from those prevalent in North America. CONTENT WARNING:\nThis paper contains stereotype examples that may be offensive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1\">Akshita Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davani_A/0/1/0/all/0/1\">Aida Davani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1\">Chandan K. Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dave_S/0/1/0/all/0/1\">Shachi Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1\">Vinodkumar Prabhakaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Does Generative Retrieval Scale to Millions of Passages?. (arXiv:2305.11841v1 [cs.IR])","link":"http://arxiv.org/abs/2305.11841","description":"<p>Popularized by the Differentiable Search Index, the emerging paradigm of\ngenerative retrieval re-frames the classic information retrieval problem into a\nsequence-to-sequence modeling task, forgoing external indices and encoding an\nentire document corpus within a single Transformer. Although many different\napproaches have been proposed to improve the effectiveness of generative\nretrieval, they have only been evaluated on document corpora on the order of\n100k in size. We conduct the first empirical study of generative retrieval\ntechniques across various corpus scales, ultimately scaling up to the entire MS\nMARCO passage ranking task with a corpus of 8.8M passages and evaluating model\nsizes up to 11B parameters. We uncover several findings about scaling\ngenerative retrieval to millions of passages; notably, the central importance\nof using synthetic queries as document representations during indexing, the\nineffectiveness of existing proposed architecture modifications when accounting\nfor compute cost, and the limits of naively scaling model parameters with\nrespect to retrieval performance. While we find that generative retrieval is\ncompetitive with state-of-the-art dual encoders on small corpora, scaling to\nmillions of passages remains an important and unsolved challenge. We believe\nthese findings will be valuable for the community to clarify the current state\nof generative retrieval, highlight the unique challenges, and inspire new\nresearch directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pradeep_R/0/1/0/all/0/1\">Ronak Pradeep</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1\">Kai Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lelkes_A/0/1/0/all/0/1\">Adam D. Lelkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_H/0/1/0/all/0/1\">Honglei Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing. (arXiv:2305.11845v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11845","description":"<p>Reaction diagram parsing is the task of extracting reaction schemes from a\ndiagram in the chemistry literature. The reaction diagrams can be arbitrarily\ncomplex, thus robustly parsing them into structured data is an open challenge.\nIn this paper, we present RxnScribe, a machine learning model for parsing\nreaction diagrams of varying styles. We formulate this structured prediction\ntask with a sequence generation approach, which condenses the traditional\npipeline into an end-to-end model. We train RxnScribe on a dataset of 1,378\ndiagrams and evaluate it with cross validation, achieving an 80.0% soft match\nF1 score, with significant improvements over previous models. Our code and data\nare publicly available at https://github.com/thomas0809/RxnScribe.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yujie Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhengkai Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coley_C/0/1/0/all/0/1\">Connor W. Coley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Any-to-Any Generation via Composable Diffusion. (arXiv:2305.11846v1 [cs.CV])","link":"http://arxiv.org/abs/2305.11846","description":"<p>We present Composable Diffusion (CoDi), a novel generative model capable of\ngenerating any combination of output modalities, such as language, image,\nvideo, or audio, from any combination of input modalities. Unlike existing\ngenerative AI systems, CoDi can generate multiple modalities in parallel and\nits input is not limited to a subset of modalities like text or image. Despite\nthe absence of training datasets for many combinations of modalities, we\npropose to align modalities in both the input and output space. This allows\nCoDi to freely condition on any input combination and generate any group of\nmodalities, even if they are not present in the training data. CoDi employs a\nnovel composable generation strategy which involves building a shared\nmultimodal space by bridging alignment in the diffusion process, enabling the\nsynchronized generation of intertwined modalities, such as temporally aligned\nvideo and audio. Highly customizable and flexible, CoDi achieves strong\njoint-modality generation quality, and outperforms or is on par with the\nunimodal state-of-the-art for single-modality synthesis. The project page with\ndemonstrations and code is at https://codi-gen.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zineng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings. (arXiv:2305.11853v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11853","description":"<p>Large language models (LLMs) with in-context learning have demonstrated\nremarkable capability in the text-to-SQL task. Previous research has prompted\nLLMs with various demonstration-retrieval strategies and intermediate reasoning\nsteps to enhance the performance of LLMs. However, those works often employ\nvaried strategies when constructing the prompt text for text-to-SQL inputs,\nsuch as databases and demonstration examples. This leads to a lack of\ncomparability in both the prompt constructions and their primary contributions.\nFurthermore, selecting an effective prompt construction has emerged as a\npersistent problem for future research. To address this limitation, we\ncomprehensively investigate the impact of prompt constructions across various\nsettings and provide insights for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuaichen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fosler_Lussier_E/0/1/0/all/0/1\">Eric Fosler-Lussier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complex Claim Verification with Evidence Retrieved in the Wild. (arXiv:2305.11859v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11859","description":"<p>Evidence retrieval is a core part of automatic fact-checking. Prior work\nmakes simplifying assumptions in retrieval that depart from real-world use\ncases: either no access to evidence, access to evidence curated by a human\nfact-checker, or access to evidence available long after the claim has been\nmade. In this work, we present the first fully automated pipeline to check\nreal-world claims by retrieving raw evidence from the web. We restrict our\nretriever to only search documents available prior to the claim's making,\nmodeling the realistic scenario where an emerging claim needs to be checked.\nOur pipeline includes five components: claim decomposition, raw document\nretrieval, fine-grained evidence retrieval, claim-focused summarization, and\nveracity judgment. We conduct experiments on complex political claims in the\nClaimDecomp dataset and show that the aggregated evidence produced by our\npipeline improves veracity judgments. Human evaluation finds the evidence\nsummary produced by our system is reliable (it does not hallucinate\ninformation) and relevant to answering key questions about a claim, suggesting\nthat it can assist fact-checkers even when it cannot surface a complete\nevidence set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Grace Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sriram_A/0/1/0/all/0/1\">Aniruddh Sriram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs. (arXiv:2305.11860v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11860","description":"<p>A popular approach for improving the correctness of output from large\nlanguage models (LLMs) is Self-Consistency - poll the LLM multiple times and\noutput the most frequent solution. Existing Self-Consistency techniques always\ndraw a constant number of samples per question, where a better approach will be\nto non-uniformly distribute the available budget based on the amount of\nagreement in the samples drawn so far. In response, we introduce\nAdaptive-Consistency, a cost-efficient, model-agnostic technique that\ndynamically adjusts the number of samples per question using a lightweight\nstopping criterion. Our experiments over 13 datasets and two LLMs demonstrate\nthat Adaptive-Consistency reduces sample budget by up to 6.0 times with an\naverage accuracy drop of less than 0.1%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_P/0/1/0/all/0/1\">Pranjal Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing Sequence Length by Predicting Edit Operations with Large Language Models. (arXiv:2305.11862v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11862","description":"<p>Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious tasks and gained significant attention. LLMs are also used for local\nsequence transduction tasks, including grammatical error correction (GEC) and\nformality style transfer, where most tokens in a source text are kept\nunchanged. However, it is inefficient to generate all target tokens because a\nprediction error of a target token may cause a catastrophe in predicting\nsubsequent tokens and because the computational cost grows quadratically with\nthe target sequence length. This paper proposes to predict a set of edit\noperations for the source text for local sequence transduction tasks.\nRepresenting an edit operation with a span of the source text and changed\ntokens, we can reduce the length of the target sequence and thus the\ncomputational cost for inference. We apply instruction tuning for LLMs on the\nsupervision data of edit operations. Experiments show that the proposed method\nachieves comparable performance to the baseline in four tasks, paraphrasing,\nformality style transfer, GEC, and text simplification, despite reducing the\nlength of the target text by as small as 21\\%. Furthermore, we report that the\ninstruction tuning with the proposed method achieved the state-of-the-art\nperformance in the four tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_M/0/1/0/all/0/1\">Masahiro Kaneko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling laws for language encoding models in fMRI. (arXiv:2305.11863v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11863","description":"<p>Representations from transformer-based unidirectional language models are\nknown to be effective at predicting brain responses to natural language.\nHowever, most studies comparing language models to brains have used GPT-2 or\nsimilarly sized language models. Here we tested whether larger open-source\nmodels such as those from the OPT and LLaMA families are better at predicting\nbrain responses recorded using fMRI. Mirroring scaling results from other\ncontexts, we found that brain prediction performance scales log-linearly with\nmodel size from 125M to 30B parameter models, with ~15% increased encoding\nperformance as measured by correlation with a held-out test set across 3\nsubjects. Similar log-linear behavior was observed when scaling the size of the\nfMRI training set. We also characterized scaling for acoustic encoding models\nthat use HuBERT, WavLM, and Whisper, and we found comparable improvements with\nmodel size. A noise ceiling analysis of these large, high-performance encoding\nmodels showed that performance is nearing the theoretical maximum for brain\nareas such as the precuneus and higher auditory cortex. These results suggest\nthat increasing scale in both models and data will yield incredibly effective\nmodels of language processing in the brain, enabling better scientific\nunderstanding as well as applications such as decoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antonello_R/0/1/0/all/0/1\">Richard Antonello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaidya_A/0/1/0/all/0/1\">Aditya Vaidya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huth_A/0/1/0/all/0/1\">Alexander G. Huth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"North S\\'{a}mi Dialect Identification with Self-supervised Speech Models. (arXiv:2305.11864v1 [eess.AS])","link":"http://arxiv.org/abs/2305.11864","description":"<p>The North S\\'{a}mi (NS) language encapsulates four primary dialectal variants\nthat are related but that also have differences in their phonology, morphology,\nand vocabulary. The unique geopolitical location of NS speakers means that in\nmany cases they are bilingual in S\\'{a}mi as well as in the dominant state\nlanguage: Norwegian, Swedish, or Finnish. This enables us to study the NS\nvariants both with respect to the spoken state language and their acoustic\ncharacteristics. In this paper, we investigate an extensive set of acoustic\nfeatures, including MFCCs and prosodic features, as well as state-of-the-art\nself-supervised representations, namely, XLS-R, WavLM, and HuBERT, for the\nautomatic detection of the four NS variants. In addition, we examine how the\nmajority state language is reflected in the dialects. Our results show that NS\ndialects are influenced by the state language and that the four dialects are\nseparable, reaching high classification accuracy, especially with the XLS-R\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kakouros_S/0/1/0/all/0/1\">Sofoklis Kakouros</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hiovain_Asikainen_K/0/1/0/all/0/1\">Katri Hiovain-Asikainen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Secondary Use of Clinical Problem List Entries for Neural Network-Based Disease Code Assignment. (arXiv:2112.13756v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.13756","description":"<p>Clinical information systems have become large repositories for\nsemi-structured and partly annotated electronic health record data, which have\nreached a critical mass that makes them interesting for supervised data-driven\nneural network approaches. We explored automated coding of 50 character long\nclinical problem list entries using the International Classification of\nDiseases (ICD-10) and evaluated three different types of network architectures\non the top 100 ICD-10 three-digit codes. A fastText baseline reached a\nmacro-averaged F1-score of 0.83, followed by a character-level LSTM with a\nmacro-averaged F1-score of 0.84. The top performing approach used a\ndownstreamed RoBERTa model with a custom language model, yielding a\nmacro-averaged F1-score of 0.88. A neural network activation analysis together\nwith an investigation of the false positives and false negatives unveiled\ninconsistent manual coding as a main limiting factor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kreuzthaler_M/0/1/0/all/0/1\">Markus Kreuzthaler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeifer_B/0/1/0/all/0/1\">Bastian Pfeifer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kramer_D/0/1/0/all/0/1\">Diether Kramer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_S/0/1/0/all/0/1\">Stefan Schulz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?. (arXiv:2207.12101v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.12101","description":"<p>The use of Deep Learning and Computer Vision in the Cultural Heritage domain\nis becoming highly relevant in the last few years with lots of applications\nabout audio smart guides, interactive museums and augmented reality. All these\ntechnologies require lots of data to work effectively and be useful for the\nuser. In the context of artworks, such data is annotated by experts in an\nexpensive and time consuming process. In particular, for each artwork, an image\nof the artwork and a description sheet have to be collected in order to perform\ncommon tasks like Visual Question Answering. In this paper we propose a method\nfor Visual Question Answering that allows to generate at runtime a description\nsheet that can be used for answering both visual and contextual questions about\nthe artwork, avoiding completely the image and the annotation process. For this\npurpose, we investigate on the use of GPT-3 for generating descriptions for\nartworks analyzing the quality of generated descriptions through captioning\nmetrics. Finally we evaluate the performance for Visual Question Answering and\ncaptioning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bongini_P/0/1/0/all/0/1\">Pietro Bongini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becattini_F/0/1/0/all/0/1\">Federico Becattini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1\">Alberto Del Bimbo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient NLP Model Finetuning via Multistage Data Filtering. (arXiv:2207.14386v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.14386","description":"<p>As model finetuning is central to the modern NLP, we set to maximize its\nefficiency. Motivated by redundancy in training examples and the sheer sizes of\npretrained models, we exploit a key opportunity: training only on important\ndata. To this end, we set to filter training examples in a streaming fashion,\nin tandem with training the target model. Our key techniques are two: (1)\nautomatically determine a training loss threshold for skipping backward\ntraining passes; (2) run a meta predictor for further skipping forward training\npasses. We integrate the above techniques in a holistic, three-stage training\nprocess. On a diverse set of benchmarks, our method reduces the required\ntraining examples by up to 5.3$\\times$ and training time by up to 6.8$\\times$,\nwhile only seeing minor accuracy degradation. Our method is effective even when\ntraining one epoch, where each training example is encountered only once. It is\nsimple to implement and is compatible with the existing finetuning techniques.\nCode is available at: https://github.com/xo28/efficient-\nNLP-multistage-training\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_X/0/1/0/all/0/1\">Xu Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ansari_S/0/1/0/all/0/1\">Shahina Mohd Azam Ansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Felix Xiaozhu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Environmental Claim Detection. (arXiv:2209.00507v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.00507","description":"<p>To transition to a green economy, environmental claims made by companies must\nbe reliable, comparable, and verifiable. To analyze such claims at scale,\nautomated methods are needed to detect them in the first place. However, there\nexist no datasets or models for this. Thus, this paper introduces the task of\nenvironmental claim detection. To accompany the task, we release an\nexpert-annotated dataset and models trained on this dataset. We preview one\npotential application of such models: We detect environmental claims made in\nquarterly earning calls and find that the number of environmental claims has\nsteadily increased since the Paris Agreement in 2015.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stammbach_D/0/1/0/all/0/1\">Dominik Stammbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webersinke_N/0/1/0/all/0/1\">Nicolas Webersinke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bingler_J/0/1/0/all/0/1\">Julia Anna Bingler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraus_M/0/1/0/all/0/1\">Mathias Kraus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leippold_M/0/1/0/all/0/1\">Markus Leippold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Application of Knowledge Distillation to Multi-task Speech Representation Learning. (arXiv:2210.16611v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2210.16611","description":"<p>Model architectures such as wav2vec 2.0 and HuBERT have been proposed to\nlearn speech representations from audio waveforms in a self-supervised manner.\nWhen they are combined with downstream tasks such as keyword spotting and\nspeaker verification, they provide state-of-the-art performance. However, these\nmodels use a large number of parameters, the smallest version of which has 95\nmillion parameters. This constitutes a challenge for edge AI device\ndeployments. In this paper, we investigate the application of knowledge\ndistillation to speech representation learning (SRL) models followed by joint\nfine-tuning with multiple downstream voice-activated tasks. In our experiments\non two such tasks, our approach results in nearly 75% reduction in model size\nwhile suffering only 0.1% accuracy and 0.9% equal error rate degradation\ncompared to the full-size model. In addition, we show that fine-tuning the SRL\nmodels results in a significant performance boost compared to using frozen SRL\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kerpicci_M/0/1/0/all/0/1\">Mine Kerpicci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_V/0/1/0/all/0/1\">Van Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shuhua Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Visser_E/0/1/0/all/0/1\">Erik Visser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLPeer: A Unified Resource for the Computational Study of Peer Review. (arXiv:2211.06651v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.06651","description":"<p>Peer review constitutes a core component of scholarly publishing; yet it\ndemands substantial expertise and training, and is susceptible to errors and\nbiases. Various applications of NLP for peer reviewing assistance aim to\nsupport reviewers in this complex process, but the lack of clearly licensed\ndatasets and multi-domain corpora prevent the systematic study of NLP for peer\nreview. To remedy this, we introduce NLPeer -- the first ethically sourced\nmultidomain corpus of more than 5k papers and 11k review reports from five\ndifferent venues. In addition to the new datasets of paper drafts, camera-ready\nversions and peer reviews from the NLP community, we establish a unified data\nrepresentation and augment previous peer review datasets to include parsed and\nstructured paper representations, rich metadata and versioning information. We\ncomplement our resource with implementations and analysis of three reviewing\nassistance tasks, including a novel guided skimming task. Our work paves the\npath towards systematic, multi-faceted, evidence-based study of peer review in\nNLP and beyond. The data and code are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dycke_N/0/1/0/all/0/1\">Nils Dycke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_I/0/1/0/all/0/1\">Ilia Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning. (arXiv:2211.11275v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2211.11275","description":"<p>Although speech is a simple and effective way for humans to communicate with\nthe outside world, a more realistic speech interaction contains multimodal\ninformation, e.g., vision, text. How to design a unified framework to integrate\ndifferent modal information and leverage different resources (e.g.,\nvisual-audio pairs, audio-text pairs, unlabeled speech, and unlabeled text) to\nfacilitate speech representation learning was not well explored. In this paper,\nwe propose a unified cross-modal representation learning framework VATLM\n(Visual-Audio-Text Language Model). The proposed VATLM employs a unified\nbackbone network to model the modality-independent information and utilizes\nthree simple modality-dependent modules to preprocess visual, speech, and text\ninputs. In order to integrate these three modalities into one shared semantic\nspace, VATLM is optimized with a masked prediction task of unified tokens,\ngiven by our proposed unified tokenizer. We evaluate the pre-trained VATLM on\naudio-visual related downstream tasks, including audio-visual speech\nrecognition (AVSR), visual speech recognition (VSR) tasks. Results show that\nthe proposed VATLM outperforms previous the state-of-the-art models, such as\naudio-visual pre-trained AV-HuBERT model, and analysis also demonstrates that\nVATLM is capable of aligning different modalities into the same space. To\nfacilitate future research, we release the code and pre-trained models at\nhttps://aka.ms/vatlm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Q/0/1/0/all/0/1\">Qiushi Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziqiang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiao_B/0/1/0/all/0/1\">Binxing Jiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_L/0/1/0/all/0/1\">Lirong Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Inference from Transformers via Speculative Decoding. (arXiv:2211.17192v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2211.17192","description":"<p>Inference from large autoregressive models like Transformers is slow -\ndecoding K tokens takes K serial runs of the model. In this work we introduce\nspeculative decoding - an algorithm to sample from autoregressive models faster\nwithout any changes to the outputs, by computing several tokens in parallel. At\nthe heart of our approach lie the observations that (1) hard language-modeling\ntasks often include easier subtasks that can be approximated well by more\nefficient models, and (2) using speculative execution and a novel sampling\nmethod, we can make exact decoding from the large models faster, by running\nthem in parallel on the outputs of the approximation models, potentially\ngenerating several tokens concurrently, and without changing the distribution.\nOur method can accelerate existing off-the-shelf models without retraining or\narchitecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration\ncompared to the standard T5X implementation, with identical outputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leviathan_Y/0/1/0/all/0/1\">Yaniv Leviathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalman_M/0/1/0/all/0/1\">Matan Kalman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matias_Y/0/1/0/all/0/1\">Yossi Matias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of Utterance Embeddings and Clustering Methods Related to Intent Induction for Task-Oriented Dialogue. (arXiv:2212.02021v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.02021","description":"<p>The focus of this work is to investigate unsupervised approaches to overcome\nquintessential challenges in designing task-oriented dialog schema: assigning\nintent labels to each dialog turn (intent clustering) and generating a set of\nintents based on the intent clustering methods (intent induction). We postulate\nthere are two salient factors for automatic induction of intents: (1)\nclustering algorithm for intent labeling and (2) user utterance embedding\nspace. We compare existing off-the-shelf clustering models and embeddings based\non DSTC11 evaluation. Our extensive experiments demonstrate that the combined\nselection of utterance embedding and clustering method in the intent induction\ntask should be carefully considered. We also present that pretrained MiniLM\nwith Agglomerative clustering shows significant improvement in NMI, ARI, F1,\naccuracy and example coverage in intent induction tasks. The source codes are\navailable at https://github.com/Jeiyoon/dstc11-track2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jeiyoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Yoonna Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chanhee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Heuiseok Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SODA: A Natural Language Processing Package to Extract Social Determinants of Health for Cancer Studies. (arXiv:2212.03000v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.03000","description":"<p>Objective: We aim to develop an open-source natural language processing (NLP)\npackage, SODA (i.e., SOcial DeterminAnts), with pre-trained transformer models\nto extract social determinants of health (SDoH) for cancer patients, examine\nthe generalizability of SODA to a new disease domain (i.e., opioid use), and\nevaluate the extraction rate of SDoH using cancer populations.\n</p>\n<p>Methods: We identified SDoH categories and attributes and developed an SDoH\ncorpus using clinical notes from a general cancer cohort. We compared four\ntransformer-based NLP models to extract SDoH, examined the generalizability of\nNLP models to a cohort of patients prescribed with opioids, and explored\ncustomization strategies to improve performance. We applied the best NLP model\nto extract 19 categories of SDoH from the breast (n=7,971), lung (n=11,804),\nand colorectal cancer (n=6,240) cohorts.\n</p>\n<p>Results and Conclusion: We developed a corpus of 629 cancer patients notes\nwith annotations of 13,193 SDoH concepts/attributes from 19 categories of SDoH.\nThe Bidirectional Encoder Representations from Transformers (BERT) model\nachieved the best strict/lenient F1 scores of 0.9216 and 0.9441 for SDoH\nconcept extraction, 0.9617 and 0.9626 for linking attributes to SDoH concepts.\nFine-tuning the NLP models using new annotations from opioid use patients\nimproved the strict/lenient F1 scores from 0.8172/0.8502 to 0.8312/0.8679. The\nextraction rates among 19 categories of SDoH varied greatly, where 10 SDoH\ncould be extracted from &gt;70% of cancer patients, but 9 SDoH had a low\nextraction rate (&lt;70% of cancer patients). The SODA package with pre-trained\ntransformer models is publicly available at\nhttps://github.com/uf-hobiinformatics-lab/SDoH_SODA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zehao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_C/0/1/0/all/0/1\">Chong Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adekkanattu_P/0/1/0/all/0/1\">Prakash Adekkanattu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patra_B/0/1/0/all/0/1\">Braja Gopal Patra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_J/0/1/0/all/0/1\">Jyotishman Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1\">Debbie L. Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Ching-Yuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_Ciganic_W/0/1/0/all/0/1\">Wei-Hsuan Lo-Ciganic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+George_T/0/1/0/all/0/1\">Thomas J. George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hogan_W/0/1/0/all/0/1\">William R. Hogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Text-based Personality Computing: Challenges and Future Directions. (arXiv:2212.06711v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.06711","description":"<p>Text-based personality computing (TPC) has gained many research interests in\nNLP. In this paper, we describe 15 challenges that we consider deserving the\nattention of the research community. These challenges are organized by the\nfollowing topics: personality taxonomies, measurement quality, datasets,\nperformance evaluation, modelling choices, as well as ethics and fairness. When\naddressing each challenge, not only do we combine perspectives from both NLP\nand social sciences, but also offer concrete suggestions. We hope to inspire\nmore valid and reliable TPC research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qixiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giachanou_A/0/1/0/all/0/1\">Anastasia Giachanou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagheri_A/0/1/0/all/0/1\">Ayoub Bagheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boeschoten_L/0/1/0/all/0/1\">Laura Boeschoten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kesteren_E/0/1/0/all/0/1\">Erik-Jan van Kesteren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamalabad_M/0/1/0/all/0/1\">Mahdi Shafiee Kamalabad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberski_D/0/1/0/all/0/1\">Daniel L Oberski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages. (arXiv:2212.06742v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.06742","description":"<p>Software engineers working with the same programming language (PL) may speak\ndifferent natural languages (NLs) and vice versa, erecting huge barriers to\ncommunication and working efficiency. Recent studies have demonstrated the\neffectiveness of generative pre-training in computer programs, yet they are\nalways English-centric. In this work, we step towards bridging the gap between\nmultilingual NLs and multilingual PLs for large language models (LLMs). We\nrelease ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs.\nWe employ two methods for universal cross-lingual pre-training: span-corruption\nlanguage modeling that learns patterns from monolingual NL or PL; and\npivot-based translation language modeling that relies on parallel data of many\nNLs and PLs. Extensive results show that ERNIE-Code outperforms previous\nmultilingual LLMs for PL or NL across a wide range of end tasks of code\nintelligence, including multilingual code-to-text, text-to-code, code-to-code,\nand text-to-text generation. We further show its advantage of zero-shot\nprompting on multilingual code summarization and text-to-text translation. We\nrelease our code and pre-trained checkpoints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1\">Yekun Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_C/0/1/0/all/0/1\">Chao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1\">Hao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causes and Cures for Interference in Multilingual Translation. (arXiv:2212.07530v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.07530","description":"<p>Multilingual machine translation models can benefit from synergy between\ndifferent language pairs, but also suffer from interference. While there is a\ngrowing number of sophisticated methods that aim to eliminate interference, our\nunderstanding of interference as a phenomenon is still limited. This work\nidentifies the main factors that contribute to interference in multilingual\nmachine translation. Through systematic experimentation, we find that\ninterference (or synergy) are primarily determined by model size, data size,\nand the proportion of each language pair within the total dataset. We observe\nthat substantial interference occurs mainly when the model is very small with\nrespect to the available training data, and that using standard transformer\nconfigurations with less than one billion parameters largely alleviates\ninterference and promotes synergy. Moreover, we show that tuning the sampling\ntemperature to control the proportion of each language pair in the data is key\nto balancing the amount of interference between low and high resource language\npairs effectively, and can lead to superior performance overall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaham_U/0/1/0/all/0/1\">Uri Shaham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elbayad_M/0/1/0/all/0/1\">Maha Elbayad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goswami_V/0/1/0/all/0/1\">Vedanuj Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhosale_S/0/1/0/all/0/1\">Shruti Bhosale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Know What I don't Know: Handling Ambiguous and Unanswerable Questions for Text-to-SQL. (arXiv:2212.08902v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08902","description":"<p>The task of text-to-SQL aims to convert a natural language question into its\ncorresponding SQL query within the context of relational tables. Existing\ntext-to-SQL parsers generate a \"plausible\" SQL query for an arbitrary user\nquestion, thereby failing to correctly handle problematic user questions. To\nformalize this problem, we conduct a preliminary study on the observed\nambiguous and unanswerable cases in text-to-SQL and summarize them into 6\nfeature categories. Correspondingly, we identify the causes behind each\ncategory and propose requirements for handling ambiguous and unanswerable\nquestions. Following this study, we propose a simple yet effective\ncounterfactual example generation approach that automatically produces\nambiguous and unanswerable text-to-SQL examples. Furthermore, we propose a\nweakly supervised DTE (Detecting-Then-Explaining) model for error detection,\nlocalization, and explanation. Experimental results show that our model\nachieves the best result on both real-world examples and generated examples\ncompared with various baselines. We release our data and code at:\n\\href{https://github.com/wbbeyourself/DTE}{https://github.com/wbbeyourself/DTE}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimal Transport for Unsupervised Hallucination Detection in Neural Machine Translation. (arXiv:2212.09631v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09631","description":"<p>Neural machine translation (NMT) has become the de-facto standard in\nreal-world machine translation applications. However, NMT models can\nunpredictably produce severely pathological translations, known as\nhallucinations, that seriously undermine user trust. It becomes thus crucial to\nimplement effective preventive strategies to guarantee their proper\nfunctioning. In this paper, we address the problem of hallucination detection\nin NMT by following a simple intuition: as hallucinations are detached from the\nsource content, they exhibit encoder-decoder attention patterns that are\nstatistically different from those of good quality translations. We frame this\nproblem with an optimal transport formulation and propose a fully unsupervised,\nplug-in detector that can be used with any attention-based NMT model.\nExperimental results show that our detector not only outperforms all previous\nmodel-based detectors, but is also competitive with detectors that employ large\nmodels trained on millions of samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guerreiro_N/0/1/0/all/0/1\">Nuno M. Guerreiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1\">Pablo Piantanida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Foveate, Attribute, and Rationalize: Towards Physically Safe and Trustworthy AI. (arXiv:2212.09667v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09667","description":"<p>Users' physical safety is an increasing concern as the market for intelligent\nsystems continues to grow, where unconstrained systems may recommend users\ndangerous actions that can lead to serious injury. Covertly unsafe text is an\narea of particular interest, as such text may arise from everyday scenarios and\nare challenging to detect as harmful. We propose FARM, a novel framework\nleveraging external knowledge for trustworthy rationale generation in the\ncontext of safety. In particular, FARM foveates on missing knowledge to qualify\nthe information required to reason in specific scenarios and retrieves this\ninformation with attribution to trustworthy sources. This knowledge is used to\nboth classify the safety of the original text and generate human-interpretable\nrationales, shedding light on the risk of systems to specific user groups and\nhelping both stakeholders manage the risks of their systems and policymakers to\nprovide concrete safeguards for consumer safety. Our experiments show that FARM\nobtains state-of-the-art results on the SafeText dataset, showing absolute\nimprovement in safety classification accuracy by 5.9%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mei_A/0/1/0/all/0/1\">Alex Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Sharon Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Blind Spots of Model-Based Evaluation Metrics for Text Generation. (arXiv:2212.10020v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10020","description":"<p>In this work, we explore a useful but often neglected methodology for\nrobustness analysis of text generation evaluation metrics: stress tests with\nsynthetic data. Basically, we design and synthesize a wide range of potential\nerrors and check whether they result in a commensurate drop in the metric\nscores. We examine a range of recently proposed evaluation metrics based on\npretrained language models, for the tasks of open-ended generation,\ntranslation, and summarization. Our experiments reveal interesting\ninsensitivities, biases, or even loopholes in existing metrics. For example, we\nfind that BERTScore is confused by truncation errors in summarization, and\nMAUVE (built on top of GPT-2) is insensitive to errors at the beginning or\nmiddle of generations. Further, we investigate the reasons behind these blind\nspots and suggest practical workarounds for a more reliable evaluation of text\ngeneration. We have released our code and data at\nhttps://github.com/cloudygoose/blindspot_nlg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianxing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianle Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sachin Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empowering Sentence Encoders with Prompting and Label Retrieval for Zero-shot Text Classification. (arXiv:2212.10391v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10391","description":"<p>With contrastive pre-training, sentence encoders are generally optimized to\nlocate semantically similar samples closer to each other in their embedding\nspaces. In this work, we focus on the potential of their embedding spaces to be\nreadily adapted to zero-shot text classification, as semantically distinct\nsamples are already well-separated. Our framework, RaLP (Retrieval augmented\nLabel Prompts for sentence encoder), encodes prompted label candidates with a\nsentence encoder, then assigns the label whose prompt embedding has the highest\nsimilarity with the input text embedding. In order to compensate for the\npotentially poorly descriptive labels in their original format, RaLP retrieves\nsentences that are semantically similar to the original label prompt from\nexternal corpora and use them as additional pseudo-label prompts. RaLP achieves\ncompetitive or stronger performance than much larger baselines on various\nclosed-set classification and multiple-choice QA datasets under zero-shot\nsettings. We show that the retrieval component plays a pivotal role in RaLP's\nsuccess, and its results are robustly attained regardless of verbalizer\nvariations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jimin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jungsoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Seongjae Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_B/0/1/0/all/0/1\">Bokyung Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewook Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Execution-Based Evaluation for Open-Domain Code Generation. (arXiv:2212.10481v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2212.10481","description":"<p>To extend the scope of coding queries to more realistic settings, we propose\nODEX, the first Open-Domain EXecution-based natural language (NL) to Python\ncode generation dataset. ODEX has 945 NL-Code pairs spanning 79 diverse\nlibraries, along with 1,707 human-written test cases for execution. Our NL-Code\npairs are harvested from StackOverflow forums to encourage natural and\npractical coding queries. Moreover, ODEX supports four natural languages as\nintents, in English, Spanish, Japanese, and Russian. ODEX unveils intriguing\nbehavioral differences among top-performing code language models (LM). While\nCODEX achieves better overall results, CODEGEN improves effectively via scaling\n-- CODEGEN 6.1B performs comparably with CODEX 12B. Both models show\nsubstantial gaps between open and closed domains, but CODEGEN gaps tend to\ndecrease with model size while CODEX gaps increase. We release ODEX to\nfacilitate research into open-domain problems for the code generation\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiruo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.10410","description":"<p>Cross-domain NER is a challenging task to address the low-resource problem in\npractical scenarios. Previous typical solutions mainly obtain a NER model by\npre-trained language models (PLMs) with data from a rich-resource domain and\nadapt it to the target domain. Owing to the mismatch issue among entity types\nin different domains, previous approaches normally tune all parameters of PLMs,\nending up with an entirely new NER model for each domain. Moreover, current\nmodels only focus on leveraging knowledge in one general source domain while\nfailing to successfully transfer knowledge from multiple sources to the target.\nTo address these issues, we introduce Collaborative Domain-Prefix Tuning for\ncross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically,\nwe present text-to-text generation grounding domain-related instructors to\ntransfer knowledge to new domain NER tasks without structural modifications. We\nutilize frozen PLMs and conduct collaborative domain-prefix tuning to stimulate\nthe potential of PLMs to handle NER tasks across various domains. Experimental\nresults on the Cross-NER benchmark show that the proposed approach has flexible\ntransfer ability and performs better on both one-source and multiple-source\ncross-domain NER tasks. Codes are available in\nhttps://github.com/zjunlp/DeepKE/tree/main/example/ner/cross.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1\">Shuofei Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Migration Reframed? A multilingual analysis on the stance shift in Europe during the Ukrainian crisis. (arXiv:2302.02813v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2302.02813","description":"<p>The war in Ukraine seems to have positively changed the attitude toward the\ncritical societal topic of migration in Europe -- at least towards refugees\nfrom Ukraine. We investigate whether this impression is substantiated by how\nthe topic is reflected in online news and social media, thus linking the\nrepresentation of the issue on the Web to its perception in society. For this\npurpose, we combine and adapt leading-edge automatic text processing for a\nnovel multilingual stance detection approach. Starting from 5.5M Twitter posts\npublished by 565 European news outlets in one year, beginning September 2021,\nplus replies, we perform a multilingual analysis of migration-related media\ncoverage and associated social media interaction for Europe and selected\nEuropean countries.\n</p>\n<p>The results of our analysis show that there is actually a reframing of the\ndiscussion illustrated by the terminology change, e.g., from \"migrant\" to\n\"refugee\", often even accentuated with phrases such as \"real refugees\".\nHowever, concerning a stance shift in public perception, the picture is more\ndiverse than expected. All analyzed cases show a noticeable temporal stance\nshift around the start of the war in Ukraine. Still, there are apparent\nnational differences in the size and stability of this shift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wildemann_S/0/1/0/all/0/1\">Sergej Wildemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niederee_C/0/1/0/all/0/1\">Claudia Nieder&#xe9;e</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elejalde_E/0/1/0/all/0/1\">Erick Elejalde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Big Little Transformer Decoder. (arXiv:2302.07863v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.07863","description":"<p>The recent emergence of Large Language Models based on the Transformer\narchitecture has enabled dramatic advancements in the field of Natural Language\nProcessing. However, these models have long inference latency, which limits\ntheir deployment, and which makes them prohibitively expensive for various\nreal-time applications. The inference latency is further exacerbated by\nautoregressive generative tasks, as models need to run iteratively to generate\ntokens sequentially without leveraging token-level parallelization. To address\nthis, we propose Big Little Decoder (BiLD), a framework that can improve\ninference efficiency and latency for a wide range of text generation\napplications. The BiLD framework contains two models with different sizes that\ncollaboratively generate text. The small model runs autoregressively to\ngenerate text with a low inference cost, and the large model is only invoked\noccasionally to refine the small model's inaccurate predictions in a\nnon-autoregressive manner. To coordinate the small and large models, BiLD\nintroduces two simple yet effective policies: (1) the fallback policy that\ndetermines when to hand control over to the large model; and (2) the rollback\npolicy that determines when the large model needs to correct the small model's\ninaccurate predictions. To evaluate our framework across different tasks and\nmodels, we apply BiLD to various text generation scenarios encompassing machine\ntranslation on IWSLT 2017 De-En and WMT 2014 De-En, and summarization on XSUM\nand CNN/DailyMail. On an NVIDIA T4 GPU, our framework achieves a speedup of up\nto 2.12x speedup with minimal generation quality degradation. Furthermore, our\nframework is fully plug-and-play and can be applied without any modifications\nin the training process or model architecture. Our code is open-sourced\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangalam_K/0/1/0/all/0/1\">Karttikeya Mangalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Suhong Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canny_J/0/1/0/all/0/1\">John Canny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Epicurus at SemEval-2023 Task 4: Improving Prediction of Human Values behind Arguments by Leveraging Their Definitions. (arXiv:2302.13925v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.13925","description":"<p>We describe our experiments for SemEval-2023 Task 4 on the identification of\nhuman values behind arguments (ValueEval). Because human values are subjective\nconcepts which require precise definitions, we hypothesize that incorporating\nthe definitions of human values (in the form of annotation instructions and\nvalidated survey items) during model training can yield better prediction\nperformance. We explore this idea and show that our proposed models perform\nbetter than the challenge organizers' baselines, with improvements in macro F1\nscores of up to 18%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Christian Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qixiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dong Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniFLG: Unified Facial Landmark Generator from Text or Speech. (arXiv:2302.14337v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2302.14337","description":"<p>Talking face generation has been extensively investigated owing to its wide\napplicability. The two primary frameworks used for talking face generation\ncomprise a text-driven framework, which generates synchronized speech and\ntalking faces from text, and a speech-driven framework, which generates talking\nfaces from speech. To integrate these frameworks, this paper proposes a unified\nfacial landmark generator (UniFLG). The proposed system exploits end-to-end\ntext-to-speech not only for synthesizing speech but also for extracting a\nseries of latent representations that are common to text and speech, and feeds\nit to a landmark decoder to generate facial landmarks. We demonstrate that our\nsystem achieves higher naturalness in both speech synthesis and facial landmark\ngeneration compared to the state-of-the-art text-driven method. We further\ndemonstrate that our system can generate facial landmarks from speech of\nspeakers without facial video data or even speech data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitsui_K/0/1/0/all/0/1\">Kentaro Mitsui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hono_Y/0/1/0/all/0/1\">Yukiya Hono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawada_K/0/1/0/all/0/1\">Kei Sawada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.15714","description":"<p>Language models have been shown to perform remarkably well on a wide range of\nnatural language processing tasks. In this paper, we propose a novel system\nthat uses language models to perform multi-step logical reasoning. Our system\nincorporates explicit planning into its inference procedure, thus able to make\nmore informed reasoning decisions at each step by looking ahead into their\nfuture effects. Moreover, we propose a training strategy that safeguards the\nplanning process from being led astray by spurious features. Our full system\nsignificantly outperforms other competing methods on multiple standard\ndatasets. When using a T5 model as its core component, our system performs\ncompetitively compared to GPT-3 despite having only about 1B parameters (i.e.,\n175 times smaller than GPT-3). When using GPT-3.5, it significantly outperforms\nchain-of-thought prompting on the challenging PrOntoQA dataset. We have\nconducted extensive empirical studies to demonstrate that explicit planning\nplays a crucial role in the system's performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hongyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kangrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1\">Hongyuan Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive-Hint Prompting Improves Reasoning in Large Language Models. (arXiv:2304.09797v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.09797","description":"<p>The performance of Large Language Models (LLMs) in reasoning tasks depends\nheavily on prompt design, with Chain-of-Thought (CoT) and self-consistency\nbeing critical methods that enhance this ability. However, these methods do not\nfully exploit the answers generated by the LLM to guide subsequent responses.\nThis paper proposes a new prompting method, named Progressive-Hint Prompting\n(PHP), that enables automatic multiple interactions between users and LLMs by\nusing previously generated answers as hints to progressively guide toward the\ncorrect answers. PHP is orthogonal to CoT and self-consistency, making it easy\nto combine with state-of-the-art techniques to further improve performance. We\nconducted extensive and comprehensive experiments on seven benchmarks. The\nresults show that PHP significantly improves accuracy while remaining highly\nefficient. For instance, with text-davinci-003, we observed a 4.2% improvement\non GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction\nin sample paths with self-consistency. With GPT-4 and PHP, we achieve\nstate-of-the-art performances on SVAMP (89.1% -&gt; 91.9%), GSM8K (92% -&gt; 95.5%),\nAQuA (76.4% -&gt; 79.9%) and MATH (50.3% -&gt; 53.9%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chuanyang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PersonaLLM: Investigating the Ability of GPT-3.5 to Express Personality Traits and Gender Differences. (arXiv:2305.02547v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.02547","description":"<p>Despite the many use cases for large language models (LLMs) in the design of\nchatbots in various industries and the research showing the importance of\npersonalizing chatbots to cater to different personality traits, little work\nhas been done to evaluate whether the behaviors of personalized LLMs can\nreflect certain personality traits accurately and consistently. We consider\nstudying the behavior of LLM-based simulated agents which refer to as LLM\npersonas and present a case study with GPT-3.5 (text-davinci-003) to\ninvestigate whether LLMs can generate content with consistent, personalized\ntraits when assigned Big Five personality types and gender roles. We created\n320 LLM personas (5 females and 5 males for each of the 32 Big Five personality\ntypes) and prompted them to complete the classic 44-item Big Five Inventory\n(BFI) and then write an 800-word story about their childhood. Results showed\nthat LLM personas' self-reported BFI scores are consistent with their assigned\npersonality types, with large effect sizes found on all five traits. Moreover,\nsignificant correlations were found between assigned personality types and some\nLinguistic Inquiry and Word Count (LIWC) psycholinguistic features of their\nwritings. For instance, extroversion is associated with pro-social and active\nwords, and neuroticism is associated with words related to negative emotions\nand mental health. Besides, we only found significant differences in using\ntechnological and cultural words in writing between LLM-generated female and\nmale personas. This work provides a first step for further research on\npersonalized LLMs and their applications in Human-AI conversation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiajie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xubo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabbara_J/0/1/0/all/0/1\">Jad Kabbara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FactKG: Fact Verification via Reasoning on Knowledge Graphs. (arXiv:2305.06590v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06590","description":"<p>In real world applications, knowledge graphs (KG) are widely used in various\ndomains (e.g. medical applications and dialogue agents). However, for fact\nverification, KGs have not been adequately utilized as a knowledge source. KGs\ncan be a valuable knowledge source in fact verification due to their\nreliability and broad applicability. A KG consists of nodes and edges which\nmakes it clear how concepts are linked together, allowing machines to reason\nover chains of topics. However, there are many challenges in understanding how\nthese machine-readable concepts map to information in text. To enable the\ncommunity to better use KGs, we introduce a new dataset, FactKG: Fact\nVerification via Reasoning on Knowledge Graphs. It consists of 108k natural\nlanguage claims with five types of reasoning: One-hop, Conjunction, Existence,\nMulti-hop, and Negation. Furthermore, FactKG contains various linguistic\npatterns, including colloquial style claims as well as written style claims to\nincrease practicality. Lastly, we develop a baseline approach and analyze\nFactKG over these reasoning types. We believe FactKG can advance both\nreliability and practicality in KG-based fact verification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungjin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Yeonsu Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1\">Yohan Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1\">James Thorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Fine-Tuning with Layer Pruning on Free-Text Sequence-to-Sequence Modeling. (arXiv:2305.08285v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08285","description":"<p>The increasing size of language models raises great research interests in\nparameter-efficient fine-tuning such as LoRA that freezes the pre-trained\nmodel, and injects small-scale trainable parameters for multiple downstream\ntasks (e.g., summarization, question answering and translation). To further\nenhance the efficiency of fine-tuning, we propose a framework that integrates\nLoRA and structured layer pruning. The integrated framework is validated on two\ncreated deidentified medical report summarization datasets based on\nMIMIC-IV-Note and two public medical dialogue datasets. By tuning 0.6%\nparameters of the original model and pruning over 30% Transformer-layers, our\nframework can reduce 50% of GPU memory usage and speed up 100% of the training\nphase, while preserving over 92% generation qualities on free-text\nsequence-to-sequence tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yunqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuebing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuanyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wensheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Schema-adaptable Knowledge Graph Construction. (arXiv:2305.08703v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08703","description":"<p>Conventional Knowledge Graph Construction (KGC) approaches typically follow\nthe static information extraction paradigm with a closed set of pre-defined\nschema. As a result, such approaches fall short when applied to dynamic\nscenarios or domains, whereas a new type of knowledge emerges. This\nnecessitates a system that can handle evolving schema automatically to extract\ninformation for KGC. To address this need, we propose a new task called\nschema-adaptable KGC, which aims to continually extract entity, relation, and\nevent based on a dynamically changing schema graph without re-training. We\nfirst split and convert existing datasets based on three principles to build a\nbenchmark, i.e., horizontal schema expansion, vertical schema expansion, and\nhybrid schema expansion; then investigate the schema-adaptable performance of\nseveral well-known approaches such as Text2Event, TANL, UIE and GPT-3.5. We\nfurther propose a simple yet effective baseline dubbed AdaKGC, which contains\nschema-enriched prefix instructor and schema-conditioned dynamic decoding to\nbetter handle evolving schema. Comprehensive experimental results illustrate\nthat AdaKGC can outperform baselines but still have room for improvement. We\nhope the proposed work can deliver benefits to the community. Code and datasets\nwill be available in https://github.com/zjunlp/AdaKGC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_H/0/1/0/all/0/1\">Honghao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Easy-to-Hard Learning for Information Extraction. (arXiv:2305.09193v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.09193","description":"<p>Information extraction (IE) systems aim to automatically extract structured\ninformation, such as named entities, relations between entities, and events,\nfrom unstructured texts. While most existing work addresses a particular IE\ntask, universally modeling various IE tasks with one model has achieved great\nsuccess recently. Despite their success, they employ a one-stage learning\nstrategy, i.e., directly learning to extract the target structure given the\ninput text, which contradicts the human learning process. In this paper, we\npropose a unified easy-to-hard learning framework consisting of three stages,\ni.e., the easy stage, the hard stage, and the main stage, for IE by mimicking\nthe human learning process. By breaking down the learning process into multiple\nstages, our framework facilitates the model to acquire general IE task\nknowledge and improve its generalization ability. Extensive experiments across\nfour IE tasks demonstrate the effectiveness of our framework. We achieve new\nstate-of-the-art results on 13 out of 17 datasets. Our code is available at\n\\url{https://github.com/DAMO-NLP-SG/IE-E2H}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation. (arXiv:2305.09515v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.09515","description":"<p>Diffusion models have gained significant attention in the realm of image\ngeneration due to their exceptional performance. Their success has been\nrecently expanded to text generation via generating all tokens within a\nsequence concurrently. However, natural language exhibits a far more pronounced\nsequential dependency in comparison to images, and the majority of existing\nlanguage models are trained with a left-to-right auto-regressive approach. To\naccount for the inherent sequential characteristic of natural language, we\nintroduce Auto-Regressive Diffusion (AR-Diffusion). AR-Diffusion ensures that\nthe generation of tokens on the right depends on the generated ones on the\nleft, a mechanism achieved through employing a dynamic number of denoising\nsteps that vary based on token position. This results in tokens on the left\nundergoing fewer denoising steps than those on the right, thereby enabling them\nto generate earlier and subsequently influence the generation of tokens on the\nright. In a series of experiments on various text generation tasks, including\ntext summarization, machine translation, and common sense generation,\nAR-Diffusion clearly demonstrated its superiority over existing diffusion\nlanguage models and that it can be $100\\times\\sim600\\times$ faster when\nachieving comparable results. Our code is available at\nhttps://github.com/microsoft/ProphetNet/tree/master/AR-diffusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhihao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPL-NoViD: Context-Aware Prompt-based Learning for Norm Violation Detection in Online Communities. (arXiv:2305.09846v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.09846","description":"<p>Detecting norm violations in online communities is critical to maintaining\nhealthy and safe spaces for online discussions. Existing machine learning\napproaches often struggle to adapt to the diverse rules and interpretations\nacross different communities due to the inherent challenges of fine-tuning\nmodels for such context-specific tasks. In this paper, we introduce\nContext-aware Prompt-based Learning for Norm Violation Detection (CPL-NoViD), a\nnovel method that employs prompt-based learning to detect norm violations\nacross various types of rules. CPL-NoViD outperforms the baseline by\nincorporating context through natural language prompts and demonstrates\nimproved performance across different rule types. Significantly, it not only\nexcels in cross-rule-type and cross-community norm violation detection but also\nexhibits adaptability in few-shot learning scenarios. Most notably, it\nestablishes a new state-of-the-art in norm violation detection, surpassing\nexisting benchmarks. Our work highlights the potential of prompt-based learning\nfor context-sensitive norm violation detection and paves the way for future\nresearch on more adaptable, context-aware models to better support online\ncommunity moderators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zihao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_K/0/1/0/all/0/1\">Kristina Lerman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective. (arXiv:2305.10306v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10306","description":"<p>We propose a new paradigm for universal information extraction (IE) that is\ncompatible with any schema format and applicable to a list of IE tasks, such as\nnamed entity recognition, relation extraction, event extraction and sentiment\nanalysis. Our approach converts the text-based IE tasks as the token-pair\nproblem, which uniformly disassembles all extraction targets into joint span\ndetection, classification and association problems with a unified extractive\nframework, namely UniEX. UniEX can synchronously encode schema-based prompt and\ntextual information, and collaboratively learn the generalized knowledge from\npre-defined information using the auto-encoder language models. We develop a\ntraffine attention mechanism to integrate heterogeneous factors including\ntasks, labels and inside tokens, and obtain the extraction target via a scoring\nmatrix. Experiment results show that UniEX can outperform generative universal\nIE models in terms of performance and inference-speed on $14$ benchmarks IE\ndatasets with the supervised setting. The state-of-the-art performance in\nlow-resource scenarios also verifies the transferability and effectiveness of\nUniEX.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Ping Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Junyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_R/0/1/0/all/0/1\">Ruyi Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingjian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Large Language Models Fit For Guided Reading?. (arXiv:2305.10645v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10645","description":"<p>This paper looks at the ability of large language models to participate in\neducational guided reading. We specifically, evaluate their ability to generate\nmeaningful questions from the input text, generate diverse questions both in\nterms of content coverage and difficulty of the questions and evaluate their\nability to recommend part of the text that a student should re-read based on\nthe student's responses to the questions. Based on our evaluation of ChatGPT\nand Bard, we report that,\n</p>\n<p>1) Large language models are able to generate high quality meaningful\nquestions that have high correlation with the input text, 2) They generate\ndiverse question that cover most topics in the input text even though this\nability is significantly degraded as the input text increases, 3)The large\nlanguage models are able to generate both low and high cognitive questions even\nthough they are significantly biased toward low cognitive question, 4) They are\nable to effectively summarize responses and extract a portion of text that\nshould be re-read.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ochieng_P/0/1/0/all/0/1\">Peter Ochieng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models can be Guided to Evade AI-Generated Text Detection. (arXiv:2305.10847v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10847","description":"<p>Large Language Models (LLMs) have demonstrated exceptional performance in a\nvariety of tasks, including essay writing and question answering. However, it\nis crucial to address the potential misuse of these models, which can lead to\ndetrimental outcomes such as plagiarism and spamming. Recently, several\ndetectors have been proposed, including fine-tuned classifiers and various\nstatistical methods. In this study, we reveal that with the aid of carefully\ncrafted prompts, LLMs can effectively evade these detection systems. We propose\na novel Substitution-based In-Context example Optimization method (SICO) to\nautomatically generate such prompts. On three real-world tasks where LLMs can\nbe misused, SICO successfully enables ChatGPT to evade six existing detectors,\ncausing a significant 0.54 AUC drop on average. Surprisingly, in most cases\nthese detectors perform even worse than random classifiers. These results\nfirmly reveal the vulnerability of existing detectors. Finally, the strong\nperformance of SICO suggests itself as a reliable evaluation protocol for any\nnew detector in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_N/0/1/0/all/0/1\">Ning Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengcai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Rui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Document-Grounded Dialogue Pre-training. (arXiv:2305.10927v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10927","description":"<p>The goal of document-grounded dialogue (DocGD) is to generate a response by\ngrounding the evidence in a supporting document in accordance with the dialogue\ncontext. This process involves four variables that are causally connected.\nRecently, task-specific pre-training has greatly boosted performances on many\ndownstream tasks. Existing DocGD methods, however, continue to rely on general\npre-trained language models without a specifically tailored pre-training\napproach that explicitly captures the causal relationships. To tackle this\nissue, we are the first to present a causally-complete dataset construction\nstrategy for building million-level DocGD pre-training corpora. To better\ncapture causality, we further propose a causally-perturbed pre-training\nstrategy, which introduces causal perturbations on the variables and optimizes\nthe overall causal effect. Experiments on three benchmark datasets demonstrate\nthat our causal pre-training achieves considerable and consistent improvements\nunder fully-supervised, low-resource, few-shot, and zero-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yingxiu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bowen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Nevin L. Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making More of Little Data: Improving Low-Resource Automatic Speech Recognition Using Data Augmentation. (arXiv:2305.10951v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10951","description":"<p>The performance of automatic speech recognition (ASR) systems has advanced\nsubstantially in recent years, particularly for languages for which a large\namount of transcribed speech is available. Unfortunately, for low-resource\nlanguages, such as minority languages, regional languages or dialects, ASR\nperformance generally remains much lower. In this study, we investigate whether\ndata augmentation techniques could help improve low-resource ASR performance,\nfocusing on four typologically diverse minority languages or language variants\n(West Germanic: Gronings, West-Frisian; Malayo-Polynesian: Besemah, Nasal). For\nall four languages, we examine the use of self-training, where an ASR system\ntrained with the available human-transcribed data is used to generate\ntranscriptions, which are then combined with the original data to train a new\nASR system. For Gronings, for which there was a pre-existing text-to-speech\n(TTS) system available, we also examined the use of TTS to generate ASR\ntraining data from text-only sources. We find that using a self-training\napproach consistently yields improved performance (a relative WER reduction up\nto 20.5% compared to using an ASR system trained on 24 minutes of manually\ntranscribed speech). The performance gain from TTS augmentation for Gronings\nwas even stronger (up to 25.5% relative reduction in WER compared to a system\nbased on 24 minutes of manually transcribed speech). In sum, our results show\nthe benefit of using self-training or (if possible) TTS-generated data as an\nefficient solution to overcome the limitations of data availability for\nresource-scarce languages in order to improve ASR performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartelds_M/0/1/0/all/0/1\">Martijn Bartelds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+San_N/0/1/0/all/0/1\">Nay San</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonnell_B/0/1/0/all/0/1\">Bradley McDonnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieling_M/0/1/0/all/0/1\">Martijn Wieling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities. (arXiv:2305.11000v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11000","description":"<p>Multi-modal large language models are regarded as a crucial step towards\nArtificial General Intelligence (AGI) and have garnered significant interest\nwith the emergence of ChatGPT. However, current speech-language models\ntypically adopt the cascade paradigm, preventing inter-modal knowledge\ntransfer. In this paper, we propose SpeechGPT, a large language model with\nintrinsic cross-modal conversational abilities, capable of perceiving and\ngenerating multi-model content. With discrete speech representations, we first\nconstruct SpeechInstruct, a large-scale cross-modal speech instruction dataset.\nAdditionally, we employ a three-stage training strategy that includes\nmodality-adaptation pre-training, cross-modal instruction fine-tuning, and\nchain-of-modality instruction fine-tuning. The experimental results demonstrate\nthat SpeechGPT has an impressive capacity to follow multi-modal human\ninstructions and highlight the potential of handling multiple modalities with\none model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shimin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1\">Jun Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yaqian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taxonomy Completion with Probabilistic Scorer via Box Embedding. (arXiv:2305.11004v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11004","description":"<p>Taxonomy completion, a task aimed at automatically enriching an existing\ntaxonomy with new concepts, has gained significant interest in recent years.\nPrevious works have introduced complex modules, external information, and\npseudo-leaves to enrich the representation and unify the matching process of\nattachment and insertion. While they have achieved good performance, these\nintroductions may have brought noise and unfairness during training and\nscoring. In this paper, we present TaxBox, a novel framework for taxonomy\ncompletion that maps taxonomy concepts to box embeddings and employs two\nprobabilistic scorers for concept attachment and insertion, avoiding the need\nfor pseudo-leaves. Specifically, TaxBox consists of three components: (1) a\ngraph aggregation module to leverage the structural information of the taxonomy\nand two lightweight decoders that map features to box embedding and capture\ncomplex relationships between concepts; (2) two probabilistic scorers that\ncorrespond to attachment and insertion operations and ensure the avoidance of\npseudo-leaves; and (3) three learning objectives that assist the model in\nmapping concepts more granularly onto the box embedding space. Experimental\nresults on four real-world datasets suggest that TaxBox outperforms baseline\nmethods by a considerable margin and surpasses previous state-of-art methods to\na certain extent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Wei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yongliang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Wenqi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jietian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weiming Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-05-21T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}