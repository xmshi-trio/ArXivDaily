{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-05-17T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Watermarking Text Generated by Black-Box Language Models. (arXiv:2305.08883v1 [cs.CL])","link":"http://arxiv.org/abs/2305.08883","description":"<p>LLMs now exhibit human-like skills in various fields, leading to worries\nabout misuse. Thus, detecting generated text is crucial. However, passive\ndetection methods are stuck in domain specificity and limited adversarial\nrobustness. To achieve reliable detection, a watermark-based method was\nproposed for white-box LLMs, allowing them to embed watermarks during text\ngeneration. The method involves randomly dividing the model vocabulary to\nobtain a special list and adjusting the probability distribution to promote the\nselection of words in the list. A detection algorithm aware of the list can\nidentify the watermarked text. However, this method is not applicable in many\nreal-world scenarios where only black-box language models are available. For\ninstance, third-parties that develop API-based vertical applications cannot\nwatermark text themselves because API providers only supply generated text and\nwithhold probability distributions to shield their commercial interests. To\nallow third-parties to autonomously inject watermarks into generated text, we\ndevelop a watermarking framework for black-box language model usage scenarios.\nSpecifically, we first define a binary encoding function to compute a random\nbinary encoding corresponding to a word. The encodings computed for\nnon-watermarked text conform to a Bernoulli distribution, wherein the\nprobability of a word representing bit-1 being approximately 0.5. To inject a\nwatermark, we alter the distribution by selectively replacing words\nrepresenting bit-0 with context-based synonyms that represent bit-1. A\nstatistical test is then used to identify the watermark. Experiments\ndemonstrate the effectiveness of our method on both Chinese and English\ndatasets. Furthermore, results under re-translation, polishing, word deletion,\nand synonym substitution attacks reveal that it is arduous to remove the\nwatermark without compromising the original semantics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kejiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Han Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An assessment of measuring local levels of homelessness through proxy social media signals. (arXiv:2305.08978v1 [cs.SI])","link":"http://arxiv.org/abs/2305.08978","description":"<p>Recent studies suggest social media activity can function as a proxy for\nmeasures of state-level public health, detectable through natural language\nprocessing. We present results of our efforts to apply this approach to\nestimate homelessness at the state level throughout the US during the period\n2010-2019 and 2022 using a dataset of roughly 1 million geotagged tweets\ncontaining the substring ``homeless.'' Correlations between\nhomelessness-related tweet counts and ranked per capita homelessness volume,\nbut not general-population densities, suggest a relationship between the\nlikelihood of Twitter users to personally encounter or observe homelessness in\ntheir everyday lives and their likelihood to communicate about it online. An\nincrease to the log-odds of ``homeless'' appearing in an English-language\ntweet, as well as an acceleration in the increase in average tweet sentiment,\nsuggest that tweets about homelessness are also affected by trends at the\nnation-scale. Additionally, changes to the lexical content of tweets over time\nsuggest that reversals to the polarity of national or state-level trends may be\ndetectable through an increase in political or service-sector language over the\nsemantics of charity or direct appeals. An analysis of user account type also\nrevealed changes to Twitter-use patterns by accounts authored by individuals\nversus entities that may provide an additional signal to confirm changes to\nhomelessness density in a given jurisdiction. While a computational approach to\nsocial media analysis may provide a low-cost, real-time dataset rich with\ninformation about nationwide and localized impacts of homelessness and\nhomelessness policy, we find that practical issues abound, limiting the\npotential of social media as a proxy to complement other measures of\nhomelessness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bird_Y/0/1/0/all/0/1\">Yoshi Meke Bird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grobe_S/0/1/0/all/0/1\">Sarah E. Grobe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1\">Michael V. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_S/0/1/0/all/0/1\">Sean P. Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fudolig_M/0/1/0/all/0/1\">Mikaela I. Fudolig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmerman_J/0/1/0/all/0/1\">Julia Witte Zimmerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danforth_C/0/1/0/all/0/1\">Christopher M. Danforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodds_P/0/1/0/all/0/1\">Peter Sheridan Dodds</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Helping the Helper: Supporting Peer Counselors via AI-Empowered Practice and Feedback. (arXiv:2305.08982v1 [cs.HC])","link":"http://arxiv.org/abs/2305.08982","description":"<p>Millions of users come to online peer counseling platforms to seek support on\ndiverse topics ranging from relationship stress to anxiety. However, studies\nshow that online peer support groups are not always as effective as expected\nlargely due to users' negative experiences with unhelpful counselors. Peer\ncounselors are key to the success of online peer counseling platforms, but most\nof them often do not have systematic ways to receive guidelines or supervision.\nIn this work, we introduce CARE: an interactive AI-based tool to empower peer\ncounselors through automatic suggestion generation. During the practical\ntraining stage, CARE helps diagnose which specific counseling strategies are\nmost suitable in the given context and provides tailored example responses as\nsuggestions. Counselors can choose to select, modify, or ignore any suggestion\nbefore replying to the support seeker. Building upon the Motivational\nInterviewing framework, CARE utilizes large-scale counseling conversation data\ntogether with advanced natural language generation techniques to achieve these\nfunctionalities. We demonstrate the efficacy of CARE by performing both\nquantitative evaluations and qualitative user studies through simulated chats\nand semi-structured interviews. We also find that CARE especially helps novice\ncounselors respond better in challenging situations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_S/0/1/0/all/0/1\">Shang-Ling Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Raj Sanjay Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senthil_P/0/1/0/all/0/1\">Prathik Senthil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashktorab_Z/0/1/0/all/0/1\">Zahra Ashktorab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dugan_C/0/1/0/all/0/1\">Casey Dugan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geyer_W/0/1/0/all/0/1\">Werner Geyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It Takes Two to Tango: Navigating Conceptualizations of NLP Tasks and Measurements of Performance. (arXiv:2305.09022v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09022","description":"<p>Progress in NLP is increasingly measured through benchmarks; hence,\ncontextualizing progress requires understanding when and why practitioners may\ndisagree about the validity of benchmarks. We develop a taxonomy of\ndisagreement, drawing on tools from measurement modeling, and distinguish\nbetween two types of disagreement: 1) how tasks are conceptualized and 2) how\nmeasurements of model performance are operationalized. To provide evidence for\nour taxonomy, we conduct a meta-analysis of relevant literature to understand\nhow NLP tasks are conceptualized, as well as a survey of practitioners about\ntheir impressions of different factors that affect benchmark validity. Our\nmeta-analysis and survey across eight tasks, ranging from coreference\nresolution to question answering, uncover that tasks are generally not clearly\nand consistently conceptualized and benchmarks suffer from operationalization\ndisagreements. These findings support our proposed taxonomy of disagreement.\nFinally, based on our taxonomy, we present a framework for constructing\nbenchmarks and documenting their limitations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Subramonian_A/0/1/0/all/0/1\">Arjun Subramonian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xingdi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daume_H/0/1/0/all/0/1\">Hal Daum&#xe9; III</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blodgett_S/0/1/0/all/0/1\">Su Lin Blodgett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Soft Prompt Decoding for Multilingual Dense Retrieval. (arXiv:2305.09025v1 [cs.IR])","link":"http://arxiv.org/abs/2305.09025","description":"<p>In this work, we explore a Multilingual Information Retrieval (MLIR) task,\nwhere the collection includes documents in multiple languages. We demonstrate\nthat applying state-of-the-art approaches developed for cross-lingual\ninformation retrieval to MLIR tasks leads to sub-optimal performance. This is\ndue to the heterogeneous and imbalanced nature of multilingual collections --\nsome languages are better represented in the collection and some benefit from\nlarge-scale training data. To address this issue, we present KD-SPD, a novel\nsoft prompt decoding approach for MLIR that implicitly \"translates\" the\nrepresentation of documents in different languages into the same embedding\nspace. To address the challenges of data scarcity and imbalance, we introduce a\nknowledge distillation strategy. The teacher model is trained on rich English\nretrieval data, and by leveraging bi-text data, our distillation framework\ntransfers its retrieval knowledge to the multilingual document encoder.\nTherefore, our approach does not require any multilingual retrieval training\ndata. Extensive experiments on three MLIR datasets with a total of 15 languages\ndemonstrate that KD-SPD significantly outperforms competitive baselines in all\ncases. We conduct extensive analyses to show that our method has less language\nbias and better zero-shot transfer ability towards new languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Hansi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1\">Hamed Zamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allan_J/0/1/0/all/0/1\">James Allan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting. (arXiv:2305.09067v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09067","description":"<p>Building end-to-end task bots and maintaining their integration with new\nfunctionalities using minimal human efforts is a long-standing challenge in\ndialog research. Recently large language models (LLMs) have demonstrated\nexceptional proficiency in conversational engagement and adherence to\ninstructions across various downstream tasks. In this work, we introduce\nSGP-TOD, Schema-Guided Prompting for building Task-Oriented Dialog systems\neffortlessly based on LLMs. Utilizing the symbolic knowledge -- task schema, we\ninstruct fixed LLMs to generate appropriate responses on novel tasks,\ncircumventing the need for training data. Specifically, SGP-TOD comprises three\ncomponents: a LLM for engaging with users, a DST Prompter to aid the LLM with\ndialog state tracking, which is then used to retrieve database items, and a\nPolicy Prompter to elicit proper responses adhering to the provided dialog\npolicy. Experimental results on Multiwoz, RADDLE and STAR datasets show that\nour training-free strategy SGP-TOD, without any task-specific data, yields\nstate-of-the-art (SOTA) zero-shot performance, greatly surpasses the few-shot\napproaches. In a domain-extension setting, SGP-TOD aptly adapts to new\nfunctionalities by merely adding supplementary schema rules. We make our code\nand data publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weight-Inherited Distillation for Task-Agnostic BERT Compression. (arXiv:2305.09098v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09098","description":"<p>Knowledge Distillation (KD) is a predominant approach for BERT compression.\nPrevious KD-based methods focus on designing extra alignment losses for the\nstudent model to mimic the behavior of the teacher model. These methods\ntransfer the knowledge in an indirect way. In this paper, we propose a novel\nWeight-Inherited Distillation (WID), which directly transfers knowledge from\nthe teacher. WID does not require any additional alignment loss and trains a\ncompact student by inheriting the weights, showing a new perspective of\nknowledge distillation. Specifically, we design the row compactors and column\ncompactors as mappings and then compress the weights via structural\nre-parameterization. Experimental results on the GLUE and SQuAD benchmarks show\nthat WID outperforms previous state-of-the-art KD-based baselines. Further\nanalysis indicates that WID can also learn the attention patterns from the\nteacher model without any alignment loss on attention distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Taiqiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_C/0/1/0/all/0/1\">Cheng Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lao_S/0/1/0/all/0/1\">Shanshan Lao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiayi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_N/0/1/0/all/0/1\">Ngai Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is a Video worth $n\\times n$ Images? A Highly Efficient Approach to Transformer-based Video Question Answering. (arXiv:2305.09107v1 [cs.CV])","link":"http://arxiv.org/abs/2305.09107","description":"<p>Conventional Transformer-based Video Question Answering (VideoQA) approaches\ngenerally encode frames independently through one or more image encoders\nfollowed by interaction between frames and question. However, such schema would\nincur significant memory use and inevitably slow down the training and\ninference speed. In this work, we present a highly efficient approach for\nVideoQA based on existing vision-language pre-trained models where we\nconcatenate video frames to a $n\\times n$ matrix and then convert it to one\nimage. By doing so, we reduce the use of the image encoder from $n^{2}$ to $1$\nwhile maintaining the temporal structure of the original video. Experimental\nresults on MSRVTT and TrafficQA show that our proposed approach achieves\nstate-of-the-art performance with nearly $4\\times$ faster speed and only 30%\nmemory use. We show that by integrating our approach into VideoQA systems we\ncan achieve comparable, even superior, performance with a significant speed up\nfor training and inference. We believe the proposed approach can facilitate\nVideoQA-related research by reducing the computational requirements for those\nwho have limited access to budgets and resources. Our code will be made\npublicly available for research use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chenyang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_T/0/1/0/all/0/1\">Tianbo Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graham_Y/0/1/0/all/0/1\">Yvette Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">Jennifer Foster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-Training to Learn in Context. (arXiv:2305.09137v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09137","description":"<p>In-context learning, where pre-trained language models learn to perform tasks\nfrom task examples and instructions in their contexts, has attracted much\nattention in the NLP community. However, the ability of in-context learning is\nnot fully exploited because language models are not explicitly trained to learn\nin context. To this end, we propose PICL (Pre-training for In-Context\nLearning), a framework to enhance the language models' in-context learning\nability by pre-training the model on a large collection of \"intrinsic tasks\" in\nthe general plain-text corpus using the simple language modeling objective.\nPICL encourages the model to infer and perform tasks by conditioning on the\ncontexts while maintaining task generalization of pre-trained models. We\nevaluate the in-context learning performance of the model trained with PICL on\nseven widely-used text classification datasets and the Super-NaturalInstrctions\nbenchmark, which contains 100+ NLP tasks formulated to text generation. Our\nexperiments show that PICL is more effective and task-generalizable than a\nrange of baselines, outperforming larger language models with nearly 4x\nparameters. The code is publicly available at https://github.com/thu-coai/PICL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuxian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models. (arXiv:2305.09144v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09144","description":"<p>Memory is one of the most essential cognitive functions serving as a\nrepository of world knowledge and episodes of activities. In recent years,\nlarge-scale pre-trained language models have shown remarkable memorizing\nability. On the contrary, vanilla neural networks without pre-training have\nbeen long observed suffering from the catastrophic forgetting problem. To\ninvestigate such a retentive-forgetful contradiction and understand the memory\nmechanism of language models, we conduct thorough experiments by controlling\nthe target knowledge types, the learning strategies and the learning schedules.\nWe find that: 1) Vanilla language models are forgetful; 2) Pre-training leads\nto retentive language models; 3) Knowledge relevance and diversification\nsignificantly influence the memory formation. These conclusions are useful for\nunderstanding the abilities of pre-trained language models and shed light on\ndesigning and evaluating new learning and inference algorithms of language\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Boxi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1\">Qiaoyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianshu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Alignment Pre-training for Cross-lingual Sentence Embedding. (arXiv:2305.09148v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09148","description":"<p>Recent studies have shown that dual encoder models trained with the\nsentence-level translation ranking task are effective methods for cross-lingual\nsentence embedding. However, our research indicates that token-level alignment\nis also crucial in multilingual scenarios, which has not been fully explored\npreviously. Based on our findings, we propose a dual-alignment pre-training\n(DAP) framework for cross-lingual sentence embedding that incorporates both\nsentence-level and token-level alignment. To achieve this, we introduce a novel\nrepresentation translation learning (RTL) task, where the model learns to use\none-side contextualized token representation to reconstruct its translation\ncounterpart. This reconstruction objective encourages the model to embed\ntranslation information into the token representation. Compared to other\ntoken-level alignment methods such as translation language modeling, RTL is\nmore suitable for dual encoder architectures and is computationally efficient.\nExtensive experiments on three sentence-level cross-lingual benchmarks\ndemonstrate that our approach can significantly improve sentence embedding. Our\ncode is available at https://github.com/ChillingDream/DAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhi-Hong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Q/0/1/0/all/0/1\">Qiang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haizhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Weiwei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Translation: Improving Domain Robustness of Neural Machine Translation with Intermediate Sequences. (arXiv:2305.09154v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09154","description":"<p>Previous studies show that intermediate supervision signals benefit various\nNatural Language Processing tasks. However, it is not clear whether there exist\nintermediate signals that benefit Neural Machine Translation (NMT). Borrowing\ntechniques from Statistical Machine Translation, we propose intermediate\nsignals which are intermediate sequences from the \"source-like\" structure to\nthe \"target-like\" structure. Such intermediate sequences introduce an inductive\nbias that reflects a domain-agnostic principle of translation, which reduces\nspurious correlations that are harmful to out-of-domain generalisation.\nFurthermore, we introduce a full-permutation multi-task learning to alleviate\nthe spurious causal relations from intermediate sequences to the target, which\nresults from exposure bias. The Minimum Bayes Risk decoding algorithm is used\nto pick the best candidate translation from all permutations to further improve\nthe performance. Experiments show that the introduced intermediate signals can\neffectively improve the domain robustness of NMT and reduces the amount of\nhallucinations on out-of-domain translation. Further analysis shows that our\nmethods are especially promising in low-resource scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaojun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Speaker Disentanglement Using Unannotated External Data for Self-supervised Representation Based Voice Conversion. (arXiv:2305.09167v1 [cs.SD])","link":"http://arxiv.org/abs/2305.09167","description":"<p>Nowadays, recognition-synthesis-based methods have been quite popular with\nvoice conversion (VC). By introducing linguistics features with good\ndisentangling characters extracted from an automatic speech recognition (ASR)\nmodel, the VC performance achieved considerable breakthroughs. Recently,\nself-supervised learning (SSL) methods trained with a large-scale unannotated\nspeech corpus have been applied to downstream tasks focusing on the content\ninformation, which is suitable for VC tasks. However, a huge amount of speaker\ninformation in SSL representations degrades timbre similarity and the quality\nof converted speech significantly. To address this problem, we proposed a\nhigh-similarity any-to-one voice conversion method with the input of SSL\nrepresentations. We incorporated adversarial training mechanisms in the\nsynthesis module using external unannotated corpora. Two auxiliary\ndiscriminators were trained to distinguish whether a sequence of\nmel-spectrograms has been converted by the acoustic model and whether a\nsequence of content embeddings contains speaker information from external\ncorpora. Experimental results show that our proposed method achieves comparable\nsimilarity and higher naturalness than the supervised method, which needs a\nhuge amount of annotated corpora for training and is applicable to improve\nsimilarity for VC methods with other SSL representations as input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xintao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_Y/0/1/0/all/0/1\">Yang Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>,"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Easy-to-Hard Learning for Information Extraction. (arXiv:2305.09193v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09193","description":"<p>Information extraction (IE) systems aim to automatically extract structured\ninformation, such as named entities, relations between entities, and events,\nfrom unstructured texts. While most existing work addresses a particular IE\ntask, universally modeling various IE tasks with one model has achieved great\nsuccess recently. Despite their success, they employ a one-stage learning\nstrategy, i.e., directly learning to extract the target structure given the\ninput text, which contradicts the human learning process. In this paper, we\npropose a unified easy-to-hard learning framework consisting of three stages,\ni.e., the easy stage, the hard stage, and the main stage, for IE by mimicking\nthe human learning process. By breaking down the learning process into multiple\nstages, our framework facilitates the model to acquire general IE task\nknowledge and improve its generalization ability. Extensive experiments across\nfour IE tasks demonstrate the effectiveness of our framework. We achieve new\nstate-of-the-art results on 13 out of 17 datasets. Our code is available at\n\\url{https://github.com/DAMO-NLP-SG/IE-E2H}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Weighted M\\\"obius Score: A Unified Framework for Feature Attribution. (arXiv:2305.09204v1 [cs.LG])","link":"http://arxiv.org/abs/2305.09204","description":"<p>Feature attribution aims to explain the reasoning behind a black-box model's\nprediction by identifying the impact of each feature on the prediction. Recent\nwork has extended feature attribution to interactions between multiple\nfeatures. However, the lack of a unified framework has led to a proliferation\nof methods that are often not directly comparable. This paper introduces a\nparameterized attribution framework -- the Weighted M\\\"obius Score -- and (i)\nshows that many different attribution methods for both individual features and\nfeature interactions are special cases and (ii) identifies some new methods. By\nstudying the vector space of attribution methods, our framework utilizes\nstandard linear algebra tools and provides interpretations in various fields,\nincluding cooperative game theory and causal mediation analysis. We empirically\ndemonstrate the framework's versatility and effectiveness by applying these\nattribution methods to feature interactions in sentiment analysis and\nchain-of-thought prompting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinert_Threlkeld_S/0/1/0/all/0/1\">Shane Steinert-Threlkeld</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Speech Dialogue Translation Mediating Speakers of Different Languages. (arXiv:2305.09210v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09210","description":"<p>We present a new task, speech dialogue translation mediating speakers of\ndifferent languages. We construct the SpeechBSD dataset for the task and\nconduct baseline experiments. Furthermore, we consider context to be an\nimportant aspect that needs to be addressed in this task and propose two ways\nof utilizing context, namely monolingual context and bilingual context. We\nconduct cascaded speech translation experiments using Whisper and mBART, and\nshow that bilingual context performs better in our settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shimizu_S/0/1/0/all/0/1\">Shuichiro Shimizu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Chenhui Chu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1\">Sadao Kurohashi</a> (1 and 3) ((1) Kyoto University, Japan, (2) National Institute of Information and Communications Technology, Japan, (3) National Institute of Informatics, Japan)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Unifying Multi-Lingual and Cross-Lingual Summarization. (arXiv:2305.09220v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09220","description":"<p>To adapt text summarization to the multilingual world, previous work proposes\nmulti-lingual summarization (MLS) and cross-lingual summarization (CLS).\nHowever, these two tasks have been studied separately due to the different\ndefinitions, which limits the compatible and systematic research on both of\nthem. In this paper, we aim to unify MLS and CLS into a more general setting,\ni.e., many-to-many summarization (M2MS), where a single model could process\ndocuments in any language and generate their summaries also in any language. As\nthe first step towards M2MS, we conduct preliminary studies to show that M2MS\ncan better transfer task knowledge across different languages than MLS and CLS.\nFurthermore, we propose Pisces, a pre-trained M2MS model that learns language\nmodeling, cross-lingual ability and summarization ability via three-stage\npre-training. Experimental results indicate that our Pisces significantly\noutperforms the state-of-the-art baselines, especially in the zero-shot\ndirections, where there is no training data from the source-language documents\nto the target-language summaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1\">Duo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1\">Jianfeng Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning. (arXiv:2305.09246v1 [cs.AI])","link":"http://arxiv.org/abs/2305.09246","description":"<p>Instruction tuning for large language models (LLMs) has gained attention from\nresearchers due to its ability to unlock the potential of LLMs in following\ninstructions. While instruction tuning offers advantages for facilitating the\nadaptation of large language models (LLMs) to downstream tasks as a fine-tuning\napproach, training models with tens of millions or even billions of parameters\non large amounts of data results in unaffordable computational costs. To\naddress this, we focus on reducing the data used in LLM instruction tuning to\ndecrease training costs and improve data efficiency, dubbed as Low Training\nData Instruction Tuning (LTD Instruction Tuning). Specifically, this paper\nconducts a preliminary exploration into reducing the data used in LLM training\nand identifies several observations regarding task specialization for LLM\ntraining, such as the optimization of performance for a specific task, the\nnumber of instruction types required for instruction tuning, and the amount of\ndata required for task-specific models. The results suggest that task-specific\nmodels can be trained using less than 0.5% of the original dataset, with a 2%\nimprovement in performance over those trained on full task-related data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hantao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaomeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuetao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanggong_Y/0/1/0/all/0/1\">Yifan Yanggong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junbo Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"xPQA: Cross-Lingual Product Question Answering across 12 Languages. (arXiv:2305.09249v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09249","description":"<p>Product Question Answering (PQA) systems are key in e-commerce applications\nto provide responses to customers' questions as they shop for products. While\nexisting work on PQA focuses mainly on English, in practice there is need to\nsupport multiple customer languages while leveraging product information\navailable in English. To study this practical industrial task, we present xPQA,\na large-scale annotated cross-lingual PQA dataset in 12 languages across 9\nbranches, and report results in (1) candidate ranking, to select the best\nEnglish candidate containing the information to answer a non-English question;\nand (2) answer generation, to generate a natural-sounding non-English answer\nbased on the selected English candidate. We evaluate various approaches\ninvolving machine translation at runtime or offline, leveraging multilingual\npre-trained LMs, and including or excluding xPQA training data. We find that\n(1) In-domain data is essential as cross-lingual rankers trained on other\ndomains perform poorly on the PQA task; (2) Candidate ranking often prefers\nruntime-translation approaches while answer generation prefers multilingual\napproaches; (3) Translating offline to augment multilingual models helps\ncandidate ranking mainly on languages with non-Latin scripts; and helps answer\ngeneration mainly on languages with Latin scripts. Still, there remains a\nsignificant performance gap between the English and the cross-lingual test\nsets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaoyu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1\">Akari Asai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byrne_B/0/1/0/all/0/1\">Bill Byrne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gispert_A/0/1/0/all/0/1\">Adri&#xe0; de Gispert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyHTM: Hyperbolic Geometry based Hierarchical Topic Models. (arXiv:2305.09258v1 [cs.IR])","link":"http://arxiv.org/abs/2305.09258","description":"<p>Hierarchical Topic Models (HTMs) are useful for discovering topic hierarchies\nin a collection of documents. However, traditional HTMs often produce\nhierarchies where lowerlevel topics are unrelated and not specific enough to\ntheir higher-level topics. Additionally, these methods can be computationally\nexpensive. We present HyHTM - a Hyperbolic geometry based Hierarchical Topic\nModels - that addresses these limitations by incorporating hierarchical\ninformation from hyperbolic geometry to explicitly model hierarchies in topic\nmodels. Experimental results with four baselines show that HyHTM can better\nattend to parent-child relationships among topics. HyHTM produces coherent\ntopic hierarchies that specialise in granularity from generic higher-level\ntopics to specific lowerlevel topics. Further, our model is significantly\nfaster and leaves a much smaller memory footprint than our best-performing\nbaseline.We have made the source code for our algorithm publicly accessible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahid_S/0/1/0/all/0/1\">Simra Shahid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_T/0/1/0/all/0/1\">Tanay Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikanth_N/0/1/0/all/0/1\">Nikitha Srikanth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1\">Sumit Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1\">Balaji Krishnamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puri_N/0/1/0/all/0/1\">Nikaash Puri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ContrastNet: A Contrastive Learning Framework for Few-Shot Text Classification. (arXiv:2305.09269v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09269","description":"<p>Few-shot text classification has recently been promoted by the meta-learning\nparadigm which aims to identify target classes with knowledge transferred from\nsource classes with sets of small tasks named episodes. Despite their success,\nexisting works building their meta-learner based on Prototypical Networks are\nunsatisfactory in learning discriminative text representations between similar\nclasses, which may lead to contradictions during label prediction. In addition,\nthe tasklevel and instance-level overfitting problems in few-shot text\nclassification caused by a few training examples are not sufficiently tackled.\nIn this work, we propose a contrastive learning framework named ContrastNet to\ntackle both discriminative representation and overfitting problems in few-shot\ntext classification. ContrastNet learns to pull closer text representations\nbelonging to the same class and push away text representations belonging to\ndifferent classes, while simultaneously introducing unsupervised contrastive\nregularization at both task-level and instance-level to prevent overfitting.\nExperiments on 8 few-shot text classification datasets show that ContrastNet\noutperforms the current state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junfan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Richong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yongyi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jie Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Origins of Bias in NLP through the Lens of the Jim Code. (arXiv:2305.09281v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09281","description":"<p>In this paper, we trace the biases in current natural language processing\n(NLP) models back to their origins in racism, sexism, and homophobia over the\nlast 500 years. We review literature from critical race theory, gender studies,\ndata ethics, and digital humanities studies, and summarize the origins of bias\nin NLP models from these social science perspective. We show how the causes of\nthe biases in the NLP pipeline are rooted in social issues. Finally, we argue\nthat the only way to fix the bias and unfairness in NLP is by addressing the\nsocial problems that caused them in the first place and by incorporating social\nsciences and social scientists in efforts to mitigate bias in NLP models. We\nprovide actionable recommendations for the NLP research community to do so.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elsafoury_F/0/1/0/all/0/1\">Fatma Elsafoury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abercrombie_G/0/1/0/all/0/1\">Gavin Abercrombie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdversarialWord Dilution as Text Data Augmentation in Low-Resource Regime. (arXiv:2305.09287v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09287","description":"<p>Data augmentation is widely used in text classification, especially in the\nlow-resource regime where a few examples for each class are available during\ntraining. Despite the success, generating data augmentations as hard positive\nexamples that may increase their effectiveness is under-explored. This paper\nproposes an Adversarial Word Dilution (AWD) method that can generate hard\npositive examples as text data augmentations to train the low-resource text\nclassification model efficiently. Our idea of augmenting the text data is to\ndilute the embedding of strong positive words by weighted mixing with\nunknown-word embedding, making the augmented inputs hard to be recognized as\npositive by the classification model. We adversarially learn the dilution\nweights through a constrained min-max optimization process with the guidance of\nthe labels. Empirical studies on three benchmark datasets show that AWD can\ngenerate more effective data augmentations and outperform the state-of-the-art\ntext data augmentation methods. The additional analysis demonstrates that the\ndata augmentations generated by AWD are interpretable and can flexibly extend\nto new examples without further training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junfan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Richong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zheyan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chunming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yongyi Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniS-MMC: Multimodal Classification via Unimodality-supervised Multimodal Contrastive Learning. (arXiv:2305.09299v1 [cs.CV])","link":"http://arxiv.org/abs/2305.09299","description":"<p>Multimodal learning aims to imitate human beings to acquire complementary\ninformation from multiple modalities for various downstream tasks. However,\ntraditional aggregation-based multimodal fusion methods ignore the\ninter-modality relationship, treat each modality equally, suffer sensor noise,\nand thus reduce multimodal learning performance. In this work, we propose a\nnovel multimodal contrastive method to explore more reliable multimodal\nrepresentations under the weak supervision of unimodal predicting.\nSpecifically, we first capture task-related unimodal representations and the\nunimodal predictions from the introduced unimodal predicting task. Then the\nunimodal representations are aligned with the more effective one by the\ndesigned multimodal contrastive method under the supervision of the unimodal\npredictions. Experimental results with fused features on two image-text\nclassification benchmarks UPMC-Food-101 and N24News show that our proposed\nUnimodality-Supervised MultiModal Contrastive UniS-MMC learning method\noutperforms current state-of-the-art multimodal methods. The detailed ablation\nstudy and analysis further demonstrate the advantage of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1\">Heqing Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1\">Meng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuchen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_D/0/1/0/all/0/1\">Deepu Rajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chng_E/0/1/0/all/0/1\">Eng Siong Chng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Impact of Layer Normalization for Zero-shot Neural Machine Translation. (arXiv:2305.09312v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09312","description":"<p>This paper studies the impact of layer normalization (LayerNorm) on zero-shot\ntranslation (ZST). Recent efforts for ZST often utilize the Transformer\narchitecture as the backbone, with LayerNorm at the input of layers (PreNorm)\nset as the default. However, Xu et al. (2019) has revealed that PreNorm carries\nthe risk of overfitting the training data. Based on this, we hypothesize that\nPreNorm may overfit supervised directions and thus have low generalizability\nfor ZST. Through experiments on OPUS, IWSLT, and Europarl datasets for 54 ZST\ndirections, we demonstrate that the original Transformer setting of LayerNorm\nafter residual connections (PostNorm) consistently outperforms PreNorm by up to\n12.3 BLEU points. We then study the performance disparities by analyzing the\ndifferences in off-target rates and structural variations between PreNorm and\nPostNorm. This study highlights the need for careful consideration of the\nLayerNorm setting for ZST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhuoyuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qianying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Haiyue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Chenhui Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1\">Sadao Kurohashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid and Collaborative Passage Reranking. (arXiv:2305.09313v1 [cs.IR])","link":"http://arxiv.org/abs/2305.09313","description":"<p>In passage retrieval system, the initial passage retrieval results may be\nunsatisfactory, which can be refined by a reranking scheme. Existing solutions\nto passage reranking focus on enriching the interaction between query and each\npassage separately, neglecting the context among the top-ranked passages in the\ninitial retrieval list. To tackle this problem, we propose a Hybrid and\nCollaborative Passage Reranking (HybRank) method, which leverages the\nsubstantial similarity measurements of upstream retrievers for passage\ncollaboration and incorporates the lexical and semantic properties of sparse\nand dense retrievers for reranking. Besides, built on off-the-shelf retriever\nfeatures, HybRank is a plug-in reranker capable of enhancing arbitrary passage\nlists including previously reranked ones. Extensive experiments demonstrate the\nstable improvements of performance over prevalent retrieval and reranking\nmethods, and verify the effectiveness of the core components of HybRank.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zongmeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiaxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Keyphrase Extraction from Long Scientific Documents using Graph Embeddings. (arXiv:2305.09316v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09316","description":"<p>In this study, we investigate using graph neural network (GNN)\nrepresentations to enhance contextualized representations of pre-trained\nlanguage models (PLMs) for keyphrase extraction from lengthy documents. We show\nthat augmenting a PLM with graph embeddings provides a more comprehensive\nsemantic understanding of words in a document, particularly for long documents.\nWe construct a co-occurrence graph of the text and embed it using a graph\nconvolutional network (GCN) trained on the task of edge prediction. We propose\na graph-enhanced sequence tagging architecture that augments contextualized PLM\nembeddings with graph representations. Evaluating on benchmark datasets, we\ndemonstrate that enhancing PLMs with graph embeddings outperforms\nstate-of-the-art models on long documents, showing significant improvements in\nF1 scores across all the datasets. Our study highlights the potential of GNN\nrepresentations as a complementary approach to improve PLM performance for\nkeyphrase extraction from long documents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Cruz_R/0/1/0/all/0/1\">Roberto Mart&#xed;nez-Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahata_D/0/1/0/all/0/1\">Debanjan Mahata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Lopez_A/0/1/0/all/0/1\">Alvaro J.L&#xf3;pez-L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portela_J/0/1/0/all/0/1\">Jos&#xe9; Portela</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTTM: Leveraging Contextualized Word Embeddings from Pre-trained Language Models for Neural Topic Modeling. (arXiv:2305.09329v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09329","description":"<p>With the development of neural topic models in recent years, topic modelling\nis playing an increasingly important role in natural language understanding.\nHowever, most existing topic models still rely on bag-of-words (BoW)\ninformation, either as training input or training target. This limits their\nability to capture word order information in documents and causes them to\nsuffer from the out-of-vocabulary (OOV) issue, i.e. they cannot handle\nunobserved words in new documents. Contextualized word embeddings from\npre-trained language models show superiority in the ability of word sense\ndisambiguation and prove to be effective in dealing with OOV words. In this\nwork, we developed a novel neural topic model combining contextualized word\nembeddings from the pre-trained language model BERT. The model can infer the\ntopic distribution of a document without using any BoW information. In\naddition, the model can infer the topic distribution of each word in a document\ndirectly from the contextualized word embeddings. Experiments on several\ndatasets show that our model outperforms existing topic models in terms of both\ndocument classification and topic coherence metrics and can accommodate unseen\nwords from newly arrived documents. Experiments on the NER dataset also show\nthat our model can produce high-quality word topic representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1\">Rob Procter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Visual Understanding with Prompts for Semantic Information Disentanglement of Image. (arXiv:2305.09333v1 [cs.CV])","link":"http://arxiv.org/abs/2305.09333","description":"<p>Multi-modal visual understanding of images with prompts involves using\nvarious visual and textual cues to enhance the semantic understanding of\nimages. This approach combines both vision and language processing to generate\nmore accurate predictions and recognition of images. By utilizing prompt-based\ntechniques, models can learn to focus on certain features of an image to\nextract useful information for downstream tasks. Additionally, multi-modal\nunderstanding can improve upon single modality models by providing more robust\nrepresentations of images. Overall, the combination of visual and textual\ninformation is a promising area of research for advancing image recognition and\nunderstanding. In this paper we will try an amount of prompt design methods and\npropose a new method for better extraction of semantic information\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yuzhou Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MsPrompt: Multi-step Prompt Learning for Debiasing Few-shot Event Detection. (arXiv:2305.09335v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09335","description":"<p>Event detection (ED) is aimed to identify the key trigger words in\nunstructured text and predict the event types accordingly. Traditional ED\nmodels are too data-hungry to accommodate real applications with scarce labeled\ndata. Besides, typical ED models are facing the context-bypassing and disabled\ngeneralization issues caused by the trigger bias stemming from ED datasets.\nTherefore, we focus on the true few-shot paradigm to satisfy the low-resource\nscenarios. In particular, we propose a multi-step prompt learning model\n(MsPrompt) for debiasing few-shot event detection, that consists of the\nfollowing three components: an under-sampling module targeting to construct a\nnovel training set that accommodates the true few-shot setting, a multi-step\nprompt module equipped with a knowledge-enhanced ontology to leverage the event\nsemantics and latent prior knowledge in the PLMs sufficiently for tackling the\ncontext-bypassing problem, and a prototypical module compensating for the\nweakness of classifying events with sparse data and boost the generalization\nperformance. Experiments on two public datasets ACE-2005 and FewEvent show that\nMsPrompt can outperform the state-of-the-art models, especially in the strict\nlow-resource scenarios reporting 11.43% improvement in terms of weighted\nF1-score against the best-performing baseline and achieving an outstanding\ndebiasing performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jianming Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuejun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_F/0/1/0/all/0/1\">Fei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chengyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xueshan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing and Interpreting Causal Knowledge Graphs from News. (arXiv:2305.09359v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09359","description":"<p>Many jobs rely on news to learn about causal events in the past and present,\nto make informed decisions and predictions about the future. With the\never-increasing amount of news and text available on the internet, there is a\nneed to automate the extraction of causal events from unstructured texts. In\nthis work, we propose a methodology to construct causal knowledge graphs (KGs)\nfrom news using two steps: (1) Extraction of Causal Relations, and (2) Argument\nClustering and Representation into KG. We aim to build graphs that emphasize on\nrecall, precision and interpretability. For extraction, although many earlier\nworks already construct causal KGs from text, most adopt rudimentary\npattern-based methods. We close this gap by using the latest BERT-based\nextraction models alongside pattern-based ones. As a result, we achieved a high\nrecall, while still maintaining a high precision. For clustering, we utilized a\ntopic modelling approach to cluster our arguments, so as to increase the\nconnectivity of our graph. As a result, instead of 15,686 disconnected\nsubgraphs, we were able to obtain 1 connected graph that enables users to infer\nmore causal relationships from. Our final KG effectively captures and conveys\ncausal relationships, validated through multiple use cases and user feedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1\">Fiona Anting Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_D/0/1/0/all/0/1\">Debdeep Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamaura_S/0/1/0/all/0/1\">Sahim Yamaura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koji_M/0/1/0/all/0/1\">Miura Koji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1\">See-Kiong Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GIFT: Graph-Induced Fine-Tuning for Multi-Party Conversation Understanding. (arXiv:2305.09360v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09360","description":"<p>Addressing the issues of who saying what to whom in multi-party conversations\n(MPCs) has recently attracted a lot of research attention. However, existing\nmethods on MPC understanding typically embed interlocutors and utterances into\nsequential information flows, or utilize only the superficial of inherent graph\nstructures in MPCs. To this end, we present a plug-and-play and lightweight\nmethod named graph-induced fine-tuning (GIFT) which can adapt various\nTransformer-based pre-trained language models (PLMs) for universal MPC\nunderstanding. In detail, the full and equivalent connections among utterances\nin regular Transformer ignore the sparse but distinctive dependency of an\nutterance on another in MPCs. To distinguish different relationships between\nutterances, four types of edges are designed to integrate graph-induced signals\ninto attention mechanisms to refine PLMs originally designed for processing\nsequential texts. We evaluate GIFT by implementing it into three PLMs, and test\nthe performance on three downstream tasks including addressee recognition,\nspeaker identification and response selection. Experimental results show that\nGIFT can significantly improve the performance of three PLMs on three\ndownstream tasks and two benchmarks with only 4 additional parameters per\nencoding layer, achieving new state-of-the-art performance on MPC\nunderstanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jia-Chen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhen-Hua Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1\">Guoping Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Multi-Granular Rationale Extraction for Explainable Multi-hop Fact Verification. (arXiv:2305.09400v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09400","description":"<p>The success of deep learning models on multi-hop fact verification has\nprompted researchers to understand the behavior behind their veracity. One\npossible way is erasure search: obtaining the rationale by entirely removing a\nsubset of input without compromising the veracity prediction. Although\nextensively explored, existing approaches fall within the scope of the\nsingle-granular (tokens or sentences) explanation, which inevitably leads to\nexplanation redundancy and inconsistency. To address such issues, this paper\nexplores the viability of multi-granular rationale extraction with consistency\nand faithfulness for explainable multi-hop fact verification. In particular,\ngiven a pretrained veracity prediction model, both the token-level explainer\nand sentence-level explainer are trained simultaneously to obtain\nmulti-granular rationales via differentiable masking. Meanwhile, three\ndiagnostic properties (fidelity, consistency, salience) are introduced and\napplied to the training process, to ensure that the extracted rationales\nsatisfy faithfulness and consistency. Experimental results on three multi-hop\nfact verification datasets show that the proposed approach outperforms some\nstate-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_J/0/1/0/all/0/1\">Jiasheng Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yingjie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Deyu Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Preliminary Analysis on the Code Generation Capabilities of GPT-3.5 and Bard AI Models for Java Functions. (arXiv:2305.09402v1 [cs.SE])","link":"http://arxiv.org/abs/2305.09402","description":"<p>This paper evaluates the capability of two state-of-the-art artificial\nintelligence (AI) models, GPT-3.5 and Bard, in generating Java code given a\nfunction description. We sourced the descriptions from CodingBat.com, a popular\nonline platform that provides practice problems to learn programming. We\ncompared the Java code generated by both models based on correctness, verified\nthrough the platform's own test cases. The results indicate clear differences\nin the capabilities of the two models. GPT-3.5 demonstrated superior\nperformance, generating correct code for approximately 90.6% of the function\ndescriptions, whereas Bard produced correct code for 53.1% of the functions.\nWhile both models exhibited strengths and weaknesses, these findings suggest\npotential avenues for the development and refinement of more advanced\nAI-assisted code generation tools. The study underlines the potential of AI in\nautomating and supporting aspects of software development, although further\nresearch is required to fully realize this potential.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Destefanis_G/0/1/0/all/0/1\">Giuseppe Destefanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartolucci_S/0/1/0/all/0/1\">Silvia Bartolucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortu_M/0/1/0/all/0/1\">Marco Ortu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"About Evaluation of F1 Score for RECENT Relation Extraction System. (arXiv:2305.09410v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09410","description":"<p>This document contains a discussion of the F1 score evaluation used in the\narticle 'Relation Classification with Entity Type Restriction' by Shengfei Lyu,\nHuanhuan Chen published on Findings of the Association for Computational\nLinguistics: ACL-IJCNLP 2021. The authors created a system named RECENT and\nclaim it achieves (then) a new state-of-the-art result 75.2 (previous 74.8) on\nthe TACRED dataset, while after correcting errors and reevaluation the final\nresult is 65.16\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Olek_M/0/1/0/all/0/1\">Micha&#x142; Olek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MPI-rical: Data-Driven MPI Distributed Parallelism Assistance with Transformers. (arXiv:2305.09438v1 [cs.DC])","link":"http://arxiv.org/abs/2305.09438","description":"<p>Automatic source-to-source parallelization of serial code for shared and\ndistributed memory systems is a challenging task in high-performance computing.\nWhile many attempts were made to translate serial code into parallel code for a\nshared memory environment (usually using OpenMP), none has managed to do so for\na distributed memory environment. In this paper, we propose a novel approach,\ncalled MPI-rical, for automated MPI code generation using a transformer-based\nmodel trained on approximately 25,000 serial code snippets and their\ncorresponding parallelized MPI code out of more than 50,000 code snippets in\nour corpus (MPICodeCorpus). To evaluate the performance of the model, we first\nbreak down the serial code to MPI-based parallel code translation problem into\ntwo sub-problems and develop two research objectives: code completion defined\nas given a location in the source code, predict the MPI function for that\nlocation, and code translation defined as predicting an MPI function as well as\nits location in the source code. We evaluate MPI-rical on MPICodeCorpus dataset\nand on real-world scientific code benchmarks and compare its performance\nbetween the code completion and translation tasks. Our experimental results\nshow that while MPI-rical performs better on the code completion task than the\ncode translation task, the latter is better suited for real-world programming\nassistance, in which the tool suggests the need for an MPI function regardless\nof prior knowledge. Overall, our approach represents a significant step forward\nin automating the parallelization of serial code for distributed memory\nsystems, which can save valuable time and resources for software developers and\nresearchers. The source code used in this work, as well as other relevant\nsources, are available at:\nhttps://github.com/Scientific-Computing-Lab-NRCN/MPI-rical\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nadav Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadosh_T/0/1/0/all/0/1\">Tal Kadosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasabnis_N/0/1/0/all/0/1\">Niranjan Hasabnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattson_T/0/1/0/all/0/1\">Timothy Mattson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1\">Yuval Pinter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oren_G/0/1/0/all/0/1\">Gal Oren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fuzzy Temporal Protoforms for the Quantitative Description of Processes in Natural Language. (arXiv:2305.09506v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09506","description":"<p>In this paper, we propose a series of fuzzy temporal protoforms in the\nframework of the automatic generation of quantitative and qualitative natural\nlanguage descriptions of processes. The model includes temporal and causal\ninformation from processes and attributes, quantifies attributes in time during\nthe process life-span and recalls causal relations and temporal distances\nbetween events, among other features. Through integrating process mining\ntechniques and fuzzy sets within the usual Data-to-Text architecture, our\nframework is able to extract relevant quantitative temporal as well as\nstructural information from a process and describe it in natural language\ninvolving uncertain terms. A real use-case in the cardiology domain is\npresented, showing the potential of our model for providing natural language\nexplanations addressed to domain experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fontenla_Seco_Y/0/1/0/all/0/1\">Yago Fontenla-Seco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugarin_Diz_A/0/1/0/all/0/1\">Alberto Bugar&#xed;n-Diz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lama_M/0/1/0/all/0/1\">Manuel Lama</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bidirectional Generative Framework for Cross-domain Aspect-based Sentiment Analysis. (arXiv:2305.09509v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09509","description":"<p>Cross-domain aspect-based sentiment analysis (ABSA) aims to perform various\nfine-grained sentiment analysis tasks on a target domain by transferring\nknowledge from a source domain. Since labeled data only exists in the source\ndomain, a model is expected to bridge the domain gap for tackling cross-domain\nABSA. Though domain adaptation methods have proven to be effective, most of\nthem are based on a discriminative model, which needs to be specifically\ndesigned for different ABSA tasks. To offer a more general solution, we propose\na unified bidirectional generative framework to tackle various cross-domain\nABSA tasks. Specifically, our framework trains a generative model in both\ntext-to-label and label-to-text directions. The former transforms each task\ninto a unified format to learn domain-agnostic features, and the latter\ngenerates natural sentences from noisy labels for data augmentation, with which\na more accurate model can be trained. To investigate the effectiveness and\ngenerality of our framework, we conduct extensive experiments on four\ncross-domain ABSA tasks and present new state-of-the-art results on all tasks.\nOur data and code are publicly available at\n\\url{https://github.com/DAMO-NLP-SG/BGCA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yue Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Sinno Jialin Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation. (arXiv:2305.09515v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09515","description":"<p>Diffusion models have gained significant attention in the realm of image\ngeneration due to their exceptional performance. Their success has been\nrecently expanded to text generation via generating all tokens within a\nsequence concurrently. However, natural language exhibits a far more pronounced\nsequential dependency in comparison to images, and the majority of existing\nlanguage models are trained utilizing a left-to-right auto-regressive approach.\nTo account for the inherent sequential characteristic of natural language, we\nintroduce Auto-Regressive Diffusion (AR-Diffusion). AR-Diffusion ensures that\nthe generation of tokens on the right depends on the generated ones on the\nleft, a mechanism achieved through employing a dynamic number of denoising\nsteps that vary based on token position. This results in tokens on the left\nundergoing fewer denoising steps than those on the right, thereby enabling them\nto generate earlier and subsequently influence the generation of tokens on the\nright. In a series of experiments on various text generation tasks including\ntext summarization, machine translation, and common sense generation,\nAR-Diffusion clearly demonstrated the superiority over existing diffusion\nlanguage models and that it can be $100\\times\\sim600\\times$ faster when\nachieving comparable results. Our code will be publicly released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhihao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DLUE: Benchmarking Document Language Understanding. (arXiv:2305.09520v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09520","description":"<p>Understanding documents is central to many real-world tasks but remains a\nchallenging topic. Unfortunately, there is no well-established consensus on how\nto comprehensively evaluate document understanding abilities, which\nsignificantly hinders the fair comparison and measuring the progress of the\nfield. To benchmark document understanding researches, this paper summarizes\nfour representative abilities, i.e., document classification, document\nstructural analysis, document information extraction, and document\ntranscription. Under the new evaluation framework, we propose \\textbf{Document\nLanguage Understanding Evaluation} -- \\textbf{DLUE}, a new task suite which\ncovers a wide-range of tasks in various forms, domains and document genres. We\nalso systematically evaluate six well-established transformer models on DLUE,\nand find that due to the lengthy content, complicated underlying structure and\ndispersed knowledge, document understanding is still far from being solved, and\ncurrently there is no neural architecture that dominates all tasks, raising\nrequirements for a universal document understanding architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruoxi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_X/0/1/0/all/0/1\">Xinyan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yingfei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaSRL++: A Uniform Scheme for Modelling Deeper Semantics. (arXiv:2305.09534v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09534","description":"<p>Despite enormous progress in Natural Language Processing (NLP), our field is\nstill lacking a common deep semantic representation scheme. As a result, the\nproblem of meaning and understanding is typically sidestepped through more\nsimple, approximative methods. This paper argues that in order to arrive at\nsuch a scheme, we also need a common modelling scheme. It therefore introduces\nMetaSRL++, a uniform, language- and modality-independent modelling scheme based\non Semantic Graphs, as a step towards a common representation scheme; as well\nas a method for defining the concepts and entities that are used in these\ngraphs. Our output is twofold. First, we illustrate MetaSRL++ through concrete\nexamples. Secondly, we discuss how it relates to existing work in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hohl_F/0/1/0/all/0/1\">Fritz Hohl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1\">Nianheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galetti_M/0/1/0/all/0/1\">Martina Galetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trijp_R/0/1/0/all/0/1\">Remi van Trijp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Stereotypes using Entity-Centric Data. (arXiv:2305.09548v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09548","description":"<p>Stereotypes inform how we present ourselves and others, and in turn how we\nbehave. They are thus important to measure. Recent work has used projections of\nembeddings from Distributional Semantic Models (DSMs), such as BERT, to perform\nthese measurements. However, DSMs capture cognitive associations that are not\nnecessarily relevant to the interpersonal nature of stereotyping. Here, we\npropose and evaluate three novel, entity-centric methods for learning\nstereotypes from Twitter and Wikipedia biographies. Models are trained by\nleveraging the fact that multiple phrases are applied to the same person,\nmagnifying the person-centric nature of the learned associations. We show that\nthese models outperform existing approaches to stereotype measurement with\nrespect to 1) predicting which identities people apply to themselves and\nothers, and 2) quantifying stereotypes on salient social dimensions (e.g.\ngender). Via a case study, we also show the utility of these models for future\nquestions in computational social science.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madani_N/0/1/0/all/0/1\">Navid Madani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandyopadhyay_R/0/1/0/all/0/1\">Rabiraj Bandyopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoder_M/0/1/0/all/0/1\">Michael Miller Yoder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_K/0/1/0/all/0/1\">Kenneth Joseph</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Life of PII -- A PII Obfuscation Transformer. (arXiv:2305.09550v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09550","description":"<p>Protecting sensitive information is crucial in today's world of Large\nLanguage Models (LLMs) and data-driven services. One common method used to\npreserve privacy is by using data perturbation techniques to reduce\noverreaching utility of (sensitive) Personal Identifiable Information (PII)\ndata while maintaining its statistical and semantic properties. Data\nperturbation methods often result in significant information loss, making them\nimpractical for use. In this paper, we propose 'Life of PII', a novel\nObfuscation Transformer framework for transforming PII into faux-PII while\npreserving the original information, intent, and context as much as possible.\nOur approach includes an API to interface with the given document, a\nconfiguration-based obfuscator, and a model based on the Transformer\narchitecture, which has shown high context preservation and performance in\nnatural language processing tasks and LLMs.\n</p>\n<p>Our Transformer-based approach learns mapping between the original PII and\nits transformed faux-PII representation, which we call \"obfuscated\" data. Our\nexperiments demonstrate that our method, called Life of PII, outperforms\ntraditional data perturbation techniques in terms of both utility preservation\nand privacy protection. We show that our approach can effectively reduce\nutility loss while preserving the original information, offering greater\nflexibility in the trade-off between privacy protection and data utility. Our\nwork provides a solution for protecting PII in various real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deshmukh_A/0/1/0/all/0/1\">Ajinkya Deshmukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banthia_S/0/1/0/all/0/1\">Saumya Banthia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Anantha Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting Sentence Transformers for the Aviation Domain. (arXiv:2305.09556v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09556","description":"<p>Learning effective sentence representations is crucial for many Natural\nLanguage Processing (NLP) tasks, including semantic search, semantic textual\nsimilarity (STS), and clustering. While multiple transformer models have been\ndeveloped for sentence embedding learning, these models may not perform\noptimally when dealing with specialized domains like aviation, which has unique\ncharacteristics such as technical jargon, abbreviations, and unconventional\ngrammar. Furthermore, the absence of labeled datasets makes it difficult to\ntrain models specifically for the aviation domain. To address these challenges,\nwe propose a novel approach for adapting sentence transformers for the aviation\ndomain. Our method is a two-stage process consisting of pre-training followed\nby fine-tuning. During pre-training, we use Transformers and Sequential\nDenoising AutoEncoder (TSDAE) with aviation text data as input to improve the\ninitial model performance. Subsequently, we fine-tune our models using a\nNatural Language Inference (NLI) dataset in the Sentence Bidirectional Encoder\nRepresentations from Transformers (SBERT) architecture to mitigate overfitting\nissues. Experimental results on several downstream tasks show that our adapted\nsentence transformers significantly outperform general-purpose transformers,\ndemonstrating the effectiveness of our approach in capturing the nuances of the\naviation domain. Overall, our work highlights the importance of domain-specific\nadaptation in developing high-quality NLP solutions for specialized industries\nlike aviation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liya Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_J/0/1/0/all/0/1\">Jason Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouck_D/0/1/0/all/0/1\">Dave Rouck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tien_A/0/1/0/all/0/1\">Alex Tien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_D/0/1/0/all/0/1\">Diane M Baumgartner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UOR: Universal Backdoor Attacks on Pre-trained Language Models. (arXiv:2305.09574v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09574","description":"<p>Backdoors implanted in pre-trained language models (PLMs) can be transferred\nto various downstream tasks, which exposes a severe security threat. However,\nmost existing backdoor attacks against PLMs are un-targeted and task-specific.\nFew targeted and task-agnostic methods use manually pre-defined triggers and\noutput representations, which prevent the attacks from being more effective and\ngeneral. In this paper, we first summarize the requirements that a more\nthreatening backdoor attack against PLMs should satisfy, and then propose a new\nbackdoor attack method called UOR, which breaks the bottleneck of the previous\napproach by turning manual selection into automatic optimization. Specifically,\nwe define poisoned supervised contrastive learning which can automatically\nlearn the more uniform and universal output representations of triggers for\nvarious PLMs. Moreover, we use gradient search to select appropriate trigger\nwords which can be adaptive to different PLMs and vocabularies. Experiments\nshow that our method can achieve better attack performance on various text\nclassification tasks compared to manual methods. Further, we tested our method\non PLMs with different architectures, different usage paradigms, and more\ndifficult tasks, which demonstrated the universality of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boqun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haodong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Gongshen Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Event Extraction with Denoised Structure-to-Text Augmentation. (arXiv:2305.09598v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09598","description":"<p>Event extraction aims to recognize pre-defined event triggers and arguments\nfrom texts, which suffer from the lack of high-quality annotations. In most NLP\napplications, involving a large scale of synthetic training data is a practical\nand effective approach to alleviate the problem of data scarcity. However, when\napplying to the task of event extraction, recent data augmentation methods\noften neglect the problem of grammatical incorrectness, structure misalignment,\nand semantic drifting, leading to unsatisfactory performances. In order to\nsolve these problems, we propose a denoised structure-to-text augmentation\nframework for event extraction DAEE, which generates additional training data\nthrough the knowledge-based structure-to-text generation model and selects the\neffective subset from the generated data iteratively with a deep reinforcement\nlearning agent. Experimental results on several datasets demonstrate that the\nproposed method generates more diverse text representations for event\nextraction and achieves comparable results with the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+wang_b/0/1/0/all/0/1\">bo wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaochi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1\">Ge Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuaiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dawei Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are Built-in Autoregressive Search Engines. (arXiv:2305.09612v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09612","description":"<p>Document retrieval is a key stage of standard Web search engines. Existing\ndual-encoder dense retrievers obtain representations for questions and\ndocuments independently, allowing for only shallow interactions between them.\nTo overcome this limitation, recent autoregressive search engines replace the\ndual-encoder architecture by directly generating identifiers for relevant\ndocuments in the candidate pool. However, the training cost of such\nautoregressive search engines rises sharply as the number of candidate\ndocuments increases. In this paper, we find that large language models (LLMs)\ncan follow human instructions to directly generate URLs for document retrieval.\n</p>\n<p>Surprisingly, when providing a few {Query-URL} pairs as in-context\ndemonstrations, LLMs can generate Web URLs where nearly 90\\% of the\ncorresponding documents contain correct answers to open-domain questions. In\nthis way, LLMs can be thought of as built-in search engines, since they have\nnot been explicitly trained to map questions to document identifiers.\nExperiments demonstrate that our method can consistently achieve better\nretrieval performance than existing retrieval approaches by a significant\nmargin on three open-domain question answering benchmarks, under both zero and\nfew-shot settings. The code for this work can be found at\n\\url{https://github.com/Ziems/llm-url}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ziems_N/0/1/0/all/0/1\">Noah Ziems</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Expert-Level Medical Question Answering with Large Language Models. (arXiv:2305.09617v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09617","description":"<p>Recent artificial intelligence (AI) systems have reached milestones in \"grand\nchallenges\" ranging from Go to protein-folding. The capability to retrieve\nmedical knowledge, reason over it, and answer medical questions comparably to\nphysicians has long been viewed as one such grand challenge.\n</p>\n<p>Large language models (LLMs) have catalyzed significant progress in medical\nquestion answering; Med-PaLM was the first model to exceed a \"passing\" score in\nUS Medical Licensing Examination (USMLE) style questions with a score of 67.2%\non the MedQA dataset. However, this and other prior work suggested significant\nroom for improvement, especially when models' answers were compared to\nclinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by\nleveraging a combination of base LLM improvements (PaLM 2), medical domain\nfinetuning, and prompting strategies including a novel ensemble refinement\napproach.\n</p>\n<p>Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM\nby over 19% and setting a new state-of-the-art. We also observed performance\napproaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU\nclinical topics datasets.\n</p>\n<p>We performed detailed human evaluations on long-form questions along multiple\naxes relevant to clinical applications. In pairwise comparative ranking of 1066\nconsumer medical questions, physicians preferred Med-PaLM 2 answers to those\nproduced by physicians on eight of nine axes pertaining to clinical utility (p\n&lt; 0.001). We also observed significant improvements compared to Med-PaLM on\nevery evaluation axis (p &lt; 0.001) on newly introduced datasets of 240 long-form\n\"adversarial\" questions to probe LLM limitations.\n</p>\n<p>While further studies are necessary to validate the efficacy of these models\nin real-world settings, these results highlight rapid progress towards\nphysician-level performance in medical question answering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1\">Karan Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_T/0/1/0/all/0/1\">Tao Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gottweis_J/0/1/0/all/0/1\">Juraj Gottweis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayres_R/0/1/0/all/0/1\">Rory Sayres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wulczyn_E/0/1/0/all/0/1\">Ellery Wulczyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Le Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_K/0/1/0/all/0/1\">Kevin Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfohl_S/0/1/0/all/0/1\">Stephen Pfohl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cole_Lewis_H/0/1/0/all/0/1\">Heather Cole-Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neal_D/0/1/0/all/0/1\">Darlene Neal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaekermann_M/0/1/0/all/0/1\">Mike Schaekermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Amy Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1\">Mohamed Amin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lachgar_S/0/1/0/all/0/1\">Sami Lachgar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansfield_P/0/1/0/all/0/1\">Philip Mansfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1\">Sushant Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_B/0/1/0/all/0/1\">Bradley Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dominowska_E/0/1/0/all/0/1\">Ewa Dominowska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arcas_B/0/1/0/all/0/1\">Blaise Aguera y Arcas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomasev_N/0/1/0/all/0/1\">Nenad Tomasev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_R/0/1/0/all/0/1\">Renee Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semturs_C/0/1/0/all/0/1\">Christopher Semturs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_S/0/1/0/all/0/1\">S. Sara Mahdavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barral_J/0/1/0/all/0/1\">Joelle Barral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webster_D/0/1/0/all/0/1\">Dale Webster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corrado_G/0/1/0/all/0/1\">Greg S. Corrado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matias_Y/0/1/0/all/0/1\">Yossi Matias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azizi_S/0/1/0/all/0/1\">Shekoofeh Azizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karthikesalingam_A/0/1/0/all/0/1\">Alan Karthikesalingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_V/0/1/0/all/0/1\">Vivek Natarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys. (arXiv:2305.09620v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09620","description":"<p>How can we use large language models (LLMs) to augment surveys? This paper\ninvestigates three distinct applications of LLMs fine-tuned by nationally\nrepresentative surveys for opinion prediction -- missing data imputation,\nretrodiction, and zero-shot prediction. We present a new methodological\nframework that incorporates neural embeddings of survey questions, individual\nbeliefs, and temporal contexts to personalize LLMs in opinion prediction. Among\n3,110 binarized opinions from 68,846 Americans in the General Social Survey\nfrom 1972 to 2021, our best models based on Alpaca-7b excels in missing data\nimputation (AUC = 0.87 for personal opinion prediction and $\\rho$ = 0.99 for\npublic opinion prediction) and retrodiction (AUC = 0.86, $\\rho$ = 0.98). These\nremarkable prediction capabilities allow us to fill in missing trends with high\nconfidence and pinpoint when public attitudes changed, such as the rising\nsupport for same-sex marriage. However, the models show limited performance in\na zero-shot prediction task (AUC = 0.73, $\\rho$ = 0.67), highlighting\nchallenges presented by LLMs without human responses. Further, we find that the\nbest models' accuracy is lower for individuals with low socioeconomic status,\nracial minorities, and non-partisan affiliations but higher for ideologically\nsorted opinions in contemporary periods. We discuss practical constraints,\nsocio-demographic representation, and ethical concerns regarding individual\nautonomy and privacy when using LLMs for opinion prediction. This paper\nshowcases a new approach for leveraging LLMs to enhance nationally\nrepresentative surveys by predicting missing responses and trends.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junsol Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Byungkyu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StructGPT: A General Framework for Large Language Model to Reason over Structured Data. (arXiv:2305.09645v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09645","description":"<p>In this paper, we study how to improve the zero-shot reasoning ability of\nlarge language models~(LLMs) over structured data in a unified way. Inspired by\nthe study on tool augmentation for LLMs, we develop an \\emph{Iterative\nReading-then-Reasoning~(IRR)} approach for solving question answering tasks\nbased on structured data, called \\textbf{StructGPT}. In our approach, we\nconstruct the specialized function to collect relevant evidence from structured\ndata (\\ie \\emph{reading}), and let LLMs concentrate the reasoning task based on\nthe collected information (\\ie \\emph{reasoning}). Specially, we propose an\n\\emph{invoking-linearization-generation} procedure to support LLMs in reasoning\non the structured data with the help of the external interfaces. By iterating\nthis procedures with provided interfaces, our approach can gradually approach\nthe target answer to a given query. Extensive experiments conducted on three\ntypes of structured data demonstrate the effectiveness of our approach, which\ncan significantly boost the performance of ChatGPT and achieve comparable\nperformance against the full-data supervised-tuning baselines. Our codes and\ndata are publicly available at~\\url{https://github.com/RUCAIBox/StructGPT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jinhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zican Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_K/0/1/0/all/0/1\">Keming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation. (arXiv:2305.09651v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09651","description":"<p>It has been commonly observed that a teacher model with superior performance\ndoes not necessarily result in a stronger student, highlighting a discrepancy\nbetween current teacher training practices and effective knowledge transfer. In\norder to enhance the guidance of the teacher training process, we introduce the\nconcept of distillation influence to determine the impact of distillation from\neach training sample on the student's generalization ability. In this paper, we\npropose Learning Good Teacher Matters (LGTM), an efficient training technique\nfor incorporating distillation influence into the teacher's learning process.\nBy prioritizing samples that are likely to enhance the student's generalization\nability, our LGTM outperforms 10 common knowledge distillation baselines on 6\ntext classification tasks in the GLUE benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yuxin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zihan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xingjian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Interpreter Understands Your Meaning: End-to-end Spoken Language Understanding Aided by Speech Translation. (arXiv:2305.09652v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09652","description":"<p>End-to-end spoken language understanding (SLU) remains elusive even with\ncurrent large pretrained language models on text and speech, especially in\nmultilingual cases. Machine translation has been established as a powerful\npretraining objective on text as it enables the model to capture high-level\nsemantics of the input utterance and associations between different languages,\nwhich is desired for speech models that work on lower-level acoustic frames.\nMotivated particularly by the task of cross-lingual SLU, we demonstrate that\nthe task of speech translation (ST) is a good means of pretraining speech\nmodels for end-to-end SLU on both monolingual and cross-lingual scenarios.\n</p>\n<p>By introducing ST, our models give higher performance over current baselines\non monolingual and multilingual intent classification as well as spoken\nquestion answering using SLURP, MINDS-14, and NMSQA benchmarks. To verify the\neffectiveness of our methods, we also release two new benchmark datasets from\nboth synthetic and real sources, for the tasks of abstractive summarization\nfrom speech and low-resource or zero-shot transfer from English to French. We\nfurther show the value of preserving knowledge from the pretraining task, and\nexplore Bayesian transfer learning on pretrained speech models based on\ncontinual learning regularizers for that.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mutian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garner_P/0/1/0/all/0/1\">Philip N. Garner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Satisfiability-Aided Language Models Using Declarative Prompting. (arXiv:2305.09656v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09656","description":"<p>Prior work has combined chain-of-thought prompting in large language models\n(LLMs) with programmatic representations to perform effective and transparent\nreasoning. While such an approach works very well for tasks that only require\nforward reasoning (e.g., straightforward arithmetic), it is less effective for\nconstraint solving tasks that require more sophisticated planning and search.\nIn this paper, we propose a new satisfiability-aided language modeling approach\nfor improving the reasoning capabilities of LLMs. We use an LLM to generate a\ndeclarative task specification rather than an imperative program and leverage\nan off-the-shelf automated theorem prover to derive the final answer. This\napproach has two key advantages. The declarative specification is closer to the\nproblem description than the reasoning steps are, so the LLM can parse it more\naccurately. Furthermore, by offloading the actual reasoning task to an\nautomated theorem prover, our approach can guarantee the correctness of the\nanswer with respect to the parsed specification and avoid planning errors in\nthe reasoning process. We evaluate SATLM on 6 different datasets and show that\nit consistently outperforms program-aided LMs in an imperative paradigm\n(PROGLM). In particular, SATLM outperforms PROGLM by 23% on a challenging\nsubset of GSM; SATLM also achieves a new SoTA on LSAT, surpassing previous\nmodels that are trained on the full training set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiaochu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dillig_I/0/1/0/all/0/1\">Isil Dillig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Korean Corpora: A Practical Report. (arXiv:2012.15621v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15621","description":"<p>Korean is often referred to as a low-resource language in the research\ncommunity. While this claim is partially true, it is also because the\navailability of resources is inadequately advertised and curated. This work\ncurates and reviews a list of Korean corpora, first describing\ninstitution-level resource development, then further iterate through a list of\ncurrent open datasets for different types of tasks. We then propose a direction\non how open-source dataset construction and releases should be done for\nless-resourced languages to promote research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_W/0/1/0/all/0/1\">Won Ik Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Sangwhan Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Youngsook Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v6 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2108.08614","description":"<p>Question answering over knowledge graphs and other RDF data has been greatly\nadvanced, with a number of good systems providing crisp answers for natural\nlanguage questions or telegraphic queries. Some of these systems incorporate\ntextual sources as additional evidence for the answering process, but cannot\ncompute answers that are present in text alone. Conversely, systems from the IR\nand NLP communities have addressed QA over text, but such systems barely\nutilize semantic data and knowledge. This paper presents a method for complex\nquestions that can seamlessly operate over a mixture of RDF datasets and text\ncorpora, or individual sources, in a unified framework. Our method, called\nUNIQORN, builds a context graph on-the-fly, by retrieving question-relevant\nevidences from the RDF data and/or a text corpus, using fine-tuned BERT models.\nThe resulting graph is typically contains all question-relevant evidences but\nalso a lot of noise. UNIQORN copes with this input by a graph algorithm for\nGroup Steiner Trees, that identifies the best answer candidates in the context\ngraph. Experimental results on several benchmarks of complex questions with\nmultiple entities and relations, show that UNIQORN significantly outperforms\nstate-of-the-art methods for heterogeneous QA. The graph-based methodology\nprovides user-interpretable evidence for the complete answering process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pramanik_S/0/1/0/all/0/1\">Soumajit Pramanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Implicit Sentiment Learning via Local Sentiment Aggregation. (arXiv:2110.08604v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08604","description":"<p>Aspect-based sentiment classification (ABSC) has revealed the potential\ndependency of sentiment polarities among different aspects. Our study further\nexplores this phenomenon, positing that adjacent aspects often exhibit similar\nsentiments, a concept we term \"aspect sentiment coherency.\" We argue that the\ncurrent research landscape has not fully appreciated the significance of\nmodeling aspect sentiment coherency. To address this gap, we introduce a local\nsentiment aggregation paradigm (LSA) that facilitates fine-grained sentiment\ncoherency modeling. This approach enables the extraction of implicit sentiments\nfor aspects lacking explicit sentiment descriptions. Leveraging gradient\ndescent, we design a differential-weighted sentiment aggregation window that\nguides the modeling of aspect sentiment coherency. Experimental results affirm\nthe efficacy of LSA in learning sentiment coherency, as it achieves\nstate-of-the-art performance across three public datasets, thus significantly\nenhancing existing ABSC models. We have made our code available, providing a\nready tool for existing methods to harness the potential of sentiment coherency\ninformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Heng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faking Fake News for Real Fake News Detection: Propaganda-loaded Training Data Generation. (arXiv:2203.05386v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.05386","description":"<p>Despite recent advances in detecting fake news generated by neural models,\ntheir results are not readily applicable to effective detection of\nhuman-written disinformation. What limits the successful transfer between them\nis the sizable gap between machine-generated fake news and human-authored ones,\nincluding the notable differences in terms of style and underlying intent. With\nthis in mind, we propose a novel framework for generating training examples\nthat are informed by the known styles and strategies of human-authored\npropaganda. Specifically, we perform self-critical sequence training guided by\nnatural language inference to ensure the validity of the generated articles,\nwhile also incorporating propaganda techniques, such as appeal to authority and\nloaded language. In particular, we create a new training dataset, PropaNews,\nwith 2,256 examples, which we release for future use. Our experimental results\nshow that fake news detectors trained on PropaNews are better at detecting\nhuman-written disinformation by 3.62 - 7.69% F1 score on two public datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kung-Hsiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Aligned Simple German Corpus. (arXiv:2209.01106v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.01106","description":"<p>\"Leichte Sprache\", the German counterpart to Simple English, is a regulated\nlanguage aiming to facilitate complex written language that would otherwise\nstay inaccessible to different groups of people. We present a new\nsentence-aligned monolingual corpus for Simple German -- German. It contains\nmultiple document-aligned sources which we have aligned using automatic\nsentence-alignment methods. We evaluate our alignments based on a manually\nlabelled subset of aligned documents. The quality of our sentence alignments,\nas measured by F1-score, surpasses previous work. We publish the dataset under\nCC BY-SA and the accompanying code under MIT license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toborek_V/0/1/0/all/0/1\">Vanessa Toborek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_M/0/1/0/all/0/1\">Moritz Busch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bossert_M/0/1/0/all/0/1\">Malte Bo&#xdf;ert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauckhage_C/0/1/0/all/0/1\">Christian Bauckhage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welke_P/0/1/0/all/0/1\">Pascal Welke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Parametric Temporal Adaptation for Social Media Topic Classification. (arXiv:2209.05706v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.05706","description":"<p>User-generated social media data is constantly changing as new trends\ninfluence online discussion and personal information is deleted due to privacy\nconcerns. However, most current NLP models are static and rely on fixed\ntraining data, which means they are unable to adapt to temporal change -- both\ntest distribution shift and deleted training data -- without frequent, costly\nre-training. In this paper, we study temporal adaptation through the task of\nlongitudinal hashtag prediction and propose a non-parametric dense retrieval\ntechnique, which does not require re-training, as a simple but effective\nsolution. In experiments on a newly collected, publicly available, year-long\nTwitter dataset exhibiting temporal distribution shift, our method improves by\n64.12% over the best parametric baseline without any of its costly\ngradient-based updating. Our dense retrieval approach is also particularly\nwell-suited to dynamically deleted user data in line with data privacy laws,\nwith negligible computational cost and performance loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogler_N/0/1/0/all/0/1\">Nikolai Vogler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florez_O/0/1/0/all/0/1\">Omar Florez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Kishky_A/0/1/0/all/0/1\">Ahmed El-Kishky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WeLM: A Well-Read Pre-trained Language Model for Chinese. (arXiv:2209.10372v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.10372","description":"<p>Large Language Models pre-trained with self-supervised learning have\ndemonstrated impressive zero-shot generalization capabilities on a wide\nspectrum of tasks. In this work, we present WeLM: a well-read pre-trained\nlanguage model for Chinese that is able to seamlessly perform different types\nof tasks with zero or few-shot demonstrations. WeLM is trained with 10B\nparameters by \"reading\" a curated high-quality corpus covering a wide range of\ntopics. We show that WeLM is equipped with broad knowledge on various domains\nand languages. On 18 monolingual (Chinese) tasks, WeLM can significantly\noutperform existing pre-trained models with similar sizes and match the\nperformance of models up to 25 times larger. WeLM also exhibits strong\ncapabilities in multi-lingual and code-switching understanding, outperforming\nexisting multilingual language models pre-trained on 30 languages. Furthermore,\nWe collected human-written prompts for a large set of supervised datasets in\nChinese and fine-tuned WeLM with multi-prompted training. The resulting model\ncan attain strong generalization on unseen types of tasks and outperform the\nunsupervised WeLM in zero-shot learning. Finally, we demonstrate that WeLM has\nbasic skills at explaining and calibrating the decisions from itself, which can\nbe promising directions for future research. Our models can be applied from\nhttps://welm.weixin.qq.com/docs/api/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hui Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Houjin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaoyu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zilin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast-FNet: Accelerating Transformer Encoder Models via Efficient Fourier Layers. (arXiv:2209.12816v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.12816","description":"<p>Transformer-based language models utilize the attention mechanism for\nsubstantial performance improvements in almost all natural language processing\n(NLP) tasks. Similar attention structures are also extensively studied in\nseveral other areas. Although the attention mechanism enhances the model\nperformances significantly, its quadratic complexity prevents efficient\nprocessing of long sequences. Recent works focused on eliminating the\ndisadvantages of computational inefficiency and showed that transformer-based\nmodels can still reach competitive results without the attention layer. A\npioneering study proposed the FNet, which replaces the attention layer with the\nFourier Transform (FT) in the transformer encoder architecture. FNet achieves\ncompetitive performances concerning the original transformer encoder model\nwhile accelerating training process by removing the computational burden of the\nattention mechanism. However, the FNet model ignores essential properties of\nthe FT from the classical signal processing that can be leveraged to increase\nmodel efficiency further. We propose different methods to deploy FT efficiently\nin transformer encoder models. Our proposed architectures have smaller number\nof model parameters, shorter training times, less memory usage, and some\nadditional performance improvements. We demonstrate these improvements through\nextensive experiments on common benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sevim_N/0/1/0/all/0/1\">Nurullah Sevim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozyedek_E/0/1/0/all/0/1\">Ege Ozan &#xd6;zyedek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahinuc_F/0/1/0/all/0/1\">Furkan &#x15e;ahinu&#xe7;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koc_A/0/1/0/all/0/1\">Aykut Ko&#xe7;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Makes Pre-trained Language Models Better Zero-shot Learners?. (arXiv:2209.15206v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.15206","description":"<p>Current methods for prompt learning in zeroshot scenarios widely rely on a\ndevelopment set with sufficient human-annotated data to select the\nbest-performing prompt template a posteriori. This is not ideal because in a\nrealworld zero-shot scenario of practical relevance, no labelled data is\navailable. Thus, we propose a simple yet effective method for screening\nreasonable prompt templates in zero-shot text classification: Perplexity\nSelection (Perplection). We hypothesize that language discrepancy can be used\nto measure the efficacy of prompt templates, and thereby develop a\nsubstantiated perplexity-based scheme allowing for forecasting the performance\nof prompt templates in advance. Experiments show that our method leads to\nimproved prediction performance in a realistic zero-shot setting, eliminating\nthe need for any labelled examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jinghui Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dongsheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Weidong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1\">Brian Mac Namee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1\">Fei Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pruning Pre-trained Language Models Without Fine-Tuning. (arXiv:2210.06210v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06210","description":"<p>To overcome the overparameterized problem in Pre-trained Language Models\n(PLMs), pruning is widely used as a simple and straightforward compression\nmethod by directly removing unimportant weights. Previous first-order methods\nsuccessfully compress PLMs to extremely high sparsity with little performance\ndrop. These methods, such as movement pruning, use first-order information to\nprune PLMs while fine-tuning the remaining weights. In this work, we argue\nfine-tuning is redundant for first-order pruning, since first-order pruning is\nsufficient to converge PLMs to downstream tasks without fine-tuning. Under this\nmotivation, we propose Static Model Pruning (SMP), which only uses first-order\npruning to adapt PLMs to downstream tasks while achieving the target sparsity\nlevel. In addition, we also design a new masking function and training\nobjective to further improve SMP. Extensive experiments at various sparsity\nlevels show SMP has significant improvements over first-order and zero-order\nmethods. Unlike previous first-order methods, SMP is also applicable to low\nsparsity and outperforms zero-order methods. Meanwhile, SMP is more parameter\nefficient than other methods due to it does not require fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Ting Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Deqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Ruobing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Feng Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Pre-Trained Models with Extra-Large Vocabularies: A Contrastive Analysis of Hebrew BERT Models and a New One to Outperform Them All. (arXiv:2211.15199v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.15199","description":"<p>We present a new pre-trained language model (PLM) for modern Hebrew, termed\nAlephBERTGimmel, which employs a much larger vocabulary (128K items) than\nstandard Hebrew PLMs before. We perform a contrastive analysis of this model\nagainst all previous Hebrew PLMs (mBERT, heBERT, AlephBERT) and assess the\neffects of larger vocabularies on task performance. Our experiments show that\nlarger vocabularies lead to fewer splits, and that reducing splits is better\nfor model performance, across different tasks. All in all this new model\nachieves new SOTA on all available Hebrew benchmarks, including Morphological\nSegmentation, POS Tagging, Full Morphological Analysis, NER, and Sentiment\nAnalysis. Subsequently we advocate for PLMs that are larger not only in terms\nof number of layers or training data, but also in terms of their vocabulary. We\nrelease the new model publicly for unrestricted use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gueta_E/0/1/0/all/0/1\">Eylon Gueta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmidman_A/0/1/0/all/0/1\">Avi Shmidman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmidman_S/0/1/0/all/0/1\">Shaltiel Shmidman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmidman_C/0/1/0/all/0/1\">Cheyn Shmuel Shmidman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guedalia_J/0/1/0/all/0/1\">Joshua Guedalia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppel_M/0/1/0/all/0/1\">Moshe Koppel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bareket_D/0/1/0/all/0/1\">Dan Bareket</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seker_A/0/1/0/all/0/1\">Amit Seker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frustratingly Easy Label Projection for Cross-lingual Transfer. (arXiv:2211.15613v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.15613","description":"<p>Translating training data into many languages has emerged as a practical\nsolution for improving cross-lingual transfer. For tasks that involve\nspan-level annotations, such as information extraction or question answering,\nan additional label projection step is required to map annotated spans onto the\ntranslated texts. Recently, a few efforts have utilized a simple\nmark-then-translate method to jointly perform translation and projection by\ninserting special markers around the labeled spans in the original sentence.\nHowever, as far as we are aware, no empirical analysis has been conducted on\nhow this approach compares to traditional annotation projection based on word\nalignment. In this paper, we present an extensive empirical study across 57\nlanguages and three tasks (QA, NER, and Event Extraction) to evaluate the\neffectiveness and limitations of both methods, filling an important gap in the\nliterature. Experimental results show that our optimized version of\nmark-then-translate, which we call EasyProject, is easily applied to many\nlanguages and works surprisingly well, outperforming the more complex word\nalignment-based methods. We analyze several key factors that affect the\nend-task performance, and show EasyProject works well because it can accurately\npreserve label span boundaries after translation. We will publicly release all\nour code and data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AIONER: All-in-one scheme-based biomedical named entity recognition using deep learning. (arXiv:2211.16944v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.16944","description":"<p>Biomedical named entity recognition (BioNER) seeks to automatically recognize\nbiomedical entities in natural language text, serving as a necessary foundation\nfor downstream text mining tasks and applications such as information\nextraction and question answering. Manually labeling training data for the\nBioNER task is costly, however, due to the significant domain expertise\nrequired for accurate annotation. The resulting data scarcity causes current\nBioNER approaches to be prone to overfitting, to suffer from limited\ngeneralizability, and to address a single entity type at a time (e.g., gene or\ndisease). We therefore propose a novel all-in-one (AIO) scheme that uses\nexternal data from existing annotated resources to enhance the accuracy and\nstability of BioNER models. We further present AIONER, a general-purpose BioNER\ntool based on cutting-edge deep learning and our AIO schema. We evaluate AIONER\non 14 BioNER benchmark tasks and show that AIONER is effective, robust, and\ncompares favorably to other state-of-the-art approaches such as multi-task\nlearning. We further demonstrate the practical utility of AIONER in three\nindependent tasks to recognize entity types not previously seen in training\ndata, as well as the advantages of AIONER over existing methods for processing\nbiomedical text at a large scale (e.g., the entire PubMed data).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Ling Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chih-Hsuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_P/0/1/0/all/0/1\">Po-Ting Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leaman_R/0/1/0/all/0/1\">Robert Leaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CREPE: Can Vision-Language Foundation Models Reason Compositionally?. (arXiv:2212.07796v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.07796","description":"<p>A fundamental characteristic common to both human vision and natural language\nis their compositional nature. Yet, despite the performance gains contributed\nby large vision and language pretraining, we find that: across 7 architectures\ntrained with 4 algorithms on massive datasets, they struggle at\ncompositionality. To arrive at this conclusion, we introduce a new\ncompositionality evaluation benchmark, CREPE, which measures two important\naspects of compositionality identified by cognitive science literature:\nsystematicity and productivity. To measure systematicity, CREPE consists of a\ntest dataset containing over $370K$ image-text pairs and three different\nseen-unseen splits. The three splits are designed to test models trained on\nthree popular training datasets: CC-12M, YFCC-15M, and LAION-400M. We also\ngenerate $325K$, $316K$, and $309K$ hard negative captions for a subset of the\npairs. To test productivity, CREPE contains $17K$ image-text pairs with nine\ndifferent complexities plus $183K$ hard negative captions with atomic, swapping\nand negation foils. The datasets are generated by repurposing the Visual Genome\nscene graphs and region descriptions and applying handcrafted templates and\nGPT-3. For systematicity, we find that model performance decreases consistently\nwhen novel compositions dominate the retrieval set, with Recall@1 dropping by\nup to $12\\%$. For productivity, models' retrieval success decays as complexity\nincreases, frequently nearing random chance at high complexity. These results\nhold regardless of model and training dataset size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zixian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jerry Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gul_M/0/1/0/all/0/1\">Mustafa Omer Gul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_M/0/1/0/all/0/1\">Mona Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_I/0/1/0/all/0/1\">Irena Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Prompting Large Language Models for Zero-Shot Open-Domain QA. (arXiv:2212.08635v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08635","description":"<p>Open-Domain Question Answering (ODQA) aims at answering factoid questions\nwithout explicitly providing specific background documents. In a zero-shot\nsetting, this task is more challenging since no data is available to train\ncustomized models like Retriever-Readers. Recently, Large Language Models\n(LLMs) like GPT-3 have shown their power in zero-shot ODQA with direct\nprompting methods, but these methods are still far from releasing the full\npowerfulness of LLMs only in an implicitly invoking way. In this paper, we\npropose a Self-Prompting framework to explicitly utilize the massive knowledge\nstored in the parameters of LLMs and their strong instruction understanding\nabilities. Concretely, we prompt LLMs step by step to generate multiple pseudo\nQA pairs with background passages and explanations from scratch and then use\nthose generated elements for in-context learning. Experimental results show our\nmethod surpasses previous SOTA methods significantly on three widely-used ODQA\ndatasets, and even achieves comparable performance with some Retriever-Reader\nmodels fine-tuned on full training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PVGRU: Generating Diverse and Relevant Dialogue Responses via Pseudo-Variational Mechanism. (arXiv:2212.09086v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09086","description":"<p>We investigate response generation for multi-turn dialogue in\ngenerative-based chatbots. Existing generative models based on RNNs (Recurrent\nNeural Networks) usually employ the last hidden state to summarize the\nsequences, which makes models unable to capture the subtle variability observed\nin different dialogues and cannot distinguish the differences between dialogues\nthat are similar in composition. In this paper, we propose a Pseudo-Variational\nGated Recurrent Unit (PVGRU) component without posterior knowledge through\nintroducing a recurrent summarizing variable into the GRU, which can aggregate\nthe accumulated distribution variations of subsequences. PVGRU can perceive the\nsubtle semantic variability through summarizing variables that are optimized by\nthe devised distribution consistency and reconstruction objectives. In\naddition, we build a Pseudo-Variational Hierarchical Dialogue (PVHD) model\nbased on PVGRU. Experimental results demonstrate that PVGRU can broadly improve\nthe diversity and relevance of responses on two benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongkang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CiteBench: A benchmark for Scientific Citation Text Generation. (arXiv:2212.09577v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09577","description":"<p>Science progresses by incrementally building upon the prior body of knowledge\ndocumented in scientific publications. The acceleration of research across many\nfields makes it hard to stay up-to-date with the recent developments and to\nsummarize the ever-growing body of prior work. To target this issue, the task\nof citation text generation aims to produce accurate textual summaries given a\nset of papers-to-cite and the citing paper context. Existing studies in\ncitation text generation are based upon widely diverging task definitions,\nwhich makes it hard to study this task systematically. To address this\nchallenge, we propose CiteBench: a benchmark for citation text generation that\nunifies multiple diverse datasets and enables standardized evaluation of\ncitation text generation models across task designs and domains. Using the new\nbenchmark, we investigate the performance of multiple strong baselines, test\ntheir transferability between the datasets, and deliver new insights into the\ntask definition and evaluation to guide future research in citation text\ngeneration. We make the code for CiteBench publicly available at\nhttps://github.com/UKPLab/citebench.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Funkquist_M/0/1/0/all/0/1\">Martin Funkquist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_I/0/1/0/all/0/1\">Ilia Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yufang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships. (arXiv:2212.10545v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10545","description":"<p>In this paper, we propose DimonGen, which aims to generate diverse sentences\ndescribing concept relationships in various everyday scenarios. To support\nthis, we first create a benchmark dataset for this task by adapting the\nexisting CommonGen dataset. We then propose a two-stage model called MoREE to\ngenerate the target sentences. MoREE consists of a mixture of retrievers model\nthat retrieves diverse context sentences related to the given concepts, and a\nmixture of generators model that generates diverse sentences based on the\nretrieved contexts. We conduct experiments on the DimonGen task and show that\nMoREE outperforms strong baselines in terms of both the quality and diversity\nof the generated sentences. Our results demonstrate that MoREE is able to\ngenerate diverse sentences that reflect different relationships between\nconcepts, leading to a comprehensive understanding of concept relationships.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenzhengyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kerui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias. (arXiv:2212.11261v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2212.11261","description":"<p>Nine language-vision AI models trained on web scrapes with the Contrastive\nLanguage-Image Pretraining (CLIP) objective are evaluated for evidence of a\nbias studied by psychologists: the sexual objectification of girls and women,\nwhich occurs when a person's human characteristics, such as emotions, are\ndisregarded and the person is treated as a body. We replicate three experiments\nin psychology quantifying sexual objectification and show that the phenomena\npersist in AI. A first experiment uses standardized images of women from the\nSexual OBjectification and EMotion Database, and finds that human\ncharacteristics are disassociated from images of objectified women: the model's\nrecognition of emotional state is mediated by whether the subject is fully or\npartially clothed. Embedding association tests (EATs) return significant effect\nsizes for both anger (d &gt;0.80) and sadness (d &gt;0.50), associating images of\nfully clothed subjects with emotions. GRAD-CAM saliency maps highlight that\nCLIP gets distracted from emotional expressions in objectified images. A second\nexperiment measures the effect in a representative application: an automatic\nimage captioner (Antarctic Captions) includes words denoting emotion less than\n50% as often for images of partially clothed women than for images of fully\nclothed women. A third experiment finds that images of female professionals\n(scientists, doctors, executives) are likely to be associated with sexual\ndescriptions relative to images of male professionals. A fourth experiment\nshows that a prompt of \"a [age] year old girl\" generates sexualized images (as\ndetermined by an NSFW classifier) up to 73% of the time for VQGAN-CLIP and\nStable Diffusion; the corresponding rate for boys never surpasses 9%. The\nevidence indicates that language-vision AI models trained on web scrapes learn\nbiases of sexual objectification, which propagate to downstream applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_R/0/1/0/all/0/1\">Robert Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howe_B/0/1/0/all/0/1\">Bill Howe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caliskan_A/0/1/0/all/0/1\">Aylin Caliskan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"tasksource: A Dataset Harmonization Framework for Streamlined NLP Multi-Task Learning and Evaluation. (arXiv:2301.05948v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.05948","description":"<p>The HuggingFace Datasets Hub hosts thousands of datasets, offering exciting\nopportunities for language model training and evaluation. However, datasets for\na specific task type often have different schemas, making harmonization\nchallenging. Multi-task training or evaluation necessitates manual work to fit\ndata into task templates. Several initiatives independently tackle this issue\nby releasing harmonized datasets or providing harmonization codes to preprocess\ndatasets into a consistent format. We identify patterns across previous\npreprocessing efforts, such as column name mapping and extracting specific\nsub-fields from structured data in a column. We then propose a structured\nannotation framework that ensures our annotations are fully exposed and not\nhidden within unstructured code. We release a dataset annotation framework and\ndataset annotations for more than 500 English\ntasks\\footnote{\\url{https://github.com/sileod/tasksource}}. These annotations\ninclude metadata, such as the names of columns to be used as input or labels\nfor all datasets, which can save time for future dataset preprocessing,\nregardless of whether our framework is utilized. We fine-tune a multi-task text\nencoder on all tasksource tasks, outperforming every publicly available text\nencoder of comparable size in an external evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sileo_D/0/1/0/all/0/1\">Damien Sileo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram. (arXiv:2301.10856v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2301.10856","description":"<p>In response to disinformation and propaganda from Russian online media\nfollowing the Russian invasion of Ukraine, Russian outlets including Russia\nToday and Sputnik News were banned throughout Europe. To maintain viewership,\nmany of these Russian outlets began to heavily promote their content on\nmessaging services like Telegram. In this work, we study how 16 Russian media\noutlets interacted with and utilized 732 Telegram channels throughout 2022.\nLeveraging the foundational model MPNet, DP-means clustering, and Hawkes\nProcesses, we trace how narratives spread between news sites and Telegram\nchannels. We show that news outlets not only propagate existing narratives\nthrough Telegram, but that they source material from the messaging platform.\nAcross the sites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru)\nof articles discuss content that originated/resulted from activity on Telegram.\nFinally, tracking the spread of individual topics, we measure the rate at which\nnews websites and their Telegram channels disseminate content within the\nRussian media ecosystem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hanley_H/0/1/0/all/0/1\">Hans W. A. Hanley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durumeric_Z/0/1/0/all/0/1\">Zakir Durumeric</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Clinical Entity Recognition using ChatGPT. (arXiv:2303.16416v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.16416","description":"<p>In this study, we investigated the potential of ChatGPT, a large language\nmodel developed by OpenAI, for the clinical named entity recognition task\ndefined in the 2010 i2b2 challenge, in a zero-shot setting with two different\nprompt strategies. We compared its performance with GPT-3 in a similar\nzero-shot setting, as well as a fine-tuned BioClinicalBERT model using a set of\nsynthetic clinical notes from MTSamples. Our findings revealed that ChatGPT\noutperformed GPT-3 in the zero-shot setting, with F1 scores of 0.418 (vs.0.250)\nand 0.620 (vs. 0.480) for exact- and relaxed-matching, respectively. Moreover,\nprompts affected ChatGPT's performance greatly, with relaxed-matching F1 scores\nof 0.628 vs.0.541 for two different prompt strategies. Although ChatGPT's\nperformance was still lower than that of the supervised BioClinicalBERT model\n(i.e., relaxed-matching F1 scores of 0.620 vs. 0.888), our study demonstrates\nthe great potential of ChatGPT for clinical NER tasks in a zero-shot setting,\nwhich is much more appealing as it does not require any annotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ameer_I/0/1/0/all/0/1\">Iqra Ameer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xu Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xueqing Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yujia Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zehan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianfu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis. (arXiv:2304.00020v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2304.00020","description":"<p>The prevalence of memes on social media has created the need to sentiment\nanalyze their underlying meanings for censoring harmful content. Meme censoring\nsystems by machine learning raise the need for a semi-supervised learning\nsolution to take advantage of the large number of unlabeled memes available on\nthe internet and make the annotation process less challenging. Moreover, the\napproach needs to utilize multimodal data as memes' meanings usually come from\nboth images and texts. This research proposes a multimodal semi-supervised\nlearning approach that outperforms other multimodal semi-supervised learning\nand supervised learning state-of-the-art models on two datasets, the Multimedia\nAutomatic Misogyny Identification and Hateful Memes dataset. Building on the\ninsights gained from Contrastive Language-Image Pre-training, which is an\neffective multimodal learning technique, this research introduces SemiMemes, a\nnovel training method that combines auto-encoder and classification task to\nmake use of the resourceful unlabeled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tung_P/0/1/0/all/0/1\">Pham Thai Hoang Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viet_N/0/1/0/all/0/1\">Nguyen Tan Viet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anh_N/0/1/0/all/0/1\">Ngo Tien Anh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_P/0/1/0/all/0/1\">Phan Duy Hung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rediscovering Hashed Random Projections for Efficient Quantization of Contextualized Sentence Embeddings. (arXiv:2304.02481v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.02481","description":"<p>Training and inference on edge devices often requires an efficient setup due\nto computational limitations. While pre-computing data representations and\ncaching them on a server can mitigate extensive edge device computation, this\nleads to two challenges. First, the amount of storage required on the server\nthat scales linearly with the number of instances. Second, the bandwidth\nrequired to send extensively large amounts of data to an edge device. To reduce\nthe memory footprint of pre-computed data representations, we propose a simple,\nyet effective approach that uses randomly initialized hyperplane projections.\nTo further reduce their size by up to 98.96%, we quantize the resulting\nfloating-point representations into binary vectors. Despite the greatly reduced\nsize, we show that the embeddings remain effective for training models across\nvarious English and German sentence classification tasks that retain 94%--99%\nof their floating-point.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamster_U/0/1/0/all/0/1\">Ulf A. Hamster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Ji-Ung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geyken_A/0/1/0/all/0/1\">Alexander Geyken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Interpretable Mental Health Analysis with ChatGPT. (arXiv:2304.03347v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.03347","description":"<p>Automated mental health analysis shows great potential for enhancing the\nefficiency and accessibility of mental health care, with recent methods using\npre-trained language models (PLMs) and incorporated emotional information. The\nlatest large language models (LLMs), such as ChatGPT, exhibit dramatic\ncapabilities on diverse natural language processing tasks. However, existing\nstudies on ChatGPT for mental health analysis bear limitations in inadequate\nevaluations, ignorance of emotional information, and lack of explainability. To\nbridge these gaps, we comprehensively evaluate the mental health analysis and\nemotional reasoning ability of ChatGPT on 11 datasets across 5 tasks, and\nanalyze the effects of various emotion-based prompting strategies. Based on\nthese prompts, we further explore LLMs for interpretable mental health analysis\nby instructing them to also generate explanations for each of their decisions.\nWith an annotation protocol designed by domain experts, we convey human\nevaluations to assess the quality of explanations generated by ChatGPT and\nGPT-3. The annotated corpus will be released for future research. Experimental\nresults show that ChatGPT outperforms traditional neural network-based methods\nbut still has a significant gap with advanced task-specific methods. Prompt\nengineering with emotional cues can be effective in improving performance on\nmental health analysis but suffers from a lack of robustness and inaccurate\nreasoning. In addition, ChatGPT significantly outperforms GPT-3 on all criteria\nin human evaluations of the explanations and approaches to human performance,\nshowing its great potential in explainable mental health analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1\">Ziyan Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1\">Sophia Ananiadou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information. (arXiv:2304.09667v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.09667","description":"<p>While large language models (LLMs) have been successfully applied to various\ntasks, they still face challenges with hallucinations. Augmenting LLMs with\ndomain-specific tools such as database utilities can facilitate easier and more\nprecise access to specialized knowledge. In this paper, we present GeneGPT, a\nnovel method for teaching LLMs to use the Web APIs of the National Center for\nBiotechnology Information (NCBI) for answering genomics questions.\nSpecifically, we prompt Codex to solve the GeneTuring tests with NCBI Web APIs\nby in-context learning and an augmented decoding algorithm that can detect and\nexecute API calls. Experimental results show that GeneGPT achieves\nstate-of-the-art performance on eight tasks in the GeneTuring benchmark with an\naverage score of 0.83, largely surpassing retrieval-augmented LLMs such as the\nnew Bing (0.44), biomedical LLMs such as BioMedLM (0.08) and BioGPT (0.04), as\nwell as GPT-3 (0.16) and ChatGPT (0.12). Our further analyses suggest that: (1)\nAPI demonstrations have good cross-task generalizability and are more useful\nthan documentations for in-context learning; (2) GeneGPT can generalize to\nlonger chains of API calls and answer multi-hop questions in GeneHop, a novel\ndataset introduced in this work; (3) Different types of errors are enriched in\ndifferent tasks, providing valuable insights for future improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qiao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yifan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.00586","description":"<p>Pre-trained language models can be surprisingly adept at tasks they were not\nexplicitly trained on, but how they implement these capabilities is poorly\nunderstood. In this paper, we investigate the basic mathematical abilities\noften acquired by pre-trained language models. Concretely, we use mechanistic\ninterpretability techniques to explain the (limited) mathematical abilities of\nGPT-2 small. As a case study, we examine its ability to take in sentences such\nas \"The war lasted from the year 1732 to the year 17\", and predict valid\ntwo-digit end years (years &gt; 32). We first identify a circuit, a small subset\nof GPT-2 small's computational graph that computes this task's output. Then, we\nexplain the role of each circuit component, showing that GPT-2 small's final\nmulti-layer perceptrons boost the probability of end years greater than the\nstart year. Finally, we find related tasks that activate our circuit. Our\nresults suggest that GPT-2 small computes greater-than using a complex but\ngeneral mechanism that activates across diverse contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hanna_M/0/1/0/all/0/1\">Michael Hanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_O/0/1/0/all/0/1\">Ollie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Variengien_A/0/1/0/all/0/1\">Alexandre Variengien</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Code generation for Information Technology Tasks in YAML through Large Language Models. (arXiv:2305.02783v3 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2305.02783","description":"<p>The recent improvement in code generation capabilities due to the use of\nlarge language models has mainly benefited general purpose programming\nlanguages. Domain specific languages, such as the ones used for IT Automation,\nhave received far less attention, despite involving many active developers and\nbeing an essential component of modern cloud platforms. This work focuses on\nthe generation of Ansible-YAML, a widely used markup language for IT\nAutomation. We present Ansible Wisdom, a natural-language to Ansible-YAML code\ngeneration tool, aimed at improving IT automation productivity. Ansible Wisdom\nis a transformer-based model, extended by training with a new dataset\ncontaining Ansible-YAML. We also develop two novel performance metrics for YAML\nand Ansible to capture the specific characteristics of this domain. Results\nshow that Ansible Wisdom can accurately generate Ansible script from natural\nlanguage prompts with performance comparable or better than existing state of\nthe art code generation models. In few-shot settings we asses the impact of\ntraining with Ansible, YAML data and compare with different baselines including\nCodex-Davinci-002. We also show that after finetuning, our Ansible specific\nmodel can beat the performance of a much larger Codex-Davinci-002 in few shot\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pujar_S/0/1/0/all/0/1\">Saurabh Pujar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buratti_L/0/1/0/all/0/1\">Luca Buratti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaojie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupuis_N/0/1/0/all/0/1\">Nicolas Dupuis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_B/0/1/0/all/0/1\">Burn Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suneja_S/0/1/0/all/0/1\">Sahil Suneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sood_A/0/1/0/all/0/1\">Atin Sood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nalawade_G/0/1/0/all/0/1\">Ganesh Nalawade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1\">Matthew Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morari_A/0/1/0/all/0/1\">Alessandro Morari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puri_R/0/1/0/all/0/1\">Ruchir Puri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LMs stand their Ground: Investigating the Effect of Embodiment in Figurative Language Interpretation by Language Models. (arXiv:2305.03445v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03445","description":"<p>Figurative language is a challenge for language models since its\ninterpretation is based on the use of words in a way that deviates from their\nconventional order and meaning. Yet, humans can easily understand and interpret\nmetaphors, similes or idioms as they can be derived from embodied metaphors.\nLanguage is a proxy for embodiment and if a metaphor is conventional and\nlexicalised, it becomes easier for a system without a body to make sense of\nembodied concepts. Yet, the intricate relation between embodiment and features\nsuch as concreteness or age of acquisition has not been studied in the context\nof figurative language interpretation concerning language models. Hence, the\npresented study shows how larger language models perform better at interpreting\nmetaphoric sentences when the action of the metaphorical sentence is more\nembodied. The analysis rules out multicollinearity with other features (e.g.\nword length or concreteness) and provides initial evidence that larger language\nmodels conceptualise embodied concepts to a degree that facilitates figurative\nlanguage understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wicke_P/0/1/0/all/0/1\">Philipp Wicke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive loose optimization for robust question answering. (arXiv:2305.03971v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03971","description":"<p>Question answering methods are well-known for leveraging data bias, such as\nthe language prior in visual question answering and the position bias in\nmachine reading comprehension (extractive question answering). Current\ndebiasing methods often come at the cost of significant in-distribution\nperformance to achieve favorable out-of-distribution generalizability, while\nnon-debiasing methods sacrifice a considerable amount of out-of-distribution\nperformance in order to obtain high in-distribution performance. Therefore, it\nis challenging for them to deal with the complicated changing real-world\nsituations. In this paper, we propose a simple yet effective novel loss\nfunction with adaptive loose optimization, which seeks to make the best of both\nworlds for question answering. Our main technical contribution is to reduce the\nloss adaptively according to the ratio between the previous and current\noptimization state on mini-batch training data. This loose optimization can be\nused to prevent non-debiasing methods from overlearning data bias while\nenabling debiasing methods to maintain slight bias learning. Experiments on the\nvisual question answering datasets, including VQA v2, VQA-CP v1, VQA-CP v2,\nGQA-OOD, and the extractive question answering dataset SQuAD demonstrate that\nour approach enables QA methods to obtain state-of-the-art in- and\nout-of-distribution performance in most cases. The source code has been\nreleased publicly in \\url{https://github.com/reml-group/ALO}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jie Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pinghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zewei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Dechen Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Min Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Ting Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Demonstration Retriever for In-Context Learning. (arXiv:2305.04320v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.04320","description":"<p>In-context learning is a new learning paradigm where a language model\nconditions on a few input-output pairs (demonstrations) and a test input, and\ndirectly outputs the prediction. It has been shown highly dependent on the\nprovided demonstrations and thus promotes the research of demonstration\nretrieval: given a test input, relevant examples are retrieved from the\ntraining set to serve as informative demonstrations for in-context learning.\nWhile previous works focus on training task-specific retrievers for several\ntasks separately, these methods are often hard to transfer and scale on various\ntasks, and separately trained retrievers incur a lot of parameter storage and\ndeployment cost. In this paper, we propose Unified Demonstration Retriever\n(\\textbf{UDR}), a single model to retrieve demonstrations for a wide range of\ntasks. To train UDR, we cast various tasks' training signals into a unified\nlist-wise ranking formulation by language model's feedback. Then we propose a\nmulti-task list-wise ranking training framework, with an iterative mining\nstrategy to find high-quality candidates, which can help UDR fully incorporate\nvarious tasks' signals. Experiments on 30+ tasks across 13 task families and\nmultiple data domains show that UDR significantly outperforms baselines.\nFurther analyses show the effectiveness of each proposed component and UDR's\nstrong ability in various scenarios including different LMs (1.3B - 175B),\nunseen datasets, varying demonstration quantities, etc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaonan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_K/0/1/0/all/0/1\">Kai Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tianyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1\">Yuan Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guotong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-UniMorph: Korean Universal Morphology and its Feature Schema. (arXiv:2305.06335v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06335","description":"<p>We present in this work a new Universal Morphology dataset for Korean.\nPreviously, the Korean language has been underrepresented in the field of\nmorphological paradigms amongst hundreds of diverse world languages. Hence, we\npropose this Universal Morphological paradigms for the Korean language that\npreserve its distinct characteristics. For our K-UniMorph dataset, we outline\neach grammatical criterion in detail for the verbal endings, clarify how to\nextract inflected forms, and demonstrate how we generate the morphological\nschemata. This dataset adopts morphological feature schema from Sylak-Glassman\net al. (2015) and Sylak-Glassman (2016) for the Korean language as we extract\ninflected verb forms from the Sejong morphologically analyzed corpus that is\none of the largest annotated corpora for Korean. During the data creation, our\nmethodology also includes investigating the correctness of the conversion from\nthe Sejong corpus. Furthermore, we carry out the inflection task using three\ndifferent Korean word forms: letters, syllables and morphemes. Finally, we\ndiscuss and describe future perspectives on Korean morphological paradigms and\nthe dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jo_E/0/1/0/all/0/1\">Eunkyul Leah Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyuwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xihan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1\">KyungTae Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jungyeul Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chulwoo Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bot or Human? Detecting ChatGPT Imposters with A Single Question. (arXiv:2305.06424v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06424","description":"<p>Large language models like ChatGPT have recently demonstrated impressive\ncapabilities in natural language understanding and generation, enabling various\napplications including translation, essay writing, and chit-chatting. However,\nthere is a concern that they can be misused for malicious purposes, such as\nfraud or denial-of-service attacks. Therefore, it is crucial to develop methods\nfor detecting whether the party involved in a conversation is a bot or a human.\nIn this paper, we propose a framework named FLAIR, Finding Large language model\nAuthenticity via a single Inquiry and Response, to detect conversational bots\nin an online manner. Specifically, we target a single question scenario that\ncan effectively differentiate human users from bots. The questions are divided\ninto two categories: those that are easy for humans but difficult for bots\n(e.g., counting, substitution, positioning, noise filtering, and ASCII art),\nand those that are easy for bots but difficult for humans (e.g., memorization\nand computation). Our approach shows different strengths of these questions in\ntheir effectiveness, providing a new way for online service providers to\nprotect themselves against nefarious activities and ensure that they are\nserving real users. We open-sourced our dataset on\nhttps://github.com/hongwang600/FLAIR and welcome contributions from the\ncommunity to enrich such detection datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xifeng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QURG: Question Rewriting Guided Context-Dependent Text-to-SQL Semantic Parsing. (arXiv:2305.06655v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06655","description":"<p>Context-dependent Text-to-SQL aims to translate multi-turn natural language\nquestions into SQL queries. Despite various methods have exploited\ncontext-dependence information implicitly for contextual SQL parsing, there are\nfew attempts to explicitly address the dependencies between current question\nand question context. This paper presents QURG, a novel Question Rewriting\nGuided approach to help the models achieve adequate contextual understanding.\nSpecifically, we first train a question rewriting model to complete the current\nquestion based on question context, and convert them into a rewriting edit\nmatrix. We further design a two-stream matrix encoder to jointly model the\nrewriting relations between question and context, and the schema linking\nrelations between natural language and structured schema. Experimental results\nshow that QURG significantly improves the performances on two large-scale\ncontext-dependent datasets SParC and CoSQL, especially for hard and long-turn\nquestions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1\">Linzheng Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_D/0/1/0/all/0/1\">Dongling Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liqun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian-Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhao Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text2Cohort: Democratizing the NCI Imaging Data Commons with Natural Language Cohort Discovery. (arXiv:2305.07637v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.07637","description":"<p>The Imaging Data Commons (IDC) is a cloud-based database that provides\nresearchers with open access to cancer imaging data, with the goal of\nfacilitating collaboration in medical imaging research. However, querying the\nIDC database for cohort discovery and access to imaging data has a significant\nlearning curve for researchers due to its complex nature. We developed\nText2Cohort, a large language model (LLM) based toolkit to facilitate\nuser-friendly and intuitive natural language cohort discovery in the IDC.\nText2Cohorts translates user input into IDC database queries using prompt\nengineering and autocorrection and returns the query's response to the user.\nAutocorrection resolves errors in queries by passing the errors back to the\nmodel for interpretation and correction. We evaluate Text2Cohort on 50 natural\nlanguage user inputs ranging from information extraction to cohort discovery.\nThe resulting queries and outputs were verified by two computer scientists to\nmeasure Text2Cohort's accuracy and F1 score. Text2Cohort successfully generated\nqueries and their responses with an 88% accuracy and F1 score of 0.94. However,\nit failed to generate queries for 6/50 (12%) user inputs due to syntax and\nsemantic errors. Our results indicate that Text2Cohort succeeded at generating\nqueries with correct responses, but occasionally failed due to a lack of\nunderstanding of the data schema. Despite these shortcomings, Text2Cohort\ndemonstrates the utility of LLMs to enable researchers to discover and curate\ncohorts using data hosted on IDC with high levels of accuracy using natural\nlanguage in a more intuitive and user-friendly way.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_P/0/1/0/all/0/1\">Pranav Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanhere_A/0/1/0/all/0/1\">Adway Kanhere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_P/0/1/0/all/0/1\">Paul H. Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_V/0/1/0/all/0/1\">Vishwa S. Parekh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization. (arXiv:2305.08503v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08503","description":"<p>Pre-trained language models (PLMs) have accomplished impressive achievements\nin abstractive single-document summarization (SDS). However, such benefits may\nnot be readily extended to muti-document summarization (MDS), where the\ninteractions among documents are more complex. Previous works either design new\narchitectures or new pre-training objectives for MDS, or apply PLMs to MDS\nwithout considering the complex document interactions. While the former does\nnot make full use of previous pre-training efforts and may not generalize well\nacross multiple domains, the latter cannot fully attend to the intricate\nrelationships unique to MDS tasks. In this paper, we enforce hierarchy on both\nthe encoder and decoder and seek to make better use of a PLM to facilitate\nmulti-document interactions for the MDS task. We test our design on 10 MDS\ndatasets across a wide range of domains. Extensive experiments show that our\nproposed method can achieve consistent improvements on all these datasets,\noutperforming the previous best models, and even achieving better or\ncompetitive results as compared to some models with additional MDS pre-training\nor larger model parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chenhui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-05-16T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}