{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-01-09T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Deep Latent Variable Models for Semi-supervised Paraphrase Generation. (arXiv:2301.02275v1 [cs.CL])","link":"http://arxiv.org/abs/2301.02275","description":"<p>This paper explores deep latent variable models for semi-supervised\nparaphrase generation, where the missing target pair is modelled as a latent\nparaphrase sequence. We present a novel unsupervised model named variational\nsequence auto-encoding reconstruction (VSAR), which performs latent sequence\ninference given an observed text. To leverage information from text pairs, we\nintroduce a supervised model named dual directional learning (DDL). Combining\nVSAR with DDL (DDL+VSAR) enables us to conduct semi-supervised learning;\nhowever, the combined model suffers from a cold-start problem. To combat this\nissue, we propose to deal with better weight initialisation, leading to a\ntwo-stage training scheme named knowledge reinforced training. Our empirical\nevaluations suggest that the combined model yields competitive performance\nagainst the state-of-the-art supervised baselines on complete data.\nFurthermore, in scenarios where only a fraction of the labelled pairs are\navailable, our combined model consistently outperforms the strong supervised\nmodel baseline (DDL and Transformer) by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jialin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristea_A/0/1/0/all/0/1\">Alexandra I. Cristea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harit_A/0/1/0/all/0/1\">Anoushka Harit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhongtian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aduragba_O/0/1/0/all/0/1\">Olanrewaju Tahir Aduragba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1\">Lei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moubayed_N/0/1/0/all/0/1\">Noura Al Moubayed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Broadcast News Summarization; a comparative study on Maximal Marginal Relevance (MMR) and Latent Semantic Analysis (LSA). (arXiv:2301.02284v1 [cs.CL])","link":"http://arxiv.org/abs/2301.02284","description":"<p>The methods of automatic speech summarization are classified into two groups:\nsupervised and unsupervised methods. Supervised methods are based on a set of\nfeatures, while unsupervised methods perform summarization based on a set of\nrules. Latent Semantic Analysis (LSA) and Maximal Marginal Relevance (MMR) are\nconsidered the most important and well-known unsupervised methods in automatic\nspeech summarization. This study set out to investigate the performance of two\naforementioned unsupervised methods in transcriptions of Persian broadcast news\nsummarization. The results show that in generic summarization, LSA outperforms\nMMR, and in query-based summarization, MMR outperforms LSA in broadcast news\nsummarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1\">Majid Ramezani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahryari_M/0/1/0/all/0/1\">Mohammad-Salar Shahryari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_A/0/1/0/all/0/1\">Amir-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1\">Mohammad-Reza Feizi-Derakhshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequentially Controlled Text Generation. (arXiv:2301.02299v1 [cs.CL])","link":"http://arxiv.org/abs/2301.02299","description":"<p>While GPT-2 generates sentences that are remarkably human-like, longer\ndocuments can ramble and do not follow human-like writing structure. We study\nthe problem of imposing structure on long-range text. We propose a novel\ncontrolled text generation task, sequentially controlled text generation, and\nidentify a dataset, NewsDiscourse as a starting point for this task. We develop\na sequential controlled text generation pipeline with generation and editing.\nWe test different degrees of structural awareness and show that, in general,\nmore structural awareness results in higher control-accuracy, grammaticality,\ncoherency and topicality, approaching human-level writing performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spangher_A/0/1/0/all/0/1\">Alexander Spangher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xinyu Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1\">Yao Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Truly Understand What I Need: Intellectual and Friendly Dialogue Agents grounding Knowledge and Persona. (arXiv:2301.02401v1 [cs.CL])","link":"http://arxiv.org/abs/2301.02401","description":"<p>To build a conversational agent that interacts fluently with humans, previous\nstudies blend knowledge or personal profile into the pre-trained language\nmodel. However, the model that considers knowledge and persona at the same time\nis still limited, leading to hallucination and a passive way of using personas.\nWe propose an effective dialogue agent that grounds external knowledge and\npersona simultaneously. The agent selects the proper knowledge and persona to\nuse for generating the answers with our candidate scoring implemented with a\npoly-encoder. Then, our model generates the utterance with lesser hallucination\nand more engagingness utilizing retrieval augmented generation with\nknowledge-persona enhanced query. We conduct experiments on the\npersona-knowledge chat and achieve state-of-the-art performance in grounding\nand generation tasks on the automatic metrics. Moreover, we validate the\nanswers from the models regarding hallucination and engagingness through human\nevaluation and qualitative results. We show our retriever's effectiveness in\nextracting relevant documents compared to the other previous retrievers, along\nwith the comparison of multiple candidate scoring methods. Code is available at\nhttps://github.com/dlawjddn803/INFO\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">Jungwoo Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Myunghoon Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hur_Y/0/1/0/all/0/1\">Yuna Hur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Seungwon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinsung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Yoonna Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongyub Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Hyesung Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1\">Donghoon Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Heuiseok Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mask-then-Fill: A Flexible and Effective Data Augmentation Framework for Event Extraction. (arXiv:2301.02427v1 [cs.CL])","link":"http://arxiv.org/abs/2301.02427","description":"<p>We present Mask-then-Fill, a flexible and effective data augmentation\nframework for event extraction. Our approach allows for more flexible\nmanipulation of text and thus can generate more diverse data while keeping the\noriginal event structure unchanged as much as possible. Specifically, it first\nrandomly masks out an adjunct sentence fragment and then infills a\nvariable-length text span with a fine-tuned infilling model. The main advantage\nlies in that it can replace a fragment of arbitrary length in the text with\nanother fragment of variable length, compared to the existing methods which can\nonly replace a single word or a fixed-length fragment. On trigger and argument\nextraction tasks, the proposed framework is more effective than baseline\nmethods and it demonstrates particularly strong results in the low-resource\nsetting. Our further analysis shows that it achieves a good balance between\ndiversity and distributional similarity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Changlong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Huan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topics as Entity Clusters: Entity-based Topics from Language Models and Graph Neural Networks. (arXiv:2301.02458v1 [cs.CL])","link":"http://arxiv.org/abs/2301.02458","description":"<p>Topic models aim to reveal the latent structure behind a corpus, typically\nconducted over a bag-of-words representation of documents. In the context of\ntopic modeling, most vocabulary is either irrelevant for uncovering underlying\ntopics or contains strong relationships with relevant concepts, impacting the\ninterpretability of these topics. Furthermore, their limited expressiveness and\ndependency on language demand considerable computation resources. Hence, we\npropose a novel approach for cluster-based topic modeling that employs\nconceptual entities. Entities are language-agnostic representations of\nreal-world concepts rich in relational information. To this end, we extract\nvector representations of entities from (i) an encyclopedic corpus using a\nlanguage model; and (ii) a knowledge base using a graph neural network. We\ndemonstrate that our approach consistently outperforms other state-of-the-art\ntopic models across coherency metrics and find that the explicit knowledge\nencoded in the graph-based embeddings provides more coherent topics than the\nimplicit knowledge encoded with the contextualized embeddings of language\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loureiro_M/0/1/0/all/0/1\">Manuel V. Loureiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derby_S/0/1/0/all/0/1\">Steven Derby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijaya_T/0/1/0/all/0/1\">Tri Kurniawan Wijaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPD@NL4Opt: An ensemble approach for the NER task of the optimization problem. (arXiv:2301.02459v1 [cs.CL])","link":"http://arxiv.org/abs/2301.02459","description":"<p>In this paper, we present an ensemble approach for the NL4Opt competition\nsubtask 1(NER task). For this task, we first fine tune the pretrained language\nmodels based on the competition dataset. Then we adopt differential learning\nrates and adversarial training strategies to enhance the model generalization\nand robustness. Additionally, we use a model ensemble method for the final\nprediction, which achieves a micro-averaged F1 score of 93.3% and attains the\nsecond prize in the NER task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kangxu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ze Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiewen Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAIDS: A Novel Approach for Sentiment Analysis Informed of Dialect and Sarcasm. (arXiv:2301.02521v1 [cs.CL])","link":"http://arxiv.org/abs/2301.02521","description":"<p>Sentiment analysis becomes an essential part of every social network, as it\nenables decision-makers to know more about users' opinions in almost all life\naspects. Despite its importance, there are multiple issues it encounters like\nthe sentiment of the sarcastic text which is one of the main challenges of\nsentiment analysis. This paper tackles this challenge by introducing a novel\nsystem (SAIDS) that predicts the sentiment, sarcasm and dialect of Arabic\ntweets. SAIDS uses its prediction of sarcasm and dialect as known information\nto predict the sentiment. It uses MARBERT as a language model to generate\nsentence embedding, then passes it to the sarcasm and dialect models, and then\nthe outputs of the three models are concatenated and passed to the sentiment\nanalysis model. Multiple system design setups were experimented with and\nreported. SAIDS was applied to the ArSarcasm-v2 dataset where it outperforms\nthe state-of-the-art model for the sentiment analysis task. By training all\ntasks together, SAIDS achieves results of 75.98 FPN, 59.09 F1-score and 71.13\nF1-score for sentiment analysis, sarcasm detection, and dialect identification\nrespectively. The system design can be used to enhance the performance of any\ntask which is dependent on other tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaseb_A/0/1/0/all/0/1\">Abdelrahman Kaseb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farouk_M/0/1/0/all/0/1\">Mona Farouk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"No, to the Right\" -- Online Language Corrections for Robotic Manipulation via Shared Autonomy. (arXiv:2301.02555v1 [cs.RO])","link":"http://arxiv.org/abs/2301.02555","description":"<p>Systems for language-guided human-robot interaction must satisfy two key\ndesiderata for broad adoption: adaptivity and learning efficiency.\nUnfortunately, existing instruction-following agents cannot adapt, lacking the\nability to incorporate online natural language supervision, and even if they\ncould, require hundreds of demonstrations to learn even simple policies. In\nthis work, we address these problems by presenting Language-Informed Latent\nActions with Corrections (LILAC), a framework for incorporating and adapting to\nnatural language corrections - \"to the right,\" or \"no, towards the book\" -\nonline, during execution. We explore rich manipulation domains within a shared\nautonomy paradigm. Instead of discrete turn-taking between a human and robot,\nLILAC splits agency between the human and robot: language is an input to a\nlearned model that produces a meaningful, low-dimensional control space that\nthe human can use to guide the robot. Each real-time correction refines the\nhuman's control space, enabling precise, extended behaviors - with the added\nbenefit of requiring only a handful of demonstrations to learn. We evaluate our\napproach via a user study where users work with a Franka Emika Panda\nmanipulator to complete complex manipulation tasks. Compared to existing\nlearned baselines covering both open-loop instruction following and single-turn\nshared autonomy, we show that our corrections-aware approach obtains higher\ntask completion rates, and is subjectively preferred by users because of its\nreliability, precision, and ease of use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yuchen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karamcheti_S/0/1/0/all/0/1\">Siddharth Karamcheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palleti_R/0/1/0/all/0/1\">Raj Palleti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shivakumar_N/0/1/0/all/0/1\">Nidhya Shivakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1\">Dorsa Sadigh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Categorization of Mental Health Posts using Transformers. (arXiv:2301.02589v1 [cs.CL])","link":"http://arxiv.org/abs/2301.02589","description":"<p>With recent developments in digitization of clinical psychology, NLP research\ncommunity has revolutionized the field of mental health detection on social\nmedia. Existing research in mental health analysis revolves around the\ncross-sectional studies to classify users' intent on social media. For in-depth\nanalysis, we investigate existing classifiers to solve the problem of causal\ncategorization which suggests the inefficiency of learning based methods due to\nlimited training samples. To handle this challenge, we use transformer models\nand demonstrate the efficacy of a pre-trained transfer learning on \"CAMS\"\ndataset. The experimental result improves the accuracy and depicts the\nimportance of identifying cause-and-effect relationships in the underlying\ntext.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1\">Muskan Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaur_S/0/1/0/all/0/1\">Simranjeet Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_R/0/1/0/all/0/1\">Ritika Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Aastha Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_C/0/1/0/all/0/1\">Chandni Saxena</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LTL under reductions with weaker conditions than stutter-invariance. (arXiv:2111.03342v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.03342","description":"<p>Verification of properties expressed as-regular languages such as LTL can\nbenefit hugely from stutter-insensitivity, using a diverse set of reduction\nstrategies. However properties that are not stutter-insensitive, for instance\ndue to the use of the neXt operator of LTL or to some form of counting in the\nlogic, are not covered by these techniques in general. We propose in this paper\nto study a weaker property than stutter-insensitivity. In a stutter insensitive\nlanguage both adding and removing stutter to a word does not change its\nacceptance, any stuttering can be abstracted away; by decomposing this\nequivalence relation into two implications we obtain weaker conditions. We\ndefine a shortening insensitive language where any word that stutters less than\na word in the language must also belong to the language. A lengthening\ninsensitive language has the dual property. A semi-decision procedure is then\nintroduced to reliably prove shortening insensitive properties or deny\nlengthening insensitive properties while working with a reduction of a system.\nA reduction has the property that it can only shorten runs. Lipton's\ntransaction reductions or Petri net agglomerations are examples of eligible\nstructural reduction strategies. An implementation and experimental evidence is\nprovided showing most nonrandom properties sensitive to stutter are actually\nshortening or lengthening insensitive. Performance of experiments on a large\n(random) benchmark from the model-checking competition indicate that despite\nbeing a semi-decision procedure, the approach can still improve state of the\nart verification tools.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paviot_Adet_E/0/1/0/all/0/1\">Emmanuel Paviot-Adet</a> (MoVe), <a href=\"http://arxiv.org/find/cs/1/au:+Poitrenaud_D/0/1/0/all/0/1\">Denis Poitrenaud</a> (MoVe), <a href=\"http://arxiv.org/find/cs/1/au:+Renault_E/0/1/0/all/0/1\">Etienne Renault</a> (LRDE), <a href=\"http://arxiv.org/find/cs/1/au:+Thierry_Mieg_Y/0/1/0/all/0/1\">Yann Thierry-Mieg</a> (MoVe)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DABS: A Domain-Agnostic Benchmark for Self-Supervised Learning. (arXiv:2111.12062v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.12062","description":"<p>Self-supervised learning algorithms, including BERT and SimCLR, have enabled\nsignificant strides in fields like natural language processing, computer\nvision, and speech processing. However, these algorithms are domain-specific,\nmeaning that new self-supervised learning algorithms must be developed for each\nnew setting, including myriad healthcare, scientific, and multimodal domains.\nTo catalyze progress toward domain-agnostic methods, we introduce DABS: a\nDomain-Agnostic Benchmark for Self-supervised learning. To perform well on\nDABS, an algorithm is evaluated on seven diverse domains: natural images,\nmultichannel sensor data, English text, speech recordings, multilingual text,\nchest x-rays, and images with text descriptions. Each domain contains an\nunlabeled dataset for pretraining; the model is then is scored based on its\ndownstream performance on a set of labeled tasks in the domain. We also present\ne-Mix and ShED: two baseline domain-agnostic algorithms; their relatively\nmodest performance demonstrates that significant progress is needed before\nself-supervised learning is an out-of-the-box solution for arbitrary domains.\nCode for benchmark datasets and baseline algorithms is available at\nhttps://github.com/alextamkin/dabs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tamkin_A/0/1/0/all/0/1\">Alex Tamkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_V/0/1/0/all/0/1\">Vincent Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Rongfei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fein_D/0/1/0/all/0/1\">Daniel Fein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schultz_C/0/1/0/all/0/1\">Colin Schultz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah Goodman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-Based Automatic Personality Prediction Using KGrAt-Net; A Knowledge Graph Attention Network Classifier. (arXiv:2205.13780v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.13780","description":"<p>Nowadays, a tremendous amount of human communications occur on Internet-based\ncommunication infrastructures, like social networks, email, forums,\norganizational communication platforms, etc. Indeed, the automatic prediction\nor assessment of individuals' personalities through their written or exchanged\ntext would be advantageous to ameliorate their relationships. To this end, this\npaper aims to propose KGrAt-Net, which is a Knowledge Graph Attention Network\ntext classifier. For the first time, it applies the knowledge graph attention\nnetwork to perform Automatic Personality Prediction (APP), according to the Big\nFive personality traits. After performing some preprocessing activities, it\nfirst tries to acquire a knowing-full representation of the knowledge behind\nthe concepts in the input text by building its equivalent knowledge graph. A\nknowledge graph collects interlinked descriptions of concepts, entities, and\nrelationships in a machine-readable form. Practically, it provides a\nmachine-readable cognitive understanding of concepts and semantic relationships\namong them. Then, applying the attention mechanism, it attempts to pay\nattention to the most relevant parts of the graph to predict the personality\ntraits of the input text. We used 2,467 essays from the Essays Dataset. The\nresults demonstrated that KGrAt-Net considerably improved personality\nprediction accuracies (up to 70.26% on average). Furthermore, KGrAt-Net also\nuses knowledge graph embedding to enrich the classification, which makes it\neven more accurate (on average, 72.41%) in APP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1\">Majid Ramezani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1\">Mohammad-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balafar_M/0/1/0/all/0/1\">Mohammad-Ali Balafar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking zero-shot and few-shot approaches for tokenization, tagging, and dependency parsing of Tagalog text. (arXiv:2208.01814v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.01814","description":"<p>The grammatical analysis of texts in any written language typically involves\na number of basic processing tasks, such as tokenization, morphological\ntagging, and dependency parsing. State-of-the-art systems can achieve high\naccuracy on these tasks for languages with large datasets, but yield poor\nresults for languages which have little to no annotated data. To address this\nissue for the Tagalog language, we investigate the use of alternative language\nresources for creating task-specific models in the absence of\ndependency-annotated Tagalog data. We also explore the use of word embeddings\nand data augmentation to improve performance when only a small amount of\nannotated Tagalog data is available. We show that these zero-shot and few-shot\napproaches yield substantial improvements on grammatical analysis of both\nin-domain and out-of-domain Tagalog text compared to state-of-the-art\nsupervised baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aquino_A/0/1/0/all/0/1\">Angelina Aquino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leon_F/0/1/0/all/0/1\">Franz de Leon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Examining Political Rhetoric with Epistemic Stance Detection. (arXiv:2212.14486v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.14486","description":"<p>Participants in political discourse employ rhetorical strategies -- such as\nhedging, attributions, or denials -- to display varying degrees of belief\ncommitments to claims proposed by themselves or others. Traditionally,\npolitical scientists have studied these epistemic phenomena through\nlabor-intensive manual content analysis. We propose to help automate such work\nthrough epistemic stance prediction, drawn from research in computational\nsemantics, to distinguish at the clausal level what is asserted, denied, or\nonly ambivalently suggested by the author or other mentioned entities (belief\nholders). We first develop a simple RoBERTa-based model for multi-source stance\npredictions that outperforms more complex state-of-the-art modeling. Then we\ndemonstrate its novel application to political science by conducting a\nlarge-scale analysis of the Mass Market Manifestos corpus of U.S. political\nopinion books, where we characterize trends in cited belief holders --\nrespected allies and opposed bogeymen -- across U.S. political ideologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ankita Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blodgett_S/0/1/0/all/0/1\">Su Lin Blodgett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gross_J/0/1/0/all/0/1\">Justin H Gross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_B/0/1/0/all/0/1\">Brendan O&#x27;Connor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion-Cause Pair Extraction as Question Answering. (arXiv:2301.01982v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.01982","description":"<p>The task of Emotion-Cause Pair Extraction (ECPE) aims to extract all\npotential emotion-cause pairs of a document without any annotation of emotion\nor cause clauses. Previous approaches on ECPE have tried to improve\nconventional two-step processing schemes by using complex architectures for\nmodeling emotion-cause interaction. In this paper, we cast the ECPE task to the\nquestion answering (QA) problem and propose simple yet effective BERT-based\nsolutions to tackle it. Given a document, our Guided-QA model first predicts\nthe best emotion clause using a fixed question. Then the predicted emotion is\nused as a question to predict the most potential cause for the emotion. We\nevaluate our model on a standard ECPE corpus. The experimental results show\nthat despite its simplicity, our Guided-QA achieves promising results and is\neasy to reproduce. The code of Guided-QA is also provided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huu-Hiep Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh-Tien Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-01-08T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}