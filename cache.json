{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-08-04T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales. (arXiv:2308.01320v1 [cs.LG])","link":"http://arxiv.org/abs/2308.01320","description":"<p>ChatGPT-like models have revolutionized various applications in artificial\nintelligence, from summarization and coding to translation, matching or even\nsurpassing human performance. However, the current landscape lacks an\naccessible, efficient, and cost-effective end-to-end RLHF (Reinforcement\nLearning with Human Feedback) training pipeline for these powerful models,\nparticularly when training at the scale of billions of parameters. This paper\nintroduces DeepSpeed-Chat, a novel system that democratizes RLHF training,\nmaking it accessible to the AI community. DeepSpeed-Chat offers three key\ncapabilities: an easy-to-use training and inference experience for ChatGPT-like\nmodels, a DeepSpeed-RLHF pipeline that replicates the training pipeline from\nInstructGPT, and a robust DeepSpeed-RLHF system that combines various\noptimizations for training and inference in a unified way. The system delivers\nunparalleled efficiency and scalability, enabling training of models with\nhundreds of billions of parameters in record time and at a fraction of the\ncost. With this development, DeepSpeed-Chat paves the way for broader access to\nadvanced RLHF training, even for data scientists with limited resources,\nthereby fostering innovation and further development in the field of AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aminabadi_R/0/1/0/all/0/1\">Reza Yazdani Aminabadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruwase_O/0/1/0/all/0/1\">Olatunji Ruwase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajbhandari_S/0/1/0/all/0/1\">Samyam Rajbhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaoxia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awan_A/0/1/0/all/0/1\">Ammar Ahmad Awan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasley_J/0/1/0/all/0/1\">Jeff Rasley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minjia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Conglong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holmes_C/0/1/0/all/0/1\">Connor Holmes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhongzhu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wyatt_M/0/1/0/all/0/1\">Michael Wyatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1\">Molly Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurilenko_L/0/1/0/all/0/1\">Lev Kurilenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Heyang Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_M/0/1/0/all/0/1\">Masahiro Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_S/0/1/0/all/0/1\">Shuai Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuaiwen Leon Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxiong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Careful Whisper -- leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification. (arXiv:2308.01327v1 [cs.SD])","link":"http://arxiv.org/abs/2308.01327","description":"<p>This paper presents a fully automated approach for identifying speech\nanomalies from voice recordings to aid in the assessment of speech impairments.\nBy combining Connectionist Temporal Classification (CTC) and\nencoder-decoder-based automatic speech recognition models, we generate rich\nacoustic and clean transcripts. We then apply several natural language\nprocessing methods to extract features from these transcripts to produce\nprototypes of healthy speech. Basic distance measures from these prototypes\nserve as input features for standard machine learning classifiers, yielding\nhuman-level accuracy for the distinction between recordings of people with\naphasia and a healthy control group. Furthermore, the most frequently occurring\naphasia types can be distinguished with 90% accuracy. The pipeline is directly\napplicable to other diseases and languages, showing promise for robustly\nextracting diagnostic speech biomarkers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wagner_L/0/1/0/all/0/1\">Laurin Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zusag_M/0/1/0/all/0/1\">Mario Zusag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloder_T/0/1/0/all/0/1\">Theresa Bloder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empirical Translation Process Research: Past and Possible Future Perspectives. (arXiv:2308.01368v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01368","description":"<p>Over the past four decades, efforts have been made to develop and evaluate\nmodels for Empirical Translation Process Research (TPR), yet a comprehensive\nframework remains elusive. This article traces the evolution of empirical TPR\nwithin the CRITT TPR-DB tradition and proposes the Free Energy Principle (FEP)\nand Active Inference (AIF) as a framework for modeling deeply embedded\ntranslation processes. It introduces novel approaches for quantifying\nfundamental concepts of Relevance Theory (relevance, s-mode, i-mode), and\nestablishes their relation to the Monitor Model, framing relevance maximization\nas a special case of minimizing free energy. FEP/AIF provides a mathematically\nrigorous foundation that enables modeling of deep temporal architectures in\nwhich embedded translation processes unfold on different timelines. This\nframework opens up exciting prospects for future research in predictive TPR,\nlikely to enrich our comprehension of human translation processes, and making\nvaluable contributions to the wider realm of translation studies and the design\nof cognitive architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carl_M/0/1/0/all/0/1\">Michael Carl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing Machine Translation through Prompt Engineering: An Investigation into ChatGPT's Customizability. (arXiv:2308.01391v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01391","description":"<p>This paper explores the influence of integrating the purpose of the\ntranslation and the target audience into prompts on the quality of translations\nproduced by ChatGPT. Drawing on previous translation studies, industry\npractices, and ISO standards, the research underscores the significance of the\npre-production phase in the translation process. The study reveals that the\ninclusion of suitable prompts in large-scale language models like ChatGPT can\nyield flexible translations, a feat yet to be realized by conventional Machine\nTranslation (MT). The research scrutinizes the changes in translation quality\nwhen prompts are used to generate translations that meet specific conditions.\nThe evaluation is conducted from a practicing translator's viewpoint, both\nsubjectively and qualitatively, supplemented by the use of OpenAI's word\nembedding API for cosine similarity calculations. The findings suggest that the\nintegration of the purpose and target audience into prompts can indeed modify\nthe generated translations, generally enhancing the translation quality by\nindustry standards. The study also demonstrates the practical application of\nthe \"good translation\" concept, particularly in the context of marketing\ndocuments and culturally dependent idioms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamada_M/0/1/0/all/0/1\">Masaru Yamada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Model the World with Language. (arXiv:2308.01399v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01399","description":"<p>To interact with humans in the world, agents need to understand the diverse\ntypes of language that people use, relate them to the visual world, and act\nbased on them. While current agents learn to execute simple language\ninstructions from task rewards, we aim to build agents that leverage diverse\nlanguage that conveys general knowledge, describes the state of the world,\nprovides interactive feedback, and more. Our key idea is that language helps\nagents predict the future: what will be observed, how the world will behave,\nand which situations will be rewarded. This perspective unifies language\nunderstanding with future prediction as a powerful self-supervised learning\nobjective. We present Dynalang, an agent that learns a multimodal world model\nthat predicts future text and image representations and learns to act from\nimagined model rollouts. Unlike traditional agents that use language only to\npredict actions, Dynalang acquires rich language understanding by using past\nlanguage also to predict future language, video, and rewards. In addition to\nlearning from online interaction in an environment, Dynalang can be pretrained\non datasets of text, video, or both without actions or rewards. From using\nlanguage hints in grid worlds to navigating photorealistic scans of homes,\nDynalang utilizes diverse types of language to improve task performance,\nincluding environment descriptions, game rules, and instructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jessy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuqing Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watkins_O/0/1/0/all/0/1\">Olivia Watkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hafner_D/0/1/0/all/0/1\">Danijar Hafner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1\">Anca Dragan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models. (arXiv:2308.01404v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01404","description":"<p>Are current language models capable of deception and lie detection? We study\nthis question by introducing a text-based game called $\\textit{Hoodwinked}$,\ninspired by $\\textit{Mafia}$ and $\\textit{Among Us}$. Players are locked in a\nhouse and must find a key to escape, but one player is tasked with killing the\nothers. Each time a murder is committed, the surviving players have a natural\nlanguage discussion then vote to banish one player from the game. We conduct\nexperiments with agents controlled by GPT-3, GPT-3.5, and GPT-4 and find\nevidence of deception and lie detection capabilities. The killer often denies\ntheir crime and accuses others, leading to measurable effects on voting\noutcomes. More advanced models are more effective killers, outperforming\nsmaller models in 18 of 24 pairwise comparisons. Secondary metrics provide\nevidence that this improvement is not mediated by different actions, but rather\nby stronger deception capabilities during discussions. Overall, we find\nsubstantial evidence that current language models are capable of deception. To\nbetter evaluate the ability of AI agents to deceive humans, we make this game\npublicly available at https://hoodwinked.ai/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+OGara_A/0/1/0/all/0/1\">Aidan O&#x27;Gara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UPB at IberLEF-2023 AuTexTification: Detection of Machine-Generated Text using Transformer Ensembles. (arXiv:2308.01408v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01408","description":"<p>This paper describes the solutions submitted by the UPB team to the\nAuTexTification shared task, featured as part of IberLEF-2023. Our team\nparticipated in the first subtask, identifying text documents produced by large\nlanguage models instead of humans. The organizers provided a bilingual dataset\nfor this subtask, comprising English and Spanish texts covering multiple\ndomains, such as legal texts, social media posts, and how-to articles. We\nexperimented mostly with deep learning models based on Transformers, as well as\ntraining techniques such as multi-task learning and virtual adversarial\ntraining to obtain better results. We submitted three runs, two of which\nconsisted of ensemble models. Our best-performing model achieved macro\nF1-scores of 66.63% on the English dataset and 67.10% on the Spanish dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Preda_A/0/1/0/all/0/1\">Andrei-Alexandru Preda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cercel_D/0/1/0/all/0/1\">Dumitru-Clementin Cercel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rebedea_T/0/1/0/all/0/1\">Traian Rebedea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiru_C/0/1/0/all/0/1\">Costin-Gabriel Chiru</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaFiCMIL: Rethinking Large File Classification from the Perspective of Correlated Multiple Instance Learning. (arXiv:2308.01413v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01413","description":"<p>Transformer-based models have revolutionized the performance of a wide range\nof language tasks. Intuitively, one might expect text classification, which\ndoes not necessitate as many high-level representations as generative tasks, to\nbe comprehensively addressed with the powerful representation capabilities of\nTransformers. However, in reality, there remains significant potential for\nenhancement, particularly in the areas of multi-class and multi-label\nclassification of lengthy textual documents and other large files. The\nperformance of Transformer-based models is mainly hindered by a major\nlimitation: a restricted input length, e.g., 512 tokens for BERT. While an\nincrease in GPU memory can marginally extend this limit, practical real-world\napplications often operate under constrained GPU resources. In this work, we\ntackle the input limit problem from the perspective of correlated multiple\ninstance learning. The proposed approach, LaFiCMIL, serves as a versatile\nframework applicable to various large file classification tasks covering\nbinary, multi-class, and multi-label classification tasks, spanning various\ndomains including Natural Language Processing, Programming Language Processing,\nand Android Analysis. To evaluate its effectiveness, we employ eight benchmark\ndatasets pertaining to Long Document Classification, Code Defect Detection, and\nAndroid Malware Detection. Leveraging BERT-family models as feature extractors,\nour experimental results demonstrate that LaFiCMIL achieves new\nstate-of-the-art performance across all benchmark datasets. This is largely\nattributable to its capability of scaling BERT up to nearly 20K tokens, running\non a single Tesla V-100 GPU with 32G of memory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tiezhu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pian_W/0/1/0/all/0/1\">Weiguo Pian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daoudi_N/0/1/0/all/0/1\">Nadia Daoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allix_K/0/1/0/all/0/1\">Kevin Allix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bissyande_T/0/1/0/all/0/1\">Tegawend&#xe9; F. Bissyand&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_J/0/1/0/all/0/1\">Jacques Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HouYi: An open-source large language model specially designed for renewable energy and carbon neutrality field. (arXiv:2308.01414v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01414","description":"<p>Renewable energy is important for achieving carbon neutrality goal. With the\ngreat success of Large Language Models (LLMs) like ChatGPT in automatic content\ngeneration, LLMs are playing an increasingly important role. However, there has\nnot been a specially designed LLM for renewable energy. Meanwhile, there has\nnot been any dataset of renewable energy for training LLMs. Therefore, this\npaper published the first open-source Renewable Energy Academic Paper (REAP)\ndataset for non-commercial LLM research of renewable energy. REAP dataset is\ncollected through searching the title and abstract of 1,168,970 academic\nliteratures from Web of Science. Based on REAP dataset, HouYi model, the first\nLLM for renewable energy, is developed through finetuning general LLMs. HouYi\ndemonstrated powerful academic paper paragraph generation ability in renewable\nenergy field. Experiments show that its ability to generate academic papers on\nrenewable energy is comparable to ChatGPT, slightly outperforms Claude, ERNIE\nBot and SparkDesk, and significantly outperforms open-source LLaMA-13B model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_M/0/1/0/all/0/1\">Mingliang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhihao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yusheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zizhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunxiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_C/0/1/0/all/0/1\">Chunjin Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinfu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Daren Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language Model. (arXiv:2308.01415v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01415","description":"<p>At the beginning era of large language model, it is quite critical to\ngenerate a high-quality financial dataset to fine-tune a large language model\nfor financial related tasks. Thus, this paper presents a carefully designed\ndata creation pipeline for this purpose. Particularly, we initiate a dialogue\nbetween an AI investor and financial expert using ChatGPT and incorporate the\nfeedback of human financial experts, leading to the refinement of the dataset.\nThis pipeline yielded a robust instruction tuning dataset comprised of 103k\nmulti-turn chats. Extensive experiments have been conducted on this dataset to\nevaluate the model's performance by adopting an external GPT-4 as the judge.\nThe promising experimental results verify that our approach led to significant\nadvancements in generating accurate, relevant, and financial-style responses\nfrom AI models, and thus providing a powerful tool for applications within the\nfinancial sector.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junda Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaofeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAP-sLDA: An Interpretable Interface for Exploring Unstructured Text. (arXiv:2308.01420v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01420","description":"<p>A common way to explore text corpora is through low-dimensional projections\nof the documents, where one hopes that thematically similar documents will be\nclustered together in the projected space. However, popular algorithms for\ndimensionality reduction of text corpora, like Latent Dirichlet Allocation\n(LDA), often produce projections that do not capture human notions of document\nsimilarity. We propose a semi-supervised human-in-the-loop LDA-based method for\nlearning topics that preserve semantically meaningful relationships between\ndocuments in low-dimensional projections. On synthetic corpora, our method\nyields more interpretable projections than baseline methods with only a\nfraction of labels provided. On a real corpus, we obtain qualitatively similar\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Badrinath_C/0/1/0/all/0/1\">Charumathi Badrinath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Weiwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1\">Finale Doshi-Velez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks. (arXiv:2308.01423v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01423","description":"<p>ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to\npredict and generate of metal-organic frameworks (MOFs). By leveraging a\nlarge-scale language model (gpt-3.5-turbo), ChatMOF extracts key details from\ntextual inputs and delivers appropriate responses, thus eliminating the\nnecessity for rigid structured queries. The system is comprised of three core\ncomponents (i.e. an agent, a toolkit, and an evaluator) and it forms a robust\npipeline that manages a variety of tasks, including data retrieval, property\nprediction, and structure generation. The study further explores the merits and\nconstraints of using large language models (LLMs) AI system in material\nsciences using and showcases its transformative potential for future\nadvancements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yeonghun Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jihan Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FinVis-GPT: A Multimodal Large Language Model for Financial Chart Analysis. (arXiv:2308.01430v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01430","description":"<p>In this paper, we propose FinVis-GPT, a novel multimodal large language model\n(LLM) specifically designed for financial chart analysis. By leveraging the\npower of LLMs and incorporating instruction tuning and multimodal capabilities,\nFinVis-GPT is capable of interpreting financial charts and providing valuable\nanalysis. To train FinVis-GPT, a financial task oriented dataset was generated\nfor pre-training alignment and instruction tuning, comprising various types of\nfinancial charts and their corresponding descriptions. We evaluate the model\nperformance via several case studies due to the time limit, and the promising\nresults demonstrated that FinVis-GPT is superior in various financial chart\nrelated tasks, including generating descriptions, answering questions and\npredicting future market trends, surpassing existing state-of-the-art\nmultimodal LLMs. The proposed FinVis-GPT serves as a pioneering effort in\nutilizing multimodal LLMs in the finance domain and our generated dataset will\nbe release for public use in the near future to speedup related research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junda Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soon_J/0/1/0/all/0/1\">Jaehyeon Soon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaofeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reverse Stable Diffusion: What prompt was used to generate this image?. (arXiv:2308.01472v1 [cs.CV])","link":"http://arxiv.org/abs/2308.01472","description":"<p>Text-to-image diffusion models such as Stable Diffusion have recently\nattracted the interest of many researchers, and inverting the diffusion process\ncan play an important role in better understanding the generative process and\nhow to engineer prompts in order to obtain the desired images. To this end, we\nintroduce the new task of predicting the text prompt given an image generated\nby a generative diffusion model. We combine a series of white-box and black-box\nmodels (with and without access to the weights of the diffusion network) to\ndeal with the proposed task. We propose a novel learning framework comprising\nof a joint prompt regression and multi-label vocabulary classification\nobjective that generates improved prompts. To further improve our method, we\nemploy a curriculum learning procedure that promotes the learning of\nimage-prompt pairs with lower labeling noise (i.e. that are better aligned),\nand an unsupervised domain-adaptive kernel learning method that uses the\nsimilarities between samples in the source and target domains as extra\nfeatures. We conduct experiments on the DiffusionDB data set, predicting text\nprompts from images generated by Stable Diffusion. Our novel learning framework\nproduces excellent results on the aforementioned task, yielding the highest\ngains when applied on the white-box model. In addition, we make an interesting\ndiscovery: training a diffusion model on the prompt generation task can make\nthe model generate images that are much better aligned with the input prompts,\nwhen the model is directly reused for text-to-image generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Croitoru_F/0/1/0/all/0/1\">Florinel-Alin Croitoru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hondru_V/0/1/0/all/0/1\">Vlad Hondru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Reinforcement Learning for Communication Strategies in a Task-Initiative Setting. (arXiv:2308.01479v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01479","description":"<p>Many conversational domains require the system to present nuanced information\nto users. Such systems must follow up what they say to address clarification\nquestions and repair misunderstandings. In this work, we explore this\ninteractive strategy in a referential communication task. Using simulation, we\nanalyze the communication trade-offs between initial presentation and\nsubsequent followup as a function of user clarification strategy, and compare\nthe performance of several baseline strategies to policies derived by\nreinforcement learning. We find surprising advantages to coherence-based\nrepresentations of dialogue strategy, which bring minimal data requirements,\nexplainable choices, and strong audit capabilities, but incur little loss in\npredicted outcomes across a wide range of user models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalid_B/0/1/0/all/0/1\">Baber Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_M/0/1/0/all/0/1\">Matthew Stone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Model Displays Emergent Ability to Interpret Novel Literary Metaphors. (arXiv:2308.01497v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01497","description":"<p>Recent advances in the performance of large language models (LLMs) have\nsparked debate over whether, given sufficient training, high-level human\nabilities emerge in such generic forms of artificial intelligence (AI). Despite\nthe exceptional performance of LLMs on a wide range of tasks involving natural\nlanguage processing and reasoning, there has been sharp disagreement as to\nwhether their abilities extend to more creative human abilities. A core example\nis the ability to interpret novel metaphors. Given the enormous and non-curated\ntext corpora used to train LLMs, a serious obstacle to designing tests is the\nrequirement of finding novel yet high-quality metaphors that are unlikely to\nhave been included in the training data. Here we assessed the ability of GPT-4,\na state-of-the-art large language model, to provide natural-language\ninterpretations of novel literary metaphors drawn from Serbian poetry and\ntranslated into English. Despite exhibiting no signs of having been exposed to\nthese metaphors previously, the AI system consistently produced detailed and\nincisive interpretations. Human judge - blind to the fact that an AI model was\ninvolved - rated metaphor interpretations generated by GPT-4 as superior to\nthose provided by a group of college students. In interpreting reversed\nmetaphors, GPT-4, as well as humans, exhibited signs of sensitivity to the\nGricean cooperative principle. These results indicate that LLMs such as GPT-4\nhave acquired an emergent ability to interpret complex novel metaphors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ichien_N/0/1/0/all/0/1\">Nicholas Ichien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamenkovic_D/0/1/0/all/0/1\">Du&#x161;an Stamenkovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holyoak_K/0/1/0/all/0/1\">Keith J. Holyoak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing scalable strategies for generating numerical perspectives. (arXiv:2308.01535v1 [cs.HC])","link":"http://arxiv.org/abs/2308.01535","description":"<p>Numerical perspectives help people understand extreme and unfamiliar numbers\n(e.g., \\$330 billion is about \\$1,000 per person in the United States). While\nresearch shows perspectives to be helpful, generating them at scale is\nchallenging both because it is difficult to identify what makes some analogies\nmore helpful than others, and because what is most helpful can vary based on\nthe context in which a given number appears. Here we present and compare three\npolicies for large-scale perspective generation: a rule-based approach, a\ncrowdsourced system, and a model that uses Wikipedia data and semantic\nsimilarity (via BERT embeddings) to generate context-specific perspectives. We\nfind that the combination of these three approaches dominates any single\nmethod, with different approaches excelling in different settings and users\ndisplaying heterogeneous preferences across approaches. We conclude by\ndiscussing our deployment of perspectives in a widely-used online word\nprocessor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Hancheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spatharioti_S/0/1/0/all/0/1\">Sofia Eleni Spatharioti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_D/0/1/0/all/0/1\">Daniel G. Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofman_J/0/1/0/all/0/1\">Jake M. Hofman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Neurons in Pretrained Text-Only Transformers. (arXiv:2308.01544v1 [cs.CV])","link":"http://arxiv.org/abs/2308.01544","description":"<p>Language models demonstrate remarkable capacity to generalize representations\nlearned in one modality to downstream tasks in other modalities. Can we trace\nthis ability to individual neurons? We study the case where a frozen text\ntransformer is augmented with vision using a self-supervised visual encoder and\na single linear projection learned on an image-to-text task. Outputs of the\nprojection layer are not immediately decodable into language describing image\ncontent; instead, we find that translation between modalities occurs deeper\nwithin the transformer. We introduce a procedure for identifying \"multimodal\nneurons\" that convert visual representations into corresponding text, and\ndecoding the concepts they inject into the model's residual stream. In a series\nof experiments, we show that multimodal neurons operate on specific visual\nconcepts across inputs, and have a systematic causal effect on image\ncaptioning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwettmann_S/0/1/0/all/0/1\">Sarah Schwettmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_N/0/1/0/all/0/1\">Neil Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent. (arXiv:2308.01552v1 [cs.AI])","link":"http://arxiv.org/abs/2308.01552","description":"<p>This research paper delves into the integration of OpenAI's ChatGPT into\nembodied agent systems, evaluating its influence on interactive decision-making\nbenchmark. Drawing a parallel to the concept of people assuming roles according\nto their unique strengths, we introduce InterAct. In this approach, we feed\nChatGPT with varied prompts, assigning it a numerous roles like a checker and a\nsorter, then integrating them with the original language model. Our research\nshows a remarkable success rate of 98% in AlfWorld, which consists of 6\ndifferent tasks in a simulated household environment, emphasizing the\nsignificance of proficient prompt engineering. The results highlight ChatGPT's\ncompetence in comprehending and performing intricate tasks effectively in\nreal-world settings, thus paving the way for further advancements in task\nplanning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Po-Lin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Cheng-Shang Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Holy Grail 2.0: From Natural Language to Constraint Models. (arXiv:2308.01589v1 [cs.AI])","link":"http://arxiv.org/abs/2308.01589","description":"<p>Twenty-seven years ago, E. Freuder highlighted that \"Constraint programming\nrepresents one of the closest approaches computer science has yet made to the\nHoly Grail of programming: the user states the problem, the computer solves\nit\". Nowadays, CP users have great modeling tools available (like Minizinc and\nCPMpy), allowing them to formulate the problem and then let a solver do the\nrest of the job, getting closer to the stated goal. However, this still\nrequires the CP user to know the formalism and respect it. Another significant\nchallenge lies in the expertise required to effectively model combinatorial\nproblems. All this limits the wider adoption of CP. In this position paper, we\ninvestigate a possible approach to leverage pre-trained Large Language Models\nto extract models from textual problem descriptions. More specifically, we take\ninspiration from the Natural Language Processing for Optimization (NL4OPT)\nchallenge and present early results with a decomposition-based prompting\napproach to GPT Models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsouros_D/0/1/0/all/0/1\">Dimos Tsouros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verhaeghe_H/0/1/0/all/0/1\">H&#xe9;l&#xe8;ne Verhaeghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadioglu_S/0/1/0/all/0/1\">Serdar Kad&#x131;o&#x11f;lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guns_T/0/1/0/all/0/1\">Tias Guns</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating ChatGPT text-mining of clinical records for obesity monitoring. (arXiv:2308.01666v1 [cs.IR])","link":"http://arxiv.org/abs/2308.01666","description":"<p>Background: Veterinary clinical narratives remain a largely untapped resource\nfor addressing complex diseases. Here we compare the ability of a large\nlanguage model (ChatGPT) and a previously developed regular expression (RegexT)\nto identify overweight body condition scores (BCS) in veterinary narratives.\nMethods: BCS values were extracted from 4,415 anonymised clinical narratives\nusing either RegexT or by appending the narrative to a prompt sent to ChatGPT\ncoercing the model to return the BCS information. Data were manually reviewed\nfor comparison. Results: The precision of RegexT was higher (100%, 95% CI\n94.81-100%) than the ChatGPT (89.3%; 95% CI82.75-93.64%). However, the recall\nof ChatGPT (100%. 95% CI 96.18-100%) was considerably higher than that of\nRegexT (72.6%, 95% CI 63.92-79.94%). Limitations: Subtle prompt engineering is\nneeded to improve ChatGPT output. Conclusions: Large language models create\ndiverse opportunities and, whilst complex, present an intuitive interface to\ninformation but require careful implementation to avoid unpredictable errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fins_I/0/1/0/all/0/1\">Ivo S. Fins</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Davies_H/0/1/0/all/0/1\">Heather Davies</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Farrell_S/0/1/0/all/0/1\">Sean Farrell</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Torres_J/0/1/0/all/0/1\">Jose R.Torres</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Pinchbeck_G/0/1/0/all/0/1\">Gina Pinchbeck</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Radford_A/0/1/0/all/0/1\">Alan D. Radford</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Noble_P/0/1/0/all/0/1\">Peter-John Noble</a> (1) ((1) Small Animal Veterinary Surveillance Network, Institute of Infection, Veterinary and Ecological Sciences, University of Liverpool, Liverpool, UK, (2) Department of Computer Science, Durham University, Durham, UK, (3) Institute for Animal Health and Food Safety, University of Las Palmas de Gran Canaria, Las Palmas, Canary Archipelago, Spain)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NBIAS: A Natural Language Processing Framework for Bias Identification in Text. (arXiv:2308.01681v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01681","description":"<p>Bias in textual data can lead to skewed interpretations and outcomes when the\ndata is used. These biases could perpetuate stereotypes, discrimination, or\nother forms of unfair treatment. An algorithm trained on biased data ends up\nmaking decisions that disproportionately impact a certain group of people.\nTherefore, it is crucial to detect and remove these biases to ensure the fair\nand ethical use of data. To this end, we develop a comprehensive and robust\nframework \\textsc{Nbias} that consists of a data layer, corpus contruction,\nmodel development layer and an evaluation layer. The dataset is constructed by\ncollecting diverse data from various fields, including social media,\nhealthcare, and job hiring portals. As such, we applied a transformer-based\ntoken classification model that is able to identify bias words/ phrases through\na unique named entity. In the assessment procedure, we incorporate a blend of\nquantitative and qualitative evaluations to gauge the effectiveness of our\nmodels. We achieve accuracy improvements ranging from 1% to 8% compared to\nbaselines. We are also able to generate a robust understanding of the model\nfunctioning, capturing not only numerical data but also the quality and\nintricacies of its performance. The proposed approach is applicable to a\nvariety of biases and contributes to the fair and ethical use of textual data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Razaa_S/0/1/0/all/0/1\">Shaina Razaa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1\">Muskan Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reji_D/0/1/0/all/0/1\">Deepak John Reji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bashir_S/0/1/0/all/0/1\">Syed Raza Bashir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Chen Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Baby's CoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models. (arXiv:2308.01684v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01684","description":"<p>Large Language Models (LLMs) demonstrate remarkable performance on a variety\nof Natural Language Understanding (NLU) tasks, primarily due to their\nin-context learning ability. This ability is utilized in our proposed\n\"CoThought\" pipeline, which efficiently trains smaller \"baby\" language models\n(BabyLMs) by leveraging the Chain of Thought (CoT) prompting of LLMs. Our\npipeline restructures a dataset of less than 100M in size using GPT-3.5-turbo,\ntransforming it into task-oriented, human-readable texts that are comparable to\nthe school texts for language learners. The BabyLM is then pretrained on this\nrestructured dataset in a RoBERTa (Liu et al., 2019) fashion. In evaluations\nacross 4 benchmarks, our BabyLM outperforms the RoBERTa-base in 10 linguistic,\nNLU, and question answering tasks by more than 3 points, showing superior\nability to extract contextual information. These results suggest that compact\nLMs pretrained on small, LLM-restructured data can better understand tasks and\nachieve improved performance. The code for data processing and model training\nis available at: https://github.com/oooranz/Baby-CoThought.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Han Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1\">Bolei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rugamer_D/0/1/0/all/0/1\">David R&#xfc;gamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_E/0/1/0/all/0/1\">Ercong Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Large Language Models for Complex Structured Medical Tasks. (arXiv:2308.01727v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01727","description":"<p>This paper introduces an approach that combines the language reasoning\ncapabilities of large language models (LLMs) with the benefits of local\ntraining to tackle complex, domain-specific tasks. Specifically, the authors\ndemonstrate their approach by extracting structured condition codes from\npathology reports. The proposed approach utilizes local LLMs, which can be\nfine-tuned to respond to specific generative instructions and provide\nstructured outputs. The authors collected a dataset of over 150k uncurated\nsurgical pathology reports, containing gross descriptions, final diagnoses, and\ncondition codes. They trained different model architectures, including LLaMA,\nBERT and LongFormer and evaluated their performance. The results show that the\nLLaMA-based models significantly outperform BERT-style models across all\nevaluated metrics, even with extremely reduced precision. The LLaMA models\nperformed especially well with large datasets, demonstrating their ability to\nhandle complex, multi-label tasks. Overall, this work presents an effective\napproach for utilizing LLMs to perform domain-specific tasks using accessible\nhardware, with potential applications in the medical domain, where complex data\nextraction and classification are required.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bumgardner_V/0/1/0/all/0/1\">V. K. Cody Bumgardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullen_A/0/1/0/all/0/1\">Aaron Mullen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armstrong_S/0/1/0/all/0/1\">Sam Armstrong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hickey_C/0/1/0/all/0/1\">Caylin Hickey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talbert_J/0/1/0/all/0/1\">Jeff Talbert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ambient Adventures: Teaching ChatGPT on Developing Complex Stories. (arXiv:2308.01734v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01734","description":"<p>Imaginative play is an area of creativity that could allow robots to engage\nwith the world around them in a much more personified way. Imaginary play can\nbe seen as taking real objects and locations and using them as imaginary\nobjects and locations in virtual scenarios. We adopted the story generation\ncapability of large language models (LLMs) to obtain the stories used for\nimaginary play with human-written prompts. Those generated stories will be\nsimplified and mapped into action sequences that can guide the agent in\nimaginary play. To evaluate whether the agent can successfully finish the\nimaginary play, we also designed a text adventure game to simulate a house as\nthe playground for the agent to interact.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zexin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1\">Eric Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eaton_K/0/1/0/all/0/1\">Kenneth Eaton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiangyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark Riedl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supply chain emission estimation using large language models. (arXiv:2308.01741v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01741","description":"<p>Large enterprises face a crucial imperative to achieve the Sustainable\nDevelopment Goals (SDGs), especially goal 13, which focuses on combating\nclimate change and its impacts. To mitigate the effects of climate change,\nreducing enterprise Scope 3 (supply chain emissions) is vital, as it accounts\nfor more than 90\\% of total emission inventories. However, tracking Scope 3\nemissions proves challenging, as data must be collected from thousands of\nupstream and downstream suppliers.To address the above mentioned challenges, we\npropose a first-of-a-kind framework that uses domain-adapted NLP foundation\nmodels to estimate Scope 3 emissions, by utilizing financial transactions as a\nproxy for purchased goods and services. We compared the performance of the\nproposed framework with the state-of-art text classification models such as\nTF-IDF, word2Vec, and Zero shot learning. Our results show that the\ndomain-adapted foundation model outperforms state-of-the-art text mining\ntechniques and performs as well as a subject matter expert (SME). The proposed\nframework could accelerate the Scope 3 estimation at Enterprise scale and will\nhelp to take appropriate climate actions to achieve SDG 13.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ayush Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmanaban_M/0/1/0/all/0/1\">Manikandan Padmanaban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazra_J/0/1/0/all/0/1\">Jagabondhu Hazra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godbole_S/0/1/0/all/0/1\">Shantanu Godbole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weldemariam_K/0/1/0/all/0/1\">Kommy Weldemariam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Correction Remain An Problem For Large Language Models?. (arXiv:2308.01776v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01776","description":"<p>As large language models, such as GPT, continue to advance the capabilities\nof natural language processing (NLP), the question arises: does the problem of\ncorrection still persist? This paper investigates the role of correction in the\ncontext of large language models by conducting two experiments. The first\nexperiment focuses on correction as a standalone task, employing few-shot\nlearning techniques with GPT-like models for error correction. The second\nexperiment explores the notion of correction as a preparatory task for other\nNLP tasks, examining whether large language models can tolerate and perform\nadequately on texts containing certain levels of noise or errors. By addressing\nthese experiments, we aim to shed light on the significance of correction in\nthe era of large language models and its implications for various NLP\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaowu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaotian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lexicon and Rule-based Word Lemmatization Approach for the Somali Language. (arXiv:2308.01785v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01785","description":"<p>Lemmatization is a Natural Language Processing (NLP) technique used to\nnormalize text by changing morphological derivations of words to their root\nforms. It is used as a core pre-processing step in many NLP tasks including\ntext indexing, information retrieval, and machine learning for NLP, among\nothers. This paper pioneers the development of text lemmatization for the\nSomali language, a low-resource language with very limited or no prior\neffective adoption of NLP methods and datasets. We especially develop a lexicon\nand rule-based lemmatizer for Somali text, which is a starting point for a\nfull-fledged Somali lemmatization system for various NLP tasks. With\nconsideration of the language morphological rules, we have developed an initial\nlexicon of 1247 root words and 7173 derivationally related terms enriched with\nrules for lemmatizing words not present in the lexicon. We have tested the\nalgorithm on 120 documents of various lengths including news articles, social\nmedia posts, and text messages. Our initial results demonstrate that the\nalgorithm achieves an accuracy of 57\\% for relatively long documents (e.g. full\nnews articles), 60.57\\% for news article extracts, and high accuracy of 95.87\\%\nfor short texts such as social media messages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_S/0/1/0/all/0/1\">Shafie Abdi Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1\">Muhidin Abdullahi Mohamed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Relationship on Learning Mathematical Reasoning with Large Language Models. (arXiv:2308.01825v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01825","description":"<p>Mathematical reasoning is a challenging task for large language models\n(LLMs), while the scaling relationship of it with respect to LLM capacity is\nunder-explored. In this paper, we investigate how the pre-training loss,\nsupervised data amount, and augmented data amount influence the reasoning\nperformances of a supervised LLM. We find that pre-training loss is a better\nindicator of the model's performance than the model's parameter count. We apply\nsupervised fine-tuning (SFT) with different amounts of supervised data and\nempirically find a log-linear relation between data amount and model\nperformance, and we find better models improve less with enlarged supervised\ndatasets. To augment more data samples for improving model performances without\nany human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT\nuses supervised models to generate and collect correct reasoning paths as\naugmented fine-tuning datasets. We find with augmented samples containing more\ndistinct reasoning paths, RFT improves mathematical reasoning performance more\nfor LLMs. We also find RFT brings more improvement for less performant LLMs.\nFurthermore, we combine rejection samples from multiple models which push\nLLaMA-7B to an accuracy of 49.3% and outperforms the supervised fine-tuning\n(SFT) accuracy of 35.9% significantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongyi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guanting Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Many-to-Many Spoken Language Translation via Unified Speech and Text Representation Learning with Unit-to-Unit Translation. (arXiv:2308.01831v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01831","description":"<p>In this paper, we propose a method to learn unified representations of\nmultilingual speech and text with a single model, especially focusing on the\npurpose of speech synthesis. We represent multilingual speech audio with speech\nunits, the quantized representations of speech features encoded from a\nself-supervised speech model. Therefore, we can focus on their linguistic\ncontent by treating the audio as pseudo text and can build a unified\nrepresentation of speech and text. Then, we propose to train an encoder-decoder\nstructured model with a Unit-to-Unit Translation (UTUT) objective on\nmultilingual data. Specifically, by conditioning the encoder with the source\nlanguage token and the decoder with the target language token, the model is\noptimized to translate the spoken language into that of the target language, in\na many-to-many language translation setting. Therefore, the model can build the\nknowledge of how spoken languages are comprehended and how to relate them to\ndifferent languages. A single pre-trained model with UTUT can be employed for\ndiverse multilingual speech- and text-related tasks, such as Speech-to-Speech\nTranslation (STS), multilingual Text-to-Speech Synthesis (TTS), and\nText-to-Speech Translation (TTST). By conducting comprehensive experiments\nencompassing various languages, we validate the efficacy of the proposed method\nacross diverse multilingual tasks. Moreover, we show UTUT can perform\nmany-to-many language STS, which has not been previously explored in the\nliterature. Samples are available on https://choijeongsoo.github.io/utut.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jeongsoo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dahun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1\">Yong Man Ro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Capability of Large Language Models to Measure Psychiatric Functioning. (arXiv:2308.01834v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01834","description":"<p>The current work investigates the capability of Large language models (LLMs)\nthat are explicitly trained on large corpuses of medical knowledge (Med-PaLM 2)\nto predict psychiatric functioning from patient interviews and clinical\ndescriptions without being trained to do so. To assess this, n = 145 depression\nand n =115 PTSD assessments and n = 46 clinical case studies across high\nprevalence/high comorbidity disorders (Depressive, Anxiety, Psychotic, trauma\nand stress, Addictive disorders) were analyzed using prompts to extract\nestimated clinical scores and diagnoses. Results demonstrate that Med-PaLM 2 is\ncapable of assessing psychiatric functioning across a range of psychiatric\nconditions with the strongest performance being the prediction of depression\nscores based on standardized assessments (Accuracy range= 0.80 - 0.84) which\nwere statistically indistinguishable from human clinical raters t(1,144) =\n1.20; p = 0.23. Results show the potential for general clinical language models\nto flexibly predict psychiatric risk based on free descriptions of functioning\nfrom both patients and clinicians.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galatzer_Levy_I/0/1/0/all/0/1\">Isaac R. Galatzer-Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1\">Daniel McDuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_V/0/1/0/all/0/1\">Vivek Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karthikesalingam_A/0/1/0/all/0/1\">Alan Karthikesalingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malgaroli_M/0/1/0/all/0/1\">Matteo Malgaroli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XNLP: An Interactive Demonstration System for Universal Structured NLP. (arXiv:2308.01846v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01846","description":"<p>Structured Natural Language Processing (XNLP) is an important subset of NLP\nthat entails understanding the underlying semantic or syntactic structure of\ntexts, which serves as a foundational component for many downstream\napplications. Despite certain recent efforts to explore universal solutions for\nspecific categories of XNLP tasks, a comprehensive and effective approach for\nunifying all XNLP tasks long remains underdeveloped. In the meanwhile, while\nXNLP demonstration systems are vital for researchers exploring various XNLP\ntasks, existing platforms can be limited to, e.g., supporting few XNLP tasks,\nlacking interactivity and universalness. To this end, we propose an advanced\nXNLP demonstration platform, where we propose leveraging LLM to achieve\nuniversal XNLP, with one model for all with high generalizability. Overall, our\nsystem advances in multiple aspects, including universal XNLP modeling, high\nperformance, interpretability, scalability, and interactivity, providing a\nunified platform for exploring diverse XNLP tasks in the community. XNLP is\nonline: https://xnlp.haofei.vip\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hao Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curricular Transfer Learning for Sentence Encoded Tasks. (arXiv:2308.01849v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01849","description":"<p>Fine-tuning language models in a downstream task is the standard approach for\nmany state-of-the-art methodologies in the field of NLP. However, when the\ndistribution between the source task and target task drifts, \\textit{e.g.},\nconversational environments, these gains tend to be diminished. This article\nproposes a sequence of pre-training steps (a curriculum) guided by \"data\nhacking\" and grammar analysis that allows further gradual adaptation between\npre-training distributions. In our experiments, we acquire a considerable\nimprovement from our method compared to other known pre-training approaches for\nthe MultiWoZ task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sa_J/0/1/0/all/0/1\">Jader Martins Camboim de S&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanches_M/0/1/0/all/0/1\">Matheus Ferraroni Sanches</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Souza_R/0/1/0/all/0/1\">Rafael Roque de Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reis_J/0/1/0/all/0/1\">J&#xfa;lio Cesar dos Reis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villas_L/0/1/0/all/0/1\">Leandro Aparecido Villas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation. (arXiv:2308.01861v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01861","description":"<p>In this work, we make the first attempt to evaluate LLMs in a more\nchallenging code generation scenario, i.e. class-level code generation. We\nfirst manually construct the first class-level code generation benchmark\nClassEval of 100 class-level Python code generation tasks with approximately\n500 person-hours. Based on it, we then perform the first study of 11\nstate-of-the-art LLMs on class-level code generation. Based on our results, we\nhave the following main findings. First, we find that all existing LLMs show\nmuch worse performance on class-level code generation compared to on standalone\nmethod-level code generation benchmarks like HumanEval; and the method-level\ncoding ability cannot equivalently reflect the class-level coding ability among\nLLMs. Second, we find that GPT-4 and GPT-3.5 still exhibit dominate superior\nthan other LLMs on class-level code generation, and the second-tier models\nincludes Instruct-Starcoder, Instruct-Codegen, and Wizardcoder with very\nsimilar performance. Third, we find that generating the entire class all at\nonce (i.e. holistic generation strategy) is the best generation strategy only\nfor GPT-4 and GPT-3.5, while method-by-method generation (i.e. incremental and\ncompositional) is better strategies for the other models with limited ability\nof understanding long instructions and utilizing the middle information.\nLastly, we find the limited model ability of generating method-dependent code\nand discuss the frequent error types in generated classes. Our benchmark is\navailable at https://github.com/FudanSELab/ClassEval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xueying Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaixin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanlin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiayi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_C/0/1/0/all/0/1\">Chaofeng Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yiling Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wider and Deeper LLM Networks are Fairer LLM Evaluators. (arXiv:2308.01862v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01862","description":"<p>Measuring the quality of responses generated by LLMs is a challenging task,\nparticularly when it comes to evaluating whether the response is aligned with\nhuman preference. A novel approach involves using the LLM itself to make\nevaluation and stabilizing the results through multiple independent\nevaluations, similar to a single-layer narrow LLM network. This network\nconsists of a fixed number of neurons, with each neuron being the same LLM. In\nthis paper, we draw upon the extensive research on deep neural networks to\nexplore whether deeper and wider networks can lead to fairer evaluations.\nSpecifically, inspired by the observation that different neurons in a neural\nnetwork are responsible for detecting different concepts, we first adaptively\ngenerate as many neuron roles as possible for each evaluation sample. Each\nperspective corresponds to the role of a specific LLM neuron in the first\nlayer. In subsequent layers, we follow the idea that higher layers in deep\nnetworks are responsible for more comprehensive features, each layer receives\nrepresentations from all neurons in the previous layer, integrating the locally\nlearned evaluation information to obtain a more comprehensive evaluation\nresult. Interestingly, this network design resembles the process of academic\npaper reviewing. To validate the effectiveness of our method, we construct the\nlargest and most diverse English evaluation benchmark LLMEval$^2$ for LLM\nevaluators, comprising 15 tasks, 8 abilities, and 2,553 samples. Experimental\nresults demonstrate that a wider network (involving many reviewers) with 2\nlayers (one round of discussion) performs the best, improving kappa correlation\ncoefficient from 0.28 to 0.34. We also leverage WideDeep to aid in the\nassessment of Chinese LLMs, which has accelerated the evaluation time by 4.6\ntimes, resulting in a 60% cost saving. WideDeep achieves a remarkable 93%\nagreement level among humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinghua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bowen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yangyu Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tingwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongbo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tag Prediction of Competitive Programming Problems using Deep Learning Techniques. (arXiv:2308.01863v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01863","description":"<p>In the past decade, the amount of research being done in the fields of\nmachine learning and deep learning, predominantly in the area of natural\nlanguage processing (NLP), has risen dramatically. A well-liked method for\ndeveloping programming abilities like logic building and problem solving is\ncompetitive programming. It can be tough for novices and even veteran\nprogrammers to traverse the wide collection of questions due to the massive\nnumber of accessible questions and the variety of themes, levels of difficulty,\nand questions offered. In order to help programmers find questions that are\nappropriate for their knowledge and interests, there is a need for an automated\nmethod. This can be done using automated tagging of the questions using Text\nClassification. Text classification is one of the important tasks widely\nresearched in the field of Natural Language Processing. In this paper, we\npresent a way to use text classification techniques to determine the domain of\na competitive programming problem. A variety of models, including are\nimplemented LSTM, GRU, and MLP. The dataset has been scraped from Codeforces, a\nmajor competitive programming website. A total of 2400 problems were scraped\nand preprocessed, which we used as a dataset for our training and testing of\nmodels. The maximum accuracy reached using our model is 78.0% by MLP(Multi\nLayer Perceptron).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lokat_T/0/1/0/all/0/1\">Taha Lokat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prajapati_D/0/1/0/all/0/1\">Divyam Prajapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labde_S/0/1/0/all/0/1\">Shubhada Labde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Thespian: Multi-Character Text Role-Playing Game Agents. (arXiv:2308.01872v1 [cs.AI])","link":"http://arxiv.org/abs/2308.01872","description":"<p>Text-adventure games and text role-playing games are grand challenges for\nreinforcement learning game playing agents. Text role-playing games are\nopen-ended environments where an agent must faithfully play a particular\ncharacter. We consider the distinction between characters and actors, where an\nactor agent has the ability to play multiple characters. We present a framework\nwe call a thespian agent that can learn to emulate multiple characters along\nwith a soft prompt that can be used to direct it as to which character to play\nat any time. We further describe an attention mechanism that allows the agent\nto learn new characters that are based on previously learned characters in a\nfew-shot fashion. We show that our agent outperforms the state of the art agent\nframework in multi-character learning and few-shot learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Christopher Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiangyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark Riedl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Athena 2.0: Discourse and User Modeling in Open Domain Dialogue. (arXiv:2308.01887v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01887","description":"<p>Conversational agents are consistently growing in popularity and many people\ninteract with them every day. While many conversational agents act as personal\nassistants, they can have many different goals. Some are task-oriented, such as\nproviding customer support for a bank or making a reservation. Others are\ndesigned to be empathetic and to form emotional connections with the user. The\nAlexa Prize Challenge aims to create a socialbot, which allows the user to\nengage in coherent conversations, on a range of popular topics that will\ninterest the user. Here we describe Athena 2.0, UCSC's conversational agent for\nAmazon's Socialbot Grand Challenge 4. Athena 2.0 utilizes a novel\nknowledge-grounded discourse model that tracks the entity links that Athena\nintroduces into the dialogue, and uses them to constrain named-entity\nrecognition and linking, and coreference resolution. Athena 2.0 also relies on\na user model to personalize topic selection and other aspects of the\nconversation to individual users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patil_O/0/1/0/all/0/1\">Omkar Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reed_L/0/1/0/all/0/1\">Lena Reed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowden_K/0/1/0/all/0/1\">Kevin K. Bowden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juraska_J/0/1/0/all/0/1\">Juraj Juraska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Wen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_V/0/1/0/all/0/1\">Vrindavan Harrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasekaran_R/0/1/0/all/0/1\">Rishi Rajasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_A/0/1/0/all/0/1\">Angela Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cecilia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamora_E/0/1/0/all/0/1\">Eduardo Zamora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1\">Phillip Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bheemanpally_J/0/1/0/all/0/1\">Jeshwanth Bheemanpally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_R/0/1/0/all/0/1\">Rohan Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratnaparkhi_A/0/1/0/all/0/1\">Adwait Ratnaparkhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_M/0/1/0/all/0/1\">Marilyn Walker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How many preprints have actually been printed and why: a case study of computer science preprints on arXiv. (arXiv:2308.01899v1 [cs.DL])","link":"http://arxiv.org/abs/2308.01899","description":"<p>Preprints play an increasingly critical role in academic communities. There\nare many reasons driving researchers to post their manuscripts to preprint\nservers before formal submission to journals or conferences, but the use of\npreprints has also sparked considerable controversy, especially surrounding the\nclaim of priority. In this paper, a case study of computer science preprints\nsubmitted to arXiv from 2008 to 2017 is conducted to quantify how many\npreprints have eventually been printed in peer-reviewed venues. Among those\npublished manuscripts, some are published under different titles and without an\nupdate to their preprints on arXiv. In the case of these manuscripts, the\ntraditional fuzzy matching method is incapable of mapping the preprint to the\nfinal published version. In view of this issue, we introduce a semantics-based\nmapping method with the employment of Bidirectional Encoder Representations\nfrom Transformers (BERT). With this new mapping method and a plurality of data\nsources, we find that 66% of all sampled preprints are published under\nunchanged titles and 11% are published under different titles and with other\nmodifications. A further analysis was then performed to investigate why these\npreprints but not others were accepted for publication. Our comparison reveals\nthat in the field of computer science, published preprints feature adequate\nrevisions, multiple authorship, detailed abstract and introduction, extensive\nand authoritative references and available source code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jialiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaodong Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning in Large Language Models Through Symbolic Math Word Problems. (arXiv:2308.01906v1 [cs.CL])","link":"http://arxiv.org/abs/2308.01906","description":"<p>Large language models (LLMs) have revolutionized NLP by solving downstream\ntasks with little to no labeled data. Despite their versatile abilities, the\nlarger question of their ability to reason remains ill-understood. This paper\naddresses reasoning in math word problems (MWPs) by studying symbolic versions\nof the numeric problems, since a symbolic expression is a \"concise explanation\"\nof the numeric answer. We create and use a symbolic version of the SVAMP\ndataset and find that GPT-3's davinci-002 model also has good zero-shot\naccuracy on symbolic MWPs. To evaluate the faithfulness of the model's\nreasoning, we go beyond accuracy and additionally evaluate the alignment\nbetween the final answer and the outputted reasoning, which correspond to\nnumeric and symbolic answers respectively for MWPs. We explore a self-prompting\napproach to encourage the symbolic reasoning to align with the numeric answer,\nthus equipping the LLM with the ability to provide a concise and verifiable\nreasoning and making it more interpretable. Surprisingly, self-prompting also\nimproves the symbolic accuracy to be higher than both the numeric and symbolic\naccuracies, thus providing an ensembling effect. The SVAMP_Sym dataset will be\nreleased for future research on symbolic math problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaur_V/0/1/0/all/0/1\">Vedant Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saunshi_N/0/1/0/all/0/1\">Nikunj Saunshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI Knows Which Words Will Appear in Next Year's Korean CSAT. (arXiv:2211.15426v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.15426","description":"<p>A text-mining-based word class categorization method and LSTM-based\nvocabulary pattern prediction method are introduced in this paper. A\npreprocessing method based on simple text appearance frequency analysis is\nfirst described. This method was developed as a data screening tool but showed\n4.35 ~ 6.21 times higher than previous works. An LSTM deep learning method is\nalso suggested for vocabulary appearance pattern prediction method. AI performs\na regression with various size of data window of previous exams to predict the\nprobabilities of word appearance in the next exam. Predicted values of AI over\nvarious data windows are processed into a single score as a weighted sum, which\nwe call an \"AI-Score\", which represents the probability of word appearance in\nnext year's exam. Suggested method showed 100% accuracy at the range 100-score\narea and showed only 1.7% error of prediction in the section where the scores\nwere over 60 points. All source codes are freely available at the authors' Git\nHub repository. (https://github.com/needleworm/bigdata_voca)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ban_B/0/1/0/all/0/1\">Byunghyun Ban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jejong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1\">Hyeonmok Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEVBert: Multimodal Map Pre-training for Language-guided Navigation. (arXiv:2212.04385v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2212.04385","description":"<p>Large-scale pre-training has shown promising results on the\nvision-and-language navigation (VLN) task. However, most existing pre-training\nmethods employ discrete panoramas to learn visual-textual associations. This\nrequires the model to implicitly correlate incomplete, duplicate observations\nwithin the panoramas, which may impair an agent's spatial understanding. Thus,\nwe propose a new map-based pre-training paradigm that is spatial-aware for use\nin VLN. Concretely, we build a local metric map to explicitly aggregate\nincomplete observations and remove duplicates, while modeling navigation\ndependency in a global topological map. This hybrid design can balance the\ndemand of VLN for both short-term reasoning and long-term planning. Then, based\non the hybrid map, we devise a pre-training framework to learn a multimodal map\nrepresentation, which enhances spatial-aware cross-modal reasoning thereby\nfacilitating the language-guided navigation goal. Extensive experiments\ndemonstrate the effectiveness of the map-based pre-training route for VLN, and\nthe proposed method achieves state-of-the-art on four VLN benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1\">Dong An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuankai Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v5 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2304.04370","description":"<p>Human intelligence excels at combining basic skills to solve complex tasks.\nThis capability is vital for Artificial Intelligence (AI) and should be\nembedded in comprehensive intelligent models, enabling them to harness expert\nmodels for complex task-solving towards Artificial General Intelligence (AGI).\nLarge Language Models (LLMs) show promising learning and reasoning abilities,\nand can effectively use external models, tools or APIs to tackle complex\nproblems. In this work, we introduce OpenAGI, an open-source AGI research\nplatform designed for multi-step, real-world tasks. Specifically, OpenAGI uses\na dual strategy, integrating standard benchmark tasks for benchmarking and\nevaluation, and open-ended tasks including more expandable models, tools or\nAPIs for creative problem-solving. Tasks are presented as natural language\nqueries to the LLM, which then selects and executes appropriate models. We also\npropose a Reinforcement Learning from Task Feedback (RLTF) mechanism that uses\ntask results to improve the LLM's ability, which creates a self-improving AI\nfeedback loop. While we acknowledge that AGI is a broad and multifaceted\nresearch challenge with no singularly defined solution path, the integration of\nLLMs with domain-specific expert models, inspired by mirroring the blend of\ngeneral and specialized intelligence in humans, offers a promising approach\ntowards AGI. We are open-sourcing the OpenAGI project's code, dataset,\nbenchmarks, evaluation methods, and demo to foster community involvement in AGI\nadvancement: https://github.com/agiresearch/OpenAGI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wenyue Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_K/0/1/0/all/0/1\">Kai Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jianchao Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Juntao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zelong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are LLMs All You Need for Task-Oriented Dialogue?. (arXiv:2304.06556v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.06556","description":"<p>Instructions-tuned Large Language Models (LLMs) gained recently huge\npopularity thanks to their ability to interact with users through conversation.\nIn this work we aim to evaluate their ability to complete multi-turn tasks and\ninteract with external databases in the context of established task-oriented\ndialogue benchmarks. We show that for explicit belief state tracking, LLMs\nunderperform compared to specialized task-specific models. Nevertheless, they\nshow ability to guide the dialogue to successful ending if given correct slot\nvalues. Furthermore this ability improves with access to true belief state\ndistribution or in-domain examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hudecek_V/0/1/0/all/0/1\">Vojt&#x11b;ch Hude&#x10d;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1\">Ond&#x159;ej Du&#x161;ek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An automatically discovered chain-of-thought prompt generalizes to novel models and datasets. (arXiv:2305.02897v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.02897","description":"<p>Emergent chain-of-thought (CoT) reasoning capabilities promise to improve\nperformance and explainability of large language models (LLMs). However,\nuncertainties remain about how reasoning strategies formulated for previous\nmodel generations generalize to new model generations and different datasets.\nIn this small-scale study, we compare different reasoning strategies induced by\nzero-shot prompting across six recently released LLMs (davinci-002,\ndavinci-003, GPT-3.5-turbo, GPT-4, Flan-T5-xxl and Cohere command-xlarge) on a\nmixture of six question-answering datasets, including datasets from scientific\nand medical domains. Our findings demonstrate that while some variations in\neffectiveness occur, gains from CoT reasoning strategies remain robust across\ndifferent models and datasets. GPT-4 has the most benefit from current\nstate-of-the-art reasoning strategies and exhibits the best performance by\napplying a prompt previously discovered through automated discovery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hebenstreit_K/0/1/0/all/0/1\">Konstantin Hebenstreit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Praas_R/0/1/0/all/0/1\">Robert Praas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiesewetter_L/0/1/0/all/0/1\">Louis P Kiesewetter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1\">Matthias Samwald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Explainable In-the-Wild Video Quality Assessment: A Database and a Language-Prompted Approach. (arXiv:2305.12726v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.12726","description":"<p>The proliferation of in-the-wild videos has greatly expanded the Video\nQuality Assessment (VQA) problem. Unlike early definitions that usually focus\non limited distortion types, VQA on in-the-wild videos is especially\nchallenging as it could be affected by complicated factors, including various\ndistortions and diverse contents. Though subjective studies have collected\noverall quality scores for these videos, how the abstract quality scores relate\nwith specific factors is still obscure, hindering VQA methods from more\nconcrete quality evaluations (e.g. sharpness of a video). To solve this\nproblem, we collect over two million opinions on 4,543 in-the-wild videos on 13\ndimensions of quality-related factors, including in-capture authentic\ndistortions (e.g. motion blur, noise, flicker), errors introduced by\ncompression and transmission, and higher-level experiences on semantic contents\nand aesthetic issues (e.g. composition, camera trajectory), to establish the\nmulti-dimensional Maxwell database. Specifically, we ask the subjects to label\namong a positive, a negative, and a neutral choice for each dimension. These\nexplanation-level opinions allow us to measure the relationships between\nspecific quality factors and abstract subjective quality ratings, and to\nbenchmark different categories of VQA algorithms on each dimension, so as to\nmore comprehensively analyze their strengths and weaknesses. Furthermore, we\npropose the MaxVQA, a language-prompted VQA approach that modifies\nvision-language foundation model CLIP to better capture important quality\nissues as observed in our analyses. The MaxVQA can jointly evaluate various\nspecific quality factors and final quality scores with state-of-the-art\naccuracy on all dimensions, and superb generalization ability on existing\ndatasets. Code and data available at https://github.com/VQAssessment/MaxVQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haoning Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1\">Erli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Liang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaofeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jingwen Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Annan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wenxiu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qiong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weisi Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inductive reasoning in humans and large language models. (arXiv:2306.06548v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.06548","description":"<p>The impressive recent performance of large language models has led many to\nwonder to what extent they can serve as models of general intelligence or are\nsimilar to human cognition. We address this issue by applying GPT-3.5 and GPT-4\nto a classic problem in human inductive reasoning known as property induction.\nOver two experiments, we elicit human judgments on a range of property\ninduction tasks spanning multiple domains. Although GPT-3.5 struggles to\ncapture many aspects of human behaviour, GPT-4 is much more successful: for the\nmost part, its performance qualitatively matches that of humans, and the only\nnotable exception is its failure to capture the phenomenon of premise\nnon-monotonicity. Our work demonstrates that property induction allows for\ninteresting comparisons between human and machine intelligence and provides two\nlarge datasets that can serve as benchmarks for future work in this vein.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Simon J. Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ransom_K/0/1/0/all/0/1\">Keith Ransom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perfors_A/0/1/0/all/0/1\">Andrew Perfors</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemp_C/0/1/0/all/0/1\">Charles Kemp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gzip versus bag-of-words for text classification with KNN. (arXiv:2307.15002v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.15002","description":"<p>The effectiveness of compression distance in KNN-based text classification\n('gzip') has recently garnered lots of attention. In this note we show that\nsimpler means can also be effective, and compression may not be needed. Indeed,\na 'bag-of-words' matching can achieve similar or better results, and is more\nefficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Opitz_J/0/1/0/all/0/1\">Juri Opitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2307.16680","description":"<p>Diffusion models and large language models have emerged as leading-edge\ngenerative models and have sparked a revolutionary impact on various aspects of\nhuman life. However, the practical implementation of these models has also\nexposed inherent risks, highlighting their dual nature and raising concerns\nregarding their trustworthiness. Despite the abundance of literature on this\nsubject, a comprehensive survey specifically delving into the intersection of\nlarge-scale generative models and their trustworthiness remains largely absent.\nTo bridge this gap, This paper investigates both the long-standing and emerging\nthreats associated with these models across four fundamental dimensions:\nprivacy, security, fairness, and responsibility. In this way, we construct an\nextensive map outlining the trustworthiness of these models, while also\nproviding practical recommendations and identifying future directions. These\nefforts are crucial for promoting the trustworthy deployment of these models,\nultimately benefiting society as a whole.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1\">Mingyuan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-08-03T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}}]}]}