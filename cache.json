{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-04-24T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Supporting Qualitative Analysis with Large Language Models: Combining Codebook with GPT-3 for Deductive Coding. (arXiv:2304.10548v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10548","description":"<p>Qualitative analysis of textual contents unpacks rich and valuable\ninformation by assigning labels to the data. However, this process is often\nlabor-intensive, particularly when working with large datasets. While recent\nAI-based tools demonstrate utility, researchers may not have readily available\nAI resources and expertise, let alone be challenged by the limited\ngeneralizability of those task-specific models. In this study, we explored the\nuse of large language models (LLMs) in supporting deductive coding, a major\ncategory of qualitative analysis where researchers use pre-determined codebooks\nto label the data into a fixed set of codes. Instead of training task-specific\nmodels, a pre-trained LLM could be used directly for various tasks without\nfine-tuning through prompt learning. Using a curiosity-driven questions coding\ntask as a case study, we found, by combining GPT-3 with expert-drafted\ncodebooks, our proposed approach achieved fair to substantial agreements with\nexpert-coded results. We lay out challenges and opportunities in using LLMs to\nsupport qualitative coding and beyond.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Ziang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xingdi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Q. Vera Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelghani_R/0/1/0/all/0/1\">Rania Abdelghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-aspect Repetition Suppression and Content Moderation of Large Language Models. (arXiv:2304.10611v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10611","description":"<p>Natural language generation is one of the most impactful fields in NLP, and\nrecent years have witnessed its evolution brought about by large language\nmodels (LLMs). As the key instrument for writing assistance applications, they\nare generally prone to replicating or extending offensive content provided in\nthe input. In low-resource data regime, they can also lead to repetitive\noutputs (Holtzman et al., 2019) [1]. Usually, offensive content and repetitions\nare mitigated with post-hoc methods, including n-gram level blocklists, top-k\nand nucleus sampling. In this paper, we introduce a combination of exact and\nnon-exact repetition suppression using token and sequence level unlikelihood\nloss, repetition penalty during training, inference, and post-processing\nrespectively. We further explore multi-level unlikelihood loss to the extent\nthat it endows the model with abilities to avoid generating offensive words and\nphrases from the beginning. Finally, with comprehensive experiments, we\ndemonstrate that our proposed methods work exceptionally in controlling the\nrepetition and content quality of LLM outputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minghui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolov_A/0/1/0/all/0/1\">Alex Sokolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weixin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si-Qing Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"HOT\" ChatGPT: The promise of ChatGPT in detecting and discriminating hateful, offensive, and toxic comments on social media. (arXiv:2304.10619v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10619","description":"<p>Harmful content is pervasive on social media, poisoning online communities\nand negatively impacting participation. A common approach to address this issue\nis to develop detection models that rely on human annotations. However, the\ntasks required to build such models expose annotators to harmful and offensive\ncontent and may require significant time and cost to complete. Generative AI\nmodels have the potential to understand and detect harmful content. To\ninvestigate this potential, we used ChatGPT and compared its performance with\nMTurker annotations for three frequently discussed concepts related to harmful\ncontent: Hateful, Offensive, and Toxic (HOT). We designed five prompts to\ninteract with ChatGPT and conducted four experiments eliciting HOT\nclassifications. Our results show that ChatGPT can achieve an accuracy of\napproximately 80% when compared to MTurker annotations. Specifically, the model\ndisplays a more consistent classification for non-HOT comments than HOT\ncomments compared to human annotations. Our findings also suggest that ChatGPT\nclassifications align with provided HOT definitions, but ChatGPT classifies\n\"hateful\" and \"offensive\" as subsets of \"toxic.\" Moreover, the choice of\nprompts used to interact with ChatGPT impacts its performance. Based on these\nin-sights, our study provides several meaningful implications for employing\nChatGPT to detect HOT content, particularly regarding the reliability and\nconsistency of its performance, its understand-ing and reasoning of the HOT\nconcept, and the impact of prompts on its performance. Overall, our study\nprovides guidance about the potential of using generative AI models to moderate\nlarge volumes of user-generated content on social media.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lingyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lizhou Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atreja_S/0/1/0/all/0/1\">Shubham Atreja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemphill_L/0/1/0/all/0/1\">Libby Hemphill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IXA/Cogcomp at SemEval-2023 Task 2: Context-enriched Multilingual Named Entity Recognition using Knowledge Bases. (arXiv:2304.10637v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10637","description":"<p>Named Entity Recognition (NER) is a core natural language processing task in\nwhich pre-trained language models have shown remarkable performance. However,\nstandard benchmarks like CoNLL 2003 \\cite{conll03} do not address many of the\nchallenges that deployed NER systems face, such as having to classify emerging\nor complex entities in a fine-grained way. In this paper we present a novel NER\ncascade approach comprising three steps: first, identifying candidate entities\nin the input sentence; second, linking the each candidate to an existing\nknowledge base; third, predicting the fine-grained category for each entity\ncandidate. We empirically demonstrate the significance of external knowledge\nbases in accurately classifying fine-grained and emerging entities. Our system\nexhibits robust performance in the MultiCoNER2 \\cite{multiconer2-data} shared\ntask, even in the low-resource language setting where we leverage knowledge\nbases of high-resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Ferrero_I/0/1/0/all/0/1\">Iker Garc&#xed;a-Ferrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_J/0/1/0/all/0/1\">Jon Ander Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainz_O/0/1/0/all/0/1\">Oscar Sainz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salaberria_A/0/1/0/all/0/1\">Ander Salaberria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Sense Induction with Knowledge Distillation from BERT. (arXiv:2304.10642v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10642","description":"<p>Pre-trained contextual language models are ubiquitously employed for language\nunderstanding tasks, but are unsuitable for resource-constrained systems.\nNoncontextual word embeddings are an efficient alternative in these settings.\nSuch methods typically use one vector to encode multiple different meanings of\na word, and incur errors due to polysemy. This paper proposes a two-stage\nmethod to distill multiple word senses from a pre-trained language model (BERT)\nby using attention over the senses of a word in a context and transferring this\nsense information to fit multi-sense embeddings in a skip-gram-like framework.\nWe demonstrate an effective approach to training the sense disambiguation\nmechanism in our model with a distribution over word senses extracted from the\noutput layer embeddings of BERT. Experiments on the contextual word similarity\nand sense induction tasks show that this method is superior to or competitive\nwith state-of-the-art multi-sense embeddings on multiple benchmark data sets,\nand experiments with an embedding-based topic model (ETM) demonstrates the\nbenefits of using this multi-sense embedding in a downstream application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1\">Anik Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gittens_A/0/1/0/all/0/1\">Alex Gittens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yener_B/0/1/0/all/0/1\">Bulent Yener</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta Semantics: Towards better natural language understanding and reasoning. (arXiv:2304.10663v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10663","description":"<p>Natural language understanding is one of the most challenging topics in\nartificial intelligence. Deep neural network methods, particularly large\nlanguage module (LLM) methods such as ChatGPT and GPT-3, have powerful\nflexibility to adopt informal text but are weak on logical deduction and suffer\nfrom the out-of-vocabulary (OOV) problem. On the other hand, rule-based methods\nsuch as Mathematica, Semantic web, and Lean, are excellent in reasoning but\ncannot handle the complex and changeable informal text. Inspired by pragmatics\nand structuralism, we propose two strategies to solve the OOV problem and a\nsemantic model for better natural language understanding and reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaolin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReCEval: Evaluating Reasoning Chains via Correctness and Informativeness. (arXiv:2304.10703v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10703","description":"<p>Multi-step reasoning ability is fundamental to many natural language tasks,\nyet it is unclear what constitutes a good reasoning chain and how to evaluate\nthem. Most existing methods focus solely on whether the reasoning chain leads\nto the correct conclusion, but this answer-oriented view may confound the\nquality of reasoning with other spurious shortcuts to predict the answer. To\nbridge this gap, we evaluate reasoning chains by viewing them as informal\nproofs that derive the final answer. Specifically, we propose ReCEval\n(Reasoning Chain Evaluation), a framework that evaluates reasoning chains\nthrough two key properties: (1) correctness, i.e., each step makes a valid\ninference based on the information contained within the step, preceding steps,\nand input context, and (2) informativeness, i.e., each step provides new\ninformation that is helpful towards deriving the generated answer. We implement\nReCEval using natural language inference models and information-theoretic\nmeasures. On multiple datasets, ReCEval is highly effective in identifying\ndifferent types of errors, resulting in notable improvements compared to prior\nmethods. We demonstrate that our informativeness metric captures the expected\nflow of information in high-quality reasoning chains and we also analyze the\nimpact of previous steps on evaluating correctness and informativeness.\nFinally, we show that scoring reasoning chains based on ReCEval can improve\ndownstream performance of reasoning tasks. Our code is publicly available at:\nhttps://github.com/archiki/ReCEval\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1\">Archiki Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Swarnadeep Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TC-GAT: Graph Attention Network for Temporal Causality Discovery. (arXiv:2304.10706v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10706","description":"<p>The present study explores the intricacies of causal relationship extraction,\na vital component in the pursuit of causality knowledge. Causality is\nfrequently intertwined with temporal elements, as the progression from cause to\neffect is not instantaneous but rather ensconced in a temporal dimension. Thus,\nthe extraction of temporal causality holds paramount significance in the field.\nIn light of this, we propose a method for extracting causality from the text\nthat integrates both temporal and causal relations, with a particular focus on\nthe time aspect. To this end, we first compile a dataset that encompasses\ntemporal relationships. Subsequently, we present a novel model, TC-GAT, which\nemploys a graph attention mechanism to assign weights to the temporal\nrelationships and leverages a causal knowledge graph to determine the adjacency\nmatrix. Additionally, we implement an equilibrium mechanism to regulate the\ninterplay between temporal and causal relations. Our experiments demonstrate\nthat our proposed method significantly surpasses baseline models in the task of\ncausality extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaosong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Ke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wanli Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yijia Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KitchenScale: Learning to predict ingredient quantities from recipe contexts. (arXiv:2304.10739v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10739","description":"<p>Determining proper quantities for ingredients is an essential part of cooking\npractice from the perspective of enriching tastiness and promoting healthiness.\nWe introduce KitchenScale, a fine-tuned Pre-trained Language Model (PLM) that\npredicts a target ingredient's quantity and measurement unit given its recipe\ncontext. To effectively train our KitchenScale model, we formulate an\ningredient quantity prediction task that consists of three sub-tasks which are\ningredient measurement type classification, unit classification, and quantity\nregression task. Furthermore, we utilized transfer learning of cooking\nknowledge from recipe texts to PLMs. We adopted the Discrete Latent Exponent\n(DExp) method to cope with high variance of numerical scales in recipe corpora.\nExperiments with our newly constructed dataset and recommendation examples\ndemonstrate KitchenScale's understanding of various recipe contexts and\ngeneralizability in predicting ingredient quantities. We implemented a web\napplication for KitchenScale to demonstrate its functionality in recommending\ningredient quantities expressed in numerals (e.g., 2) with units (e.g., ounce).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Donghee Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gim_M/0/1/0/all/0/1\">Mogan Gim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badreddine_S/0/1/0/all/0/1\">Samy Badreddine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hajung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Donghyeon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback. (arXiv:2304.10750v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10750","description":"<p>Many approaches to Natural Language Processing (NLP) tasks often treat them\nas single-step problems, where an agent receives an instruction, executes it,\nand is evaluated based on the final outcome. However, human language is\ninherently interactive, as evidenced by the back-and-forth nature of human\nconversations. In light of this, we posit that human-AI collaboration should\nalso be interactive, with humans monitoring the work of AI agents and providing\nfeedback that the agent can understand and utilize. Further, the AI agent\nshould be able to detect when it needs additional information and proactively\nask for help. Enabling this scenario would lead to more natural, efficient, and\nengaging human-AI collaborations.\n</p>\n<p>In this work, we explore these directions using the challenging task defined\nby the IGLU competition, an interactive grounded language understanding task in\na MineCraft-like world. We explore multiple types of help players can give to\nthe AI to guide it and analyze the impact of this help in AI behavior,\nresulting in performance improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_N/0/1/0/all/0/1\">Nikhil Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teruel_M/0/1/0/all/0/1\">Milagro Teruel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanz_P/0/1/0/all/0/1\">Patricio Figueroa Sanz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1\">Julia Kiseleva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GeoLayoutLM: Geometric Pre-training for Visual Information Extraction. (arXiv:2304.10759v1 [cs.CV])","link":"http://arxiv.org/abs/2304.10759","description":"<p>Visual information extraction (VIE) plays an important role in Document\nIntelligence. Generally, it is divided into two tasks: semantic entity\nrecognition (SER) and relation extraction (RE). Recently, pre-trained models\nfor documents have achieved substantial progress in VIE, particularly in SER.\nHowever, most of the existing models learn the geometric representation in an\nimplicit way, which has been found insufficient for the RE task since geometric\ninformation is especially crucial for RE. Moreover, we reveal another factor\nthat limits the performance of RE lies in the objective gap between the\npre-training phase and the fine-tuning phase for RE. To tackle these issues, we\npropose in this paper a multi-modal framework, named GeoLayoutLM, for VIE.\nGeoLayoutLM explicitly models the geometric relations in pre-training, which we\ncall geometric pre-training. Geometric pre-training is achieved by three\nspecially designed geometry-related pre-training tasks. Additionally, novel\nrelation heads, which are pre-trained by the geometric pre-training tasks and\nfine-tuned for RE, are elaborately designed to enrich and enhance the feature\nrepresentation. According to extensive experiments on standard VIE benchmarks,\nGeoLayoutLM achieves highly competitive scores in the SER task and\nsignificantly outperforms the previous state-of-the-arts for RE (\\eg, the F1\nscore of RE on FUNSD is boosted from 80.35\\% to 89.45\\%). The code and models\nare publicly available at\nhttps://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/GeoLayoutLM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chuwei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Changxu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1\">Cong Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading. (arXiv:2304.10784v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10784","description":"<p>Eye movements during reading offer insights into both the reader's cognitive\nprocesses and the characteristics of the text that is being read. Hence, the\nanalysis of scanpaths in reading have attracted increasing attention across\nfields, ranging from cognitive science over linguistics to computer science. In\nparticular, eye-tracking-while-reading data has been argued to bear the\npotential to make machine-learning-based language models exhibit a more\nhuman-like linguistic behavior. However, one of the main challenges in modeling\nhuman scanpaths in reading is their dual-sequence nature: the words are ordered\nfollowing the grammatical rules of the language, whereas the fixations are\nchronologically ordered. As humans do not strictly read from left-to-right, but\nrather skip or refixate words and regress to previous words, the alignment of\nthe linguistic and the temporal sequence is non-trivial. In this paper, we\ndevelop Eyettention, the first dual-sequence model that simultaneously\nprocesses the sequence of words and the chronological sequence of fixations.\nThe alignment of the two sequences is achieved by a cross-sequence attention\nmechanism. We show that Eyettention outperforms state-of-the-art models in\npredicting scanpaths. We provide an extensive within- and across-data set\nevaluation on different languages. An ablation study and qualitative analysis\nsupport an in-depth understanding of the model's behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shuwen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reich_D/0/1/0/all/0/1\">David R. Reich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasse_P/0/1/0/all/0/1\">Paul Prasse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haller_P/0/1/0/all/0/1\">Patrick Haller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheffer_T/0/1/0/all/0/1\">Tobias Scheffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jager_L/0/1/0/all/0/1\">Lena A. J&#xe4;ger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Which Factors Predict the Chat Experience of a Natural Language Generation Dialogue Service?. (arXiv:2304.10785v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10785","description":"<p>In this paper, we proposed a conceptual model to predict the chat experience\nin a natural language generation dialog system. We evaluated the model with 120\nparticipants with Partial Least Squares Structural Equation Modeling (PLS-SEM)\nand obtained an R-square (R2) with 0.541. The model considers various factors,\nincluding the prompts used for generation; coherence, sentiment, and similarity\nin the conversation; and users' perceived dialog agents' favorability. We then\nfurther explore the effectiveness of the subset of our proposed model. The\nresults showed that users' favorability and coherence, sentiment, and\nsimilarity in the dialogue are positive predictors of users' chat experience.\nMoreover, we found users may prefer dialog agents with characteristics of\nExtroversion, Openness, Conscientiousness, Agreeableness, and Non-Neuroticism.\nThrough our research, an adaptive dialog system might use collected data to\ninfer factors in our model, predict the chat experience for users through these\nfactors, and optimize it by adjusting prompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Eason Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Downstream Task-Oriented Neural Tokenizer Optimization with Vocabulary Restriction as Post Processing. (arXiv:2304.10808v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10808","description":"<p>This paper proposes a method to optimize tokenization for the performance\nimprovement of already trained downstream models. Our method generates\ntokenization results attaining lower loss values of a given downstream model on\nthe training data for restricting vocabularies and trains a tokenizer\nreproducing the tokenization results. Therefore, our method can be applied to\nvariety of tokenization methods, while existing work cannot due to the\nsimultaneous learning of the tokenizer and the downstream model. This paper\nproposes an example of the BiLSTM-based tokenizer with vocabulary restriction,\nwhich can capture wider contextual information for the tokenization process\nthan non-neural-based tokenization methods used in existing work. Experimental\nresults on text classification in Japanese, Chinese, and English text\nclassification tasks show that the proposed method improves performance\ncompared to the existing methods for tokenization optimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hiraoka_T/0/1/0/all/0/1\">Tatsuya Hiraoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwakura_T/0/1/0/all/0/1\">Tomoya Iwakura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tokenization Tractability for Human and Machine Learning Model: An Annotation Study. (arXiv:2304.10813v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10813","description":"<p>Is tractable tokenization for humans also tractable for machine learning\nmodels? This study investigates relations between tractable tokenization for\nhumans (e.g., appropriateness and readability) and one for models of machine\nlearning (e.g., performance on an NLP task). We compared six tokenization\nmethods on the Japanese commonsense question-answering dataset (JCommmonsenseQA\nin JGLUE). We tokenized question texts of the QA dataset with different\ntokenizers and compared the performance of human annotators and\nmachine-learning models. Besides,we analyze relationships among the\nperformance, appropriateness of tokenization, and response time to questions.\nThis paper provides a quantitative investigation result that shows the\ntractable tokenizations for humans and machine learning models are not\nnecessarily the same as each other.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hiraoka_T/0/1/0/all/0/1\">Tatsuya Hiraoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwakura_T/0/1/0/all/0/1\">Tomoya Iwakura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Sign Language Translation with Monolingual Data. (arXiv:2304.10844v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10844","description":"<p>Sign language translation (SLT) systems, which are often decomposed into\nvideo-to-gloss (V2G) recognition and gloss-to-text (G2T) translation through\nthe pivot gloss, heavily relies on the availability of large-scale parallel G2T\npairs. However, the manual annotation of pivot gloss, which is a sequence of\ntranscribed written-language words in the order in which they are signed,\nfurther exacerbates the scarcity of data for SLT. To address this issue, this\npaper proposes a simple and efficient rule transformation method to transcribe\nthe large-scale target monolingual data into its pseudo glosses automatically\nfor enhancing the SLT translation. Empirical results show that the proposed\napproach can significantly improve the performance of SLT, especially achieving\nstate-of-the-art results on two SLT benchmark datasets PHEONIX-WEATHER 2014T\nand ASLG-PC12. Our code has been released at:\nhttps://github.com/pengr/Mono\\_SLT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1\">Ru Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yawen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junbo Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text2Time: Transformer-based article time period predictor. (arXiv:2304.10859v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10859","description":"<p>We explore the problem of predicting the publication period of text document,\nsuch as a news article, using the text from that document. In order to do so,\nwe created our own extensive labeled dataset of over 350,000 news articles\npublished by The New York Times over six decades. We then provide an\nimplementation of a simple Naive Bayes baseline model, which surprisingly\nachieves decent performance in terms of accuracy.Finally, for our approach, we\nuse a pretrained BERT model fine-tuned for the task of text classification.\nThis model exceeds our expectations and provides some very impressive results\nin terms of accurately classifying news articles into their respective\npublication decades. The results beat the performance of the few previously\ntried models for this relatively unexplored task of time prediction from text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gunasekaran_K/0/1/0/all/0/1\">Karthick Prasad Gunasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babrich_B/0/1/0/all/0/1\">B Chase Babrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shirodkar_S/0/1/0/all/0/1\">Saurabh Shirodkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1\">Hee Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CancerGPT: Few-shot Drug Pair Synergy Prediction using Large Pre-trained Language Models. (arXiv:2304.10946v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10946","description":"<p>Large pre-trained language models (LLMs) have been shown to have significant\npotential in few-shot learning across various fields, even with minimal\ntraining data. However, their ability to generalize to unseen tasks in more\ncomplex fields, such as biology, has yet to be fully evaluated. LLMs can offer\na promising alternative approach for biological inference, particularly in\ncases where structured data and sample size are limited, by extracting prior\nknowledge from text corpora. Our proposed few-shot learning approach uses LLMs\nto predict the synergy of drug pairs in rare tissues that lack structured data\nand features. Our experiments, which involved seven rare tissues from different\ncancer types, demonstrated that the LLM-based prediction model achieved\nsignificant accuracy with very few or zero samples. Our proposed model, the\nCancerGPT (with $\\sim$ 124M parameters), was even comparable to the larger\nfine-tuned GPT-3 model (with $\\sim$ 175B parameters). Our research is the first\nto tackle drug pair synergy prediction in rare tissues with limited data. We\nare also the first to utilize an LLM-based prediction model for biological\nreaction prediction tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shetty_S/0/1/0/all/0/1\">Sandesh Shetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_A/0/1/0/all/0/1\">Advaith Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1\">Ajay Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xianqian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Ying Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yejin Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEIA: Linguistic Embeddings for the Identification of Affect. (arXiv:2304.10973v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10973","description":"<p>The wealth of text data generated by social media has enabled new kinds of\nanalysis of emotions with language models. These models are often trained on\nsmall and costly datasets of text annotations produced by readers who guess the\nemotions expressed by others in social media posts. This affects the quality of\nemotion identification methods due to training data size limitations and noise\nin the production of labels used in model development. We present LEIA, a model\nfor emotion identification in text that has been trained on a dataset of more\nthan 6 million posts with self-annotated emotion labels for happiness,\naffection, sadness, anger, and fear. LEIA is based on a word masking method\nthat enhances the learning of emotion words during model pre-training. LEIA\nachieves macro-F1 values of approximately 73 on three in-domain test datasets,\noutperforming other supervised and unsupervised methods in a strong benchmark\nthat shows that LEIA generalizes across posts, users, and time periods. We\nfurther perform an out-of-domain evaluation on five different datasets of\nsocial media and other sources, showing LEIA's robust performance across media,\ndata collection methods, and annotation schemes. Our results show that LEIA\ngeneralizes its classification of anger, happiness, and sadness beyond the\ndomain it was trained on. LEIA can be applied in future research to provide\nbetter identification of emotions in text from the perspective of the writer.\nThe models produced for this article are publicly available at\nhttps://huggingface.co/LEIA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aroyehun_S/0/1/0/all/0/1\">Segun Taofeek Aroyehun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_L/0/1/0/all/0/1\">Lukas Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_H/0/1/0/all/0/1\">Hannah Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haimerl_N/0/1/0/all/0/1\">Nikolas Haimerl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natale_A/0/1/0/all/0/1\">Anna Di Natale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_D/0/1/0/all/0/1\">David Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition. (arXiv:2304.10977v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10977","description":"<p>In recent years, Large Language Models such as GPT-3 showed remarkable\ncapabilities in performing NLP tasks in the zero and few shot settings. On the\nother hand, the experiments highlighted the difficulty of GPT-3 in carrying out\ntasks that require a certain degree of reasoning, such as arithmetic\noperations. In this paper we evaluate the ability of Transformer Language\nModels to perform arithmetic operations following a pipeline that, before\nperforming computations, decomposes numbers in units, tens, and so on. We\ndenote the models fine-tuned with this pipeline with the name Calculon and we\ntest them in the task of performing additions, subtractions and multiplications\non the same test sets of GPT-3. Results show an increase of accuracy of 63% in\nthe five-digit addition task. Moreover, we demonstrate the importance of the\ndecomposition pipeline introduced, since fine-tuning the same Language Model\nwithout decomposing numbers results in 0% accuracy in the five-digit addition\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muffo_M/0/1/0/all/0/1\">Matteo Muffo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cocco_A/0/1/0/all/0/1\">Aldo Cocco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertino_E/0/1/0/all/0/1\">Enrico Bertino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information Extraction from Documents: Question Answering vs Token Classification in real-world setups. (arXiv:2304.10994v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10994","description":"<p>Research in Document Intelligence and especially in Document Key Information\nExtraction (DocKIE) has been mainly solved as Token Classification problem.\nRecent breakthroughs in both natural language processing (NLP) and computer\nvision helped building document-focused pre-training methods, leveraging a\nmultimodal understanding of the document text, layout and image modalities.\nHowever, these breakthroughs also led to the emergence of a new DocKIE subtask\nof extractive document Question Answering (DocQA), as part of the Machine\nReading Comprehension (MRC) research field. In this work, we compare the\nQuestion Answering approach with the classical token classification approach\nfor document key information extraction. We designed experiments to benchmark\nfive different experimental setups : raw performances, robustness to noisy\nenvironment, capacity to extract long entities, fine-tuning speed on Few-Shot\nLearning and finally Zero-Shot Learning. Our research showed that when dealing\nwith clean and relatively short entities, it is still best to use token\nclassification-based approach, while the QA approach could be a good\nalternative for noisy environment or long entities use-cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lam_L/0/1/0/all/0/1\">Laurent Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratnamogan_P/0/1/0/all/0/1\">Pirashanth Ratnamogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jo&#xeb;l Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanhuffel_W/0/1/0/all/0/1\">William Vanhuffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caspani_F/0/1/0/all/0/1\">Fabien Caspani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT Based Clinical Knowledge Extraction for Biomedical Knowledge Graph Construction and Analysis. (arXiv:2304.10996v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10996","description":"<p>Background : Knowledge is evolving over time, often as a result of new\ndiscoveries or changes in the adopted methods of reasoning. Also, new facts or\nevidence may become available, leading to new understandings of complex\nphenomena. This is particularly true in the biomedical field, where scientists\nand physicians are constantly striving to find new methods of diagnosis,\ntreatment and eventually cure. Knowledge Graphs (KGs) offer a real way of\norganizing and retrieving the massive and growing amount of biomedical\nknowledge.\n</p>\n<p>Objective : We propose an end-to-end approach for knowledge extraction and\nanalysis from biomedical clinical notes using the Bidirectional Encoder\nRepresentations from Transformers (BERT) model and Conditional Random Field\n(CRF) layer.\n</p>\n<p>Methods : The approach is based on knowledge graphs, which can effectively\nprocess abstract biomedical concepts such as relationships and interactions\nbetween medical entities. Besides offering an intuitive way to visualize these\nconcepts, KGs can solve more complex knowledge retrieval problems by\nsimplifying them into simpler representations or by transforming the problems\ninto representations from different perspectives. We created a biomedical\nKnowledge Graph using using Natural Language Processing models for named entity\nrecognition and relation extraction. The generated biomedical knowledge graphs\n(KGs) are then used for question answering.\n</p>\n<p>Results : The proposed framework can successfully extract relevant structured\ninformation with high accuracy (90.7% for Named-entity recognition (NER), 88%\nfor relation extraction (RE)), according to experimental findings based on\nreal-world 505 patient biomedical unstructured clinical notes.\n</p>\n<p>Conclusions : In this paper, we propose a novel end-to-end system for the\nconstruction of a biomedical knowledge graph from clinical textual using a\nvariation of BERT models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harnoune_A/0/1/0/all/0/1\">Ayoub Harnoune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhanoui_M/0/1/0/all/0/1\">Maryem Rhanoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikram_M/0/1/0/all/0/1\">Mounia Mikram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yousfi_S/0/1/0/all/0/1\">Siham Yousfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elkaimbillah_Z/0/1/0/all/0/1\">Zineb Elkaimbillah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asri_B/0/1/0/all/0/1\">Bouchra El Asri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction. (arXiv:2304.11015v1 [cs.CL])","link":"http://arxiv.org/abs/2304.11015","description":"<p>We study the problem of decomposing a complex text-to-sql task into smaller\nsub-tasks and how such a decomposition can significantly improve the\nperformance of Large Language Models (LLMs) in the reasoning process. There is\ncurrently a significant gap between the performance of fine-tuned models and\nprompting approaches using LLMs on challenging text-to-sql datasets such as\nSpider. We show that SQL queries, despite their declarative structure, can be\nbroken down into sub-problems and the solutions of those sub-problems can be\nfed into LLMs to significantly improve their performance. Our experiments with\nthree LLMs show that this approach consistently improves their performance by\nroughly 10%, pushing the accuracy of LLMs towards state-of-the-art, and even\nbeating large fine-tuned models on the holdout Spider dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pourreza_M/0/1/0/all/0/1\">Mohammadreza Pourreza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafiei_D/0/1/0/all/0/1\">Davood Rafiei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotional Expression Detection in Spoken Language Employing Machine Learning Algorithms. (arXiv:2304.11040v1 [cs.SD])","link":"http://arxiv.org/abs/2304.11040","description":"<p>There are a variety of features of the human voice that can be classified as\npitch, timbre, loudness, and vocal tone. It is observed in numerous incidents\nthat human expresses their feelings using different vocal qualities when they\nare speaking. The primary objective of this research is to recognize different\nemotions of human beings such as anger, sadness, fear, neutrality, disgust,\npleasant surprise, and happiness by using several MATLAB functions namely,\nspectral descriptors, periodicity, and harmonicity. To accomplish the work, we\nanalyze the CREMA-D (Crowd-sourced Emotional Multimodal Actors Data) &amp; TESS\n(Toronto Emotional Speech Set) datasets of human speech. The audio file\ncontains data that have various characteristics (e.g., noisy, speedy, slow)\nthereby the efficiency of the ML (Machine Learning) models increases\nsignificantly. The EMD (Empirical Mode Decomposition) is utilized for the\nprocess of signal decomposition. Then, the features are extracted through the\nuse of several techniques such as the MFCC, GTCC, spectral centroid, roll-off\npoint, entropy, spread, flux, harmonic ratio, energy, skewness, flatness, and\naudio delta. The data is trained using some renowned ML models namely, Support\nVector Machine, Neural Network, Ensemble, and KNN. The algorithms show an\naccuracy of 67.7%, 63.3%, 61.6%, and 59.0% respectively for the test data and\n77.7%, 76.1%, 99.1%, and 61.2% for the training data. We have conducted\nexperiments using Matlab and the result shows that our model is very prominent\nand flexible than existing similar works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hosain_M/0/1/0/all/0/1\">Mehrab Hosain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arafat_M/0/1/0/all/0/1\">Most. Yeasmin Arafat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_G/0/1/0/all/0/1\">Gazi Zahirul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uddin_J/0/1/0/all/0/1\">Jia Uddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Md. Mobarak Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Fatema Alam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Affective social anthropomorphic intelligent system. (arXiv:2304.11046v1 [cs.SD])","link":"http://arxiv.org/abs/2304.11046","description":"<p>Human conversational styles are measured by the sense of humor, personality,\nand tone of voice. These characteristics have become essential for\nconversational intelligent virtual assistants. However, most of the\nstate-of-the-art intelligent virtual assistants (IVAs) are failed to interpret\nthe affective semantics of human voices. This research proposes an\nanthropomorphic intelligent system that can hold a proper human-like\nconversation with emotion and personality. A voice style transfer method is\nalso proposed to map the attributes of a specific emotion. Initially, the\nfrequency domain data (Mel-Spectrogram) is created by converting the temporal\naudio wave data, which comprises discrete patterns for audio features such as\nnotes, pitch, rhythm, and melody. A collateral CNN-Transformer-Encoder is used\nto predict seven different affective states from voice. The voice is also fed\nparallelly to the deep-speech, an RNN model that generates the text\ntranscription from the spectrogram. Then the transcripted text is transferred\nto the multi-domain conversation agent using blended skill talk,\ntransformer-based retrieve-and-generate generation strategy, and beam-search\ndecoding, and an appropriate textual response is generated. The system learns\nan invertible mapping of data to a latent space that can be manipulated and\ngenerates a Mel-spectrogram frame based on previous Mel-spectrogram frames to\nvoice synthesize and style transfer. Finally, the waveform is generated using\nWaveGlow from the spectrogram. The outcomes of the studies we conducted on\nindividual models were auspicious. Furthermore, users who interacted with the\nsystem provided positive feedback, demonstrating the system's effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mamun_M/0/1/0/all/0/1\">Md. Adyelullahil Mamun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdullah_H/0/1/0/all/0/1\">Hasnat Md. Abdullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Md. Golam Rabiul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_M/0/1/0/all/0/1\">Muhammad Mehedi Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uddin_M/0/1/0/all/0/1\">Md. Zia Uddin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparison of Semi-Supervised Learning Techniques for Streaming ASR at Scale. (arXiv:2304.11053v1 [cs.CL])","link":"http://arxiv.org/abs/2304.11053","description":"<p>Unpaired text and audio injection have emerged as dominant methods for\nimproving ASR performance in the absence of a large labeled corpus. However,\nlittle guidance exists on deploying these methods to improve production ASR\nsystems that are trained on very large supervised corpora and with realistic\nrequirements like a constrained model size and CPU budget, streaming\ncapability, and a rich lattice for rescoring and for downstream NLU tasks. In\nthis work, we compare three state-of-the-art semi-supervised methods\nencompassing both unpaired text and audio as well as several of their\ncombinations in a controlled setting using joint training. We find that in our\nsetting these methods offer many improvements beyond raw WER, including\nsubstantial gains in tail-word WER, decoder computation during inference, and\nlattice density.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peyser_C/0/1/0/all/0/1\">Cal Peyser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picheny_M/0/1/0/all/0/1\">Michael Picheny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Ronny Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara Sainath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Novel Intent Detection and Active Learning Based Classification (Student Abstract). (arXiv:2304.11058v1 [cs.CL])","link":"http://arxiv.org/abs/2304.11058","description":"<p>Novel intent class detection is an important problem in real world scenario\nfor conversational agents for continuous interaction. Several research works\nhave been done to detect novel intents in a mono-lingual (primarily English)\ntexts and images. But, current systems lack an end-to-end universal framework\nto detect novel intents across various different languages with less human\nannotation effort for mis-classified and system rejected samples. This paper\nproposes NIDAL (Novel Intent Detection and Active Learning based\nclassification), a semi-supervised framework to detect novel intents while\nreducing human annotation cost. Empirical results on various benchmark datasets\ndemonstrate that this system outperforms the baseline methods by more than 10%\nmargin for accuracy and macro-F1. The system achieves this while maintaining\noverall annotation cost to be just ~6-10% of the unlabeled data available to\nthe system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mullick_A/0/1/0/all/0/1\">Ankan Mullick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SkillGPT: a RESTful API service for skill extraction and standardization using a Large Language Model. (arXiv:2304.11060v1 [cs.CL])","link":"http://arxiv.org/abs/2304.11060","description":"<p>We present SkillGPT, a tool for skill extraction and standardization (SES)\nfrom free-style job descriptions and user profiles with an open-source Large\nLanguage Model (LLM) as backbone. Most previous methods for similar tasks\neither need supervision or rely on heavy data-preprocessing and feature\nengineering. Directly prompting the latest conversational LLM for standard\nskills, however, is slow, costly and inaccurate. In contrast, SkillGPT utilizes\na LLM to perform its tasks in steps via summarization and vector similarity\nsearch, to balance speed with precision. The backbone LLM of SkillGPT is based\non Llama, free for academic use and thus useful for exploratory research and\nprototype development. Hence, our cost-free SkillGPT gives users the\nconvenience of conversational SES, efficiently and reliably.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1\">Bo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bie_T/0/1/0/all/0/1\">Tijl De Bie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CEIL: A General Classification-Enhanced Iterative Learning Framework for Text Clustering. (arXiv:2304.11061v1 [cs.CL])","link":"http://arxiv.org/abs/2304.11061","description":"<p>Text clustering, as one of the most fundamental challenges in unsupervised\nlearning, aims at grouping semantically similar text segments without relying\non human annotations. With the rapid development of deep learning, deep\nclustering has achieved significant advantages over traditional clustering\nmethods. Despite the effectiveness, most existing deep text clustering methods\nrely heavily on representations pre-trained in general domains, which may not\nbe the most suitable solution for clustering in specific target domains. To\naddress this issue, we propose CEIL, a novel Classification-Enhanced Iterative\nLearning framework for short text clustering, which aims at generally promoting\nthe clustering performance by introducing a classification objective to\niteratively improve feature representations. In each iteration, we first adopt\na language model to retrieve the initial text representations, from which the\nclustering results are collected using our proposed Category Disentangled\nContrastive Clustering (CDCC) algorithm. After strict data filtering and\naggregation processes, samples with clean category labels are retrieved, which\nserve as supervision information to update the language model with the\nclassification objective via a prompt learning approach. Finally, the updated\nlanguage model with improved representation ability is used to enhance\nclustering in the next iteration. Extensive experiments demonstrate that the\nCEIL framework significantly improves the clustering performance over\niterations, and is generally effective on various clustering algorithms.\nMoreover, by incorporating CEIL on CDCC, we achieve the state-of-the-art\nclustering performance on a wide range of short text clustering benchmarks\noutperforming other strong baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mingjun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengzhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yinglong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Di Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haijiang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Transformer to 1M tokens and beyond with RMT. (arXiv:2304.11062v1 [cs.CL])","link":"http://arxiv.org/abs/2304.11062","description":"<p>This technical report presents the application of a recurrent memory to\nextend the context length of BERT, one of the most effective Transformer-based\nmodels in natural language processing. By leveraging the Recurrent Memory\nTransformer architecture, we have successfully increased the model's effective\ncontext length to an unprecedented two million tokens, while maintaining high\nmemory retrieval accuracy. Our method allows for the storage and processing of\nboth local and global information and enables information flow between segments\nof the input sequence through the use of recurrence. Our experiments\ndemonstrate the effectiveness of our approach, which holds significant\npotential to enhance long-term dependency handling in natural language\nunderstanding and generation tasks as well as enable large-scale context\nprocessing for memory-intensive applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bulatov_A/0/1/0/all/0/1\">Aydar Bulatov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuratov_Y/0/1/0/all/0/1\">Yuri Kuratov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burtsev_M/0/1/0/all/0/1\">Mikhail S. Burtsev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Think Before You Act: Unified Policy for Interleaving Language Reasoning with Actions. (arXiv:2304.11063v1 [cs.CL])","link":"http://arxiv.org/abs/2304.11063","description":"<p>The success of transformer models trained with a language modeling objective\nbrings a promising opportunity to the reinforcement learning framework.\nDecision Transformer is a step towards this direction, showing how to train\ntransformers with a similar next-step prediction objective on offline data.\nAnother important development in this area is the recent emergence of\nlarge-scale datasets collected from the internet, such as the ones composed of\ntutorial videos with captions where people talk about what they are doing. To\ntake advantage of this language component, we propose a novel method for\nunifying language reasoning with actions in a single policy. Specifically, we\naugment a transformer policy with word outputs, so it can generate textual\ncaptions interleaved with actions. When tested on the most challenging task in\nBabyAI, with captions describing next subgoals, our reasoning policy\nconsistently outperforms the caption-free baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mezghani_L/0/1/0/all/0/1\">Lina Mezghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1\">Piotr Bojanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahari_K/0/1/0/all/0/1\">Karteek Alahari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1\">Sainbayar Sukhbaatar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversational Process Modelling: State of the Art, Applications, and Implications in Practice. (arXiv:2304.11065v1 [cs.CL])","link":"http://arxiv.org/abs/2304.11065","description":"<p>Chatbots such as ChatGPT have caused a tremendous hype lately. For BPM\napplications, it is often not clear how to apply chatbots to generate business\nvalue. Hence, this work aims at the systematic analysis of existing chatbots\nfor their support of conversational process modelling as process-oriented\ncapability. Application scenarios are identified along the process life cycle.\nThen a systematic literature review on conversational process modelling is\nperformed. The resulting taxonomy serves as input for the identification of\napplication scenarios for conversational process modelling, including\nparaphrasing and improvement of process descriptions. The application scenarios\nare evaluated for existing chatbots based on a real-world test set from the\nhigher education domain. It contains process descriptions as well as\ncorresponding process models, together with an assessment of the model quality.\nBased on the literature and application scenario analyses, recommendations for\nthe usage (practical implications) and further development (research\ndirections) of conversational process modelling are derived.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klievtsova_N/0/1/0/all/0/1\">Nataliia Klievtsova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benzin_J/0/1/0/all/0/1\">Janik-Vasily Benzin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kampik_T/0/1/0/all/0/1\">Timotheus Kampik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangler_J/0/1/0/all/0/1\">Juergen Mangler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rinderle_Ma_S/0/1/0/all/0/1\">Stefanie Rinderle-Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OLISIA: a Cascade System for Spoken Dialogue State Tracking. (arXiv:2304.11073v1 [eess.AS])","link":"http://arxiv.org/abs/2304.11073","description":"<p>Though Dialogue State Tracking (DST) is a core component of spoken dialogue\nsystems, recent work on this task mostly deals with chat corpora, disregarding\nthe discrepancies between spoken and written language.In this paper, we propose\nOLISIA, a cascade system which integrates an Automatic Speech Recognition (ASR)\nmodel and a DST model. We introduce several adaptations in the ASR and DST\nmodules to improve integration and robustness to spoken conversations.With\nthese adaptations, our system ranked first in DSTC11 Track 3, a benchmark to\nevaluate spoken DST. We conduct an in-depth analysis of the results and find\nthat normalizing the ASR outputs and adapting the DST inputs through data\naugmentation, along with increasing the pre-trained models size all play an\nimportant role in reducing the performance discrepancy between written and\nspoken conversations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jacqmin_L/0/1/0/all/0/1\">L&#xe9;o Jacqmin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Druart_L/0/1/0/all/0/1\">Lucas Druart</a> (LIA), <a href=\"http://arxiv.org/find/eess/1/au:+Vielzeuf_V/0/1/0/all/0/1\">Valentin Vielzeuf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rojas_Barahona_L/0/1/0/all/0/1\">Lina Maria Rojas-Barahona</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Esteve_Y/0/1/0/all/0/1\">Yannick Est&#xe8;ve</a> (LIA), <a href=\"http://arxiv.org/find/eess/1/au:+Favre_B/0/1/0/all/0/1\">Beno&#xee;t Favre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spaiche: Extending State-of-the-Art ASR Models to Swiss German Dialects. (arXiv:2304.11075v1 [cs.CL])","link":"http://arxiv.org/abs/2304.11075","description":"<p>Recent breakthroughs in NLP largely increased the presence of ASR systems in\nour daily lives. However, for many low-resource languages, ASR models still\nneed to be improved due in part to the difficulty of acquiring pertinent data.\nThis project aims to help advance research in ASR models for Swiss German\ndialects, by providing insights about the performance of state-of-the-art ASR\nmodels on recently published Swiss German speech datasets. We propose a novel\nloss that takes into account the semantic distance between the predicted and\nthe ground-truth labels. We outperform current state-of-the-art results by\nfine-tuning OpenAI's Whisper model on Swiss-German datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sicard_C/0/1/0/all/0/1\">Cl&#xe9;ment Sicard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyszkowski_K/0/1/0/all/0/1\">Kajetan Pyszkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillioz_V/0/1/0/all/0/1\">Victor Gillioz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can ChatGPT-like Generative Models Guarantee Factual Accuracy? On the Mistakes of New Generation Search Engines. (arXiv:2304.11076v1 [cs.CL])","link":"http://arxiv.org/abs/2304.11076","description":"<p>Although large conversational AI models such as OpenAI's ChatGPT have\ndemonstrated great potential, we question whether such models can guarantee\nfactual accuracy. Recently, technology companies such as Microsoft and Google\nhave announced new services which aim to combine search engines with\nconversational AI. However, we have found numerous mistakes in the public\ndemonstrations that suggest we should not easily trust the factual claims of\nthe AI models. Rather than criticizing specific models or companies, we hope to\ncall on researchers and developers to improve AI models' transparency and\nfactual correctness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruochen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chia_Y/0/1/0/all/0/1\">Yew Ken Chia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bosheng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HeRo: RoBERTa and Longformer Hebrew Language Models. (arXiv:2304.11077v1 [cs.CL])","link":"http://arxiv.org/abs/2304.11077","description":"<p>In this paper, we fill in an existing gap in resources available to the\nHebrew NLP community by providing it with the largest so far pre-train dataset\nHeDC4, a state-of-the-art pre-trained language model HeRo for standard length\ninputs and an efficient transformer LongHeRo for long input sequences. The HeRo\nmodel was evaluated on the sentiment analysis, the named entity recognition,\nand the question answering tasks while the LongHeRo model was evaluated on the\ndocument classification task with a dataset composed of long documents. Both\nHeRo and LongHeRo presented state-of-the-art performance. The dataset and model\ncheckpoints used in this work are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shalumov_V/0/1/0/all/0/1\">Vitaly Shalumov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haskey_H/0/1/0/all/0/1\">Harel Haskey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Academic Writing with GPT-3.5: Reflections on Practices, Efficacy and Transparency. (arXiv:2304.11079v1 [cs.CL])","link":"http://arxiv.org/abs/2304.11079","description":"<p>The debate around the use of GPT 3.5 has been a popular topic among academics\nsince the release of ChatGPT. Whilst some have argued for the advantages of GPT\n3.5 in enhancing academic writing, others have raised concerns such as\nplagiarism, the spread of false information, and ecological issues. The need\nfor finding ways to use GPT 3.5 models transparently has been voiced, and\nsuggestions have been made on social media as to how to use GPT 3.5 models in a\nsmart way. Nevertheless, to date, there is a lack of literature which clearly\noutlines how to use GPT 3.5 models in academic writing, how effective they are,\nand how to use them transparently. To address this, I conducted a personal\nexperience experiment with GPT 3.5, specifically by using OpenAI text davinci\n003 model, for writing this article. I identified five ways of using GPT 3.5:\nChunk Stylist, Bullet to Paragraph, Talk Textualizer, Research Buddy, and\nPolisher. I reflected on their efficacy, and commented on their potential\nimpact on writing ethics. Additionally, I provided a comprehensive document\nwhich shows the prompts I used, results I got from GPT 3.5, the final edits and\nvisually compares those by showing the differences in percentages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buruk_O/0/1/0/all/0/1\">O&#x11f;uz &#x27;Oz&#x27; Buruk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fundamental Limitations of Alignment in Large Language Models. (arXiv:2304.11082v1 [cs.CL])","link":"http://arxiv.org/abs/2304.11082","description":"<p>An important aspect in developing language models that interact with humans\nis aligning their behavior to be useful and unharmful for their human users.\nThis is usually achieved by tuning the model in a way that enhances desired\nbehaviors and inhibits undesired ones, a process referred to as alignment. In\nthis paper, we propose a theoretical approach called Behavior Expectation\nBounds (BEB) which allows us to formally investigate several inherent\ncharacteristics and limitations of alignment in large language models.\nImportantly, we prove that for any behavior that has a finite probability of\nbeing exhibited by the model, there exist prompts that can trigger the model\ninto outputting this behavior, with probability that increases with the length\nof the prompt. This implies that any alignment process that attenuates\nundesired behavior but does not remove it altogether, is not safe against\nadversarial prompting attacks. Furthermore, our framework hints at the\nmechanism by which leading alignment approaches such as reinforcement learning\nfrom human feedback increase the LLM's proneness to being prompted into the\nundesired behaviors. Moreover, we include the notion of personas in our BEB\nframework, and find that behaviors which are generally very unlikely to be\nexhibited by the model can be brought to the front by prompting the model to\nbehave as specific persona. This theoretical result is being experimentally\ndemonstrated in large scale by the so called contemporary \"chatGPT jailbreaks\",\nwhere adversarial users trick the LLM into breaking its alignment guardrails by\ntriggering it into acting as a malicious persona. Our results expose\nfundamental limitations in alignment of LLMs and bring to the forefront the\nneed to devise reliable mechanisms for ensuring AI safety.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolf_Y/0/1/0/all/0/1\">Yotam Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wies_N/0/1/0/all/0/1\">Noam Wies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1\">Yoav Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Testing the Reliability of ChatGPT for Text Annotation and Classification: A Cautionary Remark. (arXiv:2304.11085v1 [cs.CL])","link":"http://arxiv.org/abs/2304.11085","description":"<p>Recent studies have demonstrated promising potential of ChatGPT for various\ntext annotation and classification tasks. However, ChatGPT is non-deterministic\nwhich means that, as with human coders, identical input can lead to different\noutputs. Given this, it seems appropriate to test the reliability of ChatGPT.\nTherefore, this study investigates the consistency of ChatGPT's zero-shot\ncapabilities for text annotation and classification, focusing on different\nmodel parameters, prompt variations, and repetitions of identical inputs. Based\non the real-world classification task of differentiating website texts into\nnews and not news, results show that consistency in ChatGPT's classification\noutput can fall short of scientific thresholds for reliability. For example,\neven minor wording alterations in prompts or repeating the identical input can\nlead to varying outputs. Although pooling outputs from multiple repetitions can\nimprove reliability, this study advises caution when using ChatGPT for\nzero-shot text annotation and underscores the need for thorough validation,\nsuch as comparison against human-annotated data. The unsupervised application\nof ChatGPT for text annotation and classification is not recommended.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reiss_M/0/1/0/all/0/1\">Michael V. Reiss</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Profiling the news spreading barriers using news headlines. (arXiv:2304.11088v1 [cs.CL])","link":"http://arxiv.org/abs/2304.11088","description":"<p>News headlines can be a good data source for detecting the news spreading\nbarriers in news media, which may be useful in many real-world applications. In\nthis paper, we utilize semantic knowledge through the inference-based model\nCOMET and sentiments of news headlines for barrier classification. We consider\nfive barriers including cultural, economic, political, linguistic, and\ngeographical, and different types of news headlines including health, sports,\nscience, recreation, games, homes, society, shopping, computers, and business.\nTo that end, we collect and label the news headlines automatically for the\nbarriers using the metadata of news publishers. Then, we utilize the extracted\ncommonsense inferences and sentiments as features to detect the news spreading\nbarriers. We compare our approach to the classical text classification methods,\ndeep learning, and transformer-based methods. The results show that the\nproposed approach using inferences-based semantic knowledge and sentiment\noffers better performance than the usual (the average F1-score of the ten\ncategories improves from 0.41, 0.39, 0.59, and 0.59 to 0.47, 0.55, 0.70, and\n0.76 for the cultural, economic, political, and geographical respectively) for\nclassifying the news-spreading barriers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sittar_A/0/1/0/all/0/1\">Abdul Sittar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mladenic_D/0/1/0/all/0/1\">Dunja Mladenic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grobelnik_M/0/1/0/all/0/1\">Marko Grobelnik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Responsible AI in the Era of ChatGPT: A Reference Architecture for Designing Foundation Model-based AI Systems. (arXiv:2304.11090v1 [cs.CL])","link":"http://arxiv.org/abs/2304.11090","description":"<p>The release of ChatGPT, Bard, and other large language model (LLM)-based\nchatbots has drawn huge attention on foundations models worldwide. There is a\ngrowing trend that foundation models will serve as the fundamental building\nblocks for most of the future AI systems. However, incorporating foundation\nmodels in AI systems raises significant concerns about responsible AI due to\ntheir black box nature and rapidly advancing super-intelligence. Additionally,\nthe foundation model's growing capabilities can eventually absorb the other\ncomponents of AI systems, introducing the moving boundary and interface\nevolution challenges in architecture design. To address these challenges, this\npaper proposes a pattern-oriented responsible-AI-by-design reference\narchitecture for designing foundation model-based AI systems. Specially, the\npaper first presents an architecture evolution of AI systems in the era of\nfoundation models, from \"foundation-model-as-a-connector\" to\n\"foundation-model-as-a-monolithic architecture\". The paper then identifies the\nkey design decision points and proposes a pattern-oriented reference\narchitecture to provide reusable responsible-AI-by-design architectural\nsolutions to address the new architecture evolution and responsible AI\nchallenges. The patterns can be embedded as product features of foundation\nmodel-based AI systems and can enable organisations to capitalise on the\npotential of foundation models while minimising associated risks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qinghua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Liming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1\">Zhenchang Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whittle_J/0/1/0/all/0/1\">Jon Whittle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hi Sheldon! Creating Deep Personalized Characters from TV Shows. (arXiv:2304.11093v1 [cs.CL])","link":"http://arxiv.org/abs/2304.11093","description":"<p>Imagine an interesting multimodal interactive scenario that you can see,\nhear, and chat with an AI-generated digital character, who is capable of\nbehaving like Sheldon from The Big Bang Theory, as a DEEP copy from appearance\nto personality. Towards this fantastic multimodal chatting scenario, we propose\na novel task, named Deep Personalized Character Creation (DPCC): creating\nmultimodal chat personalized characters from multimodal data such as TV shows.\nSpecifically, given a single- or multi-modality input (text, audio, video), the\ngoal of DPCC is to generate a multi-modality (text, audio, video) response,\nwhich should be well-matched the personality of a specific character such as\nSheldon, and of high quality as well. To support this novel task, we further\ncollect a character centric multimodal dialogue dataset, named Deep\nPersonalized Character Dataset (DPCD), from TV shows. DPCD contains\ncharacter-specific multimodal dialogue data of ~10k utterances and ~6 hours of\naudio/video per character, which is around 10 times larger compared to existing\nrelated datasets.On DPCD, we present a baseline method for the DPCC task and\ncreate 5 Deep personalized digital Characters (DeepCharacters) from Big Bang TV\nShows. We conduct both subjective and objective experiments to evaluate the\nmultimodal response from DeepCharacters in terms of characterization and\nquality. The results demonstrates that, on our collected DPCD dataset, the\nproposed baseline can create personalized digital characters for generating\nmultimodal response.Our collected DPCD dataset, the code of data collection and\nour baseline will be published soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xuanyuan_M/0/1/0/all/0/1\">Meidai Xuanyuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuwang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Honglei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuchen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qionghai Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effectiveness of Debiasing Techniques: An Indigenous Qualitative Analysis. (arXiv:2304.11094v1 [cs.CL])","link":"http://arxiv.org/abs/2304.11094","description":"<p>An indigenous perspective on the effectiveness of debiasing techniques for\npre-trained language models (PLMs) is presented in this paper. The current\ntechniques used to measure and debias PLMs are skewed towards the US racial\nbiases and rely on pre-defined bias attributes (e.g. \"black\" vs \"white\"). Some\nrequire large datasets and further pre-training. Such techniques are not\ndesigned to capture the underrepresented indigenous populations in other\ncountries, such as M\\=aori in New Zealand. Local knowledge and understanding\nmust be incorporated to ensure unbiased algorithms, especially when addressing\na resource-restricted society.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yogarajan_V/0/1/0/all/0/1\">Vithya Yogarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobbie_G/0/1/0/all/0/1\">Gillian Dobbie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gouk_H/0/1/0/all/0/1\">Henry Gouk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Cross-modal Information Retrieval Possible without Training?. (arXiv:2304.11095v1 [cs.LG])","link":"http://arxiv.org/abs/2304.11095","description":"<p>Encoded representations from a pretrained deep learning model (e.g., BERT\ntext embeddings, penultimate CNN layer activations of an image) convey a rich\nset of features beneficial for information retrieval. Embeddings for a\nparticular modality of data occupy a high-dimensional space of its own, but it\ncan be semantically aligned to another by a simple mapping without training a\ndeep neural net. In this paper, we take a simple mapping computed from the\nleast squares and singular value decomposition (SVD) for a solution to the\nProcrustes problem to serve a means to cross-modal information retrieval. That\nis, given information in one modality such as text, the mapping helps us locate\na semantically equivalent data item in another modality such as image. Using\noff-the-shelf pretrained deep learning models, we have experimented the\naforementioned simple cross-modal mappings in tasks of text-to-image and\nimage-to-text retrieval. Despite simplicity, our mappings perform reasonably\nwell reaching the highest accuracy of 77% on recall@10, which is comparable to\nthose requiring costly neural net training and fine-tuning. We have improved\nthe simple mappings by contrastive learning on the pretrained models.\nContrastive learning can be thought as properly biasing the pretrained encoders\nto enhance the cross-modal mapping quality. We have further improved the\nperformance by multilayer perceptron with gating (gMLP), a simple neural\narchitecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hyunjin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyunjae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joe_S/0/1/0/all/0/1\">Seongho Joe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwon_Y/0/1/0/all/0/1\">Youngjune L. Gwon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatABL: Abductive Learning via Natural Language Interaction with ChatGPT. (arXiv:2304.11107v1 [cs.CL])","link":"http://arxiv.org/abs/2304.11107","description":"<p>Large language models (LLMs) such as ChatGPT have recently demonstrated\nsignificant potential in mathematical abilities, providing valuable reasoning\nparadigm consistent with human natural language. However, LLMs currently have\ndifficulty in bridging perception, language understanding and reasoning\ncapabilities due to incompatibility of the underlying information flow among\nthem, making it challenging to accomplish tasks autonomously. On the other\nhand, abductive learning (ABL) frameworks for integrating the two abilities of\nperception and reasoning has seen significant success in inverse decipherment\nof incomplete facts, but it is limited by the lack of semantic understanding of\nlogical reasoning rules and the dependence on complicated domain knowledge\nrepresentation. This paper presents a novel method (ChatABL) for integrating\nLLMs into the ABL framework, aiming at unifying the three abilities in a more\nuser-friendly and understandable manner. The proposed method uses the strengths\nof LLMs' understanding and logical reasoning to correct the incomplete logical\nfacts for optimizing the performance of perceptual module, by summarizing and\nreorganizing reasoning rules represented in natural language format. Similarly,\nperceptual module provides necessary reasoning examples for LLMs in natural\nlanguage format. The variable-length handwritten equation deciphering task, an\nabstract expression of the Mayan calendar decoding, is used as a testbed to\ndemonstrate that ChatABL has reasoning ability beyond most existing\nstate-of-the-art methods, which has been well supported by comparative studies.\nTo our best knowledge, the proposed ChatABL is the first attempt to explore a\nnew pattern for further approaching human-level cognitive ability via natural\nlanguage interaction with ChatGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_T/0/1/0/all/0/1\">Tianyang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yaonai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Li Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaozheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Junjie Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tuo Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inducing anxiety in large language models increases exploration and bias. (arXiv:2304.11111v1 [cs.CL])","link":"http://arxiv.org/abs/2304.11111","description":"<p>Large language models are transforming research on machine learning while\ngalvanizing public debates. Understanding not only when these models work well\nand succeed but also why they fail and misbehave is of great societal\nrelevance. We propose to turn the lens of computational psychiatry, a framework\nused to computationally describe and modify aberrant behavior, to the outputs\nproduced by these models. We focus on the Generative Pre-Trained Transformer\n3.5 and subject it to tasks commonly studied in psychiatry. Our results show\nthat GPT-3.5 responds robustly to a common anxiety questionnaire, producing\nhigher anxiety scores than human subjects. Moreover, GPT-3.5's responses can be\npredictably changed by using emotion-inducing prompts. Emotion-induction not\nonly influences GPT-3.5's behavior in a cognitive task measuring exploratory\ndecision-making but also influences its behavior in a previously-established\ntask measuring biases such as racism and ableism. Crucially, GPT-3.5 shows a\nstrong increase in biases when prompted with anxiety-inducing text. Thus, it is\nlikely that how prompts are communicated to large language models has a strong\ninfluence on their behavior in applied settings. These results progress our\nunderstanding of prompt engineering and demonstrate the usefulness of methods\ntaken from computational psychiatry for studying the capable algorithms to\nwhich we increasingly delegate authority and autonomy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coda_Forno_J/0/1/0/all/0/1\">Julian Coda-Forno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witte_K/0/1/0/all/0/1\">Kristin Witte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagadish_A/0/1/0/all/0/1\">Akshay K. Jagadish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binz_M/0/1/0/all/0/1\">Marcel Binz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_E/0/1/0/all/0/1\">Eric Schulz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Mapping of CVE Vulnerability Records to MITRE CWE Weaknesses. (arXiv:2304.11130v1 [cs.CR])","link":"http://arxiv.org/abs/2304.11130","description":"<p>In recent years, a proliferation of cyber-security threats and diversity has\nbeen on the rise culminating in an increase in their reporting and analysis. To\ncounter that, many non-profit organizations have emerged in this domain, such\nas MITRE and OSWAP, which have been actively tracking vulnerabilities, and\npublishing defense recommendations in standardized formats. As producing data\nin such formats manually is very time-consuming, there have been some proposals\nto automate the process. Unfortunately, a major obstacle to adopting supervised\nmachine learning for this problem has been the lack of publicly available\nspecialized datasets. Here, we aim to bridge this gap. In particular, we focus\non mapping CVE records into MITRE CWE Weaknesses, and we release to the\nresearch community a manually annotated dataset of 4,012 records for this task.\nWith a human-in-the-loop framework in mind, we approach the problem as a\nranking task and aim to incorporate reinforced learning to make use of the\nhuman feedback in future work. Our experimental results using fine-tuned deep\nlearning models, namely Sentence-BERT and rankT5, show sizable performance\ngains over BM25, BERT, and RoBERTa, which demonstrates the need for an\narchitecture capable of good semantic understanding for this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haddad_A/0/1/0/all/0/1\">Ashraf Haddad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aaraj_N/0/1/0/all/0/1\">Najwa Aaraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mare_S/0/1/0/all/0/1\">Septimiu Fabian Mare</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent and Predictable Memorization in Large Language Models. (arXiv:2304.11158v1 [cs.CL])","link":"http://arxiv.org/abs/2304.11158","description":"<p>Memorization, or the tendency of large language models (LLMs) to output\nentire sequences from their training data verbatim, is a key concern for safely\ndeploying language models. In particular, it is vital to minimize a model's\nmemorization of sensitive datapoints such as those containing personal\nidentifiable information (PII). The prevalence of such undesirable memorization\ncan pose issues for model trainers, and may even require discarding an\notherwise functional model. We therefore seek to predict which sequences will\nbe memorized before a large model's full train-time by extrapolating the\nmemorization behavior of lower-compute trial runs. We measure memorization of\nthe Pythia model suite, and find that intermediate checkpoints are better\npredictors of a model's memorization behavior than smaller fully-trained\nmodels. We additionally provide further novel discoveries on the distribution\nof memorization scores across models and data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prashanth_U/0/1/0/all/0/1\">USVSN Sai Prashanth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutawika_L/0/1/0/all/0/1\">Lintang Sutawika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoelkopf_H/0/1/0/all/0/1\">Hailey Schoelkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anthony_Q/0/1/0/all/0/1\">Quentin Anthony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purohit_S/0/1/0/all/0/1\">Shivanshu Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raf_E/0/1/0/all/0/1\">Edward Raf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large scale analysis of gender bias and sexism in song lyrics. (arXiv:2208.02052v4 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2208.02052","description":"<p>We employ Natural Language Processing techniques to analyse 377808 English\nsong lyrics from the \"Two Million Song Database\" corpus, focusing on the\nexpression of sexism across five decades (1960-2010) and the measurement of\ngender biases. Using a sexism classifier, we identify sexist lyrics at a larger\nscale than previous studies using small samples of manually annotated popular\nsongs. Furthermore, we reveal gender biases by measuring associations in word\nembeddings learned on song lyrics. We find sexist content to increase across\ntime, especially from male artists and for popular songs appearing in Billboard\ncharts. Songs are also shown to contain different language biases depending on\nthe gender of the performer, with male solo artist songs containing more and\nstronger biases. This is the first large scale analysis of this type, giving\ninsights into language usage in such an influential part of popular culture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Betti_L/0/1/0/all/0/1\">Lorenzo Betti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abrate_C/0/1/0/all/0/1\">Carlo Abrate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaltenbrunner_A/0/1/0/all/0/1\">Andreas Kaltenbrunner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distill the Image to Nowhere: Inversion Knowledge Distillation for Multimodal Machine Translation. (arXiv:2210.04468v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04468","description":"<p>Past works on multimodal machine translation (MMT) elevate bilingual setup by\nincorporating additional aligned vision information. However, an image-must\nrequirement of the multimodal dataset largely hinders MMT's development --\nnamely that it demands an aligned form of [image, source text, target text].\nThis limitation is generally troublesome during the inference phase especially\nwhen the aligned image is not provided as in the normal NMT setup. Thus, in\nthis work, we introduce IKD-MMT, a novel MMT framework to support the\nimage-free inference phase via an inversion knowledge distillation scheme. In\nparticular, a multimodal feature generator is executed with a knowledge\ndistillation module, which directly generates the multimodal feature from\n(only) source texts as the input. While there have been a few prior works\nentertaining the possibility to support image-free inference for machine\ntranslation, their performances have yet to rival the image-must translation.\nIn our experiments, we identify our method as the first image-free approach to\ncomprehensively rival or even surpass (almost) all image-must frameworks, and\nachieved the state-of-the-art result on the often-used Multi30k benchmark. Our\ncode and data are available at: https://github.com/pengr/IKD-mmt/tree/master..\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1\">Ru Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yawen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junbo Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"tieval: An Evaluation Framework for Temporal Information Extraction Systems. (arXiv:2301.04643v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.04643","description":"<p>Temporal information extraction (TIE) has attracted a great deal of interest\nover the last two decades, leading to the development of a significant number\nof datasets. Despite its benefits, having access to a large volume of corpora\nmakes it difficult when it comes to benchmark TIE systems. On the one hand,\ndifferent datasets have different annotation schemes, thus hindering the\ncomparison between competitors across different corpora. On the other hand, the\nfact that each corpus is commonly disseminated in a different format requires a\nconsiderable engineering effort for a researcher/practitioner to develop\nparsers for all of them. This constraint forces researchers to select a limited\namount of datasets to evaluate their systems which consequently limits the\ncomparability of the systems. Yet another obstacle that hinders the\ncomparability of the TIE systems is the evaluation metric employed. While most\nresearch works adopt traditional metrics such as precision, recall, and $F_1$,\na few others prefer temporal awareness -- a metric tailored to be more\ncomprehensive on the evaluation of temporal systems. Although the reason for\nthe absence of temporal awareness in the evaluation of most systems is not\nclear, one of the factors that certainly weights this decision is the necessity\nto implement the temporal closure algorithm in order to compute temporal\nawareness, which is not straightforward to implement neither is currently\neasily available. All in all, these problems have limited the fair comparison\nbetween approaches and consequently, the development of temporal extraction\nsystems. To mitigate these problems, we have developed tieval, a Python library\nthat provides a concise interface for importing different corpora and\nfacilitates system evaluation. In this paper, we present the first public\nrelease of tieval and highlight its most relevant features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sousa_H/0/1/0/all/0/1\">Hugo Sousa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jorge_A/0/1/0/all/0/1\">Al&#xed;pio Jorge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_R/0/1/0/all/0/1\">Ricardo Campos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning. (arXiv:2301.13808v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.13808","description":"<p>Table-based reasoning has shown remarkable progress in combining deep models\nwith discrete reasoning, which requires reasoning over both free-form natural\nlanguage (NL) questions and structured tabular data. However, previous\ntable-based reasoning solutions usually suffer from significant performance\ndegradation on huge evidence (tables). In addition, most existing methods\nstruggle to reason over complex questions since the required information is\nscattered in different places. To alleviate the above challenges, we exploit\nlarge language models (LLMs) as decomposers for effective table-based\nreasoning, which (i) decompose huge evidence (a huge table) into sub-evidence\n(a small table) to mitigate the interference of useless information for table\nreasoning; and (ii) decompose complex questions into simpler sub-questions for\ntext reasoning. Specifically, we first use the LLMs to break down the evidence\n(tables) involved in the current question, retaining the relevant evidence and\nexcluding the remaining irrelevant evidence from the huge table. In addition,\nwe propose a \"parsing-execution-filling\" strategy to alleviate the\nhallucination dilemma of the chain of thought by decoupling logic and numerical\ncomputation in each step. Extensive experiments show that our method can\neffectively leverage decomposed evidence and questions and outperforms the\nstrong baselines on TabFact, WikiTableQuestion, and FetaQA datasets. Notably,\nour model outperforms human performance for the first time on the TabFact\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yunhu Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1\">Binyuan Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Binhua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combat AI With AI: Counteract Machine-Generated Fake Restaurant Reviews on Social Media. (arXiv:2302.07731v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.07731","description":"<p>Recent advances in generative models such as GPT may be used to fabricate\nindistinguishable fake customer reviews at a much lower cost, thus posing\nchallenges for social media platforms to detect these machine-generated fake\nreviews. We propose to leverage the high-quality elite restaurant reviews\nverified by Yelp to generate fake reviews from the OpenAI GPT review creator\nand ultimately fine-tune a GPT output detector to predict fake reviews that\nsignificantly outperform existing solutions. We further apply the model to\npredict non-elite reviews and identify the patterns across several dimensions,\nsuch as review, user and restaurant characteristics, and writing style. We show\nthat social media platforms are continuously challenged by machine-generated\nfake reviews, although they may implement detection systems to filter out\nsuspicious reviews.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gambetti_A/0/1/0/all/0/1\">Alessandro Gambetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1\">Qiwei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Design Translation Prompts for ChatGPT: An Empirical Study. (arXiv:2304.02182v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.02182","description":"<p>The recently released ChatGPT has demonstrated surprising abilities in\nnatural language understanding and natural language generation. Machine\ntranslation relies heavily on the abilities of language understanding and\ngeneration. Thus, in this paper, we explore how to assist machine translation\nwith ChatGPT. We adopt several translation prompts on a wide range of\ntranslations. Our experimental results show that ChatGPT with designed\ntranslation prompts can achieve comparable or better performance over\ncommercial translation systems for high-resource language translations. We\nfurther evaluate the translation quality using multiple references, and ChatGPT\nachieves superior performance compared to commercial systems. We also conduct\nexperiments on domain-specific translations, the final results show that\nChatGPT is able to comprehend the provided domain keyword and adjust\naccordingly to output proper translations. At last, we perform few-shot prompts\nthat show consistent improvement across different base prompts. Our work\nprovides empirical evidence that ChatGPT still has great potential in\ntranslations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1\">Feng Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An investigation of speaker independent phrase break models in End-to-End TTS systems. (arXiv:2304.04157v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2304.04157","description":"<p>This paper presents our work on phrase break prediction in the context of\nend-to-end TTS systems, motivated by the following questions: (i) Is there any\nutility in incorporating an explicit phrasing model in an end-to-end TTS\nsystem?, and (ii) How do you evaluate the effectiveness of a phrasing model in\nan end-to-end TTS system? In particular, the utility and effectiveness of\nphrase break prediction models are evaluated in in the context of childrens\nstory synthesis, using listener comprehension. We show by means of perceptual\nlistening evaluations that there is a clear preference for stories synthesized\nafter predicting the location of phrase breaks using a trained phrasing model,\nover stories directly synthesized without predicting the location of phrase\nbreaks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vadapalli_A/0/1/0/all/0/1\">Anandaswarup Vadapalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chinese Open Instruction Generalist: A Preliminary Release. (arXiv:2304.07987v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.07987","description":"<p>Instruction tuning is widely recognized as a key technique for building\ngeneralist language models, which has attracted the attention of researchers\nand the public with the release of InstructGPT~\\citep{ouyang2022training} and\nChatGPT\\footnote{\\url{https://chat.openai.com/}}. Despite impressive progress\nin English-oriented large-scale language models (LLMs), it is still\nunder-explored whether English-based foundation LLMs can perform similarly on\nmultilingual tasks compared to English tasks with well-designed instruction\ntuning and how we can construct the corpora needed for the tuning. To remedy\nthis gap, we propose the project as an attempt to create a Chinese instruction\ndataset by various methods adapted to the intrinsic characteristics of 4\nsub-tasks. We collect around 200k Chinese instruction tuning samples, which\nhave been manually checked to guarantee high quality. We also summarize the\nexisting English and Chinese instruction corpora and briefly describe some\npotential applications of the newly constructed Chinese instruction corpora.\nThe resulting \\textbf{C}hinese \\textbf{O}pen \\textbf{I}nstruction\n\\textbf{G}eneralist (\\textbf{COIG}) corpora are available in\nHuggingface\\footnote{\\url{https://huggingface.co/datasets/BAAI/COIG}} and\nGithub\\footnote{\\url{https://github.com/BAAI-Zlab/COIG}}, and will be\ncontinuously updated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Ge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yemin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1\">Ruibin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yizhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Siwei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1\">Yu Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaoqun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zekun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Benchmark for Scientific Understanding in Humans and Machines. (arXiv:2304.10327v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2304.10327","description":"<p>Scientific understanding is a fundamental goal of science, allowing us to\nexplain the world. There is currently no good way to measure the scientific\nunderstanding of agents, whether these be humans or Artificial Intelligence\nsystems. Without a clear benchmark, it is challenging to evaluate and compare\ndifferent levels of and approaches to scientific understanding. In this\nRoadmap, we propose a framework to create a benchmark for scientific\nunderstanding, utilizing tools from philosophy of science. We adopt a\nbehavioral notion according to which genuine understanding should be recognized\nas an ability to perform certain tasks. We extend this notion by considering a\nset of questions that can gauge different levels of scientific understanding,\ncovering information retrieval, the capability to arrange information to\nproduce an explanation, and the ability to infer how things would be different\nunder different circumstances. The Scientific Understanding Benchmark (SUB),\nwhich is formed by a set of these tests, allows for the evaluation and\ncomparison of different approaches. Benchmarking plays a crucial role in\nestablishing trust, ensuring quality control, and providing a basis for\nperformance evaluation. By aligning machine and human scientific understanding\nwe can improve their utility, ultimately advancing scientific understanding and\nhelping to discover new insights within machines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barman_K/0/1/0/all/0/1\">Kristian Gonzalez Barman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caron_S/0/1/0/all/0/1\">Sascha Caron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Claassen_T/0/1/0/all/0/1\">Tom Claassen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Regt_H/0/1/0/all/0/1\">Henk de Regt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-04-23T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}