{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-02-22T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Spoken Entity Extraction for Virtual Agents. (arXiv:2302.10186v1 [eess.AS])","link":"http://arxiv.org/abs/2302.10186","description":"<p>This paper reimagines some aspects of speech processing using speech\nencoders, specifically about extracting entities directly from speech, with no\nintermediate textual representation. In human-computer conversations,\nextracting entities such as names, postal addresses and email addresses from\nspeech is a challenging task. In this paper, we study the impact of fine-tuning\npre-trained speech encoders on extracting spoken entities in human-readable\nform directly from speech without the need for text transcription. We\nillustrate that such a direct approach optimizes the encoder to transcribe only\nthe entity relevant portions of speech, ignoring the superfluous portions such\nas carrier phrases and spellings of entities. In the context of dialogs from an\nenterprise virtual agent, we demonstrate that the 1-step approach outperforms\nthe typical 2-step cascade of first generating lexical transcriptions followed\nby text-based entity extraction for identifying spoken entities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Singla_K/0/1/0/all/0/1\">Karan Singla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_Y/0/1/0/all/0/1\">Yeon-Jun Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Price_R/0/1/0/all/0/1\">Ryan Price</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jalalvand_S/0/1/0/all/0/1\">Shahab Jalalvand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bangalore_S/0/1/0/all/0/1\">Srinivas Bangalore</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Aware Reward-based Deep Reinforcement Learning for Intent Analysis of Social Media Information. (arXiv:2302.10195v1 [cs.CL])","link":"http://arxiv.org/abs/2302.10195","description":"<p>Due to various and serious adverse impacts of spreading fake news, it is\noften known that only people with malicious intent would propagate fake news.\nHowever, it is not necessarily true based on social science studies.\nDistinguishing the types of fake news spreaders based on their intent is\ncritical because it will effectively guide how to intervene to mitigate the\nspread of fake news with different approaches. To this end, we propose an\nintent classification framework that can best identify the correct intent of\nfake news. We will leverage deep reinforcement learning (DRL) that can optimize\nthe structural representation of each tweet by removing noisy words from the\ninput sequence when appending an actor to the long short-term memory (LSTM)\nintent classifier. Policy gradient DRL model (e.g., REINFORCE) can lead the\nactor to a higher delayed reward. We also devise a new uncertainty-aware\nimmediate reward using a subjective opinion that can explicitly deal with\nmultidimensional uncertainty for effective decision-making. Via 600K training\nepisodes from a fake news tweets dataset with an annotated intent class, we\nevaluate the performance of uncertainty-aware reward in DRL. Evaluation results\ndemonstrate that our proposed framework efficiently reduces the number of\nselected words to maintain a high 95\\% multi-class accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_X/0/1/0/all/0/1\">Xinwei An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qisheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Josang_A/0/1/0/all/0/1\">Audun J&#xf8;sang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_L/0/1/0/all/0/1\">Lance M. Kaplan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_D/0/1/0/all/0/1\">Dong H. Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jin-Hee Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT. (arXiv:2302.10198v1 [cs.CL])","link":"http://arxiv.org/abs/2302.10198","description":"<p>Recently, ChatGPT has attracted great attention, as it can generate fluent\nand high-quality responses to human inquiries. Several prior studies have shown\nthat ChatGPT attains remarkable generation ability compared with existing\nmodels. However, the quantitative analysis of ChatGPT's understanding ability\nhas been given little attention. In this report, we explore the understanding\nability of ChatGPT by evaluating it on the most popular GLUE benchmark, and\ncomparing it with 4 representative fine-tuned BERT-style models. We find that:\n1) ChatGPT falls short in handling paraphrase and similarity tasks; 2) ChatGPT\noutperforms all BERT models on inference tasks by a large margin; 3) ChatGPT\nachieves comparable performance compared with BERT on sentiment analysis and\nquestion answering tasks. Additionally, several bad cases from inference tasks\nshow the potential limitation of ChatGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1\">Qihuang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Juhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Effectiveness of Pre-trained Language Models in Predicting the Helpfulness of Online Product Reviews. (arXiv:2302.10199v1 [cs.CL])","link":"http://arxiv.org/abs/2302.10199","description":"<p>Businesses and customers can gain valuable information from product reviews.\nThe sheer number of reviews often necessitates ranking them based on their\npotential helpfulness. However, only a few reviews ever receive any helpfulness\nvotes on online marketplaces. Sorting all reviews based on the few existing\nvotes can cause helpful reviews to go unnoticed because of the limited\nattention span of readers. The problem of review helpfulness prediction is even\nmore important for higher review volumes, and newly written reviews or launched\nproducts. In this work we compare the use of RoBERTa and XLM-R language models\nto predict the helpfulness of online product reviews. The contributions of our\nwork in relation to literature include extensively investigating the efficacy\nof state-of-the-art language models -- both monolingual and multilingual --\nagainst a robust baseline, taking ranking metrics into account when assessing\nthese approaches, and assessing multilingual models for the first time. We\nemploy the Amazon review dataset for our experiments. According to our study on\nseveral product categories, multilingual and monolingual pre-trained language\nmodels outperform the baseline that utilizes random forest with handcrafted\nfeatures as much as 23% in RMSE. Pre-trained language models reduce the need\nfor complex text feature engineering. However, our results suggest that\npre-trained multilingual models may not be used for fine-tuning only one\nlanguage. We assess the performance of language models with and without\nadditional features. Our results show that including additional features like\nproduct rating by the reviewer can further help the predictive methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boluki_A/0/1/0/all/0/1\">Ali Boluki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharami_J/0/1/0/all/0/1\">Javad Pourmostafa Roshan Sharami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shterionov_D/0/1/0/all/0/1\">Dimitar Shterionov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Information Extraction via Chatting with ChatGPT. (arXiv:2302.10205v1 [cs.CL])","link":"http://arxiv.org/abs/2302.10205","description":"<p>Zero-shot information extraction (IE) aims to build IE systems from the\nunannotated text. It is challenging due to involving little human intervention.\nChallenging but worthwhile, zero-shot IE reduces the time and effort that data\nlabeling takes. Recent efforts on large language models (LLMs, e.g., GPT-3,\nChatGPT) show promising performance on zero-shot settings, thus inspiring us to\nexplore prompt-based methods. In this work, we ask whether strong IE models can\nbe constructed by directly prompting LLMs. Specifically, we transform the\nzero-shot IE task into a multi-turn question-answering problem with a two-stage\nframework (ChatIE). With the power of ChatGPT, we extensively evaluate our\nframework on three IE tasks: entity-relation triple extract, named entity\nrecognition, and event extraction. Empirical results on six datasets across two\nlanguages show that ChatIE achieves impressive performance and even surpasses\nsome full-shot models on several datasets (e.g., NYT11-HRL). We believe that\nour work could shed light on building IE models with limited resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xingyu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Ning Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaobin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wenjuan Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"See Your Heart: Psychological states Interpretation through Visual Creations. (arXiv:2302.10276v1 [cs.CV])","link":"http://arxiv.org/abs/2302.10276","description":"<p>In psychoanalysis, generating interpretations to one's psychological state\nthrough visual creations is facing significant demands. The two main tasks of\nexisting studies in the field of computer vision, sentiment/emotion\nclassification and affective captioning, can hardly satisfy the requirement of\npsychological interpreting. To meet the demands for psychoanalysis, we\nintroduce a challenging task, \\textbf{V}isual \\textbf{E}motion\n\\textbf{I}nterpretation \\textbf{T}ask (VEIT). VEIT requires AI to generate\nreasonable interpretations of creator's psychological state through visual\ncreations. To support the task, we present a multimodal dataset termed SpyIn\n(\\textbf{S}and\\textbf{p}la\\textbf{y} \\textbf{In}terpretation Dataset), which is\npsychological theory supported and professional annotated. Dataset analysis\nillustrates that SpyIn is not only able to support VEIT, but also more\nchallenging compared with other captioning datasets. Building on SpyIn, we\nconduct experiments of several image captioning method, and propose a\nvisual-semantic combined model which obtains a SOTA result on SpyIn. The\nresults indicate that VEIT is a more challenging task requiring scene graph\ninformation and psychological knowledge. Our work also show a promise for AI to\nanalyze and explain inner world of humanity through visual creations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Likun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaokun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaotang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaiqi Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiT Tuned Models for Efficient Species Detection. (arXiv:2302.10281v1 [cs.CV])","link":"http://arxiv.org/abs/2302.10281","description":"<p>Recent advances in training vision-language models have demonstrated\nunprecedented robustness and transfer learning effectiveness; however, standard\ncomputer vision datasets are image-only, and therefore not well adapted to such\ntraining methods. Our paper introduces a simple methodology for adapting any\nfine-grained image classification dataset for distributed vision-language\npretraining. We implement this methodology on the challenging iNaturalist-2021\ndataset, comprised of approximately 2.7 million images of macro-organisms\nacross 10,000 classes, and achieve a new state-of-the art model in terms of\nzero-shot classification accuracy. Somewhat surprisingly, our model (trained\nusing a new method called locked-image text tuning) uses a pre-trained, frozen\nvision representation, proving that language alignment alone can attain strong\ntransfer learning performance, even on fractious, long-tailed datasets. Our\napproach opens the door for utilizing high quality vision-language pretrained\nmodels in agriculturally relevant applications involving species detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakkab_A/0/1/0/all/0/1\">Andre Nakkab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feuer_B/0/1/0/all/0/1\">Benjamin Feuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1\">Chinmay Hegde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paparazzi: A Deep Dive into the Capabilities of Language and Vision Models for Grounding Viewpoint Descriptions. (arXiv:2302.10282v1 [cs.CV])","link":"http://arxiv.org/abs/2302.10282","description":"<p>Existing language and vision models achieve impressive performance in\nimage-text understanding. Yet, it is an open question to what extent they can\nbe used for language understanding in 3D environments and whether they\nimplicitly acquire 3D object knowledge, e.g. about different views of an\nobject. In this paper, we investigate whether a state-of-the-art language and\nvision model, CLIP, is able to ground perspective descriptions of a 3D object\nand identify canonical views of common objects based on text queries. We\npresent an evaluation framework that uses a circling camera around a 3D object\nto generate images from different viewpoints and evaluate them in terms of\ntheir similarity to natural language descriptions. We find that a pre-trained\nCLIP model performs poorly on most canonical views and that fine-tuning using\nhard negative sampling and random contrasting yields good results even under\nconditions with little available training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Voigt_H/0/1/0/all/0/1\">Henrik Voigt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hombeck_J/0/1/0/all/0/1\">Jan Hombeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuschke_M/0/1/0/all/0/1\">Monique Meuschke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawonn_K/0/1/0/all/0/1\">Kai Lawonn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarriess_S/0/1/0/all/0/1\">Sina Zarrie&#xdf;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Large Language Models Change User Preference Adversarially?. (arXiv:2302.10291v1 [cs.CL])","link":"http://arxiv.org/abs/2302.10291","description":"<p>Pretrained large language models (LLMs) are becoming increasingly powerful\nand ubiquitous in mainstream applications such as being a personal assistant, a\ndialogue model, etc. As these models become proficient in deducing user\npreferences and offering tailored assistance, there is an increasing concern\nabout the ability of these models to influence, modify and in the extreme case\nmanipulate user preference adversarially. The issue of lack of interpretability\nin these models in adversarial settings remains largely unsolved. This work\ntries to study adversarial behavior in user preferences from the lens of\nattention probing, red teaming and white-box analysis. Specifically, it\nprovides a bird's eye view of existing literature, offers red teaming samples\nfor dialogue models like ChatGPT and GODEL and probes the attention mechanism\nin the latter for non-adversarial and adversarial settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Subhash_V/0/1/0/all/0/1\">Varshini Subhash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency. (arXiv:2302.10307v1 [cs.CV])","link":"http://arxiv.org/abs/2302.10307","description":"<p>Recently, great success has been made in learning visual representations from\ntext supervision, facilitating the emergence of text-supervised semantic\nsegmentation. However, existing works focus on pixel grouping and cross-modal\nsemantic alignment, while ignoring the correspondence among multiple augmented\nviews of the same image. To overcome such limitation, we propose\nmulti-\\textbf{View} \\textbf{Co}nsistent learning (ViewCo) for text-supervised\nsemantic segmentation. Specifically, we first propose text-to-views consistency\nmodeling to learn correspondence for multiple views of the same input image.\nAdditionally, we propose cross-view segmentation consistency modeling to\naddress the ambiguity issue of text supervision by contrasting the segment\nfeatures of Siamese visual encoders. The text-to-views consistency benefits the\ndense assignment of the visual features by encouraging different crops to align\nwith the same text, while the cross-view segmentation consistency modeling\nprovides additional self-supervision, overcoming the limitation of ambiguous\ntext supervision for segmentation masks. Trained with large-scale image-text\ndata, our model can directly segment objects of arbitrary categories in a\nzero-shot manner. Extensive experiments show that ViewCo outperforms\nstate-of-the-art methods on average by up to 2.9\\%, 1.6\\%, and 2.4\\% mIoU on\nPASCAL VOC2012, PASCAL Context, and COCO, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengzhen Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangrun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Named Entity Recognition. (arXiv:2302.10314v1 [cs.CL])","link":"http://arxiv.org/abs/2302.10314","description":"<p>Named Entity Recognition (NER) is a challenging and widely studied task that\ninvolves detecting and typing entities in text. So far,NER still approaches\nentity typing as a task of classification into universal classes (e.g. date,\nperson, or location). Recent advances innatural language processing focus on\narchitectures of increasing complexity that may lead to overfitting and\nmemorization, and thus, underuse of context. Our work targets situations where\nthe type of entities depends on the context and cannot be solved solely by\nmemorization. We hence introduce a new task: Dynamic Named Entity Recognition\n(DNER), providing a framework to better evaluate the ability of algorithms to\nextract entities by exploiting the context. The DNER benchmark is based on two\ndatasets, DNER-RotoWire and DNER-IMDb. We evaluate baseline models and present\nexperiments reflecting issues and research axes related to this novel task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luiggi_T/0/1/0/all/0/1\">Tristan Luiggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soulier_L/0/1/0/all/0/1\">Laure Soulier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guigue_V/0/1/0/all/0/1\">Vincent Guigue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jendoubi_S/0/1/0/all/0/1\">Siwar Jendoubi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baelde_A/0/1/0/all/0/1\">Aur&#xe9;lien Baelde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalization algorithm of multimodal pre-training model based on graph-text self-supervised training. (arXiv:2302.10315v1 [cs.CL])","link":"http://arxiv.org/abs/2302.10315","description":"<p>Recently, a large number of studies have shown that the introduction of\nvisual information can effectively improve the effect of neural machine\ntranslation (NMT). Its effectiveness largely depends on the availability of a\nlarge number of bilingual parallel sentence pairs and manual image annotation.\nThe lack of images and the effectiveness of images have been difficult to\nsolve. In this paper, a multimodal pre-training generalization algorithm for\nself-supervised training is proposed, which overcomes the lack of visual\ninformation and inaccuracy, and thus extends the applicability of images on\nNMT. Specifically, we will search for many pictures from the existing sentences\nthrough the search engine, and then through the relationship between visual\ninformation and text, do the self-supervised training task of graphics and text\nto obtain more effective visual information for text. We show that when the\nfiltered information is used as multimodal machine translation for fine-tuning,\nthe effect of translation in the global voice dataset is 0.5 BLEU higher than\nthe baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhangxiaobing/0/1/0/all/0/1\">Zhangxiaobing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tangzhenhao/0/1/0/all/0/1\">Tangzhenhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Longzi/0/1/0/all/0/1\">Longzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuxianghua/0/1/0/all/0/1\">Fuxianghua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation. (arXiv:2302.10322v1 [cs.LG])","link":"http://arxiv.org/abs/2302.10322","description":"<p>Skip connections and normalisation layers form two standard architectural\ncomponents that are ubiquitous for the training of Deep Neural Networks (DNNs),\nbut whose precise roles are poorly understood. Recent approaches such as Deep\nKernel Shaping have made progress towards reducing our reliance on them, using\ninsights from wide NN kernel theory to improve signal propagation in vanilla\nDNNs (which we define as networks without skips or normalisation). However,\nthese approaches are incompatible with the self-attention layers present in\ntransformers, whose kernels are intrinsically more complicated to analyse and\ncontrol. And so the question remains: is it possible to train deep vanilla\ntransformers? We answer this question in the affirmative by designing several\napproaches that use combinations of parameter initialisations, bias matrices\nand location-dependent rescaling to achieve faithful signal propagation in\nvanilla transformers. Our methods address various intricacies specific to\nsignal propagation in transformers, including the interaction with positional\nencoding and causal masking. In experiments on WikiText-103 and C4, our\napproaches enable deep transformers without normalisation to train at speeds\nmatching their standard counterparts, and deep vanilla transformers to reach\nthe same performance as standard ones after about 5 times more iterations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bobby He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martens_J/0/1/0/all/0/1\">James Martens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guodong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botev_A/0/1/0/all/0/1\">Aleksandar Botev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1\">Andrew Brock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_S/0/1/0/all/0/1\">Samuel L Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fantastic Rewards and How to Tame Them: A Case Study on Reward Learning for Task-oriented Dialogue Systems. (arXiv:2302.10342v1 [cs.CL])","link":"http://arxiv.org/abs/2302.10342","description":"<p>When learning task-oriented dialogue (ToD) agents, reinforcement learning\n(RL) techniques can naturally be utilized to train dialogue strategies to\nachieve user-specific goals. Prior works mainly focus on adopting advanced RL\ntechniques to train the ToD agents, while the design of the reward function is\nnot well studied. This paper aims at answering the question of how to\nefficiently learn and leverage a reward function for training end-to-end (E2E)\nToD agents. Specifically, we introduce two generalized objectives for\nreward-function learning, inspired by the classical learning-to-rank\nliterature. Further, we utilize the learned reward function to guide the\ntraining of the E2E ToD agent. With the proposed techniques, we achieve\ncompetitive results on the E2E response-generation task on the Multiwoz 2.0\ndataset. Source code and checkpoints are publicly released at\nhttps://github.com/Shentao-YANG/Fantastic_Reward_ICLR2023.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yihao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shentao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shujian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Limits of Transfer Learning with Unified Model in the Cybersecurity Domain. (arXiv:2302.10346v1 [cs.CL])","link":"http://arxiv.org/abs/2302.10346","description":"<p>With the increase in cybersecurity vulnerabilities of software systems, the\nways to exploit them are also increasing. Besides these, malware threats,\nirregular network interactions, and discussions about exploits in public forums\nare also on the rise. To identify these threats faster, to detect potentially\nrelevant entities from any texts, and to be aware of software vulnerabilities,\nautomated approaches are necessary. Application of natural language processing\n(NLP) techniques in the Cybersecurity domain can help in achieving this.\nHowever, there are challenges such as the diverse nature of texts involved in\nthe cybersecurity domain, the unavailability of large-scale publicly available\ndatasets, and the significant cost of hiring subject matter experts for\nannotations. One of the solutions is building multi-task models that can be\ntrained jointly with limited data. In this work, we introduce a generative\nmulti-task model, Unified Text-to-Text Cybersecurity (UTS), trained on malware\nreports, phishing site URLs, programming code constructs, social media data,\nblogs, news articles, and public forum posts. We show UTS improves the\nperformance of some cybersecurity datasets. We also show that with a few\nexamples, UTS can be adapted to novel unseen tasks and the nature of data\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pal_K/0/1/0/all/0/1\">Kuntal Kumar Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashihara_K/0/1/0/all/0/1\">Kazuaki Kashihara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anantheswaran_U/0/1/0/all/0/1\">Ujjwala Anantheswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznia_K/0/1/0/all/0/1\">Kirby C. Kuznia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagtap_S/0/1/0/all/0/1\">Siddhesh Jagtap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time to Embrace Natural Language Processing (NLP)-based Digital Pathology: Benchmarking NLP- and Convolutional Neural Network-based Deep Learning Pipelines. (arXiv:2302.10406v1 [cs.CL])","link":"http://arxiv.org/abs/2302.10406","description":"<p>NLP-based computer vision models, particularly vision transformers, have been\nshown to outperform CNN models in many imaging tasks. However, most digital\npathology artificial-intelligence models are based on CNN architectures,\nprobably owing to a lack of data regarding NLP models for pathology images. In\nthis study, we developed digital pathology pipelines to benchmark the five most\nrecently proposed NLP models (vision transformer (ViT), Swin Transformer,\nMobileViT, CMT, and Sequencer2D) and four popular CNN models (ResNet18,\nResNet50, MobileNetV2, and EfficientNet) to predict biomarkers in colorectal\ncancer (microsatellite instability, CpG island methylator phenotype, and BRAF\nmutation). Hematoxylin and eosin-stained whole-slide images from Molecular and\nCellular Oncology and The Cancer Genome Atlas were used as training and\nexternal validation datasets, respectively. Cross-study external validations\nrevealed that the NLP-based models significantly outperformed the CNN-based\nmodels in biomarker prediction tasks, improving the overall prediction and\nprecision up to approximately 10% and 26%, respectively. Notably, compared with\nexisting models in the current literature using large training datasets, our\nNLP models achieved state-of-the-art predictions for all three biomarkers using\na relatively small training dataset, suggesting that large training datasets\nare not a prerequisite for NLP models or transformers, and NLP may be more\nsuitable for clinical studies in which small training datasets are commonly\ncollected. The superior performance of Sequencer2D suggests that further\nresearch and innovation on both transformer and bidirectional long short-term\nmemory architectures are warranted in the field of digital pathology. NLP\nmodels can replace classic CNN architectures and become the new workhorse\nbackbone in the field of digital pathology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cen_M/0/1/0/all/0/1\">Min Cen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Bangwei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonnagaddala_J/0/1/0/all/0/1\">Jitendra Jonnagaddala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xu Steven Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mask-guided BERT for Few Shot Text Classification. (arXiv:2302.10447v1 [cs.CL])","link":"http://arxiv.org/abs/2302.10447","description":"<p>Transformer-based language models have achieved significant success in\nvarious domains. However, the data-intensive nature of the transformer\narchitecture requires much labeled data, which is challenging in low-resource\nscenarios (i.e., few-shot learning (FSL)). The main challenge of FSL is the\ndifficulty of training robust models on small amounts of samples, which\nfrequently leads to overfitting. Here we present Mask-BERT, a simple and\nmodular framework to help BERT-based architectures tackle FSL. The proposed\napproach fundamentally differs from existing FSL strategies such as prompt\ntuning and meta-learning. The core idea is to selectively apply masks on text\ninputs and filter out irrelevant information, which guides the model to focus\non discriminative tokens that influence prediction results. In addition, to\nmake the text representations from different categories more separable and the\ntext representations from the same category more compact, we introduce a\ncontrastive learning loss function. Experimental results on public-domain\nbenchmark datasets demonstrate the effectiveness of Mask-BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">Wenxiong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Haixing Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaoke Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuzhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hongmin Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KG-ECO: Knowledge Graph Enhanced Entity Correction for Query Rewriting. (arXiv:2302.10454v1 [cs.CL])","link":"http://arxiv.org/abs/2302.10454","description":"<p>Query Rewriting (QR) plays a critical role in large-scale dialogue systems\nfor reducing frictions. When there is an entity error, it imposes extra\nchallenges for a dialogue system to produce satisfactory responses. In this\nwork, we propose KG-ECO: Knowledge Graph enhanced Entity COrrection for query\nrewriting, an entity correction system with corrupt entity span detection and\nentity retrieval/re-ranking functionalities.To boost the model performance, we\nincorporate Knowledge Graph (KG) to provide entity structural information\n(neighboring entities encoded by graph neural networks) and textual information\n(KG entity descriptions encoded by RoBERTa). Experimental results show that our\napproach yields a clear performance gain over two baselines: utterance level QR\nand entity correction without utilizing KG information. The proposed system is\nparticularly effective for few-shot learning cases where target entities are\nrarely seen in training or there is a KG relation between the target entity and\nother contextual entities in the query.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jinglun Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Ziyan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_E/0/1/0/all/0/1\">Eunah Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chenlei Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tell Model Where to Attend: Improving Interpretability of Aspect-Based Sentiment Classification via Small Explanation Annotations. (arXiv:2302.10479v1 [cs.CL])","link":"http://arxiv.org/abs/2302.10479","description":"<p>Gradient-based explanation methods play an important role in the field of\ninterpreting complex deep neural networks for NLP models. However, the existing\nwork has shown that the gradients of a model are unstable and easily\nmanipulable, which impacts the model's reliability largely. According to our\npreliminary analyses, we also find the interpretability of gradient-based\nmethods is limited for complex tasks, such as aspect-based sentiment\nclassification (ABSC). In this paper, we propose an\n\\textbf{I}nterpretation-\\textbf{E}nhanced \\textbf{G}radient-based framework for\n\\textbf{A}BSC via a small number of explanation annotations, namely\n\\texttt{{IEGA}}. Particularly, we first calculate the word-level saliency map\nbased on gradients to measure the importance of the words in the sentence\ntowards the given aspect. Then, we design a gradient correction module to\nenhance the model's attention on the correct parts (e.g., opinion words). Our\nmodel is model agnostic and task agnostic so that it can be integrated into the\nexisting ABSC methods or other tasks. Comprehensive experimental results on\nfour benchmark datasets show that our \\texttt{IEGA} can improve not only the\ninterpretability of the model but also the performance and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhenxiao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Liang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Co-Driven Recognition of Semantic Consistency via the Fusion of Transformer and HowNet Sememes Knowledge. (arXiv:2302.10570v1 [cs.CL])","link":"http://arxiv.org/abs/2302.10570","description":"<p>Semantic consistency recognition aims to detect and judge whether the\nsemantics of two text sentences are consistent with each other. However, the\nexisting methods usually encounter the challenges of synonyms, polysemy and\ndifficulty to understand long text. To solve the above problems, this paper\nproposes a co-driven semantic consistency recognition method based on the\nfusion of Transformer and HowNet sememes knowledge. Multi-level encoding of\ninternal sentence structures via data-driven is carried out firstly by\nTransformer, sememes knowledge base HowNet is introduced for knowledge-driven\nto model the semantic knowledge association among sentence pairs. Then,\ninteractive attention calculation is carried out utilizing soft-attention and\nfusion the knowledge with sememes matrix. Finally, bidirectional long\nshort-term memory network (BiLSTM) is exploited to encode the conceptual\nsemantic information and infer the semantic consistency. Experiments are\nconducted on two financial text matching datasets (BQ, AFQMC) and a\ncross-lingual adversarial dataset (PAWSX) for paraphrase identification.\nCompared with lightweight models including DSSM, MwAN, DRCN, and pre-training\nmodels such as ERNIE etc., the proposed model can not only improve the accuracy\nof semantic consistency recognition effectively (by 2.19%, 5.57% and 6.51%\ncompared with the DSSM, MWAN and DRCN models on the BQ dataset), but also\nreduce the number of model parameters (to about 16M). In addition, driven by\nthe HowNet sememes knowledge, the proposed method is promising to adapt to\nscenarios with long text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinfang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1\">Kang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jinxuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruixian He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Connecting Humanities and Social Sciences: Applying Language and Speech Technology to Online Panel Surveys. (arXiv:2302.10593v1 [cs.CL])","link":"http://arxiv.org/abs/2302.10593","description":"<p>In this paper, we explore the application of language and speech technology\nto open-ended questions in a Dutch panel survey. In an experimental wave\nrespondents could choose to answer open questions via speech or keyboard.\nAutomatic speech recognition (ASR) was used to process spoken responses. We\nevaluated answers from these input modalities to investigate differences\nbetween spoken and typed answers.We report the errors the ASR system produces\nand investigate the impact of these errors on downstream analyses. Open-ended\nquestions give more freedom to answer for respondents, but entail a non-trivial\namount of work to analyse. We evaluated the feasibility of using\ntransformer-based models (e.g. BERT) to apply sentiment analysis and topic\nmodelling on the answers of open questions. A big advantage of\ntransformer-based models is that they are trained on a large amount of language\nmaterials and do not necessarily need training on the target materials. This is\nespecially advantageous for survey data, which does not contain a lot of text\nmaterials. We tested the quality of automatic sentiment analysis by comparing\nautomatic labeling with three human raters and tested the robustness of topic\nmodelling by comparing the generated models based on automatic and manually\ntranscribed spoken answers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heuvel_H/0/1/0/all/0/1\">Henk van den Heuvel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bentum_M/0/1/0/all/0/1\">Martijn Bentum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wills_S/0/1/0/all/0/1\">Simone Wills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koops_J/0/1/0/all/0/1\">Judith C. Koops</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Playing the Werewolf game with artificial intelligence for language understanding. (arXiv:2302.10646v1 [cs.AI])","link":"http://arxiv.org/abs/2302.10646","description":"<p>The Werewolf game is a social deduction game based on free natural language\ncommunication, in which players try to deceive others in order to survive. An\nimportant feature of this game is that a large portion of the conversations are\nfalse information, and the behavior of artificial intelligence (AI) in such a\nsituation has not been widely investigated. The purpose of this study is to\ndevelop an AI agent that can play Werewolf through natural language\nconversations. First, we collected game logs from 15 human players. Next, we\nfine-tuned a Transformer-based pretrained language model to construct a value\nnetwork that can predict a posterior probability of winning a game at any given\nphase of the game and given a candidate for the next action. We then developed\nan AI agent that can interact with humans and choose the best voting target on\nthe basis of its probability from the value network. Lastly, we evaluated the\nperformance of the agent by having it actually play the game with human\nplayers. We found that our AI agent, Deep Wolf, could play Werewolf as\ncompetitively as average human players in a villager or a betrayer role,\nwhereas Deep Wolf was inferior to human players in a werewolf or a seer role.\nThese results suggest that current language models have the capability to\nsuspect what others are saying, tell a lie, or detect lies in conversations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shibata_H/0/1/0/all/0/1\">Hisaichi Shibata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miki_S/0/1/0/all/0/1\">Soichiro Miki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_Y/0/1/0/all/0/1\">Yuta Nakamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generic Dependency Modeling for Multi-Party Conversation. (arXiv:2302.10680v1 [cs.CL])","link":"http://arxiv.org/abs/2302.10680","description":"<p>To model the dependencies between utterances in multi-party conversations, we\npropose a simple and generic framework based on the dependency parsing results\nof utterances. Particularly, we present an approach to encoding the\ndependencies in the form of relative dependency encoding (ReDE) and illustrate\nhow to implement it in Transformers by modifying the computation of\nself-attention. Experimental results on four multi-party conversation\nbenchmarks show that this framework successfully boosts the general performance\nof two Transformer-based language models and leads to comparable or even\nsuperior performance compared to the state-of-the-art methods. The codes are\navailable at https://github.com/shenwzh3/ReDE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Weizhou Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1\">Xiaojun Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Ke Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallel Sentence-Level Explanation Generation for Real-World Low-Resource Scenarios. (arXiv:2302.10707v1 [cs.CL])","link":"http://arxiv.org/abs/2302.10707","description":"<p>In order to reveal the rationale behind model predictions, many works have\nexploited providing explanations in various forms. Recently, to further\nguarantee readability, more and more works turn to generate sentence-level\nhuman language explanations. However, current works pursuing sentence-level\nexplanations rely heavily on annotated training data, which limits the\ndevelopment of interpretability to only a few tasks. As far as we know, this\npaper is the first to explore this problem smoothly from weak-supervised\nlearning to unsupervised learning. Besides, we also notice the high latency of\nautoregressive sentence-level explanation generation, which leads to\nasynchronous interpretability after prediction. Therefore, we propose a\nnon-autoregressive interpretable model to facilitate parallel explanation\ngeneration and simultaneous prediction. Through extensive experiments on\nNatural Language Inference task and Spouse Prediction task, we find that users\nare able to train classifiers with comparable performance $10-15\\times$ faster\nwith parallel explanation generation using only a few or no annotated training\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaokang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qi Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT: Jack of all trades, master of none. (arXiv:2302.10724v1 [cs.CL])","link":"http://arxiv.org/abs/2302.10724","description":"<p>OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and\nrevolutionized the approach in artificial intelligence to human-model\ninteraction. The first contact with the chatbot reveals its ability to provide\ndetailed and precise answers in various areas. There are several publications\non ChatGPT evaluation, testing its effectiveness on well-known natural language\nprocessing (NLP) tasks. However, the existing studies are mostly non-automated\nand tested on a very limited scale. In this work, we examined ChatGPT's\ncapabilities on 25 diverse analytical NLP tasks, most of them subjective even\nto humans, such as sentiment analysis, emotion recognition, offensiveness and\nstance detection, natural language inference, word sense disambiguation,\nlinguistic acceptability and question answering. We automated ChatGPT's\nquerying process and analyzed more than 38k responses. Our comparison of its\nresults with available State-of-the-Art (SOTA) solutions showed that the\naverage loss in quality of the ChatGPT model was about 25% for zero-shot and\nfew-shot evaluation. We showed that the more difficult the task (lower SOTA\nperformance), the higher the ChatGPT loss. It especially refers to pragmatic\nNLP problems like emotion recognition. We also tested the ability of\npersonalizing ChatGPT responses for selected subjective tasks via Random\nContextual Few-Shot Personalization, and we obtained significantly better\nuser-based predictions. Additional qualitative analysis revealed a ChatGPT\nbias, most likely due to the rules imposed on human trainers by OpenAI. Our\nresults provide the basis for a fundamental discussion of whether the high\nquality of recent predictive NLP models can indicate a tool's usefulness to\nsociety and how the learning and validation procedures for such systems should\nbe established.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocon_J/0/1/0/all/0/1\">Jan Koco&#x144;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cichecki_I/0/1/0/all/0/1\">Igor Cichecki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaszyca_O/0/1/0/all/0/1\">Oliwier Kaszyca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochanek_M/0/1/0/all/0/1\">Mateusz Kochanek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szydlo_D/0/1/0/all/0/1\">Dominika Szyd&#x142;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baran_J/0/1/0/all/0/1\">Joanna Baran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielaniewicz_J/0/1/0/all/0/1\">Julita Bielaniewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruza_M/0/1/0/all/0/1\">Marcin Gruza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janz_A/0/1/0/all/0/1\">Arkadiusz Janz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanclerz_K/0/1/0/all/0/1\">Kamil Kanclerz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocon_A/0/1/0/all/0/1\">Anna Koco&#x144;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koptyra_B/0/1/0/all/0/1\">Bart&#x142;omiej Koptyra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mieleszczenko_Kowszewicz_W/0/1/0/all/0/1\">Wiktoria Mieleszczenko-Kowszewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milkowski_P/0/1/0/all/0/1\">Piotr Mi&#x142;kowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oleksy_M/0/1/0/all/0/1\">Marcin Oleksy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piasecki_M/0/1/0/all/0/1\">Maciej Piasecki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radlinski_L/0/1/0/all/0/1\">&#x141;ukasz Radli&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wojtasik_K/0/1/0/all/0/1\">Konrad Wojtasik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wozniak_S/0/1/0/all/0/1\">Stanis&#x142;aw Wo&#x17a;niak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazienko_P/0/1/0/all/0/1\">Przemys&#x142;aw Kazienko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-World Deployment and Evaluation of Kwame for Science, An AI Teaching Assistant for Science Education in West Africa. (arXiv:2302.10786v1 [cs.CL])","link":"http://arxiv.org/abs/2302.10786","description":"<p>Africa has a high student-to-teacher ratio which limits students' access to\nteachers for learning support such as educational question answering. In this\nwork, we extended Kwame, our previous AI teaching assistant for coding\neducation, adapted it for science education, and deployed it as a web app.\nKwame for Science provides passages from well-curated knowledge sources and\nrelated past national exam questions as answers to questions from students\nbased on the Integrated Science subject of the West African Senior Secondary\nCertificate Examination (WASSCE). Furthermore, students can view past national\nexam questions along with their answers and filter by year, question type\n(objectives, theory, and practicals), and topics that were automatically\ncategorized by a topic detection model which we developed (91% unweighted\naverage recall). We deployed Kwame for Science in the real world over 8 months\nand had 750 users across 32 countries (15 in Africa) and 1.5K questions asked.\nOur evaluation showed an 87.2% top 3 accuracy (n=109 questions) implying that\nKwame for Science has a high chance of giving at least one useful answer among\nthe 3 displayed. We categorized the reasons the model incorrectly answered\nquestions to provide insights for future improvements. We also share challenges\nand lessons with the development, deployment, and human-computer interaction\ncomponent of such a tool to enable other researchers to deploy similar tools.\nWith a first-of-its-kind tool within the African context, Kwame for Science has\nthe potential to enable the delivery of scalable, cost-effective, and quality\nremote education to millions of people across Africa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boateng_G/0/1/0/all/0/1\">George Boateng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+John_S/0/1/0/all/0/1\">Samuel John</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boateng_S/0/1/0/all/0/1\">Samuel Boateng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badu_P/0/1/0/all/0/1\">Philemon Badu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agyeman_Budu_P/0/1/0/all/0/1\">Patrick Agyeman-Budu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumbol_V/0/1/0/all/0/1\">Victor Kumbol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TherapyView: Visualizing Therapy Sessions with Temporal Topic Modeling and AI-Generated Arts. (arXiv:2302.10845v1 [cs.CL])","link":"http://arxiv.org/abs/2302.10845","description":"<p>We present the TherapyView, a demonstration system to help therapists\nvisualize the dynamic contents of past treatment sessions, enabled by the\nstate-of-the-art neural topic modeling techniques to analyze the topical\ntendencies of various psychiatric conditions and deep learning-based image\ngeneration engine to provide a visual summary. The system incorporates temporal\nmodeling to provide a time-series representation of topic similarities at a\nturn-level resolution and AI-generated artworks given the dialogue segments to\nprovide a concise representations of the contents covered in the session,\noffering interpretable insights for therapists to optimize their strategies and\nenhance the effectiveness of psychotherapy. This system provides a proof of\nconcept of AI-augmented therapy tools with e in-depth understanding of the\npatient's mental state and enabling more effective treatment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Baihan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zecevic_S/0/1/0/all/0/1\">Stefan Zecevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouneffouf_D/0/1/0/all/0/1\">Djallel Bouneffouf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cecchi_G/0/1/0/all/0/1\">Guillermo Cecchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management. (arXiv:2302.10850v1 [cs.LG])","link":"http://arxiv.org/abs/2302.10850","description":"<p>Reinforcement learning (RL) has shown great promise for developing dialogue\nmanagement (DM) agents that are non-myopic, conduct rich conversations, and\nmaximize overall user satisfaction. Despite recent developments in RL and\nlanguage models (LMs), using RL to power conversational chatbots remains\nchallenging, in part because RL requires online exploration to learn\neffectively, whereas collecting novel human-bot interactions can be expensive\nand unsafe. This issue is exacerbated by the combinatorial action spaces facing\nthese algorithms, as most LM agents generate responses at the word level. We\ndevelop a variety of RL algorithms, specialized to dialogue planning, that\nleverage recent Mixture-of-Expert Language Models (MoE-LMs) -- models that\ncapture diverse semantics, generate utterances reflecting different intents,\nand are amenable for multi-turn DM. By exploiting MoE-LM structure, our methods\nsignificantly reduce the size of the action space and improve the efficacy of\nRL-based DM.\n</p>\n<p>We evaluate our methods in open-domain dialogue to demonstrate their\neffectiveness w.r.t.\\ the diversity of intent in generated utterances and\noverall DM performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Dhawal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chow_Y/0/1/0/all/0/1\">Yinlam Chow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1\">Mohammad Ghavamzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutilier_C/0/1/0/all/0/1\">Craig Boutilier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyena Hierarchy: Towards Larger Convolutional Language Models. (arXiv:2302.10866v1 [cs.LG])","link":"http://arxiv.org/abs/2302.10866","description":"<p>Recent advances in deep learning have relied heavily on the use of large\nTransformers due to their ability to learn at scale. However, the core building\nblock of Transformers, the attention operator, exhibits quadratic cost in\nsequence length, limiting the amount of context accessible. Existing\nsubquadratic methods based on low-rank and sparse approximations need to be\ncombined with dense attention layers to match Transformers, indicating a gap in\ncapability. In this work, we propose Hyena, a subquadratic drop-in replacement\nfor attention constructed by interleaving implicitly parametrized long\nconvolutions and data-controlled gating. In recall and reasoning tasks on\nsequences of thousands to hundreds of thousands of tokens, Hyena improves\naccuracy by more than 50 points over operators relying on state-spaces and\nother implicit and explicit methods, matching attention-based models. We set a\nnew state-of-the-art for dense-attention-free architectures on language\nmodeling in standard datasets (WikiText103 and The Pile), reaching Transformer\nquality with a 20% reduction in training compute required at sequence length\n2K. Hyena operators are twice as fast as highly optimized attention at sequence\nlength 8K, and 100x faster at sequence length 64K.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poli_M/0/1/0/all/0/1\">Michael Poli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massaroli_S/0/1/0/all/0/1\">Stefano Massaroli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_E/0/1/0/all/0/1\">Eric Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1\">Daniel Y. Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dao_T/0/1/0/all/0/1\">Tri Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baccus_S/0/1/0/all/0/1\">Stephen Baccus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient CTC Regularization via Coarse Labels for End-to-End Speech Translation. (arXiv:2302.10871v1 [cs.CL])","link":"http://arxiv.org/abs/2302.10871","description":"<p>For end-to-end speech translation, regularizing the encoder with the\nConnectionist Temporal Classification (CTC) objective using the source\ntranscript or target translation as labels can greatly improve quality metrics.\nHowever, CTC demands an extra prediction layer over the vocabulary space,\nbringing in nonnegligible model parameters and computational overheads,\nalthough this layer is typically not used for inference. In this paper, we\nre-examine the need for genuine vocabulary labels for CTC for regularization\nand explore strategies to reduce the CTC label space, targeting improved\nefficiency without quality degradation. We propose coarse labeling for CTC\n(CoLaCTC), which merges vocabulary labels via simple heuristic rules, such as\nusing truncation, division or modulo (MOD) operations. Despite its simplicity,\nour experiments on 4 source and 8 target languages show that CoLaCTC with MOD\nparticularly can compress the label space aggressively to 256 and even further,\ngaining training efficiency (1.18x ~ 1.77x speedup depending on the original\nvocabulary size) yet still delivering comparable or better performance than the\nCTC baseline. We also show that CoLaCTC successfully generalizes to CTC\nregularization regardless of using transcript or translation for labeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Biao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddow_B/0/1/0/all/0/1\">Barry Haddow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$k$NN-Adapter: Efficient Domain Adaptation for Black-Box Language Models. (arXiv:2302.10879v1 [cs.CL])","link":"http://arxiv.org/abs/2302.10879","description":"<p>Fine-tuning a language model on a new domain is standard practice for domain\nadaptation. However, it can be infeasible when it comes to modern large-scale\nlanguage models such as GPT-3, which can only be accessed through APIs, making\nit difficult to access the internal parameters of the model. In this paper, we\npropose $k$NN-Adapter, a method to effectively adapt these black-box large\nlanguage models (LLMs) to a new domain. The $k$NN-Adapter builds on top of the\nretrieval-augmented language model, and adaptively learns to interpolate the\noutput of the language model with retrieval results from a datastore consisting\nof the target domain data. Our experiments on four different domains\ndemonstrate that $k$NN-Adapter significantly improves perplexity, and works\nparticularly well in settings with limited access to LLMs. Additionally, we\nshow that $k$NN-Adapter is more effective than fine-tuning when the amount of\ntraining data is limited. We also release a dataset to encourage further study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yangsibo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daogao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zexuan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weijia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yin Tat Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Wav2vec 2.0 fine-tuning for improved speech emotion recognition. (arXiv:2110.06309v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.06309","description":"<p>While Wav2Vec 2.0 has been proposed for speech recognition (ASR), it can also\nbe used for speech emotion recognition (SER); its performance can be\nsignificantly improved using different fine-tuning strategies. Two baseline\nmethods, vanilla fine-tuning (V-FT) and task adaptive pretraining (TAPT) are\nfirst presented. We show that V-FT is able to outperform state-of-the-art\nmodels on the IEMOCAP dataset. TAPT, an existing NLP fine-tuning strategy,\nfurther improves the performance on SER. We also introduce a novel fine-tuning\nmethod termed P-TAPT, which modifies the TAPT objective to learn contextualized\nemotion representations. Experiments show that P-TAPT performs better than\nTAPT, especially under low-resource settings. Compared to prior works in this\nliterature, our top-line system achieved a 7.4\\% absolute improvement in\nunweighted accuracy (UA) over the state-of-the-art performance on IEMOCAP. Our\ncode is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Li-Wei Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rudnicky_A/0/1/0/all/0/1\">Alexander Rudnicky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient and Training-Free Control of Language Generation. (arXiv:2205.06036v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.06036","description":"<p>In recent years, there has been a growing interest in the development of\nlanguage models capable of generating text with controllable attributes. While\nseveral approaches have been proposed, many of these methods require\ncondition-specific data or significant computational resources. In this study,\nwe propose a novel method called Gamma Sampling, which enables controllable\nlanguage generation without the need for any training data and maintains a fast\ngeneration speed. Gamma Sampling incorporates attribute-related information\ninto the sampling process, effectively guiding the language model to produce\ntext with desired attributes. Our experimental results demonstrate that Gamma\nSampling, when applied to GPT2, outperforms representative baselines in terms\nof diversity, attribute relevance, and overall quality of the generated\nsamples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shangda Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Ignore Adversarial Attacks. (arXiv:2205.11551v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11551","description":"<p>Despite the strong performance of current NLP models, they can be brittle\nagainst adversarial attacks. To enable effective learning against adversarial\ninputs, we introduce the use of rationale models that can explicitly learn to\nignore attack tokens. We find that the rationale models can successfully ignore\nover 90% of attack tokens. This approach leads to consistent sizable\nimprovements ($\\sim$10%) over baseline models in robustness on three datasets\nfor both BERT and RoBERTa, and also reliably outperforms data augmentation with\nadversarial examples alone. In many cases, we find that our method is able to\nclose the gap between model performance on a clean test set and an attacked\ntest set and hence reduce the effect of adversarial attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yangqiaoyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carton_S/0/1/0/all/0/1\">Samuel Carton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"History Compression via Language Models in Reinforcement Learning. (arXiv:2205.12258v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.12258","description":"<p>In a partially observable Markov decision process (POMDP), an agent typically\nuses a representation of the past to approximate the underlying MDP. We propose\nto utilize a frozen Pretrained Language Transformer (PLT) for history\nrepresentation and compression to improve sample efficiency. To avoid training\nof the Transformer, we introduce FrozenHopfield, which automatically associates\nobservations with pretrained token embeddings. To form these associations, a\nmodern Hopfield network stores these token embeddings, which are retrieved by\nqueries that are obtained by a random but fixed projection of observations. Our\nnew method, HELM, enables actor-critic network architectures that contain a\npretrained language Transformer for history representation as a memory module.\nSince a representation of the past need not be learned, HELM is much more\nsample efficient than competitors. On Minigrid and Procgen environments HELM\nachieves new state-of-the-art results. Our code is available at\nhttps://github.com/ml-jku/helm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paischer_F/0/1/0/all/0/1\">Fabian Paischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adler_T/0/1/0/all/0/1\">Thomas Adler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_V/0/1/0/all/0/1\">Vihang Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitto_Nemling_A/0/1/0/all/0/1\">Angela Bitto-Nemling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holzleitner_M/0/1/0/all/0/1\">Markus Holzleitner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehner_S/0/1/0/all/0/1\">Sebastian Lehner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eghbal_zadeh_H/0/1/0/all/0/1\">Hamid Eghbal-zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1\">Sepp Hochreiter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Affordance Extraction with an External Knowledge Database for Text-Based Simulated Environments. (arXiv:2207.00265v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.00265","description":"<p>Text-based simulated environments have proven to be a valid testbed for\nmachine learning approaches. The process of affordance extraction can be used\nto generate possible actions for interaction within such an environment. In\nthis paper the capabilities and challenges for utilizing external knowledge\ndatabases (in particular ConceptNet) in the process of affordance extraction\nare studied. An algorithm for automated affordance extraction is introduced and\nevaluated on the Interactive Fiction (IF) platforms TextWorld and Jericho. For\nthis purpose, the collected affordances are translated into text commands for\nIF agents. To probe the quality of the automated evaluation process, an\nadditional human baseline study is conducted. The paper illustrates that,\ndespite some challenges, external databases can in principle be used for\naffordance extraction. The paper concludes with recommendations for further\nmodification and improvement of the process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gelhausen_P/0/1/0/all/0/1\">P. Gelhausen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1\">M. Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_G/0/1/0/all/0/1\">G. Peters</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Parallelism Tradeoff: Limitations of Log-Precision Transformers. (arXiv:2207.00729v2 [cs.CC] UPDATED)","link":"http://arxiv.org/abs/2207.00729","description":"<p>Despite their omnipresence in modern NLP, characterizing the computational\npower of transformer neural nets remains an interesting open question. We prove\nthat transformers whose arithmetic precision is logarithmic in the number of\ninput tokens (and whose feedforward nets are computable using space linear in\ntheir input) can be simulated by constant-depth logspace-uniform threshold\ncircuits. This provides insight on the power of transformers using known\nresults in complexity theory. For example, if $\\mathsf L \\neq \\mathsf P$ (i.e.,\nnot all poly-time problems can be solved using logarithmic space), then\ntransformers cannot even accurately solve linear equalities or check membership\nin an arbitrary context-free grammar with empty productions. Our result\nintuitively emerges from the transformer architecture's high parallelizability.\nWe thus speculatively introduce the idea of a fundamental parallelism tradeoff:\nany model architecture as parallelizable as the transformer will obey\nlimitations similar to it. Since parallelism is key to training models at\nmassive scale, this suggests a potential inherent weakness of the scaling\nparadigm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-Derived Knowledge Helps Vision: A Simple Cross-modal Distillation for Video-based Action Anticipation. (arXiv:2210.05991v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.05991","description":"<p>Anticipating future actions in a video is useful for many autonomous and\nassistive technologies. Most prior action anticipation work treat this as a\nvision modality problem, where the models learn the task information primarily\nfrom the video features in the action anticipation datasets. However, knowledge\nabout action sequences can also be obtained from external textual data. In this\nwork, we show how knowledge in pretrained language models can be adapted and\ndistilled into vision-based action anticipation models. We show that a simple\ndistillation technique can achieve effective knowledge transfer and provide\nconsistent gains on a strong vision model (Anticipative Vision Transformer) for\ntwo action anticipation datasets (3.5% relative gain on EGTEA-GAZE+ and 7.2%\nrelative gain on EPIC-KITCHEN 55), giving a new state-of-the-art result.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sayontan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_T/0/1/0/all/0/1\">Tanvi Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoai_M/0/1/0/all/0/1\">Minh Hoai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information Filter upon Diversity-Improved Decoding for Diversity-Faithfulness Tradeoff in NLG. (arXiv:2210.13829v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13829","description":"<p>Some Natural Language Generation (NLG) tasks require both faithfulness and\ndiversity. The decoding strategy is intensively related to the quality of the\ngenerated text. Strategies such as beam search, greedy search, etc., perform\nwith low diversity and high repetition. On the other hand, guided decoding, the\nsolution towards diversity, may generate unfaithful expressions. To this end,\nthis paper presents Information Filter upon Diversity-Improved Decoding (IFDID)\nto obtain the tradeoff between diversity and faithfulness. IFDID is a two-stage\ndecoding strategy leveraging the proposed Enhance-Filter framework, which\nachieves the tradeoff by increasing the probabilities of some typical tokens\nbeing selected and subsequently filtering them by their information amount. To\nverify the effectiveness, we compare our method with other baselines on related\nCommonGEN, RocStories and AdGen benchmarks, which cover Chinese and English\ndatasets. Our numerical experimental results and human evaluation outcomes\nverify the effectiveness of the proposed approach, as our approach achieves a\n1.24 higher ROUGE score describing faithfulness as well as higher diversity\nrepresented by 62.5% higher upon Dist-2 than traditional approaches,\ndemonstrating that IFDID is a novel SOTA decoding strategy for the tradeoff\nbetween diversity and faithfulness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Han Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaosong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zexing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Feng Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets. (arXiv:2211.07321v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07321","description":"<p>In this paper, we provide a new perspective on self-supervised speech models\nfrom how the self-training targets are obtained. We generalize the targets\nextractor into Offline Targets Extractor (Off-TE) and Online Targets Extractor\n(On-TE). Based on this, we propose a new multi-tasking learning framework for\nself-supervised learning, MT4SSL, which stands for Boosting Self-Supervised\nSpeech Representation Learning by Integrating Multiple Targets. MT4SSL refers\nto two typical models, HuBERT and data2vec, which use the K-means algorithm as\nan Off-TE and a teacher network without gradients as an On-TE, respectively.\nOur model outperforms previous SSL methods by nontrivial margins on the\nLibriSpeech benchmark, and is comparable to or even better than the\nbest-performing models with no need for that much data. Furthermore, we find\nthat using both Off-TE and On-TE results in better convergence in the\npre-training phase. With both effectiveness and efficiency, we think that doing\nmulti-task learning on self-supervised speech models from our perspective is a\npromising trend.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Ziyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_Z/0/1/0/all/0/1\">Zhisheng Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Changli Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xie Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Lexical Borrowings from Dominant Languages in Multilingual Wordlists. (arXiv:2302.00189v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.00189","description":"<p>Language contact is a pervasive phenomenon reflected in the borrowing of\nwords from donor to recipient languages. Most computational approaches to\nborrowing detection treat all languages under study as equally important, even\nthough dominant languages have a stronger impact on heritage languages than\nvice versa. We test new methods for lexical borrowing detection in contact\nsituations where dominant languages play an important role, applying two\nclassical sequence comparison methods and one machine learning method to a\nsample of seven Latin American languages which have all borrowed extensively\nfrom Spanish. All methods perform well, with the supervised machine learning\nsystem outperforming the classical systems. A review of detection errors shows\nthat borrowing detection could be substantially improved by taking into account\ndonor words with divergent meanings from recipient words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1\">John E. Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+List_J/0/1/0/all/0/1\">Johann-Mattis List</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Categorical Archive of ChatGPT Failures. (arXiv:2302.03494v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.03494","description":"<p>Large language models have been demonstrated to be valuable in different\nfields. ChatGPT, developed by OpenAI, has been trained using massive amounts of\ndata and simulates human conversation by comprehending context and generating\nappropriate responses. It has garnered significant attention due to its ability\nto effectively answer a broad range of human inquiries, with fluent and\ncomprehensive answers surpassing prior public chatbots in both security and\nusefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,\nwhich is the focus of this study. Eleven categories of failures, including\nreasoning, factual errors, math, coding, and bias, are presented and discussed.\nThe risks, limitations, and societal implications of ChatGPT are also\nhighlighted. The goal of this study is to assist researchers and developers in\nenhancing future language models and chatbots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Song of Ice and Fire: Analyzing Textual Autotelic Agents in ScienceWorld. (arXiv:2302.05244v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2302.05244","description":"<p>Building open-ended agents that can autonomously discover a diversity of\nbehaviours is one of the long-standing goals of artificial intelligence. This\nchallenge can be studied in the framework of autotelic RL agents, i.e. agents\nthat learn by selecting and pursuing their own goals, self-organizing a\nlearning curriculum. Recent work identified language has a key dimension of\nautotelic learning, in particular because it enables abstract goal sampling and\nguidance from social peers for hindsight relabelling. Within this perspective,\nwe study the following open scientific questions: What is the impact of\nhindsight feedback from a social peer (e.g. selective vs. exhaustive)? How can\nthe agent learn from very rare language goal examples in its experience replay?\nHow can multiple forms of exploration be combined, and take advantage of easier\ngoals as stepping stones to reach harder ones? To address these questions, we\nuse ScienceWorld, a textual environment with rich abstract and combinatorial\nphysics. We show the importance of selectivity from the social peer's feedback;\nthat experience replay needs to over-sample examples of rare goals; and that\nfollowing self-generated goal sequences where the agent's competence is\nintermediate leads to significant improvements in final performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teodorescu_L/0/1/0/all/0/1\">Laetitia Teodorescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xingdi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1\">Marc-Alexandre C&#xf4;t&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI Chat Assistants can Improve Conversations about Divisive Topics. (arXiv:2302.07268v3 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2302.07268","description":"<p>A rapidly increasing amount of human conversation occurs online. But\ndivisiveness and conflict can fester in text-based interactions on social media\nplatforms, in messaging apps, and on other digital forums. Such toxicity\nincreases polarization and, importantly, corrodes the capacity of diverse\nsocieties to develop efficient solutions to complex social problems that impact\neveryone. Scholars and civil society groups promote interventions that can make\ninterpersonal conversations less divisive or more productive in offline\nsettings, but scaling these efforts to the amount of discourse that occurs\nonline is extremely challenging. We present results of a large-scale experiment\nthat demonstrates how online conversations about divisive topics can be\nimproved with artificial intelligence tools. Specifically, we employ a large\nlanguage model to make real-time, evidence-based recommendations intended to\nimprove participants' perception of feeling understood in conversations. We\nfind that these interventions improve the reported quality of the conversation,\nreduce political divisiveness, and improve the tone, without systematically\nchanging the content of the conversation or moving people's policy attitudes.\nThese findings have important implications for future research on social media,\npolitical deliberation, and the growing community of scholars interested in the\nplace of artificial intelligence within computational social science.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Argyle_L/0/1/0/all/0/1\">Lisa P. Argyle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busby_E/0/1/0/all/0/1\">Ethan Busby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gubler_J/0/1/0/all/0/1\">Joshua Gubler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bail_C/0/1/0/all/0/1\">Chris Bail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howe_T/0/1/0/all/0/1\">Thomas Howe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rytting_C/0/1/0/all/0/1\">Christopher Rytting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wingate_D/0/1/0/all/0/1\">David Wingate</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks. (arXiv:2302.08043v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.08043","description":"<p>Graphs can model complex relationships between objects, enabling a myriad of\nWeb applications such as online page/article classification and social\nrecommendation. While graph neural networks(GNNs) have emerged as a powerful\ntool for graph representation learning, in an end-to-end supervised setting,\ntheir performance heavily rely on a large amount of task-specific supervision.\nTo reduce labeling requirement, the \"pre-train, fine-tune\" and \"pre-train,\nprompt\" paradigms have become increasingly common. In particular, prompting is\na popular alternative to fine-tuning in natural language processing, which is\ndesigned to narrow the gap between pre-training and downstream objectives in a\ntask-specific manner. However, existing study of prompting on graphs is still\nlimited, lacking a universal treatment to appeal to different downstream tasks.\nIn this paper, we propose GraphPrompt, a novel pre-training and prompting\nframework on graphs. GraphPrompt not only unifies pre-training and downstream\ntasks into a common task template, but also employs a learnable prompt to\nassist a downstream task in locating the most relevant knowledge from the\npre-train model in a task-specific manner. Finally, we conduct extensive\nexperiments on five public datasets to evaluate and analyze GraphPrompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zemin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xingtong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinming Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis. (arXiv:2302.08624v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.08624","description":"<p>In this paper, we present InstructABSA, Aspect-Based Sentiment Analysis\n(ABSA) using instruction learning paradigm for all ABSA subtasks: Aspect Term\nExtraction (ATE), Aspect Term Sentiment Classification (ATSC), and Joint Task\nmodeling. Our method introduces positive, negative, and neutral examples to\neach training sample, and instruction tunes the model (Tk-Instruct Base) for\neach ABSA subtask, yielding significant performance improvements. Experimental\nresults on the Sem Eval 2014 dataset demonstrate that InstructABSA outperforms\nthe previous state-of-the-art (SOTA) approaches on all three ABSA subtasks\n(ATE, ATSC, and Joint Task) by a significant margin, outperforming 7x larger\nmodels. In particular, InstructABSA surpasses the SOTA on the restaurant ATE\nsubtask by 7.31% points and on the Laptop Joint Task by 8.63% points. Our\nresults also suggest a strong generalization ability to unseen tasks across all\nthree subtasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scaria_K/0/1/0/all/0/1\">Kevin Scaria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawant_S/0/1/0/all/0/1\">Saurabh Arjun Sawant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Emotion Knowledge Representation Emerges in Large Language Model and Supports Discrete Emotion Inference. (arXiv:2302.09582v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2302.09582","description":"<p>How humans infer discrete emotions is a fundamental research question in the\nfield of psychology. While conceptual knowledge about emotions (emotion\nknowledge) has been suggested to be essential for emotion inference, evidence\nto date is mostly indirect and inconclusive. As the large language models\n(LLMs) have been shown to support effective representations of various human\nconceptual knowledge, the present study further employed artificial neurons in\nLLMs to investigate the mechanism of human emotion inference. With artificial\nneurons activated by prompts, the LLM (RoBERTa) demonstrated a similar\nconceptual structure of 27 discrete emotions as that of human behaviors.\nFurthermore, the LLM-based conceptual structure revealed a human-like reliance\non 14 underlying conceptual attributes of emotions for emotion inference. Most\nimportantly, by manipulating attribute-specific neurons, we found that the\ncorresponding LLM's emotion inference performance deteriorated, and the\nperformance deterioration was correlated to the effectiveness of\nrepresentations of the conceptual attributes on the human side. Our findings\nprovide direct evidence for the emergence of emotion knowledge representation\nin large language models and suggest its casual support for discrete emotion\ninference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Ming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yusheng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hsiu-Yuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jiali Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinmiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huadong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation. (arXiv:2302.09664v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.09664","description":"<p>We introduce a method to measure uncertainty in large language models. For\ntasks like question answering, it is essential to know when we can trust the\nnatural language outputs of foundation models. We show that measuring\nuncertainty in natural language is challenging because of \"semantic\nequivalence\" -- different sentences can mean the same thing. To overcome these\nchallenges we introduce semantic entropy -- an entropy which incorporates\nlinguistic invariances created by shared meanings. Our method is unsupervised,\nuses only a single model, and requires no modifications to off-the-shelf\nlanguage models. In comprehensive ablation studies we show that the semantic\nentropy is more predictive of model accuracy on question answering data sets\nthan comparable baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuhn_L/0/1/0/all/0/1\">Lorenz Kuhn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farquhar_S/0/1/0/all/0/1\">Sebastian Farquhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emphasizing Unseen Words: New Vocabulary Acquisition for End-to-End Speech Recognition. (arXiv:2302.09723v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.09723","description":"<p>Due to the dynamic nature of human language, automatic speech recognition\n(ASR) systems need to continuously acquire new vocabulary. Out-Of-Vocabulary\n(OOV) words, such as trending words and new named entities, pose problems to\nmodern ASR systems that require long training times to adapt their large\nnumbers of parameters. Different from most previous research focusing on\nlanguage model post-processing, we tackle this problem on an earlier processing\nlevel and eliminate the bias in acoustic modeling to recognize OOV words\nacoustically. We propose to generate OOV words using text-to-speech systems and\nto rescale losses to encourage neural networks to pay more attention to OOV\nwords. Specifically, we enlarge the classification loss used for training\nneural networks' parameters of utterances containing OOV words\n(sentence-level), or rescale the gradient used for back-propagation for OOV\nwords (word-level), when fine-tuning a previously trained model on synthetic\naudio. To overcome catastrophic forgetting, we also explore the combination of\nloss rescaling and model regularization, i.e. L2 regularization and elastic\nweight consolidation (EWC). Compared with previous methods that just fine-tune\nsynthetic audio with EWC, the experimental results on the LibriSpeech benchmark\nreveal that our proposed loss rescaling approach can achieve significant\nimprovement on the recall rate with only a slight decrease on word error rate.\nMoreover, word-level rescaling is more stable than utterance-level rescaling\nand leads to higher recall rates and precision on OOV word recognition.\nFurthermore, our proposed combined loss rescaling and weight consolidation\nmethods can support continual learning of an ASR system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Leyuan Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-aware Bayesian Co-attention for Multimodal Emotion Recognition. (arXiv:2302.09856v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.09856","description":"<p>Multimodal emotion recognition is a challenging research area that aims to\nfuse different modalities to predict human emotion. However, most existing\nmodels that are based on attention mechanisms have difficulty in learning\nemotionally relevant parts on their own. To solve this problem, we propose to\nincorporate external emotion-related knowledge in the co-attention based fusion\nof pre-trained models. To effectively incorporate this knowledge, we enhance\nthe co-attention model with a Bayesian attention module (BAM) where a prior\ndistribution is estimated using the emotion-related knowledge. Experimental\nresults on the IEMOCAP dataset show that the proposed approach can outperform\nseveral state-of-the-art approaches by at least 0.7% unweighted accuracy (UA).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zihan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-02-21T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}