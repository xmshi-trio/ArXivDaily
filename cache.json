{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-03-03T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Building High-accuracy Multilingual ASR with Gated Language Experts and Curriculum Training. (arXiv:2303.00786v1 [cs.CL])","link":"http://arxiv.org/abs/2303.00786","description":"<p>We propose gated language experts to improve multilingual transformer\ntransducer models without any language identification (LID) input from users\nduring inference. We define gating mechanism and LID loss to let transformer\nencoders learn language-dependent information, construct the multilingual\ntransformer block with gated transformer experts and shared transformer layers\nfor compact models, and apply linear experts on joint network output to better\nregularize speech acoustic and token label joint information. Furthermore, a\ncurriculum training scheme is proposed to let LID guide the gated language\nexperts for better serving their corresponding languages. Evaluated on the\nEnglish and Spanish bilingual task, our methods achieve average 12.5% and 7.3%\nrelative word error reductions over the baseline bilingual model and\nmonolingual models, respectively, obtaining similar results to the upper bound\nmodel trained and inferred with oracle LID. We further explore our method on\ntrilingual, quadrilingual, and pentalingual models, and observe similar\nadvantages as in the bilingual models, which demonstrates the easy extension to\nmore languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_E/0/1/0/all/0/1\">Eric Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuxuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yimeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jian Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1\">Edward Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Cross-accent Data Augmentation for Automatic Speech Recognition. (arXiv:2303.00802v1 [cs.CL])","link":"http://arxiv.org/abs/2303.00802","description":"<p>The awareness for biased ASR datasets or models has increased notably in\nrecent years. Even for English, despite a vast amount of available training\ndata, systems perform worse for non-native speakers. In this work, we improve\nan accent-conversion model (ACM) which transforms native US-English speech into\naccented pronunciation. We include phonetic knowledge in the ACM training to\nprovide accurate feedback about how well certain pronunciation patterns were\nrecovered in the synthesized waveform. Furthermore, we investigate the\nfeasibility of learned accent representations instead of static embeddings.\nGenerated data was then used to train two state-of-the-art ASR systems. We\nevaluated our approach on native and non-native English datasets and found that\nsynthetically accented data helped the ASR to better understand speech from\nseen accents. This observation did not translate to unseen accents, and it was\nnot observed for a model that had been pre-trained exclusively with native\nspeech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klumpp_P/0/1/0/all/0/1\">Philipp Klumpp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitkara_P/0/1/0/all/0/1\">Pooja Chitkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sari_L/0/1/0/all/0/1\">Leda Sar&#x131;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serai_P/0/1/0/all/0/1\">Prashant Serai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jilong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veliche_I/0/1/0/all/0/1\">Irina-Elena Veliche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rongqing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers. (arXiv:2303.00807v1 [cs.IR])","link":"http://arxiv.org/abs/2303.00807","description":"<p>Many information retrieval tasks require large labeled datasets for\nfine-tuning. However, such datasets are often unavailable, and their utility\nfor real-world applications can diminish quickly due to domain shifts. To\naddress this challenge, we develop and motivate a method for using large\nlanguage models (LLMs) to generate large numbers of synthetic queries cheaply.\nThe method begins by generating a small number of synthetic queries using an\nexpensive LLM. After that, a much less expensive one is used to create large\nnumbers of synthetic queries, which are used to fine-tune a family of reranker\nmodels. These rerankers are then distilled into a single efficient retriever\nfor use in the target domain. We show that this technique boosts zero-shot\naccuracy in long-tail domains, even where only 2K synthetic queries are used\nfor fine-tuning, and that it achieves substantially lower latency than standard\nreranking methods. We make our end-to-end approach, including our synthetic\ndatasets and replication code, publicly available on Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saad_Falcon_J/0/1/0/all/0/1\">Jon Saad-Falcon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1\">Omar Khattab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santhanam_K/0/1/0/all/0/1\">Keshav Santhanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florian_R/0/1/0/all/0/1\">Radu Florian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franz_M/0/1/0/all/0/1\">Martin Franz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roukos_S/0/1/0/all/0/1\">Salim Roukos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1\">Avirup Sil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sultan_M/0/1/0/all/0/1\">Md Arafat Sultan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Soft Prompt Guided Joint Learning for Cross-Domain Sentiment Analysis. (arXiv:2303.00815v1 [cs.CL])","link":"http://arxiv.org/abs/2303.00815","description":"<p>Aspect term extraction is a fundamental task in fine-grained sentiment\nanalysis, which aims at detecting customer's opinion targets from reviews on\nproduct or service. The traditional supervised models can achieve promising\nresults with annotated datasets, however, the performance dramatically\ndecreases when they are applied to the task of cross-domain aspect term\nextraction. Existing cross-domain transfer learning methods either directly\ninject linguistic features into Language models, making it difficult to\ntransfer linguistic knowledge to target domain, or rely on the fixed predefined\nprompts, which is time-consuming to construct the prompts over all potential\naspect term spans. To resolve the limitations, we propose a soft prompt-based\njoint learning method for cross domain aspect term extraction in this paper.\nSpecifically, by incorporating external linguistic features, the proposed\nmethod learn domain-invariant representations between source and target domains\nvia multiple objectives, which bridges the gap between domains with varied\ndistributions of aspect terms. Further, the proposed method interpolates a set\nof transferable soft prompts consisted of multiple learnable vectors that are\nbeneficial to detect aspect terms in target domain. Extensive experiments are\nconducted on the benchmark datasets and the experimental results demonstrate\nthe effectiveness of the proposed method for cross-domain aspect terms\nextraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jingli Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weihua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Q/0/1/0/all/0/1\">Quan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jianhua Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control. (arXiv:2303.00855v1 [cs.RO])","link":"http://arxiv.org/abs/2303.00855","description":"<p>Recent progress in large language models (LLMs) has demonstrated the ability\nto learn and leverage Internet-scale knowledge through pre-training with\nautoregressive models. Unfortunately, applying such models to settings with\nembodied agents, such as robots, is challenging due to their lack of experience\nwith the physical world, inability to parse non-language observations, and\nignorance of rewards or safety constraints that robots may require. On the\nother hand, language-conditioned robotic policies that learn from interaction\ndata can provide the necessary grounding that allows the agent to be correctly\nsituated in the real world, but such policies are limited by the lack of\nhigh-level semantic understanding due to the limited breadth of the interaction\ndata available for training them. Thus, if we want to make use of the semantic\nknowledge in a language model while still situating it in an embodied setting,\nwe must construct an action sequence that is both likely according to the\nlanguage model and also realizable according to grounded models of the\nenvironment. We frame this as a problem similar to probabilistic filtering:\ndecode a sequence that both has high probability under the language model and\nhigh probability under a set of grounded model objectives. We demonstrate this\nguided decoding strategy is able to solve complex, long-horizon embodiment\ntasks in a robotic setting by leveraging the knowledge of both models. The\nproject's website can be found at grounded-decoding.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenlong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1\">Dhruv Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driess_D/0/1/0/all/0/1\">Danny Driess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Andy Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1\">Pete Florence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1\">Karol Hausman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1\">Brian Ichter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Clustered Federated Learning. (arXiv:2303.00897v1 [cs.LG])","link":"http://arxiv.org/abs/2303.00897","description":"<p>Federated learning is a distributed learning framework that takes full\nadvantage of private data samples kept on edge devices. In real-world federated\nlearning systems, these data samples are often decentralized and\nNon-Independently Identically Distributed (Non-IID), causing divergence and\nperformance degradation in the federated learning process. As a new solution,\nclustered federated learning groups federated clients with similar data\ndistributions to impair the Non-IID effects and train a better model for every\ncluster. This paper proposes StoCFL, a novel clustered federated learning\napproach for generic Non-IID issues. In detail, StoCFL implements a flexible\nCFL framework that supports an arbitrary proportion of client participation and\nnewly joined clients for a varying FL system, while maintaining a great\nimprovement in model performance. The intensive experiments are conducted by\nusing four basic Non-IID settings and a real-world dataset. The results show\nthat StoCFL could obtain promising cluster results even when the number of\nclusters is unknown. Based on the client clustering results, models trained\nwith StoCFL outperform baseline approaches in a variety of contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Dun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiangjing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Text Generation. (arXiv:2303.00908v1 [cs.CL])","link":"http://arxiv.org/abs/2303.00908","description":"<p>Users interact with text, image, code, or other editors on a daily basis.\nHowever, machine learning models are rarely trained in the settings that\nreflect the interactivity between users and their editor. This is\nunderstandable as training AI models with real users is not only slow and\ncostly, but what these models learn may be specific to user interface design\nchoices. Unfortunately, this means most of the research on text, code, and\nimage generation has focused on non-interactive settings, whereby the model is\nexpected to get everything right without accounting for any input from a user\nwho may be willing to help.\n</p>\n<p>We introduce a new Interactive Text Generation task that allows training\ngeneration models interactively without the costs of involving real users, by\nusing user simulators that provide edits that guide the model towards a given\ntarget text. We train our interactive models using Imitation Learning, and our\nexperiments against competitive non-interactive generation models show that\nmodels trained interactively are superior to their non-interactive\ncounterparts, even when all models are given the same budget of user inputs or\nedits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faltings_F/0/1/0/all/0/1\">Felix Faltings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1\">Michel Galley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brantley_K/0/1/0/all/0/1\">Kiant&#xe9; Brantley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weixin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolan_B/0/1/0/all/0/1\">Bill Dolan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-Scale Domain-Specific Pretraining for Biomedical Vision-Language Processing. (arXiv:2303.00915v1 [cs.CV])","link":"http://arxiv.org/abs/2303.00915","description":"<p>Contrastive pretraining on parallel image-text data has attained great\nsuccess in vision-language processing (VLP), as exemplified by CLIP and related\nmethods. However, prior explorations tend to focus on general domains in the\nweb. Biomedical images and text are rather different, but publicly available\ndatasets are small and skew toward chest X-ray, thus severely limiting\nprogress. In this paper, we conducted by far the largest study on biomedical\nVLP, using 15 million figure-caption pairs extracted from biomedical research\narticles in PubMed Central. Our dataset (PMC-15M) is two orders of magnitude\nlarger than existing biomedical image-text datasets such as MIMIC-CXR, and\nspans a diverse range of biomedical images. The standard CLIP method is\nsuboptimal for the biomedical domain. We propose BiomedCLIP with\ndomain-specific adaptations tailored to biomedical VLP. We conducted extensive\nexperiments and ablation studies on standard biomedical imaging tasks from\nretrieval to classification to visual question-answering (VQA). BiomedCLIP\nestablished new state of the art in a wide range of standard datasets,\nsubstantially outperformed prior VLP approaches. Surprisingly, BiomedCLIP even\noutperformed radiology-specific state-of-the-art models such as BioViL on\nradiology-specific tasks such as RSNA pneumonia detection, thus highlighting\nthe utility in large-scale pretraining across all biomedical image types. We\nwill release our models at https://aka.ms/biomedclip to facilitate future\nresearch in biomedical VLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanbo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagga_J/0/1/0/all/0/1\">Jaspreet Bagga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1\">Robert Tinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preston_S/0/1/0/all/0/1\">Sam Preston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_R/0/1/0/all/0/1\">Rajesh Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valluri_N/0/1/0/all/0/1\">Naveen Valluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Cliff Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Role of Reviewer Expertise in Temporal Review Helpfulness Prediction. (arXiv:2303.00923v1 [cs.CL])","link":"http://arxiv.org/abs/2303.00923","description":"<p>Helpful reviews have been essential for the success of e-commerce services,\nas they help customers make quick purchase decisions and benefit the merchants\nin their sales. While many reviews are informative, others provide little value\nand may contain spam, excessive appraisal, or unexpected biases. With the large\nvolume of reviews and their uneven quality, the problem of detecting helpful\nreviews has drawn much attention lately. Existing methods for identifying\nhelpful reviews primarily focus on review text and ignore the two key factors\nof (1) who post the reviews and (2) when the reviews are posted. Moreover, the\nhelpfulness votes suffer from scarcity for less popular products and recently\nsubmitted (a.k.a., cold-start) reviews. To address these challenges, we\nintroduce a dataset and develop a model that integrates the reviewer's\nexpertise, derived from the past review history of the reviewers, and the\ntemporal dynamics of the reviews to automatically assess review helpfulness. We\nconduct experiments on our dataset to demonstrate the effectiveness of\nincorporating these factors and report improved results compared to several\nwell-established baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayeem_M/0/1/0/all/0/1\">Mir Tafseer Nayeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafiei_D/0/1/0/all/0/1\">Davood Rafiei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Reasonability of the Test Set for Simultaneous Machine Translation. (arXiv:2303.00969v1 [cs.CL])","link":"http://arxiv.org/abs/2303.00969","description":"<p>Simultaneous machine translation (SimulMT) models start translation before\nthe end of the source sentence, making the translation monotonically aligned\nwith the source sentence. However, the general full-sentence translation test\nset is acquired by offline translation of the entire source sentence, which is\nnot designed for SimulMT evaluation, making us rethink whether this will\nunderestimate the performance of SimulMT models. In this paper, we manually\nannotate a monotonic test set based on the MuST-C English-Chinese test set,\ndenoted as SiMuST-C. Our human evaluation confirms the acceptability of our\nannotated test set. Evaluations on three different SimulMT models verify that\nthe underestimation problem can be alleviated on our test set. Further\nexperiments show that finetuning on an automatically extracted monotonic\ntraining set improves SimulMT models by up to 3 BLEU points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengge Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_J/0/1/0/all/0/1\">Jian Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuhang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuoying Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Large Text Corpora for End-to-End Speech Summarization. (arXiv:2303.00978v1 [cs.CL])","link":"http://arxiv.org/abs/2303.00978","description":"<p>End-to-end speech summarization (E2E SSum) is a technique to directly\ngenerate summary sentences from speech. Compared with the cascade approach,\nwhich combines automatic speech recognition (ASR) and text summarization\nmodels, the E2E approach is more promising because it mitigates ASR errors,\nincorporates nonverbal information, and simplifies the overall system. However,\nsince collecting a large amount of paired data (i.e., speech and summary) is\ndifficult, the training data is usually insufficient to train a robust E2E SSum\nsystem. In this paper, we present two novel methods that leverage a large\namount of external text summarization data for E2E SSum training. The first\ntechnique is to utilize a text-to-speech (TTS) system to generate synthesized\nspeech, which is used for E2E SSum training with the text summary. The second\nis a TTS-free method that directly inputs phoneme sequence instead of\nsynthesized speech to the E2E SSum model. Experiments show that our proposed\nTTS- and phoneme-based methods improve several metrics on the How2 dataset. In\nparticular, our best system outperforms a previous state-of-the-art one by a\nlarge margin (i.e., METEOR score improvements of more than 6 points). To the\nbest of our knowledge, this is the first work to use external language\nresources for E2E SSum. Moreover, we report a detailed analysis of the How2\ndataset to confirm the validity of our proposed E2E SSum system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsuura_K/0/1/0/all/0/1\">Kohei Matsuura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashihara_T/0/1/0/all/0/1\">Takanori Ashihara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moriya_T/0/1/0/all/0/1\">Takafumi Moriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1\">Tomohiro Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogawa_A/0/1/0/all/0/1\">Atsunori Ogawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delcroix_M/0/1/0/all/0/1\">Marc Delcroix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masumura_R/0/1/0/all/0/1\">Ryo Masumura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages. (arXiv:2303.01037v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01037","description":"<p>We introduce the Universal Speech Model (USM), a single large model that\nperforms automatic speech recognition (ASR) across 100+ languages. This is\nachieved by pre-training the encoder of the model on a large unlabeled\nmultilingual dataset of 12 million (M) hours spanning over 300 languages, and\nfine-tuning on a smaller labeled dataset. We use multilingual pre-training with\nrandom-projection quantization and speech-text modality matching to achieve\nstate-of-the-art performance on downstream multilingual ASR and speech-to-text\ntranslation tasks. We also demonstrate that despite using a labeled training\nset 1/7-th the size of that used for the Whisper model, our model exhibits\ncomparable or better performance on both in-domain and out-of-domain speech\nrecognition tasks across many languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">James Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhehuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nanxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Axelrod_V/0/1/0/all/0/1\">Vera Axelrod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gary Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Ke Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_A/0/1/0/all/0/1\">Andrew Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Daniel S. Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haghani_P/0/1/0/all/0/1\">Parisa Haghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riesa_J/0/1/0/all/0/1\">Jason Riesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perng_G/0/1/0/all/0/1\">Ginger Perng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltau_H/0/1/0/all/0/1\">Hagen Soltau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramabhadran_B/0/1/0/all/0/1\">Bhuvana Ramabhadran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_P/0/1/0/all/0/1\">Pedro Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1\">Chung-Cheng Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schalkwyk_J/0/1/0/all/0/1\">Johan Schalkwyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adopting the Multi-answer Questioning Task with an Auxiliary Metric for Extreme Multi-label Text Classification Utilizing the Label Hierarchy. (arXiv:2303.01064v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01064","description":"<p>Extreme multi-label text classification utilizes the label hierarchy to\npartition extreme labels into multiple label groups, turning the task into\nsimple multi-group multi-label classification tasks. Current research encodes\nlabels as a vector with fixed length which needs establish multiple classifiers\nfor different label groups. The problem is how to build only one classifier\nwithout sacrificing the label relationship in the hierarchy. This paper adopts\nthe multi-answer questioning task for extreme multi-label classification. This\npaper also proposes an auxiliary classification evaluation metric. This study\nadopts the proposed method and the evaluation metric to the legal domain. The\nutilization of legal Berts and the study on task distribution are discussed.\nThe experiment results show that the proposed hierarchy and multi-answer\nquestioning task can do extreme multi-label classification for EURLEX dataset.\nAnd in minor/fine-tuning the multi-label classification task, the domain\nadapted BERT models could not show apparent advantages in this experiment. The\nmethod is also theoretically applicable to zero-shot learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1\">Ying Wah Teh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Garadi_M/0/1/0/all/0/1\">Mohammed Ali Al-Garadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Targeted Adversarial Attacks against Neural Machine Translation. (arXiv:2303.01068v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01068","description":"<p>Neural Machine Translation (NMT) systems are used in various applications.\nHowever, it has been shown that they are vulnerable to very small perturbations\nof their inputs, known as adversarial attacks. In this paper, we propose a new\ntargeted adversarial attack against NMT models. In particular, our goal is to\ninsert a predefined target keyword into the translation of the adversarial\nsentence while maintaining similarity between the original sentence and the\nperturbed one in the source domain. To this aim, we propose an optimization\nproblem, including an adversarial loss term and a similarity term. We use\ngradient projection in the embedding space to craft an adversarial sentence.\nExperimental results show that our attack outperforms Seq2Sick, the other\ntargeted adversarial attack against NMT models, in terms of success rate and\ndecrease in translation quality. Our attack succeeds in inserting a keyword\ninto the translation for more than 75% of sentences while similarity with the\noriginal sentence stays preserved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sadrizadeh_S/0/1/0/all/0/1\">Sahar Sadrizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghdam_A/0/1/0/all/0/1\">AmirHossein Dabiri Aghdam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolamic_L/0/1/0/all/0/1\">Ljiljana Dolamic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1\">Pascal Frossard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LANDMARK: Language-guided Representation Enhancement Framework for Scene Graph Generation. (arXiv:2303.01080v1 [cs.CV])","link":"http://arxiv.org/abs/2303.01080","description":"<p>Scene graph generation (SGG) is a sophisticated task that suffers from both\ncomplex visual features and dataset long-tail problem. Recently, various\nunbiased strategies have been proposed by designing novel loss functions and\ndata balancing strategies. Unfortunately, these unbiased methods fail to\nemphasize language priors in feature refinement perspective. Inspired by the\nfact that predicates are highly correlated with semantics hidden in\nsubject-object pair and global context, we propose LANDMARK (LANguage-guiDed\nrepresentationenhanceMent frAmewoRK) that learns predicate-relevant\nrepresentations from language-vision interactive patterns, global language\ncontext and pair-predicate correlation. Specifically, we first project object\nlabels to three distinctive semantic embeddings for different representation\nlearning. Then, Language Attention Module (LAM) and Experience Estimation\nModule (EEM) process subject-object word embeddings to attention vector and\npredicate distribution, respectively. Language Context Module (LCM) encodes\nglobal context from each word embed-ding, which avoids isolated learning from\nlocal information. Finally, modules outputs are used to update visual\nrepresentations and SGG model's prediction. All language representations are\npurely generated from object categories so that no extra knowledge is needed.\nThis framework is model-agnostic and consistently improves performance on\nexisting SGG models. Besides, representation-level unbiased strategies endow\nLANDMARK the advantage of compatibility with other methods. Code is available\nat https://github.com/rafa-cxg/PySGG-cxg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaoguang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Teng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shaowei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changyin Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can BERT Refrain from Forgetting on Sequential Tasks? A Probing Study. (arXiv:2303.01081v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01081","description":"<p>Large pre-trained language models help to achieve state of the art on a\nvariety of natural language processing (NLP) tasks, nevertheless, they still\nsuffer from forgetting when incrementally learning a sequence of tasks. To\nalleviate this problem, recent works enhance existing models by sparse\nexperience replay and local adaption, which yield satisfactory performance.\nHowever, in this paper we find that pre-trained language models like BERT have\na potential ability to learn sequentially, even without any sparse memory\nreplay. To verify the ability of BERT to maintain old knowledge, we adopt and\nre-finetune single-layer probe networks with the parameters of BERT fixed. We\ninvestigate the models on two types of NLP tasks, text classification and\nextractive question answering. Our experiments reveal that BERT can actually\ngenerate high quality representations for previously learned tasks in a long\nterm, under extremely sparse replay or even no replay. We further introduce a\nseries of novel methods to interpret the mechanism of forgetting and how memory\nrehearsal plays a significant role in task incremental learning, which bridges\nthe gap between our new discovery and previous studies about catastrophic\nforgetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1\">Mingxu Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yansong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiteG2P: A fast, light and high accuracy model for grapheme-to-phoneme conversion. (arXiv:2303.01086v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01086","description":"<p>As a key component of automated speech recognition (ASR) and the front-end in\ntext-to-speech (TTS), grapheme-to-phoneme (G2P) plays the role of converting\nletters to their corresponding pronunciations. Existing methods are either slow\nor poor in performance, and are limited in application scenarios, particularly\nin the process of on-device inference. In this paper, we integrate the\nadvantages of both expert knowledge and connectionist temporal classification\n(CTC) based neural network and propose a novel method named LiteG2P which is\nfast, light and theoretically parallel. With the carefully leading design,\nLiteG2P can be applied both on cloud and on device. Experimental results on the\nCMU dataset show that the performance of the proposed method is superior to the\nstate-of-the-art CTC based method with 10 times fewer parameters, and even\ncomparable to the state-of-the-art Transformer-based sequence-to-sequence model\nwith less parameters and 33 times less computation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Peisong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuxiang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shichao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xiang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zejun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CTRLStruct: Dialogue Structure Learning for Open-Domain Response Generation. (arXiv:2303.01094v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01094","description":"<p>Dialogue structure discovery is essential in dialogue generation.\nWell-structured topic flow can leverage background information and predict\nfuture topics to help generate controllable and explainable responses. However,\nmost previous work focused on dialogue structure learning in task-oriented\ndialogue other than open-domain dialogue which is more complicated and\nchallenging. In this paper, we present a new framework CTRLStruct for dialogue\nstructure learning to effectively explore topic-level dialogue clusters as well\nas their transitions with unlabelled information. Precisely, dialogue\nutterances encoded by bi-directional Transformer are further trained through a\nspecial designed contrastive learning task to improve representation. Then we\nperform clustering to utterance-level representations and form topic-level\nclusters that can be considered as vertices in dialogue structure graph. The\nedges in the graph indicating transition probability between vertices are\ncalculated by mimicking expert behavior in datasets. Finally, dialogue\nstructure graph is integrated into dialogue model to perform controlled\nresponse generation. Experiments on two popular open-domain dialogue datasets\nshow our model can generate more coherent responses compared to some excellent\ndialogue models, as well as outperform some typical sentence embedding methods\nin dialogue utterance representation. Code is available in GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Congchi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Denoising-based UNMT is more robust to word-order divergence than MASS-based UNMT. (arXiv:2303.01191v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01191","description":"<p>We aim to investigate whether UNMT approaches with self-supervised\npre-training are robust to word-order divergence between language pairs. We\nachieve this by comparing two models pre-trained with the same self-supervised\npre-training objective. The first model is trained on language pairs with\ndifferent word-orders, and the second model is trained on the same language\npairs with source language re-ordered to match the word-order of the target\nlanguage. Ideally, UNMT approaches which are robust to word-order divergence\nshould exhibit no visible performance difference between the two\nconfigurations. In this paper, we investigate two such self-supervised\npre-training based UNMT approaches, namely Masked Sequence-to-Sequence\nPre-Training, (MASS) (which does not have shuffling noise) and Denoising\nAutoEncoder (DAE), (which has shuffling noise).\n</p>\n<p>We experiment with five English$\\rightarrow$Indic language pairs, i.e.,\nen-hi, en-bn, en-gu, en-kn, and en-ta) where word-order of the source language\nis SVO (Subject-Verb-Object), and the word-order of the target languages is SOV\n(Subject-Object-Verb). We observed that for these language pairs, DAE-based\nUNMT approach consistently outperforms MASS in terms of translation accuracies.\nMoreover, bridging the word-order gap using reordering improves the translation\naccuracy of MASS-based UNMT models, while it cannot improve the translation\naccuracy of DAE-based UNMT models. This observation indicates that DAE-based\nUNMT is more robust to word-order divergence than MASS-based UNMT.\nWord-shuffling noise in DAE approach could be the possible reason for the\napproach being robust to word-order divergence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_T/0/1/0/all/0/1\">Tamali Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V_R/0/1/0/all/0/1\">Rudra Murthy V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UZH_CLyp at SemEval-2023 Task 9: Head-First Fine-Tuning and ChatGPT Data Generation for Cross-Lingual Learning in Tweet Intimacy Prediction. (arXiv:2303.01194v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01194","description":"<p>This paper describes the submission of UZH_CLyp for the SemEval 2023 Task 9\n\"Multilingual Tweet Intimacy Analysis\". We achieved second-best results in all\n10 languages according to the official Pearson's correlation regression\nevaluation measure. Our cross-lingual transfer learning approach explores the\nbenefits of using a Head-First Fine-Tuning method (HeFiT) that first updates\nonly the regression head parameters and then also updates the pre-trained\ntransformer encoder parameters at a reduced learning rate. Additionally, we\nstudy the impact of using a small set of automatically generated examples (in\nour case, from ChatGPT) for low-resource settings where no human-labeled data\nis available. Our study shows that HeFiT stabilizes training and consistently\nimproves results for pre-trained models that lack domain adaptation to tweets.\nOur study also shows a noticeable performance increase in cross-lingual\nlearning when synthetic data is used, confirming the usefulness of current text\ngeneration systems to improve zero-shot baseline results. Finally, we examine\nhow possible inconsistencies in the annotated data contribute to cross-lingual\ninterference issues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michail_A/0/1/0/all/0/1\">Andrianos Michail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konstantinou_S/0/1/0/all/0/1\">Stefanos Konstantinou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clematide_S/0/1/0/all/0/1\">Simon Clematide</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Document Provenance and Authentication through Authorship Classification. (arXiv:2303.01197v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01197","description":"<p>Style analysis, which is relatively a less explored topic, enables several\ninteresting applications. For instance, it allows authors to adjust their\nwriting style to produce a more coherent document in collaboration. Similarly,\nstyle analysis can also be used for document provenance and authentication as a\nprimary step. In this paper, we propose an ensemble-based text-processing\nframework for the classification of single and multi-authored documents, which\nis one of the key tasks in style analysis. The proposed framework incorporates\nseveral state-of-the-art text classification algorithms including classical\nMachine Learning (ML) algorithms, transformers, and deep learning algorithms\nboth individually and in merit-based late fusion. For the merit-based late\nfusion, we employed several weight optimization and selection methods to assign\nmerit-based weights to the individual text classification algorithms. We also\nanalyze the impact of the characters on the task that are usually excluded in\nNLP applications during pre-processing by conducting experiments on both clean\nand un-clean data. The proposed framework is evaluated on a large-scale\nbenchmark dataset, significantly improving performance over the existing\nsolutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zamir_M/0/1/0/all/0/1\">Muhammad Tayyab Zamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayub_M/0/1/0/all/0/1\">Muhammad Asif Ayub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_J/0/1/0/all/0/1\">Jebran Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikram_M/0/1/0/all/0/1\">Muhammad Jawad Ikram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_N/0/1/0/all/0/1\">Nasir Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_K/0/1/0/all/0/1\">Kashif Ahmad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Misinformers: Generating and Combating Multimodal Misinformation. (arXiv:2303.01217v1 [cs.MM])","link":"http://arxiv.org/abs/2303.01217","description":"<p>With the expansion of social media and the increasing dissemination of\nmultimedia content, the spread of misinformation has become a major concern.\nThis necessitates effective strategies for multimodal misinformation detection\n(MMD) that detect whether the combination of an image and its accompanying text\ncould mislead or misinform. Due to the data-intensive nature of deep neural\nnetworks and the labor-intensive process of manual annotation, researchers have\nbeen exploring various methods for automatically generating synthetic\nmultimodal misinformation - which we refer to as Synthetic Misinformers - in\norder to train MMD models. However, limited evaluation on real-world\nmisinformation and a lack of comparisons with other Synthetic Misinformers\nmakes difficult to assess progress in the field. To address this, we perform a\ncomparative study on existing and new Synthetic Misinformers that involves (1)\nout-of-context (OOC) image-caption pairs, (2) cross-modal named entity\ninconsistency (NEI) as well as (3) hybrid approaches and we evaluate them\nagainst real-world misinformation; using the COSMOS benchmark. The comparative\nstudy showed that our proposed CLIP-based Named Entity Swapping can lead to MMD\nmodels that surpass other OOC and NEI Misinformers in terms of multimodal\naccuracy and that hybrid approaches can lead to even higher detection accuracy.\nNevertheless, after alleviating information leakage from the COSMOS evaluation\nprotocol, low Sensitivity scores indicate that the task is significantly more\nchallenging than previous studies suggested. Finally, our findings showed that\nNEI-based Synthetic Misinformers tend to suffer from a unimodal bias, where\ntext-only MMDs can outperform multimodal ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1\">Stefanos-Iordanis Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutlis_C/0/1/0/all/0/1\">Christos Koutlis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1\">Symeon Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrantonakis_P/0/1/0/all/0/1\">Panagiotis C. Petrantonakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Almanac: Knowledge-Grounded Language Models for Clinical Medicine. (arXiv:2303.01229v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01229","description":"<p>Large-language models have recently demonstrated impressive zero-shot\ncapabilities in a variety of natural language tasks such as summarization,\ndialogue generation, and question-answering. Despite many promising\napplications in clinical medicine (e.g. medical record documentation, treatment\nguideline-lookup), adoption of these models in real-world settings has been\nlargely limited by their tendency to generate factually incorrect and sometimes\neven toxic statements. In this paper we explore the ability of large-language\nmodels to facilitate and streamline medical guidelines and recommendation\nreferencing: by enabling these model to access external point-of-care tools in\nresponse to physician queries, we demonstrate significantly improved factual\ngrounding, helpfulness, and safety in a variety of clinical scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zakka_C/0/1/0/all/0/1\">Cyril Zakka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaurasia_A/0/1/0/all/0/1\">Akash Chaurasia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shad_R/0/1/0/all/0/1\">Rohan Shad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hiesinger_W/0/1/0/all/0/1\">William Hiesinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frauds Bargain Attack: Generating Adversarial Text Samples via Word Manipulation Process. (arXiv:2303.01234v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01234","description":"<p>Recent studies on adversarial examples expose vulnerabilities of natural\nlanguage processing (NLP) models. Existing techniques for generating\nadversarial examples are typically driven by deterministic heuristic rules that\nare agnostic to the optimal adversarial examples, a strategy that often results\nin attack failures. To this end, this research proposes Fraud's Bargain Attack\n(FBA) which utilizes a novel randomization mechanism to enlarge the search\nspace and enables high-quality adversarial examples to be generated with high\nprobabilities. FBA applies the Metropolis-Hasting sampler, a member of Markov\nChain Monte Carlo samplers, to enhance the selection of adversarial examples\nfrom all candidates proposed by a customized stochastic process that we call\nthe Word Manipulation Process (WMP). WMP perturbs one word at a time via\ninsertion, removal or substitution in a contextual-aware manner. Extensive\nexperiments demonstrate that FBA outperforms the state-of-the-art methods in\nterms of both attack success rate and imperceptibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_M/0/1/0/all/0/1\">Mingze Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhensu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PANACEA: An Automated Misinformation Detection System on COVID-19. (arXiv:2303.01241v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01241","description":"<p>In this demo, we introduce a web-based misinformation detection system\nPANACEA on COVID-19 related claims, which has two modules, fact-checking and\nrumour detection. Our fact-checking module, which is supported by novel natural\nlanguage inference methods with a self-attention network, outperforms\nstate-of-the-art approaches. It is also able to give automated veracity\nassessment and ranked supporting evidence with the stance towards the claim to\nbe checked. In addition, PANACEA adapts the bi-directional graph convolutional\nnetworks model, which is able to detect rumours based on comment networks of\nrelated tweets, instead of relying on the knowledge base. This rumour detection\nmodule assists by warning the users in the early stages when a knowledge base\nmay not be available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Runcong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arana_Catania_M/0/1/0/all/0/1\">Miguel Arana-Catania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lixing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochkina_E/0/1/0/all/0/1\">Elena Kochkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1\">Lin Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1\">Rob Procter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liakata_M/0/1/0/all/0/1\">Maria Liakata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can ChatGPT Assess Human Personalities? A General Evaluation Framework. (arXiv:2303.01248v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01248","description":"<p>Large Language Models (LLMs) especially ChatGPT have produced impressive\nresults in various areas, but their potential human-like psychology is still\nlargely unexplored. Existing works study the virtual personalities of LLMs but\nrarely explore the possibility of analyzing human personalities via LLMs. This\npaper presents a generic evaluation framework for LLMs to assess human\npersonalities based on Myers Briggs Type Indicator (MBTI) tests. Specifically,\nwe first devise unbiased prompts by randomly permuting options in MBTI\nquestions and adopt the average testing result to encourage more impartial\nanswer generation. Then, we propose to replace the subject in question\nstatements to enable flexible queries and assessments on different subjects\nfrom LLMs. Finally, we re-formulate the question instructions in a manner of\ncorrectness evaluation to facilitate LLMs to generate clearer responses. The\nproposed framework enables LLMs to flexibly assess personalities of different\ngroups of people. We further propose three evaluation metrics to measure the\nconsistency, robustness, and fairness of assessment results from\nstate-of-the-art LLMs including ChatGPT and InstructGPT. Our experiments reveal\nChatGPT's ability to assess human personalities, and the average results\ndemonstrate that it can achieve more consistent and fairer assessments in spite\nof lower robustness against prompt biases compared with InstructGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_H/0/1/0/all/0/1\">Haocong Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leung_C/0/1/0/all/0/1\">Cyril Leung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-Universal Adapter Learning with Knowledge Distillation for End-to-End Multilingual Speech Recognition. (arXiv:2303.01249v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01249","description":"<p>In this paper, we propose a language-universal adapter learning framework\nbased on a pre-trained model for end-to-end multilingual automatic speech\nrecognition (ASR). For acoustic modeling, the wav2vec 2.0 pre-trained model is\nfine-tuned by inserting language-specific and language-universal adapters. An\nonline knowledge distillation is then used to enable the language-universal\nadapters to learn both language-specific and universal features. The linguistic\ninformation confusion is also reduced by leveraging language identifiers\n(LIDs). With LIDs we perform a position-wise modification on the multi-head\nattention outputs. In the inference procedure, the language-specific adapters\nare removed while the language-universal adapters are kept activated. The\nproposed method improves the recognition accuracy and addresses the linear\nincrease of the number of adapters' parameters with the number of languages in\ncommon multilingual ASR systems. Experiments on the BABEL dataset confirm the\neffectiveness of the proposed framework. Compared to the conventional\nmultilingual model, a 3.3% absolute error rate reduction is achieved. The code\nis available at: https://github.com/shen9712/UniversalAdapterLearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhijie Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1\">Bin Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-adapted large language models for classifying nuclear medicine reports. (arXiv:2303.01258v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01258","description":"<p>With the growing use of transformer-based language models in medicine, it is\nunclear how well these models generalize to nuclear medicine which has\ndomain-specific vocabulary and unique reporting styles. In this study, we\nevaluated the value of domain adaptation in nuclear medicine by adapting\nlanguage models for the purpose of 5-point Deauville score prediction based on\nclinical 18F-fluorodeoxyglucose (FDG) PET/CT reports. We retrospectively\nretrieved 4542 text reports and 1664 images for FDG PET/CT lymphoma exams from\n2008-2018 in our clinical imaging database. Deauville scores were removed from\nthe reports and then the remaining text in the reports was used as the model\ninput. Multiple general-purpose transformer language models were used to\nclassify the reports into Deauville scores 1-5. We then adapted the models to\nthe nuclear medicine domain using masked language modeling and assessed its\nimpact on classification performance. The language models were compared against\nvision models, a multimodal vision language model, and a nuclear medicine\nphysician with seven-fold Monte Carlo cross validation, reported are the mean\nand standard deviations. Domain adaption improved all language models. For\nexample, BERT improved from 61.3% five-class accuracy to 65.7% following domain\nadaptation. The best performing model (domain-adapted RoBERTa) achieved a\nfive-class accuracy of 77.4%, which was better than the physician's performance\n(66%), the best vision model's performance (48.1), and was similar to the\nmultimodal model's performance (77.2). Domain adaptation improved the\nperformance of large language models in interpreting nuclear medicine text\nreports.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huemann_Z/0/1/0/all/0/1\">Zachary Huemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Changhee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Junjie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Steve Y. Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradshaw_T/0/1/0/all/0/1\">Tyler Bradshaw</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ParrotTTS: Text-to-Speech synthesis by exploiting self-supervised representations. (arXiv:2303.01261v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01261","description":"<p>Text-to-speech (TTS) systems are modelled as mel-synthesizers followed by\nspeech-vocoders since the era of statistical TTS that is carried forward into\nneural designs. We propose an alternative approach to TTS modelling referred to\nas ParrotTTS borrowing from self-supervised learning (SSL) methods. ParrotTTS\ntakes a two-step approach by initially training a speech-to-speech model on\nunlabelled data that is abundantly available, followed by a text-to-embedding\nmodel that leverages speech with aligned transcriptions to extend it to TTS.\nParrotTTS achieves competitive mean opinion scores on naturalness compared to\ntraditional TTS models but significantly improves over the latter's data\nefficiency of transcribed pairs and speaker adaptation without transcriptions.\nThis further paves the path to training TTS models on generically trained SSL\nspeech models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kosgi_S/0/1/0/all/0/1\">Saiteja Kosgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Neil Kumar Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tambrahalli_V/0/1/0/all/0/1\">Vishal Tambrahalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sherin_N/0/1/0/all/0/1\">Neha Sherin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_V/0/1/0/all/0/1\">Vineet Gandhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matching-based Term Semantics Pre-training for Spoken Patient Query Understanding. (arXiv:2303.01341v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01341","description":"<p>Medical Slot Filling (MSF) task aims to convert medical queries into\nstructured information, playing an essential role in diagnosis dialogue\nsystems. However, the lack of sufficient term semantics learning makes existing\napproaches hard to capture semantically identical but colloquial expressions of\nterms in medical conversations. In this work, we formalize MSF into a matching\nproblem and propose a Term Semantics Pre-trained Matching Network (TSPMN) that\ntakes both terms and queries as input to model their semantic interaction. To\nlearn term semantics better, we further design two self-supervised objectives,\nincluding Contrastive Term Discrimination (CTD) and Matching-based Mask Term\nModeling (MMTM). CTD determines whether it is the masked term in the dialogue\nfor each given term, while MMTM directly predicts the masked ones. Experimental\nresults on two Chinese benchmarks show that TSPMN outperforms strong baselines,\nespecially in few-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zefa Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haoran Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Minglun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Z/0/1/0/all/0/1\">Ziyi Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Letz Translate: Low-Resource Machine Translation for Luxembourgish. (arXiv:2303.01347v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01347","description":"<p>Natural language processing of Low-Resource Languages (LRL) is often\nchallenged by the lack of data. Therefore, achieving accurate machine\ntranslation (MT) in a low-resource environment is a real problem that requires\npractical solutions. Research in multilingual models have shown that some LRLs\ncan be handled with such models. However, their large size and computational\nneeds make their use in constrained environments (e.g., mobile/IoT devices or\nlimited/old servers) impractical. In this paper, we address this problem by\nleveraging the power of large multilingual MT models using knowledge\ndistillation. Knowledge distillation can transfer knowledge from a large and\ncomplex teacher model to a simpler and smaller student model without losing\nmuch in performance. We also make use of high-resource languages that are\nrelated or share the same linguistic root as the target LRL. For our\nevaluation, we consider Luxembourgish as the LRL that shares some roots and\nproperties with German. We build multiple resource-efficient models based on\nGerman, knowledge distillation from the multilingual No Language Left Behind\n(NLLB) model, and pseudo-translation. We find that our efficient models are\nmore than 30\\% faster and perform only 4\\% lower compared to the large\nstate-of-the-art NLLB model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yewei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ezzini_S/0/1/0/all/0/1\">Saad Ezzini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_J/0/1/0/all/0/1\">Jacques Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bissyande_T/0/1/0/all/0/1\">Tegawende Bissyande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lefebvre_C/0/1/0/all/0/1\">Cl&#xe9;ment Lefebvre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goujon_A/0/1/0/all/0/1\">Anne Goujon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MLANet: Multi-Level Attention Network with Sub-instruction for Continuous Vision-and-Language Navigation. (arXiv:2303.01396v1 [cs.CV])","link":"http://arxiv.org/abs/2303.01396","description":"<p>Vision-and-Language Navigation (VLN) aims to develop intelligent agents to\nnavigate in unseen environments only through language and vision supervision.\nIn the recently proposed continuous settings (continuous VLN), the agent must\nact in a free 3D space and faces tougher challenges like real-time execution,\ncomplex instruction understanding, and long action sequence prediction. For a\nbetter performance in continuous VLN, we design a multi-level instruction\nunderstanding procedure and propose a novel model, Multi-Level Attention\nNetwork (MLANet). The first step of MLANet is to generate sub-instructions\nefficiently. We design a Fast Sub-instruction Algorithm (FSA) to segment the\nraw instruction into sub-instructions and generate a new sub-instruction\ndataset named ``FSASub\". FSA is annotation-free and faster than the current\nmethod by 70 times, thus fitting the real-time requirement in continuous VLN.\nTo solve the complex instruction understanding problem, MLANet needs a global\nperception of the instruction and observations. We propose a Multi-Level\nAttention (MLA) module to fuse vision, low-level semantics, and high-level\nsemantics, which produce features containing a dynamic and global comprehension\nof the task. MLA also mitigates the adverse effects of noise words, thus\nensuring a robust understanding of the instruction. To correctly predict\nactions in long trajectories, MLANet needs to focus on what sub-instruction is\nbeing executed every step. We propose a Peak Attention Loss (PAL) to improve\nthe flexible and adaptive selection of the current sub-instruction. PAL\nbenefits the navigation agent by concentrating its attention on the local\ninformation, thus helping the agent predict the most appropriate actions. We\ntrain and test MLANet in the standard benchmark. Experiment results show MLANet\noutperforms baselines by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zongtao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liuyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qingqing Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chengju Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qijun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLP Workbench: Efficient and Extensible Integration of State-of-the-art Text Mining Tools. (arXiv:2303.01410v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01410","description":"<p>NLP Workbench is a web-based platform for text mining that allows non-expert\nusers to obtain semantic understanding of large-scale corpora using\nstate-of-the-art text mining models. The platform is built upon latest\npre-trained models and open source systems from academia that provide semantic\nanalysis functionalities, including but not limited to entity linking,\nsentiment analysis, semantic parsing, and relation extraction. Its extensible\ndesign enables researchers and developers to smoothly replace an existing model\nor integrate a new one. To improve efficiency, we employ a microservice\narchitecture that facilitates allocation of acceleration hardware and\nparallelization of computation. This paper presents the architecture of NLP\nWorkbench and discusses the challenges we faced in designing it. We also\ndiscuss diverse use cases of NLP Workbench and the benefits of using it over\nother approaches. The platform is under active development, with its source\ncode released under the MIT license. A website and a short video demonstrating\nour platform are also available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_P/0/1/0/all/0/1\">Peiran Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosmajac_M/0/1/0/all/0/1\">Matej Kosmajac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waheed_A/0/1/0/all/0/1\">Abeer Waheed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzhva_K/0/1/0/all/0/1\">Kostyantyn Guzhva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hervieux_N/0/1/0/all/0/1\">Natalie Hervieux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbosa_D/0/1/0/all/0/1\">Denilson Barbosa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semiparametric Language Models Are Scalable Continual Learners. (arXiv:2303.01421v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01421","description":"<p>Semiparametric language models (LMs) have shown promise in continuously\nlearning from new text data by combining a parameterized neural LM with a\ngrowable non-parametric memory for memorizing new content. However,\nconventional semiparametric LMs will finally become prohibitive for computing\nand storing if they are applied to continual learning over streaming data,\nbecause the non-parametric memory grows linearly with the amount of data they\nlearn from over time. To address the issue of scalability, we present a simple\nand intuitive approach called Selective Memorization (SeMem), which only\nmemorizes difficult samples that the model is likely to struggle with. We\ndemonstrate that SeMem improves the scalability of semiparametric LMs for\ncontinual learning over streaming data in two ways: (1) data-wise scalability:\nas the model becomes stronger through continual learning, it will encounter\nfewer difficult cases that need to be memorized, causing the growth of the\nnon-parametric memory to slow down over time rather than growing at a linear\nrate with the size of training data; (2) model-wise scalability: SeMem allows a\nlarger model to memorize fewer samples than its smaller counterpart because it\nis rarer for a larger model to encounter incomprehensible cases, resulting in a\nnon-parametric memory that does not scale linearly with model size. We conduct\nextensive experiments in language modeling and downstream tasks to test SeMem's\nresults, showing SeMem enables a semiparametric LM to be a scalable continual\nlearner with little forgetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_G/0/1/0/all/0/1\">Guangyue Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si-Qing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Houfeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WiCE: Real-World Entailment for Claims in Wikipedia. (arXiv:2303.01432v1 [cs.CL])","link":"http://arxiv.org/abs/2303.01432","description":"<p>Models for textual entailment have increasingly been applied to settings like\nfact-checking, presupposition verification in question answering, and\nvalidating that generation models' outputs are faithful to a source. However,\nsuch applications are quite far from the settings that existing datasets are\nconstructed in. We propose WiCE, a new textual entailment dataset centered\naround verifying claims in text, built on real-world claims and evidence in\nWikipedia with fine-grained annotations. We collect sentences in Wikipedia that\ncite one or more webpages and annotate whether the content on those pages\nentails those sentences. Negative examples arise naturally, from slight\nmisinterpretation of text to minor aspects of the sentence that are not\nattested in the evidence. Our annotations are over sub-sentence units of the\nhypothesis, decomposed automatically by GPT-3, each of which is labeled with a\nsubset of evidence sentences from the source document. We show that real claims\nin our dataset involve challenging verification problems, and we benchmark\nexisting approaches on this dataset. In addition, we show that reducing the\ncomplexity of claims by decomposing them by GPT-3 can improve entailment\nmodels' performance on various domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamoi_R/0/1/0/all/0/1\">Ryo Kamoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_T/0/1/0/all/0/1\">Tanya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_J/0/1/0/all/0/1\">Juan Diego Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Small-Text: Active Learning for Text Classification in Python. (arXiv:2107.10314v6 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.10314","description":"<p>We introduce small-text, an easy-to-use active learning library, which offers\npool-based active learning for single- and multi-label text classification in\nPython. It features numerous pre-implemented state-of-the-art query strategies,\nincluding some that leverage the GPU. Standardized interfaces allow the\ncombination of a variety of classifiers, query strategies, and stopping\ncriteria, facilitating a quick mix and match, and enabling a rapid and\nconvenient development of both active learning experiments and applications.\nWith the objective of making various classifiers and query strategies\naccessible for active learning, small-text integrates several well-known\nmachine learning libraries, namely scikit-learn, PyTorch, and Hugging Face\ntransformers. The latter integrations are optionally installable extensions, so\nGPUs can be used but are not required. Using this new library, we investigate\nthe performance of the recently published SetFit training paradigm, which we\ncompare to vanilla transformer fine-tuning, finding that it matches the latter\nin classification accuracy while outperforming it in area under the curve. The\nlibrary is available under the MIT License at\nhttps://github.com/webis-de/small-text, in version 1.3.0 at the time of\nwriting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schroder_C/0/1/0/all/0/1\">Christopher Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1\">Lydia M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekler_A/0/1/0/all/0/1\">Andreas Niekler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Realised Volatility Forecasting: Machine Learning via Financial Word Embedding. (arXiv:2108.00480v2 [q-fin.CP] UPDATED)","link":"http://arxiv.org/abs/2108.00480","description":"<p>This study develops FinText, a financial word embedding compiled from 15\nyears of business news archives. The results show that FinText produces\nsubstantially more accurate results than general word embeddings based on the\ngold-standard financial benchmark we introduced. In contrast to well-known\neconometric models, and over the sample period from 27 July 2007 to 27 January\n2022 for 23 NASDAQ stocks, using stock-related news, our simple natural\nlanguage processing model supported by different word embeddings improves\nrealised volatility forecasts on high volatility days. This improvement in\nrealised volatility forecasting performance switches to normal volatility days\nwhen general hot news is used. By utilising SHAP, an Explainable AI method, we\nalso identify and classify key phrases in stock-related and general hot news\nthat moved volatility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Rahimikia_E/0/1/0/all/0/1\">Eghbal Rahimikia</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zohren_S/0/1/0/all/0/1\">Stefan Zohren</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Poon_S/0/1/0/all/0/1\">Ser-Huang Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Poisson Stochastic Beam Search. (arXiv:2109.11034v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.11034","description":"<p>Beam search is the default decoding strategy for many sequence generation\ntasks in NLP. The set of approximate K-best items returned by the algorithm is\na useful summary of the distribution for many applications; however, the\ncandidates typically exhibit high overlap and may give a highly biased estimate\nfor expectations under our model. These problems can be addressed by instead\nusing stochastic decoding strategies. In this work, we propose a new method for\nturning beam search into a stochastic process: Conditional Poisson stochastic\nbeam search. Rather than taking the maximizing set at each iteration, we sample\nK candidates without replacement according to the conditional Poisson sampling\ndesign. We view this as a more natural alternative to Kool et. al. 2019's\nstochastic beam search (SBS). Furthermore, we show how samples generated under\nthe CPSBS design can be used to build consistent estimators and sample diverse\nsets from sequence models. In our experiments, we observe CPSBS produces lower\nvariance and more efficient estimators than SBS, even showing improvements in\nhigh entropy settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Afra Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vieira_T/0/1/0/all/0/1\">Tim Vieira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training. (arXiv:2205.12502v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12502","description":"<p>Visual dialog (VisDial) is a task of answering a sequence of questions\ngrounded in an image, using the dialog history as context. Prior work has\ntrained the dialog agents solely on VisDial data via supervised learning or\nleveraged pre-training on related vision-and-language datasets. This paper\npresents a semi-supervised learning approach for visually-grounded dialog,\ncalled Generative Self-Training (GST), to leverage unlabeled images on the Web.\nSpecifically, GST first retrieves in-domain images through out-of-distribution\ndetection and generates synthetic dialogs regarding the images via multimodal\nconditional text generation. GST then trains a dialog agent on the synthetic\nand the original VisDial data. As a result, GST scales the amount of training\ndata up to an order of magnitude that of VisDial (1.2M to 12.9M QA data). For\nrobust training of the synthetic dialogs, we also propose perplexity-based data\nselection and multimodal consistency regularization. Evaluation on VisDial v1.0\nand v0.9 datasets shows that GST achieves new state-of-the-art results on both\ndatasets. We further observe the robustness of GST against both visual and\ntextual adversarial attacks. Finally, GST yields strong performance gains in\nthe low-data regime. Code is available at\nhttps://github.com/gicheonkang/gst-visdial.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1\">Gi-Cheon Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungdong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jin-Hwa Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_D/0/1/0/all/0/1\">Donghyun Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Byoung-Tak Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TranSpeech: Speech-to-Speech Translation With Bilateral Perturbation. (arXiv:2205.12523v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12523","description":"<p>Direct speech-to-speech translation (S2ST) with discrete units leverages\nrecent progress in speech representation learning. Specifically, a sequence of\ndiscrete representations derived in a self-supervised manner are predicted from\nthe model and passed to a vocoder for speech reconstruction, while still facing\nthe following challenges: 1) Acoustic multimodality: the discrete units derived\nfrom speech with same content could be indeterministic due to the acoustic\nproperty (e.g., rhythm, pitch, and energy), which causes deterioration of\ntranslation accuracy; 2) high latency: current S2ST systems utilize\nautoregressive models which predict each unit conditioned on the sequence\npreviously generated, failing to take full advantage of parallelism. In this\nwork, we propose TranSpeech, a speech-to-speech translation model with\nbilateral perturbation. To alleviate the acoustic multimodal problem, we\npropose bilateral perturbation (BiP), which consists of the style normalization\nand information enhancement stages, to learn only the linguistic information\nfrom speech samples and generate more deterministic representations. With\nreduced multimodality, we step forward and become the first to establish a\nnon-autoregressive S2ST technique, which repeatedly masks and predicts unit\nchoices and produces high-accuracy results in just a few cycles. Experimental\nresults on three language pairs demonstrate that BiP yields an improvement of\n2.9 BLEU on average compared with a baseline textless S2ST model. Moreover, our\nparallel decoding shows a significant reduction of inference latency, enabling\nspeedup up to 21.4x than autoregressive technique. Audio samples are available\nat \\url{https://TranSpeech.github.io/}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rongjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huadai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lichao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jinzheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Zipf's Law-Driven Method for Extracting Entities from Documents. (arXiv:2205.12636v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12636","description":"<p>Entity extraction is critical to the intelligent development of various\ndomains and the construction of knowledge agents. Yet, there is category\nimbalance problem in documents in some specific domains that some categories of\nentities are common, while some are rare and scattered. This paper proposes to\nuse Zipf's law to tackle this problem and to promote the performance of entity\nextraction from documents. Using two forms of Zipf's law, words in the\ndocuments are classified into common and rare ones, and then sentences are\nclassified into common and rare ones, and are further processed by text\ngeneration models respectively. Rare entities in the generated sentences are\nlabeled with human-designed rules, and serve as a supplement to the raw dataset\nso as to alleviate the category imbalance problem. A case of extracting\nentities from technical documents on industrial safety is given and the\nexperiments results on two datasets show the effectiveness of the proposed\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Ming Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Dong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factuality Enhanced Language Models for Open-Ended Text Generation. (arXiv:2206.04624v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.04624","description":"<p>Pretrained language models (LMs) are susceptible to generate text with\nnonfactual information. In this work, we measure and improve the factual\naccuracy of large-scale LMs for open-ended text generation. We design the\nFactualityPrompts test set and metrics to measure the factuality of LM\ngenerations. Based on that, we study the factual accuracy of LMs with parameter\nsizes ranging from 126M to 530B. Interestingly, we find that larger LMs are\nmore factual than smaller ones, although a previous study suggests that larger\nLMs can be less truthful in terms of misconceptions. In addition, popular\nsampling algorithms (e.g., top-p) in open-ended text generation can harm the\nfactuality due to the ''uniform randomness'' introduced at every sampling step.\nWe propose the factual-nucleus sampling algorithm that dynamically adapts the\nrandomness to improve the factuality of generation while maintaining quality.\nFurthermore, we analyze the inefficiencies of the standard training method in\nlearning correct associations between entities from factual text corpus (e.g.,\nWikipedia). We propose a factuality-enhanced training method that uses\nTopicPrefix for better awareness of facts and sentence completion as the\ntraining objective, which can vastly reduce the factual errors. We release our\ncode and FactualityPrompts benchmark at:\nhttps://github.com/nayeon7lee/FactualityPrompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwary_M/0/1/0/all/0/1\">Mostofa Patwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TextWorldExpress: Simulating Text Games at One Million Steps Per Second. (arXiv:2208.01174v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.01174","description":"<p>Text-based games offer a challenging test bed to evaluate virtual agents at\nlanguage understanding, multi-step problem-solving, and common-sense reasoning.\nHowever, speed is a major limitation of current text-based games, capping at\n300 steps per second, mainly due to the use of legacy tooling. In this work we\npresent TextWorldExpress, a high-performance simulator that includes\nimplementations of three common text game benchmarks that increases simulation\nthroughput by approximately three orders of magnitude, reaching over one\nmillion steps per second on common desktop hardware. This significantly reduces\nexperiment runtime, enabling billion-step-scale experiments in about one day.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jansen_P/0/1/0/all/0/1\">Peter A. Jansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1\">Marc-Alexandre C&#xf4;t&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ferret: a Framework for Benchmarking Explainers on Transformers. (arXiv:2208.01575v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.01575","description":"<p>As Transformers are increasingly relied upon to solve complex NLP problems,\nthere is an increased need for their decisions to be humanly interpretable.\nWhile several explainable AI (XAI) techniques for interpreting the outputs of\ntransformer-based models have been proposed, there is still a lack of easy\naccess to using and comparing them. We introduce ferret, a Python library to\nsimplify the use and comparisons of XAI methods on transformer-based\nclassifiers. With ferret, users can visualize and compare transformers-based\nmodels output explanations using state-of-the-art XAI methods on any free-text\nor existing XAI corpora. Moreover, users can also evaluate ad-hoc XAI metrics\nto select the most faithful and plausible explanations. To align with the\nrecently consolidated process of sharing and using transformers-based models\nfrom Hugging Face, ferret interfaces directly with its Python library. In this\npaper, we showcase ferret to benchmark XAI methods used on transformers for\nsentiment analysis and hate speech detection. We show how specific methods\nprovide consistently better explanations and are preferable in the context of\ntransformer models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Attanasio_G/0/1/0/all/0/1\">Giuseppe Attanasio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pastor_E/0/1/0/all/0/1\">Eliana Pastor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonaventura_C/0/1/0/all/0/1\">Chiara Di Bonaventura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nozza_D/0/1/0/all/0/1\">Debora Nozza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Goodness of Pronunciation Pipelines for OOV Problem. (arXiv:2209.03787v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.03787","description":"<p>In the following report we propose pipelines for Goodness of Pronunciation\n(GoP) computation solving OOV problem at testing time using Vocab/Lexicon\nexpansion techniques. The pipeline uses different components of ASR system to\nquantify accent and automatically evaluate them as scores. We use the\nposteriors of an ASR model trained on native English speech, along with the\nphone level boundaries to obtain phone level pronunciation scores. We used this\nas a baseline pipeline and implemented methods to remove UNK and SPN phonemes\nin the GoP output by building three pipelines. The Online, Offline and Hybrid\npipeline which returns the scores but also can prevent unknown words in the\nfinal output. The Online method is based per utterance, Offline method\npre-incorporates a set of OOV words for a given data set and the Hybrid method\ncombines the above two ideas to expand the lexicon as well work per utterance.\nWe further provide utilities such as the Phoneme to posterior mappings, GoP\nscores of each utterance as a vector, and Word boundaries used in the GoP\npipeline for use in future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1\">Ankit Grover</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YATO: Yet Another deep learning based Text analysis Open toolkit. (arXiv:2209.13877v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.13877","description":"<p>We introduce YATO, an open-source toolkit for text analysis with deep\nlearning. It focuses on fundamental sequence labeling and sequence\nclassification tasks on text. Designed in a hierarchical structure, YATO\nsupports free combinations of three types of features including 1) traditional\nneural networks (CNN, RNN, etc.); 2) pre-trained language models (BERT,\nRoBERTa, ELECTRA, etc.); and 3) user-customed neural features via a simple\nconfigurable file. Benefiting from the advantages of flexibility and ease of\nuse, YATO can facilitate reproducing and refinement of state-of-the-art NLP\nmodels, and promote the cross-disciplinary applications of NLP techniques.\nSource code, examples, and documentation are publicly available at\nhttps://github.com/jiesutd/YATO. A demo video is also available at\nhttps://youtu.be/tSjjf5BzfQg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zeqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yile Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiageng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1\">Zhiyang Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning. (arXiv:2209.14610v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2209.14610","description":"<p>Mathematical reasoning, a core ability of human intelligence, presents unique\nchallenges for machines in abstract thinking and logical reasoning. Recent\nlarge pre-trained language models such as GPT-3 have achieved remarkable\nprogress on mathematical reasoning tasks written in text form, such as math\nword problems (MWP). However, it is unknown if the models can handle more\ncomplex problems that involve math reasoning over heterogeneous information,\nsuch as tabular data. To fill the gap, we present Tabular Math Word Problems\n(TabMWP), a new dataset containing 38,431 open-domain grade-level problems that\nrequire mathematical reasoning on both textual and tabular data. Each question\nin TabMWP is aligned with a tabular context, which is presented as an image,\nsemi-structured text, and a structured table. There are two types of questions:\nfree-text and multi-choice, and each problem is annotated with gold solutions\nto reveal the multi-step reasoning process. We evaluate different pre-trained\nmodels on TabMWP, including the GPT-3 model in a few-shot setting. As earlier\nstudies suggest, since few-shot GPT-3 relies on the selection of in-context\nexamples, its performance is unstable and can degrade to near chance. The\nunstable issue is more severe when handling complex problems like TabMWP. To\nmitigate this, we further propose a novel approach, PromptPG, which utilizes\npolicy gradient to learn to select in-context examples from a small amount of\ntraining data and then constructs the corresponding prompt for the test\nexample. Experimental results show that our method outperforms the best\nbaseline by 5.31% on the accuracy metric and reduces the prediction variance\nsignificantly compared to random selection, which verifies its effectiveness in\nselecting in-context examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurohit_T/0/1/0/all/0/1\">Tanmay Rajpurohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1\">Ashwin Kalyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought. (arXiv:2210.01240v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.01240","description":"<p>Large language models (LLMs) have shown remarkable reasoning capabilities\ngiven chain-of-thought prompts (examples with intermediate reasoning steps).\nExisting benchmarks measure reasoning ability indirectly, by evaluating\naccuracy on downstream tasks such as mathematical reasoning. However, it is\nunclear how these models obtain the answers and whether they rely on simple\nheuristics rather than the generated chain-of-thought. To enable systematic\nexploration of the reasoning ability of LLMs, we present a new synthetic\nquestion-answering dataset called PrOntoQA, where each example is generated\nfrom a synthetic world model represented in first-order logic. This allows us\nto parse the generated chain-of-thought into symbolic proofs for formal\nanalysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite\ncapable of making correct individual deduction steps, and so are generally\ncapable of reasoning, even in fictional contexts. However, they have difficulty\nwith proof planning: When multiple valid deduction steps are available, they\nare not able to systematically explore the different options.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saparov_A/0/1/0/all/0/1\">Abulhair Saparov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Viterbi Decoding of Directed Acyclic Transformer for Non-Autoregressive Machine Translation. (arXiv:2210.05193v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05193","description":"<p>Non-autoregressive models achieve significant decoding speedup in neural\nmachine translation but lack the ability to capture sequential dependency.\nDirected Acyclic Transformer (DA-Transformer) was recently proposed to model\nsequential dependency with a directed acyclic graph. Consequently, it has to\napply a sequential decision process at inference time, which harms the global\ntranslation accuracy. In this paper, we present a Viterbi decoding framework\nfor DA-Transformer, which guarantees to find the joint optimal solution for the\ntranslation and decoding path under any length constraint. Experimental results\ndemonstrate that our approach consistently improves the performance of\nDA-Transformer while maintaining a similar decoding speedup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1\">Chenze Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhengrui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Locate Visual Answer in Video Corpus Using Question. (arXiv:2210.05423v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.05423","description":"<p>We introduce a new task, named video corpus visual answer localization\n(VCVAL), which aims to locate the visual answer in a large collection of\nuntrimmed instructional videos using a natural language question. This task\nrequires a range of skills - the interaction between vision and language, video\nretrieval, passage comprehension, and visual answer localization. In this\npaper, we propose a cross-modal contrastive global-span (CCGS) method for the\nVCVAL, jointly training the video corpus retrieval and visual answer\nlocalization subtasks with the global-span matrix. We have reconstructed a\ndataset named MedVidCQA, on which the VCVAL task is benchmarked. Experimental\nresults show that the proposed method outperforms other competitive methods\nboth in the video corpus retrieval and visual answer localization subtasks.\nMost importantly, we perform detailed analyses on extensive experiments, paving\na new path for understanding the instructional videos, which ushers in further\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Internal Language Model Estimation based Adaptive Language Model Fusion for Domain Adaptation. (arXiv:2211.00968v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.00968","description":"<p>ASR model deployment environment is ever-changing, and the incoming speech\ncan be switched across different domains during a session. This brings a\nchallenge for effective domain adaptation when only target domain text data is\navailable, and our objective is to obtain obviously improved performance on the\ntarget domain while the performance on the general domain is less undermined.\nIn this paper, we propose an adaptive LM fusion approach called internal\nlanguage model estimation based adaptive domain adaptation (ILME-ADA). To\nrealize such an ILME-ADA, an interpolated log-likelihood score is calculated\nbased on the maximum of the scores from the internal LM and the external LM\n(ELM) respectively. We demonstrate the efficacy of the proposed ILME-ADA method\nwith both RNN-T and LAS modeling frameworks employing neural network and n-gram\nLMs as ELMs respectively on two domain specific (target) test sets. The\nproposed method can achieve significantly better performance on the target test\nsets while it gets minimal performance degradation on the general test set,\ncompared with both shallow and ILME-based LM fusion methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaobo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jin Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yanan Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haihua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zejun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Pre-trained Language Models for Antibody. (arXiv:2301.12112v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.12112","description":"<p>Antibodies are vital proteins offering robust protection for the human body\nfrom pathogens. The development of general protein and antibody-specific\npre-trained language models both facilitate antibody prediction tasks. However,\nthere have been limited studies that comprehensively explore the representation\ncapability of distinct pre-trained language models on different antibody tasks.\nTo investigate the problem, we aim to answer several key questions in this\npaper, such as how pre-trained language models perform in antibody tasks with\ndifferent specificity and how introducing specific biological mechanisms to the\npre-training process can benefit the model. Additionally, we evaluate if the\nlearned antibody pre-trained representations can be applied to real-world\nantibody problems, like drug discovery and immune process understanding.\nPreviously, no benchmark available largely hindered the study to answer these\nquestions. To aid in our investigation, we provide an AnTibody Understanding\nEvaluation (ATUE) benchmark. We comprehensively evaluate the performance of\nprotein pre-trained language models by empirical study along with conclusions\nand new insights. Our ATUE and code are released at\nhttps://github.com/dqwang122/EATLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Danqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Like a Good Nearest Neighbor: Practical Content Moderation with Sentence Transformers. (arXiv:2302.08957v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.08957","description":"<p>Modern text classification systems have impressive capabilities but are\ninfeasible to deploy and use reliably due to their dependence on prompting and\nbillion-parameter language models. SetFit (Tunstall et al., 2022) is a recent,\npractical approach that fine-tunes a Sentence Transformer under a contrastive\nlearning paradigm and achieves similar results to more unwieldy systems. Text\nclassification is important for addressing the problem of domain drift in\ndetecting harmful content, which plagues all social media platforms. Here, we\npropose Like a Good Nearest Neighbor (LaGoNN), an inexpensive modification to\nSetFit that requires no additional parameters or hyperparameters but modifies\ninput with information about its nearest neighbor, for example, the label and\ntext, in the training data, making novel data appear similar to an instance on\nwhich the model was optimized. LaGoNN is effective at the task of detecting\nharmful content and generally improves performance compared to SetFit. To\ndemonstrate the value of our system, we conduct a thorough study of text\nclassification systems in the context of content moderation under four label\ndistributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bates_L/0/1/0/all/0/1\">Luke Bates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT. (arXiv:2302.10198v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.10198","description":"<p>Recently, ChatGPT has attracted great attention, as it can generate fluent\nand high-quality responses to human inquiries. Several prior studies have shown\nthat ChatGPT attains remarkable generation ability compared with existing\nmodels. However, the quantitative analysis of ChatGPT's understanding ability\nhas been given little attention. In this report, we explore the understanding\nability of ChatGPT by evaluating it on the most popular GLUE benchmark, and\ncomparing it with 4 representative fine-tuned BERT-style models. We find that:\n1) ChatGPT falls short in handling paraphrase and similarity tasks; 2) ChatGPT\noutperforms all BERT models on inference tasks by a large margin; 3) ChatGPT\nachieves comparable performance compared with BERT on sentiment analysis and\nquestion-answering tasks. Additionally, by combining some advanced prompting\nstrategies, we show that the understanding ability of ChatGPT can be further\nimproved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1\">Qihuang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Juhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective. (arXiv:2302.12095v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2302.12095","description":"<p>ChatGPT is a recent chatbot service released by OpenAI and is receiving\nincreasing attention over the past few months. While evaluations of various\naspects of ChatGPT have been done, its robustness, i.e., the performance to\nunexpected inputs, is still unclear to the public. Robustness is of particular\nconcern in responsible AI, especially for safety-critical applications. In this\npaper, we conduct a thorough evaluation of the robustness of ChatGPT from the\nadversarial and out-of-distribution (OOD) perspective. To do so, we employ the\nAdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart\nreview and DDXPlus medical diagnosis datasets for OOD evaluation. We select\nseveral popular foundation models as baselines. Results show that ChatGPT shows\nconsistent advantages on most adversarial and OOD classification and\ntranslation tasks. However, the absolute performance is far from perfection,\nwhich suggests that adversarial and OOD robustness remains a significant threat\nto foundation models. Moreover, ChatGPT shows astounding performance in\nunderstanding dialogue-related texts and we find that it tends to provide\ninformal suggestions for medical tasks instead of definitive answers. Finally,\nwe present in-depth discussions of possible research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xixu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1\">Wenxin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Runkai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haojun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_B/0/1/0/all/0/1\">Binxin Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KHAN: Knowledge-Aware Hierarchical Attention Networks for Accurate Political Stance Prediction. (arXiv:2302.12126v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12126","description":"<p>The political stance prediction for news articles has been widely studied to\nmitigate the echo chamber effect -- people fall into their thoughts and\nreinforce their pre-existing beliefs. The previous works for the political\nstance problem focus on (1) identifying political factors that could reflect\nthe political stance of a news article and (2) capturing those factors\neffectively. Despite their empirical successes, they are not sufficiently\njustified in terms of how effective their identified factors are in the\npolitical stance prediction. Motivated by this, in this work, we conduct a user\nstudy to investigate important factors in political stance prediction, and\nobserve that the context and tone of a news article (implicit) and external\nknowledge for real-world entities appearing in the article (explicit) are\nimportant in determining its political stance. Based on this observation, we\npropose a novel knowledge-aware approach to political stance prediction (KHAN),\nemploying (1) hierarchical attention networks (HAN) to learn the relationships\namong words and sentences in three different levels and (2) knowledge encoding\n(KE) to incorporate external knowledge for real-world entities into the process\nof political stance prediction. Also, to take into account the subtle and\nimportant difference between opposite political stances, we build two\nindependent political knowledge graphs (KG) (i.e., KG-lib and KG-con) by\nourselves and learn to fuse the different political knowledge. Through\nextensive evaluations on three real-world datasets, we demonstrate the\nsuperiority of DASH in terms of (1) accuracy, (2) efficiency, and (3)\neffectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ko_Y/0/1/0/all/0/1\">Yunyong Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_S/0/1/0/all/0/1\">Seongeun Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soeun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_Y/0/1/0/all/0/1\">Yeongseung Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sohyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kyungsik Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1\">Hanghang Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sang-Wook Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Let's have a chat! A Conversation with ChatGPT: Technology, Applications, and Limitations. (arXiv:2302.13817v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.13817","description":"<p>The emergence of an AI-powered chatbot that can generate human-like sentences\nand write coherent essays has caught the world's attention. This paper\ndiscusses the historical overview of chatbots and the technology behind Chat\nGenerative Pre-trained Transformer, better known as ChatGPT. Moreover,\npotential applications of ChatGPT in various domains, including healthcare,\neducation, and research, are highlighted. Despite promising results, there are\nseveral privacy and ethical concerns surrounding ChatGPT. In addition, we\nhighlight some of the important limitations of the current version of ChatGPT.\nWe also ask ChatGPT to provide its point of view and present its responses to\nseveral questions we attempt to answer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1\">Sakib Shahriar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayawi_K/0/1/0/all/0/1\">Kadhim Hayawi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-03-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}