{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-06-29T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Implementing contextual biasing in GPU decoder for online ASR. (arXiv:2306.15685v1 [eess.AS])","link":"http://arxiv.org/abs/2306.15685","description":"<p>GPU decoding significantly accelerates the output of ASR predictions. While\nGPUs are already being used for online ASR decoding, post-processing and\nrescoring on GPUs have not been properly investigated yet. Rescoring with\navailable contextual information can considerably improve ASR predictions.\nPrevious studies have proven the viability of lattice rescoring in decoding and\nbiasing language model (LM) weights in offline and online CPU scenarios. In\nreal-time GPU decoding, partial recognition hypotheses are produced without\nlattice generation, which makes the implementation of biasing more complex. The\npaper proposes and describes an approach to integrate contextual biasing in\nreal-time GPU decoding while exploiting the standard Kaldi GPU decoder. Besides\nthe biasing of partial ASR predictions, our approach also permits dynamic\ncontext switching allowing a flexible rescoring per each speech segment\ndirectly on GPU. The code is publicly released and tested with open-sourced\ntest sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nigmatulina_I/0/1/0/all/0/1\">Iuliia Nigmatulina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Madikeri_S/0/1/0/all/0/1\">Srikanth Madikeri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Villatoro_Tello_E/0/1/0/all/0/1\">Esa&#xfa; Villatoro-Tello</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motli&#x10d;ek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pandia_K/0/1/0/all/0/1\">Karthik Pandia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ganapathiraju_A/0/1/0/all/0/1\">Aravind Ganapathiraju</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Master-ASR: Achieving Multilingual Scalability and Low-Resource Adaptation in ASR with Modular Learning. (arXiv:2306.15686v1 [eess.AS])","link":"http://arxiv.org/abs/2306.15686","description":"<p>Despite the impressive performance recently achieved by automatic speech\nrecognition (ASR), we observe two primary challenges that hinder its broader\napplications: (1) The difficulty of introducing scalability into the model to\nsupport more languages with limited training, inference, and storage overhead;\n(2) The low-resource adaptation ability that enables effective low-resource\nadaptation while avoiding over-fitting and catastrophic forgetting issues.\nInspired by recent findings, we hypothesize that we can address the above\nchallenges with modules widely shared across languages. To this end, we propose\nan ASR framework, dubbed \\METHODNS, that, \\textit{for the first time},\nsimultaneously achieves strong multilingual scalability and low-resource\nadaptation ability thanks to its modularize-then-assemble strategy.\nSpecifically, \\METHOD learns a small set of generalizable sub-modules and\nadaptively assembles them for different languages to reduce the multilingual\noverhead and enable effective knowledge transfer for low-resource adaptation.\nExtensive experiments and visualizations demonstrate that \\METHOD can\neffectively discover language similarity and improve multilingual and\nlow-resource ASR performance over state-of-the-art (SOTA) methods, e.g., under\nmultilingual-ASR, our framework achieves a 0.13$\\sim$2.41 lower character error\nrate (CER) with 30\\% smaller inference overhead over SOTA solutions on\nmultilingual ASR and a comparable CER, with nearly 50 times fewer trainable\nparameters over SOTA solutions on low-resource tuning, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yu_Z/0/1/0/all/0/1\">Zhongzhi Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_K/0/1/0/all/0/1\">Kaizhi Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1\">Yonggan Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1\">Yingyan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale. (arXiv:2306.15687v1 [eess.AS])","link":"http://arxiv.org/abs/2306.15687","description":"<p>Large-scale generative models such as GPT and DALL-E have revolutionized\nnatural language processing and computer vision research. These models not only\ngenerate high fidelity text or image outputs, but are also generalists which\ncan solve tasks not explicitly taught. In contrast, speech generative models\nare still primitive in terms of scale and task generalization. In this paper,\nwe present Voicebox, the most versatile text-guided generative model for speech\nat scale. Voicebox is a non-autoregressive flow-matching model trained to\ninfill speech, given audio context and text, trained on over 50K hours of\nspeech that are neither filtered nor enhanced. Similar to GPT, Voicebox can\nperform many different tasks through in-context learning, but is more flexible\nas it can also condition on future context. Voicebox can be used for mono or\ncross-lingual zero-shot text-to-speech synthesis, noise removal, content\nediting, style conversion, and diverse sample generation. In particular,\nVoicebox outperforms the state-of-the-art zero-shot TTS model VALL-E on both\nintelligibility (5.9% vs 1.9% word error rates) and audio similarity (0.580 vs\n0.681) while being up to 20 times faster. See voicebox.metademolab.com for a\ndemo of the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Le_M/0/1/0/all/0/1\">Matthew Le</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vyas_A/0/1/0/all/0/1\">Apoorv Vyas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_B/0/1/0/all/0/1\">Bowen Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karrer_B/0/1/0/all/0/1\">Brian Karrer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sari_L/0/1/0/all/0/1\">Leda Sari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moritz_R/0/1/0/all/0/1\">Rashel Moritz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Williamson_M/0/1/0/all/0/1\">Mary Williamson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Manohar_V/0/1/0/all/0/1\">Vimal Manohar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahadeokar_J/0/1/0/all/0/1\">Jay Mahadeokar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Universal Adversarial Perturbations for Efficient Data-free Adversarial Detection. (arXiv:2306.15705v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15705","description":"<p>Detecting adversarial samples that are carefully crafted to fool the model is\na critical step to socially-secure applications. However, existing adversarial\ndetection methods require access to sufficient training data, which brings\nnoteworthy concerns regarding privacy leakage and generalizability. In this\nwork, we validate that the adversarial sample generated by attack algorithms is\nstrongly related to a specific vector in the high-dimensional inputs. Such\nvectors, namely UAPs (Universal Adversarial Perturbations), can be calculated\nwithout original training data. Based on this discovery, we propose a\ndata-agnostic adversarial detection framework, which induces different\nresponses between normal and adversarial samples to UAPs. Experimental results\nshow that our method achieves competitive detection performance on various text\nclassification tasks, and maintains an equivalent time consumption to normal\ninference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Songyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1\">Shihan Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Weakly Supervised Classifier and Dataset of White Supremacist Language. (arXiv:2306.15732v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15732","description":"<p>We present a dataset and classifier for detecting the language of white\nsupremacist extremism, a growing issue in online hate speech. Our weakly\nsupervised classifier is trained on large datasets of text from explicitly\nwhite supremacist domains paired with neutral and anti-racist data from similar\ndomains. We demonstrate that this approach improves generalization performance\nto new domains. Incorporating anti-racist texts as counterexamples to white\nsupremacist language mitigates bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoder_M/0/1/0/all/0/1\">Michael Miller Yoder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diab_A/0/1/0/all/0/1\">Ahmad Diab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1\">David West Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carley_K/0/1/0/all/0/1\">Kathleen M. Carley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biomedical Entity Recognition by Detection and Matching. (arXiv:2306.15736v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15736","description":"<p>Biomedical named entity recognition (BNER) serves as the foundation for\nnumerous biomedical text mining tasks. Unlike general NER, BNER require a\ncomprehensive grasp of the domain, and incorporating external knowledge beyond\ntraining data poses a significant challenge. In this study, we propose a novel\nBNER framework called DMNER. By leveraging existing entity representation\nmodels SAPBERT, we tackle BNER as a two-step process: entity boundary detection\nand biomedical entity matching. DMNER exhibits applicability across multiple\nNER scenarios: 1) In supervised NER, we observe that DMNER effectively\nrectifies the output of baseline NER models, thereby further enhancing\nperformance. 2) In distantly supervised NER, combining MRC and AutoNER as span\nboundary detectors enables DMNER to achieve satisfactory results. 3) For\ntraining NER by merging multiple datasets, we adopt a framework similar to\nDS-NER but additionally leverage ChatGPT to obtain high-quality phrases in the\ntraining. Through extensive experiments conducted on 10 benchmark datasets, we\ndemonstrate the versatility and effectiveness of DMNER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Junyi Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Rongze Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1\">Weiqi Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tianyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shanfeng Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identity Construction in a Misogynist Incels Forum. (arXiv:2306.15745v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15745","description":"<p>Online communities of involuntary celibates (incels) are a prominent source\nof misogynist hate speech. In this paper, we use quantitative text and network\nanalysis approaches to examine how identity groups are discussed on incels.is,\nthe largest black-pilled incels forum. We find that this community produces a\nwide range of novel identity terms and, while terms for women are most common,\nmentions of other minoritized identities are increasing. An analysis of the\nassociations made with identity groups suggests an essentialist ideology where\nphysical appearance, as well as gender and racial hierarchies, determine human\nvalue. We discuss implications for research into automated misogynist hate\nspeech detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoder_M/0/1/0/all/0/1\">Michael Miller Yoder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perry_C/0/1/0/all/0/1\">Chloe Perry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1\">David West Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carley_K/0/1/0/all/0/1\">Kathleen M. Carley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruden_M/0/1/0/all/0/1\">Meredith Pruden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost. (arXiv:2306.15766v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15766","description":"<p>State-of-the-art supervised NLP models achieve high accuracy but are also\nsusceptible to failures on inputs from low-data regimes, such as domains that\nare not represented in training data. As an approximation to collecting\nground-truth labels for the specific domain, we study the use of large language\nmodels (LLMs) for annotating inputs and improving the generalization of NLP\nmodels. Specifically, given a budget for LLM annotations, we present an\nalgorithm for sampling the most informative inputs to annotate and retrain the\nNLP model. We find that popular active learning strategies such as\nuncertainty-based sampling do not work well. Instead, we propose a sampling\nstrategy based on the difference in prediction scores between the base model\nand the finetuned NLP model, utilizing the fact that most NLP models are\nfinetuned from a base model. Experiments with classification (semantic\nsimilarity) and ranking (semantic search) tasks show that our sampling strategy\nleads to significant gains in accuracy for both the training and target\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bansal_P/0/1/0/all/0/1\">Parikshit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Amit Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Next Steps for Human-Centered Generative AI: A Technical Perspective. (arXiv:2306.15774v1 [cs.HC])","link":"http://arxiv.org/abs/2306.15774","description":"<p>Through iterative, cross-disciplinary discussions, we define and propose\nnext-steps for Human-centered Generative AI (HGAI) from a technical\nperspective. We contribute a roadmap that lays out future directions of\nGenerative AI spanning three levels: Aligning with human values; Accommodating\nhumans' expression of intents; and Augmenting humans' abilities in a\ncollaborative workflow. This roadmap intends to draw interdisciplinary research\nteams to a comprehensive list of emergent ideas in HGAI, identifying their\ninterested topics while maintaining a coherent big picture of the future work\nlandscape.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang &#x27;Anthony&#x27; Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burke_J/0/1/0/all/0/1\">Jeff Burke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1\">Ruofei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1\">Matthew K. Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_J/0/1/0/all/0/1\">Jennifer Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1\">Philippe Laban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dingzeyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willis_K/0/1/0/all/0/1\">Karl D. D. Willis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating GPT-3.5 and GPT-4 on Grammatical Error Correction for Brazilian Portuguese. (arXiv:2306.15788v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15788","description":"<p>We investigate the effectiveness of GPT-3.5 and GPT-4, two large language\nmodels, as Grammatical Error Correction (GEC) tools for Brazilian Portuguese\nand compare their performance against Microsoft Word and Google Docs. We\nintroduce a GEC dataset for Brazilian Portuguese with four categories: Grammar,\nSpelling, Internet, and Fast typing. Our results show that while GPT-4 has\nhigher recall than other methods, LLMs tend to have lower precision, leading to\novercorrection. This study demonstrates the potential of LLMs as practical GEC\ntools for Brazilian Portuguese and encourages further exploration of LLMs for\nnon-English languages and other educational settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Penteado_M/0/1/0/all/0/1\">Maria Carolina Penteado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_F/0/1/0/all/0/1\">F&#xe1;bio Perez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FLuRKA: Fast fused Low-Rank & Kernel Attention. (arXiv:2306.15799v1 [cs.LG])","link":"http://arxiv.org/abs/2306.15799","description":"<p>Many efficient approximate self-attention techniques have become prevalent\nsince the inception of the transformer architecture. Two popular classes of\nthese techniques are low-rank and kernel methods. Each of these methods has its\nown strengths. We observe these strengths synergistically complement each other\nand exploit these synergies to fuse low-rank and kernel methods, producing a\nnew class of transformers: FLuRKA (Fast Low-Rank and Kernel Attention). FLuRKA\nprovide sizable performance gains over these approximate techniques and are of\nhigh quality. We theoretically and empirically evaluate both the runtime\nperformance and quality of FLuRKA. Our runtime analysis posits a variety of\nparameter configurations where FLuRKA exhibit speedups and our accuracy\nanalysis bounds the error of FLuRKA with respect to full-attention. We\ninstantiate three FLuRKA variants which experience empirical speedups of up to\n3.3x and 1.7x over low-rank and kernel methods respectively. This translates to\nspeedups of up to 30x over models with full-attention. With respect to model\nquality, FLuRKA can match the accuracy of low-rank and kernel methods on GLUE\nafter pre-training on wiki-text 103. When pre-training on a fixed time budget,\nFLuRKA yield better perplexity scores than models with full-attention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ahan Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yueming Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendis_C/0/1/0/all/0/1\">Charith Mendis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Confidence-based Ensembles of End-to-End Speech Recognition Models. (arXiv:2306.15824v1 [eess.AS])","link":"http://arxiv.org/abs/2306.15824","description":"<p>The number of end-to-end speech recognition models grows every year. These\nmodels are often adapted to new domains or languages resulting in a\nproliferation of expert systems that achieve great results on target data,\nwhile generally showing inferior performance outside of their domain of\nexpertise. We explore combination of such experts via confidence-based\nensembles: ensembles of models where only the output of the most-confident\nmodel is used. We assume that models' target data is not available except for a\nsmall validation set. We demonstrate effectiveness of our approach with two\napplications. First, we show that a confidence-based ensemble of 5 monolingual\nmodels outperforms a system where model selection is performed via a dedicated\nlanguage identification block. Second, we demonstrate that it is possible to\ncombine base and adapted models to achieve strong results on both original and\ntarget data. We validate all our results on multiple datasets and model\narchitectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gitman_I/0/1/0/all/0/1\">Igor Gitman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lavrukhin_V/0/1/0/all/0/1\">Vitaly Lavrukhin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laptev_A/0/1/0/all/0/1\">Aleksandr Laptev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAT: Mixed-Strategy Game of Adversarial Training in Fine-tuning. (arXiv:2306.15826v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15826","description":"<p>Fine-tuning large-scale pre-trained language models has been demonstrated\neffective for various natural language processing (NLP) tasks. Previous studies\nhave established that incorporating adversarial training during the fine-tuning\nstage can significantly enhance model generalization and robustness. However,\nfrom the perspective of game theory, such utilizations of adversarial training\ncorrespond to pure-strategy games, which are inherently limited in terms of the\nscope of their strategies, thereby still having room for improvement. In order\nto push the performance boundaries, we propose a novel Mixed-strategy\nAdversarial Training algorithm (MAT). Methodologically, we derive the Nash\nequilibrium of a mixed-strategy game for adversarial training using Entropy\nMirror Descent to establish MAT by sampling method. To verify the effectiveness\nof MAT, we conducted extensive benchmark experiments on large-scale pre-trained\nmodels, such as BERT and RoBERTa. MAT significantly outperforms the\nstate-of-the-art methods on both the GLUE and ANLI benchmarks in terms of\ngeneralization and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhehua Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhen Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Symbol emergence as interpersonal cross-situational learning: the emergence of lexical knowledge with combinatoriality. (arXiv:2306.15837v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15837","description":"<p>We present a computational model for a symbol emergence system that enables\nthe emergence of lexical knowledge with combinatoriality among agents through a\nMetropolis-Hastings naming game and cross-situational learning. Many\ncomputational models have been proposed to investigate combinatoriality in\nemergent communication and symbol emergence in cognitive and developmental\nrobotics. However, existing models do not sufficiently address category\nformation based on sensory-motor information and semiotic communication through\nthe exchange of word sequences within a single integrated model. Our proposed\nmodel facilitates the emergence of lexical knowledge with combinatoriality by\nperforming category formation using multimodal sensory-motor information and\nenabling semiotic communication through the exchange of word sequences among\nagents in a unified model. Furthermore, the model enables an agent to predict\nsensory-motor information for unobserved situations by combining words\nassociated with categories in each modality. We conducted two experiments with\ntwo humanoid robots in a simulated environment to evaluate our proposed model.\nThe results demonstrated that the agents can acquire lexical knowledge with\ncombinatoriality through interpersonal cross-situational learning based on the\nMetropolis-Hastings naming game and cross-situational learning. Furthermore,\nour results indicate that the lexical knowledge developed using our proposed\nmodel exhibits generalization performance for novel situations through\ninterpersonal cross-modal inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagiwara_Y/0/1/0/all/0/1\">Yoshinobu Hagiwara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furukawa_K/0/1/0/all/0/1\">Kazuma Furukawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horie_T/0/1/0/all/0/1\">Takafumi Horie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taniguchi_A/0/1/0/all/0/1\">Akira Taniguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taniguchi_T/0/1/0/all/0/1\">Tadahiro Taniguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias. (arXiv:2306.15895v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15895","description":"<p>Large language models (LLMs) have been recently leveraged as training data\ngenerators for various natural language processing (NLP) tasks. While previous\nresearch has explored different approaches to training models using generated\ndata, they generally rely on simple class-conditional prompts, which may limit\nthe diversity of the generated data and inherit systematic biases of LLM. Thus,\nwe investigate training data generation with diversely attributed prompts\n(e.g., specifying attributes like length and style), which have the potential\nto yield diverse and attributed generated data. Our investigation focuses on\ndatasets with high cardinality and diverse domains, wherein we demonstrate that\nattributed prompts outperform simple class-conditional prompts in terms of the\nresulting model's performance. Additionally, we present a comprehensive\nempirical study on data generation encompassing vital aspects like bias,\ndiversity, and efficiency, and highlight three key observations: firstly,\nsynthetic datasets generated by simple prompts exhibit significant biases, such\nas regional bias; secondly, attribute diversity plays a pivotal role in\nenhancing model performance; lastly, attributed prompts achieve the performance\nof simple class-conditional prompts while utilizing only 5\\% of the querying\ncost of ChatGPT associated with the latter. We release the generated dataset\nand used prompts to facilitate future research. The data and code will be\navailable on \\url{https://github.com/yueyu1030/AttrPrompt}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yuchen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jieyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratner_A/0/1/0/all/0/1\">Alexander Ratner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiaming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Confidence-Calibrated Ensemble Dense Phrase Retrieval. (arXiv:2306.15917v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15917","description":"<p>In this paper, we consider the extent to which the transformer-based Dense\nPassage Retrieval (DPR) algorithm, developed by (Karpukhin et. al. 2020), can\nbe optimized without further pre-training. Our method involves two particular\ninsights: we apply the DPR context encoder at various phrase lengths (e.g.\none-sentence versus five-sentence segments), and we take a\nconfidence-calibrated ensemble prediction over all of these different\nsegmentations. This somewhat exhaustive approach achieves start-of-the-art\nresults on benchmark datasets such as Google NQ and SQuAD. We also apply our\nmethod to domain-specific datasets, and the results suggest how different\ngranularities are optimal for different domains\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">William Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergam_N/0/1/0/all/0/1\">Noah Bergam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Arnav Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheikhoslami_N/0/1/0/all/0/1\">Nima Sheikhoslami</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Most Language Models can be Poets too: An AI Writing Assistant and Constrained Text Generation Studio. (arXiv:2306.15926v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15926","description":"<p>Despite rapid advancement in the field of Constrained Natural Language\nGeneration, little time has been spent on exploring the potential of language\nmodels which have had their vocabularies lexically, semantically, and/or\nphonetically constrained. We find that most language models generate compelling\ntext even under significant constraints. We present a simple and universally\napplicable technique for modifying the output of a language model by\ncompositionally applying filter functions to the language models vocabulary\nbefore a unit of text is generated. This approach is plug-and-play and requires\nno modification to the model. To showcase the value of this technique, we\npresent an easy to use AI writing assistant called Constrained Text Generation\nStudio (CTGS). CTGS allows users to generate or choose from text with any\ncombination of a wide variety of constraints, such as banning a particular\nletter, forcing the generated words to have a certain number of syllables,\nand/or forcing the words to be partial anagrams of another word. We introduce a\nnovel dataset of prose that omits the letter e. We show that our method results\nin strictly superior performance compared to fine-tuning alone on this dataset.\nWe also present a Huggingface space web-app presenting this technique called\nGadsby. The code is available to the public here:\nhttps://github.com/Hellisotherpeople/Constrained-Text-Generation-Studio\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roush_A/0/1/0/all/0/1\">Allen Roush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1\">Sanjay Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moorthy_A/0/1/0/all/0/1\">Akshay Moorthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubovoy_D/0/1/0/all/0/1\">Dmitry Dubovoy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Can Generate It Again: Data-to-text Generation with Verification and Correction Prompting. (arXiv:2306.15933v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15933","description":"<p>Despite significant advancements in existing models, generating text\ndescriptions from structured data input, known as data-to-text generation,\nremains a challenging task. In this paper, we propose a novel approach that\ngoes beyond traditional one-shot generation methods by introducing a multi-step\nprocess consisting of generation, verification, and correction stages. Our\napproach, VCP(Verification and Correction Prompting), begins with the model\ngenerating an initial output. We then proceed to verify the correctness of\ndifferent aspects of the generated text. The observations from the verification\nstep are converted into a specialized error-indication prompt, which instructs\nthe model to regenerate the output while considering the identified errors. To\nenhance the model's correction ability, we have developed a carefully designed\ntraining procedure. This procedure enables the model to incorporate feedback\nfrom the error-indication prompt, resulting in improved output generation.\nThrough experimental results, we demonstrate that our approach effectively\nreduces slot error rates while maintaining the overall quality of the generated\ntext.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuan Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingqiao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence-to-Label Generation Framework for Multi-task Learning of Japanese Sentence Classification and Named Entity Recognition. (arXiv:2306.15978v1 [cs.CL])","link":"http://arxiv.org/abs/2306.15978","description":"<p>Information extraction(IE) is a crucial subfield within natural language\nprocessing. In this study, we introduce a Sentence Classification and Named\nEntity Recognition Multi-task (SCNM) approach that combines Sentence\nClassification (SC) and Named Entity Recognition (NER). We develop a\nSentence-to-Label Generation (SLG) framework for SCNM and construct a Wikipedia\ndataset containing both SC and NER. Using a format converter, we unify input\nformats and employ a generative model to generate SC-labels, NER-labels, and\nassociated text segments. We propose a Constraint Mechanism (CM) to improve\ngenerated format accuracy. Our results show SC accuracy increased by 1.13\npoints and NER by 1.06 points in SCNM compared to standalone tasks, with CM\nraising format accuracy from 63.61 to 100. The findings indicate mutual\nreinforcement effects between SC and NER, and integration enhances both tasks'\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chengguang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_T/0/1/0/all/0/1\">Tatsunori Mori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Streamlining Social Media Information Retrieval for Public Health Research with Deep Learning. (arXiv:2306.16001v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16001","description":"<p>The utilization of social media in epidemic surveillance has been well\nestablished. Nonetheless, bias is often introduced when pre-defined lexicons\nare used to retrieve relevant corpus. This study introduces a framework aimed\nat curating extensive dictionaries of medical colloquialisms and Unified\nMedical Language System (UMLS) concepts. The framework comprises three modules:\na BERT-based Named Entity Recognition (NER) model that identifies medical\nentities from social media content, a deep-learning powered normalization\nmodule that standardizes the extracted entities, and a semi-supervised\nclustering module that assigns the most probable UMLS concept to each\nstandardized entity. We applied this framework to COVID-19-related tweets from\nFebruary 1, 2020, to April 30, 2022, generating a symptom dictionary (available\nat https://github.com/ningkko/UMLS_colloquialism/) composed of 9,249\nstandardized entities mapped to 876 UMLS concepts and 38,175 colloquial\nexpressions. This framework demonstrates encouraging potential in addressing\nthe constraints of keyword matching information retrieval in social media-based\npublic health research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yining Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shixu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yujie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Peilin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_Y/0/1/0/all/0/1\">Ying-Chih Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Li Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting Large Language Models for Zero-Shot Domain Adaptation in Speech Recognition. (arXiv:2306.16007v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16007","description":"<p>The integration of Language Models (LMs) has proven to be an effective way to\naddress domain shifts in speech recognition. However, these approaches usually\nrequire a significant amount of target domain text data for the training of\nLMs. Different from these methods, in this work, with only a domain-specific\ntext prompt, we propose two zero-shot ASR domain adaptation methods using\nLLaMA, a 7-billion-parameter large language model (LLM). LLM is used in two\nways: 1) second-pass rescoring: reranking N-best hypotheses of a given ASR\nsystem with LLaMA; 2) deep LLM-fusion: incorporating LLM into the decoder of an\nencoder-decoder based ASR system. Experiments show that, with only one domain\nprompt, both methods can effectively reduce word error rates (WER) on\nout-of-domain TedLium-2 and SPGISpeech datasets. Especially, the deep\nLLM-fusion has the advantage of better recall of entity and out-of-vocabulary\nwords.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Transducers through Adjacent Token Merging. (arXiv:2306.16009v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16009","description":"<p>Recent end-to-end automatic speech recognition (ASR) systems often utilize a\nTransformer-based acoustic encoder that generates embedding at a high frame\nrate. However, this design is inefficient, particularly for long speech signals\ndue to the quadratic computation of self-attention. To address this, we propose\na new method, Adjacent Token Merging (A-ToMe), which gradually combines\nadjacent tokens with high similarity scores between their key values. In this\nway, the total time step could be reduced, and the inference of both the\nencoder and joint network is accelerated. Experiments on LibriSpeech show that\nour method can reduce 57% of tokens and improve the inference speed on GPU by\n70% without any notable loss of accuracy. Additionally, we demonstrate that\nA-ToMe is also an effective solution to reduce tokens in long-form ASR, where\nthe input speech consists of multiple utterances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Spatial-Temporal Variations of Public Discourse on Social Media: A Case Study on the First Wave of the Coronavirus Pandemic in Italy. (arXiv:2306.16031v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16031","description":"<p>This paper proposes a methodology for exploring how linguistic behaviour on\nsocial media can be used to explore societal reactions to important events such\nas those that transpired during the SARS CoV2 pandemic. In particular, where\nspatial and temporal aspects of events are important features. Our methodology\nconsists of grounding spatial-temporal categories in tweet usage trends using\ntime-series analysis and clustering. Salient terms in each category were then\nidentified through qualitative comparative analysis based on scaled f-scores\naggregated into hand-coded categories. To exemplify this approach, we conducted\na case study on the first wave of the coronavirus in Italy. We used our\nproposed methodology to explore existing psychological observations which\nclaimed that physical distance from events affects what is communicated about\nthem. We confirmed these findings by showing that the epicentre of the disease\nand peripheral regions correspond to clear time-series clusters and that those\nliving in the epicentre of the SARS CoV2 outbreak were more focused on\nsolidarity and policy than those from more peripheral regions. Furthermore, we\nalso found that temporal categories corresponded closely to policy changes\nduring the handling of the pandemic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michael_A/0/1/0/all/0/1\">Anslow Michael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martina_G/0/1/0/all/0/1\">Galletti Martina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Sentiment and Fun Facts We Learnt Before FIFA World Cup Qatar 2022 Using Twitter and AI. (arXiv:2306.16049v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16049","description":"<p>Twitter is a social media platform bridging most countries and allows\nreal-time news discovery. Since the tweets on Twitter are usually short and\nexpress public feelings, thus provide a source for opinion mining and sentiment\nanalysis for global events. This paper proposed an effective solution, in\nproviding a sentiment on tweets related to the FIFA World Cup. At least 130k\ntweets, as the first in the community, are collected and implemented as a\ndataset to evaluate the performance of the proposed machine learning solution.\nThese tweets are collected with the related hashtags and keywords of the Qatar\nWorld Cup 2022. The Vader algorithm is used in this paper for sentiment\nanalysis. Through the machine learning method and collected Twitter tweets, we\ndiscovered the sentiments and fun facts of several aspects important to the\nperiod before the World Cup. The result shows people are positive to the\nopening of the World Cup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+She_J/0/1/0/all/0/1\">James She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swart_Arries_K/0/1/0/all/0/1\">Kamilla Swart-Arries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belal_M/0/1/0/all/0/1\">Mohammad Belal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_S/0/1/0/all/0/1\">Simon Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-term Conversation Analysis: Exploring Utility and Privacy. (arXiv:2306.16071v1 [eess.AS])","link":"http://arxiv.org/abs/2306.16071","description":"<p>The analysis of conversations recorded in everyday life requires privacy\nprotection. In this contribution, we explore a privacy-preserving feature\nextraction method based on input feature dimension reduction, spectral\nsmoothing and the low-cost speaker anonymization technique based on McAdams\ncoefficient. We assess the utility of the feature extraction methods with a\nvoice activity detection and a speaker diarization system, while privacy\nprotection is determined with a speech recognition and a speaker verification\nmodel. We show that the combination of McAdams coefficient and spectral\nsmoothing maintains the utility while improving privacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nespoli_F/0/1/0/all/0/1\">Francesco Nespoli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pohlhausen_J/0/1/0/all/0/1\">Jule Pohlhausen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naylor_P/0/1/0/all/0/1\">Patrick A. Naylor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bitzer_J/0/1/0/all/0/1\">Joerg Bitzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases. (arXiv:2306.16092v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16092","description":"<p>Large Language Models (LLMs) have shown the potential to revolutionize\nnatural language processing tasks in various domains, sparking great interest\nin vertical-specific large models. However, unlike proprietary models such as\nBloombergGPT and FinGPT, which have leveraged their unique data accumulations\nto make strides in the finance domain, there hasn't not many similar large\nlanguage models in the Chinese legal domain to facilitate its digital\ntransformation.\n</p>\n<p>In this paper, we propose an open-source legal large language model named\nChatLaw. Due to the importance of data quality, we carefully designed a legal\ndomain fine-tuning dataset. Additionally, to overcome the problem of model\nhallucinations in legal data screening during reference data retrieval, we\nintroduce a method that combines vector database retrieval with keyword\nretrieval to effectively reduce the inaccuracy of relying solely on vector\ndatabase retrieval. Furthermore, we propose a self-attention method to enhance\nthe ability of large models to overcome errors present in reference data,\nfurther optimizing the issue of model hallucinations at the model level and\nimproving the problem-solving capabilities of large models. We also\nopen-sourced our model and part of the data at\nhttps://github.com/PKU-YuanGroup/ChatLaw.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jiaxi Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zongjian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bohua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT a Biomedical Expert? -- Exploring the Zero-Shot Performance of Current GPT Models in Biomedical Tasks. (arXiv:2306.16108v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16108","description":"<p>We assessed the performance of commercial Large Language Models (LLMs)\nGPT-3.5-Turbo and GPT-4 on tasks from the 2023 BioASQ challenge. In Task 11b\nPhase B, which is focused on answer generation, both models demonstrated\ncompetitive abilities with leading systems. Remarkably, they achieved this with\nsimple zero-shot learning, grounded with relevant snippets. Even without\nrelevant snippets, their performance was decent, though not on par with the\nbest systems. Interestingly, the older and cheaper GPT-3.5-Turbo system was\nable to compete with GPT-4 in the grounded Q&amp;A setting on factoid and list\nanswers. In Task 11b Phase A, focusing on retrieval, query expansion through\nzero-shot learning improved performance, but the models fell short compared to\nother systems. The code needed to rerun these experiments is available through\nGitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ateia_S/0/1/0/all/0/1\">Samy Ateia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruschwitz_U/0/1/0/all/0/1\">Udo Kruschwitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Framework for Identifying Depression on Social Media: MentalRiskES@IberLEF 2023. (arXiv:2306.16125v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16125","description":"<p>This paper describes our participation in the MentalRiskES task at IberLEF\n2023. The task involved predicting the likelihood of an individual experiencing\ndepression based on their social media activity. The dataset consisted of\nconversations from 175 Telegram users, each labeled according to their evidence\nof suffering from the disorder. We used a combination of traditional machine\nlearning and deep learning techniques to solve four predictive subtasks: binary\nclassification, simple regression, multiclass classification, and multiclass\nregression. We approached this by training a model to solve the multiclass\nregression case and then transforming the predictions to work for the other\nthree subtasks. We compare the performance of two different modeling\napproaches: fine-tuning a BERT-based model and using sentence embeddings as\ninputs to a linear regressor, with the latter yielding better results. The code\nto reproduce our results can be found at:\nhttps://github.com/simonsanvil/EarlyDepression-MentalRiskES.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Viloria_S/0/1/0/all/0/1\">Simon Sanchez Viloria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rio_D/0/1/0/all/0/1\">Daniel Peix del R&#xed;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabo_R/0/1/0/all/0/1\">Rub&#xe9;n Berm&#xfa;dez Cabo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuentes_G/0/1/0/all/0/1\">Guillermo Arturo Arrojo Fuentes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segura_Bedmar_I/0/1/0/all/0/1\">Isabel Segura-Bedmar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative User-Experience Research for Developing Domain-specific Natural Language Processing Applications. (arXiv:2306.16143v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16143","description":"<p>User experience (UX) is a part of human-computer interaction (HCI) research\nand focuses on increasing intuitiveness, transparency, simplicity, and trust\nfor system users. Most of the UX research for machine learning (ML) or natural\nlanguage processing (NLP) focuses on a data-driven methodology, i.e., it fails\nto focus on users' requirements, and engages domain users mainly for usability\nevaluation. Moreover, more typical UX methods tailor the systems towards user\nusability, unlike learning about the user needs first. The paper proposes a\nmethodology for integrating generative UX research into developing domain NLP\napplications. Generative UX research employs domain users at the initial stages\nof prototype development, i.e., ideation and concept evaluation, and the last\nstage for evaluating the change in user value. In the case study, we report the\nfull-cycle prototype development of a domain-specific semantic search for daily\noperations in the process industry. Our case study shows that involving domain\nexperts increases their interest and trust in the final NLP application.\nMoreover, we show that synergetic UX+NLP research efficiently considers data-\nand user-driven opportunities and constraints, which can be crucial for NLP\napplications in narrow domains\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhukova_A/0/1/0/all/0/1\">Anastasia Zhukova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sperl_L/0/1/0/all/0/1\">Lukas von Sperl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matt_C/0/1/0/all/0/1\">Christian E. Matt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SkillNet-X: A Multilingual Multitask Model with Sparsely Activated Skills. (arXiv:2306.16176v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16176","description":"<p>Traditional multitask learning methods basically can only exploit common\nknowledge in task- or language-wise, which lose either cross-language or\ncross-task knowledge. This paper proposes a general multilingual multitask\nmodel, named SkillNet-X, which enables a single model to tackle many different\ntasks from different languages. To this end, we define several\nlanguage-specific skills and task-specific skills, each of which corresponds to\na skill module. SkillNet-X sparsely activates parts of the skill modules which\nare relevant either to the target task or the target language. Acting as\nknowledge transit hubs, skill modules are capable of absorbing task-related\nknowledge and language-related knowledge consecutively. Based on Transformer,\nwe modify the multi-head attention layer and the feed forward network layer to\naccommodate skill modules. We evaluate SkillNet-X on eleven natural language\nunderstanding datasets in four languages. Results show that SkillNet-X performs\nbetter than task-specific baselines and two multitask learning baselines (i.e.,\ndense joint model and Mixture-of-Experts model). Furthermore, skill\npre-training further improves the performance of SkillNet-X on almost all\ndatasets. To investigate the generalization of our model, we conduct\nexperiments on two new tasks and find that SkillNet-X significantly outperforms\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhangyin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Duyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaocheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuangzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation. (arXiv:2306.16195v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16195","description":"<p>Incorporating external graph knowledge into neural chatbot models has been\nproven effective for enhancing dialogue generation. However, in conventional\ngraph neural networks (GNNs), message passing on a graph is independent from\ntext, resulting in the graph representation hidden space differing from that of\nthe text. This training regime of existing models therefore leads to a semantic\ngap between graph knowledge and text. In this study, we propose a novel\nframework for knowledge graph enhanced dialogue generation. We dynamically\nconstruct a multi-hop knowledge graph with pseudo nodes to involve the language\nmodel in feature aggregation within the graph at all steps. To avoid the\nsemantic biases caused by learning on vanilla subgraphs, the proposed framework\napplies hierarchical graph attention to aggregate graph features on pseudo\nnodes and then attains a global feature. Therefore, the framework can better\nutilise the heterogeneous features from both the post and external graph\nknowledge. Extensive experiments demonstrate that our framework outperforms\nstate-of-the-art (SOTA) baselines on dialogue generation. Further analysis also\nshows that our representation learning framework can fill the semantic gap by\ncoagulating representations of both text and graph knowledge. Moreover, the\nlanguage model also learns how to better select knowledge triples for a more\ninformative response via exploiting subgraph patterns within our feature\naggregation process. Our code and resources are available at\nhttps://github.com/tangg555/SaBART.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loakman_T/0/1/0/all/0/1\">Tyler Loakman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1\">Frank Guerin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inferring the Goals of Communicating Agents from Actions and Instructions. (arXiv:2306.16207v1 [cs.AI])","link":"http://arxiv.org/abs/2306.16207","description":"<p>When humans cooperate, they frequently coordinate their activity through both\nverbal communication and non-verbal actions, using this information to infer a\nshared goal and plan. How can we model this inferential ability? In this paper,\nwe introduce a model of a cooperative team where one agent, the principal, may\ncommunicate natural language instructions about their shared plan to another\nagent, the assistant, using GPT-3 as a likelihood function for instruction\nutterances. We then show how a third person observer can infer the team's goal\nvia multi-modal Bayesian inverse planning from actions and instructions,\ncomputing the posterior distribution over goals under the assumption that\nagents will act and communicate rationally to achieve them. We evaluate this\napproach by comparing it with human goal inferences in a multi-agent gridworld,\nfinding that our model's inferences closely correlate with human judgments (R =\n0.96). When compared to inference from actions alone, we also find that\ninstructions lead to more rapid and less uncertain goal inference, highlighting\nthe importance of verbal communication for cooperative agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ying_L/0/1/0/all/0/1\">Lance Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhi_Xuan_T/0/1/0/all/0/1\">Tan Zhi-Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansinghka_V/0/1/0/all/0/1\">Vikash Mansinghka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models. (arXiv:2306.16244v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16244","description":"<p>Holistically measuring societal biases of large language models is crucial\nfor detecting and reducing ethical risks in highly capable AI models. In this\nwork, we present a Chinese Bias Benchmark dataset that consists of over 100K\nquestions jointly constructed by human experts and generative language models,\ncovering stereotypes and societal biases in 14 social dimensions related to\nChinese culture and values. The curation process contains 4 essential steps:\nbias identification via extensive literature review, ambiguous context\ngeneration, AI-assisted disambiguous context generation, snd manual review \\&amp;\nrecomposition. The testing instances in the dataset are automatically derived\nfrom 3K+ high-quality templates manually authored with stringent quality\ncontrol. The dataset exhibits wide coverage and high diversity. Extensive\nexperiments demonstrate the effectiveness of the dataset in detecting model\nbias, with all 10 publicly available Chinese large language models exhibiting\nstrong bias in certain categories. Additionally, we observe from our\nexperiments that fine-tuned models could, to a certain extent, heed\ninstructions and avoid generating outputs that are morally harmful in some\ntypes, in the way of \"moral self-correction\". Our dataset and results are\npublicly available at\n\\href{https://github.com/YFHuangxxxx/CBBQ}{https://github.com/YFHuangxxxx/CBBQ},\noffering debiasing research opportunities to a widened community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion Analysis of Tweets Banning Education in Afghanistan. (arXiv:2306.16268v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16268","description":"<p>This paper introduces the first emotion annotated dataset for the Dari\nvariant of Persian spoken in Afghanistan. The LetHerLearn dataset contains\n7,600 tweets posted in reaction to the Taliban ban of women rights to education\nin 2022 and has been manually annotated according to Ekman emotion categories.\nWe here detail the data collection and annotation process, present relevant\ndataset statistics as well as initial experiments on the resulting dataset,\nbenchmarking a number of different neural architectures for the task of Dari\nemotion classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hussiny_M/0/1/0/all/0/1\">Mohammad Ali Hussiny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovrelid_L/0/1/0/all/0/1\">Lilja &#xd8;vrelid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting. (arXiv:2306.16275v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16275","description":"<p>Food effect summarization from New Drug Application (NDA) is an essential\ncomponent of product-specific guidance (PSG) development and assessment.\nHowever, manual summarization of food effect from extensive drug application\nreview documents is time-consuming, which arouses a need to develop automated\nmethods. Recent advances in large language models (LLMs) such as ChatGPT and\nGPT-4, have demonstrated great potential in improving the effectiveness of\nautomated text summarization, but its ability regarding the accuracy in\nsummarizing food effect for PSG assessment remains unclear. In this study, we\nintroduce a simple yet effective approach, iterative prompting, which allows\none to interact with ChatGPT or GPT-4 more effectively and efficiently through\nmulti-turn interaction. Specifically, we propose a three-turn iterative\nprompting approach to food effect summarization in which the keyword-focused\nand length-controlled prompts are respectively provided in consecutive turns to\nrefine the quality of the generated summary. We conduct a series of extensive\nevaluations, ranging from automated metrics to FDA professionals and even\nevaluation by GPT-4, on 100 NDA review documents selected over the past five\nyears. We observe that the summary quality is progressively improved throughout\nthe process. Moreover, we find that GPT-4 performs better than ChatGPT, as\nevaluated by FDA professionals (43% vs. 12%) and GPT-4 (64% vs. 35%).\nImportantly, all the FDA professionals unanimously rated that 85% of the\nsummaries generated by GPT-4 are factually consistent with the golden reference\nsummary, a finding further supported by GPT-4 rating of 72% consistency. These\nresults strongly suggest a great potential for GPT-4 to draft food effect\nsummaries that could be reviewed by FDA professionals, thereby improving the\nefficiency of PSG assessment cycle and promoting the generic drug product\ndevelopment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yiwen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Ping Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Biao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ValizadehAslani_T/0/1/0/all/0/1\">Taha ValizadehAslani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agbavor_F/0/1/0/all/0/1\">Felix Agbavor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Meng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hualou Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT may excel in States Medical Licensing Examination but falters in basic Linear Algebra. (arXiv:2306.16282v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16282","description":"<p>The emergence of ChatGPT has been rapid, and although it has demonstrated\npositive impacts in certain domains, its influence is not universally\nadvantageous. Our analysis focuses on ChatGPT's capabilities in Mathematics\nEducation, particularly in teaching basic Linear Algebra. While there are\ninstances where ChatGPT delivers accurate and well-motivated answers, it is\ncrucial to recognize numerous cases where it makes significant mathematical\nerrors and fails in logical inference. These occurrences raise concerns\nregarding the system's genuine understanding of mathematics, as it appears to\nrely more on visual patterns rather than true comprehension. Additionally, the\nsuitability of ChatGPT as a teacher for students also warrants consideration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bagno_E/0/1/0/all/0/1\">Eli Bagno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dana_Picard_T/0/1/0/all/0/1\">Thierry Dana-Picard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reches_S/0/1/0/all/0/1\">Shulamit Reches</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Adversarial Multi-Task Learning Method for Chinese Text Correction with Semantic Detection. (arXiv:2306.16313v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16313","description":"<p>Text correction, especially the semantic correction of more widely used\nscenes, is strongly required to improve, for the fluency and writing efficiency\nof the text. An adversarial multi-task learning method is proposed to enhance\nthe modeling and detection ability of character polysemy in Chinese sentence\ncontext. Wherein, two models, the masked language model and scoring language\nmodel, are introduced as a pair of not only coupled but also adversarial\nlearning tasks. Moreover, the Monte Carlo tree search strategy and a policy\nnetwork are introduced to accomplish the efficient Chinese text correction task\nwith semantic detection. The experiments are executed on three datasets and\nfive comparable methods, and the experimental results show that our method can\nobtain good performance in Chinese text correction task for better semantic\nrationality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fanyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhenping Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models. (arXiv:2306.16322v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16322","description":"<p>Large language models (LLMs) have demonstrated impressive performance on\nvarious downstream tasks without requiring fine-tuning, including ChatGPT, a\nchat-based model built on top of LLMs such as GPT-3.5 and GPT-4. Despite having\na lower training proportion compared to English, these models also exhibit\nremarkable capabilities in other languages. In this study, we assess the\nperformance of GPT-3.5 and GPT-4 models on seven distinct Arabic NLP tasks:\nsentiment analysis, translation, transliteration, paraphrasing, part of speech\ntagging, summarization, and diacritization. Our findings reveal that GPT-4\noutperforms GPT-3.5 on five out of the seven tasks. Furthermore, we conduct an\nextensive analysis of the sentiment analysis task, providing insights into how\nLLMs achieve exceptional results on a challenging dialectal dataset.\nAdditionally, we introduce a new Python interface\nhttps://github.com/ARBML/Taqyim that facilitates the evaluation of these tasks\neffortlessly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alyafeai_Z/0/1/0/all/0/1\">Zaid Alyafeai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshaibani_M/0/1/0/all/0/1\">Maged S. Alshaibani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AlKhamissi_B/0/1/0/all/0/1\">Badr AlKhamissi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luqman_H/0/1/0/all/0/1\">Hamzah Luqman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alareqi_E/0/1/0/all/0/1\">Ebrahim Alareqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fadel_A/0/1/0/all/0/1\">Ali Fadel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Learning via Variational Bayesian Networks. (arXiv:2306.16326v1 [cs.LG])","link":"http://arxiv.org/abs/2306.16326","description":"<p>We present Variational Bayesian Network (VBN) - a novel Bayesian entity\nrepresentation learning model that utilizes hierarchical and relational side\ninformation and is particularly useful for modeling entities in the\n``long-tail'', where the data is scarce. VBN provides better modeling for\nlong-tail entities via two complementary mechanisms: First, VBN employs\ninformative hierarchical priors that enable information propagation between\nentities sharing common ancestors. Additionally, VBN models explicit relations\nbetween entities that enforce complementary structure and consistency, guiding\nthe learned representations towards a more meaningful arrangement in space.\nSecond, VBN represents entities by densities (rather than vectors), hence\nmodeling uncertainty that plays a complementary role in coping with data\nscarcity. Finally, we propose a scalable Variational Bayes optimization\nalgorithm that enables fast approximate Bayesian inference. We evaluate the\neffectiveness of VBN on linguistic, recommendations, and medical inference\ntasks. Our findings show that VBN outperforms other existing methods across\nmultiple datasets, and especially in the long-tail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barkan_O/0/1/0/all/0/1\">Oren Barkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1\">Avi Caciularu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rejwan_I/0/1/0/all/0/1\">Idan Rejwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_O/0/1/0/all/0/1\">Ori Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weill_J/0/1/0/all/0/1\">Jonathan Weill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malkiel_I/0/1/0/all/0/1\">Itzik Malkiel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koenigstein_N/0/1/0/all/0/1\">Noam Koenigstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Site Clinical Federated Learning using Recursive and Attentive Models and NVFlare. (arXiv:2306.16367v1 [cs.LG])","link":"http://arxiv.org/abs/2306.16367","description":"<p>The prodigious growth of digital health data has precipitated a mounting\ninterest in harnessing machine learning methodologies, such as natural language\nprocessing (NLP), to scrutinize medical records, clinical notes, and other\ntext-based health information. Although NLP techniques have exhibited\nsubstantial potential in augmenting patient care and informing clinical\ndecision-making, data privacy and adherence to regulations persist as critical\nconcerns. Federated learning (FL) emerges as a viable solution, empowering\nmultiple organizations to train machine learning models collaboratively without\ndisseminating raw data. This paper proffers a pragmatic approach to medical NLP\nby amalgamating FL, NLP models, and the NVFlare framework, developed by NVIDIA.\nWe introduce two exemplary NLP models, the Long-Short Term Memory (LSTM)-based\nmodel and Bidirectional Encoder Representations from Transformers (BERT), which\nhave demonstrated exceptional performance in comprehending context and\nsemantics within medical data. This paper encompasses the development of an\nintegrated framework that addresses data privacy and regulatory compliance\nchallenges while maintaining elevated accuracy and performance, incorporating\nBERT pretraining, and comprehensively substantiating the efficacy of the\nproposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yun_W/0/1/0/all/0/1\">Won Joon Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Samuel Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joongheon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Measuring the Representation of Subjective Global Opinions in Language Models. (arXiv:2306.16388v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16388","description":"<p>Large language models (LLMs) may not equitably represent diverse global\nperspectives on societal issues. In this paper, we develop a quantitative\nframework to evaluate whose opinions model-generated responses are more similar\nto. We first build a dataset, GlobalOpinionQA, comprised of questions and\nanswers from cross-national surveys designed to capture diverse opinions on\nglobal issues across different countries. Next, we define a metric that\nquantifies the similarity between LLM-generated survey responses and human\nresponses, conditioned on country. With our framework, we run three experiments\non an LLM trained to be helpful, honest, and harmless with Constitutional AI.\nBy default, LLM responses tend to be more similar to the opinions of certain\npopulations, such as those from the USA, and some European and South American\ncountries, highlighting the potential for biases. When we prompt the model to\nconsider a particular country's perspective, responses shift to be more similar\nto the opinions of the prompted populations, but can reflect harmful cultural\nstereotypes. When we translate GlobalOpinionQA questions to a target language,\nthe model's responses do not necessarily become the most similar to the\nopinions of speakers of those languages. We release our dataset for others to\nuse and build on. Our data is at\nhttps://huggingface.co/datasets/Anthropic/llm_global_opinions. We also provide\nan interactive visualization at https://llmglobalvalues.anthropic.com.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyugen_K/0/1/0/all/0/1\">Karina Nyugen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_T/0/1/0/all/0/1\">Thomas I. Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiefer_N/0/1/0/all/0/1\">Nicholas Schiefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askell_A/0/1/0/all/0/1\">Amanda Askell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakhtin_A/0/1/0/all/0/1\">Anton Bakhtin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Carol Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hatfield_Dodds_Z/0/1/0/all/0/1\">Zac Hatfield-Dodds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_D/0/1/0/all/0/1\">Danny Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_N/0/1/0/all/0/1\">Nicholas Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovitt_L/0/1/0/all/0/1\">Liane Lovitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCandlish_S/0/1/0/all/0/1\">Sam McCandlish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sikder_O/0/1/0/all/0/1\">Orowa Sikder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamkin_A/0/1/0/all/0/1\">Alex Tamkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thamkul_J/0/1/0/all/0/1\">Janel Thamkul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1\">Jared Kaplan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jack Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_D/0/1/0/all/0/1\">Deep Ganguli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language. (arXiv:2306.16410v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16410","description":"<p>We propose LENS, a modular approach for tackling computer vision problems by\nleveraging the power of large language models (LLMs). Our system uses a\nlanguage model to reason over outputs from a set of independent and highly\ndescriptive vision modules that provide exhaustive information about an image.\nWe evaluate the approach on pure computer vision settings such as zero- and\nfew-shot object recognition, as well as on vision and language problems. LENS\ncan be applied to any off-the-shelf LLM and we find that the LLMs with LENS\nperform highly competitively with much bigger and much more sophisticated\nsystems, without any multimodal training whatsoever. We open-source our code at\nhttps://github.com/ContextualAI/lens and provide an interactive demo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berrios_W/0/1/0/all/0/1\">William Berrios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_G/0/1/0/all/0/1\">Gautam Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thrush_T/0/1/0/all/0/1\">Tristan Thrush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amanpreet Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning. (arXiv:2306.16413v1 [cs.LG])","link":"http://arxiv.org/abs/2306.16413","description":"<p>Learning multimodal representations involves integrating information from\nmultiple heterogeneous sources of data. In order to accelerate progress towards\nunderstudied modalities and tasks while ensuring real-world robustness, we\nrelease MultiZoo, a public toolkit consisting of standardized implementations\nof &gt; 20 core multimodal algorithms and MultiBench, a large-scale benchmark\nspanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas.\nTogether, these provide an automated end-to-end machine learning pipeline that\nsimplifies and standardizes data loading, experimental setup, and model\nevaluation. To enable holistic evaluation, we offer a comprehensive methodology\nto assess (1) generalization, (2) time and space complexity, and (3) modality\nrobustness. MultiBench paves the way towards a better understanding of the\ncapabilities and limitations of multimodal models, while ensuring ease of use,\naccessibility, and reproducibility. Our toolkits are publicly available, will\nbe regularly updated, and welcome inputs from the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Arav Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-Modality Multimodal Transformer: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning. (arXiv:2203.01311v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.01311","description":"<p>Many real-world problems are inherently multimodal, from spoken language,\ngestures, and paralinguistics humans use to communicate, to force,\nproprioception, and visual sensors on robots. While there has been an explosion\nof interest in multimodal learning, these methods are focused on a small set of\nmodalities primarily in language, vision, and audio. In order to accelerate\ngeneralization towards diverse and understudied modalities, this paper studies\nefficient representation learning for high-modality scenarios involving a large\nset of diverse modalities. Since adding new models for every new modality\nbecomes prohibitively expensive, a critical technical challenge is\nheterogeneity quantification: how can we measure which modalities encode\nsimilar information and interactions in order to permit parameter sharing with\nprevious modalities? This paper proposes two new information theoretic metrics\nfor heterogeneity quantification: (1) modality heterogeneity studies how\nsimilar 2 modalities {X1,X2} are by measuring how much information can be\ntransferred from X1 to X2, while (2) interaction heterogeneity studies how\nsimilarly pairs of modalities {X1,X2}, {X3,X4} interact by measuring how much\ninformation can be transferred from fusing {X1,X2} to {X3,X4}. We show the\nimportance of these 2 proposed metrics as a way to automatically prioritize the\nfusion of modalities that contain unique information or interactions. The\nresult is a single model, HighMMT, that scales up to 10 modalities (text,\nimage, audio, video, sensors, proprioception, speech, time-series, sets, and\ntables) and 15 tasks from 5 research areas. Not only does HighMMT outperform\nprior methods on the tradeoff between performance and efficiency, it also\ndemonstrates a crucial scaling behavior: performance continues to improve with\neach modality added, and it transfers to entirely new modalities and tasks\nduring fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaw_J/0/1/0/all/0/1\">Jeffrey Tsaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yudong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Shentong Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EHRKit: A Python Natural Language Processing Toolkit for Electronic Health Record Texts. (arXiv:2204.06604v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06604","description":"<p>The Electronic Health Record (EHR) is an essential part of the modern medical\nsystem and impacts healthcare delivery, operations, and research. Unstructured\ntext is attracting much attention despite structured information in the EHRs\nand has become an exciting research field. The success of the recent neural\nNatural Language Processing (NLP) method has led to a new direction for\nprocessing unstructured clinical notes. In this work, we create a python\nlibrary for clinical texts, EHRKit. This library contains two main parts:\nMIMIC-III-specific functions and tasks specific functions. The first part\nintroduces a list of interfaces for accessing MIMIC-III NOTEEVENTS data,\nincluding basic search, information retrieval, and information extraction. The\nsecond part integrates many third-party libraries for up to 12 off-shelf NLP\ntasks such as named entity recognition, summarization, machine translation,\netc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_K/0/1/0/all/0/1\">Keen You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yujie Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lucas Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Chia-Chun Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosand_B/0/1/0/all/0/1\">Benjamin Rosand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_J/0/1/0/all/0/1\">Jeremy Goldwasser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Byte Fusion for Neural Machine Translation. (arXiv:2205.11490v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11490","description":"<p>Subword tokenization schemes are the dominant technique used in current NLP\nmodels. However, such schemes can be rigid and tokenizers built on one corpus\ndo not adapt well to other parallel corpora. It has also been observed that in\nmultilingual corpora, subword tokenization schemes over-segment low-resource\nlanguages leading to a drop in translation performance. A simple alternative to\nsubword tokenizers is byte-based methods i.e. tokenization into byte sequences\nusing encoding schemes such as UTF-8. Byte tokens often represent inputs at a\nsub-character granularity i.e. one character can be represented by a sequence\nof multiple byte tokens. This results in byte sequences that are significantly\nlonger than character sequences. Enforcing aggregation of local information in\nthe lower layers can guide the model to build higher-level semantic\ninformation. We propose a Local Byte Fusion (LOBEF) method for byte-based\nmachine translation -- utilizing byte $n$-gram and word boundaries -- to\naggregate local semantic information. Extensive experiments on multilingual\ntranslation, zero-shot cross-lingual transfer, and domain adaptation reveal a\nconsistent improvement over traditional byte-based models and even over subword\ntechniques. Further analysis also indicates that our byte-based models are\nparameter-efficient and can be trained faster than subword models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sreedhar_M/0/1/0/all/0/1\">Makesh Narsimhan Sreedhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiangpeng Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Junjie Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical MixUp Multi-label Classification with Imbalanced Interdisciplinary Research Proposals. (arXiv:2209.13912v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.13912","description":"<p>Funding agencies are largely relied on a topic matching between domain\nexperts and research proposals to assign proposal reviewers. As proposals are\nincreasingly interdisciplinary, it is challenging to profile the\ninterdisciplinary nature of a proposal, and, thereafter, find expert reviewers\nwith an appropriate set of expertise. An essential step in solving this\nchallenge is to accurately model and classify the interdisciplinary labels of a\nproposal. Existing methodological and application-related literature, such as\ntextual classification and proposal classification, are insufficient in jointly\naddressing the three key unique issues introduced by interdisciplinary proposal\ndata: 1) the hierarchical structure of discipline labels of a proposal from\ncoarse-grain to fine-grain, e.g., from information science to AI to\nfundamentals of AI. 2) the heterogeneous semantics of various main textual\nparts that play different roles in a proposal; 3) the number of proposals is\nimbalanced between non-interdisciplinary and interdisciplinary research. Can we\nsimultaneously address the three issues in understanding the proposal's\ninterdisciplinary nature? In response to this question, we propose a\nhierarchical mixup multiple-label classification framework, which we called\nH-MixUp. H-MixUp leverages a transformer-based semantic information extractor\nand a GCN-based interdisciplinary knowledge extractor for the first and second\nissues. H-MixUp develops a fused training method of Wold-level MixUp,\nWord-level CutMix, Manifold MixUp, and Document-level MixUp to address the\nthird issue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1\">Meng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Min Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1\">Ziyue Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Z/0/1/0/all/0/1\">Zhiyuan Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanjie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuanchun Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QueryForm: A Simple Zero-shot Form Entity Query Framework. (arXiv:2211.07730v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2211.07730","description":"<p>Zero-shot transfer learning for document understanding is a crucial yet\nunder-investigated scenario to help reduce the high cost involved in annotating\ndocument entities. We present a novel query-based framework, QueryForm, that\nextracts entity values from form-like documents in a zero-shot fashion.\nQueryForm contains a dual prompting mechanism that composes both the document\nschema and a specific entity type into a query, which is used to prompt a\nTransformer model to perform a single entity extraction task. Furthermore, we\npropose to leverage large-scale query-entity pairs generated from form-like\nwebpages with weak HTML annotations to pre-train QueryForm. By unifying\npre-training and fine-tuning into the same query-based framework, QueryForm\nenables models to learn from structured documents containing various entities\nand layouts, leading to better generalization to target document types without\nthe need for target-specific training data. QueryForm sets new state-of-the-art\naverage F1 score on both the XFUND (+4.6%~10.1%) and the Payment (+3.2%~9.5%)\nzero-shot benchmark, with a smaller model size and no additional image input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zizhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devlin_J/0/1/0/all/0/1\">Jacob Devlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1\">Guolong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dy_J/0/1/0/all/0/1\">Jennifer Dy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perot_V/0/1/0/all/0/1\">Vincent Perot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NarraSum: A Large-Scale Dataset for Abstractive Narrative Summarization. (arXiv:2212.01476v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.01476","description":"<p>Narrative summarization aims to produce a distilled version of a narrative to\ndescribe its most salient events and characters. Summarizing a narrative is\nchallenging as it requires an understanding of event causality and character\nbehaviors. To encourage research in this direction, we propose NarraSum, a\nlarge-scale narrative summarization dataset. It contains 122K narrative\ndocuments, which are collected from plot descriptions of movies and TV episodes\nwith diverse genres, and their corresponding abstractive summaries. Experiments\nshow that there is a large performance gap between humans and the\nstate-of-the-art summarization models on NarraSum. We hope that this dataset\nwill promote future research in summarization, as well as broader studies of\nnatural language understanding and generation. The dataset is available at\nhttps://github.com/zhaochaocs/narrasum.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaiqiang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wenlin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localising In-Domain Adaptation of Transformer-Based Biomedical Language Models. (arXiv:2212.10422v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10422","description":"<p>In the era of digital healthcare, the huge volumes of textual information\ngenerated every day in hospitals constitute an essential but underused asset\nthat could be exploited with task-specific, fine-tuned biomedical language\nrepresentation models, improving patient care and management. For such\nspecialized domains, previous research has shown that fine-tuning models\nstemming from broad-coverage checkpoints can largely benefit additional\ntraining rounds over large-scale in-domain resources. However, these resources\nare often unreachable for less-resourced languages like Italian, preventing\nlocal medical institutions to employ in-domain adaptation. In order to reduce\nthis gap, our work investigates two accessible approaches to derive biomedical\nlanguage models in languages other than English, taking Italian as a concrete\nuse-case: one based on neural machine translation of English resources,\nfavoring quantity over quality; the other based on a high-grade, narrow-scoped\ncorpus natively written in Italian, thus preferring quality over quantity. Our\nstudy shows that data quantity is a harder constraint than data quality for\nbiomedical adaptation, but the concatenation of high-quality data can improve\nmodel performance even when dealing with relatively size-limited corpora. The\nmodels published from our investigations have the potential to unlock important\nresearch opportunities for Italian hospitals and academia. Finally, the set of\nlessons learned from the study constitutes valuable insights towards a solution\nto build biomedical language models that are generalizable to other\nless-resourced languages and different domain settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buonocore_T/0/1/0/all/0/1\">Tommaso Mario Buonocore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crema_C/0/1/0/all/0/1\">Claudio Crema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redolfi_A/0/1/0/all/0/1\">Alberto Redolfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellazzi_R/0/1/0/all/0/1\">Riccardo Bellazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parimbelli_E/0/1/0/all/0/1\">Enea Parimbelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records. (arXiv:2301.07695v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.07695","description":"<p>We present a new text-to-SQL dataset for electronic health records (EHRs).\nThe utterances were collected from 222 hospital staff members, including\nphysicians, nurses, and insurance review and health records teams. To construct\nthe QA dataset on structured EHR data, we conducted a poll at a university\nhospital and used the responses to create seed questions. We then manually\nlinked these questions to two open-source EHR databases, MIMIC-III and eICU,\nand included various time expressions and held-out unanswerable questions in\nthe dataset, which were also collected from the poll. Our dataset poses a\nunique set of challenges: the model needs to 1) generate SQL queries that\nreflect a wide range of needs in the hospital, including simple retrieval and\ncomplex operations such as calculating survival rate, 2) understand various\ntime expressions to answer time-sensitive questions in healthcare, and 3)\ndistinguish whether a given question is answerable or unanswerable. We believe\nour dataset, EHRSQL, can serve as a practical benchmark for developing and\nassessing QA models on structured EHR data and take a step further towards\nbridging the gap between text-to-SQL research and its real-life deployment in\nhealthcare. EHRSQL is available at https://github.com/glee4810/EHRSQL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gyubok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1\">Hyeonji Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1\">Seongsu Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Yeonsu Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1\">Woncheol Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Seongjun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong-Yeup Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Manifold Learning for Reading Comprehension and Logical Reasoning Tasks with Polytuplet Loss. (arXiv:2304.01046v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01046","description":"<p>The current trend in developing machine learning models for reading\ncomprehension and logical reasoning tasks is focused on improving the models'\nabilities to understand and utilize logical rules. This work focuses on\nproviding a novel loss function and accompanying model architecture that has\nmore interpretable components than some other models by representing a common\nstrategy employed by humans when given reading comprehension and logical\nreasoning tasks. This strategy involves emphasizing relative accuracy over\nabsolute accuracy and can theoretically produce the correct answer without full\nknowledge of the information required to solve the question. We examine the\neffectiveness of applying such a strategy to train transfer learning models to\nsolve reading comprehension and logical reasoning questions. The models were\nevaluated on the ReClor dataset, a challenging reading comprehension and\nlogical reasoning benchmark. We propose the polytuplet loss function, an\nextension of the triplet loss function, to ensure prioritization of learning\nthe relative correctness of answer choices over learning the true accuracy of\neach choice. Our results indicate that models employing polytuplet loss\noutperform existing baseline models. Although polytuplet loss is a promising\nalternative to other contrastive loss functions, further research is required\nto quantify the benefits it may present.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jeffrey Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_I/0/1/0/all/0/1\">Ivan Rodriguez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective. (arXiv:2305.15408v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.15408","description":"<p>Recent studies have discovered that Chain-of-Thought prompting (CoT) can\ndramatically improve the performance of Large Language Models (LLMs),\nparticularly when dealing with complex tasks involving mathematics or\nreasoning. Despite the enormous empirical success, the underlying mechanisms\nbehind CoT and how it unlocks the potential of LLMs remain elusive. In this\npaper, we take a first step towards theoretically answering these questions.\nSpecifically, we examine the expressivity of LLMs with CoT in solving\nfundamental mathematical and decision-making problems. We start by giving an\nimpossibility result showing that bounded-depth Transformers are unable to\ndirectly produce correct answers for basic arithmetic/equation tasks unless the\nmodel size grows super-polynomially with respect to the input length. In\ncontrast, we then prove by construction that autoregressive Transformers of\nconstant size suffice to solve both tasks by generating CoT derivations using a\ncommonly-used math language format. Moreover, we show LLMs with CoT are capable\nof solving a general class of decision-making problems known as Dynamic\nProgramming, thus justifying its power in tackling complex real-world tasks.\nFinally, extensive experiments on four tasks show that, while Transformers\nalways fail to predict the answers directly, they can consistently learn to\ngenerate correct solutions step-by-step given sufficient CoT demonstrations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_G/0/1/0/all/0/1\">Guhao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bohang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuntian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Haotian Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DC CoMix TTS: An End-to-End Expressive TTS with Discrete Code Collaborated with Mixer. (arXiv:2305.19567v4 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2305.19567","description":"<p>Despite the huge successes made in neutral TTS, content-leakage remains a\nchallenge. In this paper, we propose a new input representation and simple\narchitecture to achieve improved prosody modeling. Inspired by the recent\nsuccess in the use of discrete code in TTS, we introduce discrete code to the\ninput of the reference encoder. Specifically, we leverage the vector quantizer\nfrom the audio compression model to exploit the diverse acoustic information it\nhas already been trained on. In addition, we apply the modified MLP-Mixer to\nthe reference encoder, making the architecture lighter. As a result, we train\nthe prosody transfer TTS in an end-to-end manner. We prove the effectiveness of\nour method through both subjective and objective evaluations. We demonstrate\nthat the reference encoder learns better speaker-independent prosody when\ndiscrete code is utilized as input in the experiments. In addition, we obtain\ncomparable results even when fewer parameters are inputted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yerin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_M/0/1/0/all/0/1\">Myoung-Wan Koo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stance Prediction and Analysis of Twitter data : A case study of Ghana 2020 Presidential Elections. (arXiv:2306.14203v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.14203","description":"<p>On December 7, 2020, Ghanaians participated in the polls to determine their\npresident for the next four years. To gain insights from this presidential\nelection, we conducted stance analysis (which is not always equivalent to\nsentiment analysis) to understand how Twitter, a popular social media platform,\nreflected the opinions of its users regarding the two main presidential\ncandidates. We collected a total of 99,356 tweets using the Twitter API\n(Tweepy) and manually annotated 3,090 tweets into three classes: Against,\nNeutral, and Support. We then performed preprocessing on the tweets. The\nresulting dataset was evaluated using two lexicon-based approaches, VADER and\nTextBlob, as well as five supervised machine learning-based approaches: Support\nVector Machine (SVM), Logistic Regression (LR), Multinomial Na\\\"ive Bayes\n(MNB), Stochastic Gradient Descent (SGD), and Random Forest (RF), based on\nmetrics such as accuracy, precision, recall, and F1-score. The best performance\nwas achieved by Logistic Regression with an accuracy of 71.13%. We utilized\nLogistic Regression to classify all the extracted tweets and subsequently\nconducted an analysis and discussion of the results. For access to our data and\ncode, please visit:\nhttps://github.com/ShesterG/Stance-Detection-Ghana-2020-Elections.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gueuwou_S/0/1/0/all/0/1\">Shester Gueuwou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gyening_R/0/1/0/all/0/1\">Rose-Mary Owusuaa Mensah Gyening</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncovering Political Hate Speech During Indian Election Campaign: A New Low-Resource Dataset and Baselines. (arXiv:2306.14764v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.14764","description":"<p>The detection of hate speech in political discourse is a critical issue, and\nthis becomes even more challenging in low-resource languages. To address this\nissue, we introduce a new dataset named IEHate, which contains 11,457 manually\nannotated Hindi tweets related to the Indian Assembly Election Campaign from\nNovember 1, 2021, to March 9, 2022. We performed a detailed analysis of the\ndataset, focusing on the prevalence of hate speech in political communication\nand the different forms of hateful language used. Additionally, we benchmark\nthe dataset using a range of machine learning, deep learning, and\ntransformer-based algorithms. Our experiments reveal that the performance of\nthese models can be further improved, highlighting the need for more advanced\ntechniques for hate speech detection in low-resource languages. In particular,\nthe relatively higher score of human evaluation over algorithms emphasizes the\nimportance of utilizing both human and automated approaches for effective hate\nspeech moderation. Our IEHate dataset can serve as a valuable resource for\nresearchers and practitioners working on developing and evaluating hate speech\ndetection techniques in low-resource languages. Overall, our work underscores\nthe importance of addressing the challenges of identifying and mitigating hate\nspeech in political discourse, particularly in the context of low-resource\nlanguages. The dataset and resources for this work are made available at\nhttps://github.com/Farhan-jafri/Indian-Election.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jafri_F/0/1/0/all/0/1\">Farhan Ahmad Jafri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_M/0/1/0/all/0/1\">Mohammad Aman Siddiqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thapa_S/0/1/0/all/0/1\">Surendrabikram Thapa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rauniyar_K/0/1/0/all/0/1\">Kritesh Rauniyar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naseem_U/0/1/0/all/0/1\">Usman Naseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razzak_I/0/1/0/all/0/1\">Imran Razzak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Cross-Domain Behaviors of BERT in Review Understanding. (arXiv:2306.15123v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.15123","description":"<p>Review score prediction requires review text understanding, a critical\nreal-world application of natural language processing. Due to dissimilar text\ndomains in product reviews, a common practice is fine-tuning BERT models upon\nreviews of differing domains. However, there has not yet been an empirical\nstudy of cross-domain behaviors of BERT models in the various tasks of product\nreview understanding. In this project, we investigate text classification BERT\nmodels fine-tuned on single-domain and multi-domain Amazon review data. In our\nfindings, though single-domain models achieved marginally improved performance\non their corresponding domain compared to multi-domain models, multi-domain\nmodels outperformed single-domain models when evaluated on multi-domain data,\nsingle-domain data the single-domain model was not fine-tuned on, and on\naverage when considering all tests. Though slight increases in accuracy can be\nachieved through single-domain model fine-tuning, computational resources and\ncosts can be reduced by utilizing multi-domain models that perform well across\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1\">Albert Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MindDial: Belief Dynamics Tracking with Theory-of-Mind Modeling for Situated Neural Dialogue Generation. (arXiv:2306.15253v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.15253","description":"<p>Humans talk in free-form while negotiating the expressed meanings or common\nground. Despite the impressive conversational abilities of the large generative\nlanguage models, they do not consider the individual differences in contextual\nunderstanding in a shared situated environment. In this work, we propose\nMindDial, a novel conversational framework that can generate situated free-form\nresponses to negotiate common ground. We design an explicit mind module that\ncan track three-level beliefs -- the speaker's belief, the speaker's prediction\nof the listener's belief, and the common belief based on the gap between the\nfirst two. Then the speaking act classification head will decide to continue to\ntalk, end this turn, or take task-related action. We augment a common ground\nalignment dataset MutualFriend with belief dynamics annotation, of which the\ngoal is to find a single mutual friend based on the free chat between two\nagents. Experiments show that our model with mental state modeling can resemble\nhuman responses when aligning common ground meanwhile mimic the natural human\nconversation flow. The ablation study further validates the third-level common\nbelief can aggregate information of the first and second-order beliefs and\nalign common ground more efficiently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shuwen Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zilong Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-Speaker: A Large-Scale Multi-Device, Multi-Distance, and Multi-Dialect Corpus for Speech Representation Disentanglement. (arXiv:2306.15354v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.15354","description":"<p>Disentangling uncorrelated information in speech utterances is a crucial\nresearch topic within speech community. Different speech-related tasks focus on\nextracting distinct speech representations while minimizing the affects of\nother uncorrelated information. We present a large-scale speech corpus to\nfacilitate the research of speech representation disentanglement. 3D-Speaker\ncontains over 10,000 speakers, each of whom are simultaneously recorded by\nmultiple Devices, locating at different Distances, and some speakers are\nspeaking multiple Dialects. The controlled combinations of multi-dimensional\naudio data yield a matrix of a diverse blend of speech representation\nentanglement, thereby motivating intriguing methods to untangle them. The\nmulti-domain nature of 3D-Speaker also makes it a suitable resource to evaluate\nlarge universal speech models and experiment methods of out-of-domain learning\nand self-supervised learning. https://3dspeaker.github.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Siqi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Luyao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yafeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extending Context Window of Large Language Models via Positional Interpolation. (arXiv:2306.15595v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.15595","description":"<p>We present Position Interpolation (PI) that extends the context window sizes\nof RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal\nfine-tuning (within 1000 steps), while demonstrating strong empirical results\non various tasks that require long context, including passkey retrieval,\nlanguage modeling, and long document summarization from LLaMA 7B to 65B.\nMeanwhile, the extended model by Position Interpolation preserve quality\nrelatively well on tasks within its original context window. To achieve this\ngoal, Position Interpolation linearly down-scales the input position indices to\nmatch the original context window size, rather than extrapolating beyond the\ntrained context length which may lead to catastrophically high attention scores\nthat completely ruin the self-attention mechanism. Our theoretical study shows\nthat the upper bound of interpolation is at least $\\sim 600 \\times$ smaller\nthan that of extrapolation, further demonstrating its stability. Models\nextended via Position Interpolation retain its original architecture and can\nreuse most pre-existing optimization and infrastructure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shouyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_S/0/1/0/all/0/1\">Sherman Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liangjian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Annotation of Direct Speech in Written French Narratives. (arXiv:2306.15634v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.15634","description":"<p>The automatic annotation of direct speech (AADS) in written text has been\noften used in computational narrative understanding. Methods based on either\nrules or deep neural networks have been explored, in particular for English or\nGerman languages. Yet, for French, our target language, not many works exist.\nOur goal is to create a unified framework to design and evaluate AADS models in\nFrench. For this, we consolidated the largest-to-date French narrative dataset\nannotated with DS per word; we adapted various baselines for sequence labelling\nor from AADS in other languages; and we designed and conducted an extensive\nevaluation focused on generalisation. Results show that the task still requires\nsubstantial efforts and emphasise characteristics of each baseline. Although\nthis framework could be improved, it is a step further to encourage more\nresearch on the topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Durandard_N/0/1/0/all/0/1\">No&#xe9; Durandard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Viet-Anh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michel_G/0/1/0/all/0/1\">Gaspard Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Epure_E/0/1/0/all/0/1\">Elena V. Epure</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Longitudinal Multi-modal Dataset for Dementia Monitoring and Diagnosis. (arXiv:2109.01537v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2109.01537","description":"<p>Dementia is a family of neurogenerative conditions affecting memory and\ncognition in an increasing number of individuals in our globally aging\npopulation. Automated analysis of language, speech and paralinguistic\nindicators have been gaining popularity as potential indicators of cognitive\ndecline. Here we propose a novel longitudinal multi-modal dataset collected\nfrom people with mild dementia and age matched controls over a period of\nseveral months in a natural setting. The multi-modal data consists of spoken\nconversations, a subset of which are transcribed, as well as typed and written\nthoughts and associated extra-linguistic information such as pen strokes and\nkeystrokes. We describe the dataset in detail and proceed to focus on a task\nusing the speech modality. The latter involves distinguishing controls from\npeople with dementia by exploiting the longitudinal nature of the data. Our\nexperiments showed significant differences in how the speech varied from\nsession to session in the control and dementia groups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gkoumas_D/0/1/0/all/0/1\">Dimitris Gkoumas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsakalidis_A/0/1/0/all/0/1\">Adam Tsakalidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolters_M/0/1/0/all/0/1\">Maria Wolters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1\">Matthew Purver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liakata_M/0/1/0/all/0/1\">Maria Liakata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-06-28T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}