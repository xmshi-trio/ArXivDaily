{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-06-30T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Pre-Training Multi-Modal Dense Retrievers for Outside-Knowledge Visual Question Answering. (arXiv:2306.16478v1 [cs.IR])","link":"http://arxiv.org/abs/2306.16478","description":"<p>This paper studies a category of visual question answering tasks, in which\naccessing external knowledge is necessary for answering the questions. This\ncategory is called outside-knowledge visual question answering (OK-VQA). A\nmajor step in developing OK-VQA systems is to retrieve relevant documents for\nthe given multi-modal query. Current state-of-the-art asymmetric dense\nretrieval model for this task uses an architecture with a multi-modal query\nencoder and a uni-modal document encoder. Such an architecture requires a large\namount of training data for effective performance. We propose an automatic data\ngeneration pipeline for pre-training passage retrieval models for OK-VQA tasks.\nThe proposed approach leads to 26.9% Precision@5 improvements compared to the\ncurrent state-of-the-art asymmetric architecture. Additionally, the proposed\npre-training approach exhibits a good ability in zero-shot retrieval scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salemi_A/0/1/0/all/0/1\">Alireza Salemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafiee_M/0/1/0/all/0/1\">Mahta Rafiee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1\">Hamed Zamani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ICSVR: Investigating Compositional and Semantic Understanding in Video Retrieval Models. (arXiv:2306.16533v1 [cs.CV])","link":"http://arxiv.org/abs/2306.16533","description":"<p>Video retrieval (VR) involves retrieving the ground truth video from the\nvideo database given a text caption or vice-versa. The two important components\nof compositionality: objects \\&amp; attributes and actions are joined using correct\nsemantics to form a proper text query. These components (objects \\&amp; attributes,\nactions and semantics) each play an important role to help distinguish among\nvideos and retrieve the correct ground truth video. However, it is unclear what\nis the effect of these components on the video retrieval performance. We\ntherefore, conduct a systematic study to evaluate the compositional and\nsemantic understanding of video retrieval models on standard benchmarks such as\nMSRVTT, MSVD and DIDEMO. The study is performed on two categories of video\nretrieval models: (i) which are pre-trained on video-text pairs and fine-tuned\non downstream video retrieval datasets (Eg. Frozen-in-Time, Violet, MCQ etc.)\n(ii) which adapt pre-trained image-text representations like CLIP for video\nretrieval (Eg. CLIP4Clip, XCLIP, CLIP2Video etc.). Our experiments reveal that\nactions and semantics play a minor role compared to objects \\&amp; attributes in\nvideo understanding. Moreover, video retrieval models that use pre-trained\nimage-text representations (CLIP) have better semantic and compositional\nunderstanding as compared to models pre-trained on video-text data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madasu_A/0/1/0/all/0/1\">Avinash Madasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1\">Vasudev Lal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision. (arXiv:2306.16564v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16564","description":"<p>Large language models (LLMs) have demonstrated remarkable capabilities out of\nbox for a wide range of applications, yet accuracy still remains a major growth\narea, especially in mission-critical domains such as biomedicine. An effective\nmethod to calibrate the confidence level on LLM responses is essential to\nautomatically detect errors and facilitate human-in-the-loop verification. An\nimportant source of calibration signals stems from expert-stipulated\nprogrammatic supervision, which is often available at low cost but has its own\nlimitations such as noise and coverage. In this paper, we introduce a Pareto\noptimal self-supervision framework that can leverage available programmatic\nsupervision to systematically calibrate LLM responses by producing a risk score\nfor every response, without any additional manual efforts. This is accomplished\nby learning a harmonizer model to align LLM output with other available\nsupervision sources, which would assign higher risk scores to more uncertain\nLLM responses and facilitate error correction. Experiments on standard relation\nextraction tasks in biomedical and general domains demonstrate the promise of\nthis approach, with our proposed risk scores highly correlated with the real\nerror rate of LLMs. For the most uncertain test instances, dynamic prompting\nbased on our proposed risk scores results in significant accuracy improvement\nfor off-the-shelf LLMs, boosting GPT-3 results past state-of-the-art (SOTA)\nweak supervision and GPT-4 results past SOTA supervised results on challenging\nevaluation datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Theodore Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preston_J/0/1/0/all/0/1\">J. Samuel Preston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs. (arXiv:2306.16601v1 [cs.LG])","link":"http://arxiv.org/abs/2306.16601","description":"<p>In recent years, Transformer-based language models have become the standard\napproach for natural language processing tasks. However, stringent throughput\nand latency requirements in industrial applications are limiting their\nadoption. To mitigate the gap, model compression techniques such as structured\npruning are being used to improve inference efficiency. However, most existing\nneural network inference runtimes lack adequate support for structured\nsparsity. In this paper, we propose an efficient sparse deep learning inference\nsoftware stack for Transformer-based language models where the weights are\npruned with constant block size. Our sparse software accelerator leverages\nIntel Deep Learning Boost to maximize the performance of sparse matrix - dense\nmatrix multiplication (commonly abbreviated as SpMM) on CPUs. Our SpMM kernel\noutperforms the existing sparse libraries (oneMKL, TVM, and LIBXSMM) by an\norder of magnitude on a wide range of GEMM shapes under 5 representative\nsparsity ratios (70%, 75%, 80%, 85%, 90%). Moreover, our SpMM kernel shows up\nto 5x speedup over dense GEMM kernel of oneDNN, a well-optimized dense library\nwidely used in industry. We apply our sparse accelerator on widely-used\nTransformer-based language models including Bert-Mini, DistilBERT, Bert-Base,\nand BERT-Large. Our sparse inference software shows up to 1.5x speedup over\nNeural Magic's Deepsparse under same configurations on Xeon on Amazon Web\nServices under proxy production latency constraints. We also compare our\nsolution with two framework-based inference solutions, ONNX Runtime and\nPyTorch, and demonstrate up to 37x speedup over ONNX Runtime and 345x over\nPyTorch on Xeon under the latency constraints. All the source code is publicly\navailable on Github: https://github.com/intel/intel-extension-for-transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Haihao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Hengyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafrir_O/0/1/0/all/0/1\">Ofir Zafrir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hanwen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boudoukh_G/0/1/0/all/0/1\">Guy Boudoukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasserblat_M/0/1/0/all/0/1\">Moshe Wasserblat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?. (arXiv:2306.16636v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16636","description":"<p>We present the Chinese Elementary School Math Word Problems (CMATH) dataset,\ncomprising 1.7k elementary school-level math word problems with detailed\nannotations, source from actual Chinese workbooks and exams. This dataset aims\nto provide a benchmark tool for assessing the following question: to what grade\nlevel of elementary school math do the abilities of popular large language\nmodels (LLMs) correspond? We evaluate a variety of popular LLMs, including both\ncommercial and open-source options, and discover that only GPT-4 achieves\nsuccess (accuracy $\\geq$ 60\\%) across all six elementary school grades, while\nother models falter at different grade levels. Furthermore, we assess the\nrobustness of several top-performing LLMs by augmenting the original problems\nin the CMATH dataset with distracting information. Our findings reveal that\nGPT-4 is able to maintains robustness, while other model fail. We anticipate\nthat our study will expose limitations in LLMs' arithmetic and reasoning\ncapabilities, and promote their ongoing development and advancement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1\">Tianwen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_J/0/1/0/all/0/1\">Jian Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Shuang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A negation detection assessment of GPTs: analysis with the xNot360 dataset. (arXiv:2306.16638v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16638","description":"<p>Negation is a fundamental aspect of natural language, playing a critical role\nin communication and comprehension. Our study assesses the negation detection\nperformance of Generative Pre-trained Transformer (GPT) models, specifically\nGPT-2, GPT-3, GPT-3.5, and GPT-4. We focus on the identification of negation in\nnatural language using a zero-shot prediction approach applied to our custom\nxNot360 dataset. Our approach examines sentence pairs labeled to indicate\nwhether the second sentence negates the first. Our findings expose a\nconsiderable performance disparity among the GPT models, with GPT-4 surpassing\nits counterparts and GPT-3.5 displaying a marked performance reduction. The\noverall proficiency of the GPT models in negation detection remains relatively\nmodest, indicating that this task pushes the boundaries of their natural\nlanguage understanding capabilities. We not only highlight the constraints of\nGPT models in handling negation but also emphasize the importance of logical\nreliability in high-stakes domains such as healthcare, science, and law.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1\">Randy Goebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1\">Francesca Toni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stathis_K/0/1/0/all/0/1\">Kostas Stathis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satoh_K/0/1/0/all/0/1\">Ken Satoh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistic Linguistic Knowledge and Token-level Text Augmentation. (arXiv:2306.16644v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16644","description":"<p>This paper investigates the effectiveness of token-level text augmentation\nand the role of probabilistic linguistic knowledge within a\nlinguistically-motivated evaluation context. Two text augmentation programs,\nREDA and REDA$_{NG}$, were developed, both implementing five token-level text\nediting operations: Synonym Replacement (SR), Random Swap (RS), Random\nInsertion (RI), Random Deletion (RD), and Random Mix (RM). REDA$_{NG}$\nleverages pretrained $n$-gram language models to select the most likely\naugmented texts from REDA's output. Comprehensive and fine-grained experiments\nwere conducted on a binary question matching classification task in both\nChinese and English. The results strongly refute the general effectiveness of\nthe five token-level text augmentation techniques under investigation, whether\napplied together or separately, and irrespective of various common\nclassification model types used, including transformers. Furthermore, the role\nof probabilistic linguistic knowledge is found to be minimal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhengxiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZeroGen: Zero-shot Multimodal Controllable Text Generation with Multiple Oracles. (arXiv:2306.16649v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16649","description":"<p>Automatically generating textual content with desired attributes is an\nambitious task that people have pursued long. Existing works have made a series\nof progress in incorporating unimodal controls into language models (LMs),\nwhereas how to generate controllable sentences with multimodal signals and high\nefficiency remains an open question. To tackle the puzzle, we propose a new\nparadigm of zero-shot controllable text generation with multimodal signals\n(\\textsc{ZeroGen}). Specifically, \\textsc{ZeroGen} leverages controls of text\nand image successively from token-level to sentence-level and maps them into a\nunified probability space at decoding, which customizes the LM outputs by\nweighted addition without extra training. To achieve better inter-modal\ntrade-offs, we further introduce an effective dynamic weighting mechanism to\nregulate all control weights. Moreover, we conduct substantial experiments to\nprobe the relationship of being in-depth or in-width between signals from\ndistinct modalities. Encouraging empirical results on three downstream tasks\nshow that \\textsc{ZeroGen} not only outperforms its counterparts on captioning\ntasks by a large margin but also shows great potential in multimodal news\ngeneration with a higher degree of control. Our code will be released at\nhttps://github.com/ImKeTT/ZeroGen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_H/0/1/0/all/0/1\">Haoqin Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bowen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xianfeng Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-source Semantic Graph-based Multimodal Sarcasm Explanation Generation. (arXiv:2306.16650v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16650","description":"<p>Multimodal Sarcasm Explanation (MuSE) is a new yet challenging task, which\naims to generate a natural language sentence for a multimodal social post (an\nimage as well as its caption) to explain why it contains sarcasm. Although the\nexisting pioneer study has achieved great success with the BART backbone, it\noverlooks the gap between the visual feature space and the decoder semantic\nspace, the object-level metadata of the image, as well as the potential\nexternal knowledge. To solve these limitations, in this work, we propose a\nnovel mulTi-source sEmantic grAph-based Multimodal sarcasm explanation scheme,\nnamed TEAM. In particular, TEAM extracts the object-level semantic meta-data\ninstead of the traditional global visual features from the input image.\nMeanwhile, TEAM resorts to ConceptNet to obtain the external related knowledge\nconcepts for the input text and the extracted object meta-data. Thereafter,\nTEAM introduces a multi-source semantic graph that comprehensively characterize\nthe multi-source (i.e., caption, object meta-data, external knowledge) semantic\nrelations to facilitate the sarcasm reasoning. Extensive experiments on a\npublic released dataset MORE verify the superiority of our model over\ncutting-edge methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liqiang Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xuemeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_K/0/1/0/all/0/1\">Kun Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1\">Mengzhao Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Speech Recognition of Non-Native Child Speech for Language Learning Applications. (arXiv:2306.16710v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16710","description":"<p>Voicebots have provided a new avenue for supporting the development of\nlanguage skills, particularly within the context of second language learning.\nVoicebots, though, have largely been geared towards native adult speakers. We\nsought to assess the performance of two state-of-the-art ASR systems,\nWav2Vec2.0 and Whisper AI, with a view to developing a voicebot that can\nsupport children acquiring a foreign language. We evaluated their performance\non read and extemporaneous speech of native and non-native Dutch children. We\nalso investigated the utility of using ASR technology to provide insight into\nthe children's pronunciation and fluency. The results show that recent,\npre-trained ASR transformer-based models achieve acceptable performance from\nwhich detailed feedback on phoneme pronunciation quality can be extracted,\ndespite the challenging nature of child and non-native speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wills_S/0/1/0/all/0/1\">Simone Wills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tejedor_Garcia_C/0/1/0/all/0/1\">Cristian Tejedor-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiarini_C/0/1/0/all/0/1\">Catia Cucchiarini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strik_H/0/1/0/all/0/1\">Helmer Strik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Paraphrastic Robustness in Textual Entailment Models. (arXiv:2306.16722v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16722","description":"<p>We present PaRTE, a collection of 1,126 pairs of Recognizing Textual\nEntailment (RTE) examples to evaluate whether models are robust to\nparaphrasing. We posit that if RTE models understand language, their\npredictions should be consistent across inputs that share the same meaning. We\nuse the evaluation set to determine if RTE models' predictions change when\nexamples are paraphrased. In our experiments, contemporary models change their\npredictions on 8-16\\% of paraphrased examples, indicating that there is still\nroom for improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Verma_D/0/1/0/all/0/1\">Dhruv Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_Y/0/1/0/all/0/1\">Yash Kumar Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Shreyashee Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poliak_A/0/1/0/all/0/1\">Adam Poliak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Language Representation for Question Answering over Text, Tables, and Images. (arXiv:2306.16762v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16762","description":"<p>When trying to answer complex questions, people often rely on multiple\nsources of information, such as visual, textual, and tabular data. Previous\napproaches to this problem have focused on designing input features or model\nstructure in the multi-modal space, which is inflexible for cross-modal\nreasoning or data-efficient training. In this paper, we call for an alternative\nparadigm, which transforms the images and tables into unified language\nrepresentations, so that we can simplify the task into a simpler textual QA\nproblem that can be solved using three steps: retrieval, ranking, and\ngeneration, all within a language space. This idea takes advantage of the power\nof pre-trained language models and is implemented in a framework called Solar.\nOur experimental results show that Solar outperforms all existing methods by\n10.6-32.3 pts on two datasets, MultimodalQA and MMCoQA, across ten different\nmetrics. Additionally, Solar achieves the best performance on the WebQA\nleaderboard\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bowen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Cheng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialoGPS: Dialogue Path Sampling in Continuous Semantic Space for Data Augmentation in Multi-Turn Conversations. (arXiv:2306.16770v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16770","description":"<p>In open-domain dialogue generation tasks, contexts and responses in most\ndatasets are one-to-one mapped, violating an important many-to-many\ncharacteristic: a context leads to various responses, and a response answers\nmultiple contexts. Without such patterns, models poorly generalize and prefer\nresponding safely. Many attempts have been made in either multi-turn settings\nfrom a one-to-many perspective or in a many-to-many perspective but limited to\nsingle-turn settings. The major challenge to many-to-many augment multi-turn\ndialogues is that discretely replacing each turn with semantic similarity\nbreaks fragile context coherence. In this paper, we propose DialoGue Path\nSampling (DialoGPS) method in continuous semantic space, the first many-to-many\naugmentation method for multi-turn dialogues. Specifically, we map a dialogue\nto our extended Brownian Bridge, a special Gaussian process. We sample latent\nvariables to form coherent dialogue paths in the continuous space. A dialogue\npath corresponds to a new multi-turn dialogue and is used as augmented training\ndata. We show the effect of DialoGPS with both automatic and human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lv_A/0/1/0/all/0/1\">Ang Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuhan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages. (arXiv:2306.16774v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16774","description":"<p>Vision-Language Pre-training (VLP) has advanced the performance of many\nvision-language tasks, such as image-text retrieval, visual entailment, and\nvisual reasoning. The pre-training mostly utilizes lexical databases and image\nqueries in English. Previous work has demonstrated that the pre-training in\nEnglish does not transfer well to other languages in a zero-shot setting.\nHowever, multilingual pre-trained language models (MPLM) have excelled at a\nvariety of single-modal language tasks. In this paper, we propose a simple yet\nefficient approach to adapt VLP to unseen languages using MPLM. We utilize a\ncross-lingual contextualized token embeddings alignment approach to train text\nencoders for non-English languages. Our approach does not require image input\nand primarily uses machine translation, eliminating the need for target\nlanguage data. Our evaluation across three distinct tasks (image-text\nretrieval, visual entailment, and natural language visual reasoning)\ndemonstrates that this approach outperforms the state-of-the-art multilingual\nvision-language models without requiring large parallel corpora. Our code is\navailable at https://github.com/Yasminekaroui/CliCoTea.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karoui_Y/0/1/0/all/0/1\">Yasmine Karoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lebret_R/0/1/0/all/0/1\">R&#xe9;mi Lebret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foroutan_N/0/1/0/all/0/1\">Negar Foroutan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aberer_K/0/1/0/all/0/1\">Karl Aberer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Large Language Model Capabilities for Conditional Generation. (arXiv:2306.16793v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16793","description":"<p>Pre-trained large language models (PLMs) underlie most new developments in\nnatural language processing. They have shifted the field from\napplication-specific model pipelines to a single model that is adapted to a\nwide range of tasks. Autoregressive PLMs like GPT-3 or PaLM, alongside\ntechniques like few-shot learning, have additionally shifted the output\nmodality to generation instead of classification or regression. Despite their\nubiquitous use, the generation quality of language models is rarely evaluated\nwhen these models are introduced. Additionally, it is unclear how existing\ngeneration tasks--while they can be used to compare systems at a high\nlevel--relate to the real world use cases for which people have been adopting\nthem. In this work, we discuss how to adapt existing application-specific\ngeneration benchmarks to PLMs and provide an in-depth, empirical study of the\nlimitations and capabilities of PLMs in natural language generation tasks along\ndimensions such as scale, architecture, input and output language. Our results\nshow that PLMs differ in their applicability to different data regimes and\ntheir generalization to multiple languages and inform which PLMs to use for a\ngiven generation task setup. We share best practices to be taken into\nconsideration when benchmarking generation capabilities during the development\nof upcoming PLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maynez_J/0/1/0/all/0/1\">Joshua Maynez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1\">Priyanka Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIPAG: Towards Generator-Free Text-to-Image Generation. (arXiv:2306.16805v1 [cs.CV])","link":"http://arxiv.org/abs/2306.16805","description":"<p>Perceptually Aligned Gradients (PAG) refer to an intriguing property observed\nin robust image classification models, wherein their input gradients align with\nhuman perception and pose semantic meanings. While this phenomenon has gained\nsignificant research attention, it was solely studied in the context of\nunimodal vision-only architectures. In this work, we extend the study of PAG to\nVision-Language architectures, which form the foundations for diverse\nimage-text tasks and applications. Through an adversarial robustification\nfinetuning of CLIP, we demonstrate that robust Vision-Language models exhibit\nPAG in contrast to their vanilla counterparts. This work reveals the merits of\nCLIP with PAG (CLIPAG) in several vision-language generative tasks. Notably, we\nshow that seamlessly integrating CLIPAG in a \"plug-n-play\" manner leads to\nsubstantial improvements in vision-language generative applications.\nFurthermore, leveraging its PAG property, CLIPAG enables text-to-image\ngeneration without any generative model, which typically requires huge\ngenerators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganz_R/0/1/0/all/0/1\">Roy Ganz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elad_M/0/1/0/all/0/1\">Michael Elad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Formal Perspective on Byte-Pair Encoding. (arXiv:2306.16837v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16837","description":"<p>Byte-Pair Encoding (BPE) is a popular algorithm used for tokenizing data in\nNLP, despite being devised initially as a compression method. BPE appears to be\na greedy algorithm at face value, but the underlying optimization problem that\nBPE seeks to solve has not yet been laid down. We formalize BPE as a\ncombinatorial optimization problem. Via submodular functions, we prove that the\niterative greedy version is a\n$\\frac{1}{{\\sigma(\\boldsymbol{\\mu}^\\star)}}(1-e^{-{\\sigma(\\boldsymbol{\\mu}^\\star)}})$-approximation\nof an optimal merge sequence, where ${\\sigma(\\boldsymbol{\\mu}^\\star)}$ is the\ntotal backward curvature with respect to the optimal merge sequence\n$\\boldsymbol{\\mu}^\\star$. Empirically the lower bound of the approximation is\n$\\approx 0.37$.\n</p>\n<p>We provide a faster implementation of BPE which improves the runtime\ncomplexity from $\\mathcal{O}\\left(N M\\right)$ to $\\mathcal{O}\\left(N \\log\nM\\right)$, where $N$ is the sequence length and $M$ is the merge count.\nFinally, we optimize the brute-force algorithm for optimal BPE using\nmemoization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zouhar_V/0/1/0/all/0/1\">Vil&#xe9;m Zouhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gastaldi_J/0/1/0/all/0/1\">Juan Luis Gastaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Li Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vieira_T/0/1/0/all/0/1\">Tim Vieira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tokenization and the Noiseless Channel. (arXiv:2306.16842v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16842","description":"<p>Subword tokenization is a key part of many NLP pipelines. However, little is\nknown about why some tokenizer and hyperparameter combinations lead to better\ndownstream model performance than others. We propose that good tokenizers lead\nto \\emph{efficient} channel usage, where the channel is the means by which some\ninput is conveyed to the model and efficiency can be quantified in\ninformation-theoretic terms as the ratio of the Shannon entropy to the maximum\npossible entropy of the token distribution. Yet, an optimal encoding according\nto Shannon entropy assigns extremely long codes to low-frequency tokens and\nvery short codes to high-frequency tokens. Defining efficiency in terms of\nR\\'enyi entropy, on the other hand, penalizes distributions with either very\nhigh or very low-frequency tokens. In machine translation, we find that across\nmultiple tokenizers, the R\\'enyi entropy with $\\alpha = 2.5$ has a very strong\ncorrelation with \\textsc{Bleu}: $0.78$ in comparison to just $-0.32$ for\ncompressed length.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zouhar_V/0/1/0/all/0/1\">Vil&#xe9;m Zouhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gastaldi_J/0/1/0/all/0/1\">Juan Luis Gastaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Li Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research. (arXiv:2306.16900v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16900","description":"<p>Many recent improvements in NLP stem from the development and use of large\npre-trained language models (PLMs) with billions of parameters. Large model\nsizes makes computational cost one of the main limiting factors for training\nand evaluating such models; and has raised severe concerns about the\nsustainability, reproducibility, and inclusiveness for researching PLMs. These\nconcerns are often based on personal experiences and observations. However,\nthere had not been any large-scale surveys that investigate them. In this work,\nwe provide a first attempt to quantify these concerns regarding three topics,\nnamely, environmental impact, equity, and impact on peer reviewing. By\nconducting a survey with 312 participants from the NLP community, we capture\nexisting (dis)parities between different and within groups with respect to\nseniority, academia, and industry; and their impact on the peer reviewing\nprocess. For each topic, we provide an analysis and devise recommendations to\nmitigate found disparities, some of which already successfully implemented.\nFinally, we discuss additional concerns raised by many participants in\nfree-text responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Ji-Ung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puerto_H/0/1/0/all/0/1\">Haritz Puerto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aken_B/0/1/0/all/0/1\">Betty van Aken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arase_Y/0/1/0/all/0/1\">Yuki Arase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forde_J/0/1/0/all/0/1\">Jessica Zosa Forde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1\">Leon Derczynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruckle_A/0/1/0/all/0/1\">Andreas R&#xfc;ckl&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1\">Emma Strubell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1\">Jesse Dodge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Cross-Utterance Context For ASR Decoding. (arXiv:2306.16903v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16903","description":"<p>While external language models (LMs) are often incorporated into the decoding\nstage of automated speech recognition systems, these models usually operate\nwith limited context. Cross utterance information has been shown to be\nbeneficial during second pass re-scoring, however this limits the hypothesis\nspace based on the local information available to the first pass LM. In this\nwork, we investigate the incorporation of long-context transformer LMs for\ncross-utterance decoding of acoustic models via beam search, and compare\nagainst results from n-best rescoring. Results demonstrate that beam search\nallows for an improved use of cross-utterance context. When evaluating on the\nlong-format dataset AMI, results show a 0.7\\% and 0.3\\% absolute reduction on\ndev and test sets compared to the single-utterance setting, with improvements\nwhen including up to 500 tokens of prior context. Evaluations are also provided\nfor Tedlium-1 with less significant improvements of around 0.1\\% absolute.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Flynn_R/0/1/0/all/0/1\">Robert Flynn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragni_A/0/1/0/all/0/1\">Anton Ragni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UMASS_BioNLP at MEDIQA-Chat 2023: Can LLMs generate high-quality synthetic note-oriented doctor-patient conversations?. (arXiv:2306.16931v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16931","description":"<p>This paper presents UMASS_BioNLP team participation in the MEDIQA-Chat 2023\nshared task for Task-A and Task-C. We focus especially on Task-C and propose a\nnovel LLMs cooperation system named a doctor-patient loop to generate\nhigh-quality conversation data sets. The experiment results demonstrate that\nour approaches yield reasonable performance as evaluated by automatic metrics\nsuch as ROUGE, medical concept recall, BLEU, and Self-BLEU. Furthermore, we\nconducted a comparative analysis between our proposed method and ChatGPT and\nGPT-4. This analysis also investigates the potential of utilizing cooperation\nLLMs to generate high-quality datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junda Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zonghai Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1\">Avijit Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osebe_S/0/1/0/all/0/1\">Samuel Osebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Music Hierarchies with a Graph-Based Neural Decoder. (arXiv:2306.16955v1 [cs.SD])","link":"http://arxiv.org/abs/2306.16955","description":"<p>This paper describes a data-driven framework to parse musical sequences into\ndependency trees, which are hierarchical structures used in music cognition\nresearch and music analysis. The parsing involves two steps. First, the input\nsequence is passed through a transformer encoder to enrich it with contextual\ninformation. Then, a classifier filters the graph of all possible dependency\narcs to produce the dependency tree. One major benefit of this system is that\nit can be easily integrated into modern deep-learning pipelines. Moreover,\nsince it does not rely on any particular symbolic grammar, it can consider\nmultiple musical features simultaneously, make use of sequential context\ninformation, and produce partial results for noisy inputs. We test our approach\non two datasets of musical trees -- time-span trees of monophonic note\nsequences and harmonic trees of jazz chord sequences -- and show that our\napproach outperforms previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Foscarin_F/0/1/0/all/0/1\">Francesco Foscarin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harasim_D/0/1/0/all/0/1\">Daniel Harasim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Widmer_G/0/1/0/all/0/1\">Gerhard Widmer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEMD-ABSA: A Multi-Element Multi-Domain Dataset for Aspect-Based Sentiment Analysis. (arXiv:2306.16956v1 [cs.CL])","link":"http://arxiv.org/abs/2306.16956","description":"<p>Aspect-based sentiment analysis is a long-standing research interest in the\nfield of opinion mining, and in recent years, researchers have gradually\nshifted their focus from simple ABSA subtasks to end-to-end multi-element ABSA\ntasks. However, the datasets currently used in the research are limited to\nindividual elements of specific tasks, usually focusing on in-domain settings,\nignoring implicit aspects and opinions, and with a small data scale. To address\nthese issues, we propose a large-scale Multi-Element Multi-Domain dataset\n(MEMD) that covers the four elements across five domains, including nearly\n20,000 review sentences and 30,000 quadruples annotated with explicit and\nimplicit aspects and opinions for ABSA research. Meanwhile, we evaluate\ngenerative and non-generative baselines on multiple ABSA subtasks under the\nopen domain setting, and the results show that open domain ABSA as well as\nmining implicit aspects and opinions remain ongoing challenges to be addressed.\nThe datasets are publicly released at \\url{https://github.com/NUSTM/MEMD-ABSA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hongjie Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_N/0/1/0/all/0/1\">Nan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zengzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qiming Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qiankun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Siwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shijie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianfei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_R/0/1/0/all/0/1\">Rui Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-Quality Automatic Voice Over with Accurate Alignment: Supervision through Self-Supervised Discrete Speech Units. (arXiv:2306.17005v1 [eess.AS])","link":"http://arxiv.org/abs/2306.17005","description":"<p>The goal of Automatic Voice Over (AVO) is to generate speech in sync with a\nsilent video given its text script. Recent AVO frameworks built upon\ntext-to-speech synthesis (TTS) have shown impressive results. However, the\ncurrent AVO learning objective of acoustic feature reconstruction brings in\nindirect supervision for inter-modal alignment learning, thus limiting the\nsynchronization performance and synthetic speech quality. To this end, we\npropose a novel AVO method leveraging the learning objective of self-supervised\ndiscrete speech unit prediction, which not only provides more direct\nsupervision for the alignment learning, but also alleviates the mismatch\nbetween the text-video context and acoustic features. Experimental results show\nthat our proposed method achieves remarkable lip-speech synchronization and\nhigh speech quality by outperforming baselines in both objective and subjective\nevaluations. Code and speech samples are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lu_J/0/1/0/all/0/1\">Junchen Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sisman_B/0/1/0/all/0/1\">Berrak Sisman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classifying Crime Types using Judgment Documents from Social Media. (arXiv:2306.17020v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17020","description":"<p>The task of determining crime types based on criminal behavior facts has\nbecome a very important and meaningful task in social science. But the problem\nfacing the field now is that the data samples themselves are unevenly\ndistributed, due to the nature of the crime itself. At the same time, data sets\nin the judicial field are less publicly available, and it is not practical to\nproduce large data sets for direct training. This article proposes a new\ntraining model to solve this problem through NLP processing methods. We first\npropose a Crime Fact Data Preprocessing Module (CFDPM), which can balance the\ndefects of uneven data set distribution by generating new samples. Then we use\na large open source dataset (CAIL-big) as our pretraining dataset and a small\ndataset collected by ourselves for Fine-tuning, giving it good generalization\nability to unfamiliar small datasets. At the same time, we use the improved\nBert model with dynamic masking to improve the model. Experiments show that the\nproposed method achieves state-of-the-art results on the present dataset. At\nthe same time, the effectiveness of module CFDPM is proved by experiments. This\narticle provides a valuable methodology contribution for classifying social\nscience texts such as criminal behaviors. Extensive experiments on public\nbenchmarks show that the proposed method achieves new state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoxuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zeyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1\">Mengfan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Songning Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Ziqiang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring & Exploiting High-Order Graph Structure for Sparse Knowledge Graph Completion. (arXiv:2306.17034v1 [cs.AI])","link":"http://arxiv.org/abs/2306.17034","description":"<p>Sparse knowledge graph (KG) scenarios pose a challenge for previous Knowledge\nGraph Completion (KGC) methods, that is, the completion performance decreases\nrapidly with the increase of graph sparsity. This problem is also exacerbated\nbecause of the widespread existence of sparse KGs in practical applications. To\nalleviate this challenge, we present a novel framework, LR-GCN, that is able to\nautomatically capture valuable long-range dependency among entities to\nsupplement insufficient structure features and distill logical reasoning\nknowledge for sparse KGC. The proposed approach comprises two main components:\na GNN-based predictor and a reasoning path distiller. The reasoning path\ndistiller explores high-order graph structures such as reasoning paths and\nencodes them as rich-semantic edges, explicitly compositing long-range\ndependencies into the predictor. This step also plays an essential role in\ndensifying KGs, effectively alleviating the sparse issue. Furthermore, the path\ndistiller further distills logical reasoning knowledge from these mined\nreasoning paths into the predictor. These two components are jointly optimized\nusing a well-designed variational EM algorithm. Extensive experiments and\nanalyses on four sparse benchmarks demonstrate the effectiveness of our\nproposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zekun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zihao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1\">Zheng Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Grammatical Tagging for the Legal Language of Cybersecurity. (arXiv:2306.17042v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17042","description":"<p>Legal language can be understood as the language typically used by those\nengaged in the legal profession and, as such, it may come both in spoken or\nwritten form. Recent legislation on cybersecurity obviously uses legal language\nin writing, thus inheriting all its interpretative complications due to the\ntypical abundance of cases and sub-cases as well as to the general richness in\ndetail. This paper faces the challenge of the essential interpretation of the\nlegal language of cybersecurity, namely of the extraction of the essential\nParts of Speech (POS) from the legal documents concerning cybersecurity. The\nchallenge is overcome by our methodology for POS tagging of legal language. It\nleverages state-of-the-art open-source tools for Natural Language Processing\n(NLP) as well as manual analysis to validate the outcomes of the tools. As a\nresult, the methodology is automated and, arguably, general for any legal\nlanguage following minor tailoring of the preprocessing step. It is\ndemonstrated over the most relevant EU legislation on cybersecurity, namely on\nthe NIS 2 directive, producing the first, albeit essential, structured\ninterpretation of such a relevant document. Moreover, our findings indicate\nthat tools such as SpaCy and ClausIE reach their limits over the legal language\nof the NIS 2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Castiglione_G/0/1/0/all/0/1\">Gianpietro Castiglione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bella_G/0/1/0/all/0/1\">Giampaolo Bella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santamaria_D/0/1/0/all/0/1\">Daniele Francesco Santamaria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The mapKurator System: A Complete Pipeline for Extracting and Linking Text from Historical Maps. (arXiv:2306.17059v1 [cs.AI])","link":"http://arxiv.org/abs/2306.17059","description":"<p>Documents hold spatial focus and valuable locality characteristics. For\nexample, descriptions of listings in real estate or travel blogs contain\ninformation about specific local neighborhoods. This information is valuable to\ncharacterize how humans perceive their environment. However, the first step to\nmaking use of this information is to identify the spatial focus (e.g., a city)\nof a document. Traditional approaches for identifying the spatial focus of a\ndocument rely on detecting and disambiguating toponyms from the document. This\napproach requires a vocabulary set of location phrases and ad-hoc rules, which\nignore important words related to location. Recent topic modeling approaches\nusing large language models often consider a few topics, each with broad\ncoverage. In contrast, the spatial focus of a document can be a country, a\ncity, or even a neighborhood, which together, is much larger than the number of\ntopics considered in these approaches. Additionally, topic modeling methods are\noften applied to broad topics of news articles where context is easily\ndistinguishable. To identify the geographic focus of a document effectively, we\npresent a simple but effective Joint Embedding of multi-LocaLitY (JELLY), which\njointly learns representations with separate encoders of document and location.\nJELLY significantly outperforms state-of-the-art methods for identifying\nspatial focus from documents from a number of sources. We also demonstrate case\nstudies on the arithmetic of the learned representations, including identifying\ncities with similar locality characteristics and zero-shot learning to identify\ndocument spatial focus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jina Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yijun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namgung_M/0/1/0/all/0/1\">Min Namgung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_L/0/1/0/all/0/1\">Leeje Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_Y/0/1/0/all/0/1\">Yao-Yi Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Presenting an approach based on weighted CapsuleNet networks for Arabic and Persian multi-domain sentiment analysis. (arXiv:2306.17068v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17068","description":"<p>Sentiment classification is a fundamental task in natural language\nprocessing, assigning one of the three classes, positive, negative, or neutral,\nto free texts. However, sentiment classification models are highly domain\ndependent; the classifier may perform classification with reasonable accuracy\nin one domain but not in another due to the Semantic multiplicity of words\ngetting poor accuracy. This article presents a new Persian/Arabic multi-domain\nsentiment analysis method using the cumulative weighted capsule networks\napproach. Weighted capsule ensemble consists of training separate capsule\nnetworks for each domain and a weighting measure called domain belonging degree\n(DBD). This criterion consists of TF and IDF, which calculates the dependency\nof each document for each domain separately; this value is multiplied by the\npossible output that each capsule creates. In the end, the sum of these\nmultiplications is the title of the final output, and is used to determine the\npolarity. And the most dependent domain is considered the final output for each\ndomain. The proposed method was evaluated using the Digikala dataset and\nobtained acceptable accuracy compared to the existing approaches. It achieved\nan accuracy of 0.89 on detecting the domain of belonging and 0.99 on detecting\nthe polarity. Also, for the problem of dealing with unbalanced classes, a\ncost-sensitive function was used. This function was able to achieve 0.0162\nimprovements in accuracy for sentiment classification. This approach on Amazon\nArabic data can achieve 0.9695 accuracies in domain classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kobari_M/0/1/0/all/0/1\">Mahboobeh Sadat Kobari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karimi_N/0/1/0/all/0/1\">Nima Karimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pourhosseini_B/0/1/0/all/0/1\">Benyamin Pourhosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mousa_R/0/1/0/all/0/1\">Ramin Mousa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Concept-Oriented Deep Learning with Large Language Models. (arXiv:2306.17089v1 [cs.LG])","link":"http://arxiv.org/abs/2306.17089","description":"<p>Large Language Models (LLMs) have been successfully used in many\nnatural-language tasks and applications including text generation and AI\nchatbots. They also are a promising new technology for concept-oriented deep\nlearning (CODL). However, the prerequisite is that LLMs understand concepts and\nensure conceptual consistency. We discuss these in this paper, as well as major\nuses of LLMs for CODL including concept extraction from text, concept graph\nextraction from text, and concept learning. Human knowledge consists of both\nsymbolic (conceptual) knowledge and embodied (sensory) knowledge. Text-only\nLLMs, however, can represent only symbolic (conceptual) knowledge. Multimodal\nLLMs, on the other hand, are capable of representing the full range (conceptual\nand sensory) of human knowledge. We discuss conceptual understanding in\nvisual-language LLMs, the most important multimodal LLMs, and major uses of\nthem for CODL including concept extraction from image, concept graph extraction\nfrom image, and concept learning. While uses of LLMs for CODL are valuable\nstandalone, they are particularly valuable as part of LLM applications such as\nAI chatbots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Daniel T. Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by Whispering to ChatGPT. (arXiv:2306.17103v1 [cs.CL])","link":"http://arxiv.org/abs/2306.17103","description":"<p>We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic\nlyrics transcription method achieving state-of-the-art performance on various\nlyrics transcription datasets, even in challenging genres such as rock and\nmetal. Our novel, training-free approach utilizes Whisper, a weakly supervised\nrobust speech recognition model, and GPT-4, today's most performant chat-based\nlarge language model. In the proposed method, Whisper functions as the \"ear\" by\ntranscribing the audio, while GPT-4 serves as the \"brain,\" acting as an\nannotator with a strong performance for contextualized output selection and\ncorrection. Our experiments show that LyricWhiz significantly reduces Word\nError Rate compared to existing methods in English and can effectively\ntranscribe lyrics across multiple languages. Furthermore, we use LyricWhiz to\ncreate the first publicly available, large-scale, multilingual lyrics\ntranscription dataset with a CC-BY-NC-SA copyright license, based on\nMTG-Jamendo, and offer a human-annotated subset for noise level estimation and\nevaluation. We anticipate that our proposed method and dataset will advance the\ndevelopment of multilingual lyrics transcription, a challenging and emerging\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_L/0/1/0/all/0/1\">Le Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1\">Ruibin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jiahao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yinghao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LI_Y/0/1/0/all/0/1\">Yizhi LI</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Ge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dannenberg_R/0/1/0/all/0/1\">Roger Dannenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1\">Emmanouil Benetos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Wei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding. (arXiv:2306.17107v1 [cs.CV])","link":"http://arxiv.org/abs/2306.17107","description":"<p>Instruction tuning unlocks the superior capability of Large Language Models\n(LLM) to interact with humans. Furthermore, recent instruction-following\ndatasets include images as visual inputs, collecting responses for image-based\ninstructions. However, visual instruction-tuned models cannot comprehend\ntextual details within images well. This work enhances the current visual\ninstruction tuning pipeline with text-rich images (e.g., movie posters, book\ncovers, etc.). Specifically, we first use publicly available OCR tools to\ncollect results on 422K text-rich images from the LAION dataset. Moreover, we\nprompt text-only GPT-4 with recognized texts and image captions to generate 16K\nconversations, each containing question-answer pairs for text-rich images. By\ncombining our collected data with previous multi-modal instruction-following\ndata, our model, LLaVAR, substantially improves the LLaVA model's capability on\ntext-based VQA datasets (up to 20% accuracy improvement) while achieving an\naccuracy of 91.42% on ScienceQA. The GPT-4-based instruction-following\nevaluation also demonstrates the improvement of our model on both natural\nimages and text-rich images. Through qualitative analysis, LLaVAR shows\npromising interaction (e.g., reasoning, writing, and elaboration) skills with\nhumans based on the latest real-world online content that combines text and\nimages. We make our code/data/models publicly available at\nhttps://llavar.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanzhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruiyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiuxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yufan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1\">Nedim Lipka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors. (arXiv:2306.17156v1 [cs.CY])","link":"http://arxiv.org/abs/2306.17156","description":"<p>Generative AI and large language models hold great promise in enhancing\ncomputing education by powering next-generation educational technologies for\nintroductory programming. Recent works have studied these models for different\nscenarios relevant to programming education; however, these works are limited\nfor several reasons, as they typically consider already outdated models or only\nspecific scenario(s). Consequently, there is a lack of a systematic study that\nbenchmarks state-of-the-art models for a comprehensive set of programming\neducation scenarios. In our work, we systematically evaluate two models,\nChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human\ntutors for a variety of scenarios. We evaluate using five introductory Python\nprogramming problems and real-world buggy programs from an online platform, and\nassess performance using expert-based annotations. Our results show that GPT-4\ndrastically outperforms ChatGPT (based on GPT-3.5) and comes close to human\ntutors' performance for several scenarios. These results also highlight\nsettings where GPT-4 still struggles, providing exciting future directions on\ndeveloping techniques to improve the performance of these models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phung_T/0/1/0/all/0/1\">Tung Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padurean_V/0/1/0/all/0/1\">Victor-Alexandru P&#x103;durean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambronero_J/0/1/0/all/0/1\">Jos&#xe9; Cambronero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulwani_S/0/1/0/all/0/1\">Sumit Gulwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohn_T/0/1/0/all/0/1\">Tobias Kohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_R/0/1/0/all/0/1\">Rupak Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1\">Adish Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_G/0/1/0/all/0/1\">Gustavo Soares</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Did AI get more negative recently?. (arXiv:2202.13610v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13610","description":"<p>In this paper, we classify scientific articles in the domain of natural\nlanguage processing (NLP) and machine learning (ML), as core subfields of\nartificial intelligence (AI), into whether (i) they extend the current\nstate-of-the-art by the introduction of novel techniques which beat existing\nmodels or whether (ii) they mainly criticize the existing state-of-the-art,\ni.e. that it is deficient with respect to some property (e.g. wrong evaluation,\nwrong datasets, misleading task specification). We refer to contributions under\n(i) as having a 'positive stance' and contributions under (ii) as having a\n'negative stance' (to related work). We annotate over 1.5 k papers from NLP and\nML to train a SciBERT-based model to automatically predict the stance of a\npaper based on its title and abstract. We then analyse large-scale trends on\nover 41 k papers from the last approximately 35 years in NLP and ML, finding\nthat papers have become substantially more positive over time, but negative\npapers also got more negative and we observe considerably more negative papers\nin recent years. Negative papers are also more influential in terms of\ncitations they receive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beese_D/0/1/0/all/0/1\">Dominik Beese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altunbas_B/0/1/0/all/0/1\">Beg&#xfc;m Altunba&#x15f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzeler_G/0/1/0/all/0/1\">G&#xf6;rkem G&#xfc;zeler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"That Is a Suspicious Reaction!\": Interpreting Logits Variation to Detect NLP Adversarial Attacks. (arXiv:2204.04636v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2204.04636","description":"<p>Adversarial attacks are a major challenge faced by current machine learning\nresearch. These purposely crafted inputs fool even the most advanced models,\nprecluding their deployment in safety-critical applications. Extensive research\nin computer vision has been carried to develop reliable defense strategies.\nHowever, the same issue remains less explored in natural language processing.\nOur work presents a model-agnostic detector of adversarial text examples. The\napproach identifies patterns in the logits of the target classifier when\nperturbing the input text. The proposed detector improves the current\nstate-of-the-art performance in recognizing adversarial inputs and exhibits\nstrong generalization capabilities across different NLP models, datasets, and\nword-level attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mosca_E/0/1/0/all/0/1\">Edoardo Mosca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Shreyash Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rando_J/0/1/0/all/0/1\">Javier Rando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1\">Georg Groh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Usefulness of Embeddings, Clusters and Strings for Text Generator Evaluation. (arXiv:2205.16001v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.16001","description":"<p>A good automatic evaluation metric for language generation ideally correlates\nhighly with human judgements of text quality. Yet, there is a dearth of such\nmetrics, which inhibits the rapid and efficient progress of language\ngenerators. One exception is the recently proposed Mauve. In theory, Mauve\nmeasures an information-theoretic divergence between two probability\ndistributions over strings: one representing the language generator under\nevaluation; the other representing the true natural language distribution.\nMauve's authors argue that its success comes from the qualitative properties of\ntheir proposed divergence. Yet in practice, as this divergence is uncomputable,\nMauve approximates it by measuring the divergence between multinomial\ndistributions over clusters instead, where cluster assignments are attained by\ngrouping strings based on a pre-trained language model's embeddings. As we\nshow, however, this is not a tight approximation -- in either theory or\npractice. This begs the question: why does Mauve work so well? In this work, we\nshow that Mauve was right for the wrong reasons, and that its newly proposed\ndivergence is not necessary for its high performance. In fact, classical\ndivergences paired with its proposed cluster-based approximation may actually\nserve as better evaluation metrics. We finish the paper with a probing\nanalysis; this analysis leads us to conclude that -- by encoding syntactic- and\ncoherence-level features of text, while ignoring surface-level features -- such\ncluster-based substitutes to string distributions may simply be better for\nevaluating state-of-the-art language generators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models as Knowledge Embeddings. (arXiv:2206.12617v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.12617","description":"<p>Knowledge embeddings (KE) represent a knowledge graph (KG) by embedding\nentities and relations into continuous vector spaces. Existing methods are\nmainly structure-based or description-based. Structure-based methods learn\nrepresentations that preserve the inherent structure of KGs. They cannot well\nrepresent abundant long-tail entities in real-world KGs with limited structural\ninformation. Description-based methods leverage textual information and\nlanguage models. Prior approaches in this direction barely outperform\nstructure-based ones, and suffer from problems like expensive negative sampling\nand restrictive description demand. In this paper, we propose LMKE, which\nadopts Language Models to derive Knowledge Embeddings, aiming at both enriching\nrepresentations of long-tail entities and solving problems of prior\ndescription-based methods. We formulate description-based KE learning with a\ncontrastive learning framework to improve efficiency in training and\nevaluation. Experimental results show that LMKE achieves state-of-the-art\nperformance on KE benchmarks of link prediction and triple classification,\nespecially for long-tail entities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qianyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiaqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The expected sum of edge lengths in planar linearizations of trees. Theory and applications. (arXiv:2207.05564v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.05564","description":"<p>Dependency trees have proven to be a very successful model to represent the\nsyntactic structure of sentences of human languages. In these structures,\nvertices are words and edges connect syntactically-dependent words. The\ntendency of these dependencies to be short has been demonstrated using random\nbaselines for the sum of the lengths of the edges or its variants. A ubiquitous\nbaseline is the expected sum in projective orderings (wherein edges do not\ncross and the root word of the sentence is not covered by any edge), that can\nbe computed in time $O(n)$. Here we focus on a weaker formal constraint, namely\nplanarity. In the theoretical domain, we present a characterization of\nplanarity that, given a sentence, yields either the number of planar\npermutations or an efficient algorithm to generate uniformly random planar\npermutations of the words. We also show the relationship between the expected\nsum in planar arrangements and the expected sum in projective arrangements. In\nthe domain of applications, we derive a $O(n)$-time algorithm to calculate the\nexpected value of the sum of edge lengths. We also apply this research to a\nparallel corpus and find that the gap between actual dependency distance and\nthe random baseline reduces as the strength of the formal constraint on\ndependency structures increases, suggesting that formal constraints absorb part\nof the dependency distance minimization effect. Our research paves the way for\nreplicating past research on dependency distance minimization using random\nplanar linearizations as random baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alemany_Puig_L/0/1/0/all/0/1\">Llu&#xed;s Alemany-Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Log-linear Guardedness and its Implications. (arXiv:2210.10012v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.10012","description":"<p>Methods for erasing human-interpretable concepts from neural representations\nthat assume linearity have been found to be tractable and useful. However, the\nimpact of this removal on the behavior of downstream classifiers trained on the\nmodified representations is not fully understood. In this work, we formally\ndefine the notion of log-linear guardedness as the inability of an adversary to\npredict the concept directly from the representation, and study its\nimplications. We show that, in the binary case, under certain assumptions, a\ndownstream log-linear model cannot recover the erased concept. However, we\ndemonstrate that a multiclass log-linear model \\emph{can} be constructed that\nindirectly recovers the concept in some cases, pointing to the inherent\nlimitations of log-linear guardedness as a downstream bias mitigation\ntechnique. These findings shed light on the theoretical limitations of linear\nerasure methods and highlight the need for further research on the connections\nbetween intrinsic and extrinsic bias in neural models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MooseNet: A Trainable Metric for Synthesized Speech with a PLDA Module. (arXiv:2301.07087v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.07087","description":"<p>We present MooseNet, a trainable speech metric that predicts the listeners'\nMean Opinion Score (MOS). We propose a novel approach where the Probabilistic\nLinear Discriminative Analysis (PLDA) generative model is used on top of an\nembedding obtained from a self-supervised learning (SSL) neural network (NN)\nmodel. We show that PLDA works well with a non-finetuned SSL model when trained\nonly on 136 utterances (ca. one minute training time) and that PLDA\nconsistently improves various neural MOS prediction models, even\nstate-of-the-art models with task-specific fine-tuning. Our ablation study\nshows PLDA training superiority over SSL model fine-tuning in a low-resource\nscenario. We also improve SSL model fine-tuning using a convenient optimizer\nchoice and additional contrastive and multi-task training objectives. The\nfine-tuned MooseNet NN with the PLDA module achieves the best results,\nsurpassing the SSL baseline on the VoiceMOS Challenge data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Platek_O/0/1/0/all/0/1\">Ond&#x159;ej Pl&#xe1;tek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1\">Ond&#x159;ej Du&#x161;ek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Sentence-Level Factuality of News and Bias of Media Outlets. (arXiv:2301.11850v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.11850","description":"<p>Automated news credibility and fact-checking at scale require accurately\npredicting news factuality and media bias. This paper introduces a large\nsentence-level dataset, titled \"FactNews\", composed of 6,191 sentences expertly\nannotated according to factuality and media bias definitions proposed by\nAllSides. We use FactNews to assess the overall reliability of news sources, by\nformulating two text classification problems for predicting sentence-level\nfactuality of news reporting and bias of media outlets. Our experiments\ndemonstrate that biased sentences present a higher number of words compared to\nfactual sentences, besides having a predominance of emotions. Hence, the\nfine-grained analysis of subjectivity and impartiality of news articles\nprovided promising results for predicting the reliability of media outlets.\nFinally, due to the severity of fake news and political polarization in Brazil,\nand the lack of research for Portuguese, both dataset and baseline were\nproposed for Brazilian Portuguese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vargas_F/0/1/0/all/0/1\">Francielle Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaidka_K/0/1/0/all/0/1\">Kokil Jaidka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pardo_T/0/1/0/all/0/1\">Thiago A. S. Pardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benevenuto_F/0/1/0/all/0/1\">Fabr&#xed;cio Benevenuto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effectiveness of Data Augmentation for Parameter Efficient Tuning with Limited Data. (arXiv:2303.02577v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.02577","description":"<p>Recent work has demonstrated that using parameter efficient tuning techniques\nsuch as prefix tuning (or P-tuning) on pretrained language models can yield\nperformance that is comparable or superior to fine-tuning while dramatically\nreducing trainable parameters. Nevertheless, the effectiveness of such methods\nunder the context of data augmentation, a common strategy to improve learning\nunder low data regimes, has not been fully explored. In this paper, we examine\nthe effectiveness of several popular task-agnostic data augmentation\ntechniques, i.e., EDA, Back Translation, and Mixup, when using two general\nparameter efficient tuning methods, P-tuning v2 and LoRA, under data scarcity.\nWe show that data augmentation can be used to boost the performance of P-tuning\nand LoRA models, but the effectiveness of each technique varies and certain\nmethods can lead to a notable degradation in performance, particularly when\nusing larger models and on harder tasks. We further analyze the sentence\nrepresentations of P-tuning compared to fine-tuning to help understand the\nabove behaviour, and reveal how P-tuning generally presents a more limited\nability to separate the sentence embeddings from different classes of augmented\ndata. In addition, it displays poorer performance on heavily altered data.\nHowever, we demonstrate that by adding a simple contrastive loss function it\ncan help mitigate such issues for prefix tuning, resulting in sizable\nimprovements to augmented data performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Obadinma_S/0/1/0/all/0/1\">Stephen Obadinma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hongyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can AI-Generated Text be Reliably Detected?. (arXiv:2303.11156v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.11156","description":"<p>In this paper, both empirically and theoretically, we show that several\nAI-text detectors are not reliable in practical scenarios. Empirically, we show\nthat paraphrasing attacks, where a light paraphraser is applied on top of a\nlarge language model (LLM), can break a whole range of detectors, including\nones using watermarking schemes as well as neural network-based detectors and\nzero-shot classifiers. Our experiments demonstrate that retrieval-based\ndetectors, designed to evade paraphrasing attacks, are still vulnerable to\nrecursive paraphrasing. We then provide a theoretical impossibility result\nindicating that as language models become more sophisticated and better at\nemulating human text, the performance of even the best-possible detector\ndecreases. For a sufficiently advanced language model seeking to imitate human\ntext, even the best-possible detector may only perform marginally better than a\nrandom classifier. Our result is general enough to capture specific scenarios\nsuch as particular writing styles, clever prompt design, or text paraphrasing.\nWe also extend the impossibility result to include the case where pseudorandom\nnumber generators are used for AI-text generation instead of true randomness.\nWe show that the same result holds with a negligible correction term for all\npolynomial-time computable detectors. Finally, we show that even LLMs protected\nby watermarking schemes can be vulnerable against spoofing attacks where\nadversarial humans can infer hidden LLM text signatures and add them to\nhuman-generated text to be detected as text generated by the LLMs, potentially\ncausing reputational damage to their developers. We believe these results can\nopen an honest conversation in the community regarding the ethical and reliable\nuse of AI-generated text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sadasivan_V/0/1/0/all/0/1\">Vinu Sankar Sadasivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aounon Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_S/0/1/0/all/0/1\">Sriram Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1\">Soheil Feizi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Large Language Models. (arXiv:2303.18223v11 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.18223","description":"<p>Language is essentially a complex, intricate system of human expressions\ngoverned by grammatical rules. It poses a significant challenge to develop\ncapable AI algorithms for comprehending and grasping a language. As a major\napproach, language modeling has been widely studied for language understanding\nand generation in the past two decades, evolving from statistical language\nmodels to neural language models. Recently, pre-trained language models (PLMs)\nhave been proposed by pre-training Transformer models over large-scale corpora,\nshowing strong capabilities in solving various NLP tasks. Since researchers\nhave found that model scaling can lead to performance improvement, they further\nstudy the scaling effect by increasing the model size to an even larger size.\nInterestingly, when the parameter scale exceeds a certain level, these enlarged\nlanguage models not only achieve a significant performance improvement but also\nshow some special abilities that are not present in small-scale language\nmodels. To discriminate the difference in parameter scale, the research\ncommunity has coined the term large language models (LLM) for the PLMs of\nsignificant size. Recently, the research on LLMs has been largely advanced by\nboth academia and industry, and a remarkable progress is the launch of ChatGPT,\nwhich has attracted widespread attention from society. The technical evolution\nof LLMs has been making an important impact on the entire AI community, which\nwould revolutionize the way how we develop and use AI algorithms. In this\nsurvey, we review the recent advances of LLMs by introducing the background,\nkey findings, and mainstream techniques. In particular, we focus on four major\naspects of LLMs, namely pre-training, adaptation tuning, utilization, and\ncapacity evaluation. Besides, we also summarize the available resources for\ndeveloping LLMs and discuss the remaining issues for future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yupeng Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1\">Yingqian Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Beichen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zican Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yifan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yushuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhipeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jinhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1\">Ruiyang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xinyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zikang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Patient Pre-screening for Clinical Trials: Assisting Physicians with Large Language Models. (arXiv:2304.07396v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2304.07396","description":"<p>Physicians considering clinical trials for their patients are met with the\nlaborious process of checking many text based eligibility criteria. Large\nLanguage Models (LLMs) have shown to perform well for clinical information\nextraction and clinical reasoning, including medical tests, but not yet in\nreal-world scenarios. This paper investigates the use of InstructGPT to assist\nphysicians in determining eligibility for clinical trials based on a patient's\nsummarised medical profile. Using a prompting strategy combining one-shot,\nselection-inference and chain-of-thought techniques, we investigate the\nperformance of LLMs on 10 synthetically created patient profiles. Performance\nis evaluated at four levels: ability to identify screenable eligibility\ncriteria from a trial given a medical profile; ability to classify for each\nindividual criterion whether the patient qualifies; the overall classification\nwhether a patient is eligible for a clinical trial and the percentage of\ncriteria to be screened by physician. We evaluated against 146 clinical trials\nand a total of 4,135 eligibility criteria. The LLM was able to correctly\nidentify the screenability of 72% (2,994/4,135) of the criteria. Additionally,\n72% (341/471) of the screenable criteria were evaluated correctly. The\nresulting trial level classification as eligible or ineligible resulted in a\nrecall of 0.5. By leveraging LLMs with a physician-in-the-loop, a recall of 1.0\nand precision of 0.71 on clinical trial level can be achieved while reducing\nthe amount of criteria to be checked by an estimated 90%. LLMs can be used to\nassist physicians with pre-screening of patients for clinical trials. By\nforcing instruction-tuned LLMs to produce chain-of-thought responses, the\nreasoning can be made transparent to and the decision process becomes amenable\nby physicians, thereby making such a system feasible for use in real-world\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamer_D/0/1/0/all/0/1\">Danny M. den Hamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoor_P/0/1/0/all/0/1\">Perry Schoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polak_T/0/1/0/all/0/1\">Tobias B. Polak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapitan_D/0/1/0/all/0/1\">Daniel Kapitan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT vs State-of-the-Art Models: A Benchmarking Study in Keyphrase Generation Task. (arXiv:2304.14177v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.14177","description":"<p>Transformer-based language models, including ChatGPT, have demonstrated\nexceptional performance in various natural language generation tasks. However,\nthere has been limited research evaluating ChatGPT's keyphrase generation\nability, which involves identifying informative phrases that accurately reflect\na document's content. This study seeks to address this gap by comparing\nChatGPT's keyphrase generation performance with state-of-the-art models, while\nalso testing its potential as a solution for two significant challenges in the\nfield: domain adaptation and keyphrase generation from long documents. We\nconducted experiments on six publicly available datasets from scientific\narticles and news domains, analyzing performance on both short and long\ndocuments. Our results show that ChatGPT outperforms current state-of-the-art\nmodels in all tested datasets and environments, generating high-quality\nkeyphrases that adapt well to diverse domains and document lengths.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Cruz_R/0/1/0/all/0/1\">Roberto Mart&#xed;nez-Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Lopez_A/0/1/0/all/0/1\">Alvaro J. L&#xf3;pez-L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portela_J/0/1/0/all/0/1\">Jos&#xe9; Portela</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation Approaches for Source Code Models: A Survey. (arXiv:2305.19915v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.19915","description":"<p>The increasingly popular adoption of source code in many critical tasks\nmotivates the development of data augmentation (DA) techniques to enhance\ntraining data and improve various capabilities (e.g., robustness and\ngeneralizability) of these models. Although a series of DA methods have been\nproposed and tailored for source code models, there lacks a comprehensive\nsurvey and examination to understand their effectiveness and implications. This\npaper fills this gap by conducting a comprehensive and integrative survey of\ndata augmentation for source code, wherein we systematically compile and\nencapsulate existing literature to provide a comprehensive overview of the\nfield. We start by constructing a taxonomy of DA for source code models model\napproaches, followed by a discussion on prominent, methodologically\nillustrative approaches. Next, we highlight the general strategies and\ntechniques to optimize the DA quality. Subsequently, we underscore techniques\nthat find utility in widely-accepted source code scenarios and downstream\ntasks. Finally, we outline the prevailing challenges and potential\nopportunities for future research. In essence, this paper endeavors to\ndemystify the corpus of existing literature on DA for source code models, and\nfoster further exploration in this sphere. Complementing this, we present a\ncontinually updated GitHub repository that hosts a list of update-to-date\npapers on DA for source code models, accessible at\n\\url{https://github.com/terryyz/DataAug4Code}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1\">Terry Yue Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhensu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xiaoning Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1\">Zhenchang Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_D/0/1/0/all/0/1\">David Lo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bring Your Own Data! Self-Supervised Evaluation for Large Language Models. (arXiv:2306.13651v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.13651","description":"<p>With the rise of Large Language Models (LLMs) and their ubiquitous deployment\nin diverse domains, measuring language model behavior on realistic data is\nimperative. For example, a company deploying a client-facing chatbot must\nensure that the model will not respond to client requests with profanity.\nCurrent evaluations approach this problem using small, domain-specific datasets\nwith human-curated labels. These evaluation sets are often sampled from a\nnarrow and simplified distribution, and data sources can unknowingly be leaked\ninto the training set which can lead to misleading evaluations. To bypass these\ndrawbacks, we propose a framework for self-supervised evaluation of LLMs by\nanalyzing their sensitivity or invariance to transformations on the input text.\nSelf-supervised evaluation can directly monitor LLM behavior on datasets\ncollected in the wild or streamed during live model deployment. We demonstrate\nself-supervised evaluation strategies for measuring closed-book knowledge,\ntoxicity, and long-range context dependence, in addition to sensitivity to\ngrammatical structure and tokenization errors. When comparisons to similar\nhuman-labeled benchmarks are available, we find strong correlations between\nself-supervised and human-supervised evaluations. The self-supervised paradigm\ncomplements current evaluation strategies that rely on labeled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1\">Neel Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saifullah_K/0/1/0/all/0/1\">Khalid Saifullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchenbauer_J/0/1/0/all/0/1\">John Kirchenbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_M/0/1/0/all/0/1\">Manli Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1\">Aniruddha Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1\">Jonas Geiping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain-of-Thought Prompt Distillation for Multimodal Named Entity and Multimodal Relation Extraction. (arXiv:2306.14122v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.14122","description":"<p>Multimodal Named Entity Recognition (MNER) and Multimodal Relation Extraction\n(MRE) necessitate the fundamental reasoning capacity for intricate linguistic\nand multimodal comprehension. In this study, we explore distilling the\nreasoning ability of large language models (LLMs) into a more compact student\nmodel by generating a \\textit{chain of thought} (CoT) -- a sequence of\nintermediate reasoning steps. Specifically, we commence by exemplifying the\nelicitation of such reasoning ability from LLMs through CoT prompts covering\nmulti-grain (noun, sentence, multimodality) and data-augmentation (style,\nentity, image) dimensions. Subsequently, we present a novel conditional prompt\ndistillation method to assimilate the commonsense reasoning ability from LLMs,\nthereby enhancing the utility of the student model in addressing text-only\ninputs without the requisite addition of image and CoT knowledge. Extensive\nexperiments reveal that our approach attains state-of-the-art accuracy and\nmanifests a plethora of advantages concerning interpretability, data\nefficiency, and cross-domain generalization on MNER and MRE datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yujian Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Framework for Identifying Depression on Social Media: MentalRiskES@IberLEF 2023. (arXiv:2306.16125v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.16125","description":"<p>This paper describes our participation in the MentalRiskES task at IberLEF\n2023. The task involved predicting the likelihood of an individual experiencing\ndepression based on their social media activity. The dataset consisted of\nconversations from 175 Telegram users, each labeled according to their evidence\nof suffering from the disorder. We used a combination of traditional machine\nlearning and deep learning techniques to solve four predictive subtasks: binary\nclassification, simple regression, multiclass classification, and multi-output\nregression.\n</p>\n<p>We approached this by training a model to solve the multi-output regression\ncase and then transforming the predictions to work for the other three\nsubtasks.\n</p>\n<p>We compare the performance of two modeling approaches: fine-tuning a\nBERT-based model directly for the task or using its embeddings as inputs to a\nlinear regressor, with the latter yielding better results. The code to\nreproduce our results can be found at:\nhttps://github.com/simonsanvil/EarlyDepression-MentalRiskES\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Viloria_S/0/1/0/all/0/1\">Simon Sanchez Viloria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rio_D/0/1/0/all/0/1\">Daniel Peix del R&#xed;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabo_R/0/1/0/all/0/1\">Rub&#xe9;n Berm&#xfa;dez Cabo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuentes_G/0/1/0/all/0/1\">Guillermo Arturo Arrojo Fuentes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segura_Bedmar_I/0/1/0/all/0/1\">Isabel Segura-Bedmar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-06-29T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}