{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2024-01-29T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models. (arXiv:2401.14440v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14440","description":"<p>Recent studies of the emergent capabilities of transformer-based Natural\nLanguage Understanding (NLU) models have indicated that they have an\nunderstanding of lexical and compositional semantics. We provide evidence that\nsuggests these claims should be taken with a grain of salt: we find that\nstate-of-the-art Natural Language Inference (NLI) models are sensitive towards\nminor semantics preserving surface-form variations, which lead to sizable\ninconsistent model decisions during inference. Notably, this behaviour differs\nfrom valid and in-depth comprehension of compositional semantics, however does\nneither emerge when evaluating model accuracy on standard benchmarks nor when\nprobing for syntactic, monotonic, and logically robust reasoning. We propose a\nnovel framework to measure the extent of semantic sensitivity. To this end, we\nevaluate NLI models on adversarially generated examples containing minor\nsemantics-preserving surface-form input noise. This is achieved using\nconditional text generation, with the explicit condition that the NLI model\npredicts the relationship between the original and adversarial inputs as a\nsymmetric equivalence entailment. We systematically study the effects of the\nphenomenon across NLI models for \\emph{in-} and \\emph{out-of} domain settings.\nOur experiments show that semantic sensitivity causes performance degradations\nof $12.92\\%$ and $23.71\\%$ average over \\emph{in-} and \\emph{out-of-} domain\nsettings, respectively. We further perform ablation studies, analysing this\nphenomenon across models, datasets, and variations in inference and show that\nsemantic sensitivity can lead to major inconsistency within model predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arakelyan_E/0/1/0/all/0/1\">Erik Arakelyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaoqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wordflow: Social Prompt Engineering for Large Language Models. (arXiv:2401.14447v1 [cs.HC])","link":"http://arxiv.org/abs/2401.14447","description":"<p>Large language models (LLMs) require well-crafted prompts for effective use.\nPrompt engineering, the process of designing prompts, is challenging,\nparticularly for non-experts who are less familiar with AI technologies. While\nresearchers have proposed techniques and tools to assist LLM users in prompt\ndesign, these works primarily target AI application developers rather than\nnon-experts. To address this research gap, we propose social prompt\nengineering, a novel paradigm that leverages social computing techniques to\nfacilitate collaborative prompt design. To investigate social prompt\nengineering, we introduce Wordflow, an open-source and social text editor that\nenables everyday users to easily create, run, share, and discover LLM prompts.\nAdditionally, by leveraging modern web technologies, Wordflow allows users to\nrun LLMs locally and privately in their browsers. Two usage scenarios highlight\nhow social prompt engineering and our tool can enhance laypeople's interaction\nwith LLMs. Wordflow is publicly accessible at\nhttps://poloclub.github.io/wordflow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijie J. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthy_A/0/1/0/all/0/1\">Aishwarya Chakravarthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munechika_D/0/1/0/all/0/1\">David Munechika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1\">Duen Horng Chau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LongHealth: A Question Answering Benchmark with Long Clinical Documents. (arXiv:2401.14490v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14490","description":"<p>Background: Recent advancements in large language models (LLMs) offer\npotential benefits in healthcare, particularly in processing extensive patient\nrecords. However, existing benchmarks do not fully assess LLMs' capability in\nhandling real-world, lengthy clinical data.\n</p>\n<p>Methods: We present the LongHealth benchmark, comprising 20 detailed\nfictional patient cases across various diseases, with each case containing\n5,090 to 6,754 words. The benchmark challenges LLMs with 400 multiple-choice\nquestions in three categories: information extraction, negation, and sorting,\nchallenging LLMs to extract and interpret information from large clinical\ndocuments.\n</p>\n<p>Results: We evaluated nine open-source LLMs with a minimum of 16,000 tokens\nand also included OpenAI's proprietary and cost-efficient GPT-3.5 Turbo for\ncomparison. The highest accuracy was observed for Mixtral-8x7B-Instruct-v0.1,\nparticularly in tasks focused on information retrieval from single and multiple\npatient documents. However, all models struggled significantly in tasks\nrequiring the identification of missing information, highlighting a critical\narea for improvement in clinical data interpretation.\n</p>\n<p>Conclusion: While LLMs show considerable potential for processing long\nclinical documents, their current accuracy levels are insufficient for reliable\nclinical use, especially in scenarios requiring the identification of missing\ninformation. The LongHealth benchmark provides a more realistic assessment of\nLLMs in a healthcare setting and highlights the need for further model\nrefinement for safe and effective clinical application.\n</p>\n<p>We make the benchmark and evaluation code publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adams_L/0/1/0/all/0/1\">Lisa Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_F/0/1/0/all/0/1\">Felix Busch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tianyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Excoffier_J/0/1/0/all/0/1\">Jean-Baptiste Excoffier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortala_M/0/1/0/all/0/1\">Matthieu Ortala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loser_A/0/1/0/all/0/1\">Alexander L&#xf6;ser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aerts_H/0/1/0/all/0/1\">Hugo JWL. Aerts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kather_J/0/1/0/all/0/1\">Jakob Nikolas Kather</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truhn_D/0/1/0/all/0/1\">Daniel Truhn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bressem_K/0/1/0/all/0/1\">Keno Bressem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-QA: A Real-World Medical Q&A Benchmark. (arXiv:2401.14493v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14493","description":"<p>Ensuring the accuracy of responses provided by large language models (LLMs)\nis crucial, particularly in clinical settings where incorrect information may\ndirectly impact patient health. To address this challenge, we construct K-QA, a\ndataset containing 1,212 patient questions originating from real-world\nconversations held on K Health (an AI-driven clinical platform). We employ a\npanel of in-house physicians to answer and manually decompose a subset of K-QA\ninto self-contained statements. Additionally, we formulate two NLI-based\nevaluation metrics approximating recall and precision: (1) comprehensiveness,\nmeasuring the percentage of essential clinical information in the generated\nanswer and (2) hallucination rate, measuring the number of statements from the\nphysician-curated response contradicted by the LLM answer. Finally, we use K-QA\nalong with these metrics to evaluate several state-of-the-art models, as well\nas the effect of in-context learning and medically-oriented augmented retrieval\nschemes developed by the authors. Our findings indicate that in-context\nlearning improves the comprehensiveness of the models, and augmented retrieval\nis effective in reducing hallucinations. We make K-QA available to to the\ncommunity to spur research into medically accurate NLP applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manes_I/0/1/0/all/0/1\">Itay Manes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ronn_N/0/1/0/all/0/1\">Naama Ronn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_D/0/1/0/all/0/1\">David Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ber_R/0/1/0/all/0/1\">Ran Ilan Ber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horowitz_Kugler_Z/0/1/0/all/0/1\">Zehavi Horowitz-Kugler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empathy and the Right to Be an Exception: What LLMs Can and Cannot Do. (arXiv:2401.14523v1 [cs.CY])","link":"http://arxiv.org/abs/2401.14523","description":"<p>Advances in the performance of large language models (LLMs) have led some\nresearchers to propose the emergence of theory of mind (ToM) in artificial\nintelligence (AI). LLMs can attribute beliefs, desires, intentions, and\nemotions, and they will improve in their accuracy. Rather than employing the\ncharacteristically human method of empathy, they learn to attribute mental\nstates by recognizing linguistic patterns in a dataset that typically do not\ninclude that individual. We ask whether LLMs' inability to empathize precludes\nthem from honoring an individual's right to be an exception, that is, from\nmaking assessments of character and predictions of behavior that reflect\nappropriate sensitivity to a person's individuality. Can LLMs seriously\nconsider an individual's claim that their case is different based on internal\nmental states like beliefs, desires, and intentions, or are they limited to\njudging that case based on its similarities to others? We propose that the\nmethod of empathy has special significance for honoring the right to be an\nexception that is distinct from the value of predictive accuracy, at which LLMs\nexcel. We conclude by considering whether using empathy to consider exceptional\ncases has intrinsic or merely practical value and we introduce conceptual and\nempirical avenues for advancing this investigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kidder_W/0/1/0/all/0/1\">William Kidder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DCruz_J/0/1/0/all/0/1\">Jason D&#x27;Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_K/0/1/0/all/0/1\">Kush R. Varshney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating GPT-3.5's Awareness and Summarization Abilities for European Constitutional Texts with Shared Topics. (arXiv:2401.14524v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14524","description":"<p>Constitutions are foundational legal documents that underpin the governmental\nand societal structures. As such, they are a reflection of a nation's cultural\nand social uniqueness, but also contribute to establish topics of universal\nimportance, like citizens' rights and duties (RD). In this work, using the\nrenowned GPT-3.5, we leverage generative large language models to understand\nconstitutional passages that transcend national boundaries. A key contribution\nof our study is the introduction of a novel application of abstractive\nsummarization on a multi-source collection of constitutional texts, with a\nfocus on European countries' constitution passages related to RD topics. Our\nresults show the meaningfulness of GPT-3.5 to produce informative, coherent and\nfaithful summaries capturing RD topics across European countries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Greco_C/0/1/0/all/0/1\">Candida M. Greco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagarelli_A/0/1/0/all/0/1\">A. Tagarelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEDs for PETs: Multilingual Euphemism Disambiguation for Potentially Euphemistic Terms. (arXiv:2401.14526v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14526","description":"<p>This study investigates the computational processing of euphemisms, a\nuniversal linguistic phenomenon, across multiple languages. We train a\nmultilingual transformer model (XLM-RoBERTa) to disambiguate potentially\neuphemistic terms (PETs) in multilingual and cross-lingual settings. In line\nwith current trends, we demonstrate that zero-shot learning across languages\ntakes place. We also show cases where multilingual models perform better on the\ntask compared to monolingual models by a statistically significant margin,\nindicating that multilingual data presents additional opportunities for models\nto learn about cross-lingual, computational properties of euphemisms. In a\nfollow-up analysis, we focus on universal euphemistic \"categories\" such as\ndeath and bodily functions among others. We test to see whether cross-lingual\ndata of the same domain is more important than within-language data of other\ndomains to further understand the nature of the cross-lingual transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1\">Patrick Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trujillo_A/0/1/0/all/0/1\">Alain Chirino Trujillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plancarte_D/0/1/0/all/0/1\">Diana Cuevas Plancarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojo_O/0/1/0/all/0/1\">Olumide Ebenezer Ojo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shode_I/0/1/0/all/0/1\">Iyanuoluwa Shode</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jing Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_A/0/1/0/all/0/1\">Anna Feldman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relative Value Biases in Large Language Models. (arXiv:2401.14530v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14530","description":"<p>Studies of reinforcement learning in humans and animals have demonstrated a\npreference for options that yielded relatively better outcomes in the past,\neven when those options are associated with lower absolute reward. The present\nstudy tested whether large language models would exhibit a similar bias. We had\ngpt-4-1106-preview (GPT-4 Turbo) and Llama-2-70B make repeated choices between\npairs of options with the goal of maximizing payoffs. A complete record of\nprevious outcomes was included in each prompt. Both models exhibited relative\nvalue decision biases similar to those observed in humans and animals. Making\nrelative comparisons among outcomes more explicit magnified the bias, whereas\nprompting the models to estimate expected outcomes caused the bias to\ndisappear. These results have implications for the potential mechanisms that\ncontribute to context-dependent choice in human agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hayes_W/0/1/0/all/0/1\">William M. Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yax_N/0/1/0/all/0/1\">Nicolas Yax</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palminteri_S/0/1/0/all/0/1\">Stefano Palminteri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Not (Always) Look Right: Investigating the Capabilities of Decoder-Based Large Language Models for Sequence Labeling. (arXiv:2401.14556v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14556","description":"<p>Pre-trained language models based on masked language modeling (MLM) objective\nexcel in natural language understanding (NLU) tasks. While fine-tuned MLM-based\nencoders consistently outperform causal language modeling decoders of\ncomparable size, a recent trend of scaling decoder models to multiple billion\nparameters resulted in large language models (LLMs), making them competitive\nwith MLM-based encoders. Although scale amplifies their prowess in NLU tasks,\nLLMs fall short of SOTA results in information extraction (IE) tasks, many\nframed as sequence labeling (SL). However, whether this is an intrinsic\nlimitation of LLMs or whether their SL performance can be improved remains\nunclear. To address this, we explore strategies to enhance the SL performance\nof \"open\" LLMs (Llama2 and Mistral) on IE tasks. We investigate bidirectional\ninformation flow within groups of decoder blocks, applying layer-wise removal\nor enforcement of the causal mask (CM) during LLM fine-tuning. This approach\nyields performance gains competitive with SOTA SL models, matching or\noutperforming the results of CM removal from all blocks. Our findings hold for\ndiverse SL tasks, proving that \"open\" LLMs with layer-dependent CM removal\noutperform strong MLM-based encoders and instruction-tuned LLMs. However, we\nobserve no effect from CM removal on a small scale when maintaining an\nequivalent model size, pre-training steps, and pre-training and fine-tuning\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dukic_D/0/1/0/all/0/1\">David Duki&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snajder_J/0/1/0/all/0/1\">Jan &#x160;najder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Modelling Approaches to Adaptive Machine Translation. (arXiv:2401.14559v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14559","description":"<p>Consistency is a key requirement of high-quality translation. It is\nespecially important to adhere to pre-approved terminology and adapt to\ncorrected translations in domain-specific projects. Machine translation (MT)\nhas achieved significant progress in the area of domain adaptation. However,\nin-domain data scarcity is common in translation settings, due to the lack of\nspecialised datasets and terminology, or inconsistency and inaccuracy of\navailable in-domain translations. In such scenarios where there is insufficient\nin-domain data to fine-tune MT models, producing translations that are\nconsistent with the relevant context is challenging. While real-time adaptation\ncan make use of smaller amounts of in-domain data to improve the translation on\nthe fly, it remains challenging due to supported context limitations and\nefficiency constraints. Large language models (LLMs) have recently shown\ninteresting capabilities of in-context learning, where they learn to replicate\ncertain input-output text generation patterns, without further fine-tuning.\nSuch capabilities have opened new horizons for domain-specific data\naugmentation and real-time adaptive MT. This work attempts to address two main\nrelevant questions: 1) in scenarios involving human interaction and continuous\nfeedback, can we employ language models to improve the quality of adaptive MT\nat inference time? and 2) in the absence of sufficient in-domain data, can we\nuse pre-trained large-scale language models to improve the process of MT domain\nadaptation?\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moslem_Y/0/1/0/all/0/1\">Yasmin Moslem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Structured Language Alternations in Historical Documents by Combining Language Identification with Fourier Analysis. (arXiv:2401.14569v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14569","description":"<p>In this study, we present a generalizable workflow to identify documents in a\nhistoric language with a nonstandard language and script combination,\nArmeno-Turkish. We introduce the task of detecting distinct patterns of\nmultilinguality based on the frequency of structured language alternations\nwithin a document.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sirin_H/0/1/0/all/0/1\">Hale Sirin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sabrina Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippincott_T/0/1/0/all/0/1\">Tom Lippincott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Diagnostic Accuracy through Multi-Agent Conversations: Using Large Language Models to Mitigate Cognitive Bias. (arXiv:2401.14589v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14589","description":"<p>Background: Cognitive biases in clinical decision-making significantly\ncontribute to errors in diagnosis and suboptimal patient outcomes. Addressing\nthese biases presents a formidable challenge in the medical field. This study\nexplores the role of large language models (LLMs) in mitigating these biases\nthrough the utilization of a multi-agent framework. We simulate the clinical\ndecision-making processes through multi-agent conversation and evaluate its\nefficacy in improving diagnostic accuracy. Methods: A total of 16 published and\nunpublished case reports where cognitive biases have resulted in misdiagnoses\nwere identified from the literature. In the multi-agent system, we leveraged\nGPT-4 Turbo to facilitate interactions among four simulated agents to replicate\nclinical team dynamics. Each agent has a distinct role: 1) To make the initial\nand final diagnosis after considering the discussions, 2) The devil's advocate\nand correct confirmation and anchoring bias, 3) The tutor and facilitator of\nthe discussion to reduce premature closure bias, and 4) To record and summarize\nthe findings. A total of 80 simulations were evaluated for the accuracy of\ninitial diagnosis, top differential diagnosis and final two differential\ndiagnoses. Findings: In a total of 80 responses evaluating both initial and\nfinal diagnoses, the initial diagnosis had an accuracy of 0% (0/80), but\nfollowing multi-agent discussions, the accuracy for the top differential\ndiagnosis increased to 71.3% (57/80), and for the final two differential\ndiagnoses, to 80.0% (64/80). The system demonstrated an ability to reevaluate\nand correct misconceptions, even in scenarios with misleading initial\ninvestigations. Interpretation: The LLM-driven multi-agent conversation system\nshows promise in enhancing diagnostic accuracy in diagnostically challenging\nmedical scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_Y/0/1/0/all/0/1\">Yu He Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Rui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lie_S/0/1/0/all/0/1\">Sui An Lie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_T/0/1/0/all/0/1\">Taylor Xin Yi Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdullah_H/0/1/0/all/0/1\">Hairil Rizal Abdullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ting_D/0/1/0/all/0/1\">Daniel Shu Wei Ting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alternative Speech: Complementary Method to Counter-Narrative for Better Discourse. (arXiv:2401.14616v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14616","description":"<p>We introduce the concept of \"Alternative Speech\" as a new way to directly\ncombat hate speech and complement the limitations of counter-narrative. An\nalternative speech provides practical alternatives to hate speech in real-world\nscenarios by offering speech-level corrections to speakers while considering\nthe surrounding context and promoting speakers to reform. Further, an\nalternative speech can combat hate speech alongside counter-narratives,\noffering a useful tool to address social issues such as racial discrimination\nand gender inequality. We propose the new concept and provide detailed\nguidelines for constructing the necessary dataset. Through discussion, we\ndemonstrate that combining alternative speech and counter-narrative can be a\nmore effective strategy for combating hate speech by complementing specificity\nand guiding capacity of counter-narrative. This paper presents another\nperspective for dealing with hate speech, offering viable remedies to\ncomplement the constraints of current approaches to mitigating harmful bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungyoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1\">Dahyun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chanjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seolhwa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Heuiseok Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query of CC: Unearthing Large Scale Domain-Specific Knowledge from Public Corpora. (arXiv:2401.14624v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14624","description":"<p>Large language models have demonstrated remarkable potential in various\ntasks, however, there remains a significant scarcity of open-source models and\ndata for specific domains. Previous works have primarily focused on manually\nspecifying resources and collecting high-quality data on specific domains,\nwhich significantly consume time and effort. To address this limitation, we\npropose an efficient data collection method~\\textit{Query of CC} based on large\nlanguage models. This method bootstraps seed information through a large\nlanguage model and retrieves related data from public corpora. It not only\ncollects knowledge-related data for specific domains but unearths the data with\npotential reasoning procedures. Through the application of this method, we have\ncurated a high-quality dataset called~\\textsc{Knowledge Pile}, encompassing\nfour major domains, including stem and humanities sciences, among others.\nExperimental results demonstrate that~\\textsc{Knowledge Pile} significantly\nimproves the performance of large language models in mathematical and\nknowledge-related reasoning ability tests. To facilitate academic sharing, we\nopen-source our dataset and code, providing valuable support to the academic\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fei_Z/0/1/0/all/0/1\">Zhaoye Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhiyuan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Practical Automatic Speech Recognition and Post-Processing: a Call for Explainable Error Benchmark Guideline. (arXiv:2401.14625v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14625","description":"<p>Automatic speech recognition (ASR) outcomes serve as input for downstream\ntasks, substantially impacting the satisfaction level of end-users. Hence, the\ndiagnosis and enhancement of the vulnerabilities present in the ASR model bear\nsignificant importance. However, traditional evaluation methodologies of ASR\nsystems generate a singular, composite quantitative metric, which fails to\nprovide comprehensive insight into specific vulnerabilities. This lack of\ndetail extends to the post-processing stage, resulting in further obfuscation\nof potential weaknesses. Despite an ASR model's ability to recognize utterances\naccurately, subpar readability can negatively affect user satisfaction, giving\nrise to a trade-off between recognition accuracy and user-friendliness. To\neffectively address this, it is imperative to consider both the speech-level,\ncrucial for recognition accuracy, and the text-level, critical for\nuser-friendliness. Consequently, we propose the development of an Error\nExplainable Benchmark (EEB) dataset. This dataset, while considering both\nspeech- and text-level, enables a granular understanding of the model's\nshortcomings. Our proposition provides a structured pathway for a more\n`real-world-centric' evaluation, a marked shift away from abstracted,\ntraditional methods, allowing for the detection and rectification of nuanced\nsystem weaknesses, ultimately aiming for an improved user experience.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koo_S/0/1/0/all/0/1\">Seonmin Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chanjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinsung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Jaehyung Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eo_S/0/1/0/all/0/1\">Sugyeong Eo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_H/0/1/0/all/0/1\">Hyeonseok Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Heuiseok Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Investigation of Domain Adaptation Ability for Chinese Spelling Check Models. (arXiv:2401.14630v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14630","description":"<p>Chinese Spelling Check (CSC) is a meaningful task in the area of Natural\nLanguage Processing (NLP) which aims at detecting spelling errors in Chinese\ntexts and then correcting these errors. However, CSC models are based on\npretrained language models, which are trained on a general corpus.\nConsequently, their performance may drop when confronted with downstream tasks\ninvolving domain-specific terms. In this paper, we conduct a thorough\nevaluation about the domain adaption ability of various typical CSC models by\nbuilding three new datasets encompassing rich domain-specific terms from the\nfinancial, medical, and legal domains. Then we conduct empirical investigations\nin the corresponding domain-specific test datasets to ascertain the\ncross-domain adaptation ability of several typical CSC models. We also test the\nperformance of the popular large language model ChatGPT. As shown in our\nexperiments, the performances of the CSC models drop significantly in the new\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruoqing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hongliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"T-Rex: Text-assisted Retrosynthesis Prediction. (arXiv:2401.14637v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14637","description":"<p>As a fundamental task in computational chemistry, retrosynthesis prediction\naims to identify a set of reactants to synthesize a target molecule. Existing\ntemplate-free approaches only consider the graph structures of the target\nmolecule, which often cannot generalize well to rare reaction types and large\nmolecules. Here, we propose T-Rex, a text-assisted retrosynthesis prediction\napproach that exploits pre-trained text language models, such as ChatGPT, to\nassist the generation of reactants. T-Rex first exploits ChatGPT to generate a\ndescription for the target molecule and rank candidate reaction centers based\nboth the description and the molecular graph. It then re-ranks these candidates\nby querying the descriptions for each reactants and examines which group of\nreactants can best synthesize the target molecule. We observed that T-Rex\nsubstantially outperformed graph-based state-of-the-art approaches on two\ndatasets, indicating the effectiveness of considering text information. We\nfurther found that T-Rex outperformed the variant that only use ChatGPT-based\ndescription without the re-ranking step, demonstrate how our framework\noutperformed a straightforward integration of ChatGPT and graph information.\nCollectively, we show that text generated by pre-trained language models can\nsubstantially improve retrosynthesis prediction, opening up new avenues for\nexploiting ChatGPT to advance computational chemistry. And the codes can be\nfound at https://github.com/lauyikfung/T-Rex.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hanwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1\">Tangqi Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_H/0/1/0/all/0/1\">Haocheng Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zixuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Large Language Models in Complex Question Answering Attribution using Knowledge Graphs. (arXiv:2401.14640v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14640","description":"<p>The attribution of question answering is to provide citations for supporting\ngenerated statements, and has attracted wide research attention. The current\nmethods for automatically evaluating the attribution, which are often based on\nLarge Language Models (LLMs), are still inadequate, particularly in recognizing\nsubtle differences between attributions, and complex relationships between\ncitations and statements. To compare these attribution evaluation methods and\ndevelop new ones, we introduce a set of fine-grained categories (i.e.,\nsupportive, insufficient, contradictory and irrelevant) for measuring the\nattribution, and develop a Complex Attributed Question Answering (CAQA)\nbenchmark by leveraging knowledge graphs (KGs) for automatically generating\nattributions of different categories to question-answer pairs. Our analysis\nreveals that existing evaluators perform poorly under fine-grained attribution\nsettings and exhibit weaknesses in complex citation-statement reasoning. Our\nCAQA benchmark, validated with human annotations, emerges as a promising tool\nfor selecting and developing LLM attribution evaluators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_N/0/1/0/all/0/1\">Nan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yike Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guilin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1\">Sheng Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongtong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Korean Legal Judgment Prediction Dataset for Insurance Disputes. (arXiv:2401.14654v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14654","description":"<p>This paper introduces a Korean legal judgment prediction (LJP) dataset for\ninsurance disputes. Successful LJP models on insurance disputes can benefit\ninsurance companies and their customers. It can save both sides' time and money\nby allowing them to predict how the result would come out if they proceed to\nthe dispute mediation process. As is often the case with low-resource\nlanguages, there is a limitation on the amount of data available for this\nspecific task. To mitigate this issue, we investigate how one can achieve a\ngood performance despite the limitation in data. In our experiment, we\ndemonstrate that Sentence Transformer Fine-tuning (SetFit, Tunstall et al.,\n2022) is a good alternative to standard fine-tuning when training data are\nlimited. The models fine-tuned with the SetFit approach on our data show\nsimilar performance to the Korean LJP benchmark models (Hwang et al., 2022)\ndespite the much smaller data size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwak_A/0/1/0/all/0/1\">Alice Saebom Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_C/0/1/0/all/0/1\">Cheonkam Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">Ji Weon Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_B/0/1/0/all/0/1\">Byeongcheol Min</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scientific Large Language Models: A Survey on Biological & Chemical Domains. (arXiv:2401.14656v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14656","description":"<p>Large Language Models (LLMs) have emerged as a transformative power in\nenhancing natural language comprehension, representing a significant stride\ntoward artificial general intelligence. The application of LLMs extends beyond\nconventional linguistic boundaries, encompassing specialized linguistic systems\ndeveloped within various scientific disciplines. This growing interest has led\nto the advent of scientific LLMs, a novel subclass specifically engineered for\nfacilitating scientific discovery. As a burgeoning area in the community of AI\nfor Science, scientific LLMs warrant comprehensive exploration. However, a\nsystematic and up-to-date survey introducing them is currently lacking. In this\npaper, we endeavor to methodically delineate the concept of \"scientific\nlanguage\", whilst providing a thorough review of the latest advancements in\nscientific LLMs. Given the expansive realm of scientific disciplines, our\nanalysis adopts a focused lens, concentrating on the biological and chemical\ndomains. This includes an in-depth examination of LLMs for textual knowledge,\nsmall molecules, macromolecular proteins, genomic sequences, and their\ncombinations, analyzing them in terms of model architectures, capabilities,\ndatasets, and evaluation. Finally, we critically examine the prevailing\nchallenges and point out promising research directions along with the advances\nof LLMs. By offering a comprehensive overview of technical developments in this\nfield, this survey aspires to be an invaluable resource for researchers\nnavigating the intricate landscape of scientific LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1\">Keyang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyv_T/0/1/0/all/0/1\">Tianwen Lyv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinda Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1\">Qingyu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaotong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1\">Zhuoyi Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiang Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zeyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1\">Ming Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinlu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jiyu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renjun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiaohui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_H/0/1/0/all/0/1\">Huabin Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNIT-DSR: Dysarthric Speech Reconstruction System Using Speech Unit Normalization. (arXiv:2401.14664v1 [cs.SD])","link":"http://arxiv.org/abs/2401.14664","description":"<p>Dysarthric speech reconstruction (DSR) systems aim to automatically convert\ndysarthric speech into normal-sounding speech. The technology eases\ncommunication with speakers affected by the neuromotor disorder and enhances\ntheir social inclusion. NED-based (Neural Encoder-Decoder) systems have\nsignificantly improved the intelligibility of the reconstructed speech as\ncompared with GAN-based (Generative Adversarial Network) approaches, but the\napproach is still limited by training inefficiency caused by the cascaded\npipeline and auxiliary tasks of the content encoder, which may in turn affect\nthe quality of reconstruction. Inspired by self-supervised speech\nrepresentation learning and discrete speech units, we propose a Unit-DSR\nsystem, which harnesses the powerful domain-adaptation capacity of HuBERT for\ntraining efficiency improvement and utilizes speech units to constrain the\ndysarthric content restoration in a discrete linguistic space. Compared with\nNED approaches, the Unit-DSR system only consists of a speech unit normalizer\nand a Unit HiFi-GAN vocoder, which is considerably simpler without cascaded\nsub-modules or auxiliary tasks. Results on the UASpeech corpus indicate that\nUnit-DSR outperforms competitive baselines in terms of content restoration,\nreaching a 28.2% relative average word error rate reduction when compared to\noriginal dysarthric speech, and shows robustness against speed perturbation and\nnoise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuejiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xixin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Disong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lingwei Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaLLaM -- Malaysia Large Language Model. (arXiv:2401.14680v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14680","description":"<p>Addressing the gap in Large Language Model pretrained from scratch with\nMalaysian context, We trained models with 1.1 billion, 3 billion, and 5 billion\nparameters on a substantial 349GB dataset, equivalent to 90 billion tokens\nbased on our pretrained Byte Pair Encoding (BPE) tokenizer for a single epoch.\nMaLLaM contributes to enhanced natural language understanding and generation\ntasks in the Malay language. Although trained on a smaller dataset of 90\nbillion tokens, our instruction-tuned MaLLaM models perform competitively. When\ncompared to ChatGPT3.5 and Malaysian Mistral, MaLLaM's instruction-tuned models\ndemonstrate notable proficiency, underscoring the effectiveness of our approach\nin capturing and understanding the nuances of the Malaysian language. MaLLaM\nmodels mark a significant contribution to the field, providing comprehensive\nlanguage representations grounded in Malaysian context. This endeavor aims to\npave the way for enhanced natural language understanding and generation tasks\nspecific to the linguistic nuances present in Malaysia. We discuss the training\nmethodology, dataset composition, and the potential impact of MaLLaM in\nadvancing the capabilities of large language models within the context of the\nMalay language.\n</p>\n<p>All models released at\nhttps://huggingface.co/collections/mesolitica/mallam-6577b59d1e0b436ae75f930f\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zolkepli_H/0/1/0/all/0/1\">Husein Zolkepli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razak_A/0/1/0/all/0/1\">Aisyah Razak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adha_K/0/1/0/all/0/1\">Kamarul Adha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nazhan_A/0/1/0/all/0/1\">Ariff Nazhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MasonTigers@LT-EDI-2024: An Ensemble Approach towards Detecting Homophobia and Transphobia in Social Media Comments. (arXiv:2401.14681v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14681","description":"<p>In this paper, we describe our approaches and results for Task 2 of the\nLT-EDI 2024 Workshop, aimed at detecting homophobia and/or transphobia across\nten languages. Our methodologies include monolingual transformers and ensemble\nmethods, capitalizing on the strengths of each to enhance the performance of\nthe models. The ensemble models worked well, placing our team, MasonTigers, in\nthe top five for eight of the ten languages, as measured by the macro F1 score.\nOur work emphasizes the efficacy of ensemble methods in multilingual scenarios,\naddressing the complexities of language-specific tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goswami_D/0/1/0/all/0/1\">Dhiman Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puspo_S/0/1/0/all/0/1\">Sadiya Sayara Chowdhury Puspo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raihan_M/0/1/0/all/0/1\">Md Nishat Raihan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emran_A/0/1/0/all/0/1\">Al Nahian Bin Emran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with Large Vision-Language Model Support. (arXiv:2401.14688v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14688","description":"<p>Recent advancements in text-to-image models have significantly enhanced image\ngeneration capabilities, yet a notable gap of open-source models persists in\nbilingual or Chinese language support. To address this need, we present\nTaiyi-Diffusion-XL, a new Chinese and English bilingual text-to-image model\nwhich is developed by extending the capabilities of CLIP and\nStable-Diffusion-XL through a process of bilingual continuous pre-training.\nThis approach includes the efficient expansion of vocabulary by integrating the\nmost frequently used Chinese characters into CLIP's tokenizer and embedding\nlayers, coupled with an absolute position encoding expansion. Additionally, we\nenrich text prompts by large vision-language model, leading to better images\ncaptions and possess higher visual quality. These enhancements are subsequently\napplied to downstream text-to-image models. Our empirical results indicate that\nthe developed CLIP model excels in bilingual image-text retrieval.Furthermore,\nthe bilingual image generation capabilities of Taiyi-Diffusion-XL surpass\nprevious models. This research leads to the development and open-sourcing of\nthe Taiyi-Diffusion-XL model, representing a notable advancement in the field\nof image generation, particularly for Chinese language applications. This\ncontribution is a step forward in addressing the need for more diverse language\nsupport in multimodal research. The model and demonstration are made publicly\navailable at\n\\href{https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-XL-3.5B/}{this\nhttps URL}, fostering further research and collaboration in this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaojun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dixiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_R/0/1/0/all/0/1\">Ruyi Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Junyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Renliang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingjian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yan Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Under the Surface: Tracking the Artifactuality of LLM-Generated Data. (arXiv:2401.14698v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14698","description":"<p>This work delves into the expanding role of large language models (LLMs) in\ngenerating artificial data. LLMs are increasingly employed to create a variety\nof outputs, including annotations, preferences, instruction prompts, simulated\ndialogues, and free text. As these forms of LLM-generated data often intersect\nin their application, they exert mutual influence on each other and raise\nsignificant concerns about the quality and diversity of the artificial data\nincorporated into training cycles, leading to an artificial data ecosystem. To\nthe best of our knowledge, this is the first study to aggregate various types\nof LLM-generated text data, from more tightly constrained data like \"task\nlabels\" to more lightly constrained \"free-form text\". We then stress test the\nquality and implications of LLM-generated artificial data, comparing it with\nhuman data across various existing benchmarks. Despite artificial data's\ncapability to match human performance, this paper reveals significant hidden\ndisparities, especially in complex tasks where LLMs often miss the nuanced\nunderstanding of intrinsic human-generated content. This study critically\nexamines diverse LLM-generated data and emphasizes the need for ethical\npractices in data creation and when using LLMs. It highlights the LLMs'\nshortcomings in replicating human traits and behaviors, underscoring the\nimportance of addressing biases and artifacts produced in LLM-generated content\nfor future research and development. All data and code are available on our\nproject page.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Debarati Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langis_K/0/1/0/all/0/1\">Karin De Langis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1\">Anna Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaehyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minhwa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Z/0/1/0/all/0/1\">Zae Myung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayati_S/0/1/0/all/0/1\">Shirley Hayati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owan_R/0/1/0/all/0/1\">Risako Owan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parkar_R/0/1/0/all/0/1\">Ritik Parkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_R/0/1/0/all/0/1\">Ryan Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jonginn Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_A/0/1/0/all/0/1\">Aahan Tyagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferland_L/0/1/0/all/0/1\">Libby Ferland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Sanjali Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_V/0/1/0/all/0/1\">Vincent Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Turn-taking and Backchannel Prediction with Acoustic and Large Language Model Fusion. (arXiv:2401.14717v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14717","description":"<p>We propose an approach for continuous prediction of turn-taking and\nbackchanneling locations in spoken dialogue by fusing a neural acoustic model\nwith a large language model (LLM). Experiments on the Switchboard human-human\nconversation dataset demonstrate that our approach consistently outperforms the\nbaseline models with single modality. We also develop a novel multi-task\ninstruction fine-tuning strategy to further benefit from LLM-encoded knowledge\nfor understanding the tasks and conversational contexts, leading to additional\nimprovements. Our approach demonstrates the potential of combined LLMs and\nacoustic models for a more natural and conversational interaction between\nhumans and speech-enabled AI agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khare_A/0/1/0/all/0/1\">Aparna Khare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raju_A/0/1/0/all/0/1\">Anirudh Raju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dheram_P/0/1/0/all/0/1\">Pranav Dheram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minhua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1\">Andreas Stolcke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_V/0/1/0/all/0/1\">Venkatesh Ravichandran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Model Adaptation for Financial Sentiment Analysis. (arXiv:2401.14777v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14777","description":"<p>Natural language processing (NLP) has recently gained relevance within\nfinancial institutions by providing highly valuable insights into companies and\nmarkets' financial documents. However, the landscape of the financial domain\npresents extra challenges for NLP, due to the complexity of the texts and the\nuse of specific terminology. Generalist language models tend to fall short in\ntasks specifically tailored for finance, even when using large language models\n(LLMs) with great natural language understanding and generative capabilities.\nThis paper presents a study on LLM adaptation methods targeted at the financial\ndomain and with high emphasis on financial sentiment analysis. To this purpose,\ntwo foundation models with less than 1.5B parameters have been adapted using a\nwide range of strategies. We show that through careful fine-tuning on both\nfinancial documents and instructions, these foundation models can be adapted to\nthe target domain. Moreover, we observe that small LLMs have comparable\nperformance to larger scale models, while being more efficient in terms of\nparameters and data. In addition to the models, we show how to generate\nartificial instructions through LLMs to augment the number of samples of the\ninstruction dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inserte_P/0/1/0/all/0/1\">Pau Rodriguez Inserte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakhle_M/0/1/0/all/0/1\">Mariam Nakhl&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qader_R/0/1/0/all/0/1\">Raheel Qader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caillaut_G/0/1/0/all/0/1\">Gaetan Caillaut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingshu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChemDFM: Dialogue Foundation Model for Chemistry. (arXiv:2401.14818v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14818","description":"<p>Large language models (LLMs) have established great success in the general\ndomain of natural language processing. Their emerging task generalization and\nfree-form dialogue capabilities can greatly help to design Chemical General\nIntelligence (CGI) to assist real-world research in chemistry. However, the\nexistence of specialized language and knowledge in the field of chemistry, such\nas the highly informative SMILES notation, hinders the performance of\ngeneral-domain LLMs in chemistry. To this end, we develop ChemDFM, the first\nLLM towards CGI. ChemDFM-13B is trained on 34B tokens from chemical literature,\ntextbooks, and instructions as well as various data from the general domain.\nTherefore, it can store, understand, and reason over chemical knowledge and\nlanguages while still possessing advanced free-form language comprehension\ncapabilities. Extensive quantitative evaluation shows that ChemDFM can\nsignificantly outperform the representative open-sourced LLMs. Moreover,\nChemDFM can also surpass GPT-4 on a great portion of chemical tasks, despite\nthe significant size difference. Further qualitative evaluations demonstrate\nthe efficiency and effectiveness of ChemDFM in real-world research scenarios.\nWe will open-source the ChemDFM model soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zihan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1\">Da Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Liangtai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zihao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongshen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zichen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Su Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1\">Shuai Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_G/0/1/0/all/0/1\">Guodong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Kai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods. (arXiv:2401.14869v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14869","description":"<p>Large language models (LLMs) garner significant attention for their\nunprecedented performance, leading to an increasing number of researches\nevaluating LLMs. However, these evaluation benchmarks are limited to assessing\nthe instruction-following capabilities, overlooking the fundamental abilities\nthat emerge during the pre-training stage. Previous subjective evaluation\nmethods mainly reply on scoring by API models. However, in the absence of\nreferences, large models have shown limited ability to discern subtle\ndifferences. To bridge the gap, we propose F-Eval, a bilingual evaluation\nbenchmark to evaluate the fundamental abilities, including expression,\ncommonsense and logic. The tasks in F-Eval include multi-choice objective\ntasks, open-ended objective tasks, reference-based subjective tasks and\nreference-free subjective tasks. For reference-free subjective tasks, we devise\nnew evaluation methods, serving as alternatives to scoring by API models. We\nconduct evaluations on 13 advanced LLMs. Results show that our evaluation\nmethods show higher correlation coefficients and larger distinction than other\nevaluators. Additionally, we discuss the influence of different model sizes,\ndimensions, and normalization methods. We anticipate that F-Eval will\nfacilitate the study of LLMs' fundamental abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Keyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shujie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qipeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Power of Noise: Redefining Retrieval for RAG Systems. (arXiv:2401.14887v1 [cs.IR])","link":"http://arxiv.org/abs/2401.14887","description":"<p>Retrieval-Augmented Generation (RAG) systems represent a significant\nadvancement over traditional Large Language Models (LLMs). RAG systems enhance\ntheir generation ability by incorporating external data retrieved through an\nInformation Retrieval (IR) phase, overcoming the limitations of standard LLMs,\nwhich are restricted to their pre-trained knowledge and limited context window.\nMost research in this area has predominantly concentrated on the generative\naspect of LLMs within RAG systems. Our study fills this gap by thoroughly and\ncritically analyzing the influence of IR components on RAG systems. This paper\nanalyzes which characteristics a retriever should possess for an effective\nRAG's prompt formulation, focusing on the type of documents that should be\nretrieved. We evaluate various elements, such as the relevance of the documents\nto the prompt, their position, and the number included in the context. Our\nfindings reveal, among other insights, that including irrelevant documents can\nunexpectedly enhance performance by more than 30% in accuracy, contradicting\nour initial assumption of diminished quality. These findings call for\ndeveloping specialized approaches tailored to the specific demands of\nintegrating retrieval with language generation models and pave the way for\nfuture research. These results underscore the need for developing specialized\nstrategies to integrate retrieval with language generation models, thereby\nlaying the groundwork for future research in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cuconasu_F/0/1/0/all/0/1\">Florin Cuconasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trappolini_G/0/1/0/all/0/1\">Giovanni Trappolini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siciliano_F/0/1/0/all/0/1\">Federico Siciliano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filice_S/0/1/0/all/0/1\">Simone Filice</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campagnano_C/0/1/0/all/0/1\">Cesare Campagnano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maarek_Y/0/1/0/all/0/1\">Yoelle Maarek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonellotto_N/0/1/0/all/0/1\">Nicola Tonellotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1\">Fabrizio Silvestri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparison of parameters of vowel sounds of russian and english languages. (arXiv:2401.14890v1 [cs.SD])","link":"http://arxiv.org/abs/2401.14890","description":"<p>In multilingual speech recognition systems, a situation can often arise when\nthe language is not known in advance, but the signal has already been received\nand is being processed. For such cases, some generalized model is needed that\nwill be able to respond to phonetic differences and, depending on them,\ncorrectly recog-nize speech in the desired language. To build such a model, it\nis necessary to set the values of phonetic parameters, and then compare similar\nsounds, establishing significant differences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fedoseev_V/0/1/0/all/0/1\">V.I. Fedoseev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konev_A/0/1/0/all/0/1\">A.A. Konev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yakimuk_A/0/1/0/all/0/1\">A. Yu. Yakimuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do LLMs Dream of Ontologies?. (arXiv:2401.14931v1 [cs.CL])","link":"http://arxiv.org/abs/2401.14931","description":"<p>Large language models (LLMs) have recently revolutionized automated text\nunderstanding and generation. The performance of these models relies on the\nhigh number of parameters of the underlying neural architectures, which allows\nLLMs to memorize part of the vast quantity of data seen during the training.\nThis paper investigates whether and to what extent general-purpose pre-trained\nLLMs have memorized information from known ontologies. Our results show that\nLLMs partially know ontologies: they can, and do indeed, memorize concepts from\nontologies mentioned in the text, but the level of memorization of their\nconcepts seems to vary proportionally to their popularity on the Web, the\nprimary source of their training material. We additionally propose new metrics\nto estimate the degree of memorization of ontological information in LLMs by\nmeasuring the consistency of the output produced across different prompt\nrepetitions, query languages, and degrees of determinism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bombieri_M/0/1/0/all/0/1\">Marco Bombieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiorini_P/0/1/0/all/0/1\">Paolo Fiorini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponzetto_S/0/1/0/all/0/1\">Simone Paolo Ponzetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rospocher_M/0/1/0/all/0/1\">Marco Rospocher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embedding-based search in JetBrains IDEs. (arXiv:2401.14975v1 [cs.SE])","link":"http://arxiv.org/abs/2401.14975","description":"<p>Most modern Integrated Development Environments (IDEs) and code editors have\na feature to search across available functionality and items in an open\nproject. In JetBrains IDEs, this feature is called Search Everywhere: it allows\nusers to search for files, actions, classes, symbols, settings, and anything\nfrom VCS history from a single entry point. However, it works with the\ncandidates obtained by algorithms that don't account for semantics, e.g.,\nsynonyms, complex word permutations, part of the speech modifications, and\ntypos. In this work, we describe the machine learning approach we implemented\nto improve the discoverability of search items. We also share the obstacles\nencountered during this process and how we overcame them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abramov_E/0/1/0/all/0/1\">Evgeny Abramov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palchikov_N/0/1/0/all/0/1\">Nikolai Palchikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Airavata: Introducing Hindi Instruction-tuned LLM. (arXiv:2401.15006v1 [cs.CL])","link":"http://arxiv.org/abs/2401.15006","description":"<p>We announce the initial release of \"Airavata,\" an instruction-tuned LLM for\nHindi. Airavata was created by fine-tuning OpenHathi with diverse,\ninstruction-tuning Hindi datasets to make it better suited for assistive tasks.\nAlong with the model, we also share the IndicInstruct dataset, which is a\ncollection of diverse instruction-tuning datasets to enable further research\nfor Indic LLMs. Additionally, we present evaluation benchmarks and a framework\nfor assessing LLM performance across tasks in Hindi. Currently, Airavata\nsupports Hindi, but we plan to expand this to all 22 scheduled Indic languages.\nYou can access all artifacts at https://ai4bharat.github.io/airavata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gala_J/0/1/0/all/0/1\">Jay Gala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayakumar_T/0/1/0/all/0/1\">Thanmay Jayakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Husain_J/0/1/0/all/0/1\">Jaavid Aktar Husain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M_A/0/1/0/all/0/1\">Aswanth Kumar M</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mohammed Safi Ur Rahman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanojia_D/0/1/0/all/0/1\">Diptesh Kanojia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puduppully_R/0/1/0/all/0/1\">Ratish Puduppully</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murthy_R/0/1/0/all/0/1\">Rudra Murthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SliceGPT: Compress Large Language Models by Deleting Rows and Columns. (arXiv:2401.15024v1 [cs.LG])","link":"http://arxiv.org/abs/2401.15024","description":"<p>Large language models have become the cornerstone of natural language\nprocessing, but their use comes with substantial costs in terms of compute and\nmemory resources. Sparsification provides a solution to alleviate these\nresource constraints, and recent works have shown that trained models can be\nsparsified post-hoc. Existing sparsification techniques face challenges as they\nneed additional data structures and offer constrained speedup with current\nhardware. In this paper we present SliceGPT, a new post-training sparsification\nscheme which replaces each weight matrix with a smaller (dense) matrix,\nreducing the embedding dimension of the network. Through extensive\nexperimentation, we show that SliceGPT can remove up to 25% of the model\nparameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models\nwhile maintaining 99%, 99% and 90% zero-shot task performance of the dense\nmodel respectively. Our sliced models run on fewer GPUs and run faster without\nany additional code optimization: on 24GB consumer GPUs we reduce the total\ncompute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB\nA100 GPUs we reduce it to 66%. We offer a new insight, computational invariance\nin transformer networks, which enables SliceGPT and we hope it will inspire and\nenable future avenues to reduce memory and computation demands for pre-trained\nmodels. Code is available at:\nhttps://github.com/microsoft/TransformerCompression\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ashkboos_S/0/1/0/all/0/1\">Saleh Ashkboos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Croci_M/0/1/0/all/0/1\">Maximilian L. Croci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nascimento_M/0/1/0/all/0/1\">Marcelo Gennari do Nascimento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1\">Torsten Hoefler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hensman_J/0/1/0/all/0/1\">James Hensman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models. (arXiv:2401.15042v1 [cs.CL])","link":"http://arxiv.org/abs/2401.15042","description":"<p>Large Language Models (LLMs) have exhibited remarkable success in long-form\ncontext comprehension tasks. However, their capacity to generate long contents,\nsuch as reports and articles, remains insufficiently explored. Current\nbenchmarks do not adequately assess LLMs' ability to produce informative and\ncomprehensive content, necessitating a more rigorous evaluation approach. In\nthis study, we introduce \\textsc{ProxyQA}, a framework for evaluating long-form\ntext generation, comprising in-depth human-curated \\textit{meta-questions}\nspanning various domains. Each meta-question contains corresponding\n\\textit{proxy-questions} with annotated answers. LLMs are prompted to generate\nextensive content in response to these meta-questions. Utilizing an evaluator\nand incorporating generated content as background context, \\textsc{ProxyQA}\nevaluates the quality of generated content based on the evaluator's performance\nin answering the \\textit{proxy-questions}. We examine multiple LLMs,\nemphasizing \\textsc{ProxyQA}'s demanding nature as a high-quality assessment\ntool. Human evaluation demonstrates that evaluating through\n\\textit{proxy-questions} is a highly self-consistent and\nhuman-criteria-correlated validation method. The dataset and leaderboard will\nbe available at \\url{https://github.com/Namco0816/ProxyQA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Haochen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhijiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhili Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linqi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning. (arXiv:2401.15043v1 [cs.CL])","link":"http://arxiv.org/abs/2401.15043","description":"<p>Objective: The reading level of health educational materials significantly\ninfluences information understandability and accessibility, particularly for\nminoritized populations. Many patient educational resources surpass the reading\nlevel and complexity of widely accepted standards. There is a critical need for\nhigh-performing text simplification models in health information to enhance\ndissemination and literacy. This need is particularly acute in cancer\neducation, where effective prevention and screening education can substantially\nreduce morbidity and mortality.\n</p>\n<p>Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel\ncorpus of cancer education materials tailored for health text simplification\nresearch. Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore\nLarge Language Model (LLM)-based simplification methods, including fine-tuning,\nreinforcement learning (RL), reinforcement learning with human feedback (RLHF),\ndomain adaptation, and prompt-based approaches. Our experimentation encompasses\nLlama 2 and GPT-4. A novel RLHF reward function is introduced, featuring a\nlightweight model adept at distinguishing between original and simplified\ntexts, thereby enhancing the model's effectiveness with unlabeled data.\n</p>\n<p>Results: Fine-tuned Llama 2 models demonstrated high performance across\nvarious metrics. Our innovative RLHF reward function surpassed existing RL text\nsimplification reward functions in effectiveness. The results underscore that\nRL/RLHF can augment fine-tuning, facilitating model training on unlabeled text\nand improving performance. Additionally, these methods effectively adapt\nout-of-domain text simplification models to targeted domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md Mushfiqur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1\">Mohammad Sabik Irbaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+North_K/0/1/0/all/0/1\">Kai North</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_M/0/1/0/all/0/1\">Michelle S. Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lybarger_K/0/1/0/all/0/1\">Kevin Lybarger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LongFin: A Multimodal Document Understanding Model for Long Financial Domain Documents. (arXiv:2401.15050v1 [cs.CL])","link":"http://arxiv.org/abs/2401.15050","description":"<p>Document AI is a growing research field that focuses on the comprehension and\nextraction of information from scanned and digital documents to make everyday\nbusiness operations more efficient. Numerous downstream tasks and datasets have\nbeen introduced to facilitate the training of AI models capable of parsing and\nextracting information from various document types such as receipts and scanned\nforms. Despite these advancements, both existing datasets and models fail to\naddress critical challenges that arise in industrial contexts. Existing\ndatasets primarily comprise short documents consisting of a single page, while\nexisting models are constrained by a limited maximum length, often set at 512\ntokens. Consequently, the practical application of these methods in financial\nservices, where documents can span multiple pages, is severely impeded. To\novercome these challenges, we introduce LongFin, a multimodal document AI model\ncapable of encoding up to 4K tokens. We also propose the LongForms dataset, a\ncomprehensive financial dataset that encapsulates several industrial challenges\nin financial documents. Through an extensive evaluation, we demonstrate the\neffectiveness of the LongFin model on the LongForms dataset, surpassing the\nperformance of existing public models while maintaining comparable results on\nexisting single-page benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Masry_A/0/1/0/all/0/1\">Ahmed Masry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajian_A/0/1/0/all/0/1\">Amir Hajian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning-based approach for tomato classification in complex scenes. (arXiv:2401.15055v1 [cs.CV])","link":"http://arxiv.org/abs/2401.15055","description":"<p>Tracking ripening tomatoes is time consuming and labor intensive. Artificial\nintelligence technologies combined with those of computer vision can help users\noptimize the process of monitoring the ripening status of plants. To this end,\nwe have proposed a tomato ripening monitoring approach based on deep learning\nin complex scenes. The objective is to detect mature tomatoes and harvest them\nin a timely manner. The proposed approach is declined in two parts. Firstly,\nthe images of the scene are transmitted to the pre-processing layer. This\nprocess allows the detection of areas of interest (area of the image containing\ntomatoes). Then, these images are used as input to the maturity detection\nlayer. This layer, based on a deep neural network learning algorithm,\nclassifies the tomato thumbnails provided to it in one of the following five\ncategories: green, brittle, pink, pale red, mature red. The experiments are\nbased on images collected from the internet gathered through searches using\ntomato state across diverse languages including English, German, French, and\nSpanish. The experimental results of the maturity detection layer on a dataset\ncomposed of images of tomatoes taken under the extreme conditions, gave a good\nclassification rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mousse_M/0/1/0/all/0/1\">Mikael A. Mousse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atohoun_B/0/1/0/all/0/1\">Bethel C. A. R. K. Atohoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motamed_C/0/1/0/all/0/1\">Cina Motamed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pairing Orthographically Variant Literary Words to Standard Equivalents Using Neural Edit Distance Models. (arXiv:2401.15068v1 [cs.CL])","link":"http://arxiv.org/abs/2401.15068","description":"<p>We present a novel corpus consisting of orthographically variant words found\nin works of 19th century U.S. literature annotated with their corresponding\n\"standard\" word pair. We train a set of neural edit distance models to pair\nthese variants with their standard forms, and compare the performance of these\nmodels to the performance of a set of neural edit distance models trained on a\ncorpus of orthographic errors made by L2 English learners. Finally, we analyze\nthe relative performance of these models in the light of different negative\ntraining sample generation strategies, and offer concluding remarks on the\nunique challenge literary orthographic variation poses to string pairing\nmethodologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Messner_C/0/1/0/all/0/1\">Craig Messner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippincott_T/0/1/0/all/0/1\">Tom Lippincott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty. (arXiv:2401.15077v1 [cs.LG])","link":"http://arxiv.org/abs/2401.15077","description":"<p>Auto-regressive decoding makes the inference of Large Language Models (LLMs)\ntime-consuming. We propose a simple framework, EAGLE (Extrapolation Algorithm\nfor Greater Language-model Efficiency), for lossless acceleration. Unlike\ntraditional speculative sampling methods, EAGLE operates the drafting process\nauto-regressively at the more regular (second-top-layer) feature level and\naddresses the sampling uncertainty issues in the next-feature prediction\nproblems by integrating tokens from one time step ahead. The acceleration\nprovided by EAGLE is lossless: it involves no fine-tuning of the target LLM,\nand the generated text maintains the same distribution as that of vanilla\nauto-regressive decoding. As of the submission of this paper, EAGLE is the\nfastest known framework within the speculative sampling family. On MT-bench,\nEAGLE is 3x faster than vanilla decoding, 2x faster than Lookahead, and 1.6x\nfaster than Medusa. Using gpt-fast, EAGLE attains on average 160 tokens/s with\nLLaMA2-Chat 13B on a single RTX 3090 GPU, compared to 24 tokens/s of\nHuggingface's implementations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Fangyun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongyang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A* shortest string decoding for non-idempotent semirings. (arXiv:2204.07236v2 [cs.FL] UPDATED)","link":"http://arxiv.org/abs/2204.07236","description":"<p>The single shortest path algorithm is undefined for weighted finite-state\nautomata over non-idempotent semirings because such semirings do not guarantee\nthe existence of a shortest path. However, in non-idempotent semirings\nadmitting an order satisfying a monotonicity condition (such as the plus-times\nor log semirings), the notion of shortest string is well-defined. We describe\nan algorithm which finds the shortest string for a weighted non-deterministic\nautomaton over such semirings using the backwards shortest distance of an\nequivalent deterministic automaton (DFA) as a heuristic for A* search performed\nover a companion idempotent semiring, which is proven to return the shortest\nstring. While there may be exponentially more states in the DFA, this algorithm\nneeds to visit only a small fraction of them if determinization is performed\n\"on the fly\".\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gorman_K/0/1/0/all/0/1\">Kyle Gorman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allauzen_C/0/1/0/all/0/1\">Cyril Allauzen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation. (arXiv:2302.14220v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.14220","description":"<p>Pretrained character-level and byte-level language models have been shown to\nbe competitive with popular subword models across a range of Natural Language\nProcessing (NLP) tasks. However, there has been little research on their\neffectiveness for neural machine translation (NMT), particularly within the\npopular pretrain-then-finetune paradigm. This work performs an extensive\ncomparison across multiple languages and experimental conditions of character-\nand subword-level pretrained models (ByT5 and mT5, respectively) on NMT. We\nshow the effectiveness of character-level modeling in translation, particularly\nin cases where fine-tuning data is limited. In our analysis, we show how\ncharacter models' gains in translation quality are reflected in better\ntranslations of orthographically similar words and rare words. While evaluating\nthe importance of source texts in driving model predictions, we highlight\nword-level patterns within ByT5, suggesting an ability to modulate word-level\nand character-level information during generation. We conclude by assessing the\nefficiency tradeoff of byte models, suggesting their usage in non-time-critical\nscenarios to boost translation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Edman_L/0/1/0/all/0/1\">Lukas Edman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarti_G/0/1/0/all/0/1\">Gabriele Sarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toral_A/0/1/0/all/0/1\">Antonio Toral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noord_G/0/1/0/all/0/1\">Gertjan van Noord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisazza_A/0/1/0/all/0/1\">Arianna Bisazza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations. (arXiv:2304.14827v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.14827","description":"<p>This paper aims to quantitatively evaluate the performance of ChatGPT, an\ninteractive large language model, on inter-sentential relations such as\ntemporal relations, causal relations, and discourse relations. Given ChatGPT's\npromising performance across various tasks, we proceed to carry out thorough\nevaluations on the whole test sets of 11 datasets, including temporal and\ncausal relations, PDTB2.0-based, and dialogue-based discourse relations. To\nensure the reliability of our findings, we employ three tailored prompt\ntemplates for each task, including the zero-shot prompt template, zero-shot\nprompt engineering (PE) template, and in-context learning (ICL) prompt\ntemplate, to establish the initial baseline scores for all popular\nsentence-pair relation classification tasks for the first time. Through our\nstudy, we discover that ChatGPT exhibits exceptional proficiency in detecting\nand reasoning about causal relations, albeit it may not possess the same level\nof expertise in identifying the temporal order between two events. While it is\ncapable of identifying the majority of discourse relations with existing\nexplicit discourse connectives, the implicit discourse relation remains a\nformidable challenge. Concurrently, ChatGPT demonstrates subpar performance in\nthe dialogue discourse parsing task that requires structural understanding in a\ndialogue before being aware of the discourse relation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Chunkit Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jiayang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1\">Tianqing Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiple output samples per input in a single-output Gaussian process. (arXiv:2306.02719v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.02719","description":"<p>The standard Gaussian Process (GP) only considers a single output sample per\ninput in the training set. Datasets for subjective tasks, such as spoken\nlanguage assessment, may be annotated with output labels from multiple human\nraters per input. This paper proposes to generalise the GP to allow for these\nmultiple output samples in the training set, and thus make use of available\noutput uncertainty information. This differs from a multi-output GP, as all\noutput samples are from the same task here. The output density function is\nformulated to be the joint likelihood of observing all output samples, and\nlatent variables are not repeated to reduce computation cost. The test set\npredictions are inferred similarly to a standard GP, with a difference being in\nthe optimised hyper-parameters. This is evaluated on speechocean762, showing\nthat it allows the GP to compute a test set output distribution that is more\nsimilar to the collection of reference outputs from the multiple human raters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Jeremy H. M. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huayun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Retrieve In-Context Examples for Large Language Models. (arXiv:2307.07164v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.07164","description":"<p>Large language models (LLMs) have demonstrated their ability to learn\nin-context, allowing them to perform various tasks based on a few input-output\nexamples. However, the effectiveness of in-context learning is heavily reliant\non the quality of the selected examples. In this paper, we propose a novel\nframework to iteratively train dense retrievers that can identify high-quality\nin-context examples for LLMs. Our framework initially trains a reward model\nbased on LLM feedback to evaluate the quality of candidate examples, followed\nby knowledge distillation to train a bi-encoder based dense retriever. Our\nexperiments on a suite of $30$ tasks demonstrate that our framework\nsignificantly enhances in-context learning performance. Furthermore, we show\nthe generalization ability of our framework to unseen tasks during training. An\nin-depth analysis reveals that our model improves performance by retrieving\nexamples with similar patterns, and the gains are consistent across LLMs of\nvarying sizes. The code and data are available at\nhttps://github.com/microsoft/LMOps/tree/main/llm_retriever .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Nan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Dataset Annotation Quality Management in the Wild. (arXiv:2307.08153v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.08153","description":"<p>Data quality is crucial for training accurate, unbiased, and trustworthy\nmachine learning models as well as for their correct evaluation. Recent works,\nhowever, have shown that even popular datasets used to train and evaluate\nstate-of-the-art models contain a non-negligible amount of erroneous\nannotations, biases, or artifacts. While practices and guidelines regarding\ndataset creation projects exist, to our knowledge, large-scale analysis has yet\nto be performed on how quality management is conducted when creating natural\nlanguage datasets and whether these recommendations are followed. Therefore, we\nfirst survey and summarize recommended quality management practices for dataset\ncreation as described in the literature and provide suggestions for applying\nthem. Then, we compile a corpus of 591 scientific publications introducing text\ndatasets and annotate it for quality-related aspects, such as annotator\nmanagement, agreement, adjudication, or data validation. Using these\nannotations, we then analyze how quality management is conducted in practice. A\nmajority of the annotated publications apply good or excellent quality\nmanagement. However, we deem the effort of 30\\% of the works as only subpar.\nOur analysis also shows common errors, especially when using inter-annotator\nagreement and computing annotation error rates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klie_J/0/1/0/all/0/1\">Jan-Christoph Klie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castilho_R/0/1/0/all/0/1\">Richard Eckart de Castilho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems. (arXiv:2307.11394v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.11394","description":"<p>MeetEval is an open-source toolkit to evaluate all kinds of meeting\ntranscription systems. It provides a unified interface for the computation of\ncommonly used Word Error Rates (WERs), specifically cpWER, ORC-WER and MIMO-WER\nalong other WER definitions. We extend the cpWER computation by a temporal\nconstraint to ensure that only words are identified as correct when the\ntemporal alignment is plausible. This leads to a better quality of the matching\nof the hypothesis string to the reference string that more closely resembles\nthe actual transcription quality, and a system is penalized if it provides poor\ntime annotations. Since word-level timing information is often not available,\nwe present a way to approximate exact word-level timings from segment-level\ntimings (e.g., a sentence) and show that the approximation leads to a similar\nWER as a matching with exact word-level annotations. At the same time, the time\nconstraint leads to a speedup of the matching algorithm, which outweighs the\nadditional overhead caused by processing the time stamps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neumann_T/0/1/0/all/0/1\">Thilo von Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boeddeker_C/0/1/0/all/0/1\">Christoph Boeddeker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delcroix_M/0/1/0/all/0/1\">Marc Delcroix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haeb_Umbach_R/0/1/0/all/0/1\">Reinhold Haeb-Umbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Zero-Shot Instruction Following. (arXiv:2308.03795v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.03795","description":"<p>This work proposes a challenging yet more realistic setting for zero-shot\ncross-task generalization: zero-shot instruction following, presuming the\nexistence of a paragraph-style task definition while no demonstrations exist.\nTo better learn the task supervision from the definition, we propose two\nstrategies: first, to automatically find out the critical sentences in the\ndefinition; second, a ranking objective to force the model to generate the gold\noutputs with higher probabilities when those critical parts are highlighted in\nthe definition. The joint efforts of the two strategies yield state-of-the-art\nperformance on the Super-NaturalInstructions. Our code is available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_R/0/1/0/all/0/1\">Renze Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenpeng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Online Log Analysis Using Large Language Models with Prompt Strategies. (arXiv:2308.07610v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2308.07610","description":"<p>Automated log analysis is crucial in modern software-intensive systems for\nfacilitating program comprehension throughout software maintenance and\nengineering life cycles. Existing methods perform tasks such as log parsing and\nlog anomaly detection by providing a single prediction value without\ninterpretation. However, given the increasing volume of system events, the\nlimited interpretability of analysis results hinders analysts' comprehension of\nprogram status and their ability to take appropriate actions. Moreover, these\nmethods require substantial in-domain training data, and their performance\ndeclines sharply (by up to 62.5%) in online scenarios involving unseen logs\nfrom new domains, a common occurrence due to rapid software updates. In this\npaper, we propose LogPrompt, a novel interpretable log analysis approach for\nonline scenarios. LogPrompt employs large language models (LLMs) to perform\nonline log analysis tasks via a suite of advanced prompt strategies tailored\nfor log tasks, which enhances LLMs' performance by up to 380.7% compared with\nsimple prompts. Experiments on nine publicly available evaluation datasets\nacross two tasks demonstrate that LogPrompt, despite requiring no in-domain\ntraining, outperforms existing approaches trained on thousands of logs by up to\n55.9%. We also conduct a human evaluation of LogPrompt's interpretability, with\nsix practitioners possessing over 10 years of experience, who highly rated the\ngenerated content in terms of usefulness and readability (averagely 4.42/5).\nLogPrompt also exhibits remarkable compatibility with open-source and\nsmaller-scale LLMs, making it flexible for practical deployment. Code of\nLogPrompt is available at https://github.com/lunyiliu/LogPrompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yilun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Shimin Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_W/0/1/0/all/0/1\">Weibin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wenbing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanqing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yanfei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering. (arXiv:2308.12060v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.12060","description":"<p>Knowledge base question answering (KBQA) is a critical yet challenging task\ndue to the vast number of entities within knowledge bases and the diversity of\nnatural language questions posed by users. Unfortunately, the performance of\nmost KBQA models tends to decline significantly in real-world scenarios where\nhigh-quality annotated data is insufficient. To mitigate the burden associated\nwith manual annotation, we introduce FlexKBQA by utilizing Large Language\nModels (LLMs) as program translators for addressing the challenges inherent in\nthe few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithms\nto sample diverse programs, such as SPARQL queries, from the knowledge base,\nwhich are subsequently converted into natural language questions via LLMs. This\nsynthetic dataset facilitates training a specialized lightweight model for the\nKB. Additionally, to reduce the barriers of distribution shift between\nsynthetic data and real user questions, FlexKBQA introduces an executionguided\nself-training method to iterative leverage unlabeled user questions.\nFurthermore, we explore harnessing the inherent reasoning capability of LLMs to\nenhance the entire framework. Consequently, FlexKBQA delivers substantial\nflexibility, encompassing data annotation, deployment, and being domain\nagnostic. Through extensive experiments on GrailQA, WebQSP, and KQA Pro, we\nobserve that under the few-shot even the more challenging zero-shot scenarios,\nFlexKBQA achieves impressive results with a few annotations, surpassing all\nprevious baselines and even approaching the performance of supervised models,\nachieving a remarkable 93% performance relative to the fully-supervised models.\nWe posit that FlexKBQA represents a significant advancement towards exploring\nbetter integration of large and lightweight models. The code is open-sourced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1\">Sunqi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiuxing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1\">Zhichao Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bowen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HC3 Plus: A Semantic-Invariant Human ChatGPT Comparison Corpus. (arXiv:2309.02731v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.02731","description":"<p>ChatGPT has gained significant interest due to its impressive performance,\nbut people are increasingly concerned about its potential risks, particularly\naround the detection of AI-generated content (AIGC), which is often difficult\nfor untrained humans to identify. Current datasets utilized for detecting\nChatGPT-generated text primarily center around question-answering, yet they\ntend to disregard tasks that possess semantic-invariant properties, such as\nsummarization, translation, and paraphrasing. Our primary studies demonstrate\nthat detecting model-generated text on semantic-invariant tasks is more\ndifficult. To fill this gap, we introduce a more extensive and comprehensive\ndataset that considers more types of tasks than previous work, including\nsemantic-invariant tasks. In addition, the model after a large number of task\ninstruction fine-tuning shows a strong powerful performance. Owing to its\nprevious success, we further instruct fine-tuning T\\textit{k}-instruct and\nbuild a more powerful detection system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhenpeng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1\">Guangyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songlin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OYXOY: A Modern NLP Test Suite for Modern Greek. (arXiv:2309.07009v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.07009","description":"<p>This paper serves as a foundational step towards the development of a\nlinguistically motivated and technically relevant evaluation suite for Greek\nNLP. We initiate this endeavor by introducing four expert-verified evaluation\ntasks, specifically targeted at natural language inference, word sense\ndisambiguation (through example comparison or sense selection) and metaphor\ndetection. More than language-adapted replicas of existing tasks, we contribute\ntwo innovations which will resonate with the broader resource and evaluation\ncommunity. Firstly, our inference dataset is the first of its kind, marking not\njust \\textit{one}, but rather \\textit{all} possible inference labels,\naccounting for possible shifts due to e.g. ambiguity or polysemy. Secondly, we\ndemonstrate a cost-efficient method to obtain datasets for under-resourced\nlanguages. Using ChatGPT as a language-neutral parser, we transform the\nDictionary of Standard Modern Greek into a structured format, from which we\nderive the other three tasks through simple projections. Alongside each task,\nwe conduct experiments using currently available state of the art machinery.\nOur experimental baselines affirm the challenging nature of our tasks and\nhighlight the need for expedited progress in order for the Greek NLP ecosystem\nto keep pace with contemporary mainstream research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kogkalidis_K/0/1/0/all/0/1\">Konstantinos Kogkalidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzikyriakidis_S/0/1/0/all/0/1\">Stergios Chatzikyriakidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannikouri_E/0/1/0/all/0/1\">Eirini Chrysovalantou Giannikouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsouli_V/0/1/0/all/0/1\">Vassiliki Katsouli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klironomou_C/0/1/0/all/0/1\">Christina Klironomou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koula_C/0/1/0/all/0/1\">Christina Koula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadakis_D/0/1/0/all/0/1\">Dimitris Papadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasparaki_T/0/1/0/all/0/1\">Thelka Pasparaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Psaltaki_E/0/1/0/all/0/1\">Erofili Psaltaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakellariou_E/0/1/0/all/0/1\">Efthymia Sakellariou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soupiona_H/0/1/0/all/0/1\">Hara Soupiona</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning. (arXiv:2309.11082v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2309.11082","description":"<p>In recent years, the explosion of web videos makes text-video retrieval\nincreasingly essential and popular for video filtering, recommendation, and\nsearch. Text-video retrieval aims to rank relevant text/video higher than\nirrelevant ones. The core of this task is to precisely measure the cross-modal\nsimilarity between texts and videos. Recently, contrastive learning methods\nhave shown promising results for text-video retrieval, most of which focus on\nthe construction of positive and negative pairs to learn text and video\nrepresentations. Nevertheless, they do not pay enough attention to hard\nnegative pairs and lack the ability to model different levels of semantic\nsimilarity. To address these two issues, this paper improves contrastive\nlearning using two novel techniques. First, to exploit hard examples for robust\ndiscriminative power, we propose a novel Dual-Modal Attention-Enhanced Module\n(DMAE) to mine hard negative pairs from textual and visual clues. By further\nintroducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively\nidentify all these hard negatives and explicitly highlight their impacts in the\ntraining loss. Second, our work argues that triplet samples can better model\nfine-grained semantic similarity compared to pairwise samples. We thereby\npresent a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to\nconstruct partial order triplet samples by automatically generating\nfine-grained hard negatives for matched text-video pairs. The proposed TPM-CL\ndesigns an adaptive token masking strategy with cross-modal interaction to\nmodel subtle semantic differences. Extensive experiments demonstrate that the\nproposed approach outperforms existing methods on four widely-used text-video\nretrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xuzheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhongyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qingpei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuan Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events. (arXiv:2309.12244v3 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2309.12244","description":"<p>Children typically learn to identify and express emotions through sharing\ntheir stories and feelings with others, particularly their family. However, it\nis challenging for parents or siblings to have emotional communication with\nchildren since children are still developing their communication skills. We\npresent ChaCha, a chatbot that encourages and guides children to share personal\nevents and associated emotions. ChaCha combines a state machine and large\nlanguage models (LLMs) to keep the dialogue on track while carrying on\nfree-form conversations. Through an exploratory study with 20 children (aged\n8-12), we examine how ChaCha prompts children to share personal events and\nguides them to describe associated emotions. Participants perceived ChaCha as a\nclose friend and shared their stories on various topics, such as family trips\nand personal achievements. Based on the findings, we discuss opportunities for\nleveraging LLMs to design child-friendly chatbots to support children in\nsharing emotions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seo_W/0/1/0/all/0/1\">Woosuk Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chanmo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Ho Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models. (arXiv:2310.02998v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2310.02998","description":"<p>Large Vision-Language Models (LVLMs) can understand the world comprehensively\nby integrating rich information from different modalities, achieving remarkable\nadvancements on various multimodal downstream tasks. However, deploying LVLMs\nis often problematic due to their massive computational/energy costs and carbon\nconsumption. Such issues make it infeasible to adopt conventional iterative\nglobal pruning, which is costly due to computing the Hessian matrix of the\nentire large model for sparsification. Alternatively, several studies have\nrecently proposed layer-wise pruning approaches to avoid the expensive\ncomputation of global pruning and efficiently compress model weights according\nto their importance within a layer. However, they often suffer from suboptimal\nmodel compression due to their lack of a global perspective. To address this\nlimitation in recent efficient pruning methods for large models, we propose\nEfficient Coarse-to-Fine LayerWise Pruning (ECoFLaP), a two-stage\ncoarse-to-fine weight pruning approach for LVLMs. We first determine the\nsparsity ratios of different layers or blocks by leveraging the global\nimportance score, which is efficiently computed based on the zeroth-order\napproximation of the global model gradients. Then, the model performs local\nlayer-wise unstructured weight pruning based on globally-informed sparsity\nratios. We validate our proposed method across various multimodal and unimodal\nmodels and datasets, demonstrating significant performance improvements over\nprevalent pruning techniques in the high-sparsity regime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yi-Lin Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jaehong Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Product Attribute Value Extraction using Large Language Models. (arXiv:2310.12537v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.12537","description":"<p>E-commerce platforms rely on structured product descriptions, in the form of\nattribute/value pairs to enable features such as faceted product search and\nproduct comparison. However, vendors on these platforms often provide\nunstructured product descriptions consisting of a title and a textual\ndescription. To process such offers, e-commerce platforms must extract\nattribute/value pairs from the unstructured descriptions. State-of-the-art\nattribute/value extraction methods based on pre-trained language models (PLMs),\nsuch as BERT, face two drawbacks (i) the methods require significant amounts of\ntask-specific training data and (ii) the fine-tuned models have problems to\ngeneralize to attribute values that were not part of the training data. We\nexplore the potential of using large language models (LLMs) as a more training\ndata-efficient and more robust alternative to existing attribute/value\nextraction methods. We propose different prompt templates for instructing LLMs\nabout the target schema of the extraction, covering both zero-shot and few-shot\nscenarios. In the zero-shot scenario, textual and JSON-based approaches for\nrepresenting information about the target attributes are compared. In the\nscenario with training data, we investigate (i) the provision of example\nattribute values, (ii) the selection of in-context demonstrations, (iii)\nshuffled ensembling to prevent position bias, and (iv) fine-tuning the LLM. The\nprompt templates are evaluated in combination with hosted LLMs, such as GPT-3.5\nand GPT-4, and open-source LLMs based on Llama2 which can be run locally. The\nbest average F1-score of 86% was reached by GPT-4 using an ensemble of shuffled\nprompts that combine attribute names, attribute descriptions, example values,\nand demonstrations. Given the same amount of training data, this prompt/model\ncombination outperforms the best PLM baseline by an average of 6% F1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brinkmann_A/0/1/0/all/0/1\">Alexander Brinkmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shraga_R/0/1/0/all/0/1\">Roee Shraga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bizer_C/0/1/0/all/0/1\">Christian Bizer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation. (arXiv:2310.18628v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.18628","description":"<p>With the rise of powerful closed-sourced LLMs (ChatGPT, GPT-4), there are\nincreasing interests in distilling the capabilies of close-sourced LLMs to\nsmaller open-sourced LLMs. Previous distillation methods usually prompt ChatGPT\nto generate a set of instructions and answers, for the student model to learn.\nHowever, such standard distillation approach neglects the merits and conditions\nof the student model. Inspired by modern teaching principles, we design a\npersonalised distillation process, in which the student attempts to solve a\ntask first, then the teacher provides an adaptive refinement for the student to\nimprove. Instead of feeding the student with teacher's prior, personalised\ndistillation enables personalised learning for the student model, as it only\nlearns on examples it makes mistakes upon and learns to improve its own\nsolution. On code generation, personalised distillation consistently\noutperforms standard distillation with only one third of the data. With only\n2.5-3K personalised examples that incur a data-collection cost of 4-6$, we\nboost CodeGen-mono-16B by 7% to achieve 36.4% pass@1 and StarCoder by 12.2% to\nachieve 45.8% pass@1 on HumanEval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hailin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1\">Amrita Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven Hoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArabIcros: AI-Powered Arabic Crossword Puzzle Generation for Educational Applications. (arXiv:2312.01339v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.01339","description":"<p>This paper presents the first Arabic crossword puzzle generator driven by\nadvanced AI technology. Leveraging cutting-edge large language models including\nGPT4, GPT3-Davinci, GPT3-Curie, GPT3-Babbage, GPT3-Ada, and BERT, the system\ngenerates distinctive and challenging clues. Based on a dataset comprising over\n50,000 clue-answer pairs, the generator employs fine-tuning, few/zero-shot\nlearning strategies, and rigorous quality-checking protocols to enforce the\ngeneration of high-quality clue-answer pairs. Importantly, educational\ncrosswords contribute to enhancing memory, expanding vocabulary, and promoting\nproblem-solving skills, thereby augmenting the learning experience through a\nfun and engaging approach, reshaping the landscape of traditional learning\nmethods. The overall system can be exploited as a powerful educational tool\nthat amalgamates AI and innovative learning techniques, heralding a\ntransformative era for Arabic crossword puzzles and the intersection of\ntechnology and education.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeinalipour_K/0/1/0/all/0/1\">Kamyar Zeinalipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saad_M/0/1/0/all/0/1\">Mohamed Zaky Saad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maggini_M/0/1/0/all/0/1\">Marco Maggini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1\">Marco Gori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DevEval: Evaluating Code Generation in Practical Software Projects. (arXiv:2401.06401v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2401.06401","description":"<p>How to evaluate Large Language Models (LLMs) in code generation is an open\nquestion. Many benchmarks have been proposed but are inconsistent with\npractical software projects, e.g., unreal program distributions, insufficient\ndependencies, and small-scale project contexts. Thus, the capabilities of LLMs\nin practical projects are still unclear. In this paper, we propose a new\nbenchmark named DevEval, aligned with Developers' experiences in practical\nprojects. DevEval is collected through a rigorous pipeline, containing 2,690\nsamples from 119 practical projects and covering 10 domains. Compared to\nprevious benchmarks, DevEval aligns to practical projects in multiple\ndimensions, e.g., real program distributions, sufficient dependencies, and\nenough-scale project contexts. We assess five popular LLMs on DevEval (e.g.,\ngpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual\nabilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo\nonly is 42 in our experiments. We also discuss the challenges and future\ndirections of code generation in practical projects. We open-source DevEval and\nhope it can facilitate the development of code generation in practical\nprojects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yunfei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongmin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huanyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kaibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lecheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lanshen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jiazheng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuanming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yihong Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1\">Bin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mengfei Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Explain Transformers by Illuminating Important Information. (arXiv:2401.09972v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2401.09972","description":"<p>Transformer-based models excel in various natural language processing (NLP)\ntasks, attracting countless efforts to explain their inner workings. Prior\nmethods explain Transformers by focusing on the raw gradient and attention as\ntoken attribution scores, where non-relevant information is often considered\nduring explanation computation, resulting in confusing results. In this work,\nwe propose highlighting the important information and eliminating irrelevant\ninformation by a refined information flow on top of the layer-wise relevance\npropagation (LRP) method. Specifically, we consider identifying syntactic and\npositional heads as important attention heads and focus on the relevance\nobtained from these important heads. Experimental results demonstrate that\nirrelevant information does distort output attribution scores and then should\nbe masked during explanation computation. Compared to eight baselines on both\nclassification and question-answering datasets, our method consistently\noutperforms with over 3\\% to 33\\% improvement on explanation metrics, providing\nsuperior explanation performance. Our anonymous code repository is available\nat: https://github.com/LinxinS97/Mask-LRP\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linxin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_A/0/1/0/all/0/1\">Ao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lecue_F/0/1/0/all/0/1\">Freddy Lecue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2401.10189","description":"<p>Fine-grained few-shot entity extraction in the chemical domain faces two\nunique challenges. First, compared with entity extraction tasks in the general\ndomain, sentences from chemical papers usually contain more entities. Moreover,\nentity extraction models usually have difficulty extracting entities of\nlong-tailed types. In this paper, we propose Chem-FINESE, a novel\nsequence-to-sequence (seq2seq) based few-shot entity extraction approach, to\naddress these two challenges. Our Chem-FINESE has two components: a seq2seq\nentity extractor to extract named entities from the input sentence and a\nseq2seq self-validation module to reconstruct the original input sentence from\nextracted entities. Inspired by the fact that a good entity extraction system\nneeds to extract entities faithfully, our new self-validation module leverages\nentity extraction results to reconstruct the original input sentence. Besides,\nwe design a new contrastive loss to reduce excessive copying during the\nextraction process. Finally, we release ChemNER+, a new fine-grained chemical\nentity extraction dataset that is annotated by domain experts with the ChemNER\nschema. Experiments in few-shot settings with both ChemNER+ and CHEMET datasets\nshow that our newly proposed framework has contributed up to 8.26% and 6.84%\nabsolute F1-score gains respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingyun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zixuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongxiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Huimin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2401.11143","description":"<p>We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a\nnovel probabilistic attention framework, and the Gaussian Adaptive Transformer\n(GAT), designed to enhance information aggregation across multiple modalities,\nincluding Speech, Text and Vision. GAAM integrates learnable mean and variance\ninto its attention mechanism, implemented in a Multi-Headed framework enabling\nit to collectively model any Probability Distribution for dynamic recalibration\nof feature significance. This method demonstrates significant improvements,\nespecially with highly non-stationary data, surpassing the state-of-the-art\nattention techniques in model performance (up to approximately +20% in\naccuracy) by identifying key elements within the feature space. GAAM's\ncompatibility with dot-product-based attention models and relatively low number\nof parameters showcases its adaptability and potential to boost existing\nattention frameworks. Empirically, GAAM exhibits superior adaptability and\nefficacy across a diverse range of tasks, including emotion recognition in\nspeech, image classification, and text classification, thereby establishing its\nrobustness and versatility in handling multi-modal data. Furthermore, we\nintroduce the Importance Factor (IF), a new learning-based metric that enhances\nthe explainability of models trained with GAAM-based methods. Overall, GAAM\nrepresents an advancement towards development of better performing and more\nexplainable attention models across multiple modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ioannides_G/0/1/0/all/0/1\">Georgios Ioannides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elkins_A/0/1/0/all/0/1\">Aaron Elkins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unfair TOS: An Automated Approach using Customized BERT. (arXiv:2401.11207v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2401.11207","description":"<p>Terms of Service (ToS) form an integral part of any agreement as it defines\nthe legal relationship between a service provider and an end-user. Not only do\nthey establish and delineate reciprocal rights and responsibilities, but they\nalso provide users with information on essential aspects of contracts that\npertain to the use of digital spaces. These aspects include a wide range of\ntopics, including limitation of liability, data protection, etc. Users tend to\naccept the ToS without going through it before using any application or\nservice. Such ignorance puts them in a potentially weaker situation in case any\naction is required. Existing methodologies for the detection or classification\nof unfair clauses are however obsolete and show modest performance. In this\nresearch paper, we present SOTA(State of The Art) results on unfair clause\ndetection from ToS documents based on unprecedented custom BERT Fine-tuning in\nconjunction with SVC(Support Vector Classifier). The study shows proficient\nperformance with a macro F1-score of 0.922 at unfair clause detection, and\nsuperior performance is also shown in the classification of unfair clauses by\neach tag. Further, a comparative analysis is performed by answering research\nquestions on the Transformer models utilized. In order to further research and\nexperimentation the code and results are made available on\nhttps://github.com/batking24/Unfair-TOS-An-Automated-Approach-based-on-Fine-tuning-BERT-in-conjunction-with-ML.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akash_B/0/1/0/all/0/1\">Bathini Sai Akash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kupireddy_A/0/1/0/all/0/1\">Akshara Kupireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murthy_L/0/1/0/all/0/1\">Lalita Bhanu Murthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning. (arXiv:2401.14011v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2401.14011","description":"<p>Multi-modal large language models(MLLMs) have achieved remarkable progress\nand demonstrated powerful knowledge comprehension and reasoning abilities.\nHowever, the mastery of domain-specific knowledge, which is essential for\nevaluating the intelligence of MLLMs, continues to be a challenge. Current\nmulti-modal benchmarks for domain-specific knowledge concentrate on\nmultiple-choice questions and are predominantly available in English, which\nimposes limitations on the comprehensiveness of the evaluation. To this end, we\nintroduce CMMU, a novel benchmark for multi-modal and multi-type question\nunderstanding and reasoning in Chinese. CMMU consists of 3,603 questions in 7\nsubjects, covering knowledge from primary to high school. The questions can be\ncategorized into 3 types: multiple-choice, multiple-response, and\nfill-in-the-blank, bringing greater challenges to MLLMs. In addition, we\npropose a rigorous evaluation strategy called ShiftCheck for assessing\nmultiple-choice questions. The strategy aims to reduce position bias, minimize\nthe influence of randomness on correctness, and perform a quantitative analysis\nof position bias. We evaluate seven open-source MLLMs along with GPT4-V,\nGemini-Pro, and Qwen-VL-Plus. The results demonstrate that CMMU poses a\nsignificant challenge to the recent MLLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zheqi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinya Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pengfei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_R/0/1/0/all/0/1\">Richeng Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qiannan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hua Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence. (arXiv:2401.14196v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2401.14196","description":"<p>The rapid development of large language models has revolutionized code\nintelligence in software development. However, the predominance of\nclosed-source models has restricted extensive research and development. To\naddress this, we introduce the DeepSeek-Coder series, a range of open-source\ncode models with sizes from 1.3B to 33B, trained from scratch on 2 trillion\ntokens. These models are pre-trained on a high-quality project-level code\ncorpus and employ a fill-in-the-blank task with a 16K window to enhance code\ngeneration and infilling. Our extensive evaluations demonstrate that\nDeepSeek-Coder not only achieves state-of-the-art performance among open-source\ncode models across multiple benchmarks but also surpasses existing\nclosed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models\nare under a permissive license that allows for both research and unrestricted\ncommercial use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Daya Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dejian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhenda Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_K/0/1/0/all/0/1\">Kai Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wentao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_X/0/1/0/all/0/1\">Xiao Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Y. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Y.K. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Fuli Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yingfei Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1\">Wenfeng Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2024-01-28T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}}]}]}