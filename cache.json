{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-03-23T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Positive-Augmented Constrastive Learning for Image and Video Captioning Evaluation. (arXiv:2303.12112v1 [cs.CV])","link":"http://arxiv.org/abs/2303.12112","description":"<p>The CLIP model has been recently proven to be very effective for a variety of\ncross-modal tasks, including the evaluation of captions generated from\nvision-and-language architectures. In this paper, we propose a new recipe for a\ncontrastive-based evaluation metric for image captioning, namely\nPositive-Augmented Contrastive learning Score (PAC-S), that in a novel way\nunifies the learning of a contrastive visual-semantic space with the addition\nof generated images and text on curated data. Experiments spanning several\ndatasets demonstrate that our new metric achieves the highest correlation with\nhuman judgments on both images and videos, outperforming existing\nreference-based metrics like CIDEr and SPICE and reference-free metrics like\nCLIP-Score. Finally, we test the system-level correlation of the proposed\nmetric when considering popular image captioning approaches, and assess the\nimpact of employing different cross-modal features. Our source code and trained\nmodels are publicly available at: https://github.com/aimagelab/pacscore.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarto_S/0/1/0/all/0/1\">Sara Sarto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barraco_M/0/1/0/all/0/1\">Manuele Barraco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense. (arXiv:2303.12132v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12132","description":"<p>Generative Language Models gained significant attention in late 2022 / early\n2023, notably with the introduction of models refined to act consistently with\nusers' expectations of interactions with AI (conversational models). Arguably\nthe focal point of public attention has been such a refinement of the GPT3\nmodel -- the ChatGPT and its subsequent integration with auxiliary\ncapabilities, including search as part of Microsoft Bing. Despite extensive\nprior research invested in their development, their performance and\napplicability to a range of daily tasks remained unclear and niche. However,\ntheir wider utilization without a requirement for technical expertise, made in\nlarge part possible through conversational fine-tuning, revealed the extent of\ntheir true capabilities in a real-world environment. This has garnered both\npublic excitement for their potential applications and concerns about their\ncapabilities and potential malicious uses. This review aims to provide a brief\noverview of the history, state of the art, and implications of Generative\nLanguage Models in terms of their principles, abilities, limitations, and\nfuture prospects -- especially in the context of cyber-defense, with a focus on\nthe Swiss operational environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kucharavy_A/0/1/0/all/0/1\">Andrei Kucharavy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schillaci_Z/0/1/0/all/0/1\">Zachary Schillaci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marechal_L/0/1/0/all/0/1\">Lo&#xef;c Mar&#xe9;chal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wursch_M/0/1/0/all/0/1\">Maxime W&#xfc;rsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolamic_L/0/1/0/all/0/1\">Ljiljana Dolamic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabonnadiere_R/0/1/0/all/0/1\">Remi Sabonnadiere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_D/0/1/0/all/0/1\">Dimitri Percia David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mermoud_A/0/1/0/all/0/1\">Alain Mermoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenders_V/0/1/0/all/0/1\">Vincent Lenders</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understand Legal Documents with Contextualized Large Language Models. (arXiv:2303.12135v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12135","description":"<p>The growth of pending legal cases in populous countries, such as India, has\nbecome a major issue. Developing effective techniques to process and understand\nlegal documents is extremely useful in resolving this problem. In this paper,\nwe present our systems for SemEval-2023 Task 6: understanding legal texts (Modi\net al., 2023). Specifically, we first develop the Legal-BERT-HSLN model that\nconsiders the comprehensive context information in both intra- and\ninter-sentence levels to predict rhetorical roles (subtask A) and then train a\nLegal-LUKE model, which is legal-contextualized and entity-aware, to recognize\nlegal entities (subtask B). Our evaluations demonstrate that our designed\nmodels are more accurate than baselines, e.g., with an up to 15.0% better F1\nscore in subtask B. We achieved notable performance in the task leaderboard,\ne.g., 0.834 micro F1 score, and ranked No.5 out of 27 teams in subtask A.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuchen Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAGVLT: Masked Generative Vision-and-Language Transformer. (arXiv:2303.12208v1 [cs.CV])","link":"http://arxiv.org/abs/2303.12208","description":"<p>While generative modeling on multimodal image-text data has been actively\ndeveloped with large-scale paired datasets, there have been limited attempts to\ngenerate both image and text data by a single model rather than a generation of\none fixed modality conditioned on the other modality. In this paper, we explore\na unified generative vision-and-language (VL) model that can produce both\nimages and text sequences. Especially, we propose a generative VL transformer\nbased on the non-autoregressive mask prediction, named MAGVLT, and compare it\nwith an autoregressive generative VL transformer (ARGVLT). In comparison to\nARGVLT, the proposed MAGVLT enables bidirectional context encoding, fast\ndecoding by parallel token predictions in an iterative refinement, and extended\nediting capabilities such as image and text infilling. For rigorous training of\nour MAGVLT with image-text pairs from scratch, we combine the image-to-text,\ntext-to-image, and joint image-and-text mask prediction tasks. Moreover, we\ndevise two additional tasks based on the step-unrolled mask prediction and the\nselective prediction on the mixture of two image-text pairs. Experimental\nresults on various downstream generation tasks of VL benchmarks show that our\nMAGVLT outperforms ARGVLT by a large margin even with significant inference\nspeedup. Particularly, MAGVLT achieves competitive results on both zero-shot\nimage-to-text and text-to-image generation tasks from MS-COCO by one\nmoderate-sized model (fewer than 500M parameters) even without the use of\nmonomodal data and networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungwoong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_D/0/1/0/all/0/1\">Daejin Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Donghoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jongmin Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Taxonomy of Deep Syntactic Relations. (arXiv:2303.12220v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12220","description":"<p>This paper analyzes multiple deep-syntactic frameworks with the goal of\ncreating a proposal for a set of universal semantic role labels. The proposal\nexamines various theoretic linguistic perspectives and focuses on Meaning-Text\nTheory and Functional Generative Description frameworks.\n</p>\n<p>For the purpose of this research, data from four languages is used -- Spanish\nand Catalan (Taule et al., 2011), Czech (Hajic et al., 2017), and English\n(Hajic et al., 2012). This proposal is oriented towards Universal Dependencies\n(de Marneffe et al., 2021) with a further intention of applying the universal\nsemantic role labels to the UD data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Droganova_K/0/1/0/all/0/1\">Kira Droganova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeman_D/0/1/0/all/0/1\">Daniel Zeman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Transformer Models and Human Behaviors on Chinese Character Naming. (arXiv:2303.12294v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12294","description":"<p>Neural network models have been proposed to explain the grapheme-phoneme\nmapping process in humans for many alphabet languages. These models not only\nsuccessfully learned the correspondence of the letter strings and their\npronunciation, but also captured human behavior in nonce word naming tasks. How\nwould the neural models perform for a non-alphabet language (e.g., Chinese)\nunknown character task? How well would the model capture human behavior? In\nthis study, we evaluate a set of transformer models and compare their\nperformances with human behaviors on an unknown Chinese character naming task.\nWe found that the models and humans behaved very similarly, that they had\nsimilar accuracy distribution for each character, and had a substantial overlap\nin answers. In addition, the models' answers are highly correlated with humans'\nanswers. These results suggested that the transformer models can well capture\nhuman's character naming behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaomeng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lingyu Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Turkish Speech Recognition via Hybrid CTC/Attention Architecture and Multi-feature Fusion Network. (arXiv:2303.12300v1 [cs.SD])","link":"http://arxiv.org/abs/2303.12300","description":"<p>In recent years, End-to-End speech recognition technology based on deep\nlearning has developed rapidly. Due to the lack of Turkish speech data, the\nperformance of Turkish speech recognition system is poor. Firstly, this paper\nstudies a series of speech recognition tuning technologies. The results show\nthat the performance of the model is the best when the data enhancement\ntechnology combining speed perturbation with noise addition is adopted and the\nbeam search width is set to 16. Secondly, to maximize the use of effective\nfeature information and improve the accuracy of feature extraction, this paper\nproposes a new feature extractor LSPC. LSPC and LiGRU network are combined to\nform a shared encoder structure, and model compression is realized. The results\nshow that the performance of LSPC is better than MSPC and VGGnet when only\nusing Fbank features, and the WER is improved by 1.01% and 2.53% respectively.\nFinally, based on the above two points, a new multi-feature fusion network is\nproposed as the main structure of the encoder. The results show that the WER of\nthe proposed feature fusion network based on LSPC is improved by 0.82% and\n1.94% again compared with the single feature (Fbank feature and Spectrogram\nfeature) extraction using LSPC. Our model achieves performance comparable to\nthat of advanced End-to-End models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zeyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yolwas_N/0/1/0/all/0/1\">Nurmement Yolwas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huiru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slamu_W/0/1/0/all/0/1\">Wushour Slamu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XWikiGen: Cross-lingual Summarization for Encyclopedic Text Generation in Low Resource Languages. (arXiv:2303.12308v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12308","description":"<p>Lack of encyclopedic text contributors, especially on Wikipedia, makes\nautomated text generation for \\emph{low resource (LR) languages} a critical\nproblem. Existing work on Wikipedia text generation has focused on\n\\emph{English only} where English reference articles are summarized to generate\nEnglish Wikipedia pages. But, for low-resource languages, the scarcity of\nreference articles makes monolingual summarization ineffective in solving this\nproblem. Hence, in this work, we propose \\task{}, which is the task of\ncross-lingual multi-document summarization of text from multiple reference\narticles, written in various languages, to generate Wikipedia-style text.\nAccordingly, we contribute a benchmark dataset, \\data{}, spanning $\\sim$69K\nWikipedia articles covering five domains and eight languages. We harness this\ndataset to train a two-stage system where the input is a set of citations and a\nsection title and the output is a section-specific LR summary. The proposed\nsystem is based on a novel idea of neural unsupervised extractive summarization\nto coarsely identify salient information followed by a neural abstractive model\nto generate the section-specific text. Extensive experiments show that\nmulti-domain training is better than the multi-lingual setup on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taunk_D/0/1/0/all/0/1\">Dhaval Taunk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagare_S/0/1/0/all/0/1\">Shivprasad Sagare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_A/0/1/0/all/0/1\">Anupam Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1\">Shivansh Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_V/0/1/0/all/0/1\">Vasudeva Varma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization. (arXiv:2303.12314v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12314","description":"<p>Prompt tuning is a parameter-efficient method, which learns soft prompts and\nconditions frozen language models to perform specific downstream tasks. Though\neffective, prompt tuning under few-shot settings on the one hand heavily relies\non a good initialization of soft prompts. On the other hand, it can easily\nresult in overfitting. Existing works leverage pre-training or supervised\nmeta-learning to initialize soft prompts but they cannot data-efficiently\ngeneralize to unseen downstream tasks. To address the above problems, this\npaper proposes a novel Self-sUpervised meta-Prompt learning framework with\nmeta-gradient Regularization for few-shot generalization (SUPMER). We first\ndesign a set of self-supervised anchor meta-training tasks with different task\nformats and further enrich the task distribution with curriculum-based task\naugmentation. Then a novel meta-gradient regularization method is integrated\ninto meta-prompt learning. It meta-learns to transform the raw gradients during\nfew-shot learning into a domain-generalizable direction, thus alleviating the\nproblem of overfitting. Extensive experiments show that SUPMER achieves better\nperformance for different few-shot downstream tasks, and also exhibits a\nstronger domain generalization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_K/0/1/0/all/0/1\">Kaihang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Hongye Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaozhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering. (arXiv:2303.12320v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12320","description":"<p>Commonsense question-answering (QA) methods combine the power of pre-trained\nLanguage Models (LM) with the reasoning provided by Knowledge Graphs (KG). A\ntypical approach collects nodes relevant to the QA pair from a KG to form a\nWorking Graph (WG) followed by reasoning using Graph Neural Networks(GNNs).\nThis faces two major challenges: (i) it is difficult to capture all the\ninformation from the QA in the WG, and (ii) the WG contains some irrelevant\nnodes from the KG. To address these, we propose GrapeQA with two simple\nimprovements on the WG: (i) Prominent Entities for Graph Augmentation\nidentifies relevant text chunks from the QA pair and augments the WG with\ncorresponding latent representations from the LM, and (ii) Context-Aware Node\nPruning removes nodes that are less relevant to the QA pair. We evaluate our\nresults on OpenBookQA, CommonsenseQA and MedQA-USMLE and see that GrapeQA shows\nconsistent improvements over its LM + KG predecessor (QA-GNN in particular) and\nlarge improvements on OpenBookQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taunk_D/0/1/0/all/0/1\">Dhaval Taunk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanna_L/0/1/0/all/0/1\">Lakshya Khanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandru_P/0/1/0/all/0/1\">Pavan Kandru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_V/0/1/0/all/0/1\">Vasudeva Varma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_C/0/1/0/all/0/1\">Charu Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tapaswi_M/0/1/0/all/0/1\">Makarand Tapaswi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Multimodal Multitask Multilingual Learning. (arXiv:2303.12489v1 [cs.LG])","link":"http://arxiv.org/abs/2303.12489","description":"<p>While few-shot learning as a transfer learning paradigm has gained\nsignificant traction for scenarios with limited data, it has primarily been\nexplored in the context of building unimodal and unilingual models.\nFurthermore, a significant part of the existing literature in the domain of\nfew-shot multitask learning perform in-context learning which requires manually\ngenerated prompts as the input, yielding varying outcomes depending on the\nlevel of manual prompt-engineering. In addition, in-context learning suffers\nfrom substantial computational, memory, and storage costs which eventually\nleads to high inference latency because it involves running all of the prompt's\nexamples through the model every time a prediction is made. In contrast,\nmethods based on the transfer learning via the fine-tuning paradigm avoid the\naforementioned issues at a one-time cost of fine-tuning weights on a per-task\nbasis. However, such methods lack exposure to few-shot multimodal multitask\nlearning. In this paper, we propose few-shot learning for a multimodal\nmultitask multilingual (FM3) setting by adapting pre-trained vision and\nlanguage models using task-specific hypernetworks and contrastively fine-tuning\nthem to enable few-shot learning. FM3's architecture combines the best of both\nworlds of in-context and fine-tuning based learning and consists of three major\ncomponents: (i) multimodal contrastive fine-tuning to enable few-shot learning,\n(ii) hypernetwork task adaptation to perform multitask learning, and (iii)\ntask-specific output heads to cater to a plethora of diverse tasks. FM3 learns\nthe most prominent tasks in the vision and language domains along with their\nintersections, namely visual entailment (VE), visual question answering (VQA),\nand natural language understanding (NLU) tasks such as neural entity\nrecognition (NER) and the GLUE benchmark including QNLI, MNLI, QQP, and SST-2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1\">Vinija Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding. (arXiv:2303.12513v1 [cs.CV])","link":"http://arxiv.org/abs/2303.12513","description":"<p>Most humans use visual imagination to understand and reason about language,\nbut models such as BERT reason about language using knowledge acquired during\ntext-only pretraining. In this work, we investigate whether vision-and-language\npretraining can improve performance on text-only tasks that involve implicit\nvisual reasoning, focusing primarily on zero-shot probing methods. We propose a\nsuite of visual language understanding (VLU) tasks for probing the visual\nreasoning abilities of text encoder models, as well as various non-visual\nnatural language understanding (NLU) tasks for comparison. We also contribute a\nnovel zero-shot knowledge probing method, Stroop probing, for applying models\nsuch as CLIP to text-only tasks without needing a prediction head such as the\nmasked language modelling head of models like BERT. We show that SOTA\nmultimodally trained text encoders outperform unimodally trained text encoders\non the VLU tasks while being underperformed by them on the NLU tasks, lending\nnew context to previously mixed results regarding the NLU capabilities of\nmultimodal models. We conclude that exposure to images during pretraining\naffords inherent visual reasoning knowledge that is reflected in language-only\ntasks that require implicit visual reasoning. Our findings bear importance in\nthe broader context of multimodal learning, providing principled guidelines for\nthe choice of text encoders used in such contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alper_M/0/1/0/all/0/1\">Morris Alper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiman_M/0/1/0/all/0/1\">Michael Fiman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1\">Hadar Averbuch-Elor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEGA: Multilingual Evaluation of Generative AI. (arXiv:2303.12528v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12528","description":"<p>Generative AI models have impressive performance on many Natural Language\nProcessing tasks such as language understanding, reasoning and language\ngeneration. One of the most important questions that is being asked by the AI\ncommunity today is about the capabilities and limits of these models, and it is\nclear that evaluating generative AI is very challenging. Most studies on\ngenerative Large Language Models (LLMs) are restricted to English and it is\nunclear how capable these models are at understanding and generating other\nlanguages. We present the first comprehensive benchmarking of generative LLMs -\nMEGA, which evaluates models on standard NLP benchmarks, covering 8 diverse\ntasks and 33 typologically diverse languages. We also compare the performance\nof generative LLMs to State of the Art (SOTA) non-autoregressive models on\nthese tasks to determine how well generative models perform compared to the\nprevious generation of LLMs. We present a thorough analysis of the performance\nof models across languages and discuss some of the reasons why generative LLMs\nare currently not optimal for all languages. We create a framework for\nevaluating generative LLMs in the multilingual setting and provide directions\nfor future progress in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1\">Kabir Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hada_R/0/1/0/all/0/1\">Rishav Hada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochieng_M/0/1/0/all/0/1\">Millicent Ochieng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Prachi Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diddee_H/0/1/0/all/0/1\">Harshita Diddee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maina_S/0/1/0/all/0/1\">Samuel Maina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganu_T/0/1/0/all/0/1\">Tanuja Ganu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segal_S/0/1/0/all/0/1\">Sameer Segal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Axmed_M/0/1/0/all/0/1\">Maxamed Axmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bali_K/0/1/0/all/0/1\">Kalika Bali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitaram_S/0/1/0/all/0/1\">Sunayana Sitaram</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. (arXiv:2303.12570v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12570","description":"<p>The task of repository-level code completion is to continue writing the\nunfinished code based on a broader context of the repository. While for\nautomated code completion tools, it is difficult to utilize the useful\ninformation scattered in different files. We propose RepoCoder, a simple,\ngeneric, and effective framework to address the challenge. It streamlines the\nrepository-level code completion process by incorporating a similarity-based\nretriever and a pre-trained code language model, which allows for the effective\nutilization of repository-level information for code completion and grants the\nability to generate code at various levels of granularity. Furthermore,\nRepoCoder utilizes a novel iterative retrieval-generation paradigm that bridges\nthe gap between retrieval context and the intended completion target. We also\npropose a new benchmark RepoEval, which consists of the latest and high-quality\nreal-world repositories covering line, API invocation, and function body\ncompletion scenarios. We test the performance of RepoCoder by using various\ncombinations of code retrievers and generators. Experimental results indicate\nthat RepoCoder significantly improves the zero-shot code completion baseline by\nover 10% in all settings and consistently outperforms the vanilla\nretrieval-augmented code completion approach. Furthermore, we validate the\neffectiveness of RepoCoder through comprehensive analysis, providing valuable\ninsights for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fengji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zan_D/0/1/0/all/0/1\">Daoguang Zan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AfroDigits: A Community-Driven Spoken Digit Dataset for African Languages. (arXiv:2303.12582v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12582","description":"<p>The advancement of speech technologies has been remarkable, yet its\nintegration with African languages remains limited due to the scarcity of\nAfrican speech corpora. To address this issue, we present AfroDigits, a\nminimalist, community-driven dataset of spoken digits for African languages,\ncurrently covering 38 African languages. As a demonstration of the practical\napplications of AfroDigits, we conduct audio digit classification experiments\non six African languages [Igbo (ibo), Yoruba (yor), Rundi (run), Oshiwambo\n(kua), Shona (sna), and Oromo (gax)] using the Wav2Vec2.0-Large and XLS-R\nmodels. Our experiments reveal a useful insight on the effect of mixing African\nspeech corpora during finetuning. AfroDigits is the first published audio digit\ndataset for African languages and we believe it will, among other things, pave\nthe way for Afro-centric speech applications such as the recognition of\ntelephone numbers, and street numbers. We release the dataset and platform\npublicly at https://huggingface.co/datasets/chrisjay/crowd-speech-africa and\nhttps://huggingface.co/spaces/chrisjay/afro-speech respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Chinenye Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_S/0/1/0/all/0/1\">Sanchit Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tunstall_L/0/1/0/all/0/1\">Lewis Tunstall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abid_A/0/1/0/all/0/1\">Abubakar Abid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_J/0/1/0/all/0/1\">Joshua Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lhoest_Q/0/1/0/all/0/1\">Quentin Lhoest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allen_P/0/1/0/all/0/1\">Pete Allen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Platen_P/0/1/0/all/0/1\">Patrick Von Platen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1\">Yacine Jernite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaumond_J/0/1/0/all/0/1\">Julien Chaumond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noyan_M/0/1/0/all/0/1\">Merve Noyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanseviero_O/0/1/0/all/0/1\">Omar Sanseviero</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Role of Target Arguments in Rumour Stance Classification. (arXiv:2303.12665v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12665","description":"<p>Considering a conversation thread, stance classification aims to identify the\nopinion (e.g. agree or disagree) of replies towards a given target. The target\nof the stance is expected to be an essential component in this task, being one\nof the main factors that make it different from sentiment analysis. However, a\nrecent study shows that a target-oblivious model outperforms target-aware\nmodels, suggesting that targets are not useful when predicting stance. This\npaper re-examines this phenomenon for rumour stance classification (RSC) on\nsocial media, where a target is a rumour story implied by the source tweet in\nthe conversation. We propose adversarial attacks in the test data, aiming to\nassess the models robustness and evaluate the role of the data in the models\nperformance. Results show that state-of-the-art models, including approaches\nthat use the entire conversation thread, overly relying on superficial signals.\nOur hypothesis is that the naturally high occurrence of target-independent\ndirect replies in RSC (e.g. \"this is fake\" or just \"fake\") results in the\nimpressive performance of target-oblivious models, highlighting the risk of\ntarget instances being treated as noise during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yue Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Image Features with Convolutional Sequence-to-sequence Network for Multilingual Visual Question Answering. (arXiv:2303.12671v1 [cs.CV])","link":"http://arxiv.org/abs/2303.12671","description":"<p>Visual Question Answering (VQA) is a task that requires computers to give\ncorrect answers for the input questions based on the images. This task can be\nsolved by humans with ease but is a challenge for computers. The\nVLSP2022-EVJVQA shared task carries the Visual Question Answering task in the\nmultilingual domain on a newly released dataset: UIT-EVJVQA, in which the\nquestions and answers are written in three different languages: English,\nVietnamese and Japanese. We approached the challenge as a sequence-to-sequence\nlearning task, in which we integrated hints from pre-trained state-of-the-art\nVQA models and image features with Convolutional Sequence-to-Sequence network\nto generate the desired answers. Our results obtained up to 0.3442 by F1 score\non the public test set, 0.4210 on the private test set, and placed 3rd in the\ncompetition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thai_T/0/1/0/all/0/1\">Triet Minh Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4. (arXiv:2303.12712v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12712","description":"<p>Artificial intelligence (AI) researchers have been developing and refining\nlarge language models (LLMs) that exhibit remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. The latest model developed by OpenAI, GPT-4, was trained using an\nunprecedented scale of compute and data. In this paper, we report on our\ninvestigation of an early version of GPT-4, when it was still in active\ndevelopment by OpenAI. We contend that (this early version of) GPT-4 is part of\na new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that\nexhibit more general intelligence than previous AI models. We discuss the\nrising capabilities and implications of these models. We demonstrate that,\nbeyond its mastery of language, GPT-4 can solve novel and difficult tasks that\nspan mathematics, coding, vision, medicine, law, psychology and more, without\nneeding any special prompting. Moreover, in all of these tasks, GPT-4's\nperformance is strikingly close to human-level performance, and often vastly\nsurpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's\ncapabilities, we believe that it could reasonably be viewed as an early (yet\nstill incomplete) version of an artificial general intelligence (AGI) system.\nIn our exploration of GPT-4, we put special emphasis on discovering its\nlimitations, and we discuss the challenges ahead for advancing towards deeper\nand more comprehensive versions of AGI, including the possible need for\npursuing a new paradigm that moves beyond next-word prediction. We conclude\nwith reflections on societal influences of the recent technological leap and\nfuture research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1\">S&#xe9;bastien Bubeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_V/0/1/0/all/0/1\">Varun Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eldan_R/0/1/0/all/0/1\">Ronen Eldan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrke_J/0/1/0/all/0/1\">Johannes Gehrke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamar_E/0/1/0/all/0/1\">Ece Kamar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1\">Peter Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yin Tat Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundberg_S/0/1/0/all/0/1\">Scott Lundberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nori_H/0/1/0/all/0/1\">Harsha Nori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1\">Hamid Palangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Marco Tulio Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiModal Bias: Introducing a Framework for Stereotypical Bias Assessment beyond Gender and Race in Vision Language Models. (arXiv:2303.12734v1 [cs.CV])","link":"http://arxiv.org/abs/2303.12734","description":"<p>Recent breakthroughs in self supervised training have led to a new class of\npretrained vision language models. While there have been investigations of bias\nin multimodal models, they have mostly focused on gender and racial bias,\ngiving much less attention to other relevant groups, such as minorities with\nregard to religion, nationality, sexual orientation, or disabilities. This is\nmainly due to lack of suitable benchmarks for such groups. We seek to address\nthis gap by providing a visual and textual bias benchmark called MMBias,\nconsisting of around 3,800 images and phrases covering 14 population subgroups.\nWe utilize this dataset to assess bias in several prominent self supervised\nmultimodal models, including CLIP, ALBEF, and ViLT. Our results show that these\nmodels demonstrate meaningful bias favoring certain groups. Finally, we\nintroduce a debiasing method designed specifically for such large pre-trained\nmodels that can be applied as a post-processing step to mitigate bias, while\npreserving the remaining accuracy of the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Janghorbani_S/0/1/0/all/0/1\">Sepehr Janghorbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1\">Gerard de Melo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Trajectory and Vision Modalities for Verb Representation. (arXiv:2303.12737v1 [cs.CV])","link":"http://arxiv.org/abs/2303.12737","description":"<p>Three-dimensional trajectories, or the 3D position and rotation of objects\nover time, have been shown to encode key aspects of verb semantics (e.g., the\nmeanings of roll vs. slide). However, most multimodal models in NLP use 2D\nimages as representations of the world. Given the importance of 3D space in\nformal models of verb semantics, we expect that these 2D images would result in\nimpoverished representations that fail to capture nuanced differences in\nmeaning. This paper tests this hypothesis directly in controlled experiments.\nWe train self-supervised image and trajectory encoders, and then evaluate them\non the extent to which each learns to differentiate verb concepts. Contrary to\nour initial expectations, we find that 2D visual modalities perform similarly\nwell to 3D trajectories. While further work should be conducted on this\nquestion, our initial findings challenge the conventional wisdom that richer\nenvironment representations necessarily translate into better representation\nlearning for language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ebert_D/0/1/0/all/0/1\">Dylan Ebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can we trust the evaluation on ChatGPT?. (arXiv:2303.12767v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12767","description":"<p>ChatGPT, the first large language model (LLM) with mass adoption, has\ndemonstrated remarkable performance in numerous natural language tasks. Despite\nits evident usefulness, evaluating ChatGPT's performance in diverse problem\ndomains remains challenging due to the closed nature of the model and its\ncontinuous updates via Reinforcement Learning from Human Feedback (RLHF). We\nhighlight the issue of data contamination in ChatGPT evaluations, with a case\nstudy of the task of stance detection. We discuss the challenge of preventing\ndata contamination and ensuring fair model evaluation in the age of closed and\ncontinuously trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aiyappa_R/0/1/0/all/0/1\">Rachith Aiyappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_J/0/1/0/all/0/1\">Jisun An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1\">Haewoon Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_Y/0/1/0/all/0/1\">Yong-Yeol Ahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Bangla Sarcasm Detection using BERT and Explainable AI. (arXiv:2303.12772v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12772","description":"<p>A positive phrase or a sentence with an underlying negative motive is usually\ndefined as sarcasm that is widely used in today's social media platforms such\nas Facebook, Twitter, Reddit, etc. In recent times active users in social media\nplatforms are increasing dramatically which raises the need for an automated\nNLP-based system that can be utilized in various tasks such as determining\nmarket demand, sentiment analysis, threat detection, etc. However, since\nsarcasm usually implies the opposite meaning and its detection is frequently a\nchallenging issue, data meaning extraction through an NLP-based model becomes\nmore complicated. As a result, there has been a lot of study on sarcasm\ndetection in English over the past several years, and there's been a noticeable\nimprovement and yet sarcasm detection in the Bangla language's state remains\nthe same. In this article, we present a BERT-based system that can achieve\n99.60\\% while the utilized traditional machine learning algorithms are only\ncapable of achieving 89.93\\%. Additionally, we have employed Local\nInterpretable Model-Agnostic Explanations that introduce explainability to our\nsystem. Moreover, we have utilized a newly collected bangla sarcasm dataset,\nBanglaSarc that was constructed specifically for the evaluation of this study.\nThis dataset consists of fresh records of sarcastic and non-sarcastic comments,\nthe majority of which are acquired from Facebook and YouTube comment sections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anan_R/0/1/0/all/0/1\">Ramisa Anan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apon_T/0/1/0/all/0/1\">Tasnim Sakib Apon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_Z/0/1/0/all/0/1\">Zeba Tahsin Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modhu_E/0/1/0/all/0/1\">Elizabeth Antora Modhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondal_S/0/1/0/all/0/1\">Sudipta Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">MD. Golam Rabiul Alam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-source Frame Semantic Parsing. (arXiv:2303.12788v1 [cs.CL])","link":"http://arxiv.org/abs/2303.12788","description":"<p>While the state-of-the-art for frame semantic parsing has progressed\ndramatically in recent years, it is still difficult for end-users to apply\nstate-of-the-art models in practice. To address this, we present Frame Semantic\nTransformer, an open-source Python library which achieves near state-of-the-art\nperformance on FrameNet 1.7, while focusing on ease-of-use. We use a T5 model\nfine-tuned on Propbank and FrameNet exemplars as a base, and improve\nperformance by using FrameNet lexical units to provide hints to T5 at inference\ntime. We enhance robustness to real-world data by using textual data\naugmentations during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chanin_D/0/1/0/all/0/1\">David Chanin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Spatial Reasoning. (arXiv:2205.00363v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.00363","description":"<p>Spatial relations are a basic part of human cognition. However, they are\nexpressed in natural language in a variety of ways, and previous work has\nsuggested that current vision-and-language models (VLMs) struggle to capture\nrelational information. In this paper, we present Visual Spatial Reasoning\n(VSR), a dataset containing more than 10k natural text-image pairs with 66\ntypes of spatial relations in English (such as: under, in front of, and\nfacing). While using a seemingly simple annotation format, we show how the\ndataset includes challenging linguistic phenomena, such as varying reference\nframes. We demonstrate a large gap between human and model performance: the\nhuman ceiling is above 95%, while state-of-the-art models only achieve around\n70%. We observe that VLMs' by-relation performances have little correlation\nwith the number of training examples and the tested models are in general\nincapable of recognising relations concerning the orientations of objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emerson_G/0/1/0/all/0/1\">Guy Emerson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Causal Structure of Semantic Ambiguities. (arXiv:2206.06807v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.06807","description":"<p>Ambiguity is a natural language phenomenon occurring at different levels of\nsyntax, semantics, and pragmatics. It is widely studied; in Psycholinguistics,\nfor instance, we have a variety of competing studies for the human\ndisambiguation processes. These studies are empirical and based on eyetracking\nmeasurements. Here we take first steps towards formalizing these processes for\nsemantic ambiguities where we identified the presence of two features: (1)\njoint plausibility degrees of different possible interpretations, (2) causal\nstructures according to which certain words play a more substantial role in the\nprocesses. The novel sheaf-theoretic model of definite causality developed by\nGogioso and Pinzani in QPL 2021 offers tools to model and reason about these\nfeatures. We applied this theory to a dataset of ambiguous phrases extracted\nfrom Psycholinguistics literature and their human plausibility judgements\ncollected by us using the Amazon Mechanical Turk engine. We measured the causal\nfractions of different disambiguation orders within the phrases and discovered\ntwo prominent orders: from subject to verb in the subject-verb and from object\nto verb in the verb object phrases. We also found evidence for delay in the\ndisambiguation of polysemous vs homonymous verbs, again compatible with\nPsycholinguistic findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daphne Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadrzadeh_M/0/1/0/all/0/1\">Mehrnoosh Sadrzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Stream Transformer for Generic Event Boundary Captioning. (arXiv:2207.03038v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.03038","description":"<p>This paper describes our champion solution for the CVPR2022 Generic Event\nBoundary Captioning (GEBC) competition. GEBC requires the captioning model to\nhave a comprehension of instantaneous status changes around the given video\nboundary, which makes it much more challenging than conventional video\ncaptioning task. In this paper, a Dual-Stream Transformer with improvements on\nboth video content encoding and captions generation is proposed: (1) We utilize\nthree pre-trained models to extract the video features from different\ngranularities. Moreover, we exploit the types of boundary as hints to help the\nmodel generate captions. (2) We particularly design an model, termed as\nDual-Stream Transformer, to learn discriminative representations for boundary\ncaptioning. (3) Towards generating content-relevant and human-like captions, we\nimprove the description quality by designing a word-level ensemble strategy.\nThe promising results on the GEBC test split demonstrate the efficacy of our\nproposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hanhua Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Libo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Longyin Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Bias for Robust Visual Question Answering. (arXiv:2208.00690v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2208.00690","description":"<p>The task of Visual Question Answering (VQA) is known to be plagued by the\nissue of VQA models exploiting biases within the dataset to make its final\nprediction. Various previous ensemble based debiasing methods have been\nproposed where an additional model is purposefully trained to be biased in\norder to train a robust target model. However, these methods compute the bias\nfor a model simply from the label statistics of the training data or from\nsingle modal branches. In this work, in order to better learn the bias a target\nVQA model suffers from, we propose a generative method to train the bias model\ndirectly from the target model, called GenB. In particular, GenB employs a\ngenerative network to learn the bias in the target model through a combination\nof the adversarial objective and knowledge distillation. We then debias our\ntarget model with GenB as a bias model, and show through extensive experiments\nthe effects of our method on various VQA bias datasets including VQA-CP2,\nVQA-CP1, GQA-OOD, and VQA-CE, and show state-of-the-art results with the LXMERT\narchitecture on VQA-CP2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jae Won Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dong-jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_H/0/1/0/all/0/1\">Hyeonggon Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval. (arXiv:2211.12764v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.12764","description":"<p>Many recent studies leverage the pre-trained CLIP for text-video cross-modal\nretrieval by tuning the backbone with additional heavy modules, which not only\nbrings huge computational burdens with much more parameters, but also leads to\nthe knowledge forgetting from upstream models. In this work, we propose the\nVoP: Text-Video Co-operative Prompt Tuning for efficient tuning on the\ntext-video retrieval task. The proposed VoP is an end-to-end framework with\nboth video &amp; text prompts introducing, which can be regarded as a powerful\nbaseline with only 0.1% trainable parameters. Further, based on the\nspatio-temporal characteristics of videos, we develop three novel video prompt\nmechanisms to improve the performance with different scales of trainable\nparameters. The basic idea of the VoP enhancement is to model the frame\nposition, frame context, and layer function with specific trainable prompts,\nrespectively. Extensive experiments show that compared to full fine-tuning, the\nenhanced VoP achieves a 1.4% average R@1 gain across five text-video retrieval\nbenchmarks with 6x less parameter overhead. The code will be available at\nhttps://github.com/bighuang624/VoP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siteng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Biao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yulin Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jianwen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yiliang Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Donglin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tell Me What Happened: Unifying Text-guided Video Completion via Multimodal Masked Video Generation. (arXiv:2211.12824v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.12824","description":"<p>Generating a video given the first several static frames is challenging as it\nanticipates reasonable future frames with temporal coherence. Besides video\nprediction, the ability to rewind from the last frame or infilling between the\nhead and tail is also crucial, but they have rarely been explored for video\ncompletion. Since there could be different outcomes from the hints of just a\nfew frames, a system that can follow natural language to perform video\ncompletion may significantly improve controllability. Inspired by this, we\nintroduce a novel task, text-guided video completion (TVC), which requests the\nmodel to generate a video from partial frames guided by an instruction. We then\npropose Multimodal Masked Video Generation (MMVG) to address this TVC task.\nDuring training, MMVG discretizes the video frames into visual tokens and masks\nmost of them to perform video completion from any time point. At inference\ntime, a single MMVG model can address all 3 cases of TVC, including video\nprediction, rewind, and infilling, by applying corresponding masking\nconditions. We evaluate MMVG in various video scenarios, including egocentric,\nanimation, and gaming. Extensive experimental results indicate that MMVG is\neffective in generating high-quality visual appearances with text guidance for\nTVC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1\">Tsu-Jui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Licheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Cheng-Yang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jong-Chyi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bell_S/0/1/0/all/0/1\">Sean Bell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation. (arXiv:2303.08518v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.08518","description":"<p>Large Language Models (LLMs) are popular for their impressive abilities, but\nthe need for model-specific fine-tuning or task-specific prompt engineering can\nhinder their generalization. We propose UPRISE (Universal Prompt Retrieval for\nImproving zero-Shot Evaluation), which tunes a lightweight and versatile\nretriever that automatically retrieves prompts for a given zero-shot task\ninput. Specifically, we demonstrate universality in a cross-task and\ncross-model scenario: the retriever is tuned on a diverse set of tasks, but\ntested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for\ntuning the retriever, but test the retriever on different LLMs of much larger\nscales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that\nUPRISE mitigates the hallucination problem in our experiments with ChatGPT,\nsuggesting its potential to improve even the strongest LLMs. Our model and code\nare available at https://github.com/microsoft/LMOps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1\">Daixuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1\">Junyu Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yuefeng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianfeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1\">Denvy Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT Participates in a Computer Science Exam. (arXiv:2303.09461v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.09461","description":"<p>We asked ChatGPT to participate in an undergraduate computer science exam on\n''Algorithms and Data Structures''. The program was evaluated on the entire\nexam as posed to the students. We hand-copied its answers onto an exam sheet,\nwhich was subsequently graded in a blind setup alongside those of 200\nparticipating students. We find that ChatGPT narrowly passed the exam,\nobtaining 20.5 out of 40 points. This impressive performance indicates that\nChatGPT can indeed succeed in challenging tasks like university exams. At the\nsame time, the questions in our exam are structurally similar to those of other\nexams, solved homework problems, and teaching materials that can be found\nonline and might have been part of ChatGPT's training data. Therefore, it would\nbe inadequate to conclude from this experiment that ChatGPT has any\nunderstanding of computer science. We also assess the improvements brought by\nGPT-4. We find that GPT-4 would have obtained about 17\\% more exam points than\nGPT-3.5, reaching the performance of the average student. The transcripts of\nour conversations with ChatGPT are available at\n\\url{https://github.com/tml-tuebingen/chatgpt-algorithm-exam}, and the entire\ngraded exam is in the appendix of this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bordt_S/0/1/0/all/0/1\">Sebastian Bordt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luxburg_U/0/1/0/all/0/1\">Ulrike von Luxburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 event extraction from Twitter via extractive question answering with continuous prompts. (arXiv:2303.10659v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.10659","description":"<p>As COVID-19 ravages the world, social media analytics could augment\ntraditional surveys in assessing how the pandemic evolves and capturing\nconsumer chatter that could help healthcare agencies in addressing it. This\ntypically involves mining disclosure events that mention testing positive for\nthe disease or discussions surrounding perceptions and beliefs in preventative\nor treatment options. The 2020 shared task on COVID-19 event extraction\n(conducted as part of the W-NUT workshop during the EMNLP conference)\nintroduced a new Twitter dataset for benchmarking event extraction from\nCOVID-19 tweets. In this paper, we cast the problem of event extraction as\nextractive question answering using recent advances in continuous prompting in\nlanguage models. On the shared task test dataset, our approach leads to over 5%\nabsolute micro-averaged F1-score improvement over prior best results, across\nall COVID-19 event slots. Our ablation study shows that continuous prompts have\na major impact on the eventual performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuhang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavuluru_R/0/1/0/all/0/1\">Ramakanth Kavuluru</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Character, Word, or Both? Revisiting the Segmentation Granularity for Chinese Pre-trained Language Models. (arXiv:2303.10893v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.10893","description":"<p>Pretrained language models (PLMs) have shown marvelous improvements across\nvarious NLP tasks. Most Chinese PLMs simply treat an input text as a sequence\nof characters, and completely ignore word information. Although Whole Word\nMasking can alleviate this, the semantics in words is still not well\nrepresented. In this paper, we revisit the segmentation granularity of Chinese\nPLMs. We propose a mixed-granularity Chinese BERT (MigBERT) by considering both\ncharacters and words. To achieve this, we design objective functions for\nlearning both character and word-level representations. We conduct extensive\nexperiments on various Chinese NLP tasks to evaluate existing PLMs as well as\nthe proposed MigBERT. Experimental results show that MigBERT achieves new SOTA\nperformance on all these tasks. Further analysis demonstrates that words are\nsemantically richer than characters. More interestingly, we show that MigBERT\nalso works with Japanese. Our code and model have been released\nhere~\\footnote{https://github.com/xnliang98/MigBERT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xinnian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zefan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuangzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muyun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_C/0/1/0/all/0/1\">Chao Bian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EmotionIC: Emotional Inertia and Contagion-driven Dependency Modelling for Emotion Recognition in Conversation. (arXiv:2303.11117v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.11117","description":"<p>Emotion Recognition in Conversation (ERC) has attracted growing attention in\nrecent years as a result of the advancement and implementation of\nhuman-computer interface technologies. However, previous approaches to modeling\nglobal and local context dependencies lost the diversity of dependency\ninformation and do not take the context dependency into account at the\nclassification level. In this paper, we propose a novel approach to dependency\nmodeling driven by Emotional Inertia and Contagion (EmotionIC) for\nconversational emotion recognition at the feature extraction and classification\nlevels. At the feature extraction level, our designed Identity Masked\nMulti-head Attention (IM-MHA) captures the identity-based long-distant context\nin the dialogue to contain the diverse influence of different participants and\nconstruct the global emotional atmosphere, while the devised Dialogue-based\nGate Recurrent Unit (DialogGRU) that aggregates the emotional tendencies of\ndyadic dialogue is applied to refine the contextual features with inter- and\nintra-speaker dependencies. At the classification level, by introducing skip\nconnections in Conditional Random Field (CRF), we elaborate the Skip-chain CRF\n(SkipCRF) to capture the high-order dependencies within and between speakers,\nand to emulate the emotional flow of distant participants. Experimental results\nshow that our method can significantly outperform the state-of-the-art models\non four benchmark datasets. The ablation studies confirm that our modules can\neffectively model emotional inertia and contagion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingjian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhigang Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EVA-02: A Visual Representation for Neon Genesis. (arXiv:2303.11331v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2303.11331","description":"<p>We launch EVA-02, a next-generation Transformer-based visual representation\npre-trained to reconstruct strong and robust language-aligned vision features\nvia masked image modeling. With an updated plain Transformer architecture as\nwell as extensive pre-training from an open &amp; accessible giant CLIP vision\nencoder, EVA-02 demonstrates superior performance compared to prior\nstate-of-the-art approaches across various representative vision tasks, while\nutilizing significantly fewer parameters and compute budgets. Notably, using\nexclusively publicly accessible training data, EVA-02 with only 304M parameters\nachieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set.\nAdditionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on\nImageNet-1K, outperforming the previous largest &amp; best open-sourced CLIP with\nonly ~1/6 parameters and ~1/6 image-text training data. We offer four EVA-02\nvariants in various model sizes, ranging from 6M to 304M parameters, all with\nimpressive performance. To facilitate open access and open research, we release\nthe complete suite of EVA-02 to the community at\nhttps://github.com/baaivision/EVA/tree/master/EVA-02.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuxin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Quan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Can Be Used to Estimate the Ideologies of Politicians in a Zero-Shot Learning Setting. (arXiv:2303.12057v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2303.12057","description":"<p>The mass aggregation of knowledge embedded in large language models (LLMs)\nholds the promise of new solutions to problems of observability and measurement\nin the social sciences. We examine the utility of one such model for a\nparticularly difficult measurement task: measuring the latent ideology of\nlawmakers, which allows us to better understand functions that are core to\ndemocracy, such as how politics shape policy and how political actors represent\ntheir constituents. We scale the senators of the 116th United States Congress\nalong the liberal-conservative spectrum by prompting ChatGPT to select the more\nliberal (or conservative) senator in pairwise comparisons. We show that the LLM\nproduced stable answers across repeated iterations, did not hallucinate, and\nwas not simply regurgitating information from a single source. This new scale\nstrongly correlates with pre-existing liberal-conservative scales such as\nNOMINATE, but also differs in several important ways, such as correctly placing\nsenators who vote against their party for far-left or far-right ideological\nreasons on the extreme ends. The scale also highly correlates with ideological\nmeasures based on campaign giving and political activists' perceptions of these\nsenators. In addition to the potential for better-automated data collection and\ninformation retrieval, our results suggest LLMs are likely to open new avenues\nfor measuring latent constructs like ideology that rely on aggregating large\nquantities of data from public sources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Patrick Y. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tucker_J/0/1/0/all/0/1\">Joshua A. Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagler_J/0/1/0/all/0/1\">Jonathan Nagler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messing_S/0/1/0/all/0/1\">Solomon Messing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-03-22T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}