{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-11-02T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Very Low Resource Sentence Alignment: Luhya and Swahili. (arXiv:2211.00046v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00046","description":"<p>Language-agnostic sentence embeddings generated by pre-trained models such as\nLASER and LaBSE are attractive options for mining large datasets to produce\nparallel corpora for low-resource machine translation. We test LASER and LaBSE\nin extracting bitext for two related low-resource African languages: Luhya and\nSwahili. For this work, we created a new parallel set of nearly 8000\nLuhya-English sentences which allows a new zero-shot test of LASER and LaBSE.\nWe find that LaBSE significantly outperforms LASER on both languages. Both\nLASER and LaBSE however perform poorly at zero-shot alignment on Luhya,\nachieving just 1.5% and 22.0% successful alignments respectively (P@1 score).\nWe fine-tune the embeddings on a small set of parallel Luhya sentences and show\nsignificant gains, improving the LaBSE alignment accuracy to 53.3%. Further,\nrestricting the dataset to sentence embedding pairs with cosine similarity\nabove 0.7 yielded alignments with over 85% accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chimoto_E/0/1/0/all/0/1\">Everlyn Asiko Chimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bassett_B/0/1/0/all/0/1\">Bruce A. Bassett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Sequences by Learning to Self-Correct. (arXiv:2211.00053v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00053","description":"<p>Sequence generation applications require satisfying semantic constraints,\nsuch as ensuring that programs are correct, using certain keywords, or avoiding\nundesirable content. Language models, whether fine-tuned or prompted with\nfew-shot demonstrations, frequently violate these constraints, and lack a\nmechanism to iteratively revise their outputs. Moreover, some powerful language\nmodels are of extreme scale or inaccessible, making it inefficient, if not\ninfeasible, to update their parameters for task-specific adaptation. We present\nSelf-Correction, an approach that decouples an imperfect base generator (an\noff-the-shelf language model or supervised sequence-to-sequence model) from a\nseparate corrector that learns to iteratively correct imperfect generations. To\ntrain the corrector, we propose an online training procedure that can use\neither scalar or natural language feedback on intermediate imperfect\ngenerations. We show that Self-Correction improves upon the base generator in\nthree diverse generation tasks - mathematical program synthesis,\nlexically-constrained generation, and toxicity control - even when the\ncorrector is much smaller than the base generator.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tianxiao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WHEN FLUE MEETS FLANG: Benchmarks and Large Pre-trained Language Model for Financial Domain. (arXiv:2211.00083v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00083","description":"<p>Pre-trained language models have shown impressive performance on a variety of\ntasks and domains. Previous research on financial language models usually\nemploys a generic training scheme to train standard model architectures,\nwithout completely leveraging the richness of the financial data. We propose a\nnovel domain specific Financial LANGuage model (FLANG) which uses financial\nkeywords and phrases for better masking, together with span boundary objective\nand in-filing objective. Additionally, the evaluation benchmarks in the field\nhave been limited. To this end, we contribute the Financial Language\nUnderstanding Evaluation (FLUE), an open-source comprehensive suite of\nbenchmarks for the financial domain. These include new benchmarks across 5 NLP\ntasks in financial domain as well as common benchmarks used in the previous\nresearch. Experiments on these benchmarks suggest that our model outperforms\nthose in prior literature on a variety of NLP tasks. Our models, code and\nbenchmark data are publicly available on Github and Huggingface.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Raj Sanjay Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chawla_K/0/1/0/all/0/1\">Kunal Chawla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eidnani_D/0/1/0/all/0/1\">Dheeraj Eidnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Agam Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wendi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chava_S/0/1/0/all/0/1\">Sudheer Chava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_N/0/1/0/all/0/1\">Natraj Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smiley_C/0/1/0/all/0/1\">Charese Smiley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An analysis of degenerating speech due to progressive dysarthria on ASR performance. (arXiv:2211.00089v1 [eess.AS])","link":"http://arxiv.org/abs/2211.00089","description":"<p>Although personalized automatic speech recognition (ASR) models have recently\nbeen designed to recognize even severely impaired speech, model performance may\ndegrade over time for persons with degenerating speech. The aims of this study\nwere to (1) analyze the change of performance of ASR over time in individuals\nwith degrading speech, and (2) explore mitigation strategies to optimize\nrecognition throughout disease progression. Speech was recorded by four\nindividuals with degrading speech due to amyotrophic lateral sclerosis (ALS).\nWord error rates (WER) across recording sessions were computed for three ASR\nmodels: Unadapted Speaker Independent (U-SI), Adapted Speaker Independent\n(A-SI), and Adapted Speaker Dependent (A-SD or personalized). The performance\nof all three models degraded significantly over time as speech became more\nimpaired, but the performance of the A-SD model improved markedly when it was\nupdated with recordings from the severe stages of speech progression. Recording\nadditional utterances early in the disease before speech degraded significantly\ndid not improve the performance of A-SD models. Overall, our findings emphasize\nthe importance of continuous recording (and model retraining) when providing\npersonalized models for individuals with progressive speech impairments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tomanek_K/0/1/0/all/0/1\">Katrin Tomanek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seaver_K/0/1/0/all/0/1\">Katie Seaver</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_P/0/1/0/all/0/1\">Pan-Pan Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cave_R/0/1/0/all/0/1\">Richard Cave</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harrel_L/0/1/0/all/0/1\">Lauren Harrel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Green_J/0/1/0/all/0/1\">Jordan R. Green</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Efficient Cross-Lingual Transfer with Language-Specific Subnetworks. (arXiv:2211.00106v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00106","description":"<p>Large multilingual language models typically share their parameters across\nall languages, which enables cross-lingual task transfer, but learning can also\nbe hindered when training updates from different languages are in conflict. In\nthis paper, we propose novel methods for using language-specific subnetworks,\nwhich control cross-lingual parameter sharing, to reduce conflicts and increase\npositive transfer during fine-tuning. We introduce dynamic subnetworks, which\nare jointly updated with the model, and we combine our methods with\nmeta-learning, an established, but complementary, technique for improving\ncross-lingual transfer. Finally, we provide extensive analyses of how each of\nour methods affects the models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choenni_R/0/1/0/all/0/1\">Rochelle Choenni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrette_D/0/1/0/all/0/1\">Dan Garrette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1\">Ekaterina Shutova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Where to start? Analyzing the potential value of intermediate models. (arXiv:2211.00107v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00107","description":"<p>Previous studies observed that finetuned models may be better base models\nthan the vanilla pretrained model. Such a model, finetuned on some source\ndataset, may provide a better starting point for a new finetuning process on a\ndesired target dataset. Here, we perform a systematic analysis of this\n\\emph{intertraining} scheme, over a wide range of English classification tasks.\nSurprisingly, our analysis suggests that the potential intertraining gain can\nbe analyzed \\emph{independently} for the target dataset under consideration,\nand for a base model being considered as a starting point. This is in contrast\nto current perception that the alignment between the target dataset and the\nsource dataset used to generate the base model is a major factor in determining\nintertraining success. We analyze different aspects that contribute to each.\nFurthermore, we leverage our analysis to propose a practical and efficient\napproach to determine if and how to select a base model in real-world settings.\nLast, we release an updating ranking of best models in the HuggingFace hub per\narchitecture\\anonm{remove this link: https://ibm.github.io/model-recycling/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venezian_E/0/1/0/all/0/1\">Elad Venezian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Don_Yehia_S/0/1/0/all/0/1\">Shachar Don-Yehia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1\">Yoav Katz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Textless Direct Speech-to-Speech Translation with Discrete Speech Representation. (arXiv:2211.00115v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00115","description":"<p>Research on speech-to-speech translation (S2ST) has progressed rapidly in\nrecent years. Many end-to-end systems have been proposed and show advantages\nover conventional cascade systems, which are often composed of recognition,\ntranslation and synthesis sub-systems. However, most of the end-to-end systems\nstill rely on intermediate textual supervision during training, which makes it\ninfeasible to work for languages without written forms. In this work, we\npropose a novel model, Textless Translatotron, which is based on Translatotron\n2, for training an end-to-end direct S2ST model without any textual\nsupervision. Instead of jointly training with an auxiliary task predicting\ntarget phonemes as in Translatotron 2, the proposed model uses an auxiliary\ntask predicting discrete speech representations which are obtained from learned\nor random speech quantizers. When a speech encoder pre-trained with\nunsupervised speech data is used for both models, the proposed model obtains\ntranslation quality nearly on-par with Translatotron 2 on the multilingual\nCVSS-C corpus as well as the bilingual Fisher Spanish-English corpus. On the\nlatter, it outperforms the prior state-of-the-art textless model by +18.5 BLEU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinjian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1\">Chung-Cheng Chiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TaTa: A Multilingual Table-to-Text Dataset for African Languages. (arXiv:2211.00142v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00142","description":"<p>Existing data-to-text generation datasets are mostly limited to English. To\naddress this lack of data, we create Table-to-Text in African languages (TaTa),\nthe first large multilingual table-to-text dataset with a focus on African\nlanguages. We created TaTa by transcribing figures and accompanying text in\nbilingual reports by the Demographic and Health Surveys Program, followed by\nprofessional translation to make the dataset fully parallel. TaTa includes\n8,700 examples in nine languages including four African languages (Hausa, Igbo,\nSwahili, and Yor\\`ub\\'a) and a zero-shot test language (Russian). We\nadditionally release screenshots of the original figures for future research on\nmultilingual multi-modal approaches. Through an in-depth human evaluation, we\nshow that TaTa is challenging for current models and that less than half the\noutputs from an mT5-XXL-based model are understandable and attributable to the\nsource data. We further demonstrate that existing metrics perform poorly for\nTaTa and introduce learned metrics that achieve a high correlation with human\njudgments. We release all data and annotations at\nhttps://github.com/google-research/url-nlp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaev_V/0/1/0/all/0/1\">Vitaly Nikolaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botha_J/0/1/0/all/0/1\">Jan A. Botha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chavinda_M/0/1/0/all/0/1\">Michael Chavinda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_A/0/1/0/all/0/1\">Ankur Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivera_C/0/1/0/all/0/1\">Clara Rivera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Close Look into the Calibration of Pre-trained Language Models. (arXiv:2211.00151v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00151","description":"<p>Pre-trained language models (PLMs) achieve remarkable performance on many\ndownstream tasks, but may fail in giving reliable estimates of their predictive\nuncertainty. Given the lack of a comprehensive understanding of PLMs\ncalibration, we take a close look into this new research problem, aiming to\nanswer two questions: (1) Do PLMs learn to become calibrated in the training\nprocess? (2) How effective are existing calibration methods? For the first\nquestion, we conduct fine-grained control experiments to study the dynamic\nchange in PLMs' calibration performance in training. We consider six factors as\ncontrol variables, including dataset difficulty, available training samples,\ntraining steps, the number of tunable parameters, model scale, and pretraining.\nIn experiments, we observe a consistent change in calibration performance\nacross six factors. We find that PLMs don't learn to become calibrated in\ntraining, evidenced by the continual increase in confidence, no matter the\npredictions are correct or not. We highlight that our finding presents some\ncontradiction with two established conclusions: (a) Larger PLMs are more\ncalibrated; (b) Pretraining improves model calibration. Next, we study the\neffectiveness of existing calibration methods in mitigating the overconfidence\nissue, in both in-distribution and various out-of-distribution settings.\nBesides unlearnable calibration methods, we adapt two recently proposed\nlearnable methods that directly collect data to train models to have reasonable\nconfidence estimations. Also, we propose extended learnable methods based on\nexisting ones to further improve or maintain PLMs calibration without\nsacrificing the original task performance. Experimental results show that\nlearnable methods significantly reduce PLMs' confidence in wrong predictions,\nand our methods exhibit superior performance compared with previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lifan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1\">Ganqu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do LSTMs See Gender? Probing the Ability of LSTMs to Learn Abstract Syntactic Rules. (arXiv:2211.00153v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00153","description":"<p>LSTMs trained on next-word prediction can accurately perform linguistic tasks\nthat require tracking long-distance syntactic dependencies. Notably, model\naccuracy approaches human performance on number agreement tasks (Gulordava et\nal., 2018). However, we do not have a mechanistic understanding of how LSTMs\nperform such linguistic tasks. Do LSTMs learn abstract grammatical rules, or do\nthey rely on simple heuristics? Here, we test gender agreement in French which\nrequires tracking both hierarchical syntactic structures and the inherent\ngender of lexical units. Our model is able to reliably predict long-distance\ngender agreement in two subject-predicate contexts: noun-adjective and\nnoun-passive-verb agreement. The model showed more inaccuracies on plural noun\nphrases with gender attractors compared to singular cases, suggesting a\nreliance on clues from gendered articles for agreement. Overall, our study\nhighlights key ways in which LSTMs deviate from human behaviour and questions\nwhether LSTMs genuinely learn abstract syntactic rules and categories. We\npropose using gender agreement as a useful probe to investigate the underlying\nmechanisms, internal representations, and linguistic capabilities of LSTM\nlanguage models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sukumaran_P/0/1/0/all/0/1\">Priyanka Sukumaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houghton_C/0/1/0/all/0/1\">Conor Houghton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazanina_N/0/1/0/all/0/1\">Nina Kazanina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Emotion Embeddings to Transfer Knowledge Between Emotions, Languages, and Annotation Formats. (arXiv:2211.00171v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00171","description":"<p>The need for emotional inference from text continues to diversify as more and\nmore disciplines integrate emotions into their theories and applications. These\nneeds include inferring different emotion types, handling multiple languages,\nand different annotation formats. A shared model between different\nconfigurations would enable the sharing of knowledge and a decrease in training\ncosts, and would simplify the process of deploying emotion recognition models\nin novel environments. In this work, we study how we can build a single model\nthat can transition between these different configurations by leveraging\nmultilingual models and Demux, a transformer-based model whose input includes\nthe emotions of interest, enabling us to dynamically change the emotions\npredicted by the model. Demux also produces emotion embeddings, and performing\noperations on them allows us to transition to clusters of emotions by pooling\nthe embeddings of each cluster. We show that Demux can simultaneously transfer\nknowledge in a zero-shot manner to a new language, to a novel annotation format\nand to unseen emotions. Code is available at\nhttps://github.com/gchochla/Demux-MEmo .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chochlakis_G/0/1/0/all/0/1\">Georgios Chochlakis</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_G/0/1/0/all/0/1\">Gireesh Mahajan</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Baruah_S/0/1/0/all/0/1\">Sabyasachee Baruah</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Burghardt_K/0/1/0/all/0/1\">Keith Burghardt</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_K/0/1/0/all/0/1\">Kristina Lerman</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a> (1 and 2) ((1) Signal Analysis and Interpretation Lab, University of Southern California, (2) Information Science Institute, University of Southern California, (3) Microsoft Cognitive Services)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Audio/Text Training for Transformer Rescorer of Streaming Speech Recognition. (arXiv:2211.00174v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00174","description":"<p>Recently, there has been an increasing interest in two-pass streaming\nend-to-end speech recognition (ASR) that incorporates a 2nd-pass rescoring\nmodel on top of the conventional 1st-pass streaming ASR model to improve\nrecognition accuracy while keeping latency low. One of the latest 2nd-pass\nrescoring model, Transformer Rescorer, takes the n-best initial outputs and\naudio embeddings from the 1st-pass model, and then choose the best output by\nre-scoring the n-best initial outputs. However, training this Transformer\nRescorer requires expensive paired audio-text training data because the model\nuses audio embeddings as input. In this work, we present our Joint Audio/Text\ntraining method for Transformer Rescorer, to leverage unpaired text-only data\nwhich is relatively cheaper than paired audio-text data. We evaluate\nTransformer Rescorer with our Joint Audio/Text training on Librispeech dataset\nas well as our large-scale in-house dataset and show that our training method\ncan improve word error rate (WER) significantly compared to standard\nTransformer Rescorer without requiring any extra model parameters or latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Suyoun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabela_L/0/1/0/all/0/1\">Lucas Kabela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rongqing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiedan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duc Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CCS Explorer: Relevance Prediction, Extractive Summarization, and Named Entity Recognition from Clinical Cohort Studies. (arXiv:2211.00201v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00201","description":"<p>Clinical Cohort Studies (CCS) are a great source of documented clinical\nresearch. Ideally, a clinical expert will interpret these articles for\nexploratory analysis ranging from drug discovery for evaluating the efficacy of\nexisting drugs in tackling emerging diseases to the first test of newly\ndeveloped drugs. However, more than 100 CCS articles are published on PubMed\nevery day. As a result, it can take days for a doctor to find articles and\nextract relevant information. Can we find a way to quickly sift through the\nlong list of these articles faster and document the crucial takeaways from each\nof these articles? In this work, we propose CCS Explorer, an end-to-end system\nfor relevance prediction of sentences, extractive summarization, and patient,\noutcome, and intervention entity detection from CCS. CCS Explorer is packaged\nin a web-based graphical user interface where the user can provide any disease\nname. CCS Explorer then extracts and aggregates all relevant information from\narticles on PubMed based on the results of an automatically generated query\nproduced on the back-end. CCS Explorer fine-tunes pre-trained language models\nbased on transformers with additional layers for each of these tasks. We\nevaluate the models using two publicly available datasets. CCS Explorer obtains\na recall of 80.2%, AUC-ROC of 0.843, and an accuracy of 88.3% on sentence\nrelevance prediction using BioBERT and achieves an average Micro F1-Score of\n77.8% on Patient, Intervention, Outcome detection (PIO) using PubMedBERT. Thus,\nCCS Explorer can reliably extract relevant information to summarize articles,\nsaving time by ~ 660$\\times$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_Hussaini_I/0/1/0/all/0/1\">Irfan Al-Hussaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1\">Davi Nakajima An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Albert J. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1\">Sarah Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_C/0/1/0/all/0/1\">Cassie S. Mitchell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why Is It Hate Speech? Masked Rationale Prediction for Explainable Hate Speech Detection. (arXiv:2211.00243v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00243","description":"<p>In a hate speech detection model, we should consider two critical aspects in\naddition to detection performance-bias and explainability. Hate speech cannot\nbe identified based solely on the presence of specific words: the model should\nbe able to reason like humans and be explainable. To improve the performance\nconcerning the two aspects, we propose Masked Rationale Prediction (MRP) as an\nintermediate task. MRP is a task to predict the masked human\nrationales-snippets of a sentence that are grounds for human judgment-by\nreferring to surrounding tokens combined with their unmasked rationales. As the\nmodel learns its reasoning ability based on rationales by MRP, it performs hate\nspeech detection robustly in terms of bias and explainability. The proposed\nmethod generally achieves state-of-the-art performance in various metrics,\ndemonstrating its effectiveness for hate speech detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Byounghan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kyung-Ah Sohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FADO: Feedback-Aware Double COntrolling Network for Emotional Support Conversation. (arXiv:2211.00250v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00250","description":"<p>Emotional Support Conversation (ESConv) aims to reduce help-seekers'emotional\ndistress with the supportive strategy and response. It is essential for the\nsupporter to select an appropriate strategy with the feedback of the\nhelp-seeker (e.g., emotion change during dialog turns, etc) in ESConv. However,\nprevious methods mainly focus on the dialog history to select the strategy and\nignore the help-seeker's feedback, leading to the wrong and user-irrelevant\nstrategy prediction. In addition, these approaches only model the\ncontext-to-strategy flow and pay less attention to the strategy-to-context flow\nthat can focus on the strategy-related context for generating the\nstrategy-constrain response. In this paper, we propose a Feedback-Aware Double\nCOntrolling Network (FADO) to make a strategy schedule and generate the\nsupportive response. The core module in FADO consists of a dual-level feedback\nstrategy selector and a double control reader. Specifically, the dual-level\nfeedback strategy selector leverages the turn-level and conversation-level\nfeedback to encourage or penalize strategies. The double control reader\nconstructs the novel strategy-to-context flow for generating the\nstrategy-constrain response. Furthermore, a strategy dictionary is designed to\nenrich the semantic information of the strategy and improve the quality of\nstrategy-constrain response. Experimental results on ESConv show that the\nproposed FADO has achieved the state-of-the-art performance in terms of both\nstrategy selection and response generation. Our code is available at\nhttps://github/after/reviewing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Ziyuan Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuqiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunpeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CARE: Causality Reasoning for Empathetic Responses by Conditional Graph Generation. (arXiv:2211.00255v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00255","description":"<p>Recent approaches to empathetic response generation incorporate emotion\ncausalities to enhance comprehension of both the user's feelings and\nexperiences. However, these approaches suffer from two critical issues. First,\nthey only consider causalities between the user's emotion and the user's\nexperiences, and ignore those between the user's experiences. Second, they\nneglect interdependence among causalities and reason them independently. To\nsolve the above problems, we expect to reason all plausible causalities\ninterdependently and simultaneously, given the user's emotion, dialogue\nhistory, and future dialogue content. Then, we infuse these causalities into\nresponse generation for empathetic responses. Specifically, we design a new\nmodel, i.e., the Conditional Variational Graph Auto-Encoder (CVGAE), for the\ncausality reasoning, and adopt a multi-source attention mechanism in the\ndecoder for the causality infusion. We name the whole framework as CARE,\nabbreviated for CAusality Reasoning for Empathetic conversation. Experimental\nresults indicate that our method achieves state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiashuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Vision-Language Models with Less Bimodal Supervision. (arXiv:2211.00262v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00262","description":"<p>Standard practice in pretraining multimodal models, such as vision-language\nmodels, is to rely on pairs of aligned inputs from both modalities, for\nexample, aligned image-text pairs. However, such pairs can be difficult to\nobtain in low-resource settings and for some modality pairs (e.g., structured\ntables and images). In this work, we investigate the extent to which we can\nreduce the reliance on such parallel data, which we term \\emph{bimodal\nsupervision}, and use models that are pretrained on each modality\nindependently. We experiment with a high-performing vision-language model, and\nanalyze the effect of bimodal supervision on three vision-language tasks. We\nfind that on simpler tasks, such as VQAv2 and GQA, one can eliminate bimodal\nsupervision completely, suffering only a minor loss in performance. Conversely,\nfor NLVR2, which requires more complex reasoning, training without bimodal\nsupervision leads to random performance. Nevertheless, using only 5\\% of the\nbimodal data (142K images along with their captions), or leveraging weak\nsupervision in the form of a list of machine-generated labels for each image,\nleads to only a moderate degradation compared to using 3M image-text pairs:\n74\\%$\\rightarrow$$\\sim$70\\%. Our code is available at\nhttps://github.com/eladsegal/less-bimodal-sup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Segal_E/0/1/0/all/0/1\">Elad Segal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogin_B/0/1/0/all/0/1\">Ben Bogin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FRSUM: Towards Faithful Abstractive Summarization via Enhancing Factual Robustness. (arXiv:2211.00294v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00294","description":"<p>Despite being able to generate fluent and grammatical text, current Seq2Seq\nsummarization models still suffering from the unfaithful generation problem. In\nthis paper, we study the faithfulness of existing systems from a new\nperspective of factual robustness which is the ability to correctly generate\nfactual information over adversarial unfaithful information. We first measure a\nmodel's factual robustness by its success rate to defend against adversarial\nattacks when generating factual information. The factual robustness analysis on\na wide range of current systems shows its good consistency with human judgments\non faithfulness. Inspired by these findings, we propose to improve the\nfaithfulness of a model by enhancing its factual robustness. Specifically, we\npropose a novel training strategy, namely FRSUM, which teaches the model to\ndefend against both explicit adversarial samples and implicit factual\nadversarial perturbations. Extensive automatic and human evaluation results\nshow that FRSUM consistently improves the faithfulness of various Seq2Seq\nmodels, such as T5, BART.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziqiang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sujian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CONDAQA: A Contrastive Reading Comprehension Dataset for Reasoning about Negation. (arXiv:2211.00295v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00295","description":"<p>The full power of human language-based communication cannot be realized\nwithout negation. All human languages have some form of negation. Despite this,\nnegation remains a challenging phenomenon for current natural language\nunderstanding systems. To facilitate the future development of models that can\nprocess negation effectively, we present CONDAQA, the first English reading\ncomprehension dataset which requires reasoning about the implications of\nnegated statements in paragraphs. We collect paragraphs with diverse negation\ncues, then have crowdworkers ask questions about the implications of the\nnegated statement in the passage. We also have workers make three kinds of\nedits to the passage -- paraphrasing the negated statement, changing the scope\nof the negation, and reversing the negation -- resulting in clusters of\nquestion-answer pairs that are difficult for models to answer with spurious\nshortcuts. CONDAQA features 14,182 question-answer pairs with over 200 unique\nnegation cues and is challenging for current state-of-the-art models. The best\nperforming model on CONDAQA (UnifiedQA-v2-3b) achieves only 42% on our\nconsistency metric, well below human performance which is 81%. We release our\ndataset, along with fully-finetuned, few-shot, and zero-shot evaluations, to\nfacilitate the development of future NLP methods that work on negated language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravichander_A/0/1/0/all/0/1\">Abhilasha Ravichander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marasovic_A/0/1/0/all/0/1\">Ana Marasovi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognizing Nested Entities from Flat Supervision: A New NER Subtask, Feasibility and Challenges. (arXiv:2211.00301v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00301","description":"<p>Many recent named entity recognition (NER) studies criticize flat NER for its\nnon-overlapping assumption, and switch to investigating nested NER. However,\nexisting nested NER models heavily rely on training data annotated with nested\nentities, while labeling such data is costly. This study proposes a new\nsubtask, nested-from-flat NER, which corresponds to a realistic application\nscenario: given data annotated with flat entities only, one may still desire\nthe trained model capable of recognizing nested entities. To address this task,\nwe train span-based models and deliberately ignore the spans nested inside\nlabeled entities, since these spans are possibly unlabeled entities. With\nnested entities removed from the training data, our model achieves 54.8%, 54.2%\nand 41.1% F1 scores on the subset of spans within entities on ACE 2004, ACE\n2005 and GENIA, respectively. This suggests the effectiveness of our approach\nand the feasibility of the task. In addition, the model's performance on flat\nentities is entirely unaffected. We further manually annotate the nested\nentities in the test set of CoNLL 2003, creating a nested-from-flat NER\nbenchmark. Analysis results show that the main challenges stem from the data\nand annotation inconsistencies between the flat and nested entities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1\">Enwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Ming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech-text based multi-modal training with bidirectional attention for improved speech recognition. (arXiv:2211.00325v1 [eess.AS])","link":"http://arxiv.org/abs/2211.00325","description":"<p>To let the state-of-the-art end-to-end ASR model enjoy data efficiency, as\nwell as much more unpaired text data by multi-modal training, one needs to\naddress two problems: 1) the synchronicity of feature sampling rates between\nspeech and language (aka text data); 2) the homogeneity of the learned\nrepresentations from two encoders. In this paper we propose to employ a novel\nbidirectional attention mechanism (BiAM) to jointly learn both ASR encoder\n(bottom layers) and text encoder with a multi-modal learning method. The BiAM\nis to facilitate feature sampling rate exchange, realizing the quality of the\ntransformed features for the one kind to be measured in another space, with\ndiversified objective functions. As a result, the speech representations are\nenriched with more linguistic information, while the representations generated\nby the text encoder are more similar to corresponding speech ones, and\ntherefore the shared ASR models are more amenable for unpaired text data\npretraining. To validate the efficacy of the proposed method, we perform two\ncategories of experiments with or without extra unpaired text data.\nExperimental results on Librispeech corpus show it can achieve up to 6.15% word\nerror rate reduction (WERR) with only paired data learning, while 9.23% WERR\nwhen more unpaired text data is employed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yuhang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_H/0/1/0/all/0/1\">Haihua Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">Hao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chng_E/0/1/0/all/0/1\">Eng Siong Chng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Content-Aware Neural Text-To-Speech MOS Prediction Using Prosodic and Linguistic Features. (arXiv:2211.00342v1 [cs.SD])","link":"http://arxiv.org/abs/2211.00342","description":"<p>Current state-of-the-art methods for automatic synthetic speech evaluation\nare based on MOS prediction neural models. Such MOS prediction models include\nMOSNet and LDNet that use spectral features as input, and SSL-MOS that relies\non a pretrained self-supervised learning model that directly uses the speech\nsignal as input. In modern high-quality neural TTS systems, prosodic\nappropriateness with regard to the spoken content is a decisive factor for\nspeech naturalness. For this reason, we propose to include prosodic and\nlinguistic features as additional inputs in MOS prediction systems, and\nevaluate their impact on the prediction outcome. We consider phoneme level F0\nand duration features as prosodic inputs, as well as Tacotron encoder outputs,\nPOS tags and BERT embeddings as higher-level linguistic inputs. All MOS\nprediction systems are trained on SOMOS, a neural TTS-only dataset with\ncrowdsourced naturalness MOS evaluations. Results show that the proposed\nadditional features are beneficial in the MOS prediction task, by improving the\npredicted MOS scores' correlation with the ground truths, both at\nutterance-level and system-level predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vioni_A/0/1/0/all/0/1\">Alexandra Vioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maniati_G/0/1/0/all/0/1\">Georgia Maniati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellinas_N/0/1/0/all/0/1\">Nikolaos Ellinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_J/0/1/0/all/0/1\">June Sig Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_I/0/1/0/all/0/1\">Inchul Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalamandaris_A/0/1/0/all/0/1\">Aimilios Chalamandaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsiakoulis_P/0/1/0/all/0/1\">Pirros Tsiakoulis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anytime Generation of Counterfactual Explanations for Text Classification. (arXiv:2211.00369v1 [cs.LG])","link":"http://arxiv.org/abs/2211.00369","description":"<p>In many machine learning applications, it is important for the user to\nunderstand the reasoning behind the recommendation or prediction of the\nclassifiers. The learned models, however, are often too complicated to be\nunderstood by a human. Research from the social sciences indicates that humans\nprefer counterfactual explanations over alternatives. In this paper, we present\na general framework for generating counterfactual explanations in the textual\ndomain. Our framework is model-agnostic, representation-agnostic,\ndomain-agnostic, and anytime. We model the task as a search problem in a space\nwhere the initial state is the classified text, and the goal state is a text in\nthe complementary class. The operators transform a text by replacing parts of\nit. Our framework includes domain-independent operators, but can also exploit\ndomain-specific knowledge through specialized operators. The search algorithm\nattempts to find a text from the complementary class with minimal word-level\nLevenshtein distance from the original classified object.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gilo_D/0/1/0/all/0/1\">Daniel Gilo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markovitch_S/0/1/0/all/0/1\">Shaul Markovitch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Gender-Ambiguous Text-to-Speech Voices. (arXiv:2211.00375v1 [cs.SD])","link":"http://arxiv.org/abs/2211.00375","description":"<p>The gender of a voice assistant or any voice user interface is a central\nelement of its perceived identity. While a female voice is a common choice,\nthere is an increasing interest in alternative approaches where the gender is\nambiguous rather than clearly identifying as female or male. This work\naddresses the task of generating gender-ambiguous text-to-speech (TTS) voices\nthat do not correspond to any existing person. This is accomplished by sampling\nfrom a latent speaker embeddings' space that was formed while training a\nmultilingual, multi-speaker TTS system on data from multiple male and female\nspeakers. Various options are investigated regarding the sampling process. In\nour experiments, the effects of different sampling choices on the gender\nambiguity and the naturalness of the resulting voices are evaluated. The\nproposed method is shown able to efficiently generate novel speakers that are\nsuperior to a baseline averaged speaker embedding. To our knowledge, this is\nthe first systematic approach that can reliably generate a range of\ngender-ambiguous voices to meet diverse user requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Markopoulos_K/0/1/0/all/0/1\">Konstantinos Markopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maniati_G/0/1/0/all/0/1\">Georgia Maniati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vamvoukakis_G/0/1/0/all/0/1\">Georgios Vamvoukakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellinas_N/0/1/0/all/0/1\">Nikolaos Ellinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikitaras_K/0/1/0/all/0/1\">Karolos Nikitaras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klapsas_K/0/1/0/all/0/1\">Konstantinos Klapsas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vardaxoglou_G/0/1/0/all/0/1\">Georgios Vardaxoglou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakoulidis_P/0/1/0/all/0/1\">Panos Kakoulidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_J/0/1/0/all/0/1\">June Sig Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_I/0/1/0/all/0/1\">Inchul Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalamandaris_A/0/1/0/all/0/1\">Aimilios Chalamandaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsiakoulis_P/0/1/0/all/0/1\">Pirros Tsiakoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raptis_S/0/1/0/all/0/1\">Spyros Raptis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The future is different: Large pre-trained language models fail in prediction tasks. (arXiv:2211.00384v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00384","description":"<p>Large pre-trained language models (LPLM) have shown spectacular success when\nfine-tuned on downstream supervised tasks. Yet, it is known that their\nperformance can drastically drop when there is a distribution shift between the\ndata used during training and that used at inference time. In this paper we\nfocus on data distributions that naturally change over time and introduce four\nnew REDDIT datasets, namely the WALLSTREETBETS, ASKSCIENCE, THE DONALD, and\nPOLITICS sub-reddits. First, we empirically demonstrate that LPLM can display\naverage performance drops of about 88% (in the best case!) when predicting the\npopularity of future posts from sub-reddits whose topic distribution changes\nwith time. We then introduce a simple methodology that leverages neural\nvariational dynamic topic models and attention mechanisms to infer temporal\nlanguage model representations for regression tasks. Our models display\nperformance drops of only about 40% in the worst cases (2% in the best ones)\nwhen predicting the popularity of future posts, while using only about 7% of\nthe total number of parameters of LPLM and providing interpretable\nrepresentations that offer insight into real-world events, like the GameStop\nshort squeeze of 2021\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cvejoski_K/0/1/0/all/0/1\">Kostadin Cvejoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_R/0/1/0/all/0/1\">Rams&#xe9;s J. S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojeda_C/0/1/0/all/0/1\">C&#xe9;sar Ojeda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Order-sensitive Neural Constituency Parsing. (arXiv:2211.00421v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00421","description":"<p>We propose a novel algorithm that improves on the previous neural span-based\nCKY decoder for constituency parsing. In contrast to the traditional span-based\ndecoding, where spans are combined only based on the sum of their scores, we\nintroduce an order-sensitive strategy, where the span combination scores are\nmore carefully derived from an order-sensitive basis. Our decoder can be\nregarded as a generalization over existing span-based decoder in determining a\nfiner-grain scoring scheme for the combination of lower-level spans into\nhigher-level spans, where we emphasize on the order of the lower-level spans\nand use order-sensitive span scores as well as order-sensitive combination\ngrammar rule scores to enhance prediction accuracy. We implement the proposed\ndecoding strategy harnessing GPU parallelism and achieve a decoding speed on\npar with state-of-the-art span-based parsers. Using the previous\nstate-of-the-art model without additional data as our baseline, we outperform\nit and improve the F1 score on the Penn Treebank Dataset by 0.26% and on the\nChinese Treebank Dataset by 0.35%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1\">Tianyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Liyin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VarMAE: Pre-training of Variational Masked Autoencoder for Domain-adaptive Language Understanding. (arXiv:2211.00430v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00430","description":"<p>Pre-trained language models have achieved promising performance on general\nbenchmarks, but underperform when migrated to a specific domain. Recent works\nperform pre-training from scratch or continual pre-training on domain corpora.\nHowever, in many specific domains, the limited corpus can hardly support\nobtaining precise representations. To address this issue, we propose a novel\nTransformer-based language model named VarMAE for domain-adaptive language\nunderstanding. Under the masked autoencoding objective, we design a context\nuncertainty learning module to encode the token's context into a smooth latent\ndistribution. The module can produce diverse and well-formed contextual\nrepresentations. Experiments on science- and finance-domain NLU tasks\ndemonstrate that VarMAE can be efficiently adapted to new domains with limited\nresources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dou Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xiaolong Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xiyang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mengyuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lianxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1\">Yang Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaofeng Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the Practical Effectiveness of Constituency Parse Extraction from Pre-trained Language Models. (arXiv:2211.00479v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00479","description":"<p>Constituency Parse Extraction from Pre-trained Language Models (CPE-PLM) is a\nrecent paradigm that attempts to induce constituency parse trees relying only\non the internal knowledge of pre-trained language models. While attractive in\nthe perspective that similar to in-context learning, it does not require\ntask-specific fine-tuning, the practical effectiveness of such an approach\nstill remains unclear, except that it can function as a probe for investigating\nlanguage models' inner workings. In this work, we mathematically reformulate\nCPE-PLM and propose two advanced ensemble methods tailored for it,\ndemonstrating that the new parsing paradigm can be competitive with common\nunsupervised parsers by introducing a set of heterogeneous PLMs combined using\nour techniques. Furthermore, we explore some scenarios where the trees\ngenerated by CPE-PLM are practically useful. Specifically, we show that CPE-PLM\nis more effective than typical supervised parsers in few-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taeuk Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and parallel decoding for transducer. (arXiv:2211.00484v1 [eess.AS])","link":"http://arxiv.org/abs/2211.00484","description":"<p>The transducer architecture is becoming increasingly popular in the field of\nspeech recognition, because it is naturally streaming as well as high in\naccuracy. One of the drawbacks of transducer is that it is difficult to decode\nin a fast and parallel way due to an unconstrained number of symbols that can\nbe emitted per time step. In this work, we introduce a constrained version of\ntransducer loss to learn strictly monotonic alignments between the sequences;\nwe also improve the standard greedy search and beam search algorithms by\nlimiting the number of symbols that can be emitted per time step in transducer\ndecoding, making it more efficient to decode in parallel with batches.\nFurthermore, we propose an finite state automaton-based (FSA) parallel beam\nsearch algorithm that can run with graphs on GPU efficiently. The experiment\nresults show that we achieve slight word error rate (WER) improvement as well\nas significant speedup in decoding. Our work is open-sourced and publicly\navailable\\footnote{https://github.com/k2-fsa/icefall}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kang_W/0/1/0/all/0/1\">Wei Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_L/0/1/0/all/0/1\">Liyong Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuang_F/0/1/0/all/0/1\">Fangjun Kuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Long Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_M/0/1/0/all/0/1\">Mingshuang Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_Z/0/1/0/all/0/1\">Zengwei Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zelasko_P/0/1/0/all/0/1\">Piotr &#x17b;elasko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Povey_D/0/1/0/all/0/1\">Daniel Povey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delay-penalized transducer for low-latency streaming ASR. (arXiv:2211.00490v1 [eess.AS])","link":"http://arxiv.org/abs/2211.00490","description":"<p>In streaming automatic speech recognition (ASR), it is desirable to reduce\nlatency as much as possible while having minimum impact on recognition\naccuracy. Although a few existing methods are able to achieve this goal, they\nare difficult to implement due to their dependency on external alignments. In\nthis paper, we propose a simple way to penalize symbol delay in transducer\nmodel, so that we can balance the trade-off between symbol delay and accuracy\nfor streaming models without external alignments. Specifically, our method adds\na small constant times (T/2 - t), where T is the number of frames and t is the\ncurrent frame, to all the non-blank log-probabilities (after normalization)\nthat are fed into the two dimensional transducer recursion. For both streaming\nConformer models and unidirectional long short-term memory (LSTM) models,\nexperimental results show that it can significantly reduce the symbol delay\nwith an acceptable performance degradation. Our method achieves similar\ndelay-accuracy trade-off to the previously published FastEmit, but we believe\nour method is preferable because it has a better justification: it is\nequivalent to penalizing the average symbol delay. Our work is open-sourced and\npublicly available (https://github.com/k2-fsa/k2).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kang_W/0/1/0/all/0/1\">Wei Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_Z/0/1/0/all/0/1\">Zengwei Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuang_F/0/1/0/all/0/1\">Fangjun Kuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_L/0/1/0/all/0/1\">Liyong Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+lin_L/0/1/0/all/0/1\">Long lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zelasko_P/0/1/0/all/0/1\">Piotr &#x17b;elasko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Povey_D/0/1/0/all/0/1\">Daniel Povey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Multi-Codebook Vector Quantization Indexes for Knowledge Distillation. (arXiv:2211.00508v1 [eess.AS])","link":"http://arxiv.org/abs/2211.00508","description":"<p>Knowledge distillation(KD) is a common approach to improve model performance\nin automatic speech recognition (ASR), where a student model is trained to\nimitate the output behaviour of a teacher model. However, traditional KD\nmethods suffer from teacher label storage issue, especially when the training\ncorpora are large. Although on-the-fly teacher label generation tackles this\nissue, the training speed is significantly slower as the teacher model has to\nbe evaluated every batch. In this paper, we reformulate the generation of\nteacher label as a codec problem. We propose a novel Multi-codebook Vector\nQuantization (MVQ) approach that compresses teacher embeddings to codebook\nindexes (CI). Based on this, a KD training framework (MVQ-KD) is proposed where\na student model predicts the CI generated from the embeddings of a\nself-supervised pre-trained teacher model. Experiments on the LibriSpeech\nclean-100 hour show that MVQ-KD framework achieves comparable performance as\ntraditional KD methods (l1, l2), while requiring 256 times less storage. When\nthe full LibriSpeech dataset is used, MVQ-KD framework results in 13.8% and\n8.2% relative word error rate reductions (WERRs) for non -streaming transducer\non test-clean and test-other and 4.0% and 4.9% for streaming transducer. The\nimplementation of this work is already released as a part of the open-source\nproject icefall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guo_L/0/1/0/all/0/1\">Liyong Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Quandong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kong_Y/0/1/0/all/0/1\">Yuxiang Kong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_Z/0/1/0/all/0/1\">Zengwei Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_F/0/1/0/all/0/1\">Fan Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuang_F/0/1/0/all/0/1\">Fangjun Kuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_W/0/1/0/all/0/1\">Wei Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Long Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_M/0/1/0/all/0/1\">Mingshuang Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zelasko_P/0/1/0/all/0/1\">Piotr Zelasko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Povey_D/0/1/0/all/0/1\">Daniel Povey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E2E Refined Dataset. (arXiv:2211.00513v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00513","description":"<p>Although the well-known MR-to-text E2E dataset has been used by many\nresearchers, its MR-text pairs include many deletion/insertion/substitution\nerrors. Since such errors affect the quality of MR-to-text systems, they must\nbe fixed as much as possible. Therefore, we developed a refined dataset and\nsome python programs that convert the original E2E dataset into a refined\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toyama_K/0/1/0/all/0/1\">Keisuke Toyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudoh_K/0/1/0/all/0/1\">Katsuhito Sudoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1\">Satoshi Nakamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrimTail: Low-Latency Streaming ASR with Simple but Effective Spectrogram-Level Length Penalty. (arXiv:2211.00522v1 [cs.SD])","link":"http://arxiv.org/abs/2211.00522","description":"<p>In this paper, we present TrimTail, a simple but effective emission\nregularization method to improve the latency of streaming ASR models. The core\nidea of TrimTail is to apply length penalty (i.e., by trimming trailing frames,\nsee Fig. 1-(b)) directly on the spectrogram of input utterances, which does not\nrequire any alignment. We demonstrate that TrimTail is computationally cheap\nand can be applied online and optimized with any training loss or any model\narchitecture on any dataset without any extra effort by applying it on various\nend-to-end streaming ASR networks either trained with CTC loss [1] or\nTransducer loss [2]. We achieve 100 $\\sim$ 200ms latency reduction with equal\nor even better accuracy on both Aishell-1 and Librispeech. Moreover, by using\nTrimTail, we can achieve a 400ms algorithmic improvement of User Sensitive\nDelay (USD) with an accuracy loss of less than 0.2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingchen Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuekai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhendong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Fuping Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Changbao Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning utterance-level representations through token-level acoustic latents prediction for Expressive Speech Synthesis. (arXiv:2211.00523v1 [cs.SD])","link":"http://arxiv.org/abs/2211.00523","description":"<p>This paper proposes an Expressive Speech Synthesis model that utilizes\ntoken-level latent prosodic variables in order to capture and control\nutterance-level attributes, such as character acting voice and speaking style.\nCurrent works aim to explicitly factorize such fine-grained and utterance-level\nspeech attributes into different representations extracted by modules that\noperate in the corresponding level. We show that the fine-grained latent space\nalso captures coarse-grained information, which is more evident as the\ndimension of latent space increases in order to capture diverse prosodic\nrepresentations. Therefore, a trade-off arises between the diversity of the\ntoken-level and utterance-level representations and their disentanglement. We\nalleviate this issue by first capturing rich speech attributes into a\ntoken-level latent space and then, separately train a prior network that given\nthe input text, learns utterance-level representations in order to predict the\nphoneme-level, posterior latents extracted during the previous step. Both\nqualitative and quantitative evaluations are used to demonstrate the\neffectiveness of the proposed approach. Audio samples are available in our demo\npage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nikitaras_K/0/1/0/all/0/1\">Karolos Nikitaras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klapsas_K/0/1/0/all/0/1\">Konstantinos Klapsas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellinas_N/0/1/0/all/0/1\">Nikolaos Ellinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maniati_G/0/1/0/all/0/1\">Georgia Maniati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_J/0/1/0/all/0/1\">June Sig Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_I/0/1/0/all/0/1\">Inchul Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raptis_S/0/1/0/all/0/1\">Spyros Raptis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalamandaris_A/0/1/0/all/0/1\">Aimilios Chalamandaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsiakoulis_P/0/1/0/all/0/1\">Pirros Tsiakoulis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Graph-based Cross-modal Information Fusion for Neural Sign Language Translation. (arXiv:2211.00526v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00526","description":"<p>Sign Language (SL), as the mother tongue of the deaf community, is a special\nvisual language that most hearing people cannot understand. In recent years,\nneural Sign Language Translation (SLT), as a possible way for bridging\ncommunication gap between the deaf and the hearing people, has attracted\nwidespread academic attention. We found that the current mainstream end-to-end\nneural SLT models, which tries to learning language knowledge in a weakly\nsupervised manner, could not mine enough semantic information under the\ncondition of low data resources. Therefore, we propose to introduce additional\nword-level semantic knowledge of sign language linguistics to assist in\nimproving current end-to-end neural SLT models. Concretely, we propose a novel\nneural SLT model with multi-modal feature fusion based on the dynamic graph, in\nwhich the cross-modal information, i.e. text and video, is first assembled as a\ndynamic graph according to their correlation, and then the graph is processed\nby a multi-modal graph encoder to generate the multi-modal embeddings for\nfurther usage in the subsequent neural translation models. To the best of our\nknowledge, we are the first to introduce graph neural networks, for fusing\nmulti-modal information, into neural sign language translation models.\nMoreover, we conducted experiments on a publicly available popular SLT dataset\nRWTH-PHOENIX-Weather-2014T. and the quantitative experiments show that our\nmethod can improve the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiangbin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Cheng Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yidong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClassActionPrediction: A Challenging Benchmark for Legal Judgment Prediction of Class Action Cases in the US. (arXiv:2211.00582v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00582","description":"<p>The research field of Legal Natural Language Processing (NLP) has been very\nactive recently, with Legal Judgment Prediction (LJP) becoming one of the most\nextensively studied tasks. To date, most publicly released LJP datasets\noriginate from countries with civil law. In this work, we release, for the\nfirst time, a challenging LJP dataset focused on class action cases in the US.\nIt is the first dataset in the common law system that focuses on the harder and\nmore realistic task involving the complaints as input instead of the often used\nfacts summary written by the court. Additionally, we study the difficulty of\nthe task by collecting expert human predictions, showing that even human\nexperts can only reach 53% accuracy on this dataset. Our Longformer model\nclearly outperforms the human baseline (63%), despite only considering the\nfirst 2,048 tokens. Furthermore, we perform a detailed error analysis and find\nthat the Longformer model is significantly better calibrated than the human\nexperts. Finally, we publicly release the dataset and the code used for the\nexperiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Semo_G/0/1/0/all/0/1\">Gil Semo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernsohn_D/0/1/0/all/0/1\">Dor Bernsohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagag_B/0/1/0/all/0/1\">Ben Hagag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_G/0/1/0/all/0/1\">Gila Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niklaus_J/0/1/0/all/0/1\">Joel Niklaus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"T5lephone: Bridging Speech and Text Self-supervised Models for Spoken Language Understanding via Phoneme level T5. (arXiv:2211.00586v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00586","description":"<p>In Spoken language understanding (SLU), a natural solution is concatenating\npre-trained speech models (e.g. HuBERT) and pretrained language models (PLM,\ne.g. T5). Most previous works use pretrained language models with subword-based\ntokenization. However, the granularity of input units affects the alignment of\nspeech model outputs and language model inputs, and PLM with character-based\ntokenization is underexplored. In this work, we conduct extensive studies on\nhow PLMs with different tokenization strategies affect spoken language\nunderstanding task including spoken question answering (SQA) and speech\ntranslation (ST). We further extend the idea to create T5lephone(pronounced as\ntelephone), a variant of T5 that is pretrained using phonemicized text. We\ninitialize T5lephone with existing PLMs to pretrain it using relatively\nlightweight computational resources. We reached state-of-the-art on NMSQA, and\nthe T5lephone model exceeds T5 with other types of units on end-to-end SQA and\nST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chan-Jan Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Ho-Lam Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small. (arXiv:2211.00593v1 [cs.LG])","link":"http://arxiv.org/abs/2211.00593","description":"<p>Research in mechanistic interpretability seeks to explain behaviors of\nmachine learning models in terms of their internal components. However, most\nprevious work either focuses on simple behaviors in small models, or describes\ncomplicated behaviors in larger models with broad strokes. In this work, we\nbridge this gap by presenting an explanation for how GPT-2 small performs a\nnatural language task called indirect object identification (IOI). Our\nexplanation encompasses 26 attention heads grouped into 7 main classes, which\nwe discovered using a combination of interpretability approaches relying on\ncausal interventions. To our knowledge, this investigation is the largest\nend-to-end attempt at reverse-engineering a natural behavior \"in the wild\" in a\nlanguage model. We evaluate the reliability of our explanation using three\nquantitative criteria--faithfulness, completeness and minimality. Though these\ncriteria support our explanation, they also point to remaining gaps in our\nunderstanding. Our work provides evidence that a mechanistic understanding of\nlarge ML models is feasible, opening opportunities to scale our understanding\nto both larger models and more complex tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kevin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Variengien_A/0/1/0/all/0/1\">Alexandre Variengien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conmy_A/0/1/0/all/0/1\">Arthur Conmy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlegeris_B/0/1/0/all/0/1\">Buck Shlegeris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Deduction with Incomplete Information. (arXiv:2211.00614v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00614","description":"<p>A growing body of work studies how to answer a question or verify a claim by\ngenerating a natural language \"proof\": a chain of deductive inferences yielding\nthe answer based on a set of premises. However, these methods can only make\nsound deductions when they follow from evidence that is given. We propose a new\nsystem that can handle the underspecified setting where not all premises are\nstated at the outset; that is, additional assumptions need to be materialized\nto prove a claim. By using a natural language generation model to abductively\ninfer a premise given another premise and a conclusion, we can impute missing\npieces of evidence needed for the conclusion to be true. Our system searches\nover two fringes in a bidirectional fashion, interleaving deductive\n(forward-chaining) and abductive (backward-chaining) generation steps. We\nsample multiple possible outputs for each step to achieve coverage of the\nsearch space, at the same time ensuring correctness by filtering low-quality\ngenerations with a round-trip validation procedure. Results on a modified\nversion of the EntailmentBank dataset and a new dataset called Everyday Norms:\nWhy Not? show that abductive generation with validation can recover premises\nacross in- and out-of-domain settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sprague_Z/0/1/0/all/0/1\">Zayne Sprague</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bostrom_K/0/1/0/all/0/1\">Kaj Bostrom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Swarat Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preserving In-Context Learning ability in Large Language Model Fine-tuning. (arXiv:2211.00635v1 [cs.CL])","link":"http://arxiv.org/abs/2211.00635","description":"<p>Pretrained large language models (LLMs) are strong in-context learners that\nare able to perform few-shot learning without changing model parameters.\nHowever, as we show, fine-tuning an LLM on any specific task generally destroys\nits in-context ability. We discover an important cause of this loss, format\nspecialization, where the model overfits to the format of the fine-tuned task\nand is unable to output anything beyond this format. We further show that\nformat specialization happens at the beginning of fine-tuning. To solve this\nproblem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet\neffective two-stage fine-tuning framework that preserves in-context abilities\nof the pretrained model. ProMoT first trains a soft prompt for the fine-tuning\ntarget task, and then fine-tunes the model itself with this soft prompt\nattached. ProMoT offloads task-specific formats into the soft prompt that can\nbe removed when doing other in-context tasks. We fine-tune mT5 XXL with ProMoT\non natural language inference (NLI) and English-French translation and evaluate\nthe in-context abilities of the resulting models on 8 different NLP tasks.\nProMoT achieves similar performance on the fine-tuned tasks compared with\nvanilla fine-tuning, but with much less reduction of in-context learning\nperformances across the board. More importantly, ProMoT shows remarkable\ngeneralization ability on tasks that have different formats, e.g. fine-tuning\non a NLI binary classification task improves the model's in-context ability to\ndo summarization (+0.53 Rouge-2 score compared to the pretrained model), making\nProMoT a promising method to build general purpose capabilities such as\ngrounding and reasoning into LLMs with small but high quality datasets. When\nextended to sequential or multi-task training, ProMoT can achieve even better\nout-of-domain generalization performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Si Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Daliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasik_M/0/1/0/all/0/1\">Michal Lukasik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Felix Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1\">Inderjit S Dhillon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GENIE: Toward Reproducible and Standardized Human Evaluation for Text Generation. (arXiv:2101.06561v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.06561","description":"<p>While often assumed a gold standard, effective human evaluation of text\ngeneration remains an important, open area for research. We revisit this\nproblem with a focus on producing consistent evaluations that are reproducible\n-- over time and across different populations. We study this goal in different\nstages of the human evaluation pipeline. In particular, we consider design\nchoices for the annotation interface used to elicit human judgments and their\nimpact on reproducibility. Furthermore, we develop an automated mechanism for\nmaintaining annotator quality via a probabilistic model that detects and\nexcludes noisy annotators. Putting these lessons together, we introduce GENIE:\na system for running standardized human evaluations across different generation\ntasks. We instantiate GENIE with datasets representing four core challenges in\ntext generation: machine translation, summarization, commonsense reasoning, and\nmachine comprehension. For each task, GENIE offers a leaderboard that\nautomatically crowdsources annotations for submissions, evaluating them along\naxes such as correctness, conciseness, and fluency. We have made the GENIE\nleaderboards publicly available, and have already ranked 50 submissions from 10\ndifferent research groups. We hope GENIE encourages further progress toward\neffective, standardized evaluations for text generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1\">Jonathan Bragg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lourie_N/0/1/0/all/0/1\">Nicholas Lourie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel S. Weld</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing the Transformer Decoder with Transition-based Syntax. (arXiv:2101.12640v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.12640","description":"<p>Notwithstanding recent advances, syntactic generalization remains a challenge\nfor text decoders. While some studies showed gains from incorporating\nsource-side symbolic syntactic and semantic structure into text generation\nTransformers, very little work addressed the decoding of such structure. We\npropose a general approach for tree decoding using a transition-based approach.\nExamining the challenging test case of incorporating Universal Dependencies\nsyntax into machine translation, we present substantial improvements on test\nsets that focus on syntactic generalization, while presenting improved or\ncomparable performance on standard MT benchmarks. Further qualitative analysis\naddresses cases where syntactic generalization in the vanilla Transformer\ndecoder is inadequate and demonstrates the advantages afforded by integrating\nsyntactic information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lower Perplexity is Not Always Human-Like. (arXiv:2106.01229v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.01229","description":"<p>In computational psycholinguistics, various language models have been\nevaluated against human reading behavior (e.g., eye movement) to build\nhuman-like computational models. However, most previous efforts have focused\nalmost exclusively on English, despite the recent trend towards linguistic\nuniversal within the general community. In order to fill the gap, this paper\ninvestigates whether the established results in computational psycholinguistics\ncan be generalized across languages. Specifically, we re-examine an established\ngeneralization -- the lower perplexity a language model has, the more\nhuman-like the language model is -- in Japanese with typologically different\nstructures from English. Our experiments demonstrate that this established\ngeneralization exhibits a surprising lack of universality; namely, lower\nperplexity is not always human-like. Moreover, this discrepancy between English\nand Japanese is further explored from the perspective of (non-)uniform\ninformation density. Overall, our results suggest that a cross-lingual\nevaluation will be necessary to construct human-like computational models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuribayashi_T/0/1/0/all/0/1\">Tatsuki Kuribayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oseki_Y/0/1/0/all/0/1\">Yohei Oseki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ito_T/0/1/0/all/0/1\">Takumi Ito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshida_R/0/1/0/all/0/1\">Ryo Yoshida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asahara_M/0/1/0/all/0/1\">Masayuki Asahara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Faithfulness of Importance Measures in NLP by Recursively Masking Allegedly Important Tokens and Retraining. (arXiv:2110.08412v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08412","description":"<p>To explain NLP models a popular approach is to use importance measures, such\nas attention, which inform input tokens are important for making a prediction.\nHowever, an open question is how well these explanations accurately reflect a\nmodel's logic, a property called faithfulness.\n</p>\n<p>To answer this question, we propose Recursive ROAR, a new faithfulness\nmetric. This works by recursively masking allegedly important tokens and then\nretraining the model. The principle is that this should result in worse model\nperformance compared to masking random tokens. The result is a performance\ncurve given a masking-ratio. Furthermore, we propose a summarizing metric using\nrelative area-between-curves (RACU), which allows for easy comparison across\npapers, models, and tasks.\n</p>\n<p>We evaluate 4 different importance measures on 8 different datasets, using\nboth LSTM-attention models and RoBERTa models. We find that the faithfulness of\nimportance measures is both model-dependent and task-dependent. This conclusion\ncontradicts previous evaluations in both computer vision and faithfulness of\nattention literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madsen_A/0/1/0/all/0/1\">Andreas Madsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meade_N/0/1/0/all/0/1\">Nicholas Meade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adlakha_V/0/1/0/all/0/1\">Vaibhav Adlakha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Systematic Investigation of Commonsense Knowledge in Large Language Models. (arXiv:2111.00607v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.00607","description":"<p>Language models (LMs) trained on large amounts of data have shown impressive\nperformance on many NLP tasks under the zero-shot and few-shot setup. Here we\naim to better understand the extent to which such models learn commonsense\nknowledge -- a critical component of many NLP applications. We conduct a\nsystematic and rigorous zero-shot and few-shot commonsense evaluation of large\npre-trained LMs, where we: (i) carefully control for the LMs' ability to\nexploit potential surface cues and annotation artefacts, and (ii) account for\nvariations in performance that arise from factors that are not related to\ncommonsense knowledge. Our findings highlight the limitations of pre-trained\nLMs in acquiring commonsense knowledge without task-specific supervision;\nfurthermore, using larger models or few-shot evaluation are insufficient to\nachieve human-level commonsense performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Lorraine Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuncoro_A/0/1/0/all/0/1\">Adhiguna Kuncoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_J/0/1/0/all/0/1\">Jordan Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+dAutume_C/0/1/0/all/0/1\">Cyprien de Masson d&#x27;Autume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blunsom_P/0/1/0/all/0/1\">Phil Blunsom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1\">Aida Nematzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Authorship Verification Experimental Setups. (arXiv:2112.05125v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.05125","description":"<p>One of the main drivers of the recent advances in authorship verification is\nthe PAN large-scale authorship dataset. Despite generating significant progress\nin the field, inconsistent performance differences between the closed and open\ntest sets have been reported. To this end, we improve the experimental setup by\nproposing five new public splits over the PAN dataset, specifically designed to\nisolate and identify biases related to the text topic and to the author's\nwriting style. We evaluate several BERT-like baselines on these splits, showing\nthat such models are competitive with authorship verification state-of-the-art\nmethods. Furthermore, using explainable AI, we find that these baselines are\nbiased towards named entities. We show that models trained without the named\nentities obtain better results and generalize better when tested on DarkReddit,\nour new dataset for authorship verification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brad_F/0/1/0/all/0/1\">Florin Brad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manolache_A/0/1/0/all/0/1\">Andrei Manolache</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burceanu_E/0/1/0/all/0/1\">Elena Burceanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbalau_A/0/1/0/all/0/1\">Antonio Barbalau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_M/0/1/0/all/0/1\">Marius Popescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Data Analysis with Next-step Natural Language Query Recommendation. (arXiv:2201.04868v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2201.04868","description":"<p>Natural language interfaces (NLIs) provide users with a convenient way to\ninteractively analyze data through natural language queries. Nevertheless,\ninteractive data analysis is a demanding process, especially for novice data\nanalysts. When exploring large and complex SQL databases from different\ndomains, data analysts do not necessarily have sufficient knowledge about\ndifferent data tables and application domains. It makes them unable to\nsystematically elicit a series of topically-related and meaningful queries for\ninsight discovery in target domains. We develop a NLI with a step-wise query\nrecommendation module to assist users in choosing appropriate next-step\nexploration actions. The system adopts a data-driven approach to suggest\nsemantically relevant and context-aware queries for application domains of\nusers' interest based on their query logs. Also, the system helps users\norganize query histories and results into a dashboard to communicate the\ndiscovered data insights. With a comparative user study, we show that our\nsystem can facilitate a more effective and systematic data analysis process\nthan a baseline without the recommendation module.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1\">Furui Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_J/0/1/0/all/0/1\">Jiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Emotion Recognition using Multi-task learning and a multimodal dynamic fusion network. (arXiv:2203.16794v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16794","description":"<p>Emotion Recognition (ER) aims to classify human utterances into different\nemotion categories. Based on early-fusion and self-attention-based multimodal\ninteraction between text and acoustic modalities, in this paper, we propose\nMMER, a multimodal multitask learning approach for ER from individual\nutterances in isolation. Our proposed MMER leverages a multimodal dynamic\nfusion network that adds minimal parameters over an existing speech encoder to\nleverage the semantic and syntactic properties hidden in text. Experiments on\nthe IEMOCAP benchmark show that our proposed model achieves state-of-the-art\nperformance. In addition, strong baselines and ablation studies prove the\neffectiveness of our proposed approach. We make our code publicly available on\nGitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramaneswaran_S/0/1/0/all/0/1\">S Ramaneswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_H/0/1/0/all/0/1\">Harshvardhan Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stretching Sentence-pair NLI Models to Reason over Long Documents and Clusters. (arXiv:2204.07447v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07447","description":"<p>Natural Language Inference (NLI) has been extensively studied by the NLP\ncommunity as a framework for estimating the semantic relation between sentence\npairs. While early work identified certain biases in NLI models, recent\nadvancements in modeling and datasets demonstrated promising performance. In\nthis work, we further explore the direct zero-shot applicability of NLI models\nto real applications, beyond the sentence-pair setting they were trained on.\nFirst, we analyze the robustness of these models to longer and out-of-domain\ninputs. Then, we develop new aggregation methods to allow operating over full\ndocuments, reaching state-of-the-art performance on the ContractNLI dataset.\nInterestingly, we find NLI scores to provide strong retrieval signals, leading\nto more relevant evidence extractions compared to common similarity-based\nmethods. Finally, we go further and investigate whole document clusters to\nidentify both discrepancies and consensus among sources. In a test case, we\nfind real inconsistencies between Wikipedia pages in different languages about\nthe same topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sihao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buthpitiya_S/0/1/0/all/0/1\">Senaka Buthpitiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabrikant_A/0/1/0/all/0/1\">Alex Fabrikant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrating Trust of Multi-Hop Question Answering Systems with Decompositional Probes. (arXiv:2204.07693v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07693","description":"<p>Multi-hop Question Answering (QA) is a challenging task since it requires an\naccurate aggregation of information from multiple context paragraphs and a\nthorough understanding of the underlying reasoning chains. Recent work in\nmulti-hop QA has shown that performance can be boosted by first decomposing the\nquestions into simpler, single-hop questions. In this paper, we explore one\nadditional utility of the multi-hop decomposition from the perspective of\nexplainable NLP: to create explanation by probing a neural QA model with them.\nWe hypothesize that in doing so, users will be better able to predict when the\nunderlying QA system will give the correct answer. Through human participant\nstudies, we verify that exposing the decomposition probes and answers to the\nprobes to users can increase their ability to predict system performance on a\nquestion instance basis. We show that decomposition is an effective form of\nprobing QA systems as well as a promising approach to explanation generation.\nIn-depth analyses show the need for improvements in decomposition systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1\">Kaige Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiegreffe_S/0/1/0/all/0/1\">Sarah Wiegreffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark Riedl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language to Code Translation with Execution. (arXiv:2204.11454v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.11454","description":"<p>Generative models of code, pretrained on large corpora of programs, have\nshown great success in translating natural language to code (Chen et al., 2021;\nAustin et al., 2021; Li et al., 2022, inter alia). While these models do not\nexplicitly incorporate program semantics (i.e., execution results) during\ntraining, they are able to generate correct solutions for many problems.\nHowever, choosing a single correct program from a generated set for each\nproblem remains challenging. In this work, we introduce execution result--based\nminimum Bayes risk decoding (MBR-EXEC) for program selection and show that it\nimproves the few-shot performance of pretrained code models on\nnatural-language-to-code tasks. We select output programs from a generated\ncandidate set by marginalizing over program implementations that share the same\nsemantics. Because exact equivalence is intractable, we execute each program on\na small number of test inputs to approximate semantic equivalence. Across\ndatasets, execution or simulated execution significantly outperforms the\nmethods that do not involve program semantics. We find that MBR-EXEC\nconsistently improves over all execution-unaware selection methods, suggesting\nit as an effective approach for natural language to code translation. We\nopen-source our code at github.com/facebookresearch/mbr-exec and data at\ndl.fbaipublicfiles.com/mbr-exec/mbr-exec-release.zip\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1\">Freda Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghazvininejad_M/0/1/0/all/0/1\">Marjan Ghazvininejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sida I. Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion. (arXiv:2205.02357v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02357","description":"<p>Multimodal Knowledge Graphs (MKGs), which organize visual-text factual\nknowledge, have recently been successfully applied to tasks such as information\nretrieval, question answering, and recommendation system. Since most MKGs are\nfar from complete, extensive knowledge graph completion studies have been\nproposed focusing on the multimodal entity, relation extraction and link\nprediction. However, different tasks and modalities require changes to the\nmodel architecture, and not all images/objects are relevant to text input,\nwhich hinders the applicability to diverse real-world scenarios. In this paper,\nwe propose a hybrid transformer with multi-level fusion to address those\nissues. Specifically, we leverage a hybrid transformer architecture with\nunified input-output for diverse multimodal knowledge graph completion tasks.\nMoreover, we propose multi-level fusion, which integrates visual and text\nrepresentation via coarse-grained prefix-guided interaction and fine-grained\ncorrelation-aware fusion modules. We conduct extensive experiments to validate\nthat our MKGformer can obtain SOTA performance on four datasets of multimodal\nlink prediction, multimodal RE, and multimodal NER. Code is available in\nhttps://github.com/zjunlp/MKGformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Literal and Implied Subquestions to Fact-check Complex Claims. (arXiv:2205.06938v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.06938","description":"<p>Verifying complex political claims is a challenging task, especially when\npoliticians use various tactics to subtly misrepresent the facts. Automatic\nfact-checking systems fall short here, and their predictions like \"half-true\"\nare not very useful in isolation, since we have no idea which parts of the\nclaim are true and which are not. In this work, we focus on decomposing a\ncomplex claim into a comprehensive set of yes-no subquestions whose answers\ninfluence the veracity of the claim. We present ClaimDecomp, a dataset of\ndecompositions for over 1000 claims. Given a claim and its verification\nparagraph written by fact-checkers, our trained annotators write subquestions\ncovering both explicit propositions of the original claim and its implicit\nfacets, such as asking about additional political context that changes our view\nof the claim's veracity. We study whether state-of-the-art models can generate\nsuch subquestions, showing that these models generate reasonable questions to\nask, but predicting the comprehensive set of subquestions from the original\nclaim without evidence remains challenging. We further show that these\nsubquestions can help identify relevant evidence to fact-check the full claim\nand derive the veracity through their answers, suggesting that they can be\nuseful pieces of a fact-checking pipeline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sriram_A/0/1/0/all/0/1\">Aniruddh Sriram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context Limitations Make Neural Language Models More Human-Like. (arXiv:2205.11463v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11463","description":"<p>Language models (LMs) have been used in cognitive modeling as well as\nengineering studies -- they compute information-theoretic complexity metrics\nthat simulate humans' cognitive load during reading. This study highlights a\nlimitation of modern neural LMs as the model of choice for this purpose: there\nis a discrepancy between their context access capacities and that of humans.\nOur results showed that constraining the LMs' context access improved their\nsimulation of human reading behavior. We also showed that LM-human gaps in\ncontext access were associated with specific syntactic constructions;\nincorporating syntactic biases into LMs' context access might enhance their\ncognitive plausibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuribayashi_T/0/1/0/all/0/1\">Tatsuki Kuribayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oseki_Y/0/1/0/all/0/1\">Yohei Oseki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brassard_A/0/1/0/all/0/1\">Ana Brassard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NaturalProver: Grounded Mathematical Proof Generation with Language Models. (arXiv:2205.12910v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12910","description":"<p>Theorem proving in natural mathematical language - the mixture of symbolic\nand natural language used by humans - plays a central role in mathematical\nadvances and education, and tests aspects of reasoning that are core to\nintelligence. Yet it has remained underexplored with modern generative models.\nWe study large-scale language models on two new generation tasks: suggesting\nthe next step in a mathematical proof, and full proof generation. We develop\nNaturalProver, a language model that generates proofs by conditioning on\nbackground references (e.g. theorems and definitions that are either retrieved\nor human-provided), and optionally enforces their presence with constrained\ndecoding. On theorems from the NaturalProofs benchmark, NaturalProver improves\nthe quality of next-step suggestions and generated proofs over fine-tuned\nGPT-3, according to human evaluations from university-level mathematics\nstudents. NaturalProver is capable of proving some theorems that require short\n(2-6 step) proofs, and providing next-step suggestions that are rated as\ncorrect and useful over 40% of the time, which is to our knowledge the first\ndemonstration of these capabilities using neural language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiacheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks. (arXiv:2206.08514v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.08514","description":"<p>Textual backdoor attacks are a kind of practical threat to NLP systems. By\ninjecting a backdoor in the training phase, the adversary could control model\npredictions via predefined triggers. As various attack and defense models have\nbeen proposed, it is of great significance to perform rigorous evaluations.\nHowever, we highlight two issues in previous backdoor learning evaluations: (1)\nThe differences between real-world scenarios (e.g. releasing poisoned datasets\nor models) are neglected, and we argue that each scenario has its own\nconstraints and concerns, thus requires specific evaluation protocols; (2) The\nevaluation metrics only consider whether the attacks could flip the models'\npredictions on poisoned samples and retain performances on benign samples, but\nignore that poisoned samples should also be stealthy and semantic-preserving.\nTo address these issues, we categorize existing works into three practical\nscenarios in which attackers release datasets, pre-trained models, and\nfine-tuned models respectively, then discuss their unique evaluation\nmethodologies. On metrics, to completely evaluate poisoned samples, we use\ngrammar error increase and perplexity difference for stealthiness, along with\ntext similarity for validity. After formalizing the frameworks, we develop an\nopen-source toolkit OpenBackdoor to foster the implementations and evaluations\nof textual backdoor learning. With this toolkit, we perform extensive\nexperiments to benchmark attack and defense models under the suggested\nparadigm. To facilitate the underexplored defenses against poisoned datasets,\nwe further propose CUBE, a simple yet strong clustering-based defense baseline.\nWe hope that our frameworks and benchmarks could serve as the cornerstones for\nfuture model development and evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1\">Ganqu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lifan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bingxiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models. (arXiv:2206.09557v2 [cs.DC] UPDATED)","link":"http://arxiv.org/abs/2206.09557","description":"<p>The recent advance of self-supervised learning associated with the\nTransformer architecture enables natural language processing (NLP) to exhibit\nextremely low perplexity. Such powerful models demand ever-increasing model\nsize and, thus, large amounts of computations and memory footprints. In this\npaper, we propose an efficient inference framework for large-scale generative\nlanguage models. As the key to reducing model size, we quantize weights by a\nnon-uniform quantization method. Then, quantized matrix multiplications are\naccelerated by our proposed kernel, called nuQmm, which allows a wide trade-off\nbetween compression ratio and accuracy. Our proposed nuQmm reduces the latency\nof not only each GPU but also the entire inference of large LMs because a high\ncompression ratio (by low-bit quantization) mitigates the minimum required\nnumber of GPUs. Assuming 2-bit quantization, we demonstrate that nuQmm can\nreduce latency to generate each token for OPT-175B (that requires 8 GPUs\nwithout nuQmm) by 47.3% using 8 GPUs or by 23.2% using only 2 GPUs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1\">Gunho Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_B/0/1/0/all/0/1\">Baeseong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungjae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsub Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byeongwook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1\">Se Jung Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Youngjoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongsoo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VeriDark: A Large-Scale Benchmark for Authorship Verification on the Dark Web. (arXiv:2207.03477v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.03477","description":"<p>The DarkWeb represents a hotbed for illicit activity, where users communicate\non different market forums in order to exchange goods and services. Law\nenforcement agencies benefit from forensic tools that perform authorship\nanalysis, in order to identify and profile users based on their textual\ncontent. However, authorship analysis has been traditionally studied using\ncorpora featuring literary texts such as fragments from novels or fan fiction,\nwhich may not be suitable in a cybercrime context. Moreover, the few works that\nemploy authorship analysis tools for cybercrime prevention usually employ\nad-hoc experimental setups and datasets. To address these issues, we release\nVeriDark: a benchmark comprised of three large scale authorship verification\ndatasets and one authorship identification dataset obtained from user activity\nfrom either Dark Web related Reddit communities or popular illicit Dark Web\nmarket forums. We evaluate competitive NLP baselines on the three datasets and\nperform an analysis of the predictions to better understand the limitations of\nsuch approaches. We make the datasets and baselines publicly available at\nhttps://github.com/bit-ml/VeriDark\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manolache_A/0/1/0/all/0/1\">Andrei Manolache</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brad_F/0/1/0/all/0/1\">Florin Brad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbalau_A/0/1/0/all/0/1\">Antonio Barbalau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_M/0/1/0/all/0/1\">Marius Popescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Large Language Models know what humans know?. (arXiv:2209.01515v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.01515","description":"<p>Humans can attribute mental states to others, a capacity known as Theory of\nMind. However, it is unknown to what extent this ability results from an innate\nbiological endowment or from experience accrued through child development,\nparticularly exposure to language describing others' mental states. We test the\nviability of the language exposure hypothesis by assessing whether models\nexposed to large quantities of human language develop evidence of Theory of\nMind. In pre-registered analyses, we present a linguistic version of the False\nBelief Task, widely used to assess Theory of Mind, to both human participants\nand a state-of-the-art Large Language Model, GPT-3. Both are sensitive to\nothers' beliefs, but while the language model significantly exceeds chance\nbehavior, it does not perform as well as the humans, nor does it explain the\nfull extent of their behavior -- despite being exposed to more language than a\nhuman would in a lifetime. This suggests that while statistical learning from\nlanguage exposure may in part explain how humans develop Theory of Mind, other\nmechanisms are also responsible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trott_S/0/1/0/all/0/1\">Sean Trott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_C/0/1/0/all/0/1\">Cameron Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tyler Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michaelov_J/0/1/0/all/0/1\">James Michaelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1\">Benjamin Bergen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Honest Students from Untrusted Teachers: Learning an Interpretable Question-Answering Pipeline from a Pretrained Language Model. (arXiv:2210.02498v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.02498","description":"<p>Explainable question answering systems should produce not only accurate\nanswers but also rationales that justify their reasoning and allow humans to\ncheck their work. But what sorts of rationales are useful and how can we train\nsystems to produce them? We propose a new style of rationale for open-book\nquestion answering, called \\emph{markup-and-mask}, which combines aspects of\nextractive and free-text explanations. In the markup phase, the passage is\naugmented with free-text markup that enables each sentence to stand on its own\noutside the discourse context. In the masking phase, a sub-span of the\nmarked-up passage is selected. To train a system to produce markup-and-mask\nrationales without annotations, we leverage in-context learning. Specifically,\nwe generate silver annotated data by sending a series of prompts to a frozen\npretrained language model, which acts as a teacher. We then fine-tune a smaller\nstudent model by training on the subset of rationales that led to correct\nanswers. The student is \"honest\" in the sense that it is a pipeline: the\nrationale acts as a bottleneck between the passage and the answer, while the\n\"untrusted\" teacher operates under no such constraints. Thus, we offer a new\nway to build trustworthy pipeline systems from a combination of end-task\nannotations and frozen pretrained language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andor_D/0/1/0/all/0/1\">Daniel Andor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohnet_B/0/1/0/all/0/1\">Bernd Bohnet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1\">Michael Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mimno_D/0/1/0/all/0/1\">David Mimno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation. (arXiv:2210.04873v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04873","description":"<p>Counterfactual data augmentation (CDA) -- i.e., adding minimally perturbed\ninputs during training -- helps reduce model reliance on spurious correlations\nand improves generalization to out-of-distribution (OOD) data. Prior work on\ngenerating counterfactuals only considered restricted classes of perturbations,\nlimiting their effectiveness. We present COunterfactual Generation via\nRetrieval and Editing (CORE), a retrieval-augmented generation framework for\ncreating diverse counterfactual perturbations for CDA. For each training\nexample, CORE first performs a dense retrieval over a task-related unlabeled\ntext corpus using a learned bi-encoder and extracts relevant counterfactual\nexcerpts. CORE then incorporates these into prompts to a large language model\nwith few-shot learning capabilities, for counterfactual editing. Conditioning\nlanguage model edits on naturally occurring data results in diverse\nperturbations. Experiments on natural language inference and sentiment analysis\nbenchmarks show that CORE counterfactuals are more effective at improving\ngeneralization to OOD data compared to other DA approaches. We also show that\nthe CORE retrieval framework can be used to encourage diversity in manually\nauthored perturbations\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dixit_T/0/1/0/all/0/1\">Tanay Dixit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paranjape_B/0/1/0/all/0/1\">Bhargavi Paranjape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple and Strong Baseline for End-to-End Neural RST-style Discourse Parsing. (arXiv:2210.08355v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08355","description":"<p>To promote and further develop RST-style discourse parsing models, we need a\nstrong baseline that can be regarded as a reference for reporting reliable\nexperimental results. This paper explores a strong baseline by integrating\nexisting simple parsing strategies, top-down and bottom-up, with various\ntransformer-based pre-trained language models. The experimental results\nobtained from two benchmark datasets demonstrate that the parsing performance\nstrongly relies on the pretrained language models rather than the parsing\nstrategies. In particular, the bottom-up parser achieves large performance\ngains compared to the current best parser when employing DeBERTa. We further\nreveal that language models with a span-masking scheme especially boost the\nparsing performance through our analysis within intra- and multi-sentential\nparsing, and nuclearity prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_N/0/1/0/all/0/1\">Naoki Kobayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirao_T/0/1/0/all/0/1\">Tsutomu Hirao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamigaito_H/0/1/0/all/0/1\">Hidetaka Kamigaito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okumura_M/0/1/0/all/0/1\">Manabu Okumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagata_M/0/1/0/all/0/1\">Masaaki Nagata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MCP: Self-supervised Pre-training for Personalized Chatbots with Multi-level Contrastive Sampling. (arXiv:2210.08753v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08753","description":"<p>Personalized chatbots focus on endowing the chatbots with a consistent\npersonality to behave like real users and further act as personal assistants.\nPrevious studies have explored generating implicit user profiles from the\nuser's dialogue history for building personalized chatbots. However, these\nstudies only use the response generation loss to train the entire model, thus\nit is prone to suffer from the problem of data sparsity. Besides, they\noveremphasize the final generated response's quality while ignoring the\ncorrelations and fusions between the user's dialogue history, leading to rough\ndata representations and performance degradation. To tackle these problems, we\npropose a self-supervised learning framework MCP for capturing better\nrepresentations from users' dialogue history for personalized chatbots.\nSpecifically, we apply contrastive sampling methods to leverage the supervised\nsignals hidden in user dialog history, and generate the pre-training samples\nfor enhancing the model. We design three pre-training tasks based on three\ntypes of contrastive pairs from user dialogue history, namely response pairs,\nsequence augmentation pairs, and user pairs. We pre-train the utterance encoder\nand the history encoder towards the contrastive objectives and use these\npre-trained encoders for generating user profiles while personalized response\ngeneration. Experimental results on two real-world datasets show a significant\nimprovement in our proposed model MCP compared with the existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhaoheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yutao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhengyi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoSE: Modality Split and Ensemble for Multimodal Knowledge Graph Completion. (arXiv:2210.08821v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08821","description":"<p>Multimodal knowledge graph completion (MKGC) aims to predict missing entities\nin MKGs. Previous works usually share relation representation across\nmodalities. This results in mutual interference between modalities during\ntraining, since for a pair of entities, the relation from one modality probably\ncontradicts that from another modality. Furthermore, making a unified\nprediction based on the shared relation representation treats the input in\ndifferent modalities equally, while their importance to the MKGC task should be\ndifferent. In this paper, we propose MoSE, a Modality Split representation\nlearning and Ensemble inference framework for MKGC. Specifically, in the\ntraining phase, we learn modality-split relation embeddings for each modality\ninstead of a single modality-shared one, which alleviates the modality\ninterference. Based on these embeddings, in the inference phase, we first make\nmodality-split predictions and then exploit various ensemble methods to combine\nthe predictions with different weights, which models the modality importance\ndynamically. Experimental results on three KG datasets show that MoSE\noutperforms state-of-the-art MKGC methods. Codes are available at\nhttps://github.com/OreOZhao/MoSE4MKGC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xiangrui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yike Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoqing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Ning Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information-Transport-based Policy for Simultaneous Translation. (arXiv:2210.12357v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12357","description":"<p>Simultaneous translation (ST) outputs translation while receiving the source\ninputs, and hence requires a policy to determine whether to translate a target\ntoken or wait for the next source token. The major challenge of ST is that each\ntarget token can only be translated based on the current received source\ntokens, where the received source information will directly affect the\ntranslation quality. So naturally, how much source information is received for\nthe translation of the current target token is supposed to be the pivotal\nevidence for the ST policy to decide between translating and waiting. In this\npaper, we treat the translation as information transport from source to target\nand accordingly propose an Information-Transport-based Simultaneous Translation\n(ITST). ITST quantifies the transported information weight from each source\ntoken to the current target token, and then decides whether to translate the\ntarget token according to its accumulated received information. Experiments on\nboth text-to-text ST and speech-to-text ST (a.k.a., streaming speech\ntranslation) tasks show that ITST outperforms strong baselines and achieves\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unifying Data Perspectivism and Personalization: An Application to Social Norms. (arXiv:2210.14531v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.14531","description":"<p>Instead of using a single ground truth for language processing tasks, several\nrecent studies have examined how to represent and predict the labels of the set\nof annotators. However, often little or no information about annotators is\nknown, or the set of annotators is small. In this work, we examine a corpus of\nsocial media posts about conflict from a set of 13k annotators and 210k\njudgements of social norms. We provide a novel experimental setup that applies\npersonalization methods to the modeling of annotators and compare their\neffectiveness for predicting the perception of social norms. We further provide\nan analysis of performance across subsets of social situations that vary by the\ncloseness of the relationship between parties in conflict, and assess where\npersonalization helps the most.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plepi_J/0/1/0/all/0/1\">Joan Plepi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neuendorf_B/0/1/0/all/0/1\">B&#xe9;la Neuendorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flek_L/0/1/0/all/0/1\">Lucie Flek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welch_C/0/1/0/all/0/1\">Charles Welch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can language models handle recursively nested grammatical structures? A case study on comparing models and humans. (arXiv:2210.15303v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15303","description":"<p>How should we compare the capabilities of language models and humans? Here, I\nconsider a case study: processing of recursively nested grammatical structures.\nPrior work has suggested that language models cannot handle these structures as\nreliably as humans can. However, the humans were provided with instructions and\ntraining before being evaluated, while the language models were evaluated\nzero-shot. I therefore attempt to more closely match the evaluation paradigms\nby providing language models with few-shot prompts. A simple prompt, which\ncontains substantially less content than the human training, allows large\nlanguage models to consistently outperform the human results. The same prompt\neven allows extrapolation to more deeply nested conditions than have been\ntested in humans. Further, a reanalysis of the prior human experiments suggests\nthat the humans may not perform above chance at the difficult structures\ninitially. These results suggest that large language models can in fact process\nrecursively nested grammatical structures comparably to humans. This case study\nhighlights how discrepancies in the quantity of experiment-specific context can\nconfound comparisons of language models and humans. I use this case study to\nreflect on the broader challenge of comparing human and model capabilities, and\nto suggest that there is an important difference between evaluating cognitive\nmodels of a specific phenomenon and evaluating broadly-trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1\">Andrew Kyle Lampinen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforced Question Rewriting for Conversational Question Answering. (arXiv:2210.15777v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15777","description":"<p>Conversational Question Answering (CQA) aims to answer questions contained\nwithin dialogues, which are not easily interpretable without context.\nDeveloping a model to rewrite conversational questions into self-contained ones\nis an emerging solution in industry settings as it allows using existing\nsingle-turn QA systems to avoid training a CQA model from scratch. Previous\nwork trains rewriting models using human rewrites as supervision. However, such\nobjectives are disconnected with QA models and therefore more human-like\nrewrites do not guarantee better QA performance. In this paper we propose using\nQA feedback to supervise the rewriting model with reinforcement learning.\nExperiments show that our approach can effectively improve QA performance over\nbaselines for both extractive and retrieval QA. Furthermore, human evaluation\nshows that our method can generate more accurate and detailed rewrites when\ncompared to human annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_A/0/1/0/all/0/1\">Anjie Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fetahu_B/0/1/0/all/0/1\">Besnik Fetahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rokhlenko_O/0/1/0/all/0/1\">Oleg Rokhlenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malmasi_S/0/1/0/all/0/1\">Shervin Malmasi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SDCL: Self-Distillation Contrastive Learning for Chinese Spell Checking. (arXiv:2210.17168v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.17168","description":"<p>Due to the ambiguity of homophones, Chinese Spell Checking (CSC) has\nwidespread applications. Existing systems typically utilize BERT for text\nencoding. However, CSC requires the model to account for both phonetic and\ngraphemic information. To adapt BERT to the CSC task, we propose a token-level\nself-distillation contrastive learning method. We employ BERT to encode both\nthe corrupted and corresponding correct sentence. Then, we use contrastive\nlearning loss to regularize corrupted tokens' hidden states to be closer to\ncounterparts in the correct sentence. On three CSC datasets, we confirmed our\nmethod provides a significant improvement above baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaotian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction. (arXiv:1812.11321v1 [cs.IR] CROSS LISTED)","link":"http://arxiv.org/abs/1812.11321","description":"<p>A capsule is a group of neurons, whose activity vector represents the\ninstantiation parameters of a specific type of entity. In this paper, we\nexplore the capsule networks used for relation extraction in a multi-instance\nmulti-label learning framework and propose a novel neural approach based on\ncapsule networks with attention mechanisms. We evaluate our method with\ndifferent benchmarks, and it is demonstrated that our method improves the\nprecision of the predicted relations. Particularly, we show that capsule\nnetworks improve multiple entity pairs relation extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhanlin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-tail Relation Extraction via Knowledge Graph Embeddings and Graph Convolution Networks. (arXiv:1903.01306v1 [cs.IR] CROSS LISTED)","link":"http://arxiv.org/abs/1903.01306","description":"<p>We propose a distance supervised relation extraction approach for\nlong-tailed, imbalanced data which is prevalent in real-world settings. Here,\nthe challenge is to learn accurate \"few-shot\" models for classes existing at\nthe tail of the class distribution, for which little data is available.\nInspired by the rich semantic correlations between classes at the long tail and\nthose at the head, we take advantage of the knowledge from data-rich classes at\nthe head of the distribution to boost the performance of the data-poor classes\nat the tail. First, we propose to leverage implicit relational knowledge among\nclass labels from knowledge graph embeddings and learn explicit relational\nknowledge using graph convolution networks. Second, we integrate that\nrelational knowledge into relation extraction model by coarse-to-fine\nknowledge-aware attention mechanism. We demonstrate our results for a\nlarge-scale benchmark dataset which show that our approach significantly\noutperforms other baselines, especially for long-tail relations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhanlin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v6 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2104.07650","description":"<p>Recently, prompt-tuning has achieved promising results for specific few-shot\nclassification tasks. The core idea of prompt-tuning is to insert text pieces\n(i.e., templates) into the input and transform a classification task into a\nmasked language modeling problem. However, for relation extraction, determining\nan appropriate prompt template requires domain expertise, and it is cumbersome\nand time-consuming to obtain a suitable label word. Furthermore, there exists\nabundant semantic and prior knowledge among the relation labels that cannot be\nignored. To this end, we focus on incorporating knowledge among relation labels\ninto prompt-tuning for relation extraction and propose a Knowledge-aware\nPrompt-tuning approach with synergistic optimization (KnowPrompt).\nSpecifically, we inject latent knowledge contained in relation labels into\nprompt construction with learnable virtual type words and answer words. Then,\nwe synergistically optimize their representation with structured constraints.\nExtensive experimental results on five datasets with standard and low-resource\nsettings demonstrate the effectiveness of our approach. Our code and datasets\nare available in https://github.com/zjunlp/KnowPrompt for reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MLBiNet: A Cross-Sentence Collective Event Detection Network. (arXiv:2105.09458v3 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2105.09458","description":"<p>We consider the problem of collectively detecting multiple events,\nparticularly in cross-sentence settings. The key to dealing with the problem is\nto encode semantic information and model event inter-dependency at a\ndocument-level. In this paper, we reformulate it as a Seq2Seq task and propose\na Multi-Layer Bidirectional Network (MLBiNet) to capture the document-level\nassociation of events and semantic information simultaneously. Specifically, a\nbidirectional decoder is firstly devised to model event inter-dependency within\na sentence when decoding the event tag vector sequence. Secondly, an\ninformation aggregation module is employed to aggregate sentence-level semantic\nand event tag information. Finally, we stack multiple bidirectional decoders\nand feed cross-sentence information, forming a multi-layer bidirectional\ntagging architecture to iteratively propagate information across sentences. We\nshow that our approach provides significant improvement in performance compared\nto the current state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_D/0/1/0/all/0/1\">Dongfang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1\">Zhilin Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AliCG: Fine-grained and Evolvable Conceptual Graph Construction for Semantic Search at Alibaba. (arXiv:2106.01686v2 [cs.AI] CROSS LISTED)","link":"http://arxiv.org/abs/2106.01686","description":"<p>Conceptual graphs, which is a particular type of Knowledge Graphs, play an\nessential role in semantic search. Prior conceptual graph construction\napproaches typically extract high-frequent, coarse-grained, and time-invariant\nconcepts from formal texts. In real applications, however, it is necessary to\nextract less-frequent, fine-grained, and time-varying conceptual knowledge and\nbuild taxonomy in an evolving manner. In this paper, we introduce an approach\nto implementing and deploying the conceptual graph at Alibaba. Specifically, We\npropose a framework called AliCG which is capable of a) extracting fine-grained\nconcepts by a novel bootstrapping with alignment consensus approach, b) mining\nlong-tail concepts with a novel low-resource phrase mining approach, c)\nupdating the graph dynamically via a concept distribution estimation method\nbased on implicit and explicit user behaviors. We have deployed the framework\nat Alibaba UC Browser. Extensive offline evaluation as well as online A/B\ntesting demonstrate the efficacy of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Q/0/1/0/all/0/1\">Qianghuai Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tou_H/0/1/0/all/0/1\">Huaixiao Tou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_N/0/1/0/all/0/1\">Nengwei Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark. (arXiv:2106.08087v6 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2106.08087","description":"<p>Artificial Intelligence (AI), along with the recent progress in biomedical\nlanguage understanding, is gradually changing medical practice. With the\ndevelopment of biomedical language understanding benchmarks, AI applications\nare widely used in the medical field. However, most benchmarks are limited to\nEnglish, which makes it challenging to replicate many of the successes in\nEnglish for other languages. To facilitate research in this direction, we\ncollect real-world biomedical data and present the first Chinese Biomedical\nLanguage Understanding Evaluation (CBLUE) benchmark: a collection of natural\nlanguage understanding tasks including named entity recognition, information\nextraction, clinical diagnosis normalization, single-sentence/sentence-pair\nclassification, and an associated online platform for model evaluation,\ncomparison, and analysis. To establish evaluation on these tasks, we report\nempirical results with the current 11 pre-trained Chinese models, and\nexperimental results show that state-of-the-art neural models perform by far\nworse than the human ceiling. Our benchmark is released at\n\\url{https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414&amp;lang=en-us}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_X/0/1/0/all/0/1\">Xin Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kangping Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1\">Yuan Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guotong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_H/0/1/0/all/0/1\">Hui Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zan_H/0/1/0/all/0/1\">Hongying Zan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kunli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Buzhou Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LightNER: A Lightweight Tuning Paradigm for Low-resource NER via Pluggable Prompting. (arXiv:2109.00720v5 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2109.00720","description":"<p>Most NER methods rely on extensive labeled data for model training, which\nstruggles in the low-resource scenarios with limited training data. Existing\ndominant approaches usually suffer from the challenge that the target domain\nhas different label sets compared with a resource-rich source domain, which can\nbe concluded as class transfer and domain transfer. In this paper, we propose a\nlightweight tuning paradigm for low-resource NER via pluggable prompting\n(LightNER). Specifically, we construct the unified learnable verbalizer of\nentity categories to generate the entity span sequence and entity categories\nwithout any label-specific classifiers, thus addressing the class transfer\nissue. We further propose a pluggable guidance module by incorporating\nlearnable parameters into the self-attention layer as guidance, which can\nre-modulate the attention and adapt pre-trained weights. Note that we only tune\nthose inserted module with the whole parameter of the pre-trained language\nmodel fixed, thus, making our approach lightweight and flexible for\nlow-resource scenarios and can better transfer knowledge across domains.\nExperimental results show that LightNER can obtain comparable performance in\nthe standard supervised setting and outperform strong baselines in low-resource\nsettings. Code is in\nhttps://github.com/zjunlp/DeepKE/tree/main/example/ner/few-shot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population. (arXiv:2201.03335v5 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2201.03335","description":"<p>We present an open-source and extensible knowledge extraction toolkit DeepKE,\nsupporting complicated low-resource, document-level and multimodal scenarios in\nthe knowledge base population. DeepKE implements various information extraction\ntasks, including named entity recognition, relation extraction and attribute\nextraction. With a unified framework, DeepKE allows developers and researchers\nto customize datasets and models to extract information from unstructured data\naccording to their requirements. Specifically, DeepKE not only provides various\nfunctional modules and model implementation for different tasks and scenarios\nbut also organizes all components by consistent frameworks to maintain\nsufficient modularity and extensibility. We release the source code at GitHub\nin https://github.com/zjunlp/DeepKE with Google Colab tutorials and\ncomprehensive documents for beginners. Besides, we present an online system in\n<a href=\"http://deepke.openkg.cn/EN/re_doc_show.html\">this http URL</a> for real-time extraction of various\ntasks, and a demo video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_L/0/1/0/all/0/1\">Liankuan Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1\">Shuofei Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guozhou Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OntoProtein: Protein Pretraining With Gene Ontology Embedding. (arXiv:2201.11147v6 [q-bio.BM] CROSS LISTED)","link":"http://arxiv.org/abs/2201.11147","description":"<p>Self-supervised protein language models have proved their effectiveness in\nlearning the proteins representations. With the increasing computational power,\ncurrent protein language models pre-trained with millions of diverse sequences\ncan advance the parameter scale from million-level to billion-level and achieve\nremarkable improvement. However, those prevailing approaches rarely consider\nincorporating knowledge graphs (KGs), which can provide rich structured\nknowledge facts for better protein representations. We argue that informative\nbiology knowledge in KGs can enhance protein representation with external\nknowledge. In this work, we propose OntoProtein, the first general framework\nthat makes use of structure in GO (Gene Ontology) into protein pre-training\nmodels. We construct a novel large-scale knowledge graph that consists of GO\nand its related proteins, and gene annotation texts or protein sequences\ndescribe all nodes in the graph. We propose novel contrastive learning with\nknowledge-aware negative sampling to jointly optimize the knowledge graph and\nprotein embedding during pre-training. Experimental results show that\nOntoProtein can surpass state-of-the-art methods with pre-trained protein\nlanguage models in TAPE benchmark and yield better performance compared with\nbaselines in protein-protein interaction and protein function prediction. Code\nand datasets are available in https://github.com/zjunlp/OntoProtein.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hong_H/0/1/0/all/0/1\">Haosen Hong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lian_J/0/1/0/all/0/1\">Jiazhang Lian</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Demonstration Tuning for Pre-trained Language Models. (arXiv:2204.04392v3 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2204.04392","description":"<p>Pretrained language models can be effectively stimulated by textual prompts\nor demonstrations, especially in low-data scenarios. Recent works have focused\non automatically searching discrete or continuous prompts or optimized\nverbalizers, yet studies for the demonstration are still limited. Concretely,\nthe demonstration examples are crucial for an excellent final performance of\nprompt-tuning. In this paper, we propose a novel pluggable, extensible, and\nefficient approach named contrastive demonstration tuning, which is free of\ndemonstration sampling. Furthermore, the proposed approach can be: (i) Plugged\ninto any previous prompt-tuning approaches; (ii) Extended to widespread\nclassification tasks with a large number of categories. Experimental results on\n16 datasets illustrate that our method integrated with previous approaches\nLM-BFF and P-tuning can yield better performance. Code is available in\nhttps://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation Extraction as Open-book Examination: Retrieval-enhanced Prompt Tuning. (arXiv:2205.02355v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2205.02355","description":"<p>Pre-trained language models have contributed significantly to relation\nextraction by demonstrating remarkable few-shot learning abilities. However,\nprompt tuning methods for relation extraction may still fail to generalize to\nthose rare or hard patterns. Note that the previous parametric learning\nparadigm can be viewed as memorization regarding training data as a book and\ninference as the close-book test. Those long-tailed or hard patterns can hardly\nbe memorized in parameters given few-shot instances. To this end, we regard RE\nas an open-book examination and propose a new semiparametric paradigm of\nretrieval-enhanced prompt tuning for relation extraction. We construct an\nopen-book datastore for retrieval regarding prompt-based instance\nrepresentations and corresponding relation labels as memorized key-value pairs.\nDuring inference, the model can infer relations by linearly interpolating the\nbase output of PLM with the non-parametric nearest neighbor distribution over\nthe datastore. In this way, our model not only infers relation through\nknowledge stored in the weights during training but also assists\ndecision-making by unwinding and querying examples in the open-book datastore.\nExtensive experiments on benchmark datasets show that our method can achieve\nstate-of-the-art in both standard supervised and few-shot settings. Code are\navailable in https://github.com/zjunlp/PromptKG/tree/main/research/RetrievalRE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. (arXiv:2205.14704v3 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2205.14704","description":"<p>Prompt learning approaches have made waves in natural language processing by\ninducing better few-shot performance while they still follow a parametric-based\nlearning paradigm; the oblivion and rote memorization problems in learning may\nencounter unstable generalization issues. Specifically, vanilla prompt learning\nmay struggle to utilize atypical instances by rote during fully-supervised\ntraining or overfit shallow patterns with low-shot data. To alleviate such\nlimitations, we develop RetroPrompt with the motivation of decoupling knowledge\nfrom memorization to help the model strike a balance between generalization and\nmemorization. In contrast with vanilla prompt learning, RetroPrompt constructs\nan open-book knowledge-store from training instances and implements a retrieval\nmechanism during the process of input, training and inference, thus equipping\nthe model with the ability to retrieve related contexts from the training\ncorpus as cues for enhancement. Extensive experiments demonstrate that\nRetroPrompt can obtain better performance in both few-shot and zero-shot\nsettings. Besides, we further illustrate that our proposed RetroPrompt can\nyield better generalization abilities with new datasets. Detailed analysis of\nmemorization indeed reveals RetroPrompt can reduce the reliance of language\nmodels on memorization; thus, improving generalization for downstream tasks.\nCode is available in\nhttps://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Protein Knowledge Graph Construction and Applications. (arXiv:2207.10080v2 [q-bio.QM] CROSS LISTED)","link":"http://arxiv.org/abs/2207.10080","description":"<p>Existing data-centric methods for protein science generally cannot\nsufficiently capture and leverage biology knowledge, which may be crucial for\nmany protein tasks. To facilitate research in this field, we create\nProteinKG65, a knowledge graph for protein science. Using gene ontology and\nUniprot knowledge base as a basis, we transform and integrate various kinds of\nknowledge with aligned descriptions and protein sequences, respectively, to GO\nterms and protein entities. ProteinKG65 is mainly dedicated to providing a\nspecialized protein knowledge graph, bringing the knowledge of Gene Ontology to\nprotein function and structure prediction. The current version contains about\n614,099 entities, 5,620,437 triples (including 5,510,437 protein-go triplets\nand 110,000 GO-GO triplets). We also illustrate the potential applications of\nProteinKG65 with a prototype. Our dataset can be downloaded at\nhttps://w3id.org/proteinkg65.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptKG: A Prompt Learning Framework for Knowledge Graph Representation Learning and Application. (arXiv:2210.00305v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2210.00305","description":"<p>Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph\nstructure and text-rich entity/relation information. KG representation models\nshould consider graph structures and text semantics, but no comprehensive\nopen-sourced framework is mainly designed for KG regarding informative text\ndescription. In this paper, we present PromptKG, a prompt learning framework\nfor KG representation learning and application that equips the cutting-edge\ntext-based methods, integrates a new prompt learning model and supports various\ntasks (e.g., knowledge graph completion, question answering, recommendation,\nand knowledge probing). PromptKG is publicly open-sourced at\nhttps://github.com/zjunlp/PromptKG with long-term technical support.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study. (arXiv:2210.10678v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2210.10678","description":"<p>This paper presents an empirical study to build relation extraction systems\nin low-resource settings. Based upon recent pre-trained language models, we\ncomprehensively investigate three schemes to evaluate the performance in\nlow-resource settings: (i) different types of prompt-based methods with\nfew-shot labeled data; (ii) diverse balancing methods to address the\nlong-tailed distribution issue; (iii) data augmentation technologies and\nself-training to generate more labeled in-domain data. We create a benchmark\nwith 8 relation extraction (RE) datasets covering different languages, domains\nand contexts and perform extensive comparisons over the proposed schemes with\ncombinations. Our experiments illustrate: (i) Though prompt-based tuning is\nbeneficial in low-resource RE, there is still much potential for improvement,\nespecially in extracting relations from cross-sentence contexts with multiple\nrelational triples; (ii) Balancing methods are not always helpful for RE with\nlong-tailed distribution; (iii) Data augmentation complements existing\nbaselines and can bring much performance gain, while self-training may not\nconsistently achieve advancement to low-resource RE. Code and datasets are in\nhttps://github.com/zjunlp/LREBench.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-11-01T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}