{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-06-08T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Turning large language models into cognitive models. (arXiv:2306.03917v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03917","description":"<p>Large language models are powerful systems that excel at many tasks, ranging\nfrom translation to mathematical reasoning. Yet, at the same time, these models\noften show unhuman-like characteristics. In the present paper, we address this\ngap and ask whether large language models can be turned into cognitive models.\nWe find that -- after finetuning them on data from psychological experiments --\nthese models offer accurate representations of human behavior, even\noutperforming traditional cognitive models in two decision-making domains. In\naddition, we show that their representations contain the information necessary\nto model behavior on the level of individual subjects. Finally, we demonstrate\nthat finetuning on multiple tasks enables large language models to predict\nhuman behavior in a previously unseen task. Taken together, these results\nsuggest that large, pre-trained models can be adapted to become generalist\ncognitive models, thereby opening up new research directions that could\ntransform cognitive psychology and the behavioral sciences as a whole.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Binz_M/0/1/0/all/0/1\">Marcel Binz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_E/0/1/0/all/0/1\">Eric Schulz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MISGENDERED: Limits of Large Language Models in Understanding Pronouns. (arXiv:2306.03950v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03950","description":"<p>Content Warning: This paper contains examples of misgendering and erasure\nthat could be offensive and potentially triggering.\n</p>\n<p>Gender bias in language technologies has been widely studied, but research\nhas mostly been restricted to a binary paradigm of gender. It is essential also\nto consider non-binary gender identities, as excluding them can cause further\nharm to an already marginalized group. In this paper, we comprehensively\nevaluate popular language models for their ability to correctly use English\ngender-neutral pronouns (e.g., singular they, them) and neo-pronouns (e.g., ze,\nxe, thon) that are used by individuals whose gender identity is not represented\nby binary pronouns. We introduce MISGENDERED, a framework for evaluating large\nlanguage models' ability to correctly use preferred pronouns, consisting of (i)\ninstances declaring an individual's pronoun, followed by a sentence with a\nmissing pronoun, and (ii) an experimental setup for evaluating masked and\nauto-regressive language models using a unified method. When prompted\nout-of-the-box, language models perform poorly at correctly predicting\nneo-pronouns (averaging 7.6% accuracy) and gender-neutral pronouns (averaging\n31.0% accuracy). This inability to generalize results from a lack of\nrepresentation of non-binary pronouns in training data and memorized\nassociations. Few-shot adaptation with explicit examples in the prompt improves\nthe performance but plateaus at only 45.4% for neo-pronouns. We release the\nfull dataset, code, and demo at\nhttps://tamannahossainkay.github.io/misgendered/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hossain_T/0/1/0/all/0/1\">Tamanna Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognition of Handwritten Japanese Characters Using Ensemble of Convolutional Neural Networks. (arXiv:2306.03954v1 [cs.CV])","link":"http://arxiv.org/abs/2306.03954","description":"<p>The Japanese writing system is complex, with three character types of\nHiragana, Katakana, and Kanji. Kanji consists of thousands of unique\ncharacters, further adding to the complexity of character identification and\nliterature understanding. Being able to translate handwritten Japanese\ncharacters into digital text is useful for data analysis, translation, learning\nand cultural preservation. In this study, a machine learning approach to\nanalyzing and recognizing handwritten Japanese characters (Kanji) is proposed.\nThe study used an ensemble of three convolutional neural networks (CNNs) for\nrecognizing handwritten Kanji characters and utilized four datasets of MNIST,\nK-MNIST, Kuzushiji-49 (K49) and the top 150 represented classes in the\nKuzushiji-Kanji (K-Kanji) dataset for its performance evaluation. The results\nindicate feasibility of using proposed CNN-ensemble architecture for\nrecognizing handwritten characters, achieving 99.4%, 96.4%, 95.0% and 96.4%\nclassification accuracy on MNIST, K-MNIS, K49, and K-Kanji datasets\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Solis_A/0/1/0/all/0/1\">Angel I. Solis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarkovacki_J/0/1/0/all/0/1\">Justin Zarkovacki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ly_J/0/1/0/all/0/1\">John Ly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atyabi_A/0/1/0/all/0/1\">Adham Atyabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Explicit Procedural Instructions for Data-Efficient Action Prediction. (arXiv:2306.03959v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03959","description":"<p>Task-oriented dialogues often require agents to enact complex, multi-step\nprocedures in order to meet user requests. While large language models have\nfound success automating these dialogues in constrained environments, their\nwidespread deployment is limited by the substantial quantities of task-specific\ndata required for training. The following paper presents a data-efficient\nsolution to constructing dialogue systems, leveraging explicit instructions\nderived from agent guidelines, such as company policies or customer service\nmanuals. Our proposed Knowledge-Augmented Dialogue System (KADS) combines a\nlarge language model with a knowledge retrieval module that pulls documents\noutlining relevant procedures from a predefined set of policies, given a\nuser-agent interaction. To train this system, we introduce a semi-supervised\npre-training scheme that employs dialogue-document matching and action-oriented\nmasked language modeling with partial parameter freezing. We evaluate the\neffectiveness of our approach on prominent task-oriented dialogue datasets,\nAction-Based Conversations Dataset and Schema-Guided Dialogue, for two dialogue\ntasks: action state tracking and workflow discovery. Our results demonstrate\nthat procedural knowledge augmentation improves accuracy predicting in- and\nout-of-distribution actions while preserving high performance in settings with\nlow or sparse data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1\">Julia White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghuvanshi_A/0/1/0/all/0/1\">Arushi Raghuvanshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruksachatkun_Y/0/1/0/all/0/1\">Yada Pruksachatkun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECQED: Emotion-Cause Quadruple Extraction in Dialogs. (arXiv:2306.03969v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03969","description":"<p>The existing emotion-cause pair extraction (ECPE) task, unfortunately,\nignores extracting the emotion type and cause type, while these fine-grained\nmeta-information can be practically useful in real-world applications, i.e.,\nchat robots and empathic dialog generation. Also the current ECPE is limited to\nthe scenario of single text piece, while neglecting the studies at dialog level\nthat should have more realistic values. In this paper, we extend the ECPE task\nwith a broader definition and scenario, presenting a new task, Emotion-Cause\nQuadruple Extraction in Dialogs (ECQED), which requires detecting emotion-cause\nutterance pairs and emotion and cause types. We present an ECQED model based on\na structural and semantic heterogeneous graph as well as a parallel grid\ntagging scheme, which advances in effectively incorporating the dialog context\nstructure, meanwhile solving the challenging overlapped quadruple issue. Via\nexperiments we show that introducing the fine-grained emotion and cause\nfeatures evidently helps better dialog generation. Also our proposed ECQED\nsystem shows exceptional superiority over baselines on both the emotion-cause\nquadruple or pair extraction tasks, meanwhile being highly efficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Li Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1\">Donghong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hao Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shengqiong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingye Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bobo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_C/0/1/0/all/0/1\">Chong Teng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TKDP: Threefold Knowledge-enriched Deep Prompt Tuning for Few-shot Named Entity Recognition. (arXiv:2306.03974v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03974","description":"<p>Few-shot named entity recognition (NER) exploits limited annotated instances\nto identify named mentions. Effectively transferring the internal or external\nresources thus becomes the key to few-shot NER. While the existing prompt\ntuning methods have shown remarkable few-shot performances, they still fail to\nmake full use of knowledge. In this work, we investigate the integration of\nrich knowledge to prompt tuning for stronger few-shot NER. We propose\nincorporating the deep prompt tuning framework with threefold knowledge (namely\nTKDP), including the internal 1) context knowledge and the external 2) label\nknowledge &amp; 3) sememe knowledge. TKDP encodes the three feature sources and\nincorporates them into the soft prompt embeddings, which are further injected\ninto an existing pre-trained language model to facilitate predictions. On five\nbenchmark datasets, our knowledge-enriched model boosts by at most 11.53% F1\nover the raw deep prompt method, and significantly outperforms 8\nstrong-performing baseline systems in 5-/10-/20-shot settings, showing great\npotential in few-shot NER. Our TKDP can be broadly adapted to other few-shot\ntasks without effort.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hao Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingye Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bobo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_C/0/1/0/all/0/1\">Chong Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1\">Donghong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Conversation Discourse for Dialogue Disentanglement. (arXiv:2306.03975v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03975","description":"<p>Dialogue disentanglement aims to detach the chronologically ordered\nutterances into several independent sessions. Conversation utterances are\nessentially organized and described by the underlying discourse, and thus\ndialogue disentanglement requires the full understanding and harnessing of the\nintrinsic discourse attribute. In this paper, we propose enhancing dialogue\ndisentanglement by taking full advantage of the dialogue discourse\ncharacteristics. First of all, \\textbf{in feature encoding stage}, we construct\nthe heterogeneous graph representations to model the various dialogue-specific\ndiscourse structural features, including the static speaker-role structures\n(i.e., speaker-utterance and speaker-mentioning structure) and the dynamic\ncontextual structures (i.e., the utterance-distance and partial-replying\nstructure). We then develop a structure-aware framework to integrate the rich\nstructural features for better modeling the conversational semantic context.\nSecond, \\textbf{in model learning stage}, we perform optimization with a\nhierarchical ranking loss mechanism, which groups dialogue utterances into\ndifferent discourse levels and carries training covering pair-wise and\nsession-wise levels hierarchically. Third, \\textbf{in inference stage}, we\ndevise an easy-first decoding algorithm, which performs utterance pairing under\nthe easy-to-hard manner with a global context, breaking the constraint of\ntraditional sequential decoding order. On two benchmark datasets, our overall\nsystem achieves new state-of-the-art performances on all evaluations. In-depth\nanalyses further demonstrate the efficacy of each proposed idea and also reveal\nhow our methods help advance the task. Our work has great potential to\nfacilitate broader multi-party multi-thread dialogue applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bobo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hao Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shengqiong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Lizi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yinwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1\">Donghong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"B\\\"{u}y\\\"{u}k dil modellerinin T\\\"{u}rk\\c{c}e verisetleri ile e\\u{g}itilmesi ve ince ayarlanmas\\i. (arXiv:2306.03978v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03978","description":"<p>Large language models have advanced enormously, gained vast attraction and\nare having a phase of intensed research. Some of the developed models and\ntraining datasets have been made open-accessible. Hence these may be further\nfine-tuned with some techniques to obtain specialized models for specific\ntasks. When it comes to Turkish language, open-access models do not provide\nsatisfactory coverage. This is also observed over published datasets. In this\nwork, we propose some ideas to mitigate this issue: creating large Turkish\ndatasets, training LLMs with these and fine-tuning pre-trained models with\nTurkish inputs. We report our findings on Turkish-based trainings with the\nproblems encountered along the way. We conclude with outcomes of these\nexperiments and propose ideas for further works.\n</p>\n<p>--\n</p>\n<p>B\\\"uy\\\"uk dil modelleri inan{\\i}lmaz \\\"ol\\c{c}\\\"ude geli\\c{s}mekte, b\\\"uy\\\"uk\nilgi toplayarak ve \\\"uzerlerinde yo\\u{g}un ara\\c{s}tirmalarin yapildi\\u{g}i bir\nd\\\"onemdedirler. Geli\\c{s}tirilen modeller ve e\\u{g}itimde kullanilan\nverisetlerinden bazilari a\\c{c}ik eri\\c{s}imli olarak sunulmaktadir. B\\\"oylece\nince ayarlama teknikleri uygulayarak \\\"ozelle\\c{s}mi\\c{s} g\\\"orevler i\\c{c}in\n\\c{c}ali\\c{s}abilir modeller elde edilmektedir. T\\\"urk\\c{c}e s\\\"oz konusu\noldu\\u{g}unda bu modellerinin kapsayicili\\u{g}i yeterli d\\\"uzeyde de\\u{g}ildir.\nBu durum, yayimlanan verisetlerinde de g\\\"ozlemlenebilir. Bunu a\\c{s}manin\nyollari T\\\"urk\\c{c}e i\\c{c}erikli b\\\"uy\\\"uk verisetlerinin olu\\c{s}turulmasi,\nb\\\"uy\\\"uk dil modellerinin bunlarla e\\u{g}itilmesi ve \\\"onceden\ne\\u{g}itilmi\\c{s} modellerin T\\\"urk\\c{c}e girdilerle ince ayarlanmalari\nolabilir. Bu \\c{c}ali\\c{s}mada a\\c{c}ik eri\\c{s}imli dil modelleri ve\nverisetleri \\\"uzerinde durulmakta ve T\\\"urk\\c{c}e temelli bazi deneyler,\nkar\\c{s}ila\\c{s}ilan sorunlar ve sonu\\c{c}lar irdelenmektedir.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arslan_A/0/1/0/all/0/1\">A. Taha Arslan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward More Accurate and Generalizable Evaluation Metrics for Task-Oriented Dialogs. (arXiv:2306.03984v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03984","description":"<p>Measurement of interaction quality is a critical task for the improvement of\nspoken dialog systems. Existing approaches to dialog quality estimation either\nfocus on evaluating the quality of individual turns, or collect dialog-level\nquality measurements from end users immediately following an interaction. In\ncontrast to these approaches, we introduce a new dialog-level annotation\nworkflow called Dialog Quality Annotation (DQA). DQA expert annotators evaluate\nthe quality of dialogs as a whole, and also label dialogs for attributes such\nas goal completion and user sentiment. In this contribution, we show that: (i)\nwhile dialog quality cannot be completely decomposed into dialog-level\nattributes, there is a strong relationship between some objective dialog\nattributes and judgments of dialog quality; (ii) for the task of dialog-level\nquality estimation, a supervised model trained on dialog-level annotations\noutperforms methods based purely on aggregating turn-level features; and (iii)\nthe proposed evaluation model shows better domain generalization ability\ncompared to the baselines. On the basis of these results, we argue that having\nhigh-quality human-annotated data is an important component of evaluating\ninteraction quality for large industrial-scale voice assistant platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Komma_A/0/1/0/all/0/1\">Abishek Komma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekarasastry_N/0/1/0/all/0/1\">Nagesh Panyam Chandrasekarasastry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_T/0/1/0/all/0/1\">Timothy Leffel Anuj Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metallinou_A/0/1/0/all/0/1\">Angeliki Metallinou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsoukas_S/0/1/0/all/0/1\">Spyros Matsoukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment Analysis in Finance: From Transformers Back to eXplainable Lexicons (XLex). (arXiv:2306.03997v1 [cs.CL])","link":"http://arxiv.org/abs/2306.03997","description":"<p>Lexicon-based sentiment analysis (SA) in finance leverages specialized,\nmanually annotated lexicons created by human experts to extract sentiment from\nfinancial texts. Although lexicon-based methods are simple to implement and\nfast to operate on textual data, they require considerable manual annotation\nefforts to create, maintain, and update the lexicons. These methods are also\nconsidered inferior to the deep learning-based approaches, such as transformer\nmodels, which have become dominant in various NLP tasks due to their remarkable\nperformance. However, transformers require extensive data and computational\nresources for both training and testing. Additionally, they involve significant\nprediction times, making them unsuitable for real-time production environments\nor systems with limited processing capabilities. In this paper, we introduce a\nnovel methodology named eXplainable Lexicons (XLex) that combines the\nadvantages of both lexicon-based methods and transformer models. We propose an\napproach that utilizes transformers and SHapley Additive exPlanations (SHAP)\nfor explainability to learn financial lexicons. Our study presents four main\ncontributions. Firstly, we demonstrate that transformer-aided explainable\nlexicons can enhance the vocabulary coverage of the benchmark Loughran-McDonald\n(LM) lexicon, reducing the human involvement in annotating, maintaining, and\nupdating the lexicons. Secondly, we show that the resulting lexicon outperforms\nthe standard LM lexicon in SA of financial datasets. Thirdly, we illustrate\nthat the lexicon-based approach is significantly more efficient in terms of\nmodel speed and size compared to transformers. Lastly, the XLex approach is\ninherently more interpretable than transformer models as lexicon models rely on\npredefined rules, allowing for better insights into the results of SA and\nmaking the XLex approach a viable tool for financial decision-making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rizinski_M/0/1/0/all/0/1\">Maryan Rizinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peshov_H/0/1/0/all/0/1\">Hristijan Peshov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishev_K/0/1/0/all/0/1\">Kostadin Mishev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jovanovik_M/0/1/0/all/0/1\">Milos Jovanovik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trajanov_D/0/1/0/all/0/1\">Dimitar Trajanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Triggering Multi-Hop Reasoning for Question Answering in Language Models using Soft Prompts and Random Walks. (arXiv:2306.04009v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04009","description":"<p>Despite readily memorizing world knowledge about entities, pre-trained\nlanguage models (LMs) struggle to compose together two or more facts to perform\nmulti-hop reasoning in question-answering tasks. In this work, we propose\ntechniques that improve upon this limitation by relying on random walks over\nstructured knowledge graphs. Specifically, we use soft prompts to guide LMs to\nchain together their encoded knowledge by learning to map multi-hop questions\nto random walk paths that lead to the answer. Applying our methods on two T5\nLMs shows substantial improvements over standard tuning approaches in answering\nquestions that require 2-hop reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Misra_K/0/1/0/all/0/1\">Kanishka Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_C/0/1/0/all/0/1\">Cicero Nogueira dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakeri_S/0/1/0/all/0/1\">Siamak Shakeri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Analysis of Reader Engagement in Literary Fiction through Eye Tracking and Linguistic Features. (arXiv:2306.04043v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04043","description":"<p>Capturing readers' engagement in fiction is a challenging but important\naspect of narrative understanding. In this study, we collected 23 readers'\nreactions to 2 short stories through eye tracking, sentence-level annotations,\nand an overall engagement scale survey. We analyzed the significance of various\nqualities of the text in predicting how engaging a reader is likely to find it.\nAs enjoyment of fiction is highly contextual, we also investigated individual\ndifferences in our data. Furthering our understanding of what captivates\nreaders in fiction will help better inform models used in creative narrative\ngeneration and collaborative writing tools.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neis_R/0/1/0/all/0/1\">Rose Neis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langis_K/0/1/0/all/0/1\">Karin de Langis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Z/0/1/0/all/0/1\">Zae Myung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Sparse Conversations for Improved Audio-Visual Embodied Navigation. (arXiv:2306.04047v1 [cs.CV])","link":"http://arxiv.org/abs/2306.04047","description":"<p>Efficient navigation towards an audio-goal necessitates an embodied agent to\nnot only possess the ability to use audio-visual cues effectively, but also be\nequipped to actively (but occasionally) seek human/oracle assistance without\nsacrificing autonomy, e.g., when it is uncertain of where to navigate towards\nlocating a noisy or sporadic audio goal. To this end, we present CAVEN -- a\nconversational audio-visual embodied navigation agent that is capable of posing\nnavigation questions to a human/oracle and processing the oracle responses;\nboth in free-form natural language. At the core of CAVEN is a multimodal\nhierarchical reinforcement learning (RL) setup that is equipped with a\nhigh-level policy that is trained to choose from one of three low-level\npolicies (at every step), namely: (i) to navigate using audio-visual cues, or\n(ii) to frame a question to the oracle and receive a short or detailed\nresponse, or (iii) ask generic questions (when unsure of what to ask) and\nreceive instructions. Key to generating the agent's questions is our novel\nTrajectoryNet that forecasts the most likely next steps to the goal and a\nQuestionNet that uses these steps to produce a question. All the policies are\nlearned end-to-end via the RL setup, with penalties to enforce sparsity in\nreceiving navigation instructions from the oracle. To evaluate the performance\nof CAVEN, we present extensive experiments on the SoundSpaces framework for the\ntask of semantic audio-visual navigation. Our results show that CAVEN achieves\nupto 12% gain in performance over competing methods, especially in localizing\nnew sound sources, even in the presence of auditory distractions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiulong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sudipta Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_M/0/1/0/all/0/1\">Moitreya Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1\">Anoop Cherian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLMZip: Lossless Text Compression using Large Language Models. (arXiv:2306.04050v1 [cs.IT])","link":"http://arxiv.org/abs/2306.04050","description":"<p>We provide new estimates of an asymptotic upper bound on the entropy of\nEnglish using the large language model LLaMA-7B as a predictor for the next\ntoken given a window of past tokens. This estimate is significantly smaller\nthan currently available estimates in \\cite{cover1978convergent},\n\\cite{lutati2023focus}. A natural byproduct is an algorithm for lossless\ncompression of English text which combines the prediction from the large\nlanguage model with a lossless compression scheme. Preliminary results from\nlimited experiments suggest that our scheme outperforms state-of-the-art text\ncompression schemes such as BSC, ZPAQ, and paq8h.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valmeekam_C/0/1/0/all/0/1\">Chandra Shekhara Kaushik Valmeekam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_K/0/1/0/all/0/1\">Krishna Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalathil_D/0/1/0/all/0/1\">Dileep Kalathil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chamberland_J/0/1/0/all/0/1\">Jean-Francois Chamberland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1\">Srinivas Shakkottai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting Reddit Posts to Determine Wellness Dimensions impacting Mental Health. (arXiv:2306.04059v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04059","description":"<p>Amid ongoing health crisis, there is a growing necessity to discern possible\nsigns of Wellness Dimensions (WD) manifested in self-narrated text. As the\ndistribution of WD on social media data is intrinsically imbalanced, we\nexperiment the generative NLP models for data augmentation to enable further\nimprovement in the pre-screening task of classifying WD. To this end, we\npropose a simple yet effective data augmentation approach through prompt-based\nGenerative NLP models, and evaluate the ROUGE scores and syntactic/semantic\nsimilarity among existing interpretations and augmented data. Our approach with\nChatGPT model surpasses all the other methods and achieves improvement over\nbaselines such as Easy-Data Augmentation and Backtranslation. Introducing data\naugmentation to generate more training samples and balanced dataset, results in\nthe improved F-score and the Matthew's Correlation Coefficient for upto 13.11%\nand 15.95%, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liyanage_C/0/1/0/all/0/1\">Chandreen Liyanage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1\">Muskan Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mago_V/0/1/0/all/0/1\">Vijay Mago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1\">Sunghwan Sohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models. (arXiv:2306.04067v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04067","description":"<p>The increasingly large size of modern pretrained language models not only\nmakes them inherit more human-like biases from the training corpora, but also\nmakes it computationally expensive to mitigate such biases. In this paper, we\ninvestigate recent parameter-efficient methods in combination with\ncounterfactual data augmentation (CDA) for bias mitigation. We conduct\nextensive experiments with prefix tuning, prompt tuning, and adapter tuning on\ndifferent language models and bias types to evaluate their debiasing\nperformance and abilities to preserve the internal knowledge of a pre-trained\nmodel. We find that the parameter-efficient methods (i) are effective in\nmitigating gender bias, where adapter tuning is consistently the most effective\none and prompt tuning is more suitable for GPT-2 than BERT, (ii) are less\neffective when it comes to racial and religious bias, which may be attributed\nto the limitations of CDA, and (iii) can perform similarly to or sometimes\nbetter than full fine-tuning with improved time and memory efficiency, as well\nas maintain the internal knowledge in BERT and GPT-2, evaluated via fact\nretrieval and downstream fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongbin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-only Domain Adaptation using Unified Speech-Text Representation in Transducer. (arXiv:2306.04076v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04076","description":"<p>Domain adaptation using text-only corpus is challenging in end-to-end(E2E)\nspeech recognition. Adaptation by synthesizing audio from text through TTS is\nresource-consuming. We present a method to learn Unified Speech-Text\nRepresentation in Conformer Transducer(USTR-CT) to enable fast domain\nadaptation using the text-only corpus. Different from the previous textogram\nmethod, an extra text encoder is introduced in our work to learn text\nrepresentation and is removed during inference, so there is no modification for\nonline deployment. To improve the efficiency of adaptation, single-step and\nmulti-step adaptations are also explored. The experiments on adapting\nLibriSpeech to SPGISpeech show the proposed method reduces the word error\nrate(WER) by relatively 44% on the target domain, which is better than those of\nTTS method and textogram method. Also, it is shown the proposed method can be\ncombined with internal language model estimation(ILME) to further improve the\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Lu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zejun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations. (arXiv:2306.04085v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04085","description":"<p>Cross-Lingual Semantic Parsing (CLSP) aims to translate queries in multiple\nnatural languages (NLs) into meaning representations (MRs) such as SQL, lambda\ncalculus, and logic forms. However, existing CLSP models are separately\nproposed and evaluated on datasets of limited tasks and applications, impeding\na comprehensive and unified evaluation of CLSP on a diverse range of NLs and\nMRs. To this end, we present XSemPLR, a unified benchmark for cross-lingual\nsemantic parsing featured with 22 natural languages and 8 meaning\nrepresentations by examining and selecting 9 existing datasets to cover 5 tasks\nand 164 domains. We use XSemPLR to conduct a comprehensive benchmark study on a\nwide range of multilingual language models including encoder-based models\n(mBERT, XLM-R), encoder-decoder models (mBART, mT5), and decoder-based models\n(Codex, BLOOM). We design 6 experiment settings covering various lingual\ncombinations (monolingual, multilingual, cross-lingual) and numbers of learning\nsamples (full dataset, few-shot, and zero-shot). Our experiments show that\nencoder-decoder models (mT5) achieve the highest performance compared with\nother popular models, and multilingual training can further improve the average\nperformance. Notably, multilingual large language models (e.g., BLOOM) are\nstill inadequate to perform CLSP tasks. We also find that the performance gap\nbetween monolingual training and cross-lingual transfer learning is still\nsignificant for multilingual models, though it can be mitigated by\ncross-lingual few-shot training. Our dataset and code are available at\nhttps://github.com/psunlpgroup/XSemPLR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yusen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiguo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gotta: Generative Few-shot Question Answering by Prompt-based Cloze Data Augmentation. (arXiv:2306.04101v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04101","description":"<p>Few-shot question answering (QA) aims at precisely discovering answers to a\nset of questions from context passages while only a few training samples are\navailable. Although existing studies have made some progress and can usually\nachieve proper results, they suffer from understanding deep semantics for\nreasoning out the questions. In this paper, we develop Gotta, a Generative\nprOmpT-based daTa Augmentation framework to mitigate the challenge above.\nInspired by the human reasoning process, we propose to integrate the cloze task\nto enhance few-shot QA learning. Following the recent success of prompt-tuning,\nwe present the cloze task in the same format as the main QA task, allowing the\nmodel to learn both tasks seamlessly together to fully take advantage of the\npower of prompt-tuning. Extensive experiments on widely used benchmarks\ndemonstrate that Gotta consistently outperforms competitive baselines,\nvalidating the effectiveness of our proposed prompt-tuning-based cloze task,\nwhich not only fine-tunes language models but also learns to guide reasoning in\nQA tasks. Further analysis shows that the prompt-based loss incorporates the\nauxiliary task better than the multi-task loss, highlighting the strength of\nprompt-tuning on the few-shot QA task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiusi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jinliang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jyun-Yu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unbalanced Optimal Transport for Unbalanced Word Alignment. (arXiv:2306.04116v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04116","description":"<p>Monolingual word alignment is crucial to model semantic interactions between\nsentences. In particular, null alignment, a phenomenon in which words have no\ncorresponding counterparts, is pervasive and critical in handling semantically\ndivergent sentences. Identification of null alignment is useful on its own to\nreason about the semantic similarity of sentences by indicating there exists\ninformation inequality. To achieve unbalanced word alignment that values both\nalignment and null alignment, this study shows that the family of optimal\ntransport (OT), i.e., balanced, partial, and unbalanced OT, are natural and\npowerful approaches even without tailor-made techniques. Our extensive\nexperiments covering unsupervised and supervised settings indicate that our\ngeneric OT-based alignment methods are competitive against the\nstate-of-the-arts specially designed for word alignment, remarkably on\nchallenging datasets with high null alignment frequencies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arase_Y/0/1/0/all/0/1\">Yuki Arase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Han Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokoi_S/0/1/0/all/0/1\">Sho Yokoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Fusion Interactions: A Study of Human and Automatic Quantification. (arXiv:2306.04125v1 [cs.LG])","link":"http://arxiv.org/abs/2306.04125","description":"<p>Multimodal fusion of multiple heterogeneous and interconnected signals is a\nfundamental challenge in almost all multimodal problems and applications. In\norder to perform multimodal fusion, we need to understand the types of\ninteractions that modalities can exhibit: how each modality individually\nprovides information useful for a task and how this information changes in the\npresence of other modalities. In this paper, we perform a comparative study of\nhow human annotators can be leveraged to annotate two categorizations of\nmultimodal interactions: (1) partial labels, where different randomly assigned\nannotators annotate the label given the first, second, and both modalities, and\n(2) counterfactual labels, where the same annotator is tasked to annotate the\nlabel given the first modality before giving them the second modality and\nasking them to explicitly reason about how their answer changes, before\nproposing an alternative taxonomy based on (3) information decomposition, where\nannotators annotate the degrees of redundancy: the extent to which modalities\nindividually and together give the same predictions on the task, uniqueness:\nthe extent to which one modality enables a task prediction that the other does\nnot, and synergy: the extent to which only both modalities enable one to make a\nprediction about the task that one would not otherwise make using either\nmodality individually. Through extensive experiments and annotations, we\nhighlight several opportunities and limitations of each approach and propose a\nmethod to automatically convert annotations of partial and counterfactual\nlabels to information decomposition, yielding an accurate and efficient method\nfor quantifying interactions in multimodal datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering. (arXiv:2306.04136v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04136","description":"<p>Large Language Models (LLMs) are capable of performing zero-shot closed-book\nquestion answering tasks, based on their internal knowledge stored in\nparameters during pre-training. However, such internalized knowledge might be\ninsufficient and incorrect, which could lead LLMs to generate factually wrong\nanswers. Furthermore, fine-tuning LLMs to update their knowledge is expensive.\nTo this end, we propose to augment the knowledge directly in the input of LLMs.\nSpecifically, we first retrieve the relevant facts to the input question from\nthe knowledge graph based on semantic similarities between the question and its\nassociated facts. After that, we prepend the retrieved facts to the input\nquestion in the form of the prompt, which is then forwarded to LLMs to generate\nthe answer. Our framework, Knowledge-Augmented language model PromptING\n(KAPING), requires no model training, thus completely zero-shot. We validate\nthe performance of our KAPING framework on the knowledge graph question\nanswering task, that aims to answer the user's question based on facts over a\nknowledge graph, on which ours outperforms relevant zero-shot baselines by up\nto 48% in average, across multiple LLMs of various sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1\">Jinheon Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffari_A/0/1/0/all/0/1\">Amir Saffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions. (arXiv:2306.04140v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04140","description":"<p>Large language models (LLMs) can be used to generate text data for training\nand evaluating other models. However, creating high-quality datasets with LLMs\ncan be challenging. In this work, we explore human-AI partnerships to\nfacilitate high diversity and accuracy in LLM-based text data generation. We\nfirst examine two approaches to diversify text generation: 1) logit\nsuppression, which minimizes the generation of languages that have already been\nfrequently generated, and 2) temperature sampling, which flattens the token\nsampling probability. We found that diversification approaches can increase\ndata diversity but often at the cost of data accuracy (i.e., text and labels\nbeing appropriate for the target domain). To address this issue, we examined\ntwo human interventions, 1) label replacement (LR), correcting misaligned\nlabels, and 2) out-of-scope filtering (OOSF), removing instances that are out\nof the user's domain of interest or to which no considered label applies. With\noracle studies, we found that LR increases the absolute accuracy of models\ntrained with diversified datasets by 14.4%. Moreover, we found that some models\ntrained with data generated with LR interventions outperformed LLM-based\nfew-shot classification. In contrast, OOSF was not effective in increasing\nmodel accuracy, implying the need for future work in human-in-the-loop text\ndata generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">John Joon Young Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamar_E/0/1/0/all/0/1\">Ece Kamar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amershi_S/0/1/0/all/0/1\">Saleema Amershi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From the One, Judge of the Whole: Typed Entailment Graph Construction with Predicate Generation. (arXiv:2306.04170v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04170","description":"<p>Entailment Graphs (EGs) have been constructed based on extracted corpora as a\nstrong and explainable form to indicate context-independent entailment\nrelations in natural languages. However, EGs built by previous methods often\nsuffer from the severe sparsity issues, due to limited corpora available and\nthe long-tail phenomenon of predicate distributions. In this paper, we propose\na multi-stage method, Typed Predicate-Entailment Graph Generator (TP-EGG), to\ntackle this problem. Given several seed predicates, TP-EGG builds the graphs by\ngenerating new predicates and detecting entailment relations among them. The\ngenerative nature of TP-EGG helps us leverage the recent advances from large\npretrained language models (PLMs), while avoiding the reliance on carefully\nprepared corpora. Experiments on benchmark datasets show that TP-EGG can\ngenerate high-quality and scale-controllable entailment graphs, achieving\nsignificant in-domain improvement over state-of-the-art EGs and boosting the\nperformance of down-stream inference tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yansong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When to Read Documents or QA History: On Unified and Selective Open-domain QA. (arXiv:2306.04176v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04176","description":"<p>This paper studies the problem of open-domain question answering, with the\naim of answering a diverse range of questions leveraging knowledge resources.\nTwo types of sources, QA-pair and document corpora, have been actively\nleveraged with the following complementary strength. The former is highly\nprecise when the paraphrase of given question $q$ was seen and answered during\ntraining, often posed as a retrieval problem, while the latter generalizes\nbetter for unseen questions. A natural follow-up is thus leveraging both\nmodels, while a naive pipelining or integration approaches have failed to bring\nadditional gains over either model alone. Our distinction is interpreting the\nproblem as calibration, which estimates the confidence of predicted answers as\nan indicator to decide when to use a document or QA-pair corpus. The\neffectiveness of our method was validated on widely adopted benchmarks such as\nNatural Questions and TriviaQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyungjae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Sang-eun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Seung-won Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Moontae Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Foundation Models with Language-Model-as-an-Examiner. (arXiv:2306.04181v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04181","description":"<p>Numerous benchmarks have been established to assess the performance of\nfoundation models on open-ended question answering, which serves as a\ncomprehensive test of a model's ability to understand and generate language in\na manner similar to humans. Most of these works focus on proposing new\ndatasets, however, we see two main issues within previous benchmarking\npipelines, namely testing leakage and evaluation automation. In this paper, we\npropose a novel benchmarking framework, Language-Model-as-an-Examiner, where\nthe LM serves as a knowledgeable examiner that formulates questions based on\nits knowledge and evaluates responses in a reference-free manner. Our framework\nallows for effortless extensibility as various LMs can be adopted as the\nexaminer, and the questions can be constantly updated given more diverse\ntrigger topics. For a more comprehensive and equitable evaluation, we devise\nthree strategies: (1) We instruct the LM examiner to generate questions across\na multitude of domains to probe for a broad acquisition, and raise follow-up\nquestions to engage in a more in-depth assessment. (2) Upon evaluation, the\nexaminer combines both scoring and ranking measurements, providing a reliable\nresult as it aligns closely with human annotations. (3) We additionally propose\na decentralized Peer-examination method to address the biases in a single\nexaminer. Our data and benchmarking results are available at:\nhttps://lmexam.com.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yushi Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_J/0/1/0/all/0/1\">Jiahao Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1\">Xin Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuze He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jifan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_K/0/1/0/all/0/1\">Kaisheng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yijia Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1\">Haozhe Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiayin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowing-how & Knowing-that: A New Task for Machine Reading Comprehension of User Manuals. (arXiv:2306.04187v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04187","description":"<p>The machine reading comprehension (MRC) of user manuals has huge potential in\ncustomer service. However,current methods have trouble answering complex\nquestions. Therefore, we introduce the Knowing-how &amp; Knowing-that task that\nrequires the model to answer factoid-style, procedure-style, and inconsistent\nquestions about user manuals. We resolve this task by jointly representing the\nsteps and facts in a graph (TARA), which supports a unified inference of\nvarious questions. Towards a systematical benchmarking study, we design a\nheuristic method to automatically parse user manuals into TARAs and build an\nannotated dataset to test the model's ability in answering real-world\nquestions. Empirical results demonstrate that representing user manuals as\nTARAs is a desired solution for the MRC of user manuals. An in-depth\ninvestigation of TARA further sheds light on the issues and broader impacts of\nfuture representations of user manuals. We hope our work can move the MRC of\nuser manuals to a more complex and realistic stage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hongru Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Weihong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+jin_d/0/1/0/all/0/1\">dingnan jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zujie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jiancheng Lv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Dataset and Empirical Study for Sentence Simplification in Chinese. (arXiv:2306.04188v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04188","description":"<p>Sentence Simplification is a valuable technique that can benefit language\nlearners and children a lot. However, current research focuses more on English\nsentence simplification. The development of Chinese sentence simplification is\nrelatively slow due to the lack of data. To alleviate this limitation, this\npaper introduces CSS, a new dataset for assessing sentence simplification in\nChinese. We collect manual simplifications from human annotators and perform\ndata analysis to show the difference between English and Chinese sentence\nsimplifications. Furthermore, we test several unsupervised and zero/few-shot\nlearning methods on CSS and analyze the automatic evaluation and human\nevaluation results. In the end, we explore whether Large Language Models can\nserve as high-quality Chinese sentence simplification systems by evaluating\nthem on CSS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shiping Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Renliang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An ASR-Based Tutor for Learning to Read: How to Optimize Feedback to First Graders. (arXiv:2306.04190v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04190","description":"<p>The interest in employing automatic speech recognition (ASR) in applications\nfor reading practice has been growing in recent years. In a previous study, we\npresented an ASR-based Dutch reading tutor application that was developed to\nprovide instantaneous feedback to first-graders learning to read. We saw that\nASR has potential at this stage of the reading process, as the results\nsuggested that pupils made progress in reading accuracy and fluency by using\nthe software. In the current study, we used children's speech from an existing\ncorpus (JASMIN) to develop two new ASR systems, and compared the results to\nthose of the previous study. We analyze correct/incorrect classification of the\nASR systems using human transcripts at word level, by means of evaluation\nmeasures such as Cohen's Kappa, Matthews Correlation Coefficient (MCC),\nprecision, recall and F-measures. We observe improvements for the newly\ndeveloped ASR systems regarding the agreement with human-based judgment and\ncorrect rejection (CR). The accuracy of the ASR systems varies for different\nreading tasks and word types. Our results suggest that, in the current\nconfiguration, it is difficult to classify isolated words. We discuss these\nresults, possible ways to improve our systems and avenues for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tejedor_Garcia_C/0/1/0/all/0/1\">Cristian Tejedor-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubers_F/0/1/0/all/0/1\">Ferdy Hubers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiarini_C/0/1/0/all/0/1\">Catia Cucchiarini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strik_H/0/1/0/all/0/1\">Helmer Strik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Knowledge Graph Embeddings to Enhance Contextual Representations for Relation Extraction. (arXiv:2306.04203v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04203","description":"<p>Relation extraction task is a crucial and challenging aspect of Natural\nLanguage Processing. Several methods have surfaced as of late, exhibiting\nnotable performance in addressing the task; however, most of these approaches\nrely on vast amounts of data from large-scale knowledge graphs or language\nmodels pretrained on voluminous corpora. In this paper, we hone in on the\neffective utilization of solely the knowledge supplied by a corpus to create a\nhigh-performing model. Our objective is to showcase that by leveraging the\nhierarchical structure and relational distribution of entities within a corpus\nwithout introducing external knowledge, a relation extraction model can achieve\nsignificantly enhanced performance. We therefore proposed a relation extraction\napproach based on the incorporation of pretrained knowledge graph embeddings at\nthe corpus scale into the sentence-level contextual representation. We\nconducted a series of experiments which revealed promising and very interesting\nresults for our proposed approach.The obtained results demonstrated an\noutperformance of our method compared to context-based relation extraction\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laleye_F/0/1/0/all/0/1\">Fr&#xe9;jus A. A. Laleye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakotoson_L/0/1/0/all/0/1\">Lo&#xef;c Rakotoson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massip_S/0/1/0/all/0/1\">Sylvain Massip</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Neural Topic Modeling with Embedding Clustering Regularization. (arXiv:2306.04217v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04217","description":"<p>Topic models have been prevalent for decades with various applications.\nHowever, existing topic models commonly suffer from the notorious topic\ncollapsing: discovered topics semantically collapse towards each other, leading\nto highly repetitive topics, insufficient topic discovery, and damaged model\ninterpretability. In this paper, we propose a new neural topic model, Embedding\nClustering Regularization Topic Model (ECRTM). Besides the existing\nreconstruction error, we propose a novel Embedding Clustering Regularization\n(ECR), which forces each topic embedding to be the center of a separately\naggregated word embedding cluster in the semantic space. This enables each\nproduced topic to contain distinct word semantics, which alleviates topic\ncollapsing. Regularized by ECR, our ECRTM generates diverse and coherent topics\ntogether with high-quality topic distributions of documents. Extensive\nexperiments on benchmark datasets demonstrate that ECRTM effectively addresses\nthe topic collapsing issue and consistently surpasses state-of-the-art\nbaselines in terms of topic quality, topic distributions of documents, and\ndownstream classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaobao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xinshuai Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1\">Anh Tuan Luu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Learning from Pre-trained Language Models Improves End-to-End Speech Summarization. (arXiv:2306.04233v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04233","description":"<p>End-to-end speech summarization (E2E SSum) directly summarizes input speech\ninto easy-to-read short sentences with a single model. This approach is\npromising because it, in contrast to the conventional cascade approach, can\nutilize full acoustical information and mitigate to the propagation of\ntranscription errors. However, due to the high cost of collecting\nspeech-summary pairs, an E2E SSum model tends to suffer from training data\nscarcity and output unnatural sentences. To overcome this drawback, we propose\nfor the first time to integrate a pre-trained language model (LM), which is\nhighly capable of generating natural sentences, into the E2E SSum decoder via\ntransfer learning. In addition, to reduce the gap between the independently\npre-trained encoder and decoder, we also propose to transfer the baseline E2E\nSSum encoder instead of the commonly used automatic speech recognition encoder.\nExperimental results show that the proposed model outperforms baseline and data\naugmented models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsuura_K/0/1/0/all/0/1\">Kohei Matsuura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashihara_T/0/1/0/all/0/1\">Takanori Ashihara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moriya_T/0/1/0/all/0/1\">Takafumi Moriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1\">Tomohiro Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kano_T/0/1/0/all/0/1\">Takatomo Kano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogawa_A/0/1/0/all/0/1\">Atsunori Ogawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delcroix_M/0/1/0/all/0/1\">Marc Delcroix</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-microphone Automatic Speech Segmentation in Meetings Based on Circular Harmonics Features. (arXiv:2306.04268v1 [cs.SD])","link":"http://arxiv.org/abs/2306.04268","description":"<p>Speaker diarization is the task of answering Who spoke and when? in an audio\nstream. Pipeline systems rely on speech segmentation to extract speakers'\nsegments and achieve robust speaker diarization. This paper proposes a common\nframework to solve three segmentation tasks in the distant speech scenario:\nVoice Activity Detection (VAD), Overlapped Speech Detection (OSD), and Speaker\nChange Detection (SCD). In the literature, a few studies investigate the\nmulti-microphone distant speech scenario. In this work, we propose a new set of\nspatial features based on direction-of-arrival estimations in the circular\nharmonic domain (CH-DOA). These spatial features are extracted from\nmulti-microphone audio data and combined with standard acoustic features.\nExperiments on the AMI meeting corpus show that CH-DOA can improve the\nsegmentation while being robust in the case of deactivated microphones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mariotte_T/0/1/0/all/0/1\">Th&#xe9;o Mariotte</a> (LAUM, LIUM), <a href=\"http://arxiv.org/find/cs/1/au:+Larcher_A/0/1/0/all/0/1\">Anthony Larcher</a> (LIUM), <a href=\"http://arxiv.org/find/cs/1/au:+Montresor_S/0/1/0/all/0/1\">Silvio Montr&#xe9;sor</a> (LAUM), <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_J/0/1/0/all/0/1\">Jean-Hugh Thomas</a> (LAUM)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of the Fed's communication by using textual entailment model of Zero-Shot classification. (arXiv:2306.04277v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04277","description":"<p>In this study, we analyze documents published by central banks using text\nmining techniques and propose a method to evaluate the policy tone of central\nbanks. Since the monetary policies of major central banks have a broad impact\non financial market trends, the pricing of risky assets, and the real economy,\nmarket participants are attempting to more accurately capture changes in the\noutlook for central banks' future monetary policies. Since the published\ndocuments are also an important tool for the central bank to communicate with\nthe market, they are meticulously elaborated on grammatical syntax and wording,\nand investors are urged to read more accurately about the central bank's policy\nstance. Sentiment analysis on central bank documents has long been carried out,\nbut it has been difficult to interpret the meaning of the documents accurately\nand to explicitly capture even the intentional change in nuance. This study\nattempts to evaluate the implication of the zero-shot text classification\nmethod for an unknown economic environment using the same model. We compare the\ntone of the statements, minutes, press conference transcripts of FOMC meetings,\nand the Fed officials' (chair, vice chair, and Governors) speeches. In\naddition, the minutes of the FOMC meetings were subjected to a phase analysis\nof changes in each policy stance since 1971.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakayama_Y/0/1/0/all/0/1\">Yasuhiro Nakayama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawaki_T/0/1/0/all/0/1\">Tomochika Sawaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phrase Retrieval for Open-Domain Conversational Question Answering with Conversational Dependency Modeling via Contrastive Learning. (arXiv:2306.04293v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04293","description":"<p>Open-Domain Conversational Question Answering (ODConvQA) aims at answering\nquestions through a multi-turn conversation based on a retriever-reader\npipeline, which retrieves passages and then predicts answers with them.\nHowever, such a pipeline approach not only makes the reader vulnerable to the\nerrors propagated from the retriever, but also demands additional effort to\ndevelop both the retriever and the reader, which further makes it slower since\nthey are not runnable in parallel. In this work, we propose a method to\ndirectly predict answers with a phrase retrieval scheme for a sequence of\nwords, reducing the conventional two distinct subtasks into a single one. Also,\nfor the first time, we study its capability for ODConvQA tasks. However, simply\nadopting it is largely problematic, due to the dependencies between previous\nand current turns in a conversation. To address this problem, we further\nintroduce a novel contrastive learning strategy, making sure to reflect\nprevious turns when retrieving the phrase for the current context, by\nmaximizing representational similarities of consecutive turns in a conversation\nwhile minimizing irrelevant conversational contexts. We validate our model on\ntwo ODConvQA datasets, whose experimental results show that it substantially\noutperforms the relevant baselines with the retriever-reader. Code is available\nat: https://github.com/starsuzi/PRO-ConvQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1\">Soyeong Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1\">Jinheon Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jong C. Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Allophant: Cross-lingual Phoneme Recognition with Articulatory Attributes. (arXiv:2306.04306v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04306","description":"<p>This paper proposes Allophant, a multilingual phoneme recognizer. It requires\nonly a phoneme inventory for cross-lingual transfer to a target language,\nallowing for low-resource recognition. The architecture combines a\ncompositional phone embedding approach with individually supervised phonetic\nattribute classifiers in a multi-task architecture. We also introduce\nAllophoible, an extension of the PHOIBLE database. When combined with a\ndistance based mapping approach for grapheme-to-phoneme outputs, it allows us\nto train on PHOIBLE inventories directly. By training and evaluating on 34\nlanguages, we found that the addition of multi-task learning improves the\nmodel's capability of being applied to unseen phonemes and phoneme inventories.\nOn supervised languages we achieve phoneme error rate improvements of 11\npercentage points (pp.) compared to a baseline without multi-task learning.\nEvaluation of zero-shot transfer on 84 languages yielded a decrease in PER of\n2.63 pp. over the baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glocker_K/0/1/0/all/0/1\">Kevin Glocker</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Herygers_A/0/1/0/all/0/1\">Aaricia Herygers</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Georges_M/0/1/0/all/0/1\">Munir Georges</a> (1 and 2) ((1) AImotion Bavaria Technische Hochschule Ingolstadt, (2) Intel Labs Germany)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3's personality instruments results. (arXiv:2306.04308v1 [cs.AI])","link":"http://arxiv.org/abs/2306.04308","description":"<p>To assess the potential applications and limitations of chatbot GPT-3\nDavinci-003, this study explored the temporal reliability of personality\nquestionnaires applied to the chatbot and its personality profile.\nPsychological questionnaires were administered to the chatbot on two separate\noccasions, followed by a comparison of the responses to human normative data.\nThe findings revealed varying levels of agreement in the chatbot's responses\nover time, with some scales displaying excellent while others demonstrated poor\nagreement. Overall, Davinci-003 displayed a socially desirable and pro-social\npersonality profile, particularly in the domain of communion. However, the\nunderlying basis of the chatbot's responses, whether driven by conscious\nself-reflection or predetermined algorithms, remains uncertain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bodroza_B/0/1/0/all/0/1\">Bojana Bodroza</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Dinic_B/0/1/0/all/0/1\">Bojana M. Dinic</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Bojic_L/0/1/0/all/0/1\">Ljubisa Bojic</a> (2) ((1) Department of Psychology, Faculty of Philosophy, University of Novi Sad, Serbia, (2) Digital Society Lab, Institute for Philosophy and Social Theory, University of Belgrade, Serbia)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Genre Argument Mining: Can Language Models Automatically Fill in Missing Discourse Markers?. (arXiv:2306.04314v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04314","description":"<p>Available corpora for Argument Mining differ along several axes, and one of\nthe key differences is the presence (or absence) of discourse markers to signal\nargumentative content. Exploring effective ways to use discourse markers has\nreceived wide attention in various discourse parsing tasks, from which it is\nwell-known that discourse markers are strong indicators of discourse relations.\nTo improve the robustness of Argument Mining systems across different genres,\nwe propose to automatically augment a given text with discourse markers such\nthat all relations are explicitly signaled. Our analysis unveils that popular\nlanguage models taken out-of-the-box fail on this task; however, when\nfine-tuned on a new heterogeneous dataset that we construct (including\nsynthetic and real examples), they perform considerably better. We demonstrate\nthe impact of our approach on an Argument Mining downstream task, evaluated on\ndifferent corpora, showing that language models can be trained to automatically\nfill in discourse markers across different corpora, improving the performance\nof a downstream model in some, but not all, cases. Our proposed approach can\nfurther be employed as an assistive tool for better discourse understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rocha_G/0/1/0/all/0/1\">Gil Rocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_H/0/1/0/all/0/1\">Henrique Lopes Cardoso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belouadi_J/0/1/0/all/0/1\">Jonas Belouadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IUTEAM1 at MEDIQA-Chat 2023: Is simple fine tuning effective for multilayer summarization of clinical conversations?. (arXiv:2306.04328v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04328","description":"<p>Clinical conversation summarization has become an important application of\nNatural language Processing. In this work, we intend to analyze summarization\nmodel ensembling approaches, that can be utilized to improve the overall\naccuracy of the generated medical report called chart note. The work starts\nwith a single summarization model creating the baseline. Then leads to an\nensemble of summarization models trained on a separate section of the chart\nnote. This leads to the final approach of passing the generated results to\nanother summarization model in a multi-layer/stage fashion for better coherency\nof the generated text. Our results indicate that although an ensemble of models\nspecialized in each section produces better results, the multi-layer/stage\napproach does not improve accuracy. The code for the above paper is available\nat https://github.com/dhananjay-srivastava/MEDIQA-Chat-2023-iuteam1.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_D/0/1/0/all/0/1\">Dhananjay Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Echoes from Alexandria: A Large Resource for Multilingual Book Summarization. (arXiv:2306.04334v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04334","description":"<p>In recent years, research in text summarization has mainly focused on the\nnews domain, where texts are typically short and have strong layout features.\nThe task of full-book summarization presents additional challenges which are\nhard to tackle with current resources, due to their limited size and\navailability in English only. To overcome these limitations, we present \"Echoes\nfrom Alexandria\", or in shortened form, \"Echoes\", a large resource for\nmultilingual book summarization. Echoes features three novel datasets: i)\nEcho-Wiki, for multilingual book summarization, ii) Echo-XSum, for\nextremely-compressive multilingual book summarization, and iii) Echo-FairySum,\nfor extractive book summarization. To the best of our knowledge, Echoes, with\nits thousands of books and summaries, is the largest resource, and the first to\nbe multilingual, featuring 5 languages and 25 language pairs. In addition to\nEchoes, we also introduce a new extractive-then-abstractive baseline, and,\nsupported by our experimental results and manual analysis of the summaries\ngenerated, we argue that this baseline is more suitable for book summarization\nthan purely-abstractive approaches. We release our resource and software at\nhttps://github.com/Babelscape/echoes-from-alexandria in the hope of fostering\ninnovative research in multilingual book summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scire_A/0/1/0/all/0/1\">Alessandro Scir&#xe8;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conia_S/0/1/0/all/0/1\">Simone Conia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciciliano_S/0/1/0/all/0/1\">Simone Ciciliano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navigli_R/0/1/0/all/0/1\">Roberto Navigli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study on the Reliability of Automatic Dysarthric Speech Assessments. (arXiv:2306.04337v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04337","description":"<p>Automating dysarthria assessments offers the opportunity to develop\neffective, low-cost tools that address the current limitations of manual and\nsubjective assessments. Nonetheless, it is unclear whether current approaches\nrely on dysarthria-related speech patterns or external factors. We aim toward\nobtaining a clearer understanding of dysarthria patterns. To this extent, we\nstudy the effects of noise in recordings, both through addition and reduction.\nWe design and implement a new method for visualizing and comparing feature\nextractors and models, at a patient level, in a more interpretable way. We use\nthe UA-Speech dataset with a speaker-based split of the dataset. Results\nreported in the literature appear to have been done irrespective of such split,\nleading to models that may be overconfident due to data-leakage. We hope that\nthese results raise awareness in the research community regarding the\nrequirements for establishing reliable automatic dysarthria assessment systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cadet_X/0/1/0/all/0/1\">Xavier F. Cadet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aloufi_R/0/1/0/all/0/1\">Ranya Aloufi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmadi_Abhari_S/0/1/0/all/0/1\">Sara Ahmadi-Abhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddadi_H/0/1/0/all/0/1\">Hamed Haddadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Co-evolving Graph Reasoning Network for Emotion-Cause Pair Extraction. (arXiv:2306.04340v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04340","description":"<p>Emotion-Cause Pair Extraction (ECPE) aims to extract all emotion clauses and\ntheir corresponding cause clauses from a document. Existing approaches tackle\nthis task through multi-task learning (MTL) framework in which the two subtasks\nprovide indicative clues for ECPE. However, the previous MTL framework\nconsiders only one round of multi-task reasoning and ignores the reverse\nfeedbacks from ECPE to the subtasks. Besides, its multi-task reasoning only\nrelies on semantics-level interactions, which cannot capture the explicit\ndependencies, and both the encoder sharing and multi-task hidden states\nconcatenations can hardly capture the causalities. To solve these issues, we\nfirst put forward a new MTL framework based on Co-evolving Reasoning. It (1)\nmodels the bidirectional feedbacks between ECPE and its subtasks; (2) allows\nthe three tasks to evolve together and prompt each other recurrently; (3)\nintegrates prediction-level interactions to capture explicit dependencies. Then\nwe propose a novel multi-task relational graph (MRG) to sufficiently exploit\nthe causal relations. Finally, we propose a Co-evolving Graph Reasoning Network\n(CGR-Net) that implements our MTL framework and conducts Co-evolving Reasoning\non MRG. Experimental results show that our model achieves new state-of-the-art\nperformance, and further analysis confirms the advantages of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xing_B/0/1/0/all/0/1\">Bowen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor W. Tsang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"World Models for Math Story Problems. (arXiv:2306.04347v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04347","description":"<p>Solving math story problems is a complex task for students and NLP models\nalike, requiring them to understand the world as described in the story and\nreason over it to compute an answer. Recent years have seen impressive\nperformance on automatically solving these problems with large pre-trained\nlanguage models and innovative techniques to prompt them. However, it remains\nunclear if these models possess accurate representations of mathematical\nconcepts. This leads to lack of interpretability and trustworthiness which\nimpedes their usefulness in various applications. In this paper, we consolidate\nprevious work on categorizing and representing math story problems and develop\nMathWorld, which is a graph-based semantic formalism specific for the domain of\nmath story problems. With MathWorld, we can assign world models to math story\nproblems which represent the situations and actions introduced in the text and\ntheir mathematical relationships. We combine math story problems from several\nexisting datasets and annotate a corpus of 1,019 problems and 3,204 logical\nforms with MathWorld. Using this data, we demonstrate the following use cases\nof MathWorld: (1) prompting language models with synthetically generated\nquestion-answer pairs to probe their reasoning and world modeling abilities,\nand (2) generating new problems by using the world models as a design space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Opedal_A/0/1/0/all/0/1\">Andreas Opedal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoehr_N/0/1/0/all/0/1\">Niklas Stoehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saparov_A/0/1/0/all/0/1\">Abulhair Saparov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT Self-Supervision for a Better Data Annotator. (arXiv:2306.04349v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04349","description":"<p>The task of annotating data into concise summaries poses a significant\nchallenge across various domains, frequently requiring the allocation of\nsignificant time and specialized knowledge by human experts. Despite existing\nefforts to use large language models for annotation tasks, significant problems\nsuch as limited applicability to unlabeled data, the absence of self-supervised\nmethods, and the lack of focus on complex structured data still persist. In\nthis work, we propose a GPT self-supervision annotation method. This method\nembodies a generating-recovering paradigm that leverages the capabilities of\none-shot learning capabilities in Generative Pretrained Transformer (GPT). The\nproposed approach comprises a one-shot tuning phase followed by a generation\nphase. In the one-shot tuning phase, we sample a data from the support set as\npart of the prompt for GPT to generate a textual summary, which is then used to\nrecover the original data. The alignment score between the recovered and\noriginal data serves as a self-supervision navigator to refine the process. In\nthe generation stage, the optimally selected one-shot sample serves as a\ntemplate in the prompt and is applied to generating summaries from challenging\ndatasets. The annotation performance is evaluated by tuning several human\nfeedback reward networks and by calculating alignment scores between original\nand recovered data at both sentence and structure levels. Our self-supervised\nannotation method consistently achieves competitive scores, convincingly\ndemonstrating its robust strength in various data-to-summary annotation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_X/0/1/0/all/0/1\">Xiaohuan Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems. (arXiv:2306.04357v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04357","description":"<p>Dialogue response selection aims to select an appropriate response from\nseveral candidates based on a given user and system utterance history. Recent\nstudies have been improving the accuracy of dialogue response selection through\npost-training, mostly relying on naive masked language modeling methods.\nHowever, the recently developed generative methods have shown promising text\nrepresentation capabilities in IR community, which could potentially lead to\nbetter dialogue semantics modeling. Thus, in this paper, we propose Dial-MAE\n(Dialogue Contextual Masking Auto-encoder), a straightforward yet effective\npost-training technique tailored for dialogue response selection. Dial-MAE uses\nan asymmetric encoder-decoder architecture that learns to better compress the\nsemantics of the dialogue into dialogue-dense vectors. The process of Dial-MAE\ninvolves a deep encoder creating a dialogue embedding with the masked dialogue\ncontext, followed by a shallow decoder that uses this embedding along with the\nhighly masked response to restore the original response. Our experiments have\ndemonstrated that Dial-MAE is highly effective, achieving state-of-the-art\nperformance on two commonly evaluated benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhenpeng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1\">Guangyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Songlin/0/1/0/all/0/1\">Songlin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks. (arXiv:2306.04362v1 [cs.CV])","link":"http://arxiv.org/abs/2306.04362","description":"<p>To promote the development of Vision-Language Pre-training (VLP) and\nmultimodal Large Language Model (LLM) in the Chinese community, we firstly\nrelease the largest public Chinese high-quality video-language dataset named\nYouku-mPLUG, which is collected from Youku, a well-known Chinese video-sharing\nwebsite, with strict criteria of safety, diversity, and quality. Youku-mPLUG\ncontains 10 million Chinese video-text pairs filtered from 400 million raw\nvideos across a wide range of 45 diverse categories for large-scale\npre-training. In addition, to facilitate a comprehensive evaluation of\nvideo-language models, we carefully build the largest human-annotated Chinese\nbenchmarks covering three popular video-language tasks of cross-modal\nretrieval, video captioning, and video category classification. Youku-mPLUG can\nenable researchers to conduct more in-depth multimodal research and develop\nbetter applications in the future. Furthermore, we release popular\nvideo-language pre-training models, ALPRO and mPLUG-2, and our proposed\nmodularized decoder-only model mPLUG-video pre-trained on Youku-mPLUG.\nExperiments show that models pre-trained on Youku-mPLUG gain up to 23.1%\nimprovement in video category classification. Besides, mPLUG-video achieves a\nnew state-of-the-art result on these benchmarks with 80.5% top-1 accuracy in\nvideo category classification and 68.9 CIDEr score in video captioning,\nrespectively. Finally, we scale up mPLUG-video based on the frozen Bloomz with\nonly 1.7% trainable parameters as Chinese multimodal LLM, and demonstrate\nimpressive instruction and video understanding ability. The zero-shot\ninstruction understanding experiment indicates that pretraining with\nYouku-mPLUG can enhance the ability to comprehend overall and detailed visual\nsemantics, recognize scene text, and leverage open-domain knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinghao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yuan Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiabo Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guohai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Anwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yaya Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1\">Qi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Que_M/0/1/0/all/0/1\">Maofei Que</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arabic Dysarthric Speech Recognition Using Adversarial and Signal-Based Augmentation. (arXiv:2306.04368v1 [cs.SD])","link":"http://arxiv.org/abs/2306.04368","description":"<p>Despite major advancements in Automatic Speech Recognition (ASR), the\nstate-of-the-art ASR systems struggle to deal with impaired speech even with\nhigh-resource languages. In Arabic, this challenge gets amplified, with added\ncomplexities in collecting data from dysarthric speakers. In this paper, we aim\nto improve the performance of Arabic dysarthric automatic speech recognition\nthrough a multi-stage augmentation approach. To this effect, we first propose a\nsignal-based approach to generate dysarthric Arabic speech from healthy Arabic\nspeech by modifying its speed and tempo. We also propose a second stage\nParallel Wave Generative (PWG) adversarial model that is trained on an English\ndysarthric dataset to capture language-independant dysarthric speech patterns\nand further augment the signal-adjusted speech samples. Furthermore, we propose\na fine-tuning and text-correction strategies for Arabic Conformer at different\ndysarthric speech severity levels. Our fine-tuned Conformer achieved 18% Word\nError Rate (WER) and 17.2% Character Error Rate (CER) on synthetically\ngenerated dysarthric speech from the Arabic commonvoice speech dataset. This\nshows significant WER improvement of 81.8% compared to the baseline model\ntrained solely on healthy data. We perform further validation on real English\ndysarthric speech showing a WER improvement of 124% compared to the baseline\ntrained only on healthy English LJSpeech dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baali_M/0/1/0/all/0/1\">Massa Baali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almakky_I/0/1/0/all/0/1\">Ibrahim Almakky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shehata_S/0/1/0/all/0/1\">Shady Shehata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karray_F/0/1/0/all/0/1\">Fakhri Karray</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Aware Speech Representation Learning For Language Identification. (arXiv:2306.04374v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04374","description":"<p>Speech representation learning approaches for non-semantic tasks such as\nlanguage recognition have either explored supervised embedding extraction\nmethods using a classifier model or self-supervised representation learning\napproaches using raw data. In this paper, we propose a novel framework of\ncombining self-supervised representation learning with the language label\ninformation for the pre-training task. This framework, termed as Label Aware\nSpeech Representation (LASR) learning, uses a triplet based objective function\nto incorporate language labels along with the self-supervised loss function.\nThe speech representations are further fine-tuned for the downstream task. The\nlanguage recognition experiments are performed on two public datasets - FLEURS\nand Dhwani. In these experiments, we illustrate that the proposed LASR\nframework improves over the state-of-the-art systems on language\nidentification. We also report an analysis of the robustness of LASR approach\nto noisy/missing labels as well as its application to multi-lingual speech\nrecognition tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vashishth_S/0/1/0/all/0/1\">Shikhar Vashishth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_S/0/1/0/all/0/1\">Shikhar Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganapathy_S/0/1/0/all/0/1\">Sriram Ganapathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Min Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Axelrod_V/0/1/0/all/0/1\">Vera Axelrod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Clinical NER: Translation or Cross-lingual Transfer?. (arXiv:2306.04384v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04384","description":"<p>Natural language tasks like Named Entity Recognition (NER) in the clinical\ndomain on non-English texts can be very time-consuming and expensive due to the\nlack of annotated data. Cross-lingual transfer (CLT) is a way to circumvent\nthis issue thanks to the ability of multilingual large language models to be\nfine-tuned on a specific task in one language and to provide high accuracy for\nthe same task in another language. However, other methods leveraging\ntranslation models can be used to perform NER without annotated data in the\ntarget language, by either translating the training set or test set. This paper\ncompares cross-lingual transfer with these two alternative methods, to perform\nclinical NER in French and in German without any training data in those\nlanguages. To this end, we release MedNERF a medical NER test set extracted\nfrom French drug prescriptions and annotated with the same guidelines as an\nEnglish dataset. Through extensive experiments on this dataset and on a German\nmedical dataset (Frei and Kramer, 2021), we show that translation-based methods\ncan achieve similar performance to CLT but require more care in their design.\nAnd while they can take advantage of monolingual clinical language models,\nthose do not guarantee better results than large general-purpose multilingual\nmodels, whether with cross-lingual transfer or translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fontaine_X/0/1/0/all/0/1\">Xavier Fontaine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaschi_F/0/1/0/all/0/1\">F&#xe9;lix Gaschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastin_P/0/1/0/all/0/1\">Parisa Rastin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toussaint_Y/0/1/0/all/0/1\">Yannick Toussaint</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning. (arXiv:2306.04387v1 [cs.CV])","link":"http://arxiv.org/abs/2306.04387","description":"<p>Instruction tuning has significantly advanced large language models (LLMs)\nsuch as ChatGPT, enabling them to align with human instructions across diverse\ntasks. However, progress in open vision-language models (VLMs) has been limited\ndue to the scarcity of high-quality instruction datasets. To tackle this\nchallenge and promote research in the vision-language field, we introduce the\nMulti-Modal, Multilingual Instruction Tuning (M$^3$IT) dataset, designed to\noptimize VLM alignment with human instructions. Our M$^3$IT dataset comprises\n40 carefully curated datasets, including 2.4 million instances and 400 manually\nwritten task instructions, reformatted into a vision-to-text structure. Key\ntasks are translated into 80 languages with an advanced translation system,\nensuring broader accessibility. M$^3$IT surpasses previous datasets regarding\ntask coverage, instruction number and instance scale. Moreover, we develop\nYing-VLM, a VLM model trained on our M$^3$IT dataset, showcasing its potential\nto answer complex questions requiring world knowledge, generalize to unseen\nvideo tasks, and comprehend unseen instructions in Chinese. To encourage\nfurther research, we have open-sourced both the dataset and trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yuwei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shicheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuhuai Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mukai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yazheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Learning of Transformer-based Speech Recognition Models from Czech to Slovak. (arXiv:2306.04399v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04399","description":"<p>In this paper, we are comparing several methods of training the Slovak speech\nrecognition models based on the Transformers architecture. Specifically, we are\nexploring the approach of transfer learning from the existing Czech pre-trained\nWav2Vec 2.0 model into Slovak. We are demonstrating the benefits of the\nproposed approach on three Slovak datasets. Our Slovak models scored the best\nresults when initializing the weights from the Czech model at the beginning of\nthe pre-training phase. Our results show that the knowledge stored in the Cezch\npre-trained model can be successfully reused to solve tasks in Slovak while\noutperforming even much larger public multilingual models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lehecka_J/0/1/0/all/0/1\">Jan Lehe&#x10d;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Psutka_J/0/1/0/all/0/1\">Josef V. Psutka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Psutka_J/0/1/0/all/0/1\">Josef Psutka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Examining Bias in Opinion Summarisation Through the Perspective of Opinion Diversity. (arXiv:2306.04424v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04424","description":"<p>Opinion summarisation is a task that aims to condense the information\npresented in the source documents while retaining the core message and\nopinions. A summary that only represents the majority opinions will leave the\nminority opinions unrepresented in the summary. In this paper, we use the\nstance towards a certain target as an opinion. We study bias in opinion\nsummarisation from the perspective of opinion diversity, which measures whether\nthe model generated summary can cover a diverse set of opinions. In addition,\nwe examine opinion similarity, a measure of how closely related two opinions\nare in terms of their stance on a given topic, and its relationship with\nopinion diversity. Through the lens of stances towards a topic, we examine\nopinion diversity and similarity using three debatable topics under COVID-19.\nExperimental results on these topics revealed that a higher degree of\nsimilarity of opinions did not indicate good diversity or fairly cover the\nvarious opinions originally presented in the source documents. We found that\nBART and ChatGPT can better capture diverse opinions presented in the source\ndocuments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_N/0/1/0/all/0/1\">Nannan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Lin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fayek_H/0/1/0/all/0/1\">Haytham Fayek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiuzhen Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zambezi Voice: A Multilingual Speech Corpus for Zambian Languages. (arXiv:2306.04428v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04428","description":"<p>This work introduces Zambezi Voice, an open-source multilingual speech\nresource for Zambian languages. It contains two collections of datasets:\nunlabelled audio recordings of radio news and talk shows programs (160 hours)\nand labelled data (over 80 hours) consisting of read speech recorded from text\nsourced from publicly available literature books. The dataset is created for\nspeech recognition but can be extended to multilingual speech processing\nresearch for both supervised and unsupervised learning approaches. To our\nknowledge, this is the first multilingual speech dataset created for Zambian\nlanguages. We exploit pretraining and cross-lingual transfer learning by\nfinetuning the Wav2Vec2.0 large-scale multilingual pre-trained model to build\nend-to-end (E2E) speech recognition models for our baseline models. The dataset\nis released publicly under a Creative Commons BY-NC-ND 4.0 license and can be\naccessed through the project repository. See\nhttps://github.com/unza-speech-lab/zambezi-voice\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sikasote_C/0/1/0/all/0/1\">Claytone Sikasote</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siaminwe_K/0/1/0/all/0/1\">Kalinda Siaminwe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mwape_S/0/1/0/all/0/1\">Stanly Mwape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zulu_B/0/1/0/all/0/1\">Bangiwe Zulu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phiri_M/0/1/0/all/0/1\">Mofya Phiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phiri_M/0/1/0/all/0/1\">Martin Phiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zulu_D/0/1/0/all/0/1\">David Zulu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyirenda_M/0/1/0/all/0/1\">Mayumbo Nyirenda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STEPS: A Benchmark for Order Reasoning in Sequential Tasks. (arXiv:2306.04441v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04441","description":"<p>Various human activities can be abstracted into a sequence of actions in\nnatural text, i.e. cooking, repairing, manufacturing, etc. Such action\nsequences heavily depend on the executing order, while disorder in action\nsequences leads to failure of further task execution by robots or AI agents.\nTherefore, to verify the order reasoning capability of current neural models in\nsequential tasks, we propose a challenging benchmark , named STEPS. STEPS\ninvolves two subtask settings, focusing on determining the rationality of given\nnext step in recipes and selecting the reasonable step from the multi-choice\nquestion, respectively. We describe the data construction and task\nformulations, and benchmark most of significant Large Language Models (LLMs).\nThe experimental results demonstrate 1) The commonsense reasoning of action\norders in sequential tasks are challenging to resolve via zero-shot prompting\nor few-shot in-context learning for LLMs; 2) Prompting method still\nsignificantly lags behind tuning-based method on STEPS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xifeng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty in Natural Language Processing: Sources, Quantification, and Applications. (arXiv:2306.04459v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04459","description":"<p>As a main field of artificial intelligence, natural language processing (NLP)\nhas achieved remarkable success via deep neural networks. Plenty of NLP tasks\nhave been addressed in a unified manner, with various tasks being associated\nwith each other through sharing the same paradigm. However, neural networks are\nblack boxes and rely on probability computation. Making mistakes is inevitable.\nTherefore, estimating the reliability and trustworthiness (in other words,\nuncertainty) of neural networks becomes a key research direction, which plays a\ncrucial role in reducing models' risks and making better decisions. Therefore,\nin this survey, we provide a comprehensive review of uncertainty-relevant works\nin the NLP field. Considering the data and paradigms characteristics, we first\ncategorize the sources of uncertainty in natural language into three types,\nincluding input, system, and output. Then, we systemically review uncertainty\nquantification approaches and the main applications. Finally, we discuss the\nchallenges of uncertainty estimation in NLP and discuss potential future\ndirections, taking into account recent trends in the field. Though there have\nbeen a few surveys about uncertainty estimation, our work is the first to\nreview uncertainty from the NLP perspective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Mengting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shiwan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bingzhe Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Compositional Generalization in Context Dependent Text-to-SQL Parsing. (arXiv:2306.04480v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04480","description":"<p>In the context-dependent Text-to-SQL task, the generated SQL statements are\nrefined iteratively based on the user input utterance from each interaction.\nThe input text from each interaction can be viewed as component modifications\nto the previous SQL statements, which could be further extracted as the\nmodification patterns. Since these modification patterns could also be combined\nwith other SQL statements, the models are supposed to have the compositional\ngeneralization to these novel combinations. This work is the first exploration\nof compositional generalization in context-dependent Text-to-SQL scenarios. To\nfacilitate related studies, we constructed two challenging benchmarks named\n\\textsc{CoSQL-CG} and \\textsc{SParC-CG} by recombining the modification\npatterns and existing SQL statements. The following experiments show that all\ncurrent models struggle on our proposed benchmarks. Furthermore, we found that\nbetter aligning the previous SQL statements with the input utterance could give\nmodels better compositional generalization ability. Based on these\nobservations, we propose a method named \\texttt{p-align} to improve the\ncompositional generalization of Text-to-SQL models. Further experiments\nvalidate the effectiveness of our method. Source code and data are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fukun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yawen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers. (arXiv:2306.04504v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04504","description":"<p>ChatGPT is a large language model developed by OpenAI. Despite its impressive\nperformance across various tasks, no prior work has investigated its capability\nin the biomedical domain yet. To this end, this paper aims to evaluate the\nperformance of ChatGPT on various benchmark biomedical tasks, such as relation\nextraction, document classification, question answering, and summarization. To\nthe best of our knowledge, this is the first work that conducts an extensive\nevaluation of ChatGPT in the biomedical domain. Interestingly, we find based on\nour evaluation that in biomedical datasets that have smaller training sets,\nzero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative\ntransformer models, such as BioGPT and BioBART. This suggests that ChatGPT's\npre-training on large text corpora makes it quite specialized even in the\nbiomedical domain. Our findings demonstrate that ChatGPT has the potential to\nbe a valuable tool for various tasks in the biomedical domain that lack large\nannotated data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jahan_I/0/1/0/all/0/1\">Israt Jahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskar_M/0/1/0/all/0/1\">Md Tahmid Rahman Laskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Chun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jimmy Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing In-Context Learning with Answer Feedback for Multi-Span Question Answering. (arXiv:2306.04508v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04508","description":"<p>Whereas the recent emergence of large language models (LLMs) like ChatGPT has\nexhibited impressive general performance, it still has a large gap with\nfully-supervised models on specific tasks such as multi-span question\nanswering. Previous researches found that in-context learning is an effective\napproach to exploiting LLM, by using a few task-related labeled data as\ndemonstration examples to construct a few-shot prompt for answering new\nquestions. A popular implementation is to concatenate a few questions and their\ncorrect answers through simple templates, informing LLM of the desired output.\nIn this paper, we propose a novel way of employing labeled data such that it\nalso informs LLM of some undesired output, by extending demonstration examples\nwith feedback about answers predicted by an off-the-shelf model, e.g., correct,\nincorrect, or incomplete. Experiments on three multi-span question answering\ndatasets as well as a keyphrase extraction dataset show that our new prompting\nstrategy consistently improves LLM's in-context learning performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zixian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiaying Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1\">Gengyang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gong Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can current NLI systems handle German word order? Investigating language model performance on a new German challenge set of minimal pairs. (arXiv:2306.04523v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04523","description":"<p>Compared to English, German word order is freer and therefore poses\nadditional challenges for natural language inference (NLI). We create WOGLI\n(Word Order in German Language Inference), the first adversarial NLI dataset\nfor German word order that has the following properties: (i) each premise has\nan entailed and a non-entailed hypothesis; (ii) premise and hypotheses differ\nonly in word order and necessary morphological changes to mark case and number.\nIn particular, each premise andits two hypotheses contain exactly the same\nlemmata. Our adversarial examples require the model to use morphological\nmarkers in order to recognise or reject entailment. We show that current German\nautoencoding models fine-tuned on translated NLI data can struggle on this\nchallenge set, reflecting the fact that translated NLI datasets will not mirror\nall necessary language phenomena in the target language. We also examine\nperformance after data augmentation as well as on related word order phenomena\nderived from WOGLI. Our datasets are publically available at\nhttps://github.com/ireinig/wogli.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reinig_I/0/1/0/all/0/1\">Ines Reinig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markert_K/0/1/0/all/0/1\">Katja Markert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts. (arXiv:2306.04528v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04528","description":"<p>The increasing reliance on Large Language Models (LLMs) across academia and\nindustry necessitates a comprehensive understanding of their robustness to\nprompts. In response to this vital need, we introduce PromptBench, a robustness\nbenchmark designed to measure LLMs' resilience to adversarial prompts. This\nstudy uses a plethora of adversarial textual attacks targeting prompts across\nmultiple levels: character, word, sentence, and semantic. These prompts are\nthen employed in diverse tasks, such as sentiment analysis, natural language\ninference, reading comprehension, machine translation, and math\nproblem-solving. Our study generates 4,032 adversarial prompts, meticulously\nevaluated over 8 tasks and 13 datasets, with 567,084 test samples in total. Our\nfindings demonstrate that contemporary LLMs are vulnerable to adversarial\nprompts. Furthermore, we present comprehensive analysis to understand the\nmystery behind prompt robustness and its transferability. We then offer\ninsightful robustness analysis and pragmatic recommendations for prompt\ncomposition, beneficial to both researchers and everyday users. We make our\ncode, prompts, and methodologies to generate adversarial prompts publicly\naccessible, thereby enabling and encouraging collaborative exploration in this\npivotal field: https://github.com/microsoft/promptbench.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kaijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiaheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zichen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lenient Evaluation of Japanese Speech Recognition: Modeling Naturally Occurring Spelling Inconsistency. (arXiv:2306.04530v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04530","description":"<p>Word error rate (WER) and character error rate (CER) are standard metrics in\nSpeech Recognition (ASR), but one problem has always been alternative\nspellings: If one's system transcribes adviser whereas the ground truth has\nadvisor, this will count as an error even though the two spellings really\nrepresent the same word.\n</p>\n<p>Japanese is notorious for ``lacking orthography'': most words can be spelled\nin multiple ways, presenting a problem for accurate ASR evaluation. In this\npaper we propose a new lenient evaluation metric as a more defensible CER\nmeasure for Japanese ASR. We create a lattice of plausible respellings of the\nreference transcription, using a combination of lexical resources, a Japanese\ntext-processing system, and a neural machine translation model for\nreconstructing kanji from hiragana or katakana. In a manual evaluation, raters\nrated 95.4% of the proposed spelling variants as plausible. ASR results show\nthat our method, which does not penalize the system for choosing a valid\nalternate spelling of a word, affords a 2.4%-3.1% absolute reduction in CER\ndepending on the task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karita_S/0/1/0/all/0/1\">Shigeki Karita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sproat_R/0/1/0/all/0/1\">Richard Sproat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_H/0/1/0/all/0/1\">Haruko Ishikawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptAttack: Probing Dialogue State Trackers with Adversarial Prompts. (arXiv:2306.04535v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04535","description":"<p>A key component of modern conversational systems is the Dialogue State\nTracker (or DST), which models a user's goals and needs. Toward building more\nrobust and reliable DSTs, we introduce a prompt-based learning approach to\nautomatically generate effective adversarial examples to probe DST models. Two\nkey characteristics of this approach are: (i) it only needs the output of the\nDST with no need for model parameters, and (ii) it can learn to generate\nnatural language utterances that can target any DST. Through experiments over\nstate-of-the-art DSTs, the proposed framework leads to the greatest reduction\nin accuracy and the best attack success rate while maintaining good fluency and\na low perturbation ratio. We also show how much the generated adversarial\nexamples can bolster a DST through adversarial training. These results indicate\nthe strength of prompt-based attacks on DSTs and leave open avenues for\ncontinued refinement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiangjue Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Ziwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caverlee_J/0/1/0/all/0/1\">James Caverlee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-form analogies generated by chatGPT lack human-like psycholinguistic properties. (arXiv:2306.04537v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04537","description":"<p>Psycholinguistic analyses provide a means of evaluating large language model\n(LLM) output and making systematic comparisons to human-generated text. These\nmethods can be used to characterize the psycholinguistic properties of LLM\noutput and illustrate areas where LLMs fall short in comparison to\nhuman-generated text. In this work, we apply psycholinguistic methods to\nevaluate individual sentences from long-form analogies about biochemical\nconcepts. We compare analogies generated by human subjects enrolled in\nintroductory biochemistry courses to analogies generated by chatGPT. We perform\na supervised classification analysis using 78 features extracted from\nCoh-metrix that analyze text cohesion, language, and readability (Graesser et.\nal., 2004). Results illustrate high performance for classifying\nstudent-generated and chatGPT-generated analogies. To evaluate which features\ncontribute most to model performance, we use a hierarchical clustering\napproach. Results from this analysis illustrate several linguistic differences\nbetween the two sources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seals_S/0/1/0/all/0/1\">S. M. Seals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalin_V/0/1/0/all/0/1\">Valerie L. Shalin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications. (arXiv:2306.04539v1 [cs.LG])","link":"http://arxiv.org/abs/2306.04539","description":"<p>In many machine learning systems that jointly learn from multiple modalities,\na core research question is to understand the nature of multimodal\ninteractions: the emergence of new task-relevant information during learning\nfrom both modalities that was not present in either alone. We study this\nchallenge of interaction quantification in a semi-supervised setting with only\nlabeled unimodal data and naturally co-occurring multimodal data (e.g.,\nunlabeled images and captions, video and corresponding audio) but when labeling\nthem is time-consuming. Using a precise information-theoretic definition of\ninteractions, our key contributions are the derivations of lower and upper\nbounds to quantify the amount of multimodal interactions in this\nsemi-supervised setting. We propose two lower bounds based on the amount of\nshared information between modalities and the disagreement between separately\ntrained unimodal classifiers, and derive an upper bound through connections to\napproximate algorithms for min-entropy couplings. We validate these estimated\nbounds and show how they accurately track true interactions. Finally, two\nsemi-supervised multimodal applications are explored based on these theoretical\nresults: (1) analyzing the relationship between multimodal performance and\nestimated interactions, and (2) self-supervised learning that embraces\ndisagreement between modalities beyond agreement as is typically done.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1\">Chun Kai Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obolenskiy_A/0/1/0/all/0/1\">Alex Obolenskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yudong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_R/0/1/0/all/0/1\">Rohan Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilf_A/0/1/0/all/0/1\">Alex Wilf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Bootstrapping for Label Refinement. (arXiv:2306.04544v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04544","description":"<p>Traditional text classification typically categorizes texts into pre-defined\ncoarse-grained classes, from which the produced models cannot handle the\nreal-world scenario where finer categories emerge periodically for accurate\nservices. In this work, we investigate the setting where fine-grained\nclassification is done only using the annotation of coarse-grained categories\nand the coarse-to-fine mapping. We propose a lightweight contrastive\nclustering-based bootstrapping method to iteratively refine the labels of\npassages. During clustering, it pulls away negative passage-prototype pairs\nunder the guidance of the mapping from both global and local perspectives.\nExperiments on NYT and 20News show that our method outperforms the\nstate-of-the-art methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1\">Shudi Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yu Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sujian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Training with In-Domain Language Models for Diagnostic Reasoning. (arXiv:2306.04551v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04551","description":"<p>Generative artificial intelligence (AI) is a promising direction for\naugmenting clinical diagnostic decision support and reducing diagnostic errors,\na leading contributor to medical errors. To further the development of clinical\nAI systems, the Diagnostic Reasoning Benchmark (DR.BENCH) was introduced as a\ncomprehensive generative AI framework, comprised of six tasks representing key\ncomponents in clinical reasoning. We present a comparative analysis of\nin-domain versus out-of-domain language models as well as multi-task versus\nsingle task training with a focus on the problem summarization task in DR.BENCH\n(Gao et al., 2023). We demonstrate that a multi-task, clinically trained\nlanguage model outperforms its general domain counterpart by a large margin,\nestablishing a new state-of-the-art performance, with a ROUGE-L score of 28.55.\nThis research underscores the value of domain-specific training for optimizing\nclinical diagnostic reasoning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_B/0/1/0/all/0/1\">Brihat Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yanjun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_T/0/1/0/all/0/1\">Timothy Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Churpek_M/0/1/0/all/0/1\">Matthew M. Churpek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afshar_M/0/1/0/all/0/1\">Majid Afshar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dligach_D/0/1/0/all/0/1\">Dmitriy Dligach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models. (arXiv:2306.04563v1 [cs.AI])","link":"http://arxiv.org/abs/2306.04563","description":"<p>Humor is a central aspect of human communication that has not been solved for\nartificial agents so far. Large language models (LLMs) are increasingly able to\ncapture implicit and contextual information. Especially, OpenAI's ChatGPT\nrecently gained immense public attention. The GPT3-based model almost seems to\ncommunicate on a human level and can even tell jokes. Humor is an essential\ncomponent of human communication. But is ChatGPT really funny? We put ChatGPT's\nsense of humor to the test. In a series of exploratory experiments around\njokes, i.e., generation, explanation, and detection, we seek to understand\nChatGPT's capability to grasp and reproduce human humor. Since the model itself\nis not accessible, we applied prompt-based experiments. Our empirical evidence\nindicates that jokes are not hard-coded but mostly also not newly generated by\nthe model. Over 90% of 1008 generated jokes were the same 25 Jokes. The system\naccurately explains valid jokes but also comes up with fictional explanations\nfor invalid jokes. Joke-typical characteristics can mislead ChatGPT in the\nclassification of jokes. ChatGPT has not solved computational humor yet but it\ncan be a big leap toward \"funny\" machines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jentzsch_S/0/1/0/all/0/1\">Sophie Jentzsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender, names and other mysteries: Towards the ambiguous for gender-inclusive translation. (arXiv:2306.04573v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04573","description":"<p>The vast majority of work on gender in MT focuses on 'unambiguous' inputs,\nwhere gender markers in the source language are expected to be resolved in the\noutput. Conversely, this paper explores the widespread case where the source\nsentence lacks explicit gender markers, but the target sentence contains them\ndue to richer grammatical gender. We particularly focus on inputs containing\nperson names.\n</p>\n<p>Investigating such sentence pairs casts a new light on research into MT\ngender bias and its mitigation. We find that many name-gender co-occurrences in\nMT data are not resolvable with 'unambiguous gender' in the source language,\nand that gender-ambiguous examples can make up a large proportion of training\nexamples. From this, we discuss potential steps toward gender-inclusive\ntranslation which accepts the ambiguity in both gender and translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saunders_D/0/1/0/all/0/1\">Danielle Saunders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsen_K/0/1/0/all/0/1\">Katrina Olsen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions. (arXiv:2306.04597v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04597","description":"<p>Societal biases present in pre-trained large language models are a critical\nissue as these models have been shown to propagate biases in countless\ndownstream applications, rendering them unfair towards specific groups of\npeople. Since large-scale retraining of these models from scratch is both time\nand compute-expensive, a variety of approaches have been previously proposed\nthat de-bias a pre-trained model. While the majority of current\nstate-of-the-art debiasing methods focus on changes to the training regime, in\nthis paper, we propose data intervention strategies as a powerful yet simple\ntechnique to reduce gender bias in pre-trained models. Specifically, we\nempirically show that by fine-tuning a pre-trained model on only 10 de-biased\n(intervened) training examples, the tendency to favor any gender is\nsignificantly reduced. Since our proposed method only needs a few training\nexamples, our few-shot debiasing approach is highly feasible and practical.\nThrough extensive experimentation, we show that our debiasing technique\nperforms better than competitive state-of-the-art baselines with minimal loss\nin language modeling ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_H/0/1/0/all/0/1\">Himanshu Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Atishay Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaddamanu_P/0/1/0/all/0/1\">Praneetha Vaddamanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Two Word Test: A Semantic Benchmark for Large Language Models. (arXiv:2306.04610v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04610","description":"<p>Large Language Models (LLMs) have shown remarkable abilities recently,\nincluding passing advanced professional exams and demanding benchmark tests.\nThis performance has led many to suggest that they are close to achieving\nhumanlike or 'true' understanding of language, and even Artificial General\nIntelligence (AGI). Here, we provide a new open-source benchmark that can\nassess semantic abilities of LLMs using two-word phrases using a task that can\nbe performed relatively easily by humans without advanced training. Combining\nmultiple words into a single concept is a fundamental aspect of human language\nand intelligence. The test requires meaningfulness judgments of 1768 noun-noun\ncombinations that have been rated as meaningful (e.g., baby boy) or not\nmeaningful (e.g., goat sky). by 150 human raters. We provide versions of the\ntask that probe meaningfulness ratings on a 0-4 scale as well as binary\njudgments. We conducted a series of experiments using the TWT on GPT-4,\nGPT-3.5, and Bard, with both versions. Results demonstrated that, compared to\nhumans, all models perform poorly at rating meaningfulness of these phrases.\nGPT-3.5 and Bard are also unable to make binary discriminations between\nsensible and nonsense phrases as making sense. GPT-4 makes a substantial\nimprovement in binary discrimination of combinatorial phrases but is still\nsignificantly worse than human performance. The TWT can be used to understand\nthe limitations and weaknesses of current LLMs, and potentially improve them.\nThe test also reminds us that caution is warranted in attributing 'true\nunderstanding' or AGI to LLMs. TWT is available at:\nhttps://github.com/NickRiccardi/two-word-test\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Riccardi_N/0/1/0/all/0/1\">Nicholas Riccardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_R/0/1/0/all/0/1\">Rutvik H. Desai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations. (arXiv:2306.04618v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04618","description":"<p>This paper reexamines the research on out-of-distribution (OOD) robustness in\nthe field of NLP. We find that the distribution shift settings in previous\nstudies commonly lack adequate challenges, hindering the accurate evaluation of\nOOD robustness. To address these issues, we propose a benchmark construction\nprotocol that ensures clear differentiation and challenging distribution\nshifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution\nrobustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we\nconduct a series of experiments on pre-trained language models for analysis and\nevaluation of OOD robustness. First, for vanilla fine-tuning, we examine the\nrelationship between in-distribution (ID) and OOD performance. We identify\nthree typical types that unveil the inner learning mechanism, which could\npotentially facilitate the forecasting of OOD robustness, correlating with the\nadvancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and\nfind that, despite exhibiting some effectiveness in specific cases, they do not\noffer significant improvement compared to vanilla fine-tuning. Further, we\nevaluate 5 LLMs with various adaptation paradigms and find that when sufficient\nID data is available, fine-tuning domain-specific models outperform LLMs on ID\nexamples significantly. However, in the case of OOD instances, prioritizing\nLLMs with in-context learning yields better results. We identify that both\nfine-tuned small models and LLMs face challenges in effectively addressing\ndownstream tasks. The code is public at\n\\url{https://github.com/lifan-yuan/OOD_NLP}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lifan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1\">Ganqu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Hongcheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_F/0/1/0/all/0/1\">Fangyuan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xingyi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Reliability of Watermarks for Large Language Models. (arXiv:2306.04634v1 [cs.LG])","link":"http://arxiv.org/abs/2306.04634","description":"<p>Large language models (LLMs) are now deployed to everyday use and positioned\nto produce large quantities of text in the coming decade. Machine-generated\ntext may displace human-written text on the internet and has the potential to\nbe used for malicious purposes, such as spearphishing attacks and social media\nbots. Watermarking is a simple and effective strategy for mitigating such harms\nby enabling the detection and documentation of LLM-generated text. Yet, a\ncrucial question remains: How reliable is watermarking in realistic settings in\nthe wild? There, watermarked text might be mixed with other text sources,\nparaphrased by human writers or other language models, and used for\napplications in a broad number of domains, both social and technical. In this\npaper, we explore different detection schemes, quantify their power at\ndetecting watermarks, and determine how much machine-generated text needs to be\nobserved in each scenario to reliably detect the watermark. We especially\nhighlight our human study, where we investigate the reliability of watermarking\nwhen faced with human paraphrasing. We compare watermark-based detection to\nother detection strategies, finding overall that watermarking is a reliable\nsolution, especially because of its sample complexity - for all attacks we\nconsider, the watermark evidence compounds the more examples are given, and the\nwatermark is eventually detected.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirchenbauer_J/0/1/0/all/0/1\">John Kirchenbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1\">Jonas Geiping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_M/0/1/0/all/0/1\">Manli Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saifullah_K/0/1/0/all/0/1\">Khalid Saifullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1\">Kezhi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_K/0/1/0/all/0/1\">Kasun Fernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1\">Aniruddha Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection. (arXiv:2306.04637v1 [cs.LG])","link":"http://arxiv.org/abs/2306.04637","description":"<p>Neural sequence models based on the transformer architecture have\ndemonstrated remarkable \\emph{in-context learning} (ICL) abilities, where they\ncan perform new tasks when prompted with training and test examples, without\nany parameter update to the model. This work first provides a comprehensive\nstatistical theory for transformers to perform ICL. Concretely, we show that\ntransformers can implement a broad class of standard machine learning\nalgorithms in context, such as least squares, ridge regression, Lasso, learning\ngeneralized linear models, and gradient descent on two-layer neural networks,\nwith near-optimal predictive power on various in-context data distributions.\nUsing an efficient implementation of in-context gradient descent as the\nunderlying mechanism, our transformer constructions admit mild size bounds, and\ncan be learned with polynomially many pretraining sequences.\n</p>\n<p>Building on these ``base'' ICL algorithms, intriguingly, we show that\ntransformers can implement more complex ICL procedures involving\n\\emph{in-context algorithm selection}, akin to what a statistician can do in\nreal life -- A \\emph{single} transformer can adaptively select different base\nICL algorithms -- or even perform qualitatively different tasks -- on different\ninput sequences, without any explicit prompting of the right algorithm or task.\nWe both establish this in theory by explicit constructions, and also observe\nthis phenomenon experimentally. In theory, we construct two general mechanisms\nfor algorithm selection with concrete examples: pre-ICL testing, and post-ICL\nvalidation. As an example, we use the post-ICL validation mechanism to\nconstruct a transformer that can perform nearly Bayes-optimal ICL on a\nchallenging task -- noisy linear models with mixed noise levels.\nExperimentally, we demonstrate the strong in-context algorithm selection\ncapabilities of standard transformer architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1\">Song Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ModuleFormer: Learning Modular Large Language Models From Uncurated Data. (arXiv:2306.04640v1 [cs.CL])","link":"http://arxiv.org/abs/2306.04640","description":"<p>Large Language Models (LLMs) have achieved remarkable results. But existing\nmodels are expensive to train and deploy, and it is also difficult to expand\ntheir knowledge beyond pre-training data without forgetting previous knowledge.\nThis paper proposes a new neural network architecture, ModuleFormer, that\nleverages modularity to improve the efficiency and flexibility of large\nlanguage models. ModuleFormer is based on the Sparse Mixture of Experts (SMoE).\nUnlike the previous SMoE-based modular language model [Gururangan et al.,\n2021], which requires domain-labeled data to learn domain-specific experts,\nModuleFormer can induce modularity from uncurated data with its new load\nbalancing and load concentration losses. ModuleFormer is a modular architecture\nthat includes two different types of modules, new stick-breaking attention\nheads, and feedforward experts. Different modules are sparsely activated\nconditions on the input token during training and inference. In our experiment,\nwe found that the modular architecture enables three important abilities for\nlarge pre-trained language models: 1) Efficiency, since ModuleFormer only\nactivates a subset of its modules for each input token, thus it could achieve\nthe same performance as dense LLMs with more than two times throughput; 2)\nExtendability, ModuleFormer is more immune to catastrophic forgetting than\ndense LLMs and can be easily extended with new modules to learn new knowledge\nthat is not included in the training data; 3) Specialisation, finetuning\nModuleFormer could specialize a subset of modules to the finetuning task, and\nthe task-unrelated modules could be easily pruned for a lightweight deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yikang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyou Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shawn Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenfang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Early Discovery of Emerging Entities in Persian Twitter with Semantic Similarity. (arXiv:2207.02434v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.02434","description":"<p>Discovering emerging entities (EEs) is the problem of finding entities before\ntheir establishment. These entities can be critical for individuals, companies,\nand governments. Many of these entities can be discovered on social media\nplatforms, e.g. Twitter. These identities have been the spot of research in\nacademia and industry in recent years. Similar to any machine learning problem,\ndata availability is one of the major challenges in this problem. This paper\nproposes EEPT. That is an online clustering method able to discover EEs without\nany need for training on a dataset. Additionally, due to the lack of a proper\nevaluation metric, this paper uses a new metric to evaluate the results. The\nresults show that EEPT is promising and finds significant entities before their\nestablishment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yousefi_S/0/1/0/all/0/1\">Shahin Yousefi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooshmand_M/0/1/0/all/0/1\">Mohsen Hooshmand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afsharchi_M/0/1/0/all/0/1\">Mohsen Afsharchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Context-Sensitive Word Embedding Approach for The Detection of Troll Tweets. (arXiv:2207.08230v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.08230","description":"<p>In this study, we aimed to address the growing concern of trolling behavior\non social media by developing and evaluating a set of model architectures for\nthe automatic detection of troll tweets. Utilizing deep learning techniques and\npre-trained word embedding methods such as BERT, ELMo, and GloVe, we evaluated\nthe performance of each architecture using metrics such as classification\naccuracy, F1 score, AUC, and precision. Our results indicate that BERT and ELMo\nembedding methods performed better than the GloVe method, likely due to their\nability to provide contextualized word embeddings that better capture the\nnuances and subtleties of language use in online social media. Additionally, we\nfound that CNN and GRU encoders performed similarly in terms of F1 score and\nAUC, suggesting their effectiveness in extracting relevant information from\ninput text. The best-performing method was found to be an ELMo-based\narchitecture that employed a GRU classifier, with an AUC score of 0.929. This\nresearch highlights the importance of utilizing contextualized word embeddings\nand appropriate encoder methods in the task of troll tweet detection, which can\nassist social-based systems in improving their performance in identifying and\naddressing trolling behavior on their platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_S/0/1/0/all/0/1\">Seyhmus Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zavrak_S/0/1/0/all/0/1\">Sultan Zavrak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization. (arXiv:2208.09770v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.09770","description":"<p>This paper presents Z-Code++, a new pre-trained language model optimized for\nabstractive text summarization. The model extends the state of the art\nencoder-decoder model using three techniques. First, we use a two-phase\npre-training process to improve model's performance on low-resource\nsummarization tasks. The model is first pre-trained using text corpora for\nlanguage understanding, and then is continually pre-trained on summarization\ncorpora for grounded text generation. Second, we replace self-attention layers\nin the encoder with disentangled attention layers, where each word is\nrepresented using two vectors that encode its content and position,\nrespectively. Third, we use fusion-in-encoder, a simple yet effective method of\nencoding long sequences in a hierarchical manner. Z-Code++ creates new state of\nthe art on 9 out of 13 text summarization tasks across 5 languages. Our model\nis parameter-efficient in that it outperforms the 600x larger PaLM-540B on\nXSum, and the finetuned 200x larger GPT3-175B on SAMSum. In zero-shot and\nfew-shot settings, our model substantially outperforms the competing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Liyang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1\">Jie Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadalla_H/0/1/0/all/0/1\">Hany Hassan Awadalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Wayne Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuedong Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Saliency Map Verbalization: Comparing Feature Importance Representations from Model-free and Instruction-based Methods. (arXiv:2210.07222v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07222","description":"<p>Saliency maps can explain a neural model's predictions by identifying\nimportant input features. They are difficult to interpret for laypeople,\nespecially for instances with many features. In order to make them more\naccessible, we formalize the underexplored task of translating saliency maps\ninto natural language and compare methods that address two key challenges of\nthis approach -- what and how to verbalize. In both automatic and human\nevaluation setups, using token-level attributions from text classification\ntasks, we compare two novel methods (search-based and instruction-based\nverbalizations) against conventional feature importance representations\n(heatmap visualizations and extractive rationales), measuring simulatability,\nfaithfulness, helpfulness and ease of understanding. Instructing GPT-3.5 to\ngenerate saliency map verbalizations yields plausible explanations which\ninclude associations, abstractive summarization and commonsense reasoning,\nachieving by far the highest human ratings, but they are not faithfully\ncapturing numeric information and are inconsistent in their interpretation of\nthe task. In comparison, our search-based, model-free verbalization approach\nefficiently completes templated verbalizations, is faithful by design, but\nfalls short in helpfulness and simulatability. Our results suggest that\nsaliency map verbalization makes feature attribution explanations more\ncomprehensible and less cognitively challenging to humans than conventional\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feldhus_N/0/1/0/all/0/1\">Nils Feldhus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennig_L/0/1/0/all/0/1\">Leonhard Hennig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasert_M/0/1/0/all/0/1\">Maximilian Dustin Nasert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebert_C/0/1/0/all/0/1\">Christopher Ebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwarzenberg_R/0/1/0/all/0/1\">Robert Schwarzenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_S/0/1/0/all/0/1\">Sebastian M&#xf6;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Bottleneck Adapters to Identify Cancer in Clinical Notes under Low-Resource Constraints. (arXiv:2210.09440v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.09440","description":"<p>Processing information locked within clinical health records is a challenging\ntask that remains an active area of research in biomedical NLP. In this work,\nwe evaluate a broad set of machine learning techniques ranging from simple RNNs\nto specialised transformers such as BioBERT on a dataset containing clinical\nnotes along with a set of annotations indicating whether a sample is\ncancer-related or not.\n</p>\n<p>Furthermore, we specifically employ efficient fine-tuning methods from NLP,\nnamely, bottleneck adapters and prompt tuning, to adapt the models to our\nspecialised task. Our evaluations suggest that fine-tuning a frozen BERT model\npre-trained on natural language and with bottleneck adapters outperforms all\nother strategies, including full fine-tuning of the specialised BioBERT model.\nBased on our findings, we suggest that using bottleneck adapters in\nlow-resource situations with limited access to labelled data or processing\ncapacity could be a viable strategy in biomedical text mining. The code used in\nthe experiments are going to be made available at\nhttps://github.com/omidrohanian/bottleneck-adapters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rohanian_O/0/1/0/all/0/1\">Omid Rohanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jauncey_H/0/1/0/all/0/1\">Hannah Jauncey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nouriborji_M/0/1/0/all/0/1\">Mohammadmahdi Nouriborji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_V/0/1/0/all/0/1\">Vinod Kumar Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goncalves_B/0/1/0/all/0/1\">Bronner P. Gon&#xe7;alves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kartsonaki_C/0/1/0/all/0/1\">Christiana Kartsonaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Group_ISARIC_Clinical_Characterisation/0/1/0/all/0/1\">ISARIC Clinical Characterisation Group</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merson_L/0/1/0/all/0/1\">Laura Merson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1\">David Clifton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Contrastive Batch Sampling via Optimization on Sample Permutations. (arXiv:2210.12874v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.12874","description":"<p>Contrastive Learning has recently achieved state-of-the-art performance in a\nwide range of tasks. Many contrastive learning approaches use mined hard\nnegatives to make batches more informative during training but these approaches\nare inefficient as they increase epoch length proportional to the number of\nmined negatives and require frequent updates of nearest neighbor indices or\nmining from recent batches. In this work, we provide an alternative to hard\nnegative mining, Global Contrastive Batch Sampling (GCBS), an efficient\napproximation to the batch assignment problem that upper bounds the gap between\nthe global and training losses, $\\mathcal{L}^{Global} - \\mathcal{L}^{Train}$,\nin contrastive learning settings. Through experimentation we find GCBS improves\nstate-of-the-art performance in sentence embedding and code-search tasks.\nAdditionally, GCBS is easy to implement as it requires only a few additional\nlines of code, does not maintain external data structures such as nearest\nneighbor indices, is more computationally efficient than the most minimal hard\nnegative mining approaches, and makes no changes to the model being trained.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sachidananda_V/0/1/0/all/0/1\">Vin Sachidananda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Easily Accessible Text-to-Image Generation Amplifies Demographic Stereotypes at Large Scale. (arXiv:2211.03759v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.03759","description":"<p>Machine learning models that convert user-written text descriptions into\nimages are now widely available online and used by millions of users to\ngenerate millions of images a day. We investigate the potential for these\nmodels to amplify dangerous and complex stereotypes. We find a broad range of\nordinary prompts produce stereotypes, including prompts simply mentioning\ntraits, descriptors, occupations, or objects. For example, we find cases of\nprompting for basic traits or social roles resulting in images reinforcing\nwhiteness as ideal, prompting for occupations resulting in amplification of\nracial and gender disparities, and prompting for objects resulting in\nreification of American norms. Stereotypes are present regardless of whether\nprompts explicitly mention identity and demographic language or avoid such\nlanguage. Moreover, stereotypes persist despite mitigation strategies; neither\nuser attempts to counter stereotypes by requesting images with specific\ncounter-stereotypes nor institutional attempts to add system ``guardrails''\nhave prevented the perpetuation of stereotypes. Our analysis justifies concerns\nregarding the impacts of today's models, presenting striking exemplars, and\nconnecting these findings with deep insights into harms drawn from social\nscientific and humanist disciplines. This work contributes to the effort to\nshed light on the uniquely complex biases in language-vision models and\ndemonstrates the ways that the mass deployment of text-to-image generation\nmodels results in mass dissemination of stereotypes and resulting harms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalluri_P/0/1/0/all/0/1\">Pratyusha Kalluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ladhak_F/0/1/0/all/0/1\">Faisal Ladhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Myra Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nozza_D/0/1/0/all/0/1\">Debora Nozza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caliskan_A/0/1/0/all/0/1\">Aylin Caliskan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MACSum: Controllable Summarization with Mixed Attributes. (arXiv:2211.05041v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05041","description":"<p>Controllable summarization allows users to generate customized summaries with\nspecified attributes. However, due to the lack of designated annotations of\ncontrolled summaries, existing works have to craft pseudo datasets by adapting\ngeneric summarization benchmarks. Furthermore, most research focuses on\ncontrolling single attributes individually (e.g., a short summary or a highly\nabstractive summary) rather than controlling a mix of attributes together\n(e.g., a short and highly abstractive summary). In this paper, we propose\nMACSum, the first human-annotated summarization dataset for controlling mixed\nattributes. It contains source texts from two domains, news articles and\ndialogues, with human-annotated summaries controlled by five designed\nattributes (Length, Extractiveness, Specificity, Topic, and Speaker). We\npropose two simple and effective parameter-efficient approaches for the new\ntask of mixed controllable summarization based on hard prompt tuning and soft\nprefix tuning. Results and analysis demonstrate that hard prompt models yield\nthe best performance on all metrics and human evaluations. However,\nmixed-attribute control is still challenging for summarization tasks. Our\ndataset and code are available at https://github.com/psunlpgroup/MACSum.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yusen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy-Preserving Domain Adaptation of Semantic Parsers. (arXiv:2212.10520v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10520","description":"<p>Task-oriented dialogue systems often assist users with personal or\nconfidential matters. For this reason, the developers of such a system are\ngenerally prohibited from observing actual usage. So how can they know where\nthe system is failing and needs more training data or new functionality? In\nthis work, we study ways in which realistic user utterances can be generated\nsynthetically, to help increase the linguistic and functional coverage of the\nsystem, without compromising the privacy of actual users. To this end, we\npropose a two-stage Differentially Private (DP) generation method which first\ngenerates latent semantic parses, and then generates utterances based on the\nparses. Our proposed approach improves MAUVE by 2.5X and parse tree function\ntype overlap by 1.3X relative to current approaches for private synthetic data\ngeneration, improving both on fluency and semantic coverage. We further\nvalidate our approach on a realistic domain adaptation task of adding new\nfunctionality from private user data to a semantic parser, and show overall\ngains of 8.5% points in accuracy with the new feature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_R/0/1/0/all/0/1\">Richard Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1\">Jason Eisner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Protecting Language Generation Models via Invisible Watermarking. (arXiv:2302.03162v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2302.03162","description":"<p>Language generation models have been an increasingly powerful enabler for\nmany applications. Many such models offer free or affordable API access, which\nmakes them potentially vulnerable to model extraction attacks through\ndistillation. To protect intellectual property (IP) and ensure fair use of\nthese models, various techniques such as lexical watermarking and synonym\nreplacement have been proposed. However, these methods can be nullified by\nobvious countermeasures such as \"synonym randomization\". To address this issue,\nwe propose GINSEW, a novel method to protect text generation models from being\nstolen through distillation. The key idea of our method is to inject secret\nsignals into the probability vector of the decoding steps for each target\ntoken. We can then detect the secret message by probing a suspect model to tell\nif it is distilled from the protected one. Experimental results show that\nGINSEW can effectively identify instances of IP infringement with minimal\nimpact on the generation quality of protected APIs. Our method demonstrates an\nabsolute improvement of 19 to 29 points on mean average precision (mAP) in\ndetecting suspects compared to previous methods against watermark removal\nattacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xuandong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Handling the Alignment for Wake Word Detection: A Comparison Between Alignment-Based, Alignment-Free and Hybrid Approaches. (arXiv:2302.08950v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.08950","description":"<p>Wake word detection exists in most intelligent homes and portable devices. It\noffers these devices the ability to \"wake up\" when summoned at a low cost of\npower and computing. This paper focuses on understanding alignment's role in\ndeveloping a wake-word system that answers a generic phrase. We discuss three\napproaches. The first is alignment-based, where the model is trained with\nframe-wise cross-entropy. The second is alignment-free, where the model is\ntrained with CTC. The third, proposed by us, is a hybrid solution in which the\nmodel is trained with a small set of aligned data and then tuned with a\nsizeable unaligned dataset. We compare the three approaches and evaluate the\nimpact of the different aligned-to-unaligned ratios for hybrid training. Our\nresults show that the alignment-free system performs better than the\nalignment-based for the target operating point, and with a small fraction of\nthe data (20%), we can train a model that complies with our initial\nconstraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_V/0/1/0/all/0/1\">Vinicius Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yiteng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shangguan_Y/0/1/0/all/0/1\">Yuan Shangguan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhaojun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_L/0/1/0/all/0/1\">Li Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Ming Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extrapolative Controlled Sequence Generation via Iterative Refinement. (arXiv:2303.04562v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2303.04562","description":"<p>We study the problem of extrapolative controlled generation, i.e., generating\nsequences with attribute values beyond the range seen in training. This task is\nof significant importance in automated design, especially drug discovery, where\nthe goal is to design novel proteins that are \\textit{better} (e.g., more\nstable) than existing sequences. Thus, by definition, the target sequences and\ntheir attribute values are out of the training distribution, posing challenges\nto existing methods that aim to directly generate the target sequence. Instead,\nin this work, we propose Iterative Controlled Extrapolation (ICE) which\niteratively makes local edits to a sequence to enable extrapolation. We train\nthe model on synthetically generated sequence pairs that demonstrate small\nimprovement in the attribute value. Results on one natural language task\n(sentiment analysis) and two protein engineering tasks (ACE2 stability and AAV\nfitness) show that ICE considerably outperforms state-of-the-art approaches\ndespite its simplicity. Our code and models are available at:\nhttps://github.com/vishakhpk/iter-extrapolation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_V/0/1/0/all/0/1\">Vishakh Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1\">Richard Yuanzhe Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_A/0/1/0/all/0/1\">Ankur P. Parikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models can Solve Computer Tasks. (arXiv:2303.17491v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.17491","description":"<p>Agents capable of carrying out general tasks on a computer can improve\nefficiency and productivity by automating repetitive tasks and assisting in\ncomplex problem-solving. Ideally, such agents should be able to solve new\ncomputer tasks presented to them through natural language commands. However,\nprevious approaches to this problem require large amounts of expert\ndemonstrations and task-specific reward functions, both of which are\nimpractical for new tasks. In this work, we show that a pre-trained large\nlanguage model (LLM) agent can execute computer tasks guided by natural\nlanguage using a simple prompting scheme where the agent Recursively Criticizes\nand Improves its output (RCI). The RCI approach significantly outperforms\nexisting LLM methods for automating computer tasks and surpasses supervised\nlearning (SL) and reinforcement learning (RL) approaches on the MiniWoB++\nbenchmark. We compare multiple LLMs and find that RCI with the\nInstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful\nof demonstrations per task rather than tens of thousands, and without a\ntask-specific reward function. Furthermore, we demonstrate RCI prompting's\neffectiveness in enhancing LLMs' reasoning abilities on a suite of natural\nlanguage reasoning tasks, outperforming chain of thought (CoT) prompting. We\nfind that RCI combined with CoT performs better than either separately. Our\ncode can be found here: https://github.com/posgnu/rci-agent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Geunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldi_P/0/1/0/all/0/1\">Pierre Baldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAleer_S/0/1/0/all/0/1\">Stephen McAleer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tractable Control for Autoregressive Language Generation. (arXiv:2304.07438v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.07438","description":"<p>Despite the success of autoregressive large language models in text\ngeneration, it remains a major challenge to generate text that satisfies\ncomplex constraints: sampling from the conditional distribution\n${\\Pr}(\\text{text} | \\alpha)$ is intractable for even the simplest lexical\nconstraints $\\alpha$. To overcome this challenge, we propose to use tractable\nprobabilistic models (TPMs) to impose lexical constraints in autoregressive\ntext generation models, which we refer to as GeLaTo (Generating Language with\nTractable Constraints). To demonstrate the effectiveness of this framework, we\nuse distilled hidden Markov models, where we can efficiently compute\n${\\Pr}(\\text{text} | \\alpha)$, to guide autoregressive generation from GPT2.\nGeLaTo achieves state-of-the-art performance on challenging benchmarks for\nconstrained text generation (e.g., CommonGen), beating various strong baselines\nby a large margin. Our work not only opens up new avenues for controlling large\nlanguage models but also motivates the development of more expressive TPMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honghua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_M/0/1/0/all/0/1\">Meihua Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1\">Guy Van den Broeck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models. (arXiv:2304.13835v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.13835","description":"<p>Current dialogue research primarily studies pairwise (two-party)\nconversations, and does not address the everyday setting where more than two\nspeakers converse together. In this work, we both collect and evaluate\nmulti-party conversations to study this more general case. We use the LIGHT\nenvironment to construct grounded conversations, where each participant has an\nassigned character to role-play. We thus evaluate the ability of language\nmodels to act as one or more characters in such conversations. Models require\ntwo skills that pairwise-trained models appear to lack: (1) being able to\ndecide when to talk; (2) producing coherent utterances grounded on multiple\ncharacters. We compare models trained on our new dataset to existing\npairwise-trained dialogue models, as well as large language models with\nfew-shot prompting. We find that our new dataset, MultiLIGHT, which we will\npublicly release, can help bring significant improvements in the group setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jimmy Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuster_K/0/1/0/all/0/1\">Kurt Shuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urbanek_J/0/1/0/all/0/1\">Jack Urbanek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komeili_M/0/1/0/all/0/1\">Mojtaba Komeili</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Cancer Hallmark Classification with BERT-based Deep Learning Approach. (arXiv:2305.03501v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03501","description":"<p>This paper presents a novel approach to accurately classify the hallmarks of\ncancer, which is a crucial task in cancer research. Our proposed method\nutilizes the Bidirectional Encoder Representations from Transformers (BERT)\narchitecture, which has shown exceptional performance in various downstream\napplications. By applying transfer learning, we fine-tuned the pre-trained BERT\nmodel on a small corpus of biomedical text documents related to cancer. The\noutcomes of our experimental investigations demonstrate that our approach\nattains a noteworthy accuracy of 94.45%, surpassing almost all prior findings\nwith a substantial increase of at least 8.04% as reported in the literature.\nThese findings highlight the effectiveness of our proposed model in accurately\nclassifying and comprehending text documents for cancer research, thus\ncontributing significantly to the field. As cancer remains one of the top ten\nleading causes of death globally, our approach holds great promise in advancing\ncancer research and improving patient outcomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zavrak_S/0/1/0/all/0/1\">Sultan Zavrak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_S/0/1/0/all/0/1\">Seyhmus Yilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CADGE: Context-Aware Dialogue Generation Enhanced with Graph-Structured Knowledge Aggregation. (arXiv:2305.06294v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06294","description":"<p>Commonsense knowledge is crucial to many natural language processing tasks.\nExisting works usually incorporate graph knowledge with conventional graph\nneural networks (GNNs), leading to the text and graph knowledge encoding\nprocesses being separated in a serial pipeline. We argue that these separate\nrepresentation learning stages may be suboptimal for neural networks to learn\nthe overall context contained in both types of input knowledge. In this paper,\nwe propose a novel context-aware graph-attention model (Context-aware GAT),\nwhich can effectively incorporate global features of relevant knowledge graphs\nbased on a context-enhanced knowledge aggregation process. Specifically, our\nframework leverages a novel representation learning approach to process\nheterogeneous features - combining flattened graph knowledge with text. To the\nbest of our knowledge, this is the first attempt at hierarchically applying\ngraph knowledge aggregation on a connected subgraph in addition to contextual\ninformation to support commonsense dialogue generation. This framework shows\nsuperior performance compared to conventional GNN-based language frameworks.\nBoth automatic and human evaluation demonstrates that our proposed model has\nsignificant performance uplifts over state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loakman_T/0/1/0/all/0/1\">Tyler Loakman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goetze_S/0/1/0/all/0/1\">Stefan Goetze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PALR: Personalization Aware LLMs for Recommendation. (arXiv:2305.07622v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2305.07622","description":"<p>Large language models (LLMs) have recently received significant attention for\ntheir exceptional capabilities. Despite extensive efforts in developing\ngeneral-purpose LLMs that can be utilized in various natural language\nprocessing (NLP) tasks, there has been less research exploring their potential\nin recommender systems. In this paper, we propose a novel framework, named\nPALR, which aiming to combine user history behaviors (such as clicks,\npurchases, ratings, etc.) with LLMs to generate user preferred items.\nSpecifically, we first use user/item interactions as guidance for candidate\nretrieval. Then we adopt a LLM-based ranking model to generate recommended\nitems. Unlike existing approaches that typically adopt general-purpose LLMs for\nzero/few-shot recommendation testing or training on small-sized language models\n(with less than 1 billion parameters), which cannot fully elicit LLMs'\nreasoning abilities and leverage rich item side parametric knowledge, we\nfine-tune a 7 billion parameters LLM for the ranking purpose. This model takes\nretrieval candidates in natural language format as input, with instruction\nwhich explicitly asking to select results from input candidates during\ninference. Our experimental results demonstrate that our solution outperforms\nstate-of-the-art models on various sequential recommendation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Ziyan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_E/0/1/0/all/0/1\">Eunah Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaojiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yanbin Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpokenWOZ: A Large-Scale Speech-Text Dataset for Spoken Task-Oriented Dialogue in Multiple Domains. (arXiv:2305.13040v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13040","description":"<p>Task-oriented dialogue (TOD) models have made significant progress in recent\nyears. However, previous studies primarily focus on datasets written by\nannotators, which has resulted in a gap between academic research and\nreal-world spoken conversation scenarios. While several small-scale spoken TOD\ndatasets are proposed to address robustness issues such as ASR errors, they\nignore the unique challenges in spoken conversation. To tackle the limitations,\nwe introduce SpokenWOZ, a large-scale speech-text dataset for spoken TOD,\ncontaining 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios from\nhuman-to-human spoken conversations. SpokenWOZ further incorporates common\nspoken characteristics such as word-by-word processing and reasoning in spoken\nlanguage. Based on these characteristics, we present cross-turn slot and\nreasoning slot detection as new challenges. We conduct experiments on various\nbaselines, including text-modal models, newly proposed dual-modal models, and\nLLMs, e.g., ChatGPT. The results show that the current models still have\nsubstantial room for improvement in spoken conversation, where the most\nadvanced dialogue state tracker only achieves 25.65% in joint goal accuracy and\nthe SOTA end-to-end model only correctly completes the user request in 52.1% of\ndialogues. The dataset, code, and leaderboard are available:\nhttps://spokenwoz.github.io/SpokenWOZ-github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Shuzheng Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wentao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Haoyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuchuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting-En Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yinpei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hangyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Linguistic Generalisation in Language Models: A Dataset for Brazilian Portuguese. (arXiv:2305.14070v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14070","description":"<p>Much recent effort has been devoted to creating large-scale language models.\nNowadays, the most prominent approaches are based on deep neural networks, such\nas BERT. However, they lack transparency and interpretability, and are often\nseen as black boxes. This affects not only their applicability in downstream\ntasks but also the comparability of different architectures or even of the same\nmodel trained using different corpora or hyperparameters. In this paper, we\npropose a set of intrinsic evaluation tasks that inspect the linguistic\ninformation encoded in models developed for Brazilian Portuguese. These tasks\nare designed to evaluate how different language models generalise information\nrelated to grammatical structures and multiword expressions (MWEs), thus\nallowing for an assessment of whether the model has learned different\nlinguistic phenomena. The dataset that was developed for these tasks is\ncomposed of a series of sentences with a single masked word and a cue phrase\nthat helps in narrowing down the context. This dataset is divided into MWEs and\ngrammatical structures, and the latter is subdivided into 6 tasks: impersonal\nverbs, subject agreement, verb agreement, nominal agreement, passive and\nconnectors. The subset for MWEs was used to test BERTimbau Large, BERTimbau\nBase and mBERT. For the grammatical structures, we used only BERTimbau Large,\nbecause it yielded the best results in the MWE task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilkens_R/0/1/0/all/0/1\">Rodrigo Wilkens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zilio_L/0/1/0/all/0/1\">Leonardo Zilio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villavicencio_A/0/1/0/all/0/1\">Aline Villavicencio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncovering and Categorizing Social Biases in Text-to-SQL. (arXiv:2305.16253v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16253","description":"<p>Content Warning: This work contains examples that potentially implicate\nstereotypes, associations, and other harms that could be offensive to\nindividuals in certain social groups.} Large pre-trained language models are\nacknowledged to carry social biases towards different demographics, which can\nfurther amplify existing stereotypes in our society and cause even more harm.\nText-to-SQL is an important task, models of which are mainly adopted by\nadministrative industries, where unfair decisions may lead to catastrophic\nconsequences. However, existing Text-to-SQL models are trained on clean,\nneutral datasets, such as Spider and WikiSQL. This, to some extent, cover up\nsocial bias in models under ideal conditions, which nevertheless may emerge in\nreal application scenarios. In this work, we aim to uncover and categorize\nsocial biases in Text-to-SQL models. We summarize the categories of social\nbiases that may occur in structured data for Text-to-SQL models. We build test\nbenchmarks and reveal that models with similar task accuracy can contain social\nbiases at very different rates. We show how to take advantage of our\nmethodology to uncover and assess social biases in the downstream Text-to-SQL\ntask. We will release our code and data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhe Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaokang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ash_E/0/1/0/all/0/1\">Elliott Ash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art. (arXiv:2305.16259v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16259","description":"<p>The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural\nLanguage Processing (NLP) during the past decade. However, the demands of long\ndocument analysis are quite different from those of shorter texts, while the\never increasing size of documents uploaded on-line renders automated\nunderstanding of long texts a critical area of research. This article has two\ngoals: a) it overviews the relevant neural building blocks, thus serving as a\nshort tutorial, and b) it surveys the state-of-the-art in long document NLP,\nmainly focusing on two central tasks: document classification and document\nsummarization. Sentiment analysis for long texts is also covered, since it is\ntypically treated as a particular case of document classification. Thus, this\narticle concerns document-level analysis. It discusses the main challenges and\nissues of long document NLP, along with the current solutions. Finally, the\nrelevant, publicly available, annotated datasets are presented, in order to\nfacilitate further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsirmpas_D/0/1/0/all/0/1\">Dimitrios Tsirmpas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkionis_I/0/1/0/all/0/1\">Ioannis Gkionis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mademlis_I/0/1/0/all/0/1\">Ioannis Mademlis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions. (arXiv:2305.16636v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2305.16636","description":"<p>Modern machine learning relies on datasets to develop and validate research\nideas. Given the growth of publicly available data, finding the right dataset\nto use is increasingly difficult. Any research question imposes explicit and\nimplicit constraints on how well a given dataset will enable researchers to\nanswer this question, such as dataset size, modality, and domain. We\noperationalize the task of recommending datasets given a short natural language\ndescription of a research idea, to help people find relevant datasets for their\nneeds. Dataset recommendation poses unique challenges as an information\nretrieval problem; datasets are hard to directly index for search and there are\nno corpora readily available for this task. To facilitate this task, we build\nthe DataFinder Dataset which consists of a larger automatically-constructed\ntraining set (17.5K queries) and a smaller expert-annotated evaluation set (392\nqueries). Using this data, we compare various information retrieval algorithms\non our test set and present a superior bi-encoder retriever for text-based\ndataset recommendation. This system, trained on the DataFinder Dataset, finds\nmore relevant search results than existing third-party dataset search engines.\nTo encourage progress on dataset recommendation, we release our dataset and\nmodels to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Viswanathan_V/0/1/0/all/0/1\">Vijay Viswanathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Luyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"People and Places of Historical Europe: Bootstrapping Annotation Pipeline and a New Corpus of Named Entities in Late Medieval Texts. (arXiv:2305.16718v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16718","description":"<p>Although pre-trained named entity recognition (NER) models are highly\naccurate on modern corpora, they underperform on historical texts due to\ndifferences in language OCR errors. In this work, we develop a new NER corpus\nof 3.6M sentences from late medieval charters written mainly in Czech, Latin,\nand German.\n</p>\n<p>We show that we can start with a list of known historical figures and\nlocations and an unannotated corpus of historical texts, and use information\nretrieval techniques to automatically bootstrap a NER-annotated corpus. Using\nour corpus, we train a NER model that achieves entity-level Precision of\n72.81-93.98% with 58.14-81.77% Recall on a manually-annotated test dataset.\nFurthermore, we show that using a weighted loss function helps to combat class\nimbalance in token classification tasks. To make it easy for others to\nreproduce and build upon our work, we publicly release our corpus, models, and\nexperimental code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Novotny_V/0/1/0/all/0/1\">V&#xed;t Novotn&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luger_K/0/1/0/all/0/1\">Krist&#xfd;na Luger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1\">Michal &#x160;tef&#xe1;nik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vrabcova_T/0/1/0/all/0/1\">Tereza Vrabcov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horak_A/0/1/0/all/0/1\">Ale&#x161; Hor&#xe1;k</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HowkGPT: Investigating the Detection of ChatGPT-generated University Student Homework through Context-Aware Perplexity Analysis. (arXiv:2305.18226v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18226","description":"<p>As the use of Large Language Models (LLMs) in text generation tasks\nproliferates, concerns arise over their potential to compromise academic\nintegrity. The education sector currently tussles with distinguishing\nstudent-authored homework assignments from AI-generated ones. This paper\naddresses the challenge by introducing HowkGPT, designed to identify homework\nassignments generated by AI. HowkGPT is built upon a dataset of academic\nassignments and accompanying metadata [17] and employs a pretrained LLM to\ncompute perplexity scores for student-authored and ChatGPT-generated responses.\nThese scores then assist in establishing a threshold for discerning the origin\nof a submitted assignment. Given the specificity and contextual nature of\nacademic work, HowkGPT further refines its analysis by defining\ncategory-specific thresholds derived from the metadata, enhancing the precision\nof the detection. This study emphasizes the critical need for effective\nstrategies to uphold academic integrity amidst the growing influence of LLMs\nand provides an approach to ensuring fair and accurate grading in educational\ninstitutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasilatos_C/0/1/0/all/0/1\">Christoforos Vasilatos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Manaar Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahwan_T/0/1/0/all/0/1\">Talal Rahwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaki_Y/0/1/0/all/0/1\">Yasir Zaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maniatakos_M/0/1/0/all/0/1\">Michail Maniatakos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Test-Time Training on Nearest Neighbors for Large Language Models. (arXiv:2305.18466v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18466","description":"<p>Many recent efforts aim to augment language models with relevant information\nretrieved from a database at test time. We avoid the need for prompt\nengineering by directly fine-tuning the model on data retrieved at test time\nusing its standard training setup. For this purpose, we build a large-scale\ndistributed nearest neighbor index based on text embeddings of the Pile\ndataset. Given a query to a language model, our system retrieves the neighbors\nof the query and fine-tunes the model on the text data corresponding to those\nneighbors. Surprisingly, retrieving and training on as few as 20 neighbors,\neach for only one gradient iteration, drastically improves performance across\nmore than twenty language modeling tasks in the Pile benchmark. For example,\ntest-time training significantly narrows the performance gap between a small\nGPT2 model and a GPTNeo model, more than ten times larger, that was\nspecifically trained to convergence on the Pile. Sufficient index quality and\nsize, however, are important. Our work establishes a valuable first baseline\nfor implementing test-time training in the context of large language models,\nopening the door to numerous promising research avenues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hardt_M/0/1/0/all/0/1\">Moritz Hardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT an ENFJ, Bard an ISTJ: Empirical Study on Personalities of Large Language Models. (arXiv:2305.19926v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.19926","description":"<p>Large Language Models (LLMs) have made remarkable advancements in the field\nof artificial intelligence, significantly reshaping the human-computer\ninteraction. We not only focus on the performance of LLMs, but also explore\ntheir features from a psychological perspective, acknowledging the importance\nof understanding their behavioral characteristics. Our study examines the\nbehavioral patterns displayed by LLMs by employing trait theory, a\npsychological framework. We first focus on evaluating the consistency of\npersonality types exhibited by ChatGPT. Furthermore, experiments include\ncross-lingual effects on seven additional languages, and the investigation of\nsix other LLMs. Moreover, the study investigates whether ChatGPT can exhibit\npersonality changes in response to instructions or contextual cues. The\nfindings show that ChatGPT consistently maintains its ENFJ personality\nregardless of instructions or contexts. By shedding light on the\npersonalization of LLMs, we anticipate that our study will serve as a catalyst\nfor further research in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jen-tse Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1\">Man Ho Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Eric John Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FEED PETs: Further Experimentation and Expansion on the Disambiguation of Potentially Euphemistic Terms. (arXiv:2306.00217v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.00217","description":"<p>Transformers have been shown to work well for the task of English euphemism\ndisambiguation, in which a potentially euphemistic term (PET) is classified as\neuphemistic or non-euphemistic in a particular context. In this study, we\nexpand on the task in two ways. First, we annotate PETs for vagueness, a\nlinguistic property associated with euphemisms, and find that transformers are\ngenerally better at classifying vague PETs, suggesting linguistic differences\nin the data that impact performance. Second, we present novel euphemism corpora\nin three different languages: Yoruba, Spanish, and Mandarin Chinese. We perform\neuphemism disambiguation experiments in each language using multilingual\ntransformer models mBERT and XLM-RoBERTa, establishing preliminary results from\nwhich to launch future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1\">Patrick Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shode_I/0/1/0/all/0/1\">Iyanuoluwa Shode</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trujillo_A/0/1/0/all/0/1\">Alain Chirino Trujillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojo_O/0/1/0/all/0/1\">Olumide Ebenezer Ojo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plancarte_D/0/1/0/all/0/1\">Diana Cuevas Plancarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_A/0/1/0/all/0/1\">Anna Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jing Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Anisotropy and Outliers in Multilingual Language Models for Cross-Lingual Semantic Sentence Similarity. (arXiv:2306.00458v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.00458","description":"<p>Previous work has shown that the representations output by contextual\nlanguage models are more anisotropic than static type embeddings, and typically\ndisplay outlier dimensions. This seems to be true for both monolingual and\nmultilingual models, although much less work has been done on the multilingual\ncontext. Why these outliers occur and how they affect the representations is\nstill an active area of research. We investigate outlier dimensions and their\nrelationship to anisotropy in multiple pre-trained multilingual language\nmodels. We focus on cross-lingual semantic similarity tasks, as these are\nnatural tasks for evaluating multilingual representations. Specifically, we\nexamine sentence representations. Sentence transformers which are fine-tuned on\nparallel resources (that are not always available) perform better on this task,\nand we show that their representations are more isotropic. However, we aim to\nimprove multilingual representations in general. We investigate how much of the\nperformance difference can be made up by only transforming the embedding space\nwithout fine-tuning, and visualise the resulting spaces. We test different\noperations: Removing individual outlier dimensions, cluster-based isotropy\nenhancement, and ZCA whitening. We publish our code for reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hammerl_K/0/1/0/all/0/1\">Katharina H&#xe4;mmerl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fastowski_A/0/1/0/all/0/1\">Alina Fastowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Libovicky_J/0/1/0/all/0/1\">Jind&#x159;ich Libovick&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL. (arXiv:2306.00739v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.00739","description":"<p>One impressive emergent capability of large language models (LLMs) is\ngeneration of code, including Structured Query Language (SQL) for databases.\nFor the task of converting natural language text to SQL queries, Text-to-SQL,\nadaptation of LLMs is of paramount importance, both in in-context learning and\nfine-tuning settings, depending on the amount of adaptation data used. In this\npaper, we propose an LLM-based Text-to-SQL model SQL-PaLM, leveraging on\nPaLM-2, that pushes the state-of-the-art in both settings. Few-shot SQL-PaLM is\nbased on an execution-based self-consistency prompting approach designed for\nText-to-SQL, and achieves 77.3% in test-suite accuracy on Spider, which to our\nbest knowledge is the first to outperform previous state-of-the-art with\nfine-tuning by a significant margin, 4%. Furthermore, we demonstrate that the\nfine-tuned SQL-PALM outperforms it further by another 1%. Towards applying\nSQL-PaLM to real-world scenarios we further evaluate its robustness on other\nchallenging variants of Spider and demonstrate the superior generalization\ncapability of SQL-PaLM. In addition, via extensive case studies, we demonstrate\nthe impressive intelligent capabilities and various success enablers of\nLLM-based Text-to-SQL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Ruoxi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arik_S/0/1/0/all/0/1\">Sercan O. Arik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakhost_H/0/1/0/all/0/1\">Hootan Nakhost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hanjun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_R/0/1/0/all/0/1\">Rajarishi Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Pengcheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Multi-Step Reasoning by Solving Arithmetic Tasks. (arXiv:2306.01707v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.01707","description":"<p>Mathematical reasoning is regarded as a necessary ability for Language Models\n(LMs). Recent works demonstrate large LMs' impressive performance in solving\nmath problems. The success is attributed to their Chain-of-Thought (CoT)\nreasoning abilities, i.e., the ability to decompose complex questions into\nstep-by-step reasoning chains, but such ability seems only to emerge from\nmodels with abundant parameters. This work investigates how to incorporate\nrelatively small LMs with the capabilities of multi-step reasoning. We propose\nto inject such abilities by continually pre-training LMs on a synthetic dataset\nMsAT which is composed of Multi-step Arithmetic Tasks. Our experiments on four\nmath word problem datasets show the effectiveness of the proposed method in\nenhancing LMs' math reasoning abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianduo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Tuning Language Models with Advantage-Induced Policy Alignment. (arXiv:2306.02231v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.02231","description":"<p>Reinforcement learning from human feedback (RLHF) has emerged as a reliable\napproach to aligning large language models (LLMs) to human preferences. Among\nthe plethora of RLHF techniques, proximal policy optimization (PPO) is of the\nmost widely used methods. Despite its popularity, however, PPO may suffer from\nmode collapse, instability, and poor sample efficiency. We show that these\nissues can be alleviated by a novel algorithm that we refer to as\nAdvantage-Induced Policy Alignment (APA), which leverages a squared error loss\nfunction based on the estimated advantages. We demonstrate empirically that APA\nconsistently outperforms PPO in language tasks by a large margin, when a\nseparate reward model is employed as the evaluator. In addition, compared with\nPPO, APA offers a more stable form of control over the deviation from the\nmodel's initial policy, ensuring that the model improves its performance\nwithout collapsing to deterministic output. In addition to empirical results,\nwe also provide a theoretical justification supporting the design of our loss\nfunction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Banghua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_H/0/1/0/all/0/1\">Hiteshi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frujeri_F/0/1/0/all/0/1\">Felipe Vieira Frujeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Shi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jiantao Jiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark. (arXiv:2306.02349v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.02349","description":"<p>We present bgGLUE(Bulgarian General Language Understanding Evaluation), a\nbenchmark for evaluating language models on Natural Language Understanding\n(NLU) tasks in Bulgarian. Our benchmark includes NLU tasks targeting a variety\nof NLP problems (e.g., natural language inference, fact-checking, named entity\nrecognition, sentiment analysis, question answering, etc.) and machine learning\ntasks (sequence labeling, document-level classification, and regression). We\nrun the first systematic evaluation of pre-trained language models for\nBulgarian, comparing and contrasting results across the nine tasks in the\nbenchmark. The evaluation results show strong performance on sequence labeling\ntasks, but there is a lot of room for improvement for tasks that require more\ncomplex reasoning. We make bgGLUE publicly available together with the\nfine-tuning and the evaluation code, as well as a public leaderboard at\nhttps://bgglue.github.io/, and we hope that it will enable further advancements\nin developing NLU models for Bulgarian.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hardalov_M/0/1/0/all/0/1\">Momchil Hardalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanasova_P/0/1/0/all/0/1\">Pepa Atanasova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihaylov_T/0/1/0/all/0/1\">Todor Mihaylov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelova_G/0/1/0/all/0/1\">Galia Angelova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simov_K/0/1/0/all/0/1\">Kiril Simov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osenova_P/0/1/0/all/0/1\">Petya Osenova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1\">Ves Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koychev_I/0/1/0/all/0/1\">Ivan Koychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2306.03341","description":"<p>We introduce Inference-Time Intervention (ITI), a technique designed to\nenhance the truthfulness of large language models (LLMs). ITI operates by\nshifting model activations during inference, following a set of directions\nacross a limited number of attention heads. This intervention significantly\nimproves the performance of LLaMA models on the TruthfulQA benchmark. On an\ninstruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from\n32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and\ndemonstrate how to balance it by tuning the intervention strength. ITI is\nminimally invasive and computationally inexpensive. Moreover, the technique is\ndata efficient: while approaches like RLHF require extensive annotations, ITI\nlocates truthful directions using only few hundred examples. Our findings\nsuggest that LLMs may have an internal representation of the likelihood of\nsomething being true, even as they produce falsehoods on the surface.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kenneth Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_O/0/1/0/all/0/1\">Oam Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viegas_F/0/1/0/all/0/1\">Fernanda Vi&#xe9;gas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenberg_M/0/1/0/all/0/1\">Martin Wattenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TwistList: Resources and Baselines for Tongue Twister Generation. (arXiv:2306.03457v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.03457","description":"<p>Previous work in phonetically-grounded language generation has mainly focused\non domains such as lyrics and poetry. In this paper, we present work on the\ngeneration of tongue twisters - a form of language that is required to be\nphonetically conditioned to maximise sound overlap, whilst maintaining semantic\nconsistency with an input topic, and still being grammatically correct. We\npresent \\textbf{TwistList}, a large annotated dataset of tongue twisters,\nconsisting of 2.1K+ human-authored examples. We additionally present several\nbenchmark systems (referred to as TwisterMisters) for the proposed task of\ntongue twister generation, including models that both do and do not require\ntraining on in-domain data. We present the results of automatic and human\nevaluation to demonstrate the performance of existing mainstream pre-trained\nmodels in this task with limited (or no) task specific training and data, and\nno explicit phonetic knowledge. We find that the task of tongue twister\ngeneration is challenging for models under these conditions, yet some models\nare still capable of generating acceptable examples of this language type.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loakman_T/0/1/0/all/0/1\">Tyler Loakman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT Informed Graph Neural Network for Stock Movement Prediction. (arXiv:2306.03763v2 [q-fin.ST] UPDATED)","link":"http://arxiv.org/abs/2306.03763","description":"<p>ChatGPT has demonstrated remarkable capabilities across various natural\nlanguage processing (NLP) tasks. However, its potential for inferring dynamic\nnetwork structures from temporal textual data, specifically financial news,\nremains an unexplored frontier. In this research, we introduce a novel\nframework that leverages ChatGPT's graph inference capabilities to enhance\nGraph Neural Networks (GNN). Our framework adeptly extracts evolving network\nstructures from textual data, and incorporates these networks into graph neural\nnetworks for subsequent predictive tasks. The experimental results from stock\nmovement forecasting indicate our model has consistently outperformed the\nstate-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios\nconstructed based on our model's outputs demonstrate higher annualized\ncumulative returns, alongside reduced volatility and maximum drawdown. This\nsuperior performance highlights the potential of ChatGPT for text-based network\ninferences and underscores its promising implications for the financial sector.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Chen_Z/0/1/0/all/0/1\">Zihan Chen</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zheng_L/0/1/0/all/0/1\">Lei Nico Zheng</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Lu_C/0/1/0/all/0/1\">Cheng Lu</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Yuan_J/0/1/0/all/0/1\">Jialu Yuan</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zhu_D/0/1/0/all/0/1\">Di Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deductive Verification of Chain-of-Thought Reasoning. (arXiv:2306.03872v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.03872","description":"<p>Large Language Models (LLMs) significantly benefit from Chain-of-Thought\n(CoT) prompting in performing various reasoning tasks. While CoT allows models\nto produce more comprehensive reasoning processes, its emphasis on intermediate\nreasoning steps can inadvertently introduce hallucinations and accumulated\nerrors, thereby limiting models' ability to solve complex reasoning tasks.\nInspired by how humans engage in careful and meticulous deductive logical\nreasoning processes to solve tasks, we seek to enable language models to\nperform explicit and rigorous deductive reasoning, and also ensure the\ntrustworthiness of their reasoning process through self-verification. However,\ndirectly verifying the validity of an entire deductive reasoning process is\nchallenging, even with advanced models like ChatGPT. In light of this, we\npropose to decompose a reasoning verification process into a series of\nstep-by-step subprocesses, each only receiving their necessary context and\npremises. To facilitate this procedure, we propose Natural Program, a natural\nlanguage-based deductive reasoning format. Our approach enables models to\ngenerate precise reasoning steps where subsequent steps are more rigorously\ngrounded on prior steps. It also empowers language models to carry out\nreasoning self-verification in a step-by-step manner. By integrating this\nverification process into each deductive reasoning stage, we significantly\nenhance the rigor and trustfulness of generated reasoning steps. Along this\nprocess, we also improve the answer correctness on complex reasoning tasks.\nCode will be released at https://github.com/lz1oceani/verify_cot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yunhao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Mingu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Memisevic_R/0/1/0/all/0/1\">Roland Memisevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal interventions expose implicit situation models for commonsense language understanding. (arXiv:2306.03882v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.03882","description":"<p>Accounts of human language processing have long appealed to implicit\n``situation models'' that enrich comprehension with relevant but unstated world\nknowledge. Here, we apply causal intervention techniques to recent transformer\nmodels to analyze performance on the Winograd Schema Challenge (WSC), where a\nsingle context cue shifts interpretation of an ambiguous pronoun. We identify a\nrelatively small circuit of attention heads that are responsible for\npropagating information from the context word that guides which of the\ncandidate noun phrases the pronoun ultimately attends to. We then compare how\nthis circuit behaves in a closely matched ``syntactic'' control where the\nsituation model is not strictly necessary. These analyses suggest distinct\npathways through which implicit situation models are constructed to guide\npronoun resolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamakoshi_T/0/1/0/all/0/1\">Takateru Yamakoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McClelland_J/0/1/0/all/0/1\">James L. McClelland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_A/0/1/0/all/0/1\">Adele E. Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawkins_R/0/1/0/all/0/1\">Robert D. Hawkins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory. (arXiv:2306.03901v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2306.03901","description":"<p>Large language models (LLMs) with memory are computationally universal.\nHowever, mainstream LLMs are not taking full advantage of memory, and the\ndesigns are heavily influenced by biological brains. Due to their approximate\nnature and proneness to the accumulation of errors, conventional neural memory\nmechanisms cannot support LLMs to simulate complex reasoning. In this paper, we\nseek inspiration from modern computer architectures to augment LLMs with\nsymbolic memory for complex multi-hop reasoning. Such a symbolic memory\nframework is instantiated as an LLM and a set of SQL databases, where the LLM\ngenerates SQL instructions to manipulate the SQL databases. We validate the\neffectiveness of the proposed memory framework on a synthetic dataset requiring\ncomplex reasoning. The project website is available at\nhttps://chatdatabase.github.io/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chenxu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1\">Chenzhuang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Simian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-06-07T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}