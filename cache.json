{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-11-06T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI. (arXiv:2311.01463v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01463","description":"<p>Large language models have proliferated across multiple domains in as short\nperiod of time. There is however hesitation in the medical and healthcare\ndomain towards their adoption because of issues like factuality, coherence, and\nhallucinations. Give the high stakes nature of healthcare, many researchers\nhave even cautioned against its usage until these issues are resolved. The key\nto the implementation and deployment of LLMs in healthcare is to make these\nmodels trustworthy, transparent (as much possible) and explainable. In this\npaper we describe the key elements in creating reliable, trustworthy, and\nunbiased models as a necessary condition for their adoption in healthcare.\nSpecifically we focus on the quantification, validation, and mitigation of\nhallucinations in the context in healthcare. Lastly, we discuss how the future\nof LLMs in healthcare may look like.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_M/0/1/0/all/0/1\">Muhammad Aurangzeb Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaramis_I/0/1/0/all/0/1\">Ilker Yaramis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_T/0/1/0/all/0/1\">Taposh Dutta Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Remember what you did so you know what to do next. (arXiv:2311.01468v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01468","description":"<p>We explore using a moderately sized large language model (GPT-J 6B\nparameters) to create a plan for a simulated robot to achieve 30 classes of\ngoals in ScienceWorld, a text game simulator for elementary science\nexperiments. Previously published empirical work claimed that large language\nmodels (LLMs) are a poor fit (Wang et al., 2022) compared to reinforcement\nlearning. Using the Markov assumption (a single previous step), the LLM\noutperforms the reinforcement learning-based approach by a factor of 1.4. When\nwe fill the LLM's input buffer with as many prior steps as possible,\nimprovement rises to 3.5x. Even when training on only 6.5% of the training\ndata, we observe a 2.2x improvement over the reinforcement-learning-based\napproach. Our experiments show that performance varies widely across the 30\nclasses of actions, indicating that averaging over tasks can hide significant\nperformance issues. In work contemporaneous with ours, Lin et al. (2023)\ndemonstrated a two-part approach (SwiftSage) that uses a small LLM (T5-large)\ncomplemented by OpenAI's massive LLMs to achieve outstanding results in\nScienceWorld. Our 6-B parameter, single-stage GPT-J matches the performance of\nSwiftSage's two-stage architecture when it incorporates GPT-3.5 turbo which has\n29-times more parameters than GPT-J.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ciosici_M/0/1/0/all/0/1\">Manuel R. Ciosici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedges_A/0/1/0/all/0/1\">Alex Hedges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanampati_Y/0/1/0/all/0/1\">Yash Kankanampati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1\">Justin Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freedman_M/0/1/0/all/0/1\">Marjorie Freedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weischedel_R/0/1/0/all/0/1\">Ralph Weischedel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Language Models to Detect Greenwashing. (arXiv:2311.01469v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01469","description":"<p>In recent years, climate change repercussions have increasingly captured\npublic interest. Consequently, corporations are emphasizing their environmental\nefforts in sustainability reports to bolster their public image. Yet, the\nabsence of stringent regulations in review of such reports allows potential\ngreenwashing. In this study, we introduce a novel methodology to train a\nlanguage model on generated labels for greenwashing risk. Our primary\ncontributions encompass: developing a mathematical formulation to quantify\ngreenwashing risk, a fine-tuned ClimateBERT model for this problem, and a\ncomparative analysis of results. On a test set comprising of sustainability\nreports, our best model achieved an average accuracy score of 86.34% and F1\nscore of 0.67, demonstrating that our methods show a promising direction of\nexploration for this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vinella_A/0/1/0/all/0/1\">Avalon Vinella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capetz_M/0/1/0/all/0/1\">Margaret Capetz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pattichis_R/0/1/0/all/0/1\">Rebecca Pattichis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chance_C/0/1/0/all/0/1\">Christina Chance</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1\">Reshmi Ghosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation Extraction from News Articles (RENA): A Tool for Epidemic Surveillance. (arXiv:2311.01472v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01472","description":"<p>Relation Extraction from News Articles (RENA) is a browser-based tool\ndesigned to extract key entities and their semantic relationships in English\nlanguage news articles related to infectious diseases. Constructed using the\nReact framework, this system presents users with an elegant and user-friendly\ninterface. It enables users to input a news article and select from a choice of\ntwo models to generate a comprehensive list of relations within the provided\ntext. As a result, RENA allows real-time parsing of news articles to extract\nkey information for epidemic surveillance, contributing to EPIWATCH, an\nopen-source intelligence-based epidemic warning system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jaeff Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dung_D/0/1/0/all/0/1\">Duong Dung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutchinson_D/0/1/0/all/0/1\">Danielle Hutchinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_Z/0/1/0/all/0/1\">Zubair Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Rosalie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_R/0/1/0/all/0/1\">Rebecca Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Aditya Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Samsung Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacIntyre_C/0/1/0/all/0/1\">C Raina MacIntyre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurdasani_D/0/1/0/all/0/1\">Deepti Gurdasani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Makes for Good Visual Instructions? Synthesizing Complex Visual Reasoning Instructions for Visual Instruction Tuning. (arXiv:2311.01487v1 [cs.CV])","link":"http://arxiv.org/abs/2311.01487","description":"<p>Visual instruction tuning is an essential approach to improving the zero-shot\ngeneralization capability of Multi-modal Large Language Models (MLLMs). A surge\nof visual instruction datasets with various focuses and characteristics have\nbeen proposed recently, enabling MLLMs to achieve surprising results on\nevaluation benchmarks. To develop more capable MLLMs, in this paper, we aim to\ninvestigate a more fundamental question: ``what makes for good visual\ninstructions?''. By conducting a comprehensive empirical study, we find that\ninstructions focused on complex visual reasoning tasks are particularly\neffective in improving the performance of MLLMs on evaluation benchmarks.\nBuilding upon this finding, we design a systematic approach to automatically\ncreating high-quality complex visual reasoning instructions. Our approach\nemploys a synthesis-complication-reformulation paradigm, leveraging multiple\nstages to gradually increase the complexity of the instructions while\nguaranteeing quality. Based on this approach, we create the synthetic visual\nreasoning instruction dataset consisting of 32K examples, namely ComVint, and\nfine-tune four MLLMs on it. Experimental results demonstrate that our dataset\nconsistently enhances the performance of all the compared MLLMs, e.g.,\nimproving the performance of MiniGPT-4 and BLIP-2 on MME-Cognition by 32.6% and\n28.8%, respectively. Our code and data are publicly available at the link:\nhttps://github.com/RUCAIBox/ComVint.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yifan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hangyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chuyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Mingchen Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Ruihua Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization. (arXiv:2311.01544v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01544","description":"<p>Large Language Models (LLMs) have reshaped natural language processing with\ntheir impressive capabilities. Their ever-increasing size, however, raised\nconcerns about their effective deployment and the need for LLM compressions.\nThis study introduces the Divergent Token metrics (DTMs), a novel approach for\nassessing compressed LLMs, addressing the limitations of traditional measures\nlike perplexity that fail to accurately reflect text generation quality. DTMs\nfocus on token divergence, providing deeper insights into the subtleties of\nmodel compression. Our results indicate that significant levels of precision\nand sparsity can be achieved without compromising text generation quality.\nMoreover, DTMs offers a more precise evaluation of each component's impact\nindividually. Utilizing the First Divergent Token metric (FDTM) in model\nsparsification reveals that nearly 20% of all components can be pruned over\n90%. In terms of quantization, the FDTM suggests that over 80% of parameters\ncan be straightforwardly transformed to int8 without special outlier\nmanagement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deiseroth_B/0/1/0/all/0/1\">Bj&#xf6;rn Deiseroth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuer_M/0/1/0/all/0/1\">Max Meuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritsch_N/0/1/0/all/0/1\">Nikolas Gritsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eichenberg_C/0/1/0/all/0/1\">Constantin Eichenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1\">Patrick Schramowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assenmacher_M/0/1/0/all/0/1\">Matthias A&#xdf;enmacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instruction Distillation Makes Large Language Models Efficient Zero-shot Rankers. (arXiv:2311.01555v1 [cs.IR])","link":"http://arxiv.org/abs/2311.01555","description":"<p>Recent studies have demonstrated the great potential of Large Language Models\n(LLMs) serving as zero-shot relevance rankers. The typical approach involves\nmaking comparisons between pairs or lists of documents. Although effective,\nthese listwise and pairwise methods are not efficient and also heavily rely on\nintricate prompt engineering. To tackle this problem, we introduce a novel\ninstruction distillation method. The key idea is to distill the pairwise\nranking ability of open-sourced LLMs to a simpler but more efficient pointwise\nranking. Specifically, given the same LLM, we first rank documents using the\neffective pairwise approach with complex instructions, and then distill the\nteacher predictions to the pointwise approach with simpler instructions.\nEvaluation results on the BEIR, TREC, and ReDial datasets demonstrate that\ninstruction distillation can improve efficiency by 10 to 100x and also enhance\nthe ranking performance of LLMs. Furthermore, our approach surpasses the\nperformance of existing supervised methods like monoT5 and is on par with the\nstate-of-the-art zero-shot methods. The code to reproduce our results is\navailable at www.github.com/sunnweiwei/RankGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Lingyong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuaiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dawei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preserving the knowledge of long clinical texts using aggregated ensembles of large language models. (arXiv:2311.01571v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01571","description":"<p>Clinical texts, such as admission notes, discharge summaries, and progress\nnotes, contain rich and valuable information that can be used for various\nclinical outcome prediction tasks. However, applying large language models,\nsuch as BERT-based models, to clinical texts poses two major challenges: the\nlimitation of input length and the diversity of data sources. This paper\nproposes a novel method to preserve the knowledge of long clinical texts using\naggregated ensembles of large language models. Unlike previous studies which\nuse model ensembling or text aggregation methods separately, we combine\nensemble learning with text aggregation and train multiple large language\nmodels on two clinical outcome tasks: mortality prediction and length of stay\nprediction. We show that our method can achieve better results than baselines,\nensembling, and aggregation individually, and can improve the performance of\nlarge language models while handling long inputs and diverse datasets. We\nconduct extensive experiments on the admission notes from the MIMIC-III\nclinical database by combining multiple unstructured and high-dimensional\ndatasets, demonstrating our method's effectiveness and superiority over\nexisting approaches. We also provide a comprehensive analysis and discussion of\nour results, highlighting our method's applications and limitations for future\nresearch in the domain of clinical healthcare. The results and analysis of this\nstudy is supportive of our method assisting in clinical healthcare systems by\nenabling clinical decision-making with robust performance overcoming the\nchallenges of long text inputs and varied datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Mohammad Junayed Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noor_S/0/1/0/all/0/1\">Suhra Noor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Ashrafuzzaman Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaReVision: Meta-Learning with Retrieval for Visually Grounded Compositional Concept Acquisition. (arXiv:2311.01580v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01580","description":"<p>Humans have the ability to learn novel compositional concepts by recalling\nand generalizing primitive concepts acquired from past experiences. Inspired by\nthis observation, in this paper, we propose MetaReVision, a retrieval-enhanced\nmeta-learning model to address the visually grounded compositional concept\nlearning problem. The proposed MetaReVision consists of a retrieval module and\na meta-learning module which are designed to incorporate retrieved primitive\nconcepts as a supporting set to meta-train vision-anguage models for grounded\ncompositional concept recognition. Through meta-learning from episodes\nconstructed by the retriever, MetaReVision learns a generic compositional\nrepresentation that can be fast updated to recognize novel compositional\nconcepts. We create CompCOCO and CompFlickr to benchmark the grounded\ncompositional concept learning. Our experimental results show that MetaReVision\noutperforms other competitive baselines and the retrieval module plays an\nimportant role in this compositional learning process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangyue Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kordjamshidi_P/0/1/0/all/0/1\">Parisa Kordjamshidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faithful and Robust Local Interpretability for Textual Predictions. (arXiv:2311.01605v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01605","description":"<p>Interpretability is essential for machine learning models to be trusted and\ndeployed in critical domains. However, existing methods for interpreting text\nmodels are often complex, lack solid mathematical foundations, and their\nperformance is not guaranteed. In this paper, we propose FRED (Faithful and\nRobust Explainer for textual Documents), a novel method for interpreting\npredictions over text. FRED identifies key words in a document that\nsignificantly impact the prediction when removed. We establish the reliability\nof FRED through formal definitions and theoretical analyses on interpretable\nclassifiers. Additionally, our empirical evaluation against state-of-the-art\nmethods demonstrates the effectiveness of FRED in providing insights into text\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lopardo_G/0/1/0/all/0/1\">Gianluigi Lopardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precioso_F/0/1/0/all/0/1\">Frederic Precioso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garreau_D/0/1/0/all/0/1\">Damien Garreau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KG-FRUS: a Novel Graph-based Dataset of 127 Years of US Diplomatic Relations. (arXiv:2311.01606v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01606","description":"<p>In the current paper, we present the KG-FRUS dataset, comprised of more than\n300,000 US government diplomatic documents encoded in a Knowledge Graph (KG).\nWe leverage the data of the Foreign Relations of the United States (FRUS)\n(available as XML files) to extract information about the documents and the\nindividuals and countries mentioned within them. We use the extracted entities,\nand associated metadata, to create a graph-based dataset. Further, we\nsupplement the created KG with additional entities and relations from Wikidata.\nThe relations in the KG capture the synergies and dynamics required to study\nand understand the complex fields of diplomacy, foreign relations, and\npolitics. This goes well beyond a simple collection of documents which neglects\nthe relations between entities in the text. We showcase a range of\npossibilities of the current dataset by illustrating different approaches to\nprobe the KG. In the paper, we exemplify how to use a query language to answer\nsimple research questions and how to use graph algorithms such as Node2Vec and\nPageRank, that benefit from the complete graph structure. More importantly, the\nchosen structure provides total flexibility for continuously expanding and\nenriching the graph. Our solution is general, so the proposed pipeline for\nbuilding the KG can encode other original corpora of time-dependent and complex\nphenomena. Overall, we present a mechanism to create KG databases providing a\nmore versatile representation of time-dependent related text data and a\nparticular application to the all-important FRUS database.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ozsoy_G/0/1/0/all/0/1\">G&#xf6;kberk &#xd6;zsoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salamanca_L/0/1/0/all/0/1\">Luis Salamanca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Connelly_M/0/1/0/all/0/1\">Matthew Connelly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hicks_R/0/1/0/all/0/1\">Raymond Hicks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Cruz_F/0/1/0/all/0/1\">Fernando P&#xe9;rez-Cruz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FLAP: Fast Language-Audio Pre-training. (arXiv:2311.01615v1 [cs.SD])","link":"http://arxiv.org/abs/2311.01615","description":"<p>We propose Fast Language-Audio Pre-training (FLAP), a self-supervised\napproach that efficiently and effectively learns aligned audio and language\nrepresentations through masking, contrastive learning and reconstruction. For\nefficiency, FLAP randomly drops audio spectrogram tokens, focusing solely on\nthe remaining ones for self-supervision. Through inter-modal contrastive\nlearning, FLAP learns to align paired audio and text representations in a\nshared latent space. Notably, FLAP leverages multiple augmented views via\nmasking for inter-modal contrast and learns to reconstruct the masked portion\nof audio tokens. Moreover, FLAP leverages large language models (LLMs) to\naugment the text inputs, contributing to improved performance. These approaches\nlead to more robust and informative audio-text representations, enabling FLAP\nto achieve state-of-the-art (SoTA) performance on audio-text retrieval tasks on\nAudioCaps (achieving 53.0% R@1) and Clotho (achieving 25.5% R@1).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1\">Ching-Feng Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Po-Yao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1\">Vasu Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gosh_G/0/1/0/all/0/1\">Gargi Gosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACQUIRED: A Dataset for Answering Counterfactual Questions In Real-Life Videos. (arXiv:2311.01620v1 [cs.CV])","link":"http://arxiv.org/abs/2311.01620","description":"<p>Multimodal counterfactual reasoning is a vital yet challenging ability for AI\nsystems. It involves predicting the outcomes of hypothetical circumstances\nbased on vision and language inputs, which enables AI models to learn from\nfailures and explore hypothetical scenarios. Despite its importance, there are\nonly a few datasets targeting the counterfactual reasoning abilities of\nmultimodal models. Among them, they only cover reasoning over synthetic\nenvironments or specific types of events (e.g. traffic collisions), making them\nhard to reliably benchmark the model generalization ability in diverse\nreal-world scenarios and reasoning dimensions. To overcome these limitations,\nwe develop a video question answering dataset, ACQUIRED: it consists of 3.9K\nannotated videos, encompassing a wide range of event types and incorporating\nboth first and third-person viewpoints, which ensures a focus on real-world\ndiversity. In addition, each video is annotated with questions that span three\ndistinct dimensions of reasoning, including physical, social, and temporal,\nwhich can comprehensively evaluate the model counterfactual abilities along\nmultiple aspects. We benchmark our dataset against several state-of-the-art\nlanguage-only and multimodal models and experimental results demonstrate a\nsignificant performance gap (&gt;13%) between models and humans. The findings\nsuggest that multimodal counterfactual reasoning remains an open challenge and\nACQUIRED is a comprehensive and reliable benchmark for inspiring future\nresearch in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Te-Lin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zi-Yi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qingyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_N/0/1/0/all/0/1\">Nischal Reddy Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freedman_M/0/1/0/all/0/1\">Marjorie Freedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weischedel_R/0/1/0/all/0/1\">Ralph M. Weischedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VQPy: An Object-Oriented Approach to Modern Video Analytics. (arXiv:2311.01623v1 [cs.CV])","link":"http://arxiv.org/abs/2311.01623","description":"<p>Video analytics is widely used in contemporary systems and services. At the\nforefront of video analytics are video queries that users develop to find\nobjects of particular interest. Building upon the insight that video objects\n(e.g., human, animals, cars, etc.), the center of video analytics, are similar\nin spirit to objects modeled by traditional object-oriented languages, we\npropose to develop an object-oriented approach to video analytics. This\napproach, named VQPy, consists of a frontend$\\unicode{x2015}$a Python variant\nwith constructs that make it easy for users to express video objects and their\ninteractions$\\unicode{x2015}$as well as an extensible backend that can\nautomatically construct and optimize pipelines based on video objects. We have\nimplemented and open-sourced VQPy, which has been productized in Cisco as part\nof its DeepVision framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenting Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hanchen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pengzhan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmanabhan_A/0/1/0/all/0/1\">Arthi Padmanabhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Latapie_H/0/1/0/all/0/1\">Hugo Latapie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Harry Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MARRS: Multimodal Reference Resolution System. (arXiv:2311.01650v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01650","description":"<p>Successfully handling context is essential for any dialog understanding task.\nThis context maybe be conversational (relying on previous user queries or\nsystem responses), visual (relying on what the user sees, for example, on their\nscreen), or background (based on signals such as a ringing alarm or playing\nmusic). In this work, we present an overview of MARRS, or Multimodal Reference\nResolution System, an on-device framework within a Natural Language\nUnderstanding system, responsible for handling conversational, visual and\nbackground context. In particular, we present different machine learning models\nto enable handing contextual queries; specifically, one to enable reference\nresolution, and one to handle context via query rewriting. We also describe how\nthese models complement each other to form a unified, coherent, lightweight\nsystem that can understand context while preserving user privacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ates_H/0/1/0/all/0/1\">Halim Cagri Ates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhargava_S/0/1/0/all/0/1\">Shruti Bhargava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiarui Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maddula_S/0/1/0/all/0/1\">Siddhardha Maddula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1\">Joel Ruben Antony Moniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nalamalapu_A/0/1/0/all/0/1\">Anil Kumar Nalamalapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_R/0/1/0/all/0/1\">Roman Hoang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozyildirim_M/0/1/0/all/0/1\">Melis Ozyildirim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Alkesh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piraviperumal_D/0/1/0/all/0/1\">Dhivya Piraviperumal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renkens_V/0/1/0/all/0/1\">Vincent Renkens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samal_A/0/1/0/all/0/1\">Ankit Samal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Thy Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_B/0/1/0/all/0/1\">Bo-Hsiang Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_R/0/1/0/all/0/1\">Rong Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plot Retrieval as an Assessment of Abstract Semantic Association. (arXiv:2311.01666v1 [cs.IR])","link":"http://arxiv.org/abs/2311.01666","description":"<p>Retrieving relevant plots from the book for a query is a critical task, which\ncan improve the reading experience and efficiency of readers. Readers usually\nonly give an abstract and vague description as the query based on their own\nunderstanding, summaries, or speculations of the plot, which requires the\nretrieval model to have a strong ability to estimate the abstract semantic\nassociations between the query and candidate plots. However, existing\ninformation retrieval (IR) datasets cannot reflect this ability well. In this\npaper, we propose Plot Retrieval, a labeled dataset to train and evaluate the\nperformance of IR models on the novel task Plot Retrieval. Text pairs in Plot\nRetrieval have less word overlap and more abstract semantic association, which\ncan reflect the ability of the IR models to estimate the abstract semantic\nassociation, rather than just traditional lexical or semantic matching.\nExtensive experiments across various lexical retrieval, sparse retrieval, dense\nretrieval, and cross-encoder methods compared with human studies on Plot\nRetrieval show current IR models still struggle in capturing abstract semantic\nassociation between texts. Plot Retrieval can be the benchmark for further\nresearch on the semantic association modeling ability of IR models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shicheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialogBench: Evaluating LLMs as Human-like Dialogue Systems. (arXiv:2311.01677v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01677","description":"<p>Large language models (LLMs) have achieved remarkable breakthroughs in new\ndialogue capabilities, refreshing human's impressions on dialogue systems. The\nlong-standing goal of dialogue systems is to be human-like enough to establish\nlong-term connections with users by satisfying the need for communication,\naffection and social belonging. Therefore, there has been an urgent need to\nevaluate LLMs as human-like dialogue systems. In this paper, we propose\nDialogBench, a dialogue evaluation benchmark that currently contains $12$\ndialogue tasks to assess the capabilities of LLMs as human-like dialogue\nsystems should have. Specifically, we prompt GPT-4 to generate evaluation\ninstances for each task. We first design the basic prompt based on widely-used\ndesign principles and further mitigate the existing biases to generate\nhigher-quality evaluation instances. Our extensive test over $28$ LLMs\n(including pre-trained and supervised instruction-tuning) shows that\ninstruction fine-tuning benefits improve the human likeness of LLMs to a\ncertain extent, but there is still much room to improve those capabilities for\nmost LLMs as human-like dialogue systems. In addition, experimental results\nalso indicate that LLMs perform differently in various abilities that\nhuman-like dialogue systems should have. We will publicly release DialogBench,\nalong with the associated evaluation code for the broader research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ou_J/0/1/0/all/0/1\">Jiao Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Junda Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Che Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yihong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fuzheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Di Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1\">Kun Gai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CASE: Commonsense-Augmented Score with an Expanded Answer Space. (arXiv:2311.01684v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01684","description":"<p>LLMs have demonstrated impressive zero-shot performance on NLP tasks thanks\nto the knowledge they acquired in their training. In multiple-choice QA tasks,\nthe LM probabilities are used as an imperfect measure of the plausibility of\neach answer choice. One of the major limitations of the basic score is that it\ntreats all words as equally important. We propose CASE, a Commonsense-Augmented\nScore with an Expanded Answer Space. CASE addresses this limitation by\nassigning importance weights for individual words based on their semantic\nrelations to other words in the input. The dynamic weighting approach\noutperforms basic LM scores, not only because it reduces noise from unimportant\nwords, but also because it informs the model of implicit commonsense knowledge\nthat may be useful for answering the question. We then also follow prior work\nin expanding the answer space by generating lexically-divergent answers that\nare conceptually-similar to the choices. When combined with answer space\nexpansion, our method outperforms strong baselines on 5 commonsense benchmarks.\nWe further show these two approaches are complementary and may be especially\nbeneficial when using smaller LMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenkai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1\">Sahithya Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Free Distillation of Language Model by Text-to-Text Transfer. (arXiv:2311.01689v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01689","description":"<p>Data-Free Knowledge Distillation (DFKD) plays a vital role in compressing the\nmodel when original training data is unavailable. Previous works for DFKD in\nNLP mainly focus on distilling encoder-only structures like BERT on\nclassification tasks, which overlook the notable progress of generative\nlanguage modeling. In this work, we propose a novel DFKD framework, namely\nDFKD-T$^{3}$, where the pretrained generative language model can also serve as\na controllable data generator for model compression. This novel framework\nDFKD-T$^{3}$ leads to an end-to-end learnable text-to-text framework to\ntransform the general domain corpus to compression-friendly task data,\ntargeting to improve both the \\textit{specificity} and \\textit{diversity}.\nExtensive experiments show that our method can boost the distillation\nperformance in various downstream tasks such as sentiment analysis, linguistic\nacceptability, and information extraction. Furthermore, we show that the\ngenerated texts can be directly used for distilling other language models and\noutperform the SOTA methods, making our method more appealing in a general DFKD\nsetting. Our code is available at\nhttps://gitee.com/mindspore/models/tree/master/research/nlp/DFKD\\_T3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1\">Zheyuan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinduo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hailin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tianyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinghua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Korean Text Classification Benchmark for Recognizing the Political Intents in Online Newspapers. (arXiv:2311.01712v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01712","description":"<p>Many users reading online articles in various magazines may suffer\nconsiderable difficulty in distinguishing the implicit intents in texts. In\nthis work, we focus on automatically recognizing the political intents of a\ngiven online newspaper by understanding the context of the text. To solve this\ntask, we present a novel Korean text classification dataset that contains\nvarious articles. We also provide deep-learning-based text classification\nbaseline models trained on the proposed dataset. Our dataset contains 12,000\nnews articles that may contain political intentions, from the politics section\nof six of the most representative newspaper organizations in South Korea. All\nthe text samples are labeled simultaneously in two aspects (1) the level of\npolitical orientation and (2) the level of pro-government. To the best of our\nknowledge, our paper is the most large-scale Korean news dataset that contains\nlong text and addresses multi-task classification problems. We also train\nrecent state-of-the-art (SOTA) language models that are based on transformer\narchitectures and demonstrate that the trained models show decent text\nclassification performance. All the codes, datasets, and trained models are\navailable at https://github.com/Kdavid2355/KoPolitic-Benchmark-Dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Beomjune Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">Eunsun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_D/0/1/0/all/0/1\">Dongbin Na</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of Benchmarking Chinese Aspect Sentiment Quad Prediction. (arXiv:2311.01713v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01713","description":"<p>Aspect sentiment quad prediction (ASQP) is a critical subtask of aspect-level\nsentiment analysis. Current ASQP datasets are characterized by their small size\nand low quadruple density, which hinders technical development. To expand\ncapacity, we construct two large Chinese ASQP datasets crawled from multiple\nonline platforms. The datasets hold several significant characteristics: larger\nsize (each with 10,000+ samples) and rich aspect categories, more words per\nsentence, and higher density than existing ASQP datasets. Moreover, we are the\nfirst to evaluate the performance of Generative Pre-trained Transformer (GPT)\nseries models on ASQP and exhibit potential issues. The experiments with\nstate-of-the-art ASQP baselines underscore the need to explore additional\ntechniques to address ASQP, as well as the importance of further investigation\ninto methods to improve the performance of GPTs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Junxian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haiqin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junpeng_Y/0/1/0/all/0/1\">Ye Junpeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_H/0/1/0/all/0/1\">Hao Mou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models. (arXiv:2311.01732v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01732","description":"<p>Large Language Models (LLMs) have significantly advanced the field of Natural\nLanguage Processing (NLP), but their lack of interpretability has been a major\nconcern. Current methods for interpreting LLMs are post hoc, applied after\ninference time, and have limitations such as their focus on low-level features\nand lack of explainability at higher level text units. In this work, we\nintroduce proto-lm, a prototypical network-based white-box framework that\nallows LLMs to learn immediately interpretable embeddings during the\nfine-tuning stage while maintaining competitive performance. Our method's\napplicability and interpretability are demonstrated through experiments on a\nwide range of NLP tasks, and our results indicate a new possibility of creating\ninterpretable models without sacrificing performance. This novel approach to\ninterpretability in LLMs can pave the way for more interpretable models without\nthe need to sacrifice performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sean Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassanpour_S/0/1/0/all/0/1\">Saeed Hassanpour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAC$^3$: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency. (arXiv:2311.01740v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01740","description":"<p>Hallucination detection is a critical step toward understanding the\ntrustworthiness of modern language models (LMs). To achieve this goal, we\nre-examine existing detection approaches based on the self-consistency of LMs\nand uncover two types of hallucinations resulting from 1) question-level and 2)\nmodel-level, which cannot be effectively identified through self-consistency\ncheck alone. Building upon this discovery, we propose a novel sampling-based\nmethod, i.e., semantic-aware cross-check consistency (SAC$^3$) that expands on\nthe principle of self-consistency checking. Our SAC$^3$ approach incorporates\nadditional mechanisms to detect both question-level and model-level\nhallucinations by leveraging advances including semantically equivalent\nquestion perturbation and cross-model response consistency checking. Through\nextensive and systematic empirical analysis, we demonstrate that SAC$^3$\noutperforms the state of the art in detecting both non-factual and factual\nstatements across multiple question-answering and open-domain generation\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuohang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_K/0/1/0/all/0/1\">Kamalika Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malin_B/0/1/0/all/0/1\">Bradley A. Malin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sricharan Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EmojiLM: Modeling the New Emoji Language. (arXiv:2311.01751v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01751","description":"<p>With the rapid development of the internet, online social media welcomes\npeople with different backgrounds through its diverse content. The increasing\nusage of emoji becomes a noticeable trend thanks to emoji's rich information\nbeyond cultural or linguistic borders. However, the current study on emojis is\nlimited to single emoji prediction and there are limited data resources\navailable for further study of the interesting linguistic phenomenon. To this\nend, we synthesize a large text-emoji parallel corpus, Text2Emoji, from a large\nlanguage model. Based on the parallel corpus, we distill a sequence-to-sequence\nmodel, EmojiLM, which is specialized in the text-emoji bidirectional\ntranslation. Extensive experiments on public benchmarks and human evaluation\ndemonstrate that our proposed model outperforms strong baselines and the\nparallel corpus benefits emoji-related downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Letian Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Indo LEGO-ABSA: A Multitask Generative Aspect Based Sentiment Analysis for Indonesian Language. (arXiv:2311.01757v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01757","description":"<p>Aspect-based sentiment analysis is a method in natural language processing\naimed at identifying and understanding sentiments related to specific aspects\nof an entity. Aspects are words or phrases that represent an aspect or\nattribute of a particular entity. Previous research has utilized generative\npre-trained language models to perform aspect-based sentiment analysis.\nLEGO-ABSA is one framework that has successfully employed generative\npre-trained language models in aspect-based sentiment analysis, particularly in\nEnglish. LEGO-ABSA uses a multitask learning and prompting approach to enhance\nmodel performance. However, the application of this approach has not been done\nin the context of Bahasa Indonesia. Therefore, this research aims to implement\nthe multitask learning and prompting approach in aspect-based sentiment\nanalysis for Bahasa Indonesia using generative pre-trained language models. In\nthis study, the Indo LEGO-ABSA model is developed, which is an aspect-based\nsentiment analysis model utilizing generative pre-trained language models and\ntrained with multitask learning and prompting. Indo LEGO-ABSA is trained with a\nhotel domain dataset in the Indonesian language. The obtained results include\nan f1-score of 79.55% for the Aspect Sentiment Triplet Extraction task, 86.09%\nfor Unified Aspect-based Sentiment Analysis, 79.85% for Aspect Opinion Pair\nExtraction, 87.45% for Aspect Term Extraction, and 88.09% for Opinion Term\nExtraction. Indo LEGO-ABSA adopts the LEGO-ABSA framework that employs the T5\nmodel, specifically mT5, by applying multitask learning to train all tasks\nwithin aspect-based sentiment analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suchrady_R/0/1/0/all/0/1\">Randy Zakya Suchrady</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purwarianti_A/0/1/0/all/0/1\">Ayu Purwarianti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation. (arXiv:2311.01766v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01766","description":"<p>Mis- and disinformation online have become a major societal problem as major\nsources of online harms of different kinds. One common form of mis- and\ndisinformation is out-of-context (OOC) information, where different pieces of\ninformation are falsely associated, e.g., a real image combined with a false\ntextual caption or a misleading textual description. Although some past studies\nhave attempted to defend against OOC mis- and disinformation through external\nevidence, they tend to disregard the role of different pieces of evidence with\ndifferent stances. Motivated by the intuition that the stance of evidence\nrepresents a bias towards different detection results, we propose a stance\nextraction network (SEN) that can extract the stances of different pieces of\nmulti-modal evidence in a unified framework. Moreover, we introduce a\nsupport-refutation score calculated based on the co-occurrence relations of\nnamed entities into the textual SEN. Extensive experiments on a public\nlarge-scale dataset demonstrated that our proposed method outperformed the\nstate-of-the-art baselines, with the best model achieving a performance gain of\n3.2% in accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1\">Weidong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shujun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion. (arXiv:2311.01767v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01767","description":"<p>Recent evaluations of Large Language Models (LLMs) have centered around\ntesting their zero-shot/few-shot capabilities for basic natural language tasks\nand their ability to translate instructions into tool APIs. However, the\nevaluation of LLMs utilizing complex tools to finish multi-turn, multi-modal\ninstructions in a complex multi-modal environment has not been investigated. To\naddress this gap, we introduce the PowerPoint Task Completion (PPTC) benchmark\nto assess LLMs' ability to create and edit PPT files based on user\ninstructions. It contains 279 multi-turn sessions covering diverse topics and\nhundreds of instructions involving multi-modal operations. We also propose the\nPPTX-Match Evaluation System that evaluates if LLMs finish the instruction\nbased on the prediction file rather than the label API sequence, thus it\nsupports various LLM-generated API sequences. We measure 3 closed LLMs and 6\nopen-source LLMs. The results show that GPT-4 outperforms other LLMs with\n75.1\\% accuracy in single-turn dialogue testing but faces challenges in\ncompleting entire sessions, achieving just 6\\% session accuracy. We find three\nmain error causes in our benchmark: error accumulation in the multi-turn\nsession, long PPT template processing, and multi-modality perception. These\npose great challenges for future LLM and agent systems. We release the data,\ncode, and evaluation system of PPTC at \\url{https://github.com/gydpku/PPTC}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yiduo Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zekai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yaobo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_D/0/1/0/all/0/1\">Duan Nan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UP4LS: User Profile Constructed by Multiple Attributes for Enhancing Linguistic Steganalysis. (arXiv:2311.01775v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01775","description":"<p>Linguistic steganalysis (LS) tasks aim to effectively detect stegos generated\nby linguistic steganography. Existing LS methods overlook the distinctive user\ncharacteristics, leading to weak performance in social networks. The limited\noccurrence of stegos further complicates detection. In this paper, we propose\nthe UP4LS, a novel framework with the User Profile for enhancing LS\nperformance. Specifically, by delving into post content, we explore user\nattributes like writing habits, psychological states, and focal areas, thereby\nbuilding the user profile for LS. For each attribute, we design the identified\nfeature extraction module. The extracted features are mapped to\nhigh-dimensional user features via deep-learning networks from existing\nmethods. Then the language model is employed to extract content features. The\nuser and content features are integrated to optimize feature representation.\nDuring the training phase, we prioritize the distribution of stegos.\nExperiments demonstrate that UP4LS can significantly enhance the performance of\nexisting methods, and an overall accuracy improvement of nearly 25%. In\nparticular, the improvement is especially pronounced with fewer stego samples.\nAdditionally, UP4LS also sets the stage for studies on related tasks,\nencouraging extensive applications on LS tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Ruiqi Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianyi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation in Traditional Chinese Medicine. (arXiv:2311.01786v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01786","description":"<p>Pre-training and fine-tuning have emerged as a promising paradigm across\nvarious natural language processing (NLP) tasks. The effectiveness of\npretrained large language models (LLM) has witnessed further enhancement,\nholding potential for applications in the field of medicine, particularly in\nthe context of Traditional Chinese Medicine (TCM). However, the application of\nthese general models to specific domains often yields suboptimal results,\nprimarily due to challenges like lack of domain knowledge, unique objectives,\nand computational efficiency. Furthermore, their effectiveness in specialized\ndomains, such as Traditional Chinese Medicine, requires comprehensive\nevaluation. To address the above issues, we propose a novel domain specific\nTCMDA (TCM Domain Adaptation) approach, efficient pre-training with\ndomain-specific corpus. Specifically, we first construct a large TCM-specific\ncorpus, TCM-Corpus-1B, by identifying domain keywords and retreving from\ngeneral corpus. Then, our TCMDA leverages the LoRA which freezes the pretrained\nmodel's weights and uses rank decomposition matrices to efficiently train\nspecific dense layers for pre-training and fine-tuning, efficiently aligning\nthe model with TCM-related tasks, namely TCM-GPT-7B. We further conducted\nextensive experiments on two TCM tasks, including TCM examination and TCM\ndiagnosis. TCM-GPT-7B archived the best performance across both datasets,\noutperforming other models by relative increments of 17% and 12% in accuracy,\nrespectively. To the best of our knowledge, our study represents the pioneering\nvalidation of domain adaptation of a large language model with 7 billion\nparameters in TCM domain. We will release both TCMCorpus-1B and TCM-GPT-7B\nmodel once accepted to facilitate interdisciplinary development in TCM and NLP,\nserving as the foundation for further study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guoxing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaohong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AFPQ: Asymmetric Floating Point Quantization for LLMs. (arXiv:2311.01792v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01792","description":"<p>Large language models (LLMs) show great performance in various tasks, but\nface deployment challenges from limited memory capacity and bandwidth. Low-bit\nweight quantization can save memory and accelerate inference. Although\nfloating-point (FP) formats show good performance in LLM quantization, they\ntend to perform poorly with small group sizes or sub-4 bits. We find the reason\nis that the absence of asymmetry in previous FP quantization makes it\nunsuitable for handling asymmetric value distribution of LLM weight tensors. In\nthis work, we propose asymmetric FP quantization (AFPQ), which sets separate\nscales for positive and negative values. Our method leads to large accuracy\nimprovements and can be easily plugged into other quantization methods,\nincluding GPTQ and AWQ, for better performance. Besides, no additional storage\nis needed compared with asymmetric integer (INT) quantization. The code is\navailable at https://github.com/zhangsichengsjtu/AFPQ.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yijia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shijie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1\">Dayou Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jianyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Ting Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Ningyi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Framing Bias with Polarity Minimization Loss. (arXiv:2311.01817v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01817","description":"<p>Framing bias plays a significant role in exacerbating political polarization\nby distorting the perception of actual events. Media outlets with divergent\npolitical stances often use polarized language in their reporting of the same\nevent. We propose a new loss function that encourages the model to minimize the\npolarity difference between the polarized input articles to reduce framing\nbias. Specifically, our loss is designed to jointly optimize the model to map\npolarity ends bidirectionally. Our experimental results demonstrate that\nincorporating the proposed polarity minimization loss leads to a substantial\nreduction in framing bias when compared to a BART-based multi-document\nsummarization model. Notably, we find that the effectiveness of this approach\nis most pronounced when the model is trained to minimize the polarity loss\nassociated with informational framing bias (i.e., skewed selection of\ninformation to report).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bang_Y/0/1/0/all/0/1\">Yejin Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimalist Grammar: Construction without Overgeneration. (arXiv:2311.01820v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01820","description":"<p>In this paper we give instructions on how to write a minimalist grammar (MG).\nIn order to present the instructions as an algorithm, we use a variant of\ncontext free grammars (CFG) as an input format. We can exclude overgeneration,\nif the CFG has no recursion, i.e. no non-terminal can (indirectly) derive to a\nright-hand side containing itself. The constructed MGs utilize licensors/-ees\nas a special way of exception handling. A CFG format for a derivation\n$A\\_eats\\_B\\mapsto^* peter\\_eats\\_apples$, where $A$ and $B$ generate noun\nphrases, normally leads to overgeneration, e.\\,g., $i\\_eats\\_apples$. In order\nto avoid overgeneration, a CFG would need many non-terminal symbols and rules,\nthat mainly produce the same word, just to handle exceptions. In our MGs\nhowever, we can summarize CFG rules that produce the same word in one item and\nhandle exceptions by a proper distribution of licensees/-ors. The difficulty\nwith this technique is that in most generations the majority of licensees/-ors\nis not needed, but still has to be triggered somehow. We solve this problem\nwith $\\epsilon$-items called \\emph{adapters}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maier_I/0/1/0/all/0/1\">Isidor Konrad Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhn_J/0/1/0/all/0/1\">Johannes Kuhn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beisegel_J/0/1/0/all/0/1\">Jesse Beisegel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_Liebl_M/0/1/0/all/0/1\">Markus Huber-Liebl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolff_M/0/1/0/all/0/1\">Matthias Wolff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models to the Rescue: Reducing the Complexity in Scientific Workflow Development Using ChatGPT. (arXiv:2311.01825v1 [cs.DC])","link":"http://arxiv.org/abs/2311.01825","description":"<p>Scientific workflow systems are increasingly popular for expressing and\nexecuting complex data analysis pipelines over large datasets, as they offer\nreproducibility, dependability, and scalability of analyses by automatic\nparallelization on large compute clusters. However, implementing workflows is\ndifficult due to the involvement of many black-box tools and the deep\ninfrastructure stack necessary for their execution. Simultaneously,\nuser-supporting tools are rare, and the number of available examples is much\nlower than in classical programming languages. To address these challenges, we\ninvestigate the efficiency of Large Language Models (LLMs), specifically\nChatGPT, to support users when dealing with scientific workflows. We performed\nthree user studies in two scientific domains to evaluate ChatGPT for\ncomprehending, adapting, and extending workflows. Our results indicate that\nLLMs efficiently interpret workflows but achieve lower performance for\nexchanging components or purposeful workflow extensions. We characterize their\nlimitations in these challenging scenarios and suggest future research\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanger_M/0/1/0/all/0/1\">Mario S&#xe4;nger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mecquenem_N/0/1/0/all/0/1\">Ninon De Mecquenem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewinska_K/0/1/0/all/0/1\">Katarzyna Ewa Lewi&#x144;ska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bountris_V/0/1/0/all/0/1\">Vasilis Bountris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehmann_F/0/1/0/all/0/1\">Fabian Lehmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leser_U/0/1/0/all/0/1\">Ulf Leser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosch_T/0/1/0/all/0/1\">Thomas Kosch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FAME: Flexible, Scalable Analogy Mappings Engine. (arXiv:2311.01860v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01860","description":"<p>Analogy is one of the core capacities of human cognition; when faced with new\nsituations, we often transfer prior experience from other domains. Most work on\ncomputational analogy relies heavily on complex, manually crafted input. In\nthis work, we relax the input requirements, requiring only names of entities to\nbe mapped. We automatically extract commonsense representations and use them to\nidentify a mapping between the entities. Unlike previous works, our framework\ncan handle partial analogies and suggest new entities to be added. Moreover,\nour method's output is easily interpretable, allowing for users to understand\nwhy a specific mapping was chosen.\n</p>\n<p>Experiments show that our model correctly maps 81.2% of classical 2x2 analogy\nproblems (guess level=50%). On larger problems, it achieves 77.8% accuracy\n(mean guess level=13.1%). In another experiment, we show our algorithm\noutperforms human performance, and the automatic suggestions of new entities\nresemble those suggested by humans. We hope this work will advance\ncomputational analogy by paving the way to more flexible, realistic input\nrequirements, with broader applicability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jacob_S/0/1/0/all/0/1\">Shahar Jacob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shani_C/0/1/0/all/0/1\">Chen Shani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1\">Dafna Shahaf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$R^3$-NL2GQL: A Hybrid Models Approach for for Accuracy Enhancing and Hallucinations Mitigation. (arXiv:2311.01862v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01862","description":"<p>While current NL2SQL tasks constructed using Foundation Models have achieved\ncommendable results, their direct application to Natural Language to Graph\nQuery Language (NL2GQL) tasks poses challenges due to the significant\ndifferences between GQL and SQL expressions, as well as the numerous types of\nGQL. Our extensive experiments reveal that in NL2GQL tasks, larger Foundation\nModels demonstrate superior cross-schema generalization abilities, while\nsmaller Foundation Models struggle to improve their GQL generation capabilities\nthrough fine-tuning. However, after fine-tuning, smaller models exhibit better\nintent comprehension and higher grammatical accuracy. Diverging from rule-based\nand slot-filling techniques, we introduce R3-NL2GQL, which employs both smaller\nand larger Foundation Models as reranker, rewriter and refiner. The approach\nharnesses the comprehension ability of smaller models for information reranker\nand rewriter, and the exceptional generalization and generation capabilities of\nlarger models to transform input natural language queries and code structure\nschema into any form of GQLs. Recognizing the lack of established datasets in\nthis nascent domain, we have created a bilingual dataset derived from graph\ndatabase documentation and some open-source Knowledge Graphs (KGs). We tested\nour approach on this dataset and the experimental results showed that delivers\npromising performance and robustness.Our code and dataset is available at\nhttps://github.com/zhiqix/NL2GQL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">He Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Siyu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Liuzhi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinlin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_C/0/1/0/all/0/1\">Chuanjun Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_G/0/1/0/all/0/1\">Guangnan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_H/0/1/0/all/0/1\">Hongfeng Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SortNet: Learning To Rank By a Neural-Based Sorting Algorithm. (arXiv:2311.01864v1 [cs.LG])","link":"http://arxiv.org/abs/2311.01864","description":"<p>The problem of relevance ranking consists of sorting a set of objects with\nrespect to a given criterion. Since users may prefer different relevance\ncriteria, the ranking algorithms should be adaptable to the user needs. Two\nmain approaches exist in literature for the task of learning to rank: 1) a\nscore function, learned by examples, which evaluates the properties of each\nobject yielding an absolute relevance value that can be used to order the\nobjects or 2) a pairwise approach, where a \"preference function\" is learned\nusing pairs of objects to define which one has to be ranked first. In this\npaper, we present SortNet, an adaptive ranking algorithm which orders objects\nusing a neural network as a comparator. The neural network training set\nprovides examples of the desired ordering between pairs of items and it is\nconstructed by an iterative procedure which, at each iteration, adds the most\ninformative training examples. Moreover, the comparator adopts a connectionist\narchitecture that is particularly suited for implementing a preference\nfunction. We also prove that such an architecture has the universal\napproximation property and can implement a wide class of functions. Finally,\nthe proposed algorithm is evaluated on the LETOR dataset showing promising\nperformances in comparison with other state of the art algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rigutini_L/0/1/0/all/0/1\">Leonardo Rigutini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papini_T/0/1/0/all/0/1\">Tiziano Papini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maggini_M/0/1/0/all/0/1\">Marco Maggini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarselli_F/0/1/0/all/0/1\">Franco Scarselli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Concept-Aware Large Language Models. (arXiv:2311.01866v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01866","description":"<p>Concepts play a pivotal role in various human cognitive functions, including\nlearning, reasoning and communication. However, there is very little work on\nendowing machines with the ability to form and reason with concepts. In\nparticular, state-of-the-art large language models (LLMs) work at the level of\ntokens, not concepts.\n</p>\n<p>In this work, we analyze how well contemporary LLMs capture human concepts\nand their structure. We then discuss ways to develop concept-aware LLMs, taking\nplace at different stages of the pipeline. We sketch a method for pretraining\nLLMs using concepts, and also explore the simpler approach that uses the output\nof existing LLMs. Despite its simplicity, our proof-of-concept is shown to\nbetter match human intuition, as well as improve the robustness of predictions.\nThese preliminary results underscore the promise of concept-aware LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shani_C/0/1/0/all/0/1\">Chen Shani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vreeken_J/0/1/0/all/0/1\">Jilles Vreeken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1\">Dafna Shahaf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-EuP: The Multilingual European Parliament Dataset for Analysis of Bias in Information Retrieval. (arXiv:2311.01870v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01870","description":"<p>We present Multi-EuP, a new multilingual benchmark dataset, comprising 22K\nmulti-lingual documents collected from the European Parliament, spanning 24\nlanguages. This dataset is designed to investigate fairness in a multilingual\ninformation retrieval (IR) context to analyze both language and demographic\nbias in a ranking context. It boasts an authentic multilingual corpus,\nfeaturing topics translated into all 24 languages, as well as cross-lingual\nrelevance judgments. Furthermore, it offers rich demographic information\nassociated with its documents, facilitating the study of demographic bias. We\nreport the effectiveness of Multi-EuP for benchmarking both monolingual and\nmultilingual IR. We also conduct a preliminary experiment on language bias\ncaused by the choice of tokenization strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinrui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Black-Box Adversarial Attacks on Neural Text Detectors. (arXiv:2311.01873v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01873","description":"<p>Neural text detectors are models trained to detect whether a given text was\ngenerated by a language model or written by a human. In this paper, we\ninvestigate three simple and resource-efficient strategies (parameter tweaking,\nprompt engineering, and character-level mutations) to alter texts generated by\nGPT-3.5 that are unsuspicious or unnoticeable for humans but cause\nmisclassification by neural text detectors. The results show that especially\nparameter tweaking and character-level mutations are effective strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fishchuk_V/0/1/0/all/0/1\">Vitalii Fishchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braun_D/0/1/0/all/0/1\">Daniel Braun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment Analysis through LLM Negotiations. (arXiv:2311.01876v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01876","description":"<p>A standard paradigm for sentiment analysis is to rely on a singular LLM and\nmakes the decision in a single round under the framework of in-context\nlearning. This framework suffers the key disadvantage that the single-turn\noutput generated by a single LLM might not deliver the perfect decision, just\nas humans sometimes need multiple attempts to get things right. This is\nespecially true for the task of sentiment analysis where deep reasoning is\nrequired to address the complex linguistic phenomenon (e.g., clause\ncomposition, irony, etc) in the input.\n</p>\n<p>To address this issue, this paper introduces a multi-LLM negotiation\nframework for sentiment analysis. The framework consists of a reasoning-infused\ngenerator to provide decision along with rationale, a explanation-deriving\ndiscriminator to evaluate the credibility of the generator. The generator and\nthe discriminator iterate until a consensus is reached. The proposed framework\nnaturally addressed the aforementioned challenge, as we are able to take the\ncomplementary abilities of two LLMs, have them use rationale to persuade each\nother for correction.\n</p>\n<p>Experiments on a wide range of sentiment analysis benchmarks (SST-2, Movie\nReview, Twitter, yelp, amazon, IMDB) demonstrate the effectiveness of proposed\napproach: it consistently yields better performances than the ICL baseline\nacross all benchmarks, and even superior performances to supervised baselines\non the Twitter and movie review datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Indicative Summarization of Long Discussions. (arXiv:2311.01882v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01882","description":"<p>Online forums encourage the exchange and discussion of different stances on\nmany topics. Not only do they provide an opportunity to present one's own\narguments, but may also gather a broad cross-section of others' arguments.\nHowever, the resulting long discussions are difficult to overview. This paper\npresents a novel unsupervised approach using large language models (LLMs) to\ngenerating indicative summaries for long discussions that basically serve as\ntables of contents. Our approach first clusters argument sentences, generates\ncluster labels as abstractive summaries, and classifies the generated cluster\nlabels into argumentation frames resulting in a two-level summary. Based on an\nextensively optimized prompt engineering approach, we evaluate 19~LLMs for\ngenerative cluster labeling and frame classification. To evaluate the\nusefulness of our indicative summaries, we conduct a purpose-driven user study\nvia a new visual interface called Discussion Explorer: It shows that our\nproposed indicative summaries serve as a convenient navigation tool to explore\nlong discussions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1\">Shahbaz Syed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwabe_D/0/1/0/all/0/1\">Dominik Schwabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Khatib_K/0/1/0/all/0/1\">Khalid Al-Khatib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BoschAI @ PLABA 2023: Leveraging Edit Operations in End-to-End Neural Sentence Simplification. (arXiv:2311.01907v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01907","description":"<p>Automatic simplification can help laypeople to comprehend complex scientific\ntext. Language models are frequently applied to this task by translating from\ncomplex to simple language. In this paper, we describe our system based on\nLlama 2, which ranked first in the PLABA shared task addressing the\nsimplification of biomedical text. We find that the large portion of shared\ntokens between input and output leads to weak training signals and\nconservatively editing models. To mitigate these issues, we propose\nsentence-level and token-level loss weights. They give higher weight to\nmodified tokens, indicated by edit distance and edit operations, respectively.\nWe conduct an empirical evaluation on the PLABA dataset and find that both\napproaches lead to simplifications closer to those created by human annotators\n(+1.8% / +3.5% SARI), simpler language (-1 / -1.1 FKGL) and more edits (1.6x /\n1.8x edit distance) compared to the same model fine-tuned with standard cross\nentropy. We furthermore show that the hyperparameter $\\lambda$ in token-level\nloss weights can be used to control the edit distance and the simplicity level\n(FKGL).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Knappich_V/0/1/0/all/0/1\">Valentin Knappich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1\">Simon Razniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_A/0/1/0/all/0/1\">Annemarie Friedrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review. (arXiv:2311.01918v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01918","description":"<p>With the rapid development of artificial intelligence, large language models\n(LLMs) have shown promising capabilities in mimicking human-level language\ncomprehension and reasoning. This has sparked significant interest in applying\nLLMs to enhance various aspects of healthcare, ranging from medical education\nto clinical decision support. However, medicine involves multifaceted data\nmodalities and nuanced reasoning skills, presenting challenges for integrating\nLLMs. This paper provides a comprehensive review on the applications and\nimplications of LLMs in medicine. It begins by examining the fundamental\napplications of general-purpose and specialized LLMs, demonstrating their\nutilities in knowledge retrieval, research support, clinical workflow\nautomation, and diagnostic assistance. Recognizing the inherent multimodality\nof medicine, the review then focuses on multimodal LLMs, investigating their\nability to process diverse data types like medical imaging and EHRs to augment\ndiagnostic accuracy. To address LLMs' limitations regarding personalization and\ncomplex clinical reasoning, the paper explores the emerging development of\nLLM-powered autonomous agents for healthcare. Furthermore, it summarizes the\nevaluation methodologies for assessing LLMs' reliability and safety in medical\ncontexts. Overall, this review offers an extensive analysis on the\ntransformative potential of LLMs in modern medicine. It also highlights the\npivotal need for continuous optimizations and ethical oversight before these\nmodels can be effectively integrated into clinical practice. Visit\nhttps://github.com/mingze-yuan/Awesome-LLM-Healthcare for an accompanying\nGitHub repository containing latest papers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mingze Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_P/0/1/0/all/0/1\">Peng Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jiajia Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bin Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling. (arXiv:2311.01927v1 [cs.LG])","link":"http://arxiv.org/abs/2311.01927","description":"<p>Linear Recurrence has proven to be a powerful tool for modeling long\nsequences efficiently. In this work, we show that existing models fail to take\nfull advantage of its potential. Motivated by this finding, we develop\nGateLoop, a foundational sequence model that generalizes linear recurrent\nmodels such as S4, S5, LRU and RetNet, by employing data-controlled state\ntransitions. Utilizing this theoretical advance, GateLoop empirically\noutperforms existing models for auto-regressive language modeling. Our method\ncomes with a low-cost $O(l)$ recurrent mode and an efficient $O(l \\log_{2} l)$\nparallel mode making use of highly optimized associative scan implementations.\nFurthermore, we derive an $O(l^2)$ surrogate attention mode, revealing\nremarkable implications for Transformer and recently proposed architectures.\nSpecifically, we prove that our approach can be interpreted as providing\ndata-controlled relative-positional information to Attention. While many\nexisting models solely rely on data-controlled cumulative sums for context\naggregation, our findings suggest that incorporating data-controlled complex\ncumulative products may be a crucial step towards more powerful sequence\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katsch_T/0/1/0/all/0/1\">Tobias Katsch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing Temporal Dynamic Knowledge Graphs from Interactive Text-based Games. (arXiv:2311.01928v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01928","description":"<p>In natural language processing, interactive text-based games serve as a test\nbed for interactive AI systems. Prior work has proposed to play text-based\ngames by acting based on discrete knowledge graphs constructed by the Discrete\nGraph Updater (DGU) to represent the game state from the natural language\ndescription. While DGU has shown promising results with high interpretability,\nit suffers from lower knowledge graph accuracy due to its lack of temporality\nand limited generalizability to complex environments with objects with the same\nlabel. In order to address DGU's weaknesses while preserving its high\ninterpretability, we propose the Temporal Discrete Graph Updater (TDGU), a\nnovel neural network model that represents dynamic knowledge graphs as a\nsequence of timestamped graph events and models them using a temporal point\nbased graph neural network. Through experiments on the dataset collected from a\ntext-based game TextWorld, we show that TDGU outperforms the baseline DGU. We\nfurther show the importance of temporal information for TDGU's performance\nthrough an ablation study and demonstrate that TDGU has the ability to\ngeneralize to more complex environments with objects with the same label. All\nthe relevant code can be found at\n\\url{https://github.com/yukw777/temporal-discrete-graph-updater}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Keunwoo Peter Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive tasks. (arXiv:2311.01949v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01949","description":"<p>In-context learning (ICL) ability has emerged with the increasing scale of\nlarge language models (LLMs), enabling them to learn input-label mappings from\ndemonstrations and perform well on downstream tasks. However, under the\nstandard ICL setting, LLMs may sometimes neglect query-related information in\ndemonstrations, leading to incorrect predictions. To address this limitation,\nwe propose a new paradigm called Hint-enhanced In-Context Learning (HICL) to\nexplore the power of ICL in open-domain question answering, an important form\nin knowledge-intensive tasks. HICL leverages LLMs' reasoning ability to extract\nquery-related knowledge from demonstrations, then concatenates the knowledge to\nprompt LLMs in a more explicit way. Furthermore, we track the source of this\nknowledge to identify specific examples, and introduce a Hint-related Example\nRetriever (HER) to select informative examples for enhanced demonstrations. We\nevaluate HICL with HER on 3 open-domain QA benchmarks, and observe average\nperformance gains of 2.89 EM score and 2.52 F1 score on gpt-3.5-turbo, 7.62 EM\nscore and 7.27 F1 score on LLaMA-2-Chat-7B compared with standard setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qingyan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_X/0/1/0/all/0/1\">Xinzhe Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chufan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Too Much Information: Keeping Training Simple for BabyLMs. (arXiv:2311.01955v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01955","description":"<p>This paper details the work of the University of Groningen for the BabyLM\nChallenge. We follow the idea that, like babies, language models should be\nintroduced to simpler concepts first and build off of that knowledge to\nunderstand more complex concepts. We examine this strategy of\nsimple-then-complex through a variety of lenses, namely context size,\nvocabulary, and overall linguistic complexity of the data. We find that only\none, context size, is truly beneficial to training a language model. However\nthis simple change to context size gives us improvements of 2 points on average\non (Super)GLUE tasks, 1 point on MSGS tasks, and 12\\% on average on BLiMP\ntasks. Our context-limited model outperforms the baseline that was trained on\n10$\\times$ the amount of data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Edman_L/0/1/0/all/0/1\">Lukas Edman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bylinina_L/0/1/0/all/0/1\">Lisa Bylinina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Make Your LLM an Evaluation Benchmark Cheater. (arXiv:2311.01964v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01964","description":"<p>Large language models~(LLMs) have greatly advanced the frontiers of\nartificial intelligence, attaining remarkable improvement in model capacity. To\nassess the model performance, a typical approach is to construct evaluation\nbenchmarks for measuring the ability level of LLMs in different aspects.\nDespite that a number of high-quality benchmarks have been released, the\nconcerns about the appropriate use of these benchmarks and the fair comparison\nof different models are increasingly growing. Considering these concerns, in\nthis paper, we discuss the potential risk and impact of inappropriately using\nevaluation benchmarks and misleadingly interpreting the evaluation results.\nSpecially, we focus on a special issue that would lead to inappropriate\nevaluation, \\ie \\emph{benchmark leakage}, referring that the data related to\nevaluation sets is occasionally used for model training. This phenomenon now\nbecomes more common since pre-training data is often prepared ahead of model\ntest. We conduct extensive experiments to study the effect of benchmark\nleverage, and find that it can dramatically boost the evaluation results, which\nwould finally lead to an unreliable assessment of model performance. To improve\nthe use of existing evaluation benchmarks, we finally present several\nguidelines for both LLM developers and benchmark maintainers. We hope this work\ncan draw attention to appropriate training and evaluation of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yutao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhipeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wentong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The language of prompting: What linguistic properties make a prompt successful?. (arXiv:2311.01967v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01967","description":"<p>The latest generation of LLMs can be prompted to achieve impressive zero-shot\nor few-shot performance in many NLP tasks. However, since performance is highly\nsensitive to the choice of prompts, considerable effort has been devoted to\ncrowd-sourcing prompts or designing methods for prompt optimisation. Yet, we\nstill lack a systematic understanding of how linguistic properties of prompts\ncorrelate with task performance. In this work, we investigate how LLMs of\ndifferent sizes, pre-trained and instruction-tuned, perform on prompts that are\nsemantically equivalent, but vary in linguistic structure. We investigate both\ngrammatical properties such as mood, tense, aspect and modality, as well as\nlexico-semantic variation through the use of synonyms. Our findings contradict\nthe common assumption that LLMs achieve optimal performance on lower perplexity\nprompts that reflect language use in pretraining or instruction-tuning data.\nPrompts transfer poorly between datasets or models, and performance cannot\ngenerally be explained by perplexity, word frequency, ambiguity or prompt\nlength. Based on our results, we put forward a proposal for a more robust and\ncomprehensive evaluation standard for prompting research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leidinger_A/0/1/0/all/0/1\">Alina Leidinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rooij_R/0/1/0/all/0/1\">Robert van Rooij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1\">Ekaterina Shutova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProSG: Using Prompt Synthetic Gradients to Alleviate Prompt Forgetting of RNN-like Language Models. (arXiv:2311.01981v1 [cs.CL])","link":"http://arxiv.org/abs/2311.01981","description":"<p>RNN-like language models are getting renewed attention from NLP researchers\nin recent years and several models have made significant progress, which\ndemonstrates performance comparable to traditional transformers. However, due\nto the recurrent nature of RNNs, this kind of language model can only store\ninformation in a set of fixed-length state vectors. As a consequence, they\nstill suffer from forgetfulness though after a lot of improvements and\noptimizations, when given complex instructions or prompts. As the prompted\ngeneration is the main and most concerned function of LMs, solving the problem\nof forgetting in the process of generation is no wonder of vital importance. In\nthis paper, focusing on easing the prompt forgetting during generation, we\nproposed an architecture to teach the model memorizing prompt during generation\nby synthetic gradient. To force the model to memorize the prompt, we derive the\nstates that encode the prompt, then transform it into model parameter\nmodification using low-rank gradient approximation, which hard-codes the prompt\ninto model parameters temporarily. We construct a dataset for experiments, and\nthe results have demonstrated the effectiveness of our method in solving the\nproblem of forgetfulness in the process of prompted generation. We will release\nall the code upon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haotian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kunming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_C/0/1/0/all/0/1\">Cheng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Sixian Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vicinal Risk Minimization for Few-Shot Cross-lingual Transfer in Abusive Language Detection. (arXiv:2311.02025v1 [cs.CL])","link":"http://arxiv.org/abs/2311.02025","description":"<p>Cross-lingual transfer learning from high-resource to medium and low-resource\nlanguages has shown encouraging results. However, the scarcity of resources in\ntarget languages remains a challenge. In this work, we resort to data\naugmentation and continual pre-training for domain adaptation to improve\ncross-lingual abusive language detection. For data augmentation, we analyze two\nexisting techniques based on vicinal risk minimization and propose MIXAG, a\nnovel data augmentation method which interpolates pairs of instances based on\nthe angle of their representations. Our experiments involve seven languages\ntypologically distinct from English and three different domains. The results\nreveal that the data augmentation strategies can enhance few-shot cross-lingual\nabusive language detection. Specifically, we observe that consistently in all\ntarget languages, MIXAG improves significantly in multidomain and multilingual\nenvironments. Finally, we show through an error analysis how the domain\nadaptation can favour the class of abusive texts (reducing false negatives),\nbut at the same time, declines the precision of the abusive language detection\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarracen_G/0/1/0/all/0/1\">Gretel Liz De la Pe&#xf1;a Sarrac&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosso_P/0/1/0/all/0/1\">Paolo Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litschko_R/0/1/0/all/0/1\">Robert Litschko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponzetto_S/0/1/0/all/0/1\">Simone Paolo Ponzetto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Post Turing: Mapping the landscape of LLM Evaluation. (arXiv:2311.02049v1 [cs.CL])","link":"http://arxiv.org/abs/2311.02049","description":"<p>In the rapidly evolving landscape of Large Language Models (LLMs),\nintroduction of well-defined and standardized evaluation methodologies remains\na crucial challenge. This paper traces the historical trajectory of LLM\nevaluations, from the foundational questions posed by Alan Turing to the modern\nera of AI research. We categorize the evolution of LLMs into distinct periods,\neach characterized by its unique benchmarks and evaluation criteria. As LLMs\nincreasingly mimic human-like behaviors, traditional evaluation proxies, such\nas the Turing test, have become less reliable. We emphasize the pressing need\nfor a unified evaluation system, given the broader societal implications of\nthese models. Through an analysis of common evaluation methodologies, we\nadvocate for a qualitative shift in assessment approaches, underscoring the\nimportance of standardization and objective criteria. This work serves as a\ncall for the AI community to collaboratively address the challenges of LLM\nevaluation, ensuring their reliability, fairness, and societal benefit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounded Intuition of GPT-Vision's Abilities with Scientific Images. (arXiv:2311.02069v1 [cs.CL])","link":"http://arxiv.org/abs/2311.02069","description":"<p>GPT-Vision has impressed us on a range of vision-language tasks, but it comes\nwith the familiar new challenge: we have little idea of its capabilities and\nlimitations. In our study, we formalize a process that many have instinctively\nbeen trying already to develop \"grounded intuition\" of this new model. Inspired\nby the recent movement away from benchmarking in favor of example-driven\nqualitative evaluation, we draw upon grounded theory and thematic analysis in\nsocial science and human-computer interaction to establish a rigorous framework\nfor qualitative evaluation in natural language processing. We use our technique\nto examine alt text generation for scientific figures, finding that GPT-Vision\nis particularly sensitive to prompting, counterfactual text in images, and\nrelative spatial relationships. Our method and analysis aim to help researchers\nramp up their own grounded intuitions of new models while exposing how\nGPT-Vision can be applied to make information more accessible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_A/0/1/0/all/0/1\">Alyssa Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Head_A/0/1/0/all/0/1\">Andrew Head</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoPaSul Manual -- Contour-based parametric and superpositional intonation stylization. (arXiv:1612.04765v12 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1612.04765","description":"<p>The purposes of the CoPaSul toolkit are (1) automatic prosodic annotation and\n(2) prosodic feature extraction from syllable to utterance level. CoPaSul\nstands for contour-based, parametric, superpositional intonation stylization.\nIn this framework intonation is represented as a superposition of global and\nlocal contours that are described parametrically in terms of polynomial\ncoefficients. On the global level (usually associated but not necessarily\nrestricted to intonation phrases) the stylization serves to represent register\nin terms of time-varying F0 level and range. On the local level (e.g. accent\ngroups), local contour shapes are described. From this parameterization several\nfeatures related to prosodic boundaries and prominence can be derived.\nFurthermore, by coefficient clustering prosodic contour classes can be obtained\nin a bottom-up way. Next to the stylization-based feature extraction also\nstandard F0 and energy measures (e.g. mean and variance) as well as rhythmic\naspects can be calculated. At the current state automatic annotation comprises:\nsegmentation into interpausal chunks, syllable nucleus extraction, and\nunsupervised localization of prosodic phrase boundaries and prominent\nsyllables. F0 and partly also energy feature sets can be derived for: standard\nmeasurements (as median and IQR), register in terms of F0 level and range,\nprosodic boundaries, local contour shapes, bottom-up derived contour classes,\nGestalt of accent groups in terms of their deviation from higher level prosodic\nunits, as well as for rhythmic aspects quantifying the relation between F0 and\nenergy contours and prosodic event rates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reichel_U/0/1/0/all/0/1\">Uwe D. Reichel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metaphorical User Simulators for Evaluating Task-oriented Dialogue Systems. (arXiv:2204.00763v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.00763","description":"<p>Task-oriented dialogue systems (TDSs) are assessed mainly in an offline\nsetting or through human evaluation. The evaluation is often limited to\nsingle-turn or is very time-intensive. As an alternative, user simulators that\nmimic user behavior allow us to consider a broad set of user goals to generate\nhuman-like conversations for simulated evaluation. Employing existing user\nsimulators to evaluate TDSs is challenging as user simulators are primarily\ndesigned to optimize dialogue policies for TDSs and have limited evaluation\ncapabilities. Moreover, the evaluation of user simulators is an open challenge.\n</p>\n<p>In this work, we propose a metaphorical user simulator for end-to-end TDS\nevaluation, where we define a simulator to be metaphorical if it simulates\nuser's analogical thinking in interactions with systems. We also propose a\ntester-based evaluation framework to generate variants, i.e., dialogue systems\nwith different capabilities. Our user simulator constructs a metaphorical user\nmodel that assists the simulator in reasoning by referring to prior knowledge\nwhen encountering new items. We estimate the quality of simulators by checking\nthe simulated interactions between simulators and variants. Our experiments are\nconducted using three TDS datasets. The proposed user simulator demonstrates\nbetter consistency with manual evaluation than an agenda-based simulator and a\nseq2seq model on three datasets; our tester framework demonstrates efficiency\nand has been tested on multiple tasks, such as conversational recommendation\nand e-commerce dialogues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shuyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Abstractive Timeline Summarisation using Preference-based Reinforcement Learning. (arXiv:2211.07596v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2211.07596","description":"<p>This paper introduces a novel pipeline for summarising timelines of events\nreported by multiple news sources. Transformer-based models for abstractive\nsummarisation generate coherent and concise summaries of long documents but can\nfail to outperform established extractive methods on specialised tasks such as\ntimeline summarisation (TLS). While extractive summaries are more faithful to\ntheir sources, they may be less readable and contain redundant or unnecessary\ninformation. This paper proposes a preference-based reinforcement learning\n(PBRL) method for adapting pretrained abstractive summarisers to TLS, which can\novercome the drawbacks of extractive timeline summaries. We define a compound\nreward function that learns from keywords of interest and pairwise preference\nlabels, which we use to fine-tune a pretrained abstractive summariser via\noffline reinforcement learning. We carry out both automated and human\nevaluation on three datasets, finding that our method outperforms a comparable\nextractive TLS method on two of the three benchmark datasets, and participants\nprefer our method's summaries to those of both the extractive TLS method and\nthe pretrained abstractive model. The method does not require expensive\nreference summaries and needs only a small number of preferences to align the\ngenerated summaries with human preferences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yuxuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simpson_E/0/1/0/all/0/1\">Edwin Simpson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model. (arXiv:2212.09146v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09146","description":"<p>Augmenting pretrained language models with retrievers has shown promise in\neffectively solving common NLP problems, such as language modeling and question\nanswering. In this paper, we evaluate the strengths and weaknesses of popular\nretriever-augmented language models, namely kNN-LM, REALM, DPR + FiD,\nContriever + ATLAS, and Contriever + Flan-T5, in reasoning over retrieved\nstatements across different tasks. Our findings indicate that the simple\nsimilarity metric employed by retrievers is insufficient for retrieving all the\nnecessary statements for reasoning. Additionally, the language models do not\nexhibit strong reasoning even when provided with only the required statements.\nFurthermore, when combined with imperfect retrievers, the performance of the\nlanguage models becomes even worse, e.g., Flan-T5's performance drops by 28.6%\nwhen retrieving 5 statements using Contriever. While larger language models\nimprove performance, there is still a substantial room for enhancement. Our\nfurther analysis indicates that multihop retrieve-and-read is promising for\nlarge language models like GPT-3.5, but does not generalize to other language\nmodels like Flan-T5-xxl.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+BehnamGhader_P/0/1/0/all/0/1\">Parishad BehnamGhader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miret_S/0/1/0/all/0/1\">Santiago Miret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why think step by step? Reasoning emerges from the locality of experience. (arXiv:2304.03843v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2304.03843","description":"<p>Humans have a powerful and mysterious capacity to reason. Working through a\nset of mental steps enables us to make inferences we would not be capable of\nmaking directly even though we get no additional data from the world.\nSimilarly, when large language models generate intermediate steps (a chain of\nthought) before answering a question, they often produce better answers than\nthey would directly. We investigate why and how chain-of-thought reasoning is\nuseful in language models, testing the hypothesis that reasoning is effective\nwhen training data consists of overlapping local clusters of variables that\ninfluence each other strongly. These training conditions enable the chaining of\naccurate local inferences to estimate relationships between variables that were\nnot seen together in training. We prove that there will exist a \"reasoning\ngap\", where reasoning through intermediate variables reduces bias, for the\nsimple case of an autoregressive density estimator trained on local samples\nfrom a chain-structured probabilistic model. We then test our hypothesis\nexperimentally in more complex models, training an autoregressive language\nmodel on samples from Bayes nets but only including a subset of variables in\neach sample. We test language models' ability to match conditional\nprobabilities with and without intermediate reasoning steps, finding that\nintermediate steps are only helpful when the training data is locally\nstructured with respect to dependencies between variables. The combination of\nlocally structured observations and reasoning is much more data-efficient than\ntraining on all variables. Our results illustrate how the effectiveness of\nreasoning step by step is rooted in the local statistical structure of the\ntraining data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prystawski_B/0/1/0/all/0/1\">Ben Prystawski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Michael Y. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah D. Goodman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v6 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2304.04370","description":"<p>Human Intelligence (HI) excels at combining basic skills to solve complex\ntasks. This capability is vital for Artificial Intelligence (AI) and should be\nembedded in comprehensive AI Agents, enabling them to harness expert models for\ncomplex task-solving towards Artificial General Intelligence (AGI). Large\nLanguage Models (LLMs) show promising learning and reasoning abilities, and can\neffectively use external models, tools, plugins, or APIs to tackle complex\nproblems. In this work, we introduce OpenAGI, an open-source AGI research and\ndevelopment platform designed for solving multi-step, real-world tasks.\nSpecifically, OpenAGI uses a dual strategy, integrating standard benchmark\ntasks for benchmarking and evaluation, and open-ended tasks including more\nexpandable models, tools, plugins, or APIs for creative problem-solving. Tasks\nare presented as natural language queries to the LLM, which then selects and\nexecutes appropriate models. We also propose a Reinforcement Learning from Task\nFeedback (RLTF) mechanism that uses task results to improve the LLM's\ntask-solving ability, which creates a self-improving AI feedback loop. While we\nacknowledge that AGI is a broad and multifaceted research challenge with no\nsingularly defined solution path, the integration of LLMs with domain-specific\nexpert models, inspired by mirroring the blend of general and specialized\nintelligence in humans, offers a promising approach towards AGI. We are\nopen-sourcing the OpenAGI project's code, dataset, benchmarks, evaluation\nmethods, and the UI demo to foster community involvement in AGI advancement:\nhttps://github.com/agiresearch/OpenAGI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wenyue Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_K/0/1/0/all/0/1\">Kai Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jianchao Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Juntao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zelong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction. (arXiv:2304.11015v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.11015","description":"<p>There is currently a significant gap between the performance of fine-tuned\nmodels and prompting approaches using Large Language Models (LLMs) on the\nchallenging task of text-to-SQL, as evaluated on datasets such as Spider. To\nimprove the performance of LLMs in the reasoning process, we study how\ndecomposing the task into smaller sub-tasks can be effective. In particular, we\nshow that breaking down the generation problem into sub-problems and feeding\nthe solutions of those sub-problems into LLMs can be an effective approach for\nsignificantly improving their performance. Our experiments with three LLMs show\nthat this approach consistently improves their simple few-shot performance by\nroughly 10%, pushing the accuracy of LLMs towards SOTA or surpassing it. On the\nholdout test set of Spider, the SOTA, in terms of execution accuracy, was 79.9\nand the new SOTA at the time of this writing using our approach is 85.3. Our\napproach with in-context learning beats many heavily fine-tuned models by at\nleast 5%. Additionally, when evaluated on the BIRD benchmark, our approach\nachieved an execution accuracy of 55.9%, setting a new SOTA on its holdout test\nset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pourreza_M/0/1/0/all/0/1\">Mohammadreza Pourreza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafiei_D/0/1/0/all/0/1\">Davood Rafiei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACCENT: An Automatic Event Commonsense Evaluation Metric for Open-Domain Dialogue Systems. (arXiv:2305.07797v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.07797","description":"<p>Commonsense reasoning is omnipresent in human communications and thus is an\nimportant feature for open-domain dialogue systems. However, evaluating\ncommonsense in dialogue systems is still an open challenge. We take the first\nstep by focusing on event commonsense that considers events and their\nrelations, and is crucial in both dialogues and general commonsense reasoning.\nWe propose ACCENT, an event commonsense evaluation metric empowered by\ncommonsense knowledge bases (CSKBs). ACCENT first extracts event-relation\ntuples from a dialogue, and then evaluates the response by scoring the tuples\nin terms of their compatibility with the CSKB. To evaluate ACCENT, we construct\nthe first public event commonsense evaluation dataset for open-domain\ndialogues. Our experiments show that ACCENT is an efficient metric for event\ncommonsense evaluation, which achieves higher correlations with human judgments\nthan existing baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghazarian_S/0/1/0/all/0/1\">Sarik Ghazarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yijia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Rujun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models. (arXiv:2305.13112v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13112","description":"<p>The recent success of large language models (LLMs) has shown great potential\nto develop more powerful conversational recommender systems (CRSs), which rely\non natural language conversations to satisfy user needs. In this paper, we\nembark on an investigation into the utilization of ChatGPT for conversational\nrecommendation, revealing the inadequacy of the existing evaluation protocol.\nIt might over-emphasize the matching with the ground-truth items or utterances\ngenerated by human annotators, while neglecting the interactive nature of being\na capable CRS. To overcome the limitation, we further propose an interactive\nEvaluation approach based on LLMs named iEvaLM that harnesses LLM-based user\nsimulators. Our evaluation approach can simulate various interaction scenarios\nbetween users and systems. Through the experiments on two publicly available\nCRS datasets, we demonstrate notable improvements compared to the prevailing\nevaluation protocol. Furthermore, we emphasize the evaluation of\nexplainability, and ChatGPT showcases persuasive explanation generation for its\nrecommendations. Our study contributes to a deeper comprehension of the\nuntapped potential of LLMs for CRSs and provides a more flexible and\neasy-to-use evaluation framework for future research endeavors. The codes and\ndata are publicly available at https://github.com/RUCAIBox/iEvaLM-CRS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xinyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference. (arXiv:2305.13484v3 [cs.DC] UPDATED)","link":"http://arxiv.org/abs/2305.13484","description":"<p>Autoregressive models, despite their commendable performance in a myriad of\ngenerative tasks, face challenges stemming from their inherently sequential\nstructure. Inference on these models, by design, harnesses a temporal\ndependency, where the current token's probability distribution is conditioned\non preceding tokens. This inherent characteristic severely impedes\ncomputational efficiency during inference as a typical inference request can\nrequire more than thousands of tokens, where generating each token requires a\nload of entire model weights, making the inference more memory-bound. The large\noverhead becomes profound in real deployment where requests arrive randomly,\nnecessitating various generation lengths. Existing solutions, such as dynamic\nbatching and concurrent instances, introduce significant response delays and\nbandwidth contention, falling short of achieving optimal latency and\nthroughput. To address these shortcomings, we propose Flover -- a temporal\nfusion framework for efficiently inferring multiple requests in parallel. We\ndeconstruct the general generation pipeline into pre-processing and token\ngeneration, and equip the framework with a dedicated work scheduler for fusing\nthe generation process temporally across all requests. By orchestrating the\ntoken-level parallelism, Flover exhibits optimal hardware efficiency and\nsignificantly spares the system resources. By further employing a fast buffer\nreordering algorithm that allows memory eviction of finished tasks, it brings\nover 11x inference speedup on GPT and 16x on LLAMA compared to the cutting-edge\nsolutions provided by NVIDIA FasterTransformer. Crucially, by leveraging the\nadvanced tensor parallel technique, Flover proves efficacious across diverse\ncomputational landscapes, from single-GPU setups to distributed scenarios,\nthereby offering robust performance optimization that adapts to variable use\ncases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jinghan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alnaasan_N/0/1/0/all/0/1\">Nawras Alnaasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafi_A/0/1/0/all/0/1\">Aamir Shafi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramoni_H/0/1/0/all/0/1\">Hari Subramoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K%2E_D/0/1/0/all/0/1\">Dhabaleswar K.</a> (DK) <a href=\"http://arxiv.org/find/cs/1/au:+Panda/0/1/0/all/0/1\">Panda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models. (arXiv:2305.13669v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13669","description":"<p>Large language models often necessitate grounding on external knowledge to\ngenerate faithful and reliable answers. Yet even with the correct groundings in\nthe reference, they can ignore them and rely on wrong groundings or their\ninherent biases to hallucinate when users, being largely unaware of the\nspecifics of the stored information, pose questions that might not directly\ncorrelate with the retrieved groundings. In this work, we formulate this\nknowledge alignment problem and introduce MixAlign, a framework that interacts\nwith both the human user and the knowledge base to obtain and integrate\nclarifications on how the user question relates to the stored information.\nMixAlign employs a language model to achieve automatic knowledge alignment and,\nif necessary, further enhances this alignment through human user\nclarifications. Experimental results highlight the crucial role of knowledge\nalignment in boosting model performance and mitigating hallucination, with\nimprovements noted up to 22.2% and 27.1% respectively. We also demonstrate the\neffectiveness of MixAlign in improving knowledge alignment by producing\nhigh-quality, user-centered clarifications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liangming Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junzhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender Biases in Automatic Evaluation Metrics for Image Captioning. (arXiv:2305.14711v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14711","description":"<p>Model-based evaluation metrics (e.g., CLIPScore and GPTScore) have\ndemonstrated decent correlations with human judgments in various language\ngeneration tasks. However, their impact on fairness remains largely unexplored.\nIt is widely recognized that pretrained models can inadvertently encode\nsocietal biases, thus employing these models for evaluation purposes may\ninadvertently perpetuate and amplify biases. For example, an evaluation metric\nmay favor the caption \"a woman is calculating an account book\" over \"a man is\ncalculating an account book,\" even if the image only shows male accountants. In\nthis paper, we conduct a systematic study of gender biases in model-based\nautomatic evaluation metrics for image captioning tasks. We start by curating a\ndataset comprising profession, activity, and object concepts associated with\nstereotypical gender associations. Then, we demonstrate the negative\nconsequences of using these biased metrics, including the inability to\ndifferentiate between biased and unbiased generations, as well as the\npropagation of biases to generation models through reinforcement learning.\nFinally, we present a simple and effective way to mitigate the metric bias\nwithout hurting the correlations with human judgments. Our dataset and\nframework lay the foundation for understanding the potential harm of\nmodel-based evaluation metrics, and facilitate future works to develop more\ninclusive evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Haoyi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zi-Yi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianlu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLMDet: A Third Party Large Language Models Generated Text Detection Tool. (arXiv:2305.15004v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.15004","description":"<p>Generated texts from large language models (LLMs) are remarkably close to\nhigh-quality human-authored text, raising concerns about their potential misuse\nin spreading false information and academic misconduct. Consequently, there is\nan urgent need for a highly practical detection tool capable of accurately\nidentifying the source of a given text. However, existing detection tools\ntypically rely on access to LLMs and can only differentiate between\nmachine-generated and human-authored text, failing to meet the requirements of\nfine-grained tracing, intermediary judgment, and rapid detection. Therefore, we\npropose LLMDet, a model-specific, secure, efficient, and extendable detection\ntool, that can source text from specific LLMs, such as GPT-2, OPT, LLaMA, and\nothers. In LLMDet, we record the next-token probabilities of salient n-grams as\nfeatures to calculate proxy perplexity for each LLM. By jointly analyzing the\nproxy perplexities of LLMs, we can determine the source of the generated text.\nExperimental results show that LLMDet yields impressive detection performance\nwhile ensuring speed and security, achieving 98.54% precision and x5.0 faster\nfor recognizing human-authored text. Additionally, LLMDet can effortlessly\nextend its detection capabilities to a new open-source model. We will provide\nan open-source tool at https://github.com/TrustedLLM/LLMDet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kangxi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Diffusion Models Vision-And-Language Reasoners?. (arXiv:2305.16397v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.16397","description":"<p>Text-conditioned image generation models have recently shown immense\nqualitative success using denoising diffusion processes. However, unlike\ndiscriminative vision-and-language models, it is a non-trivial task to subject\nthese diffusion-based generative models to automatic fine-grained quantitative\nevaluation of high-level phenomena such as compositionality. Towards this goal,\nwe perform two innovations. First, we transform diffusion-based models (in our\ncase, Stable Diffusion) for any image-text matching (ITM) task using a novel\nmethod called DiffusionITM. Second, we introduce the Generative-Discriminative\nEvaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language\ntasks, bias evaluation and detailed analysis. We find that Stable Diffusion +\nDiffusionITM is competitive on many tasks and outperforms CLIP on compositional\ntasks like like CLEVR and Winoground. We further boost its compositional\nperformance with a transfer setup by fine-tuning on MS-COCO while retaining\ngenerative capabilities. We also measure the stereotypical bias in diffusion\nmodels, and find that Stable Diffusion 2.1 is, for the most part, less biased\nthan Stable Diffusion 1.5. Overall, our results point in an exciting direction\nbringing discriminative and generative model evaluation closer. We will release\ncode and benchmark setup soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krojer_B/0/1/0/all/0/1\">Benno Krojer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poole_Dayan_E/0/1/0/all/0/1\">Elinor Poole-Dayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1\">Vikram Voleti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grammar Prompting for Domain-Specific Language Generation with Large Language Models. (arXiv:2305.19234v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.19234","description":"<p>Large language models (LLMs) can learn to perform a wide range of natural\nlanguage tasks from just a handful of in-context examples. However, for\ngenerating strings from highly structured languages (e.g., semantic parsing to\ncomplex domain-specific languages), it is challenging for the LLM to generalize\nfrom just a few exemplars. We propose \\emph{grammar prompting}, a simple\napproach to enable LLMs to use external knowledge and domain-specific\nconstraints, expressed through a grammar in Backus--Naur Form (BNF), during\nin-context learning. Grammar prompting augments each demonstration example with\na specialized grammar that is minimally sufficient for generating the\nparticular output example, where the specialized grammar is a subset of the\nfull DSL grammar. For inference, the LLM first predicts a BNF grammar given a\ntest input, and then generates the output according to the rules of the\ngrammar. Experiments demonstrate that grammar prompting can enable LLMs to\nperform competitively on a diverse set of DSL generation tasks, including\nsemantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and\nSMILES-based molecule generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bailin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saurous_R/0/1/0/all/0/1\">Rif A. Saurous</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Tuning Language Models with Advantage-Induced Policy Alignment. (arXiv:2306.02231v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.02231","description":"<p>Reinforcement learning from human feedback (RLHF) has emerged as a reliable\napproach to aligning large language models (LLMs) to human preferences. Among\nthe plethora of RLHF techniques, proximal policy optimization (PPO) is of the\nmost widely used methods. Despite its popularity, however, PPO may suffer from\nmode collapse, instability, and poor sample efficiency. We show that these\nissues can be alleviated by a novel algorithm that we refer to as\nAdvantage-Induced Policy Alignment (APA), which leverages a squared error loss\nfunction based on the estimated advantages. We demonstrate empirically that APA\nconsistently outperforms PPO in language tasks by a large margin, when a\nseparate reward model is employed as the evaluator. In addition, compared with\nPPO, APA offers a more stable form of control over the deviation from the\nmodel's initial policy, ensuring that the model improves its performance\nwithout collapsing to deterministic output. In addition to empirical results,\nwe also provide a theoretical justification supporting the design of our loss\nfunction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Banghua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_H/0/1/0/all/0/1\">Hiteshi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frujeri_F/0/1/0/all/0/1\">Felipe Vieira Frujeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Shi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jiantao Jiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guiding Language Models of Code with Global Context using Monitors. (arXiv:2306.10763v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.10763","description":"<p>Language models of code (LMs) work well when the surrounding code provides\nsufficient context. This is not true when it becomes necessary to use types,\nfunctionality or APIs defined elsewhere in the repository or a linked library,\nespecially those not seen during training. LMs suffer from limited awareness of\nsuch global context and end up hallucinating.\n</p>\n<p>Integrated development environments (IDEs) assist developers in understanding\nrepository context using static analysis. We extend this assistance, enjoyed by\ndevelopers, to LMs. We propose monitor-guided decoding (MGD) where a monitor\nuses static analysis to guide the decoding. We construct a repository-level\ndataset PragmaticCode for method-completion in Java and evaluate MGD on it. On\nmodels of varying parameter scale, by monitoring for type-consistent object\ndereferences, MGD consistently improves compilation rates and agreement with\nground truth. Further, LMs with fewer parameters, when augmented with MGD, can\noutperform larger LMs. With MGD, SantaCoder-1.1B achieves better compilation\nrate and next-identifier match than the much larger text-davinci-003 model.\n</p>\n<p>We also conduct a generalizability study to evaluate the ability of MGD to\ngeneralize to multiple programming languages (Java, C# and Rust), coding\nscenarios (e.g., correct number of arguments to method calls), and to enforce\nricher semantic constraints (e.g., stateful API protocols). Our data and\nimplementation are available at https://github.com/microsoft/monitors4codegen .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_L/0/1/0/all/0/1\">Lakshya A Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanade_A/0/1/0/all/0/1\">Aditya Kanade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Navin Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahiri_S/0/1/0/all/0/1\">Shuvendu K. Lahiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajamani_S/0/1/0/all/0/1\">Sriram K. Rajamani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT for GTFS: Benchmarking LLMs on GTFS Understanding and Retrieval. (arXiv:2308.02618v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2308.02618","description":"<p>The General Transit Feed Specification (GTFS) standard for publishing transit\ndata is ubiquitous. GTFS being tabular data, with information spread across\ndifferent files, necessitates specialized tools or packages to retrieve\ninformation. Concurrently, the use of Large Language Models(LLMs) for text and\ninformation retrieval is growing. The idea of this research is to see if the\ncurrent widely adopted LLMs (ChatGPT) are able to understand GTFS and retrieve\ninformation from GTFS using natural language instructions without explicitly\nproviding information. In this research, we benchmark OpenAI's GPT-3.5-Turbo\nand GPT-4 LLMs which are the backbone of ChatGPT. ChatGPT demonstrates a\nreasonable understanding of GTFS by answering 59.7% (GPT-3.5-Turbo) and 73.3%\n(GPT-4) of our multiple-choice questions (MCQ) correctly. Furthermore, we\nevaluated the LLMs on information extraction tasks using a filtered GTFS feed\ncontaining four routes. We found that program synthesis techniques outperformed\nzero-shot approaches, achieving up to 93% (90%) accuracy for simple queries and\n61% (41%) for complex ones using GPT-4 (GPT-3.5-Turbo).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Devunuri_S/0/1/0/all/0/1\">Saipraneeth Devunuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiam_S/0/1/0/all/0/1\">Shirin Qiam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehe_L/0/1/0/all/0/1\">Lewis Lehe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automating Behavioral Testing in Machine Translation. (arXiv:2309.02553v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.02553","description":"<p>Behavioral testing in NLP allows fine-grained evaluation of systems by\nexamining their linguistic capabilities through the analysis of input-output\nbehavior. Unfortunately, existing work on behavioral testing in Machine\nTranslation (MT) is currently restricted to largely handcrafted tests covering\na limited range of capabilities and languages. To address this limitation, we\npropose to use Large Language Models (LLMs) to generate a diverse set of source\nsentences tailored to test the behavior of MT models in a range of situations.\nWe can then verify whether the MT model exhibits the expected behavior through\nmatching candidate sets that are also generated using LLMs. Our approach aims\nto make behavioral testing of MT systems practical while requiring only minimal\nhuman effort. In our experiments, we apply our proposed evaluation framework to\nassess multiple available MT systems, revealing that while in general\npass-rates follow the trends observable from traditional accuracy-based\nmetrics, our method was able to uncover several important differences and\npotential bugs that go unnoticed when relying only on accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferrando_J/0/1/0/all/0/1\">Javier Ferrando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sperber_M/0/1/0/all/0/1\">Matthias Sperber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Setiawan_H/0/1/0/all/0/1\">Hendra Setiawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Telaar_D/0/1/0/all/0/1\">Dominic Telaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1\">Sa&#x161;a Hasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems. (arXiv:2310.05280v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.05280","description":"<p>Recent advancements in Large Language Models empower them to follow freeform\ninstructions, including imitating generic or specific demographic personas in\nconversations. We define generic personas to represent demographic groups, such\nas \"an Asian person\", whereas specific personas may take the form of specific\npopular Asian names like \"Yumi\". While the adoption of personas enriches user\nexperiences by making dialogue systems more engaging and approachable, it also\ncasts a shadow of potential risk by exacerbating social biases within model\nresponses, thereby causing societal harm through interactions with users. In\nthis paper, we systematically study \"persona biases\", which we define to be the\nsensitivity of dialogue models' harmful behaviors contingent upon the personas\nthey adopt. We categorize persona biases into biases in harmful expression and\nharmful agreement, and establish a comprehensive evaluation framework to\nmeasure persona biases in five aspects: Offensiveness, Toxic Continuation,\nRegard, Stereotype Agreement, and Toxic Agreement. Additionally, we propose to\ninvestigate persona biases by experimenting with UNIVERSALPERSONA, a\nsystematically constructed persona dataset encompassing various types of both\ngeneric and specific model personas. Through benchmarking on four different\nmodels -- including Blender, ChatGPT, Alpaca, and Vicuna -- our study uncovers\nsignificant persona biases in dialogue systems. Our findings also underscore\nthe pressing need to revisit the use of personas in dialogue agents to ensure\nsafe application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yixin Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jieyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Top-k Estimation Consolidates Disagreement between Feature Attribution Methods. (arXiv:2310.05619v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.05619","description":"<p>Feature attribution scores are used for explaining the prediction of a text\nclassifier to users by highlighting a k number of tokens. In this work, we\npropose a way to determine the number of optimal k tokens that should be\ndisplayed from sequential properties of the attribution scores. Our approach is\ndynamic across sentences, method-agnostic, and deals with sentence length bias.\nWe compare agreement between multiple methods and humans on an NLI task, using\nfixed k and dynamic k. We find that perturbation-based methods and Vanilla\nGradient exhibit highest agreement on most method--method and method--human\nagreement metrics with a static k. Their advantage over other methods\ndisappears with dynamic ks which mainly improve Integrated Gradient and\nGradientXInput. To our knowledge, this is the first evidence that sequential\nproperties of attribution scores are informative for consolidating attribution\nsignals for human interpretation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamp_J/0/1/0/all/0/1\">Jonathan Kamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beinborn_L/0/1/0/all/0/1\">Lisa Beinborn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fokkens_A/0/1/0/all/0/1\">Antske Fokkens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Glitter or Gold? Deriving Structured Insights from Sustainability Reports via Large Language Models. (arXiv:2310.05628v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.05628","description":"<p>Over the last decade, several regulatory bodies have started requiring the\ndisclosure of non-financial information from publicly listed companies, in\nlight of the investors' increasing attention to Environmental, Social, and\nGovernance (ESG) issues. Such information is publicly released in a variety of\nnon-structured and multi-modal documentation. Hence, it is not straightforward\nto aggregate and consolidate such data in a cohesive framework to further\nderive insights about sustainability practices across companies and markets.\nGiven these premises, it is natural to resort to Information Extraction (IE)\ntechniques to provide concise, informative, and actionable data to the\nstakeholders. Moving beyond traditional text processing techniques, in this\nwork we leverage Large Language Models (LLMs), along with the prominent\nin-context learning technique and the Retrieved Augmented Generation (RAG)\nparadigm, to extract semantically structured ESG-related information from\ncompanies' sustainability reports. We then adopt graph-based representations to\nconduct meaningful statistical, similarity and correlation analyses concerning\nthe ESG-related actions disclosed by companies in their sustainability reports.\nThese analyses unveiled that companies address ESG-related issues through\nseveral actions encompassing recognition, compliance, and partnerships;\nhighlighting the complexity and joint efforts needed to address them. Moreover,\ndisclosure similarities emerged among companies from the same region or sector.\nLastly, we investigate which factual aspects impact the most on companies' ESG\nscores using our findings and other company information. This analysis unveiled\nthat companies' disclosures affect ESG scores more than other financial or\ncompany characteristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bronzini_M/0/1/0/all/0/1\">Marco Bronzini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicolini_C/0/1/0/all/0/1\">Carlo Nicolini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1\">Bruno Lepri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passerini_A/0/1/0/all/0/1\">Andrea Passerini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staiano_J/0/1/0/all/0/1\">Jacopo Staiano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment. (arXiv:2310.08372v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.08372","description":"<p>Pretrained language models (PLMs) based knowledge-grounded dialogue systems\nare prone to generate responses that are factually inconsistent with the\nprovided knowledge source. In such inconsistent responses, the dialogue models\nfail to accurately express the external knowledge they rely upon. Inspired by\nprevious work which identified that feed-forward networks (FFNs) within\nTransformers are responsible for factual knowledge expressions, we investigate\ntwo methods to efficiently improve the factual expression capability {of FFNs}\nby knowledge enhancement and alignment respectively. We first propose\n\\textsc{K-Dial}, which {explicitly} introduces {extended FFNs in Transformers\nto enhance factual knowledge expressions} given the specific patterns of\nknowledge-grounded dialogue inputs. Additionally, we apply the reinforcement\nlearning for factual consistency (RLFC) method to implicitly adjust FFNs'\nexpressions in responses by aligning with gold knowledge for the factual\nconsistency preference. To comprehensively assess the factual consistency and\ndialogue quality of responses, we employ extensive automatic measures and human\nevaluations including sophisticated fine-grained NLI-based metrics.\nExperimental results on WoW and CMU\\_DoG datasets demonstrate that our methods\nefficiently enhance the ability of the FFN module to convey factual knowledge,\nvalidating the efficacy of improving factual consistency for knowledge-grounded\ndialogue systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_B/0/1/0/all/0/1\">Boyang Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Kelly is a Warm Person, Joseph is a Role Model\": Gender Biases in LLM-Generated Reference Letters. (arXiv:2310.09219v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.09219","description":"<p>Large Language Models (LLMs) have recently emerged as an effective tool to\nassist individuals in writing various types of content, including professional\ndocuments such as recommendation letters. Though bringing convenience, this\napplication also introduces unprecedented fairness concerns. Model-generated\nreference letters might be directly used by users in professional scenarios. If\nunderlying biases exist in these model-constructed letters, using them without\nscrutinization could lead to direct societal harms, such as sabotaging\napplication success rates for female applicants. In light of this pressing\nissue, it is imminent and necessary to comprehensively study fairness issues\nand associated harms in this real-world use case. In this paper, we critically\nexamine gender biases in LLM-generated reference letters. Drawing inspiration\nfrom social science findings, we design evaluation methods to manifest biases\nthrough 2 dimensions: (1) biases in language style and (2) biases in lexical\ncontent. We further investigate the extent of bias propagation by analyzing the\nhallucination bias of models, a term that we define to be bias exacerbation in\nmodel-hallucinated contents. Through benchmarking evaluation on 2 popular LLMs-\nChatGPT and Alpaca, we reveal significant gender biases in LLM-generated\nrecommendation letters. Our findings not only warn against using LLMs for this\napplication without scrutinization, but also illuminate the importance of\nthoroughly studying hidden biases and harms in LLM-generated professional\ndocuments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yixin Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1\">George Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garimella_A/0/1/0/all/0/1\">Aparna Garimella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can ChatGPT Perform Reasoning Using the IRAC Method in Analyzing Legal Scenarios Like a Lawyer?. (arXiv:2310.14880v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.14880","description":"<p>Large Language Models (LLMs), such as ChatGPT, have drawn a lot of attentions\nrecently in the legal domain due to its emergent ability to tackle a variety of\nlegal tasks. However, it is still unknown if LLMs are able to analyze a legal\ncase and perform reasoning in the same manner as lawyers. Therefore, we\nconstructed a novel corpus consisting of scenarios pertain to Contract Acts\nMalaysia and Australian Social Act for Dependent Child. ChatGPT is applied to\nperform analysis on the corpus using the IRAC method, which is a framework\nwidely used by legal professionals for organizing legal analysis. Each scenario\nin the corpus is annotated with a complete IRAC analysis in a semi-structured\nformat so that both machines and legal professionals are able to interpret and\nunderstand the annotations. In addition, we conducted the first empirical\nassessment of ChatGPT for IRAC analysis in order to understand how well it\naligns with the analysis of legal professionals. Our experimental results shed\nlights on possible future research directions to improve alignments between\nLLMs and legal experts in terms of legal reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xiaoxi Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soon_L/0/1/0/all/0/1\">Lay-Ki Soon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trakic_A/0/1/0/all/0/1\">Adnan Trakic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1\">Terry Yue Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emerton_P/0/1/0/all/0/1\">Patrick Charles Emerton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grant_G/0/1/0/all/0/1\">Genevieve Grant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Pretraining Data from Large Language Models. (arXiv:2310.16789v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.16789","description":"<p>Although large language models (LLMs) are widely deployed, the data used to\ntrain them is rarely disclosed. Given the incredible scale of this data, up to\ntrillions of tokens, it is all but certain that it includes potentially\nproblematic text such as copyrighted materials, personally identifiable\ninformation, and test data for widely reported reference benchmarks. However,\nwe currently have no way to know which data of these types is included or in\nwhat proportions. In this paper, we study the pretraining data detection\nproblem: given a piece of text and black-box access to an LLM without knowing\nthe pretraining data, can we determine if the model was trained on the provided\ntext? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that\nuses data created before and after model training to support gold truth\ndetection. We also introduce a new detection method Min-K% Prob based on a\nsimple hypothesis: an unseen example is likely to contain a few outlier words\nwith low probabilities under the LLM, while a seen example is less likely to\nhave words with such low probabilities. Min-K% Prob can be applied without any\nknowledge about the pretraining corpus or any additional training, departing\nfrom previous detection methods that require training a reference model on data\nthat is similar to the pretraining data. Moreover, our experiments demonstrate\nthat Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous\nmethods. We apply Min-K% Prob to three real-world scenarios, copyrighted book\ndetection, contaminated downstream example detection and privacy auditing of\nmachine unlearning, and find it a consistently effective solution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weijia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajith_A/0/1/0/all/0/1\">Anirudh Ajith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1\">Mengzhou Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yangsibo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daogao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blevins_T/0/1/0/all/0/1\">Terra Blevins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization. (arXiv:2310.20033v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.20033","description":"<p>Large Language Models (LLMs) like the GPT and LLaMA families have\ndemonstrated exceptional capabilities in capturing and condensing critical\ncontextual information and achieving state-of-the-art performance in the\nsummarization task. However, community concerns about these models'\nhallucination issues continue to rise. LLMs sometimes generate factually\nhallucinated summaries, which can be extremely harmful in the clinical domain\nNLP tasks (e.g., clinical note summarization), where factually incorrect\nstatements can lead to critically erroneous diagnoses. Fine-tuning LLMs using\nhuman feedback has shown the promise of aligning LLMs to be factually\nconsistent during generation, but such training procedure requires high-quality\nhuman-annotated data, which can be extremely expensive to get in the clinical\ndomain. In this work, we propose a new pipeline using ChatGPT instead of human\nexperts to generate high-quality feedback data for improving factual\nconsistency in the clinical note summarization task. We focus specifically on\nedit feedback because recent work discusses the shortcomings of human alignment\nvia preference feedback in complex situations (such as clinical NLP tasks that\nrequire extensive expert knowledge), as well as some advantages of collecting\nedit feedback from domain experts. In addition, although GPT has reached the\nexpert level in many clinical NLP tasks (e.g., USMLE QA), there is not much\nprevious work discussing whether GPT can generate expert-level edit feedback\nfor LMs in the clinical note summarization task. We hope to fill this gap.\nFinally, our evaluations demonstrate the potential use of GPT edits in human\nalignment, especially from a factuality perspective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1\">Prakamya Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zonghai Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Beining Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_R/0/1/0/all/0/1\">Rohan Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CapsFusion: Rethinking Image-Text Data at Scale. (arXiv:2310.20550v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2310.20550","description":"<p>Large multimodal models demonstrate remarkable generalist ability to perform\ndiverse multimodal tasks in a zero-shot manner. Large-scale web-based\nimage-text pairs contribute fundamentally to this success, but suffer from\nexcessive noise. Recent studies use alternative captions synthesized by\ncaptioning models and have achieved notable benchmark performance. However, our\nexperiments reveal significant Scalability Deficiency and World Knowledge Loss\nissues in models trained with synthetic captions, which have been largely\nobscured by their initial benchmark success. Upon closer examination, we\nidentify the root cause as the overly-simplified language structure and lack of\nknowledge details in existing synthetic captions. To provide higher-quality and\nmore scalable multimodal pretraining data, we propose CapsFusion, an advanced\nframework that leverages large language models to consolidate and refine\ninformation from both web-based image-text pairs and synthetic captions.\nExtensive experiments show that CapsFusion captions exhibit remarkable\nall-round superiority over existing captions in terms of model performance\n(e.g., 18.8 and 18.3 improvements in CIDEr score on COCO and NoCaps), sample\nefficiency (requiring 11-16 times less computation than baselines), world\nknowledge depth, and scalability. These effectiveness, efficiency and\nscalability advantages position CapsFusion as a promising candidate for future\nscaling of LMM training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qiying Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Quan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaosong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yufeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the effect of curriculum learning with developmental data for grammar acquisition. (arXiv:2311.00128v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.00128","description":"<p>This work explores the degree to which grammar acquisition is driven by\nlanguage `simplicity' and the source modality (speech vs. text) of data. Using\nBabyBERTa as a probe, we find that grammar acquisition is largely driven by\nexposure to speech data, and in particular through exposure to two of the\nBabyLM training corpora: AO-Childes and Open Subtitles. We arrive at this\nfinding by examining various ways of presenting input data to our model. First,\nwe assess the impact of various sequence-level complexity based curricula. We\nthen examine the impact of learning over `blocks' -- covering spans of text\nthat are balanced for the number of tokens in each of the source corpora\n(rather than number of lines). Finally, we explore curricula that vary the\ndegree to which the model is exposed to different corpora. In all cases, we\nfind that over-exposure to AO-Childes and Open Subtitles significantly drives\nperformance. We verify these findings through a comparable control dataset in\nwhich exposure to these corpora, and speech more generally, is limited by\ndesign. Our findings indicate that it is not the proportion of tokens occupied\nby high-utility data that aids acquisition, but rather the proportion of\ntraining steps assigned to such data. We hope this encourages future research\ninto the use of more developmentally plausible linguistic data (which tends to\nbe more scarce) to augment general purpose pre-training regimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Opper_M/0/1/0/all/0/1\">Mattia Opper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morrison_J/0/1/0/all/0/1\">J. Morrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1\">N. Siddharth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Interpersonal Communication by Simulating Audiences with Language Models. (arXiv:2311.00687v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2311.00687","description":"<p>How do we communicate with others to achieve our goals? We use our prior\nexperience or advice from others, or construct a candidate utterance by\npredicting how it will be received. However, our experiences are limited and\nbiased, and reasoning about potential outcomes can be difficult and cognitively\nchallenging. In this paper, we explore how we can leverage Large Language Model\n(LLM) simulations to help us communicate better. We propose the\nExplore-Generate-Simulate (EGS) framework, which takes as input any scenario\nwhere an individual is communicating to an audience with a goal they want to\nachieve. EGS (1) explores the solution space by producing a diverse set of\nadvice relevant to the scenario, (2) generates communication candidates\nconditioned on subsets of the advice, and (3) simulates the reactions from\nvarious audiences to determine both the best candidate and advice to use. We\nevaluate the framework on eight scenarios spanning the ten fundamental\nprocesses of interpersonal communication. For each scenario, we collect a\ndataset of human evaluations across candidates and baselines, and showcase that\nour framework's chosen candidate is preferred over popular generation\nmechanisms including Chain-of-Thought. We also find that audience simulations\nachieve reasonably high agreement with human raters across 5 of the 8\nscenarios. Finally, we demonstrate the generality of our framework by applying\nit to real-world scenarios described by users on web forums. Through\nevaluations and demonstrations, we show that EGS enhances the effectiveness and\noutcomes of goal-oriented communication across a variety of situations, thus\nopening up new possibilities for the application of large language models in\nrevolutionizing communication and decision-making processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ryan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_H/0/1/0/all/0/1\">Howard Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marjieh_R/0/1/0/all/0/1\">Raja Marjieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlashDecoding++: Faster Large Language Model Inference on GPUs. (arXiv:2311.01282v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2311.01282","description":"<p>As the Large Language Model (LLM) becomes increasingly important in various\ndomains. However, the following challenges still remain unsolved in\naccelerating LLM inference: (1) Synchronized partial softmax update. The\nsoftmax operation requires a synchronized update operation among each partial\nsoftmax result, leading to ~20% overheads for the attention computation in\nLLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices\nperforming GEMM in LLM inference is flat, leading to under-utilized computation\nand &gt;50% performance loss after padding zeros in previous designs. (3)\nPerformance loss due to static dataflow. Kernel performance in LLM depends on\nvaried input data features, hardware configurations, etc. A single and static\ndataflow may lead to a 50.25% performance loss for GEMMs of different shapes in\nLLM inference.\n</p>\n<p>We present FlashDecoding++, a fast LLM inference engine supporting mainstream\nLLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++\ncreatively proposes: (1) Asynchronized softmax with unified max value.\nFlashDecoding++ introduces a unified max value technique for different partial\nsoftmax computations to avoid synchronization. (2) Flat GEMM optimization with\ndouble buffering. FlashDecoding++ points out that flat GEMMs with different\nshapes face varied bottlenecks. Then, techniques like double buffering are\nintroduced. (3) Heuristic dataflow with hardware resource adaptation.\nFlashDecoding++ heuristically optimizes dataflow using different hardware\nresource considering input dynamics. Due to the versatility of optimizations in\nFlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on\nboth NVIDIA and AMD GPUs compared to Hugging Face implementations.\nFlashDecoding++ also achieves an average speedup of 1.37x compared to\nstate-of-the-art LLM inference engines on mainstream LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1\">Ke Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_G/0/1/0/all/0/1\">Guohao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiaming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Q/0/1/0/all/0/1\">Qiuli Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiuhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kangdi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hanyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Managing AI Risks in an Era of Rapid Progress. (arXiv:2310.17688v1 [cs.CY] CROSS LISTED)","link":"http://arxiv.org/abs/2310.17688","description":"<p>In this short consensus paper, we outline risks from upcoming, advanced AI\nsystems. We examine large-scale social harms and malicious uses, as well as an\nirreversible loss of human control over autonomous AI systems. In light of\nrapid and continuing AI progress, we propose priorities for AI R&amp;D and\ngovernance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinton_G/0/1/0/all/0/1\">Geoffrey Hinton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Andrew Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harari_Y/0/1/0/all/0/1\">Yuval Noah Harari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya-Qin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1\">Lan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalev_Shwartz_S/0/1/0/all/0/1\">Shai Shalev-Shwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfield_G/0/1/0/all/0/1\">Gillian Hadfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1\">Jeff Clune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maharaj_T/0/1/0/all/0/1\">Tegan Maharaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baydin_A/0/1/0/all/0/1\">At&#x131;l&#x131;m G&#xfc;ne&#x15f; Baydin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McIlraith_S/0/1/0/all/0/1\">Sheila McIlraith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1\">Ashwin Acharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krueger_D/0/1/0/all/0/1\">David Krueger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1\">Anca Dragan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1\">Stuart Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahneman_D/0/1/0/all/0/1\">Daniel Kahneman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1\">Jan Brauner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1\">S&#xf6;ren Mindermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-11-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}