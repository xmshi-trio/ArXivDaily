{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-12-12T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Assessing the Capacity of Transformer to Abstract Syntactic Representations: A Contrastive Analysis Based on Long-distance Agreement. (arXiv:2212.04523v1 [cs.CL])","link":"http://arxiv.org/abs/2212.04523","description":"<p>The long-distance agreement, evidence for syntactic structure, is\nincreasingly used to assess the syntactic generalization of Neural Language\nModels. Much work has shown that transformers are capable of high accuracy in\nvaried agreement tasks, but the mechanisms by which the models accomplish this\nbehavior are still not well understood. To better understand transformers'\ninternal working, this work contrasts how they handle two superficially similar\nbut theoretically distinct agreement phenomena: subject-verb and object-past\nparticiple agreement in French. Using probing and counterfactual analysis\nmethods, our experiments show that i) the agreement task suffers from several\nconfounders which partially question the conclusions drawn so far and ii)\ntransformers handle subject-verb and object-past participle agreements in a way\nthat is consistent with their modeling in theoretical linguistics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wisniewski_G/0/1/0/all/0/1\">Guillaume Wisniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crabbe_B/0/1/0/all/0/1\">Beno&#xee;t Crabb&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VASR: Visual Analogies of Situation Recognition. (arXiv:2212.04542v1 [cs.CV])","link":"http://arxiv.org/abs/2212.04542","description":"<p>A core process in human cognition is analogical mapping: the ability to\nidentify a similar relational structure between different situations. We\nintroduce a novel task, Visual Analogies of Situation Recognition, adapting the\nclassical word-analogy task into the visual domain. Given a triplet of images,\nthe task is to select an image candidate B' that completes the analogy (A to A'\nis like B to what?). Unlike previous work on visual analogy that focused on\nsimple image transformations, we tackle complex analogies requiring\nunderstanding of scenes.\n</p>\n<p>We leverage situation recognition annotations and the CLIP model to generate\na large set of 500k candidate analogies. Crowdsourced annotations for a sample\nof the data indicate that humans agree with the dataset label ~80% of the time\n(chance level 25%). Furthermore, we use human annotations to create a\ngold-standard dataset of 3,820 validated analogies. Our experiments demonstrate\nthat state-of-the-art models do well when distractors are chosen randomly\n(~86%), but struggle with carefully chosen distractors (~53%, compared to 90%\nhuman accuracy). We hope our dataset will encourage the development of new\nanalogy-making models. Website: https://vasr-dataset.github.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1\">Yonatan Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yosef_R/0/1/0/all/0/1\">Ron Yosef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strugo_E/0/1/0/all/0/1\">Eli Strugo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1\">Dafna Shahaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explain to me like I am five -- Sentence Simplification Using Transformers. (arXiv:2212.04595v1 [cs.CL])","link":"http://arxiv.org/abs/2212.04595","description":"<p>Sentence simplification aims at making the structure of text easier to read\nand understand while maintaining its original meaning. This can be helpful for\npeople with disabilities, new language learners, or those with low literacy.\nSimplification often involves removing difficult words and rephrasing the\nsentence. Previous research have focused on tackling this task by either using\nexternal linguistic databases for simplification or by using control tokens for\ndesired fine-tuning of sentences. However, in this paper we purely use\npre-trained transformer models. We experiment with a combination of GPT-2 and\nBERT models, achieving the best SARI score of 46.80 on the Mechanical Turk\ndataset, which is significantly better than previous state-of-the-art results.\nThe code can be found at https://github.com/amanbasu/sentence-simplification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Aman Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multidimensional Service Quality Scoring System. (arXiv:2212.04611v1 [cs.LG])","link":"http://arxiv.org/abs/2212.04611","description":"<p>This supplementary paper aims to introduce the Multidimensional Service\nQuality Scoring System (MSQs), a review-based method for quantifying host\nservice quality mentioned and employed in the paper Exit and transition:\nExploring the survival status of Airbnb listings in a time of\nprofessionalization. MSQs is not an end-to-end implementation and is\nessentially composed of three pipelines, namely Data Collection and\nPreprocessing, Objects Recognition and Grouping, and Aspect-based Service\nScoring. Using the study mentioned above as a case, the technical details of\nMSQs are explained in this article.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Shiyang Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-world Story Generation with Structured Knowledge Enhancement: A Comprehensive Survey. (arXiv:2212.04634v1 [cs.CL])","link":"http://arxiv.org/abs/2212.04634","description":"<p>Storytelling and narrative are fundamental to human experience, intertwined\nwith our social and cultural engagement. As such, researchers have long\nattempted to create systems that can generate stories automatically. In recent\nyears, powered by deep learning and massive data resources, automatic story\ngeneration has shown significant advances. However, considerable challenges,\nlike the need for global coherence in generated stories, still hamper\ngenerative models from reaching the same storytelling ability as human\nnarrators. To tackle these challenges, many studies seek to inject structured\nknowledge into the generation process, which is referred to as structure\nknowledge-enhanced story generation. Incorporating external knowledge can\nenhance the logical coherence among story events, achieve better knowledge\ngrounding, and alleviate over-generalization and repetition problems in\nstories. This survey provides the latest and comprehensive review of this\nresearch field: (i) we present a systematical taxonomy regarding how existing\nmethods integrate structured knowledge into story generation; (ii) we summarize\ninvolved story corpora, structured knowledge datasets, and evaluation metrics;\n(iii) we give multidimensional insights into the challenges of\nknowledge-enhanced story generation and cast light on promising directions for\nfuture study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jieru Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_B/0/1/0/all/0/1\">B&#xf6;rje F. Karlsson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparative Study of Sentiment Analysis for Multi-Sourced Social Media Platforms. (arXiv:2212.04688v1 [cs.CL])","link":"http://arxiv.org/abs/2212.04688","description":"<p>There is a vast amount of data generated every second due to the rapidly\ngrowing technology in the current world. This area of research attempts to\ndetermine the feelings or opinions of people on social media posts. The dataset\nwe used was a multi-source dataset from the comment section of various social\nnetworking sites like Twitter, Reddit, etc. Natural Language Processing\nTechniques were employed to perform sentiment analysis on the obtained dataset.\nIn this paper, we provide a comparative analysis using techniques of\nlexicon-based, machine learning and deep learning approaches. The Machine\nLearning algorithm used in this work is Naive Bayes, the Lexicon-based approach\nused in this work is TextBlob, and the deep-learning algorithm used in this\nwork is LSTM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kapur_K/0/1/0/all/0/1\">Keshav Kapur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harikrishnan_R/0/1/0/all/0/1\">Rajitha Harikrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MED-SE: Medical Entity Definition-based Sentence Embedding. (arXiv:2212.04734v1 [cs.LG])","link":"http://arxiv.org/abs/2212.04734","description":"<p>We propose Medical Entity Definition-based Sentence Embedding (MED-SE), a\nnovel unsupervised contrastive learning framework designed for clinical texts,\nwhich exploits the definitions of medical entities. To this end, we conduct an\nextensive analysis of multiple sentence embedding techniques in clinical\nsemantic textual similarity (STS) settings. In the entity-centric setting that\nwe have designed, MED-SE achieves significantly better performance, while the\nexisting unsupervised methods including SimCSE show degraded performance. Our\nexperiments elucidate the inherent discrepancies between the general- and\nclinical-domain texts, and suggest that entity-centric contrastive approaches\nmay help bridge this gap and lead to a better representation of clinical\nsentences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1\">Hyeonbin Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_H/0/1/0/all/0/1\">Haanju Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yera Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Clozing to Comprehending: Retrofitting Pre-trained Language Model to Pre-trained Machine Reader. (arXiv:2212.04755v1 [cs.CL])","link":"http://arxiv.org/abs/2212.04755","description":"<p>We present Pre-trained Machine Reader (PMR), a novel method to retrofit\nPre-trained Language Models (PLMs) into Machine Reading Comprehension (MRC)\nmodels without acquiring labeled data. PMR is capable of resolving the\ndiscrepancy between model pre-training and downstream fine-tuning of existing\nPLMs, and provides a unified solver for tackling various extraction tasks. To\nachieve this, we construct a large volume of general-purpose and high-quality\nMRC-style training data with the help of Wikipedia hyperlinks and design a Wiki\nAnchor Extraction task to guide the MRC-style pre-training process. Although\nconceptually simple, PMR is particularly effective in solving extraction tasks\nincluding Extractive Question Answering and Named Entity Recognition, where it\nshows tremendous improvements over previous approaches especially under\nlow-resource settings. Moreover, viewing sequence classification task as a\nspecial case of extraction task in our MRC formulation, PMR is even capable to\nextract high-quality rationales to explain the classification process,\nproviding more explainability of the predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Meng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Online Migration Decisions Following the Banning of Radical Communities. (arXiv:2212.04765v1 [cs.SI])","link":"http://arxiv.org/abs/2212.04765","description":"<p>The proliferation of radical online communities and their violent offshoots\nhas sparked great societal concern. However, the current practice of banning\nsuch communities from mainstream platforms has unintended consequences: (I) the\nfurther radicalization of their members in fringe platforms where they migrate;\nand (ii) the spillover of harmful content from fringe back onto mainstream\nplatforms. Here, in a large observational study on two banned subreddits,\nr/The\\_Donald and r/fatpeoplehate, we examine how factors associated with the\nRECRO radicalization framework relate to users' migration decisions.\nSpecifically, we quantify how these factors affect users' decisions to post on\nfringe platforms and, for those who do, whether they continue posting on the\nmainstream platform. Our results show that individual-level factors, those\nrelating to the behavior of users, are associated with the decision to post on\nthe fringe platform. Whereas social-level factors, users' connection with the\nradical community, only affect the propensity to be coactive on both platforms.\nOverall, our findings pave the way for evidence-based moderation policies, as\nthe decisions to migrate and remain coactive amplify unintended consequences of\ncommunity bans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Russo_G/0/1/0/all/0/1\">Giuseppe Russo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Manoel Horta Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casiraghi_G/0/1/0/all/0/1\">Giona Casiraghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verginer_L/0/1/0/all/0/1\">Luca Verginer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AUC Maximization for Low-Resource Named Entity Recognition. (arXiv:2212.04800v1 [cs.CL])","link":"http://arxiv.org/abs/2212.04800","description":"<p>Current work in named entity recognition (NER) uses either cross entropy (CE)\nor conditional random fields (CRF) as the objective/loss functions to optimize\nthe underlying NER model. Both of these traditional objective functions for the\nNER problem generally produce adequate performance when the data distribution\nis balanced and there are sufficient annotated training examples. But since NER\nis inherently an imbalanced tagging problem, the model performance under the\nlow-resource settings could suffer using these standard objective functions.\nBased on recent advances in area under the ROC curve (AUC) maximization, we\npropose to optimize the NER model by maximizing the AUC score. We give evidence\nthat by simply combining two binary-classifiers that maximize the AUC score,\nsignificant performance improvement over traditional loss functions is achieved\nunder low-resource NER settings. We also conduct extensive experiments to\ndemonstrate the advantages of our method under the low-resource and\nhighly-imbalanced data distribution settings. To the best of our knowledge,\nthis is the first work that brings AUC maximization to the NER setting.\nFurthermore, we show that our method is agnostic to different types of NER\nembeddings, models and domains. The code to replicate this work will be\nprovided upon request.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngoc Dang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1\">Wray Buntine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beare_R/0/1/0/all/0/1\">Richard Beare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lan Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CKG: Dynamic Representation Based on Context and Knowledge Graph. (arXiv:2212.04909v1 [cs.CL])","link":"http://arxiv.org/abs/2212.04909","description":"<p>Recently, neural language representation models pre-trained on large corpus\ncan capture rich co-occurrence information and be fine-tuned in downstream\ntasks to improve the performance. As a result, they have achieved\nstate-of-the-art results in a large range of language tasks. However, there\nexists other valuable semantic information such as similar, opposite, or other\npossible meanings in external knowledge graphs (KGs). We argue that entities in\nKGs could be used to enhance the correct semantic meaning of language\nsentences. In this paper, we propose a new method CKG: Dynamic Representation\nBased on \\textbf{C}ontext and \\textbf{K}nowledge \\textbf{G}raph. On the one\nside, CKG can extract rich semantic information of large corpus. On the other\nside, it can make full use of inside information such as co-occurrence in large\ncorpus and outside information such as similar entities in KGs. We conduct\nextensive experiments on a wide range of tasks, including QQP, MRPC, SST-5,\nSQuAD, CoNLL 2003, and SNLI. The experiment results show that CKG achieves SOTA\n89.2 on SQuAD compared with SAN (84.4), ELMo (85.8), and BERT$_{Base}$ (88.5).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xunzhu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tiezhu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Rujie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TRBLLmaker -- Transformer Reads Between Lyrics Lines maker. (arXiv:2212.04917v1 [cs.CL])","link":"http://arxiv.org/abs/2212.04917","description":"<p>Even for us, it can be challenging to comprehend the meaning of songs. As\npart of this project, we explore the process of generating the meaning of\nsongs. Despite the widespread use of text-to-text models, few attempts have\nbeen made to achieve a similar objective. Songs are primarily studied in the\ncontext of sentiment analysis. This involves identifying opinions and emotions\nin texts, evaluating them as positive or negative, and utilizing these\nevaluations to make music recommendations. In this paper, we present a\ngenerative model that offers implicit meanings for several lines of a song. Our\nmodel uses a decoder Transformer architecture GPT-2, where the input is the\nlyrics of a song. Furthermore, we compared the performance of this architecture\nwith that of the encoder-decoder Transformer architecture of the T5 model. We\nalso examined the effect of different prompt types with the option of appending\nadditional information, such as the name of the artist and the title of the\nsong. Moreover, we tested different decoding methods with different training\nparameters and evaluated our results using ROUGE. In order to build our\ndataset, we utilized the 'Genious' API, which allowed us to acquire the lyrics\nof songs and their explanations, as well as their rich metadata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ventura_M/0/1/0/all/0/1\">Mor Ventura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toker_M/0/1/0/all/0/1\">Michael Toker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOPRD: A multidisciplinary open peer review dataset. (arXiv:2212.04972v1 [cs.DL])","link":"http://arxiv.org/abs/2212.04972","description":"<p>Open peer review is a growing trend in academic publications. Public access\nto peer review data can benefit both the academic and publishing communities.\nIt also serves as a great support to studies on review comment generation and\nfurther to the realization of automated scholarly paper review. However, most\nof the existing peer review datasets do not provide data that cover the whole\npeer review process. Apart from this, their data are not diversified enough as\nthey are mainly collected from the field of computer science. These two\ndrawbacks of the currently available peer review datasets need to be addressed\nto unlock more opportunities for related studies. In response to this problem,\nwe construct MOPRD, a multidisciplinary open peer review dataset. This dataset\nconsists of paper metadata, multiple version manuscripts, review comments,\nmeta-reviews, author's rebuttal letters, and editorial decisions. Moreover, we\ndesign a modular guided review comment generation method based on MOPRD.\nExperiments show that our method delivers better performance indicated by both\nautomatic metrics and human evaluation. We also explore other potential\napplications of MOPRD, including meta-review generation, editorial decision\nprediction, author rebuttal generation, and scientometric analysis. MOPRD is a\nstrong endorsement for further studies in peer review-related research and\nother applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jialiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiaxin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhangping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yidong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaodong Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LADIS: Language Disentanglement for 3D Shape Editing. (arXiv:2212.05011v1 [cs.CV])","link":"http://arxiv.org/abs/2212.05011","description":"<p>Natural language interaction is a promising direction for democratizing 3D\nshape design. However, existing methods for text-driven 3D shape editing face\nchallenges in producing decoupled, local edits to 3D shapes. We address this\nproblem by learning disentangled latent representations that ground language in\n3D geometry. To this end, we propose a complementary tool set including a novel\nnetwork architecture, a disentanglement loss, and a new editing procedure.\nAdditionally, to measure edit locality, we define a new metric that we call\npart-wise edit precision. We show that our method outperforms existing SOTA\nmethods by 20% in terms of edit locality, and up to 6.6% in terms of language\nreference resolution accuracy. Our work suggests that by solely disentangling\nlanguage representations, downstream 3D shape editing can become more local to\nrelevant parts, even if the model was never given explicit part-based\nsupervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_I/0/1/0/all/0/1\">Ian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achlioptas_P/0/1/0/all/0/1\">Panos Achlioptas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1\">Minhyuk Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis. (arXiv:2212.05032v1 [cs.CV])","link":"http://arxiv.org/abs/2212.05032","description":"<p>Large-scale diffusion models have achieved state-of-the-art results on\ntext-to-image synthesis (T2I) tasks. Despite their ability to generate\nhigh-quality yet creative images, we observe that attribution-binding and\ncompositional capabilities are still considered major challenging issues,\nespecially when involving multiple objects. In this work, we improve the\ncompositional skills of T2I models, specifically more accurate attribute\nbinding and better image compositions. To do this, we incorporate linguistic\nstructures with the diffusion guidance process based on the controllable\nproperties of manipulating cross-attention layers in diffusion-based T2I\nmodels. We observe that keys and values in cross-attention layers have strong\nsemantic meanings associated with object layouts and content. Therefore, we can\nbetter preserve the compositional semantics in the generated image by\nmanipulating the cross-attention representations based on linguistic insights.\nBuilt upon Stable Diffusion, a SOTA T2I model, our structured cross-attention\ndesign is efficient that requires no additional training samples. We achieve\nbetter compositional skills in qualitative and quantitative results, leading to\na 5-8% advantage in head-to-head user comparison studies. Lastly, we conduct an\nin-depth analysis to reveal potential causes of incorrect image compositions\nand justify the properties of cross-attention layers in the generation process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Weixi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuehai He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1\">Tsu-Jui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akula_A/0/1/0/all/0/1\">Arjun Akula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayana_P/0/1/0/all/0/1\">Pradyumna Narayana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1\">Sugato Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Emotions into Health Mention Classification Task on Social Media. (arXiv:2212.05039v1 [cs.CL])","link":"http://arxiv.org/abs/2212.05039","description":"<p>The health mention classification (HMC) task is the process of identifying\nand classifying mentions of health-related concepts in text. This can be useful\nfor identifying and tracking the spread of diseases through social media posts.\nHowever, this is a non-trivial task. Here we build on recent studies suggesting\nthat using emotional information may improve upon this task. Our study results\nin a framework for health mention classification that incorporates affective\nfeatures. We present two methods, an intermediate task fine-tuning approach\n(implicit) and a multi-feature fusion approach (explicit) to incorporate\nemotions into our target task of HMC. We evaluated our approach on 5\nHMC-related datasets from different social media platforms including three from\nTwitter, one from Reddit and another from a combination of social media\nsources. Extensive experiments demonstrate that our approach results in\nstatistically significant performance gains on HMC tasks. By using the\nmulti-feature fusion approach, we achieve at least a 3% improvement in F1 score\nover BERT baselines across all datasets. We also show that considering only\nnegative emotions does not significantly affect performance on the HMC task.\nAdditionally, our results indicate that HMC models infused with emotional\nknowledge are an effective alternative, especially when other HMC datasets are\nunavailable for domain-specific fine-tuning. The source code for our models is\nfreely available at https://github.com/tahirlanre/Emotion_PHM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aduragba_O/0/1/0/all/0/1\">Olanrewaju Tahir Aduragba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jialin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristea_A/0/1/0/all/0/1\">Alexandra I. Cristea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints. (arXiv:2212.05055v1 [cs.LG])","link":"http://arxiv.org/abs/2212.05055","description":"<p>Training large, deep neural networks to convergence can be prohibitively\nexpensive. As a result, often only a small selection of popular, dense models\nare reused across different contexts and tasks. Increasingly, sparsely\nactivated models, which seek to decouple model size from computation costs, are\nbecoming an attractive alternative to dense models. Although more efficient in\nterms of quality and computation cost, sparse models remain data-hungry and\ncostly to train from scratch in the large scale regime. In this work, we\npropose sparse upcycling -- a simple way to reuse sunk training costs by\ninitializing a sparsely activated Mixture-of-Experts model from a dense\ncheckpoint. We show that sparsely upcycled T5 Base, Large, and XL language\nmodels and Vision Transformer Base and Large models, respectively,\nsignificantly outperform their dense counterparts on SuperGLUE and ImageNet,\nusing only ~50% of the initial dense pretraining sunk cost. The upcycled models\nalso outperform sparse models trained from scratch on 100% of the initial dense\npretraining computation budget.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Komatsuzaki_A/0/1/0/all/0/1\">Aran Komatsuzaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puigcerver_J/0/1/0/all/0/1\">Joan Puigcerver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Thorp_J/0/1/0/all/0/1\">James Lee-Thorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_C/0/1/0/all/0/1\">Carlos Riquelme Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1\">Basil Mustafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective and Imperceptible Adversarial Textual Attack via Multi-objectivization. (arXiv:2111.01528v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.01528","description":"<p>The field of adversarial textual attack has significantly grown over the last\nfew years, where the commonly considered objective is to craft adversarial\nexamples (AEs) that can successfully fool the target model. However, the\nimperceptibility of attacks, which is also essential for practical attackers,\nis often left out by previous studies. In consequence, the crafted AEs tend to\nhave obvious structural and semantic differences from the original\nhuman-written texts, making them easily perceptible. In this work, we advocate\nleveraging multi-objectivization to address such issue. Specifically, we\nformulate the problem of crafting AEs as a multi-objective optimization\nproblem, where the imperceptibility of attacks is considered as auxiliary\nobjectives. Then, we propose a simple yet effective evolutionary algorithm,\ndubbed HydraText, to solve this problem. To the best of our knowledge,\nHydraText is currently the only approach that can be effectively applied to\nboth score-based and decision-based attack settings. Exhaustive experiments\ninvolving 44237 instances demonstrate that HydraText consistently achieves\ncompetitive attack success rates and better attack imperceptibility than the\nrecently proposed attack approaches. A human evaluation study also shows that\nthe AEs crafted by HydraText are more indistinguishable from human-written\ntexts. Finally, these AEs exhibit good transferability and can bring notable\nrobustness improvement to the target model by adversarial training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengcai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_N/0/1/0/all/0/1\">Ning Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_W/0/1/0/all/0/1\">Wenjing Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chao Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Transpile AMR into SPARQL. (arXiv:2112.07877v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07877","description":"<p>We propose a transition-based system to transpile Abstract Meaning\nRepresentation (AMR) into SPARQL for Knowledge Base Question Answering (KBQA).\nThis allows us to delegate part of the semantic representation to a strongly\npre-trained semantic parser, while learning transpiling with small amount of\npaired data. We depart from recent work relating AMR and SPARQL constructs, but\nrather than applying a set of rules, we teach a BART model to selectively use\nthese relations. Further, we avoid explicitly encoding AMR but rather encode\nthe parser state in the attention mechanism of BART, following recent semantic\nparsing works. The resulting model is simple, provides supporting text for its\ndecisions, and outperforms recent approaches in KBQA across two knowledge\nbases: DBPedia (LC-QuAD 1.0, QALD-9) and Wikidata (WebQSP, SWQ-WD).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bornea_M/0/1/0/all/0/1\">Mihaela Bornea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Astudillo_R/0/1/0/all/0/1\">Ramon Fernandez Astudillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naseem_T/0/1/0/all/0/1\">Tahira Naseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1\">Nandana Mihindukulasooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelaziz_I/0/1/0/all/0/1\">Ibrahim Abdelaziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapanipathi_P/0/1/0/all/0/1\">Pavan Kapanipathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florian_R/0/1/0/all/0/1\">Radu Florian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roukos_S/0/1/0/all/0/1\">Salim Roukos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Study of Indian English Pronunciation Variabilities relative to Received Pronunciation. (arXiv:2204.06502v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06502","description":"<p>Analysis of Indian English (IE) pronunciation variabilities are useful in\nbuilding systems for Automatic Speech Recognition (ASR) and Text-to-Speech\n(TTS) synthesis in the Indian context. Typically, these pronunciation\nvariabilities have been explored by comparing IE pronunciation with Received\nPronunciation (RP). However, to explore these variabilities, it is required to\nhave labelled pronunciation data at the phonetic level, which is scarce for IE.\nMoreover, versatility of IE stems from the influence of a large diversity of\nthe speakers' mother tongues and demographic region differences. Prior\nlinguistic works have characterised features of IE variabilities qualitatively\nby reporting phonetic rules that represent such variations relative to RP. The\nqualitative descriptions often lack quantitative descriptors and data-driven\nanalysis of diverse IE pronunciation data to characterise IE on the phonetic\nlevel. To address these issues, in this work, we consider a corpus, Indic\nTIMIT, containing a large set of IE varieties from 80 speakers from various\nregions of India. We present an analysis to obtain the new set of phonetic\nrules representing IE pronunciation variabilities relative to RP in a\ndata-driven manner. We do this using 15,974 phonetic transcriptions, of which\n13,632 were obtained manually in addition to those part of the corpus.\nFurthermore, we validate the rules obtained from the analysis against the\nexisting phonetic rules to identify the relevance of the obtained phonetic\nrules and test the efficacy of Grapheme-to-Phoneme (G2P) conversion developed\nbased on the obtained rules considering Phoneme Error Rate (PER) as the metric\nfor performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pal_P/0/1/0/all/0/1\">Priyanshi Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shelly Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuppala_A/0/1/0/all/0/1\">Anil Vuppala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yarra_C/0/1/0/all/0/1\">Chiranjeevi Yarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_P/0/1/0/all/0/1\">Prasanta Ghosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed-effects transformers for hierarchical adaptation. (arXiv:2205.01749v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.01749","description":"<p>Language use differs dramatically from context to context. To some degree,\nmodern language models like GPT-3 are able to account for such variance by\nconditioning on a string of previous input text, or prompt. Yet prompting is\nineffective when contexts are sparse, out-of-sample, or extra-textual; for\ninstance, accounting for when and where the text was produced or who produced\nit. In this paper, we introduce the mixed-effects transformer (MET), a novel\napproach for learning hierarchically-structured prefixes -- lightweight modules\nprepended to the input -- to account for structured variation. Specifically, we\nshow how the popular class of mixed-effects models may be extended to\ntransformer-based architectures using a regularized prefix-tuning procedure\nwith dropout. We evaluate this approach on several domain-adaptation\nbenchmarks, finding that it efficiently adapts to novel contexts with minimal\ndata while still effectively generalizing to unseen contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1\">Julia White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawkins_R/0/1/0/all/0/1\">Robert Hawkins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProQA: Structural Prompt-based Pre-training for Unified Question Answering. (arXiv:2205.04040v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.04040","description":"<p>Question Answering (QA) is a longstanding challenge in natural language\nprocessing. Existing QA works mostly focus on specific question types,\nknowledge domains, or reasoning skills. The specialty in QA research hinders\nsystems from modeling commonalities between tasks and generalization for wider\napplications. To address this issue, we present ProQA, a unified QA paradigm\nthat solves various tasks through a single model. ProQA takes a unified\nstructural prompt as the bridge and improves the QA-centric ability by\nstructural prompt-based pre-training. Through a structurally designed\nprompt-based input schema, ProQA concurrently models the knowledge\ngeneralization for all QA tasks while keeping the knowledge customization for\nevery specific QA task. Furthermore, ProQA is pre-trained with structural\nprompt-formatted large-scale synthesized corpus, which empowers the model with\nthe commonly-required QA ability. Experimental results on 11 QA benchmarks\ndemonstrate that ProQA consistently boosts performance on both full data\nfine-tuning, few-shot learning, and zero-shot testing scenarios. Furthermore,\nProQA exhibits strong ability in both continual learning and transfer learning\nby taking the advantages of the structural prompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wanjun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yifan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiahai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jian Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LogiGAN: Learning Logical Reasoning via Adversarial Pre-training. (arXiv:2205.08794v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.08794","description":"<p>We present LogiGAN, an unsupervised adversarial pre-training framework for\nimproving logical reasoning abilities of language models. Upon automatic\nidentifying logical reasoning phenomena in massive text corpus via detection\nheuristics, we train language models to predict the masked-out logical\nstatements. Inspired by the facilitation effect of reflective thinking in human\nlearning, we analogically simulate the learning-thinking process with an\nadversarial Generator-Verifier architecture to assist logic learning. LogiGAN\nimplements a novel sequential GAN approach that (a) circumvents the\nnon-differentiable challenge of the sequential GAN by leveraging the Generator\nas a sentence-level generative likelihood scorer with a learning objective of\nreaching scoring consensus with the Verifier; (b) is computationally feasible\nfor large-scale pre-training with arbitrary target length. Both base and large\nsize language models pre-trained with LogiGAN demonstrate obvious performance\nimprovement on 12 datasets requiring general reasoning abilities, revealing the\nfundamental role of logic in broad reasoning, as well as the effectiveness of\nLogiGAN. Ablation studies on LogiGAN components reveal the relative\northogonality between linguistic and logic abilities and suggest that\nreflective thinking's facilitation effect might also generalize to machine\nlearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pi_X/0/1/0/all/0/1\">Xinyu Pi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wanjun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Spam Reviews on Vietnamese E-commerce Websites. (arXiv:2207.14636v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.14636","description":"<p>The reviews of customers play an essential role in online shopping. People\noften refer to reviews or comments of previous customers to decide whether to\nbuy a new product. Catching up with this behavior, some people create untruths\nand illegitimate reviews to hoax customers about the fake quality of products.\nThese are called spam reviews, confusing consumers on online shopping platforms\nand negatively affecting online shopping behaviors. We propose the dataset\ncalled ViSpamReviews, which has a strict annotation procedure for detecting\nspam reviews on e-commerce platforms. Our dataset consists of two tasks: the\nbinary classification task for detecting whether a review is spam or not and\nthe multi-class classification task for identifying the type of spam. The\nPhoBERT obtained the highest results on both tasks, 86.89% and 72.17%,\nrespectively, by macro average F1 score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dinh_C/0/1/0/all/0/1\">Co Van Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Gia-Tuan Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GMP*: Well-Tuned Gradual Magnitude Pruning Can Outperform Most BERT-Pruning Methods. (arXiv:2210.06384v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06384","description":"<p>We revisit the performance of the classic gradual magnitude pruning (GMP)\nbaseline for large language models, focusing on the classic BERT benchmark on\nvarious popular tasks. Despite existing evidence in the literature that GMP\nperforms poorly, we show that a simple and general variant, which we call GMP*,\ncan match and sometimes outperform more complex state-of-the-art methods. Our\nresults provide a simple yet strong baseline for future work, highlight the\nimportance of parameter tuning for baselines, and even improve the performance\nof the state-of-the-art second-order pruning method in this setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurtic_E/0/1/0/all/0/1\">Eldar Kurtic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1\">Dan Alistarh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Tuning on Layer Normalization for Pre-trained Language Models. (arXiv:2211.08682v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08682","description":"<p>Conventional fine-tuning encounters increasing difficulties given the size of\ncurrent Pre-trained Language Models, which makes parameter-efficient tuning\nbecome the focal point of frontier research. Previous methods in this field add\ntunable adapters into MHA or/and FFN of Transformer blocks to enable PLMs\nachieve transferability. However, as an important part of Transformer\narchitecture, the power of layer normalization for parameter-efficent tuning is\nignored. In this paper, we first propose LN-tuning, by tuning the gain and bias\nterm of Layer Normalization module with only 0.03\\% parameters, which is of\nhigh time-efficency and significantly superior to baselines which are less than\n0.1\\% tunable parameters. Further, we study the unified framework of combining\nLN-tuning with previous ones and we find that: (1) the unified framework of\ncombining prefix-tuning, the adapter-based method working on MHA, and LN-tuning\nachieves SOTA performance. (2) unified framework which tunes MHA and LayerNorm\nsimultaneously can get performance improvement but those which tune FFN and\nLayerNorm simultaneous will cause performance decrease. Ablation study\nvalidates LN-tuning is of no abundant parameters and gives a further\nunderstanding of it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1\">Wang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Y/0/1/0/all/0/1\">Yu-Ping Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_Y/0/1/0/all/0/1\">Yuan Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Taihao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AGRO: Adversarial Discovery of Error-prone groups for Robust Optimization. (arXiv:2212.00921v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2212.00921","description":"<p>Models trained via empirical risk minimization (ERM) are known to rely on\nspurious correlations between labels and task-independent input features,\nresulting in poor generalization to distributional shifts. Group\ndistributionally robust optimization (G-DRO) can alleviate this problem by\nminimizing the worst-case loss over a set of pre-defined groups over training\ndata. G-DRO successfully improves performance of the worst-group, where the\ncorrelation does not hold. However, G-DRO assumes that the spurious\ncorrelations and associated worst groups are known in advance, making it\nchallenging to apply it to new tasks with potentially multiple unknown spurious\ncorrelations. We propose AGRO -- Adversarial Group discovery for\nDistributionally Robust Optimization -- an end-to-end approach that jointly\nidentifies error-prone groups and improves accuracy on them. AGRO equips G-DRO\nwith an adversarial slicing model to find a group assignment for training\nexamples which maximizes worst-case loss over the discovered groups. On the\nWILDS benchmark, AGRO results in 8% higher model performance on average on\nknown worst-groups, compared to prior group discovery approaches used with\nG-DRO. AGRO also improves out-of-distribution performance on SST2, QQP, and\nMS-COCO -- datasets where potential spurious correlations are as yet\nuncharacterized. Human evaluation of ARGO groups shows that they contain\nwell-defined, yet previously unstudied spurious correlations that lead to model\nerrors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paranjape_B/0/1/0/all/0/1\">Bhargavi Paranjape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasigi_P/0/1/0/all/0/1\">Pradeep Dasigi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Legal Prompting: Teaching a Language Model to Think Like a Lawyer. (arXiv:2212.01326v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.01326","description":"<p>Large language models that are capable of zero or few-shot prompting\napproaches have given rise to the new research area of prompt engineering.\nRecent advances showed that for example Chain-of-Thought (CoT) prompts can\nimprove arithmetic or common sense tasks significantly. We explore how such\napproaches fare with legal reasoning tasks and take the COLIEE entailment task\nbased on the Japanese Bar exam for testing zero-shot/few-shot and fine-tuning\napproaches. Our findings show that while CoT prompting and fine-tuning with\nexplanations approaches show improvements, the best results are produced by\nprompts that are derived from specific legal reasoning techniques such as IRAC\n(Issue, Rule, Application, Conclusion). Based on our experiments we improve the\n2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 best\nsystem of 0.6789 accuracy with an accuracy of 0.7431.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fangyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quartey_L/0/1/0/all/0/1\">Lee Quartey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schilder_F/0/1/0/all/0/1\">Frank Schilder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Generative Approach for Script Event Prediction via Contrastive Fine-tuning. (arXiv:2212.03496v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.03496","description":"<p>Script event prediction aims to predict the subsequent event given the\ncontext. This requires the capability to infer the correlations between events.\nRecent works have attempted to improve event correlation reasoning by using\npretrained language models and incorporating external knowledge~(e.g.,\ndiscourse relations). Though promising results have been achieved, some\nchallenges still remain. First, the pretrained language models adopted by\ncurrent works ignore event-level knowledge, resulting in an inability to\ncapture the correlations between events well. Second, modeling correlations\nbetween events with discourse relations is limited because it can only capture\nexplicit correlations between events with discourse markers, and cannot capture\nmany implicit correlations. To this end, we propose a novel generative approach\nfor this task, in which a pretrained language model is fine-tuned with an\nevent-centric pretraining objective and predicts the next event within a\ngenerative paradigm. Specifically, we first introduce a novel event-level blank\ninfilling strategy as the learning objective to inject event-level knowledge\ninto the pretrained language model, and then design a likelihood-based\ncontrastive loss for fine-tuning the generative model. Instead of using an\nadditional prediction layer, we perform prediction by using sequence\nlikelihoods generated by the generative model. Our approach models correlations\nbetween events in a soft way without any external knowledge. The\nlikelihood-based prediction eliminates the need to use additional networks to\nmake predictions and is somewhat interpretable since it scores each word in the\nevent. Experimental results on the multi-choice narrative cloze~(MCNC) task\ndemonstrate that our approach achieves better results than other\nstate-of-the-art baselines. Our code will be available at\nhttps://github.com/zhufq00/mcnc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fangqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Changlong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_X/0/1/0/all/0/1\">Xin Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-12-11T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}