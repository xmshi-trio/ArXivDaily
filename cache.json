{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-07-10T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"PREADD: Prefix-Adaptive Decoding for Controlled Text Generation. (arXiv:2307.03214v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03214","description":"<p>We propose Prefix-Adaptive Decoding (PREADD), a flexible method for\ncontrolled text generation. Unlike existing methods that use auxiliary expert\nmodels to control for attributes, PREADD does not require an external model,\ninstead relying on linearly combining output logits from multiple prompts.\nSpecifically, PREADD contrasts the output logits generated using a raw prompt\nagainst those generated using a prefix-prepended prompt, enabling both positive\nand negative control with respect to any attribute encapsulated by the prefix.\nWe evaluate PREADD on three tasks -- toxic output mitigation, gender bias\nreduction, and sentiment control -- and find that PREADD outperforms not only\nprompting baselines, but also an auxiliary-expert control method, by 12% or\nmore in relative gain on our main metrics for each task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jonathan Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Language Transformers: A Survey. (arXiv:2307.03254v1 [cs.CV])","link":"http://arxiv.org/abs/2307.03254","description":"<p>Vision language tasks, such as answering questions about or generating\ncaptions that describe an image, are difficult tasks for computers to perform.\nA relatively recent body of research has adapted the pretrained transformer\narchitecture introduced in \\citet{vaswani2017attention} to vision language\nmodeling. Transformer models have greatly improved performance and versatility\nover previous vision language models. They do so by pretraining models on a\nlarge generic datasets and transferring their learning to new tasks with minor\nchanges in architecture and parameter values. This type of transfer learning\nhas become the standard modeling practice in both natural language processing\nand computer vision. Vision language transformers offer the promise of\nproducing similar advancements in tasks which require both vision and language.\nIn this paper, we provide a broad synthesis of the currently available research\non vision language transformer models and offer some analysis of their\nstrengths, limitations and some open questions that remain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fields_C/0/1/0/all/0/1\">Clayton Fields</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennington_C/0/1/0/all/0/1\">Casey Kennington</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It is not Sexually Suggestive, It is Educative. Separating Sex Education from Suggestive Content on TikTok Videos. (arXiv:2307.03274v1 [cs.CV])","link":"http://arxiv.org/abs/2307.03274","description":"<p>We introduce SexTok, a multi-modal dataset composed of TikTok videos labeled\nas sexually suggestive (from the annotator's point of view), sex-educational\ncontent, or neither. Such a dataset is necessary to address the challenge of\ndistinguishing between sexually suggestive content and virtual sex education\nvideos on TikTok. Children's exposure to sexually suggestive videos has been\nshown to have adversarial effects on their development. Meanwhile, virtual sex\neducation, especially on subjects that are more relevant to the LGBTQIA+\ncommunity, is very valuable. The platform's current system removes or penalizes\nsome of both types of videos, even though they serve different purposes. Our\ndataset contains video URLs, and it is also audio transcribed. To validate its\nimportance, we explore two transformer-based models for classifying the videos.\nOur preliminary results suggest that the task of distinguishing between these\ntypes of videos is learnable but challenging. These experiments suggest that\nthis dataset is meaningful and invites further study on the subject.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+George_E/0/1/0/all/0/1\">Enfa George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1\">Mihai Surdeanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gammatonegram Representation for End-to-End Dysarthric Speech Processing Tasks: Speech Recognition, Speaker Identification, and Intelligibility Assessment. (arXiv:2307.03296v1 [eess.AS])","link":"http://arxiv.org/abs/2307.03296","description":"<p>Dysarthria is a disability that causes a disturbance in the human speech\nsystem and reduces the quality and intelligibility of a person's speech.\nBecause of this effect, the normal speech processing systems can not work\nproperly on impaired speech. This disability is usually associated with\nphysical disabilities. Therefore, designing a system that can perform some\ntasks by receiving voice commands in the smart home can be a significant\nachievement. In this work, we introduce gammatonegram as an effective method to\nrepresent audio files with discriminative details, which is used as input for\nthe convolutional neural network. On the other word, we convert each speech\nfile into an image and propose image recognition system to classify speech in\ndifferent scenarios. Proposed CNN is based on the transfer learning method on\nthe pre-trained Alexnet. In this research, the efficiency of the proposed\nsystem for speech recognition, speaker identification, and intelligibility\nassessment is evaluated. According to the results on the UA dataset, the\nproposed speech recognition system achieved 91.29% accuracy in\nspeaker-dependent mode, the speaker identification system acquired 87.74%\naccuracy in text-dependent mode, and the intelligibility assessment system\nachieved 96.47% accuracy in two-class mode. Finally, we propose a multi-network\nspeech recognition system that works fully automatically. This system is\nlocated in a cascade arrangement with the two-class intelligibility assessment\nsystem, and the output of this system activates each one of the speech\nrecognition networks. This architecture achieves an accuracy of 92.3% WRR. The\nsource code of this paper is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Farhadipour_A/0/1/0/all/0/1\">Aref Farhadipour</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veisi_H/0/1/0/all/0/1\">Hadi Veisi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InfoSync: Information Synchronization across Multilingual Semi-structured Tables. (arXiv:2307.03313v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03313","description":"<p>Information Synchronization of semi-structured data across languages is\nchallenging. For instance, Wikipedia tables in one language should be\nsynchronized across languages. To address this problem, we introduce a new\ndataset InfoSyncC and a two-step method for tabular synchronization. InfoSync\ncontains 100K entity-centric tables (Wikipedia Infoboxes) across 14 languages,\nof which a subset (3.5K pairs) are manually annotated. The proposed method\nincludes 1) Information Alignment to map rows and 2) Information Update for\nupdating missing/outdated information for aligned tables across multilingual\ntables. When evaluated on InfoSync, information alignment achieves an F1 score\nof 87.91 (en &lt;-&gt; non-en). To evaluate information updation, we perform\nhuman-assisted Wikipedia edits on Infoboxes for 603 table pairs. Our approach\nobtains an acceptance rate of 77.28% on Wikipedia, showing the effectiveness of\nthe proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khincha_S/0/1/0/all/0/1\">Siddharth Khincha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_C/0/1/0/all/0/1\">Chelsi Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vivek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kataria_T/0/1/0/all/0/1\">Tushar Kataria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuo Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Covering Uncommon Ground: Gap-Focused Question Generation for Answer Assessment. (arXiv:2307.03319v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03319","description":"<p>Human communication often involves information gaps between the\ninterlocutors. For example, in an educational dialogue, a student often\nprovides an answer that is incomplete, and there is a gap between this answer\nand the perfect one expected by the teacher. Successful dialogue then hinges on\nthe teacher asking about this gap in an effective manner, thus creating a rich\nand interactive educational experience. We focus on the problem of generating\nsuch gap-focused questions (GFQs) automatically. We define the task, highlight\nkey desired aspects of a good GFQ, and propose a model that satisfies these.\nFinally, we provide an evaluation by human annotators of our generated\nquestions compared against human generated ones, demonstrating competitive\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rabin_R/0/1/0/all/0/1\">Roni Rabin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Djerbetian_A/0/1/0/all/0/1\">Alexandre Djerbetian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelberg_R/0/1/0/all/0/1\">Roee Engelberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hackmon_L/0/1/0/all/0/1\">Lidan Hackmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elidan_G/0/1/0/all/0/1\">Gal Elidan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1\">Amir Globerson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiPhone: Modeling Inter Language Phonetic Influences in Text. (arXiv:2307.03322v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03322","description":"<p>A large number of people are forced to use the Web in a language they have\nlow literacy in due to technology asymmetries. Written text in the second\nlanguage (L2) from such users often contains a large number of errors that are\ninfluenced by their native language (L1). We propose a method to mine phoneme\nconfusions (sounds in L2 that an L1 speaker is likely to conflate) for pairs of\nL1 and L2. These confusions are then plugged into a generative model (Bi-Phone)\nfor synthetically producing corrupted L2 text. Through human evaluations, we\nshow that Bi-Phone generates plausible corruptions that differ across L1s and\nalso have widespread coverage on the Web. We also corrupt the popular language\nunderstanding benchmark SuperGLUE with our technique (FunGLUE for Phonetically\nNoised GLUE) and show that SoTA language understating models perform poorly. We\nalso introduce a new phoneme prediction pre-training task which helps byte\nmodels to recover performance close to SuperGLUE. Finally, we also release the\nFunGLUE benchmark to promote further research in phonetically robust language\nmodels. To the best of our knowledge, FunGLUE is the first benchmark to\nintroduce L1-L2 interactions in text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhirut Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sai_A/0/1/0/all/0/1\">Ananya B. Sai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sproat_R/0/1/0/all/0/1\">Richard Sproat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilevski_Y/0/1/0/all/0/1\">Yuri Vasilevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">James S. Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jash_A/0/1/0/all/0/1\">Ambarish Jash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sodhi_S/0/1/0/all/0/1\">Sukhdeep S. Sodhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghuveer_A/0/1/0/all/0/1\">Aravindan Raghuveer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Token-Level Serialized Output Training for Joint Streaming ASR and ST Leveraging Textual Alignments. (arXiv:2307.03354v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03354","description":"<p>In real-world applications, users often require both translations and\ntranscriptions of speech to enhance their comprehension, particularly in\nstreaming scenarios where incremental generation is necessary. This paper\nintroduces a streaming Transformer-Transducer that jointly generates automatic\nspeech recognition (ASR) and speech translation (ST) outputs using a single\ndecoder. To produce ASR and ST content effectively with minimal latency, we\npropose a joint token-level serialized output training method that interleaves\nsource and target words by leveraging an off-the-shelf textual aligner.\nExperiments in monolingual (it-en) and multilingual (\\{de,es,it\\}-en) settings\ndemonstrate that our approach achieves the best quality-latency balance. With\nan average ASR latency of 1s and ST latency of 1.3s, our model shows no\ndegradation or even improves output quality compared to separate ASR and ST\nmodels, yielding an average improvement of 1.1 WER and 0.4 BLEU in the\nmultilingual case.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1\">Peidong Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junkun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jian Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_Y/0/1/0/all/0/1\">Yashesh Gaur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Biased Attitude Associations of Language Models in an Intersectional Context. (arXiv:2307.03360v1 [cs.CY])","link":"http://arxiv.org/abs/2307.03360","description":"<p>Language models are trained on large-scale corpora that embed implicit biases\ndocumented in psychology. Valence associations (pleasantness/unpleasantness) of\nsocial groups determine the biased attitudes towards groups and concepts in\nsocial cognition. Building on this established literature, we quantify how\nsocial groups are valenced in English language models using a sentence template\nthat provides an intersectional context. We study biases related to age,\neducation, gender, height, intelligence, literacy, race, religion, sex, sexual\norientation, social class, and weight. We present a concept projection approach\nto capture the valence subspace through contextualized word embeddings of\nlanguage models. Adapting the projection-based approach to embedding\nassociation tests that quantify bias, we find that language models exhibit the\nmost biased attitudes against gender identity, social class, and sexual\norientation signals in language. We find that the largest and better-performing\nmodel that we study is also more biased as it effectively captures bias\nembedded in sociocultural data. We validate the bias evaluation method by\noverperforming on an intrinsic valence evaluation task. The approach enables us\nto measure complex intersectional biases as they are known to manifest in the\noutputs and applications of language models that perpetuate historical biases.\nMoreover, our approach contributes to design justice as it studies the\nassociations of groups underrepresented in language such as transgender and\nhomosexual individuals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sabbaghi_S/0/1/0/all/0/1\">Shiva Omrani Sabbaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_R/0/1/0/all/0/1\">Robert Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caliskan_A/0/1/0/all/0/1\">Aylin Caliskan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Negative Transfer with Task Awareness for Sexism, Hate Speech, and Toxic Language Detection. (arXiv:2307.03377v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03377","description":"<p>This paper proposes a novelty approach to mitigate the negative transfer\nproblem. In the field of machine learning, the common strategy is to apply the\nSingle-Task Learning approach in order to train a supervised model to solve a\nspecific task. Training a robust model requires a lot of data and a significant\namount of computational resources, making this solution unfeasible in cases\nwhere data are unavailable or expensive to gather. Therefore another solution,\nbased on the sharing of information between tasks, has been developed:\nMulti-Task Learning (MTL). Despite the recent developments regarding MTL, the\nproblem of negative transfer has still to be solved. Negative transfer is a\nphenomenon that occurs when noisy information is shared between tasks,\nresulting in a drop in performance. This paper proposes a new approach to\nmitigate the negative transfer problem based on the task awareness concept. The\nproposed approach results in diminishing the negative transfer together with an\nimprovement of performance over classic MTL solution. Moreover, the proposed\napproach has been implemented in two unified architectures to detect Sexism,\nHate Speech, and Toxic Language in text comments. The proposed architectures\nset a new state-of-the-art both in EXIST-2021 and HatEval-2019 benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paula_A/0/1/0/all/0/1\">Angel Felipe Magnoss&#xe3;o de Paula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosso_P/0/1/0/all/0/1\">Paolo Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spina_D/0/1/0/all/0/1\">Damiano Spina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Side-by-side Comparison of Transformers for English Implicit Discourse Relation Classification. (arXiv:2307.03378v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03378","description":"<p>Though discourse parsing can help multiple NLP fields, there has been no wide\nlanguage model search done on implicit discourse relation classification. This\nhinders researchers from fully utilizing public-available models in discourse\nanalysis. This work is a straightforward, fine-tuned discourse performance\ncomparison of seven pre-trained language models. We use PDTB-3, a popular\ndiscourse relation annotated dataset. Through our model search, we raise SOTA\nto 0.671 ACC and obtain novel observations. Some are contrary to what has been\nreported before (Shi and Demberg, 2019b), that sentence-level pre-training\nobjectives (NSP, SBO, SOP) generally fail to produce the best performing model\nfor implicit discourse relation classification. Counterintuitively,\nsimilar-sized PLMs with MLM and full attention led to better performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bruce W. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">BongSeok Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason Hyung-Jong Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-UPV at EXIST 2023 -- Sexism Characterization Using Large Language Models Under The Learning with Disagreements Regime. (arXiv:2307.03385v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03385","description":"<p>With the increasing influence of social media platforms, it has become\ncrucial to develop automated systems capable of detecting instances of sexism\nand other disrespectful and hateful behaviors to promote a more inclusive and\nrespectful online environment. Nevertheless, these tasks are considerably\nchallenging considering different hate categories and the author's intentions,\nespecially under the learning with disagreements regime. This paper describes\nAI-UPV team's participation in the EXIST (sEXism Identification in Social\nneTworks) Lab at CLEF 2023. The proposed approach aims at addressing the task\nof sexism identification and characterization under the learning with\ndisagreements paradigm by training directly from the data with disagreements,\nwithout using any aggregated label. Yet, performances considering both soft and\nhard evaluations are reported. The proposed system uses large language models\n(i.e., mBERT and XLM-RoBERTa) and ensemble strategies for sexism identification\nand classification in English and Spanish. In particular, our system is\narticulated in three different pipelines. The ensemble approach outperformed\nthe individual large language models obtaining the best performances both\nadopting a soft and a hard label evaluation. This work describes the\nparticipation in all the three EXIST tasks, considering a soft evaluation, it\nobtained fourth place in Task 2 at EXIST and first place in Task 3, with the\nhighest ICM-Soft of -2.32 and a normalized ICM-Soft of 0.79. The source code of\nour approaches is publicly available at\nhttps://github.com/AngelFelipeMP/Sexism-LLM-Learning-With-Disagreement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paula_A/0/1/0/all/0/1\">Angel Felipe Magnoss&#xe3;o de Paula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizzi_G/0/1/0/all/0/1\">Giulia Rizzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fersini_E/0/1/0/all/0/1\">Elisabetta Fersini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spina_D/0/1/0/all/0/1\">Damiano Spina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Derivative Free Weight-space Ensembling. (arXiv:2307.03506v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03506","description":"<p>Recent work suggests that interpolating between the weights of two\nspecialized language models can transfer knowledge between tasks in a way that\nmulti-task learning cannot. However, very few have explored interpolation\nbetween more than two models, where each has a distinct knowledge base. In this\npaper, we introduce Derivative Free Weight-space Ensembling (DFWE), a new\nfew-sample task transfer approach for open-domain dialogue. Our framework\ncreates a set of diverse expert language models trained using a predefined set\nof source tasks. Next, we finetune each of the expert models on the target\ntask, approaching the target task from several distinct knowledge bases.\nFinally, we linearly interpolate between the model weights using a\ngradient-free-optimization algorithm, to efficiently find a good interpolation\nweighting. We demonstrate the effectiveness of the method on FETA-Friends\noutperforming the standard pretrain-finetune approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ninalga_D/0/1/0/all/0/1\">Dean Ninalga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying the perceptual value of lexical and non-lexical channels in speech. (arXiv:2307.03534v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03534","description":"<p>Speech is a fundamental means of communication that can be seen to provide\ntwo channels for transmitting information: the lexical channel of which words\nare said, and the non-lexical channel of how they are spoken. Both channels\nshape listener expectations of upcoming communication; however, directly\nquantifying their relative effect on expectations is challenging. Previous\nattempts require spoken variations of lexically-equivalent dialogue turns or\nconspicuous acoustic manipulations. This paper introduces a generalised\nparadigm to study the value of non-lexical information in dialogue across\nunconstrained lexical content. By quantifying the perceptual value of the\nnon-lexical channel with both accuracy and entropy reduction, we show that\nnon-lexical information produces a consistent effect on expectations of\nupcoming dialogue: even when it leads to poorer discriminative turn judgements\nthan lexical content alone, it yields higher consensus among participants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wallbridge_S/0/1/0/all/0/1\">Sarenne Wallbridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bell_P/0/1/0/all/0/1\">Peter Bell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Catherine Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models as Batteries-Included Zero-Shot ESCO Skills Matchers. (arXiv:2307.03539v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03539","description":"<p>Understanding labour market dynamics requires accurately identifying the\nskills required for and possessed by the workforce. Automation techniques are\nincreasingly being developed to support this effort. However, automatically\nextracting skills from job postings is challenging due to the vast number of\nexisting skills. The ESCO (European Skills, Competences, Qualifications and\nOccupations) framework provides a useful reference, listing over 13,000\nindividual skills. However, skills extraction remains difficult and accurately\nmatching job posts to the ESCO taxonomy is an open problem. In this work, we\npropose an end-to-end zero-shot system for skills extraction from job\ndescriptions based on large language models (LLMs). We generate synthetic\ntraining data for the entirety of ESCO skills and train a classifier to extract\nskill mentions from job posts. We also employ a similarity retriever to\ngenerate skill candidates which are then re-ranked using a second LLM. Using\nsynthetic data achieves an RP@10 score 10 points higher than previous distant\nsupervision approaches. Adding GPT-4 re-ranking improves RP@10 by over 22\npoints over previous methods. We also show that Framing the task as mock\nprogramming when prompting the LLM can lead to better performance than natural\nlanguage prompts, especially with weaker LLMs. We demonstrate the potential of\nintegrating large language models at both ends of skills matching pipelines.\nOur approach requires no human annotations and achieve extremely promising\nresults on skills extraction against ESCO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clavie_B/0/1/0/all/0/1\">Benjamin Clavi&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soulie_G/0/1/0/all/0/1\">Guillaume Souli&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DWReCO at CheckThat! 2023: Enhancing Subjectivity Detection through Style-based Data Sampling. (arXiv:2307.03550v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03550","description":"<p>This paper describes our submission for the subjectivity detection task at\nthe CheckThat! Lab. To tackle class imbalances in the task, we have generated\nadditional training materials with GPT-3 models using prompts of different\nstyles from a subjectivity checklist based on journalistic perspective. We used\nthe extended training set to fine-tune language-specific transformer models.\nOur experiments in English, German and Turkish demonstrate that different\nsubjective styles are effective across all languages. In addition, we observe\nthat the style-based oversampling is better than paraphrasing in Turkish and\nEnglish. Lastly, the GPT-3 models sometimes produce lacklustre results when\ngenerating style-based texts in non-English languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlicht_I/0/1/0/all/0/1\">Ipek Baris Schlicht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khellaf_L/0/1/0/all/0/1\">Lynn Khellaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altiok_D/0/1/0/all/0/1\">Defne Altiok</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Simplification of Scientific Texts for Non-Expert Readers. (arXiv:2307.03569v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03569","description":"<p>Reading levels are highly individual and can depend on a text's language, a\nperson's cognitive abilities, or knowledge on a topic. Text simplification is\nthe task of rephrasing a text to better cater to the abilities of a specific\ntarget reader group. Simplification of scientific abstracts helps non-experts\nto access the core information by bypassing formulations that require domain or\nexpert knowledge. This is especially relevant for, e.g., cancer patients\nreading about novel treatment options. The SimpleText lab hosts the\nsimplification of scientific abstracts for non-experts (Task 3) to advance this\nfield. We contribute three runs employing out-of-the-box summarization models\n(two based on T5, one based on PEGASUS) and one run using ChatGPT with complex\nphrase identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Engelmann_B/0/1/0/all/0/1\">Bj&#xf6;rn Engelmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haak_F/0/1/0/all/0/1\">Fabian Haak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutz_C/0/1/0/all/0/1\">Christin Katharina Kreutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khasmakhi_N/0/1/0/all/0/1\">Narjes Nikzad Khasmakhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaer_P/0/1/0/all/0/1\">Philipp Schaer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The distribution of discourse relations within and across turns in spontaneous conversation. (arXiv:2307.03645v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03645","description":"<p>Time pressure and topic negotiation may impose constraints on how people\nleverage discourse relations (DRs) in spontaneous conversational contexts. In\nthis work, we adapt a system of DRs for written language to spontaneous\ndialogue using crowdsourced annotations from novice annotators. We then test\nwhether discourse relations are used differently across several types of\nmulti-utterance contexts. We compare the patterns of DR annotation within and\nacross speakers and within and across turns. Ultimately, we find that different\ndiscourse contexts produce distinct distributions of discourse relations, with\nsingle-turn annotations creating the most uncertainty for annotators.\nAdditionally, we find that the discourse relation annotations are of sufficient\nquality to predict from embeddings of discourse units.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cortez_S/0/1/0/all/0/1\">S. Magal&#xed; L&#xf3;pez Cortez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_C/0/1/0/all/0/1\">Cassandra L. Jacobs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Testing the Predictions of Surprisal Theory in 11 Languages. (arXiv:2307.03667v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03667","description":"<p>A fundamental result in psycholinguistics is that less predictable words take\na longer time to process. One theoretical explanation for this finding is\nSurprisal Theory (Hale, 2001; Levy, 2008), which quantifies a word's\npredictability as its surprisal, i.e. its negative log-probability given a\ncontext. While evidence supporting the predictions of Surprisal Theory have\nbeen replicated widely, most have focused on a very narrow slice of data:\nnative English speakers reading English texts. Indeed, no comprehensive\nmultilingual analysis exists. We address this gap in the current literature by\ninvestigating the relationship between surprisal and reading times in eleven\ndifferent languages, distributed across five language families. Deriving\nestimates from language models trained on monolingual and multilingual corpora,\nwe test three predictions associated with surprisal theory: (i) whether\nsurprisal is predictive of reading times; (ii) whether expected surprisal, i.e.\ncontextual entropy, is predictive of reading times; (iii) and whether the\nlinking function between surprisal and reading times is linear. We find that\nall three predictions are borne out crosslinguistically. By focusing on a more\ndiverse set of languages, we argue that these results offer the most robust\nlink to-date between information theory and incremental language processing\nacross languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilcox_E/0/1/0/all/0/1\">Ethan Gotlieb Wilcox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger P. Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Effectiveness of Large Language Models in Representing Textual Descriptions of Geometry and Spatial Relations. (arXiv:2307.03678v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03678","description":"<p>This research focuses on assessing the ability of large language models\n(LLMs) in representing geometries and their spatial relations. We utilize LLMs\nincluding GPT-2 and BERT to encode the well-known text (WKT) format of\ngeometries and then feed their embeddings into classifiers and regressors to\nevaluate the effectiveness of the LLMs-generated embeddings for geometric\nattributes. The experiments demonstrate that while the LLMs-generated\nembeddings can preserve geometry types and capture some spatial relations (up\nto 73% accuracy), challenges remain in estimating numeric values and retrieving\nspatially related objects. This research highlights the need for improvement in\nterms of capturing the nuances and complexities of the underlying geospatial\ndata and integrating domain knowledge to support various GeoAI applications\nusing foundation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yuhan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Song Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Undecimated Wavelet Transform for Word Embedded Semantic Marginal Autoencoder in Security improvement and Denoising different Languages. (arXiv:2307.03679v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03679","description":"<p>By combining the undecimated wavelet transform within a Word Embedded\nSemantic Marginal Autoencoder (WESMA), this research study provides a novel\nstrategy for improving security measures and denoising multiple languages. The\nincorporation of these strategies is intended to address the issues of\nrobustness, privacy, and multilingualism in data processing applications. The\nundecimated wavelet transform is used as a feature extraction tool to identify\nprominent language patterns and structural qualities in the input data. The\nproposed system may successfully capture significant information while\npreserving the temporal and geographical links within the data by employing\nthis transform. This improves security measures by increasing the system's\nability to detect abnormalities, discover hidden patterns, and distinguish\nbetween legitimate content and dangerous threats. The Word Embedded Semantic\nMarginal Autoencoder also functions as an intelligent framework for\ndimensionality and noise reduction. The autoencoder effectively learns the\nunderlying semantics of the data and reduces noise components by exploiting\nword embeddings and semantic context. As a result, data quality and accuracy\nare increased in following processing stages. The suggested methodology is\ntested using a diversified dataset that includes several languages and security\nscenarios. The experimental results show that the proposed approach is\neffective in attaining security enhancement and denoising capabilities across\nmultiple languages. The system is strong in dealing with linguistic variances,\nproducing consistent outcomes regardless of the language used. Furthermore,\nincorporating the undecimated wavelet transform considerably improves the\nsystem's ability to efficiently address complex security concerns\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+S_S/0/1/0/all/0/1\">Shreyanth S</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging text data for causal inference using electronic health records. (arXiv:2307.03687v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03687","description":"<p>Text is a ubiquitous component of medical data, containing valuable\ninformation about patient characteristics and care that are often missing from\nstructured chart data. Despite this richness, it is rarely used in clinical\nresearch, owing partly to its complexity. Using a large database of patient\nrecords and treatment histories accompanied by extensive notes by attendant\nphysicians and nurses, we show how text data can be used to support causal\ninference with electronic health data in all stages, from conception and design\nto analysis and interpretation, with minimal additional effort. We focus on\nstudies using matching for causal inference. We augment a classic matching\nanalysis by incorporating text in three ways: by using text to supplement a\nmultiple imputation procedure, we improve the fidelity of imputed values to\nhandle missing data; by incorporating text in the matching stage, we strengthen\nthe plausibility of the matching procedure; and by conditioning on text, we can\nestimate easily interpretable text-based heterogeneous treatment effects that\nmay be stronger than those found across categories of structured covariates.\nUsing these techniques, we hope to expand the scope of secondary analysis of\nclinical data to domains where quantitative data is of poor quality or\nnonexistent, but where text is available, such as in developing countries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mozer_R/0/1/0/all/0/1\">Reagan Mozer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaufman_A/0/1/0/all/0/1\">Aaron R. Kaufman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celi_L/0/1/0/all/0/1\">Leo A. Celi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miratrix_L/0/1/0/all/0/1\">Luke Miratrix</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Apples to Apples: Generating Aspect-Aware Comparative Sentences from User Review. (arXiv:2307.03691v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03691","description":"<p>It is time-consuming to find the best product among many similar\nalternatives. Comparative sentences can help to contrast one item from others\nin a way that highlights important features of an item that stand out. Given\nreviews of one or multiple items and relevant item features, we generate\ncomparative review sentences to aid users to find the best fit. Specifically,\nour model consists of three successive components in a transformer: (i) an item\nencoding module to encode an item for comparison, (ii) a comparison generation\nmodule that generates comparative sentences in an autoregressive manner, (iii)\na novel decoding method for user personalization. We show that our pipeline\ngenerates fluent and diverse comparative sentences. We run experiments on the\nrelevance and fidelity of our generated sentences in a human evaluation study\nand find that our algorithm creates comparative review sentences that are\nrelevant and truthful.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Echterhoff_J/0/1/0/all/0/1\">Jessica Echterhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_A/0/1/0/all/0/1\">An Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Becoming self-instruct: introducing early stopping criteria for minimal instruct tuning. (arXiv:2307.03692v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03692","description":"<p>In this paper, we introduce the Instruction Following Score (IFS), a metric\nthat detects language models' ability to follow instructions. The metric has a\ndual purpose. First, IFS can be used to distinguish between base and instruct\nmodels. We benchmark publicly available base and instruct models, and show that\nthe ratio of well formatted responses to partial and full sentences can be an\neffective measure between those two model classes. Secondly, the metric can be\nused as an early stopping criteria for instruct tuning. We compute IFS for\nSupervised Fine-Tuning (SFT) of 7B and 13B LLaMA models, showing that models\nlearn to follow instructions relatively early in the training process, and the\nfurther finetuning can result in changes in the underlying base model\nsemantics. As an example of semantics change we show the objectivity of model\npredictions, as defined by an auxiliary metric ObjecQA. We show that in this\nparticular case, semantic changes are the steepest when the IFS tends to\nplateau. We hope that decomposing instruct tuning into IFS and semantic factors\nstarts a new trend in better controllable instruct tuning and opens\npossibilities for designing minimal instruct interfaces querying foundation\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+AlShikh_W/0/1/0/all/0/1\">Waseem AlShikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daaboul_M/0/1/0/all/0/1\">Manhal Daaboul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goddard_K/0/1/0/all/0/1\">Kirk Goddard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imel_B/0/1/0/all/0/1\">Brock Imel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamble_K/0/1/0/all/0/1\">Kiran Kamble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_P/0/1/0/all/0/1\">Parikshith Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russak_M/0/1/0/all/0/1\">Melisa Russak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unveiling the Potential of Knowledge-Prompted ChatGPT for Enhancing Drug Trafficking Detection on Social Media. (arXiv:2307.03699v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03699","description":"<p>Social media platforms such as Instagram and Twitter have emerged as critical\nchannels for drug marketing and illegal sale. Detecting and labeling online\nillicit drug trafficking activities becomes important in addressing this issue.\nHowever, the effectiveness of conventional supervised learning methods in\ndetecting drug trafficking heavily relies on having access to substantial\namounts of labeled data, while data annotation is time-consuming and\nresource-intensive. Furthermore, these models often face challenges in\naccurately identifying trafficking activities when drug dealers use deceptive\nlanguage and euphemisms to avoid detection. To overcome this limitation, we\nconduct the first systematic study on leveraging large language models (LLMs),\nsuch as ChatGPT, to detect illicit drug trafficking activities on social media.\nWe propose an analytical framework to compose \\emph{knowledge-informed\nprompts}, which serve as the interface that humans can interact with and use\nLLMs to perform the detection task. Additionally, we design a Monte Carlo\ndropout based prompt optimization method to further to improve performance and\ninterpretability. Our experimental findings demonstrate that the proposed\nframework outperforms other baseline language models in terms of drug\ntrafficking detection accuracy, showing a remarkable improvement of nearly\n12\\%. By integrating prior knowledge and the proposed prompts, ChatGPT can\neffectively identify and label drug trafficking activities on social networks,\neven in the presence of deceptive language and euphemisms used by drug dealers\nto evade detection. The implications of our research extend to social networks,\nemphasizing the importance of incorporating prior knowledge and scenario-based\nprompts into analytical tools to improve online security and public safety.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chuanbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yanfang Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers. (arXiv:2307.03712v1 [cs.LG])","link":"http://arxiv.org/abs/2307.03712","description":"<p>The recent rise of large language models (LLMs) has resulted in increased\nefforts towards running LLMs at reduced precision. Running LLMs at lower\nprecision supports resource constraints and furthers their democratization,\nenabling users to run billion-parameter LLMs on their personal devices. To\nsupplement this ongoing effort, we propose INT-FP-QSim: an open-source\nsimulator that enables flexible evaluation of LLMs and vision transformers at\nvarious numerical precisions and formats. INT-FP-QSim leverages existing\nopen-source repositories such as TensorRT, QPytorch and AIMET for a combined\nsimulator that supports various floating point and integer formats. With the\nhelp of our simulator, we survey the impact of different numerical formats on\nthe performance of LLMs and vision transformers at 4-bit weights and 4-bit or\n8-bit activations. We also compare recently proposed methods like Adaptive\nBlock Floating Point, SmoothQuant, GPTQ and RPTQ on the model performances. We\nhope INT-FP-QSim will enable researchers to flexibly simulate models at various\nprecisions to support further research in quantization of LLMs and vision\ntransformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nair_L/0/1/0/all/0/1\">Lakshmi Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernadskiy_M/0/1/0/all/0/1\">Mikhail Bernadskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhavan_A/0/1/0/all/0/1\">Arulselvan Madhavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Craig Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basumallik_A/0/1/0/all/0/1\">Ayon Basumallik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bunandar_D/0/1/0/all/0/1\">Darius Bunandar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Automatic Quotation Attribution in Literary Novels. (arXiv:2307.03734v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03734","description":"<p>Current models for quotation attribution in literary novels assume varying\nlevels of available information in their training and test data, which poses a\nchallenge for in-the-wild inference. Here, we approach quotation attribution as\na set of four interconnected sub-tasks: character identification, coreference\nresolution, quotation identification, and speaker attribution. We benchmark\nstate-of-the-art models on each of these sub-tasks independently, using a large\ndataset of annotated coreferences and quotations in literary novels (the\nProject Dialogism Novel Corpus). We also train and evaluate models for the\nspeaker attribution task in particular, showing that a simple sequential\nprediction model achieves accuracy scores on par with state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vishnubhotla_K/0/1/0/all/0/1\">Krishnapriya Vishnubhotla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1\">Frank Rudzicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirst_G/0/1/0/all/0/1\">Graeme Hirst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammond_A/0/1/0/all/0/1\">Adam Hammond</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QIGen: Generating Efficient Kernels for Quantized Inference on Large Language Models. (arXiv:2307.03738v1 [cs.LG])","link":"http://arxiv.org/abs/2307.03738","description":"<p>We present ongoing work on a new automatic code generation approach for\nsupporting quantized generative inference on LLMs such as LLaMA or OPT on\noff-the-shelf CPUs. Our approach is informed by the target architecture and a\nperformance model, including both hardware characteristics and method-specific\naccuracy constraints. Results on CPU-based inference for LLaMA models show that\nour approach can lead to high performance and high accuracy, comparing\nfavorably to the best existing open-source solution. A preliminary\nimplementation is available at https://github.com/IST-DASLab/QIGen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pegolotti_T/0/1/0/all/0/1\">Tommaso Pegolotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frantar_E/0/1/0/all/0/1\">Elias Frantar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1\">Dan Alistarh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puschel_M/0/1/0/all/0/1\">Markus P&#xfc;schel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Efficacy of Sampling Adapters. (arXiv:2307.03749v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03749","description":"<p>Sampling is a common strategy for generating text from probabilistic models,\nyet standard ancestral sampling often results in text that is incoherent or\nungrammatical. To alleviate this issue, various modifications to a model's\nsampling distribution, such as nucleus or top-k sampling, have been introduced\nand are now ubiquitously used in language generation systems. We propose a\nunified framework for understanding these techniques, which we term sampling\nadapters. Sampling adapters often lead to qualitatively better text, which\nraises the question: From a formal perspective, how are they changing the\n(sub)word-level distributions of language generation models? And why do these\nlocal changes lead to higher-quality text? We argue that the shift they enforce\ncan be viewed as a trade-off between precision and recall: while the model\nloses its ability to produce certain strings, its precision rate on desirable\ntext increases. While this trade-off is not reflected in standard metrics of\ndistribution quality (such as perplexity), we find that several\nprecision-emphasizing measures indeed indicate that sampling adapters can lead\nto probability distributions more aligned with the true distribution. Further,\nthese measures correlate with higher sequence-level quality scores,\nspecifically, Mauve.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malagutti_L/0/1/0/all/0/1\">Luca Malagutti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilcox_E/0/1/0/all/0/1\">Ethan G. Wilcox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Multimodal Fact-Checking and Explanation Generation: A Challenging Dataset and Models. (arXiv:2205.12487v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12487","description":"<p>We propose end-to-end multimodal fact-checking and explanation generation,\nwhere the input is a claim and a large collection of web sources, including\narticles, images, videos, and tweets, and the goal is to assess the\ntruthfulness of the claim by retrieving relevant evidence and predicting a\ntruthfulness label (e.g., support, refute or not enough information), and to\ngenerate a statement to summarize and explain the reasoning and ruling process.\nTo support this research, we construct Mocheg, a large-scale dataset consisting\nof 15,601 claims where each claim is annotated with a truthfulness label and a\nruling statement, and 33,880 textual paragraphs and 12,112 images in total as\nevidence. To establish baseline performances on Mocheg, we experiment with\nseveral state-of-the-art neural architectures on the three pipelined subtasks:\nmultimodal evidence retrieval, claim verification, and explanation generation,\nand demonstrate that the performance of the state-of-the-art end-to-end\nmultimodal fact-checking does not provide satisfactory outcomes. To the best of\nour knowledge, we are the first to build the benchmark dataset and solutions\nfor end-to-end multimodal fact-checking and explanation generation. The\ndataset, source code and model checkpoints are available at\nhttps://github.com/VT-NLP/Mocheg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Barry Menglong Yao</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Aditya Shah</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jin-Hee Cho</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a> (1) ((1) Virginia Tech, (2) Lehigh University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the Gap Between Indexing and Retrieval for Differentiable Search Index with Query Generation. (arXiv:2206.10128v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2206.10128","description":"<p>The Differentiable Search Index (DSI) is an emerging paradigm for information\nretrieval. Unlike traditional retrieval architectures where index and retrieval\nare two different and separate components, DSI uses a single transformer model\nto perform both indexing and retrieval.\n</p>\n<p>In this paper, we identify and tackle an important issue of current DSI\nmodels: the data distribution mismatch that occurs between the DSI indexing and\nretrieval processes. Specifically, we argue that, at indexing, current DSI\nmethods learn to build connections between the text of long documents and the\nidentifier of the documents, but then retrieval of document identifiers is\nbased on queries that are commonly much shorter than the indexed documents.\nThis problem is further exacerbated when using DSI for cross-lingual retrieval,\nwhere document text and query text are in different languages.\n</p>\n<p>To address this fundamental problem of current DSI models, we propose a\nsimple yet effective indexing framework for DSI, called DSI-QG. When indexing,\nDSI-QG represents documents with a number of potentially relevant queries\ngenerated by a query generation model and re-ranked and filtered by a\ncross-encoder ranker. The presence of these queries at indexing allows the DSI\nmodels to connect a document identifier to a set of queries, hence mitigating\ndata distribution mismatches present between the indexing and the retrieval\nphases. Empirical results on popular mono-lingual and cross-lingual passage\nretrieval datasets show that DSI-QG significantly outperforms the original DSI\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1\">Shengyao Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Houxing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_L/0/1/0/all/0/1\">Linjun Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Ming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccon_G/0/1/0/all/0/1\">Guido Zuccon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word to Sentence Visual Semantic Similarity for Caption Generation: Lessons Learned. (arXiv:2209.12817v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.12817","description":"<p>This paper focuses on enhancing the captions generated by image-caption\ngeneration systems. We propose an approach for improving caption generation\nsystems by choosing the most closely related output to the image rather than\nthe most likely output produced by the model. Our model revises the language\ngeneration output beam search from a visual context perspective. We employ a\nvisual semantic measure in a word and sentence level manner to match the proper\ncaption to the related information in the image. The proposed approach can be\napplied to any caption system as a post-processing based method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sabir_A/0/1/0/all/0/1\">Ahmed Sabir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breadth-First Pipeline Parallelism. (arXiv:2211.05953v2 [cs.DC] UPDATED)","link":"http://arxiv.org/abs/2211.05953","description":"<p>We introduce Breadth-First Pipeline Parallelism, a novel training schedule\nwhich optimizes the combination of pipeline and data parallelism. Breadth-First\nPipeline Parallelism lowers training time, cost and memory usage by combining a\nhigh GPU utilization with a small batch size per GPU, and by making use of\nfully sharded data parallelism. Experimentally, we observed an increase of up\nto 43% in training throughput for a 52 billion-parameter model using a small\nbatch size per GPU compared to Megatron-LM, which would reduce the training\ntime and cost by the same amount on a large GPU cluster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lamy_Poirier_J/0/1/0/all/0/1\">Joel Lamy-Poirier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrated Interpretation: Confidence Estimation in Semantic Parsing. (arXiv:2211.07443v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07443","description":"<p>Sequence generation models are increasingly being used to translate natural\nlanguage into programs, i.e. to perform executable semantic parsing. The fact\nthat semantic parsing aims to predict programs that can lead to executed\nactions in the real world motivates developing safe systems. This in turn makes\nmeasuring calibration -- a central component to safety -- particularly\nimportant. We investigate the calibration of popular generation models across\nfour popular semantic parsing datasets, finding that it varies across models\nand datasets. We then analyze factors associated with calibration error and\nrelease new confidence-based challenge splits of two parsing datasets. To\nfacilitate the inclusion of calibration in semantic parsing evaluations, we\nrelease a library for computing calibration metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stengel_Eskin_E/0/1/0/all/0/1\">Elias Stengel-Eskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALERT: Adapting Language Models to Reasoning Tasks. (arXiv:2212.08286v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08286","description":"<p>Current large language models can perform reasonably well on complex tasks\nthat require step-by-step reasoning with few-shot learning. Are these models\napplying reasoning skills they have learnt during pre-training and reason\noutside of their training context, or are they simply memorizing their training\ncorpus at finer granularity and have learnt to better understand their context?\nTo tease apart these possibilities, we introduce ALERT, a benchmark and suite\nof analyses for assessing language models' reasoning ability comparing\npre-trained and finetuned models on complex tasks that require reasoning skills\nto solve. ALERT provides a test bed to asses any language model on fine-grained\nreasoning skills, which spans over 20 datasets and covers 10 different\nreasoning skills. We leverage ALERT to further investigate the role of\nfinetuning. With extensive empirical analysis we find that language models\nlearn more reasoning skills such as textual entailment, abductive reasoning,\nand analogical reasoning during finetuning stage compared to pretraining state.\nWe also find that when language models are finetuned they tend to overfit to\nthe prompt template, which hurts the robustness of models causing\ngeneralization problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Ping Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianlu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golovneva_O/0/1/0/all/0/1\">Olga Golovneva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alkhamissy_B/0/1/0/all/0/1\">Badr Alkhamissy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1\">Mona Diab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SESCORE2: Learning Text Generation Evaluation via Synthesizing Realistic Mistakes. (arXiv:2212.09305v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09305","description":"<p>Is it possible to train a general metric for evaluating text generation\nquality without human annotated ratings? Existing learned metrics either\nperform unsatisfactorily across text generation tasks or require human ratings\nfor training on specific tasks. In this paper, we propose SESCORE2, a\nself-supervised approach for training a model-based metric for text generation\nevaluation. The key concept is to synthesize realistic model mistakes by\nperturbing sentences retrieved from a corpus. The primary advantage of the\nSESCORE2 is its ease of extension to many other languages while providing\nreliable severity estimation. We evaluate SESCORE2 and previous methods on four\ntext generation tasks across three languages. SESCORE2 outperforms unsupervised\nmetric PRISM on four text generation evaluation benchmarks, with a Kendall\nimprovement of 0.078. Surprisingly, SESCORE2 even outperforms the supervised\nBLEURT and COMET on multiple text generation tasks. The code and data are\navailable at https://github.com/xu1998hz/SEScore2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenda Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xian Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WACO: Word-Aligned Contrastive Learning for Speech Translation. (arXiv:2212.09359v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09359","description":"<p>End-to-end Speech Translation (E2E ST) aims to directly translate source\nspeech into target text. Existing ST methods perform poorly when only extremely\nsmall speech-text data are available for training. We observe that an ST\nmodel's performance closely correlates with its embedding similarity between\nspeech and source transcript. In this paper, we propose Word-Aligned\nCOntrastive learning (WACO), a simple and effective method for extremely\nlow-resource speech-to-text translation. Our key idea is bridging word-level\nrepresentations for both speech and text modalities via contrastive learning.\nWe evaluate WACO and other methods on the MuST-C dataset, a widely used ST\nbenchmark, and on a low-resource direction Maltese-English from IWSLT 2023. Our\nexperiments demonstrate that WACO outperforms the best baseline by 9+ BLEU\npoints with only 1-hour parallel ST data. Code is available at\nhttps://github.com/owaski/WACO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Siqi Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Rong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model. (arXiv:2212.09811v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09811","description":"<p>The recently released NLLB-200 is a set of multilingual Neural Machine\nTranslation models that cover 202 languages. The largest model is based on a\nMixture of Experts architecture and achieves SoTA results across many language\npairs. It contains 54.5B parameters and requires at least four 32GB GPUs just\nfor inference. In this work, we propose a pruning method that enables the\nremoval of up to 80% of experts without further finetuning and with a\nnegligible loss in translation quality, which makes it feasible to run the\nmodel on a single 32GB GPU. Further analysis suggests that our pruning metrics\ncan identify language-specific experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koishekenov_Y/0/1/0/all/0/1\">Yeskendir Koishekenov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berard_A/0/1/0/all/0/1\">Alexandre Berard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1\">Vassilina Nikoulina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guiding Large Language Models via Directional Stimulus Prompting. (arXiv:2302.11520v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.11520","description":"<p>We introduce a novel prompting framework called Directional Stimulus\nPrompting for guiding black-box large language models (LLMs) toward desired\noutputs. The framework introduces a new component called directional stimulus\ninto the prompt, providing more fine-grained guidance and control over LLMs.\nThe directional stimulus serves as hints or cues for each input query to guide\nLLMs toward the desired output, such as keywords that the desired summary\nshould include for summarization. We utilize a small tunable model (e.g., T5)\nto generate such directional stimulus for each query, allowing us to optimize\nblack-box LLMs by optimizing a small policy model. This policy model can be\ntrained through 1) supervised fine-tuning using labeled data and 2)\nreinforcement learning from offline or online rewards to explore directional\nstimulus that better aligns LLMs with desired behaviors. We evaluate our\nframework on summarization and dialogue response generation tasks. Experimental\nresults show that our framework consistently improves ChatGPT's performance\nover standard prompting with a small collection of training data, and\nreinforcement learning further improves the performance. Notably, on the\nMultWOZ dataset, our framework enables ChatGPT to achieve a remarkable 41.4%\nimprovement in its combined score with only 80 dialogues, matching or even\nsurpassing the performance of some fully trained state-of-the-art models. We\nhave made our code publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1\">Michel Galley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xifeng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Learning Based Multilingual Emoji Prediction In Clean and Attack Scenarios. (arXiv:2304.01005v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01005","description":"<p>Federated learning is a growing field in the machine learning community due\nto its decentralized and private design. Model training in federated learning\nis distributed over multiple clients giving access to lots of client data while\nmaintaining privacy. Then, a server aggregates the training done on these\nmultiple clients without access to their data, which could be emojis widely\nused in any social media service and instant messaging platforms to express\nusers' sentiments. This paper proposes federated learning-based multilingual\nemoji prediction in both clean and attack scenarios. Emoji prediction data have\nbeen crawled from both Twitter and SemEval emoji datasets. This data is used to\ntrain and evaluate different transformer model sizes including a sparsely\nactivated transformer with either the assumption of clean data in all clients\nor poisoned data via label flipping attack in some clients. Experimental\nresults on these models show that federated learning in either clean or\nattacked scenarios performs similarly to centralized training in multilingual\nemoji prediction on seen and unseen languages under different data sources and\ndistributions. Our trained transformers perform better than other techniques on\nthe SemEval emoji dataset in addition to the privacy as well as distributed\nbenefits of federated learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gamal_K/0/1/0/all/0/1\">Karim Gamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaber_A/0/1/0/all/0/1\">Ahmed Gaber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amer_H/0/1/0/all/0/1\">Hossam Amer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit. (arXiv:2304.04596v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2304.04596","description":"<p>ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by\nthe broadening interests of the spoken language translation community.\nESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2)\nsimultaneous speech-to-text translation (SST), and 3) offline speech-to-speech\ntranslation (S2ST) -- each task is supported with a wide variety of approaches,\ndifferentiating ESPnet-ST-v2 from other open source spoken language translation\ntoolkits. This toolkit offers state-of-the-art architectures such as\ntransducers, hybrid CTC/attention, multi-decoders with searchable\nintermediates, time-synchronous blockwise CTC/attention, Translatotron models,\nand direct discrete unit models. In this paper, we describe the overall design,\nexample models for each task, and performance benchmarking behind ESPnet-ST-v2,\nwhich is publicly available at https://github.com/espnet/espnet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalmia_S/0/1/0/all/0/1\">Siddharth Dalmia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polak_P/0/1/0/all/0/1\">Peter Pol&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1\">Patrick Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrebbi_D/0/1/0/all/0/1\">Dan Berrebbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_T/0/1/0/all/0/1\">Tomoki Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaohui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Z/0/1/0/all/0/1\">Zhaoheng Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hira_M/0/1/0/all/0/1\">Moto Hira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiti_S/0/1/0/all/0/1\">Soumi Maiti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Embedding APIs for Information Retrieval. (arXiv:2305.06300v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2305.06300","description":"<p>The ever-increasing size of language models curtails their widespread\navailability to the community, thereby galvanizing many companies into offering\naccess to large language models through APIs. One particular type, suitable for\ndense retrieval, is a semantic embedding service that builds vector\nrepresentations of input text. With a growing number of publicly available\nAPIs, our goal in this paper is to analyze existing offerings in realistic\nretrieval scenarios, to assist practitioners and researchers in finding\nsuitable services according to their needs. Specifically, we investigate the\ncapabilities of existing semantic embedding APIs on domain generalization and\nmultilingual retrieval. For this purpose, we evaluate these services on two\nstandard benchmarks, BEIR and MIRACL. We find that re-ranking BM25 results\nusing the APIs is a budget-friendly approach and is most effective in English,\nin contrast to the standard practice of employing them as first-stage\nretrievers. For non-English retrieval, re-ranking still improves the results,\nbut a hybrid model with BM25 works best, albeit at a higher cost. We hope our\nwork lays the groundwork for evaluating semantic embedding APIs that are\ncritical in search and more broadly, for information access.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamalloo_E/0/1/0/all/0/1\">Ehsan Kamalloo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogundepo_O/0/1/0/all/0/1\">Odunayo Ogundepo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nandan Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfonso_Hermelo_D/0/1/0/all/0/1\">David Alfonso-Hermelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Open-Domain Question Answering in the Era of Large Language Models. (arXiv:2305.06984v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06984","description":"<p>Lexical matching remains the de facto evaluation method for open-domain\nquestion answering (QA). Unfortunately, lexical matching fails completely when\na plausible candidate answer does not appear in the list of gold answers, which\nis increasingly the case as we shift from extractive to generative models. The\nrecent success of large language models (LLMs) for QA aggravates lexical\nmatching failures since candidate answers become longer, thereby making\nmatching with the gold answers even more challenging. Without accurate\nevaluation, the true progress in open-domain QA remains unknown. In this paper,\nwe conduct a thorough analysis of various open-domain QA models, including\nLLMs, by manually evaluating their answers on a subset of NQ-open, a popular\nbenchmark. Our assessments reveal that while the true performance of all models\nis significantly underestimated, the performance of the InstructGPT (zero-shot)\nLLM increases by nearly +60%, making it on par with existing top models, and\nthe InstructGPT (few-shot) model actually achieves a new state-of-the-art on\nNQ-open. We also find that more than 50% of lexical matching failures are\nattributed to semantically equivalent answers. We further demonstrate that\nregex matching ranks QA models consistent with human judgments, although still\nsuffering from unnecessary strictness. Finally, we demonstrate that automated\nevaluation models are a reasonable surrogate for lexical matching in some\ncircumstances, but not for long-form answers generated by LLMs. The automated\nmodels struggle in detecting hallucinations in LLM answers and are thus unable\nto evaluate LLMs. At this time, there appears to be no substitute for human\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamalloo_E/0/1/0/all/0/1\">Ehsan Kamalloo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1\">Nouha Dziri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clarke_C/0/1/0/all/0/1\">Charles L. A. Clarke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafiei_D/0/1/0/all/0/1\">Davood Rafiei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code-Switched Text Synthesis in Unseen Language Pairs. (arXiv:2305.16724v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16724","description":"<p>Existing efforts on text synthesis for code-switching mostly require training\non code-switched texts in the target language pairs, limiting the deployment of\nthe models to cases lacking code-switched data. In this work, we study the\nproblem of synthesizing code-switched texts for language pairs absent from the\ntraining data. We introduce GLOSS, a model built on top of a pre-trained\nmultilingual machine translation model (PMMTM) with an additional\ncode-switching module. This module, either an adapter or extra prefixes, learns\ncode-switching patterns from code-switched data during training, while the\nprimary component of GLOSS, i.e., the PMMTM, is frozen. The design of only\nadjusting the code-switching module prevents our model from overfitting to the\nconstrained training data for code-switching. Hence, GLOSS exhibits the ability\nto generalize and synthesize code-switched texts across a broader spectrum of\nlanguage pairs. Additionally, we develop a self-training algorithm on target\nlanguage pairs further to enhance the reliability of GLOSS. Automatic\nevaluations on four language pairs show that GLOSS achieves at least 55%\nrelative BLEU and METEOR scores improvements compared to strong baselines.\nHuman evaluations on two language pairs further validate the success of GLOSS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1\">Avik Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Shubham Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weaker Than You Think: A Critical Look at Weakly Supervised Learning. (arXiv:2305.17442v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17442","description":"<p>Weakly supervised learning is a popular approach for training machine\nlearning models in low-resource settings. Instead of requesting high-quality\nyet costly human annotations, it allows training models with noisy annotations\nobtained from various weak sources. Recently, many sophisticated approaches\nhave been proposed for robust training under label noise, reporting impressive\nresults. In this paper, we revisit the setup of these approaches and find that\nthe benefits brought by these approaches are significantly overestimated.\nSpecifically, we find that the success of existing weakly supervised learning\napproaches heavily relies on the availability of clean validation samples\nwhich, as we show, can be leveraged much more efficiently by simply training on\nthem. After using these clean labels in training, the advantages of using these\nsophisticated approaches are mostly wiped out. This remains true even when\nreducing the size of the available clean data to just five samples per class,\nmaking these approaches impractical. To understand the true value of weakly\nsupervised learning, we thoroughly analyze diverse NLP datasets and tasks to\nascertain when and why weakly supervised approaches work. Based on our\nfindings, we provide recommendations for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dawei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaoyu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosbach_M/0/1/0/all/0/1\">Marius Mosbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stephan_A/0/1/0/all/0/1\">Andreas Stephan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models are Bounded Pragmatic Speakers: Understanding RLHF from a Bayesian Cognitive Modeling Perspective. (arXiv:2305.17760v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17760","description":"<p>How do language models \"think\"? This paper formulates a probabilistic\ncognitive model called the bounded pragmatic speaker, which can characterize\nthe operation of different variations of language models. Specifically, we\ndemonstrate that large language models fine-tuned with reinforcement learning\nfrom human feedback (Ouyang et al., 2022) embody a model of thought that\nconceptually resembles a fast-and-slow model (Kahneman, 2011), which\npsychologists have attributed to humans. We discuss the limitations of\nreinforcement learning from human feedback as a fast-and-slow model of thought\nand propose avenues for expanding this framework. In essence, our research\nhighlights the value of adopting a cognitive probabilistic modeling approach to\ngain insights into the comprehension, evaluation, and advancement of language\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khanh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BigTranslate: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages. (arXiv:2305.18098v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18098","description":"<p>Large language models (LLMs) demonstrate promising translation performance\namong various natural languages. However, many LLMs especially the open-sourced\nones, such as BLOOM and LLaMA, are English-dominant and support only dozens of\nnatural languages, making the potential of LLMs on language translation less\nexplored. In this work, we present BigTranslate which adapts LLaMA that covers\nonly 20 languages and enhances it with multilingual translation capability on\nmore than 100 languages. BigTranslate is built upon LLaMA-13B and it is\noptimized in three steps. First, we continue training LLaMA with massive\nChinese monolingual data. Second, we continue training the model with a\nlarge-scale parallel dataset that covers 102 natural languages. Third, we\ninstruct-tune the foundation model with multilingual translation instructions,\nleading to our BigTranslate model. The preliminary experiments on multilingual\ntranslation show that BigTranslate performs comparably with ChatGPT and Google\nTranslate in many languages and even outperforms ChatGPT in 8 language pairs.\nWe release the BigTranslate model and hope it can advance the research\nprogress.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_C/0/1/0/all/0/1\">Chengqing Zong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MISGENDERED: Limits of Large Language Models in Understanding Pronouns. (arXiv:2306.03950v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.03950","description":"<p>Content Warning: This paper contains examples of misgendering and erasure\nthat could be offensive and potentially triggering.\n</p>\n<p>Gender bias in language technologies has been widely studied, but research\nhas mostly been restricted to a binary paradigm of gender. It is essential also\nto consider non-binary gender identities, as excluding them can cause further\nharm to an already marginalized group. In this paper, we comprehensively\nevaluate popular language models for their ability to correctly use English\ngender-neutral pronouns (e.g., singular they, them) and neo-pronouns (e.g., ze,\nxe, thon) that are used by individuals whose gender identity is not represented\nby binary pronouns. We introduce MISGENDERED, a framework for evaluating large\nlanguage models' ability to correctly use preferred pronouns, consisting of (i)\ninstances declaring an individual's pronoun, followed by a sentence with a\nmissing pronoun, and (ii) an experimental setup for evaluating masked and\nauto-regressive language models using a unified method. When prompted\nout-of-the-box, language models perform poorly at correctly predicting\nneo-pronouns (averaging 7.7% accuracy) and gender-neutral pronouns (averaging\n34.2% accuracy). This inability to generalize results from a lack of\nrepresentation of non-binary pronouns in training data and memorized\nassociations. Few-shot adaptation with explicit examples in the prompt improves\nperformance for neo-pronouns, but only to 64.7% even with 20 shots. We release\nthe full dataset, code, and demo at\nhttps://tamannahossainkay.github.io/misgendered/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hossain_T/0/1/0/all/0/1\">Tamanna Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Labeling of German Chest X-Ray Radiology Reports using Deep Learning. (arXiv:2306.05997v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.05997","description":"<p>Radiologists are in short supply globally, and deep learning models offer a\npromising solution to address this shortage as part of clinical\ndecision-support systems. However, training such models often requires\nexpensive and time-consuming manual labeling of large datasets. Automatic label\nextraction from radiology reports can reduce the time required to obtain\nlabeled datasets, but this task is challenging due to semantically similar\nwords and missing annotated data. In this work, we explore the potential of\nweak supervision of a deep learning-based label prediction model, using a\nrule-based labeler. We propose a deep learning-based CheXpert label prediction\nmodel, pre-trained on reports labeled by a rule-based German CheXpert model and\nfine-tuned on a small dataset of manually labeled reports. Our results\ndemonstrate the effectiveness of our approach, which significantly outperformed\nthe rule-based model on all three tasks. Our findings highlight the benefits of\nemploying deep learning-based models even in scenarios with sparse data and the\nuse of the rule-based labeler as a tool for weak supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wollek_A/0/1/0/all/0/1\">Alessandro Wollek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haitzer_P/0/1/0/all/0/1\">Philip Haitzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedlmeyr_T/0/1/0/all/0/1\">Thomas Sedlmeyr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyska_S/0/1/0/all/0/1\">Sardi Hyska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckel_J/0/1/0/all/0/1\">Johannes Rueckel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabel_B/0/1/0/all/0/1\">Bastian Sabel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingrisch_M/0/1/0/all/0/1\">Michael Ingrisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasser_T/0/1/0/all/0/1\">Tobias Lasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KoLA: Carefully Benchmarking World Knowledge of Large Language Models. (arXiv:2306.09296v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.09296","description":"<p>The unprecedented performance of large language models (LLMs) necessitates\nimprovements in evaluations. Rather than merely exploring the breadth of LLM\nabilities, we believe meticulous and thoughtful designs are essential to\nthorough, unbiased, and applicable evaluations. Given the importance of world\nknowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark\n(KoLA), in which we carefully design three crucial factors: (1) For ability\nmodeling, we mimic human cognition to form a four-level taxonomy of\nknowledge-related abilities, covering $19$ tasks. (2) For data, to ensure fair\ncomparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs,\nalong with continuously collected emerging corpora, aiming to evaluate the\ncapacity to handle unseen data and evolving knowledge. (3) For evaluation\ncriteria, we adopt a contrastive system, including overall standard scores for\nbetter numerical comparability across tasks and models and a unique\nself-contrast metric for automatically evaluating knowledge hallucination. We\nevaluate $21$ open-source and commercial LLMs and obtain some intriguing\nfindings. The KoLA dataset and open-participation leaderboard are publicly\nreleased at https://kola.xlore.cn and will be continuously updated to provide\nreferences for developing LLMs and knowledge-related systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jifan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_S/0/1/0/all/0/1\">Shangqing Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shulin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Li_D/0/1/0/all/0/1\">Daniel Zhang-Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1\">Xin Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zijun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hanming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yushi Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yantao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_A/0/1/0/all/0/1\">Amy Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nianyi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_K/0/1/0/all/0/1\">Kaifeng Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_L/0/1/0/all/0/1\">Linlu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianhui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhili Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yunjia Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weikai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yong Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_K/0/1/0/all/0/1\">Kaisheng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Ji Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by Whispering to ChatGPT. (arXiv:2306.17103v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.17103","description":"<p>We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic\nlyrics transcription method achieving state-of-the-art performance on various\nlyrics transcription datasets, even in challenging genres such as rock and\nmetal. Our novel, training-free approach utilizes Whisper, a weakly supervised\nrobust speech recognition model, and GPT-4, today's most performant chat-based\nlarge language model. In the proposed method, Whisper functions as the \"ear\" by\ntranscribing the audio, while GPT-4 serves as the \"brain,\" acting as an\nannotator with a strong performance for contextualized output selection and\ncorrection. Our experiments show that LyricWhiz significantly reduces Word\nError Rate compared to existing methods in English and can effectively\ntranscribe lyrics across multiple languages. Furthermore, we use LyricWhiz to\ncreate the first publicly available, large-scale, multilingual lyrics\ntranscription dataset with a CC-BY-NC-SA copyright license, based on\nMTG-Jamendo, and offer a human-annotated subset for noise level estimation and\nevaluation. We anticipate that our proposed method and dataset will advance the\ndevelopment of multilingual lyrics transcription, a challenging and emerging\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_L/0/1/0/all/0/1\">Le Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1\">Ruibin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jiahao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yinghao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LI_Y/0/1/0/all/0/1\">Yizhi LI</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Ge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dannenberg_R/0/1/0/all/0/1\">Roger Dannenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1\">Emmanouil Benetos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Wei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Performance Comparison of Large Language Models on VNHSGE English Dataset: OpenAI ChatGPT, Microsoft Bing Chat, and Google Bard. (arXiv:2307.02288v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.02288","description":"<p>This paper presents a performance comparison of three large language models\n(LLMs), namely OpenAI ChatGPT, Microsoft Bing Chat, and Google Bard, on the\nVNHSGE English dataset. The results show that BingChat is better than ChatGPT\nand Bard. Therefore, BingChat and Bard can replace ChatGPT while ChatGPT is not\nyet officially available in Vietnam. The results also indicate that ChatGPT,\nBing Chat, and Bard outperform Vietnamese students in English language\nproficiency. The findings of this study contribute to the understanding of the\npotential of LLMs in English language education. The remarkable performance of\nChatGPT, Bing Chat, and Bard demonstrates their potential as effective tools\nfor teaching and learning English at the high school level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dao_X/0/1/0/all/0/1\">Xuan-Quy Dao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Should Data Science Education Do with Large Language Models?. (arXiv:2307.02792v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2307.02792","description":"<p>The rapid advances of large language models (LLMs), such as ChatGPT, are\nrevolutionizing data science and statistics. These state-of-the-art tools can\nstreamline complex processes. As a result, it reshapes the role of data\nscientists. We argue that LLMs are transforming the responsibilities of data\nscientists, shifting their focus from hands-on coding, data-wrangling and\nconducting standard analyses to assessing and managing analyses performed by\nthese automated AIs. This evolution of roles is reminiscent of the transition\nfrom a software engineer to a product manager. We illustrate this transition\nwith concrete data science case studies using LLMs in this paper. These\ndevelopments necessitate a meaningful evolution in data science education.\nPedagogy must now place greater emphasis on cultivating diverse skillsets among\nstudents, such as LLM-informed creativity, critical thinking, AI-guided\nprogramming. LLMs can also play a significant role in the classroom as\ninteractive teaching and learning tools, contributing to personalized\neducation. This paper discusses the opportunities, resources and open\nchallenges for each of these directions. As with any transformative technology,\nintegrating LLMs into education calls for careful consideration. While LLMs can\nperform repetitive tasks efficiently, it's crucial to remember that their role\nis to supplement human intelligence and creativity, not to replace it.\nTherefore, the new era of data science education should balance the benefits of\nLLMs while fostering complementary human expertise and innovations. In\nconclusion, the rise of LLMs heralds a transformative period for data science\nand its education. This paper seeks to shed light on the emerging trends,\npotential opportunities, and challenges accompanying this paradigm shift,\nhoping to spark further discourse and investigation into this exciting,\nuncharted territory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_X/0/1/0/all/0/1\">Xinming Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linjun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Relationship Between Speech Features Changes When You Get Depressed: Feature Correlations for Improving Speed and Performance of Depression Detection. (arXiv:2307.02892v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.02892","description":"<p>This work shows that depression changes the correlation between features\nextracted from speech. Furthermore, it shows that using such an insight can\nimprove the training speed and performance of depression detectors based on\nSVMs and LSTMs. The experiments were performed over the Androids Corpus, a\npublicly available dataset involving 112 speakers, including 58 people\ndiagnosed with depression by professional psychiatrists. The results show that\nthe models used in the experiments improve in terms of training speed and\nperformance when fed with feature correlation matrices rather than with feature\nvectors. The relative reduction of the error rate ranges between 23.1% and\n26.6% depending on the model. The probable explanation is that feature\ncorrelation matrices appear to be more variable in the case of depressed\nspeakers. Correspondingly, such a phenomenon can be thought of as a depression\nmarker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_F/0/1/0/all/0/1\">Fuxiang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xuri Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esposito_A/0/1/0/all/0/1\">Anna Esposito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinciarelli_A/0/1/0/all/0/1\">Alessandro Vinciarelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Multi-valued Relations from Language Models. (arXiv:2307.03122v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.03122","description":"<p>The widespread usage of latent language representations via pre-trained\nlanguage models (LMs) suggests that they are a promising source of structured\nknowledge. However, existing methods focus only on a single object per\nsubject-relation pair, even though often multiple objects are correct. To\novercome this limitation, we analyze these representations for their potential\nto yield materialized multi-object relational knowledge. We formulate the\nproblem as a rank-then-select task. For ranking candidate objects, we evaluate\nexisting prompting techniques and propose new ones incorporating domain\nknowledge. Among the selection methods, we find that choosing objects with a\nlikelihood above a learned relation-specific threshold gives a 49.5% F1 score.\nOur results highlight the difficulty of employing LMs for the multi-valued\nslot-filling task and pave the way for further research on extracting\nrelational knowledge from latent language representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singhania_S/0/1/0/all/0/1\">Sneha Singhania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1\">Simon Razniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-07-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}