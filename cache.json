{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-03-30T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Zero-Shot Generalizable End-to-End Task-Oriented Dialog System using Context Summarization and Domain Schema. (arXiv:2303.16252v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16252","description":"<p>Task-oriented dialog systems empower users to accomplish their goals by\nfacilitating intuitive and expressive natural language interactions.\nState-of-the-art approaches in task-oriented dialog systems formulate the\nproblem as a conditional sequence generation task and fine-tune pre-trained\ncausal language models in the supervised setting. This requires labeled\ntraining data for each new domain or task, and acquiring such data is\nprohibitively laborious and expensive, thus making it a bottleneck for scaling\nsystems to a wide range of domains. To overcome this challenge, we introduce a\nnovel Zero-Shot generalizable end-to-end Task-oriented Dialog system, ZS-ToD,\nthat leverages domain schemas to allow for robust generalization to unseen\ndomains and exploits effective summarization of the dialog history. We employ\nGPT-2 as a backbone model and introduce a two-step training process where the\ngoal of the first step is to learn the general structure of the dialog data and\nthe second step optimizes the response generation as well as intermediate\noutputs, such as dialog state and system actions. As opposed to\nstate-of-the-art systems that are trained to fulfill certain intents in the\ngiven domains and memorize task-specific conversational patterns, ZS-ToD learns\ngeneric task-completion skills by comprehending domain semantics via domain\nschemas and generalizing to unseen domains seamlessly. We conduct an extensive\nexperimental evaluation on SGD and SGD-X datasets that span up to 20 unique\ndomains and ZS-ToD outperforms state-of-the-art systems on key metrics, with an\nimprovement of +17% on joint goal accuracy and +5 on inform. Additionally, we\npresent a detailed ablation study to demonstrate the effectiveness of the\nproposed components and training mechanism\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mosharrof_A/0/1/0/all/0/1\">Adib Mosharrof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maqbool_M/0/1/0/all/0/1\">M.H. Maqbool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddique_A/0/1/0/all/0/1\">A.B. Siddique</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable handwritten text recognition system for lexicographic sources of under-resourced languages and alphabets. (arXiv:2303.16256v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16256","description":"<p>The paper discusses an approach to decipher large collections of handwritten\nindex cards of historical dictionaries. Our study provides a working solution\nthat reads the cards, and links their lemmas to a searchable list of dictionary\nentries, for a large historical dictionary entitled the Dictionary of the 17th-\nand 18th-century Polish, which comprizes 2.8 million index cards. We apply a\ntailored handwritten text recognition (HTR) solution that involves (1) an\noptimized detection model; (2) a recognition model to decipher the handwritten\ncontent, designed as a spatial transformer network (STN) followed by\nconvolutional neural network (RCNN) with a connectionist temporal\nclassification layer (CTC), trained using a synthetic set of 500,000 generated\nPolish words of different length; (3) a post-processing step using constrained\nWord Beam Search (WBC): the predictions were matched against a list of\ndictionary entries known in advance. Our model achieved the accuracy of 0.881\non the word level, which outperforms the base RCNN model. Within this study we\nproduced a set of 20,000 manually annotated index cards that can be used for\nfuture benchmarks and transfer learning HTR applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Idziak_J/0/1/0/all/0/1\">Jan Idziak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sela_A/0/1/0/all/0/1\">Artjoms &#x160;e&#x13c;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wozniak_M/0/1/0/all/0/1\">Micha&#x142; Wo&#x17a;niak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lesniak_A/0/1/0/all/0/1\">Albert Le&#x15b;niak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byszuk_J/0/1/0/all/0/1\">Joanna Byszuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eder_M/0/1/0/all/0/1\">Maciej Eder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Writing Assistants Should Model Social Factors of Language. (arXiv:2303.16275v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16275","description":"<p>Intelligent writing assistants powered by large language models (LLMs) are\nmore popular today than ever before, but their further widespread adoption is\nprecluded by sub-optimal performance. In this position paper, we argue that a\nmajor reason for this sub-optimal performance and adoption is a singular focus\non the information content of language while ignoring its social aspects. We\nanalyze the different dimensions of these social factors in the context of\nwriting assistants and propose their incorporation into building smarter, more\neffective, and truly personalized writing assistants that would enrich the user\nexperience and contribute to increased user adoption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_V/0/1/0/all/0/1\">Vivek Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raheja_V/0/1/0/all/0/1\">Vipul Raheja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube. (arXiv:2303.16281v1 [cs.CY])","link":"http://arxiv.org/abs/2303.16281","description":"<p>Contrary to Google Search's mission of delivering information from \"many\nangles so you can form your own understanding of the world,\" we find that\nGoogle and its most prominent returned results -- Wikipedia and YouTube, simply\nreflect the narrow set of cultural stereotypes tied to the search language for\ncomplex topics like \"Buddhism,\" \"Liberalism,\" \"colonization,\" \"Iran\" and\n\"America.\" Simply stated, they present, to varying degrees, distinct\ninformation across the same search in different languages (we call it 'language\nbias'). Instead of presenting a global picture of a complex topic, our online\nsearches turn us into the proverbial blind person touching a small portion of\nan elephant, ignorant of the existence of other cultural perspectives. The\nlanguage we use to search ends up as a cultural filter to promote ethnocentric\nviews, where a person evaluates other people or ideas based on their own\nculture. We also find that language bias is deeply embedded in ChatGPT. As it\nis primarily trained on English language data, it presents the Anglo-American\nperspective as the normative view, reducing the complexity of a multifaceted\nissue to the single Anglo-American standard. In this paper, we present evidence\nand analysis of language bias and discuss its larger social implications.\nToward the end of the paper, we propose a potential framework of using\nautomatic translation to leverage language bias and argue that the task of\npiecing together a genuine depiction of the elephant is a challenging and\nimportant endeavor that deserves a new area of research in NLP and requires\ncollaboration with scholars from the humanities to create ethically sound and\nsocially responsible technology together.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Q/0/1/0/all/0/1\">Queenie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puett_M/0/1/0/all/0/1\">Michael J. Puett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1\">Michael D. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-Guided Audio-Visual Source Separation via Trimodal Consistency. (arXiv:2303.16342v1 [cs.CV])","link":"http://arxiv.org/abs/2303.16342","description":"<p>We propose a self-supervised approach for learning to perform audio source\nseparation in videos based on natural language queries, using only unlabeled\nvideo and audio pairs as training data. A key challenge in this task is\nlearning to associate the linguistic description of a sound-emitting object to\nits visual features and the corresponding components of the audio waveform, all\nwithout access to annotations during training. To overcome this challenge, we\nadapt off-the-shelf vision-language foundation models to provide pseudo-target\nsupervision via two novel loss functions and encourage a stronger alignment\nbetween the audio, visual and natural language modalities. During inference,\nour approach can separate sounds given text, video and audio input, or given\ntext and audio input alone. We demonstrate the effectiveness of our\nself-supervised approach on three audio-visual separation datasets, including\nMUSIC, SOLOS and AudioSet, where we outperform state-of-the-art strongly\nsupervised approaches despite not using object detectors or text labels during\ntraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1\">Reuben Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1\">Arijit Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burns_A/0/1/0/all/0/1\">Andrea Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1\">Bryan A. Plummer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salamon_J/0/1/0/all/0/1\">Justin Salamon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieto_O/0/1/0/all/0/1\">Oriol Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_B/0/1/0/all/0/1\">Bryan Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT or academic scientist? Distinguishing authorship with over 99% accuracy using off-the-shelf machine learning tools. (arXiv:2303.16352v1 [cs.LG])","link":"http://arxiv.org/abs/2303.16352","description":"<p>ChatGPT has enabled access to AI-generated writing for the masses, and within\njust a few months, this product has disrupted the knowledge economy, initiating\na culture shift in the way people work, learn, and write. The need to\ndiscriminate human writing from AI is now both critical and urgent,\nparticularly in domains like higher education and academic writing, where AI\nhad not been a significant threat or contributor to authorship. Addressing this\nneed, we developed a method for discriminating text generated by ChatGPT from\n(human) academic scientists, relying on prevalent and accessible supervised\nclassification methods. We focused on how a particular group of humans,\nacademic scientists, write differently than ChatGPT, and this targeted approach\nled to the discovery of new features for discriminating (these) humans from AI;\nas examples, scientists write long paragraphs and have a penchant for equivocal\nlanguage, frequently using words like but, however, and although. With a set of\n20 features, including the aforementioned ones and others, we built a model\nthat assigned the author, as human or AI, at well over 99% accuracy, resulting\nin 20 times fewer misclassified documents compared to the field-leading\napproach. This strategy for discriminating a particular set of humans writing\nfrom AI could be further adapted and developed by others with basic skills in\nsupervised classification, enabling access to many highly accurate and targeted\nmodels for detecting AI usage in academic writing and beyond.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Desaire_H/0/1/0/all/0/1\">Heather Desaire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_A/0/1/0/all/0/1\">Aleesa E. Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isom_M/0/1/0/all/0/1\">Madeline Isom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarosova_R/0/1/0/all/0/1\">Romana Jarosova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_D/0/1/0/all/0/1\">David Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Video-Moment Retrieval and Step-Captioning. (arXiv:2303.16406v1 [cs.CV])","link":"http://arxiv.org/abs/2303.16406","description":"<p>There is growing interest in searching for information from large video\ncorpora. Prior works have studied relevant tasks, such as text-based video\nretrieval, moment retrieval, video summarization, and video captioning in\nisolation, without an end-to-end setup that can jointly search from video\ncorpora and generate summaries. Such an end-to-end setup would allow for many\ninteresting applications, e.g., a text-based search that finds a relevant video\nfrom a video corpus, extracts the most relevant moment from that video, and\nsegments the moment into important steps with captions. To address this, we\npresent the HiREST (HIerarchical REtrieval and STep-captioning) dataset and\npropose a new benchmark that covers hierarchical information retrieval and\nvisual/textual stepwise summarization from an instructional video corpus.\nHiREST consists of 3.4K text-video pairs from an instructional video dataset,\nwhere 1.1K videos have annotations of moment spans relevant to text query and\nbreakdown of each moment into key instruction steps with caption and timestamps\n(totaling 8.6K step captions). Our hierarchical benchmark consists of video\nretrieval, moment retrieval, and two novel moment segmentation and step\ncaptioning tasks. In moment segmentation, models break down a video moment into\ninstruction steps and identify start-end boundaries. In step captioning, models\ngenerate a textual summary for each step. We also present starting point\ntask-specific and end-to-end joint baseline models for our new benchmark. While\nthe baseline models show some promising results, there still exists large room\nfor future improvement by the community. Project website:\nhttps://hirest-cvpr2023.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zala_A/0/1/0/all/0/1\">Abhay Zala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kottur_S/0/1/0/all/0/1\">Satwik Kottur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas O&#x11f;uz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yasher Mehdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Clinical Entity Recognition using ChatGPT. (arXiv:2303.16416v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16416","description":"<p>In this study, we investigated the potential of ChatGPT, a large language\nmodel developed by OpenAI, for the clinical named entity recognition task\ndefined in the 2010 i2b2 challenge, in a zero-shot setting with two different\nprompt strategies. We compared its performance with GPT-3 in a similar\nzero-shot setting, as well as a fine-tuned BioClinicalBERT model using a set of\nsynthetic clinical notes from MTSamples. Our findings revealed that ChatGPT\noutperformed GPT-3 in the zero-shot setting, with F1 scores of 0.418 (vs.0.250)\nand 0.620 (vs. 0.480) for exact- and relaxed-matching, respectively. Moreover,\nprompts affected ChatGPT's performance greatly, with relaxed-matching F1 scores\nof 0.628 vs.0.541 for two different prompt strategies. Although ChatGPT's\nperformance was still lower than that of the supervised BioClinicalBERT model\n(i.e., relaxed-matching F1 scores of 0.628 vs. 0.870), our study demonstrates\nthe great potential of ChatGPT for clinical NER tasks in a zero-shot setting,\nwhich is much more appealing as it does not require any annotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ameer_I/0/1/0/all/0/1\">Iqra Ameer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xu Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xueqing Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yujia Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zehan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianfu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models. (arXiv:2303.16421v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16421","description":"<p>Large language models (LLMs) such as ChatGPT and GPT-4 have made significant\nprogress in NLP. However, their ability to memorize, represent, and leverage\ncommonsense knowledge has been a well-known pain point for LLMs. It remains\nunclear that: (1) Can GPTs effectively answer commonsense questions? (2) Are\nGPTs knowledgeable in commonsense? (3) Are GPTs aware of the underlying\ncommonsense knowledge for answering a specific question? (4) Can GPTs\neffectively leverage commonsense for answering questions? To evaluate the above\ncommonsense problems, we conduct a series of experiments to evaluate ChatGPT's\ncommonsense abilities, and the experimental results show that: (1) GPTs can\nachieve good QA accuracy in commonsense tasks, while they still struggle with\ncertain types of knowledge. (2) ChatGPT is knowledgeable, and can accurately\ngenerate most of the commonsense knowledge using knowledge prompts. (3) Despite\nits knowledge, ChatGPT is an inexperienced commonsense problem solver, which\ncannot precisely identify the needed commonsense knowledge for answering a\nspecific question, i.e., ChatGPT does not precisely know what commonsense\nknowledge is required to answer a question. The above findings raise the need\nto investigate better mechanisms for utilizing commonsense knowledge in LLMs,\nsuch as instruction following, better commonsense guidance, etc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bian_N/0/1/0/all/0/1\">Ning Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yaojie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Ben He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs. (arXiv:2303.16434v1 [cs.AI])","link":"http://arxiv.org/abs/2303.16434","description":"<p>Artificial Intelligence (AI) has made incredible progress recently. On the\none hand, advanced foundation models like ChatGPT can offer powerful\nconversation, in-context learning and code generation abilities on a broad\nrange of open-domain tasks. They can also generate high-level solution outlines\nfor domain-specific tasks based on the common sense knowledge they have\nacquired. However, they still face difficulties with some specialized tasks\nbecause they lack enough domain-specific data during pre-training or they often\nhave errors in their neural network computations on those tasks that need\naccurate executions. On the other hand, there are also many existing models and\nsystems (symbolic-based or neural-based) that can do some domain-specific tasks\nvery well. However, due to the different implementation or working mechanisms,\nthey are not easily accessible or compatible with foundation models. Therefore,\nthere is a clear and pressing need for a mechanism that can leverage foundation\nmodels to propose task solution outlines and then automatically match some of\nthe sub-tasks in the outlines to the off-the-shelf models and systems with\nspecial functionalities to complete them. Inspired by this, we introduce\nTaskMatrix.AI as a new AI ecosystem that connects foundation models with\nmillions of APIs for task completion. Unlike most previous work that aimed to\nimprove a single AI model, TaskMatrix.AI focuses more on using existing\nfoundation models (as a brain-like central system) and APIs of other AI models\nand systems (as sub-task solvers) to achieve diversified tasks in both digital\nand physical domains. As a position paper, we will present our vision of how to\nbuild such an ecosystem, explain each key component, and use study cases to\nillustrate both the feasibility of this vision and the main challenges we need\nto address next.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yaobo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1\">Ting Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenshan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1\">Yang Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shuai Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1\">Lei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shaoguang Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_L/0/1/0/all/0/1\">Linjun Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Ming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning. (arXiv:2303.16445v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16445","description":"<p>Language model probing is often used to test specific capabilities of these\nmodels. However, conclusions from such studies may be limited when the probing\nbenchmarks are small and lack statistical power. In this work, we introduce\nnew, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500)\ninspired by psycholinguistic studies. We dramatically extend existing NEG-136\nand ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44\nsentence pairs to 750 each. We also create another version of extended negation\ndataset (NEG-1500-SIMP-TEMP), created using template-based generation. It\nconsists of 770 sentence pairs. We evaluate 22 models on the extended datasets,\nseeing model performance dip 20-57% compared to the original smaller\nbenchmarks. We observe high levels of negation sensitivity in models like BERT\nand ALBERT demonstrating that previous findings might have been skewed due to\nsmaller test sets. Finally, we observe that while GPT3 has generated all the\nexamples in ROLE-1500 is only able to solve 24.6% of them during probing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shivagunde_N/0/1/0/all/0/1\">Namrata Shivagunde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lialin_V/0/1/0/all/0/1\">Vladislav Lialin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1\">Anna Rumshisky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building a Knowledge Graph of Distributed Ledger Technologies. (arXiv:2303.16528v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16528","description":"<p>Distributed ledger systems have become more prominent and successful in\nrecent years, with a focus on blockchains and cryptocurrency. This has led to\nvarious misunderstandings about both the technology itself and its\ncapabilities, as in many cases blockchain and cryptocurrency is used\nsynonymously and other applications are often overlooked. Therefore, as a\nwhole, the view of distributed ledger technology beyond blockchains and\ncryptocurrencies is very limited. Existing vocabularies and ontologies often\nfocus on single aspects of the technology, or in some cases even just on one\nproduct. This potentially leads to other types of distributed ledgers and their\npossible use cases being neglected. In this paper, we present a knowledge graph\nand an ontology for distributed ledger technologies, which includes security\nconsiderations to model aspects such as threats and vulnerabilities,\napplication domains, as well as relevant standards and regulations. Such a\nknowledge graph improves the overall understanding of distributed ledgers,\nreveals their strengths, and supports the work of security personnel, i.e.\nanalysts and system architects. We discuss potential uses and follow semantic\nweb best practices to evaluate and publish the ontology and knowledge graph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Konig_L/0/1/0/all/0/1\">Lukas K&#xf6;nig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumaier_S/0/1/0/all/0/1\">Sebastian Neumaier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LMExplainer: a Knowledge-Enhanced Explainer for Language Models. (arXiv:2303.16537v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16537","description":"<p>Large language models (LMs) such as GPT-4 are very powerful and can process\ndifferent kinds of natural language processing (NLP) tasks. However, it can be\ndifficult to interpret the results due to the multi-layer nonlinear model\nstructure and millions of parameters. Lack of understanding of how the model\nworks can make the model unreliable and dangerous for everyday users in\nreal-world scenarios. Most recent works exploit the weights of attention to\nprovide explanations for model predictions. However, pure attention-based\nexplanation is unable to support the growing complexity of the models, and\ncannot reason about their decision-making processes. Thus, we propose\nLMExplainer, a knowledge-enhanced interpretation module for language models\nthat can provide human-understandable explanations. We use a knowledge graph\n(KG) and a graph attention neural network to extract the key decision signals\nof the LM. We further explore whether interpretation can also help AI\nunderstand the task better. Our experimental results show that LMExplainer\noutperforms existing LM+KG methods on CommonsenseQA and OpenBookQA. We also\ncompare the explanation results with generated explanation methods and\nhuman-annotated results. The comparison shows our method can provide more\ncomprehensive and clearer explanations. LMExplainer demonstrates the potential\nto enhance model performance and furnish explanations for the reasoning\nprocesses of models in natural language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zichen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Ambuj K Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sra_M/0/1/0/all/0/1\">Misha Sra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations. (arXiv:2303.16618v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16618","description":"<p>Personalisation of language models for dialogue sensitises them to better\ncapture the speaking patterns of people of specific characteristics, and/or in\nspecific environments. However, rich character annotations are difficult to\ncome by and to successfully leverage. In this work, we release and describe a\nnovel set of manual annotations for 863 speakers from the popular Cornell Movie\nDialog Corpus, including features like characteristic quotes and character\ndescriptions, and a set of six automatically extracted metadata for over 95% of\nthe featured films. We perform extensive experiments on two corpora and show\nthat such annotations can be effectively used to personalise language models,\nreducing perplexity by up to 8.5%. Our method can be applied even zero-shot for\nspeakers for whom no prior training data is available, by relying on\ncombinations of characters' demographic characteristics. Since collecting such\nmetadata is costly, we also contribute a cost-benefit analysis to highlight\nwhich annotations were most cost-effective relative to the reduction in\nperplexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vincent_S/0/1/0/all/0/1\">Sebastian Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumner_R/0/1/0/all/0/1\">Rowanne Sumner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dowek_A/0/1/0/all/0/1\">Alice Dowek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1\">Charlotte Blundell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preston_E/0/1/0/all/0/1\">Emily Preston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayliss_C/0/1/0/all/0/1\">Chris Bayliss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oakley_C/0/1/0/all/0/1\">Chris Oakley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AraSpot: Arabic Spoken Command Spotting. (arXiv:2303.16621v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16621","description":"<p>Spoken keyword spotting (KWS) is the task of identifying a keyword in an\naudio stream and is widely used in smart devices at the edge in order to\nactivate voice assistants and perform hands-free tasks. The task is daunting as\nthere is a need, on the one hand, to achieve high accuracy while at the same\ntime ensuring that such systems continue to run efficiently on low power and\npossibly limited computational capabilities devices. This work presents AraSpot\nfor Arabic keyword spotting trained on 40 Arabic keywords, using different\nonline data augmentation, and introducing ConformerGRU model architecture.\nFinally, we further improve the performance of the model by training a\ntext-to-speech model for synthetic data generation. AraSpot achieved a\nState-of-the-Art SOTA 99.59% result outperforming previous approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salhab_M/0/1/0/all/0/1\">Mahmoud Salhab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harmanani_H/0/1/0/all/0/1\">Haidar Harmanani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPTEval: NLG Evaluation using GPT-4 with Better Human Alignment. (arXiv:2303.16634v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16634","description":"<p>The quality of texts generated by natural language generation (NLG) systems\nis hard to measure automatically. Conventional reference-based metrics, such as\nBLEU and ROUGE, have been shown to have relatively low correlation with human\njudgments, especially for tasks that require creativity and diversity. Recent\nstudies suggest using large language models (LLMs) as reference-free metrics\nfor NLG evaluation, which have the benefit of being applicable to new tasks\nthat lack human references. However, these LLM-based evaluators still have\nlower human correspondence than medium-size neural evaluators. In this work, we\npresent GPTEval, a framework of using large language models with\nchain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of\nNLG outputs. We experiment with two generation tasks, text summarization and\ndialogue generation. We show that GPTEval with GPT-4 as the backbone model\nachieves a Spearman correlation of 0.514 with human on summarization task,\noutperforming all previous methods by a large margin. We also propose\npreliminary analysis on the behavior of LLM-based evaluators, and highlight the\npotential issue of LLM-based evaluators having a bias towards the LLM-generated\ntexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iter_D/0/1/0/all/0/1\">Dan Iter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summarizing Indian Languages using Multilingual Transformers based Models. (arXiv:2303.16657v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16657","description":"<p>With the advent of multilingual models like mBART, mT5, IndicBART etc.,\nsummarization in low resource Indian languages is getting a lot of attention\nnow a days. But still the number of datasets is low in number. In this work, we\n(Team HakunaMatata) study how these multilingual models perform on the datasets\nwhich have Indian languages as source and target text while performing\nsummarization. We experimented with IndicBART and mT5 models to perform the\nexperiments and report the ROUGE-1, ROUGE-2, ROUGE-3 and ROUGE-4 scores as a\nperformance metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taunk_D/0/1/0/all/0/1\">Dhaval Taunk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_V/0/1/0/all/0/1\">Vasudeva Varma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Semantic Similarity and Text Embedding to Measure the Social Media Echo of Strategic Communications. (arXiv:2303.16694v1 [cs.SI])","link":"http://arxiv.org/abs/2303.16694","description":"<p>Online discourse covers a wide range of topics and many actors tailor their\ncontent to impact online discussions through carefully crafted messages and\ntargeted campaigns. Yet the scale and diversity of online media content make it\ndifficult to evaluate the impact of a particular message. In this paper, we\npresent a new technique that leverages semantic similarity to quantify the\nchange in the discussion after a particular message has been published. We use\na set of press releases from environmental organisations and tweets from the\nclimate change debate to show that our novel approach reveals a heavy-tailed\ndistribution of response in online discourse to strategic communications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cann_T/0/1/0/all/0/1\">Tristan J.B. Cann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dennes_B/0/1/0/all/0/1\">Ben Dennes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coan_T/0/1/0/all/0/1\">Travis Coan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ONeill_S/0/1/0/all/0/1\">Saffron O&#x27;Neill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_H/0/1/0/all/0/1\">Hywel T.P. Williams</a> (University of Exeter)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text revision in Scientific Writing Assistance: An Overview. (arXiv:2303.16726v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16726","description":"<p>Writing a scientific article is a challenging task as it is a highly codified\ngenre. Good writing skills are essential to properly convey ideas and results\nof research work. Since the majority of scientific articles are currently\nwritten in English, this exercise is all the more difficult for non-native\nEnglish speakers as they additionally have to face language issues. This\narticle aims to provide an overview of text revision in writing assistance in\nthe scientific domain.\n</p>\n<p>We will examine the specificities of scientific writing, including the format\nand conventions commonly used in research articles.\n</p>\n<p>Additionally, this overview will explore the various types of writing\nassistance tools available for text revision. Despite the evolution of the\ntechnology behind these tools through the years, from rule-based approaches to\ndeep neural-based ones, challenges still exist (tools' accessibility, limited\nconsideration of the context, inexplicit use of discursive information, etc.)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jourdan_L/0/1/0/all/0/1\">L&#xe9;ane Jourdan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boudin_F/0/1/0/all/0/1\">Florian Boudin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufour_R/0/1/0/all/0/1\">Richard Dufour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_N/0/1/0/all/0/1\">Nicolas Hernandez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating NLG systems: A brief introduction. (arXiv:2303.16742v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16742","description":"<p>This year the International Conference on Natural Language Generation (INLG)\nwill feature an award for the paper with the best evaluation. The purpose of\nthis award is to provide an incentive for NLG researchers to pay more attention\nto the way they assess the output of their systems. This essay provides a short\nintroduction to evaluation in NLG, explaining key terms and distinctions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miltenburg_E/0/1/0/all/0/1\">Emiel van Miltenburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Code Generation by Training with Natural Language Feedback. (arXiv:2303.16749v1 [cs.SE])","link":"http://arxiv.org/abs/2303.16749","description":"<p>The potential for pre-trained large language models (LLMs) to use natural\nlanguage feedback at inference time has been an exciting recent development. We\nbuild upon this observation by formalizing an algorithm for learning from\nnatural language feedback at training time instead, which we call Imitation\nlearning from Language Feedback (ILF). ILF requires only a small amount of\nhuman-written feedback during training and does not require the same feedback\nat test time, making it both user-friendly and sample-efficient. We further\nshow that ILF can be seen as a form of minimizing the KL divergence to the\nground truth distribution and demonstrate a proof-of-concept on a neural\nprogram synthesis task. We use ILF to improve a Codegen-Mono 6.1B model's\npass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic Python\nProblems (MBPP) benchmark, outperforming both fine-tuning on MBPP and\nfine-tuning on repaired programs written by humans. Overall, our results\nsuggest that learning from human-written natural language feedback is both more\neffective and sample-efficient than training exclusively on demonstrations for\nimproving an LLM's performance on code generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Angelica Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheurer_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;my Scheurer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korbak_T/0/1/0/all/0/1\">Tomasz Korbak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_J/0/1/0/all/0/1\">Jon Ander Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1\">Jun Shern Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Judicial Intelligent Assistant System: Extracting Events from Divorce Cases to Detect Disputes for the Judge. (arXiv:2303.16751v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16751","description":"<p>In formal procedure of civil cases, the textual materials provided by\ndifferent parties describe the development process of the cases. It is a\ndifficult but necessary task to extract the key information for the cases from\nthese textual materials and to clarify the dispute focus of related parties.\nCurrently, officers read the materials manually and use methods, such as\nkeyword searching and regular matching, to get the target information. These\napproaches are time-consuming and heavily depending on prior knowledge and\ncarefulness of the officers. To assist the officers to enhance working\nefficiency and accuracy, we propose an approach to detect disputes from divorce\ncases based on a two-round-labeling event extracting technique in this paper.\nWe implement the Judicial Intelligent Assistant (JIA) system according to the\nproposed approach to 1) automatically extract focus events from divorce case\nmaterials, 2) align events by identifying co-reference among them, and 3)\ndetect conflicts among events brought by the plaintiff and the defendant. With\nthe JIA system, it is convenient for judges to determine the disputed issues.\nExperimental results demonstrate that the proposed approach and system can\nobtain the focus of cases and detect conflicts more effectively and efficiently\ncomparing with existing method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chuanyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1\">Yu Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1\">Jidong Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1\">Bin Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Pre-trained Language Models to Deeper via Parameter-efficient Architecture. (arXiv:2303.16753v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16753","description":"<p>In this paper, we propose a highly parameter-efficient approach to scaling\npre-trained language models (PLMs) to a deeper model depth. Unlike prior work\nthat shares all parameters or uses extra blocks, we design a more capable\nparameter-sharing architecture based on matrix product operator (MPO). MPO\ndecomposition can reorganize and factorize the information of a parameter\nmatrix into two parts: the major part that contains the major information\n(central tensor) and the supplementary part that only has a small proportion of\nparameters (auxiliary tensors). Based on such a decomposition, our architecture\nshares the central tensor across all layers for reducing the model size and\nmeanwhile keeps layer-specific auxiliary tensors (also using adapters) for\nenhancing the adaptation flexibility. To improve the model training, we further\npropose a stable initialization algorithm tailored for the MPO-based\narchitecture. Extensive experiments have demonstrated the effectiveness of our\nproposed model in reducing the model size and achieving highly competitive\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Ze-Feng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Language Models with Language Feedback at Scale. (arXiv:2303.16755v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16755","description":"<p>Pretrained language models often generate outputs that are not in line with\nhuman preferences, such as harmful text or factually incorrect summaries.\nRecent work approaches the above issues by learning from a simple form of human\nfeedback: comparisons between pairs of model-generated outputs. However,\ncomparison feedback only conveys limited information about human preferences.\nIn this paper, we introduce Imitation learning from Language Feedback (ILF), a\nnew approach that utilizes more informative language feedback. ILF consists of\nthree steps that are applied iteratively: first, conditioning the language\nmodel on the input, an initial LM output, and feedback to generate refinements.\nSecond, selecting the refinement incorporating the most feedback. Third,\nfinetuning the language model to maximize the likelihood of the chosen\nrefinement given the input. We show theoretically that ILF can be viewed as\nBayesian Inference, similar to Reinforcement Learning from human feedback. We\nevaluate ILF's effectiveness on a carefully-controlled toy task and a realistic\nsummarization task. Our experiments demonstrate that large language models\naccurately incorporate feedback and that finetuning with ILF scales well with\nthe dataset size, even outperforming finetuning on human summaries. Learning\nfrom both language and comparison feedback outperforms learning from each\nalone, achieving human-level summarization performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scheurer_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;my Scheurer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_J/0/1/0/all/0/1\">Jon Ander Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korbak_T/0/1/0/all/0/1\">Tomasz Korbak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1\">Jun Shern Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Angelica Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM for Patient-Trial Matching: Privacy-Aware Data Augmentation Towards Better Performance and Generalizability. (arXiv:2303.16756v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16756","description":"<p>The process of matching patients with suitable clinical trials is essential\nfor advancing medical research and providing optimal care. However, current\napproaches face challenges such as data standardization, ethical\nconsiderations, and a lack of interoperability between Electronic Health\nRecords (EHRs) and clinical trial criteria. In this paper, we explore the\npotential of large language models (LLMs) to address these challenges by\nleveraging their advanced natural language generation capabilities to improve\ncompatibility between EHRs and clinical trial descriptions. We propose an\ninnovative privacy-aware data augmentation approach for LLM-based patient-trial\nmatching (LLM-PTM), which balances the benefits of LLMs while ensuring the\nsecurity and confidentiality of sensitive patient data. Our experiments\ndemonstrate a 7.32% average improvement in performance using the proposed\nLLM-PTM method, and the generalizability to new data is improved by 12.12%.\nAdditionally, we present case studies to further illustrate the effectiveness\nof our approach and provide a deeper understanding of its underlying\nprinciples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jiayi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Ruixiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How can Deep Learning Retrieve the Write-Missing Additional Diagnosis from Chinese Electronic Medical Record For DRG. (arXiv:2303.16757v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16757","description":"<p>The purpose of write-missing diagnosis detection is to find diseases that\nhave been clearly diagnosed from medical records but are missed in the\ndischarge diagnosis. Unlike the definition of missed diagnosis, the\nwrite-missing diagnosis is clearly manifested in the medical record without\nfurther reasoning. The write-missing diagnosis is a common problem, often\ncaused by physician negligence. The write-missing diagnosis will result in an\nincomplete diagnosis of medical records. While under DRG grouping, the\nwrite-missing diagnoses will miss important additional diagnoses (CC, MCC),\nthus affecting the correct rate of DRG enrollment.\n</p>\n<p>Under the circumstance that countries generally start to adopt DRG enrollment\nand payment, the problem of write-missing diagnosis is a common and serious\nproblem. The current manual-based method is expensive due to the complex\ncontent of the full medical record. We think this problem is suitable to be\nsolved as natural language processing. But to the best of our knowledge, no\nresearchers have conducted research on this problem based on natural language\nprocessing methods.\n</p>\n<p>We propose a framework for solving the problem of write-missing diagnosis,\nwhich mainly includes three modules: disease recall module, disease context\nlogic judgment module, and disease relationship comparison module. Through this\nframework, we verify that the problem of write-missing diagnosis can be solved\nwell, and the results are interpretable. At the same time, we propose advanced\nsolutions for the disease context logic judgment module and disease\nrelationship comparison module, which have obvious advantages compared with the\nmainstream methods of the same type of problems. Finally, we verified the value\nof our proposed framework under DRG medical insurance payment in a tertiary\nhospital.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaohui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xien Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Ji Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring celebrity influence on public attitude towards the COVID-19 pandemic: social media shared sentiment analysis. (arXiv:2303.16759v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16759","description":"<p>The COVID-19 pandemic has introduced new opportunities for health\ncommunication, including an increase in the public use of online outlets for\nhealth-related emotions. People have turned to social media networks to share\nsentiments related to the impacts of the COVID-19 pandemic. In this paper we\nexamine the role of social messaging shared by Persons in the Public Eye (i.e.\nathletes, politicians, news personnel) in determining overall public discourse\ndirection. We harvested approximately 13 million tweets ranging from 1 January\n2020 to 1 March 2022. The sentiment was calculated for each tweet using a\nfine-tuned DistilRoBERTa model, which was used to compare COVID-19\nvaccine-related Twitter posts (tweets) that co-occurred with mentions of People\nin the Public Eye. Our findings suggest the presence of consistent patterns of\nemotional content co-occurring with messaging shared by Persons in the Public\nEye for the first two years of the COVID-19 pandemic influenced public opinion\nand largely stimulated online public discourse. We demonstrate that as the\npandemic progressed, public sentiment shared on social networks was shaped by\nrisk perceptions, political ideologies and health-protective behaviours shared\nby Persons in the Public Eye, often in a negative light.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+White_B/0/1/0/all/0/1\">Brianna M White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melton_C/0/1/0/all/0/1\">Chad A Melton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zareie_P/0/1/0/all/0/1\">Parya Zareie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_R/0/1/0/all/0/1\">Robert L Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bednarczyk_R/0/1/0/all/0/1\">Robert A Bednarczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaban_Nejad_A/0/1/0/all/0/1\">Arash Shaban-Nejad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACO-tagger: A Novel Method for Part-of-Speech Tagging using Ant Colony Optimization. (arXiv:2303.16760v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16760","description":"<p>Swarm Intelligence algorithms have gained significant attention in recent\nyears as a means of solving complex and non-deterministic problems. These\nalgorithms are inspired by the collective behavior of natural creatures, and\nthey simulate this behavior to develop intelligent agents for computational\ntasks. One such algorithm is Ant Colony Optimization (ACO), which is inspired\nby the foraging behavior of ants and their pheromone laying mechanism. ACO is\nused for solving difficult problems that are discrete and combinatorial in\nnature. Part-of-Speech (POS) tagging is a fundamental task in natural language\nprocessing that aims to assign a part-of-speech role to each word in a\nsentence. In this research paper, proposed a high-performance POS-tagging\nmethod based on ACO called ACO-tagger. This method achieved a high accuracy\nrate of 96.867%, outperforming several state-of-the-art methods. The proposed\nmethod is fast and efficient, making it a viable option for practical\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_A/0/1/0/all/0/1\">Amirhossein Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajiaghajani_S/0/1/0/all/0/1\">Sara Hajiaghajani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahrani_M/0/1/0/all/0/1\">Mohammad Bahrani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meeting Action Item Detection with Regularized Context Modeling. (arXiv:2303.16763v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16763","description":"<p>Meetings are increasingly important for collaborations. Action items in\nmeeting transcripts are crucial for managing post-meeting to-do tasks, which\nusually are summarized laboriously. The Action Item Detection task aims to\nautomatically detect meeting content associated with action items. However,\ndatasets manually annotated with action item detection labels are scarce and in\nsmall scale. We construct and release the first Chinese meeting corpus with\nmanual action item annotations. In addition, we propose a Context-Drop approach\nto utilize both local and global contexts by contrastive learning, and achieve\nbetter accuracy and robustness for action item detection. We also propose a\nLightweight Model Ensemble method to exploit different pre-trained models.\nExperimental results on our Chinese meeting corpus and the English AMI corpus\ndemonstrate the effectiveness of the proposed approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Few-Shot Text Classification via Distribution Estimation. (arXiv:2303.16764v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16764","description":"<p>Distribution estimation has been demonstrated as one of the most effective\napproaches in dealing with few-shot image classification, as the low-level\npatterns and underlying representations can be easily transferred across\ndifferent tasks in computer vision domain. However, directly applying this\napproach to few-shot text classification is challenging, since leveraging the\nstatistics of known classes with sufficient samples to calibrate the\ndistributions of novel classes may cause negative effects due to serious\ncategory difference in text domain. To alleviate this issue, we propose two\nsimple yet effective strategies to estimate the distributions of the novel\nclasses by utilizing unlabeled query samples, thus avoiding the potential\nnegative transfer issue. Specifically, we first assume a class or sample\nfollows the Gaussian distribution, and use the original support set and the\nnearest few query samples to estimate the corresponding mean and covariance.\nThen, we augment the labeled samples by sampling from the estimated\ndistribution, which can provide sufficient supervision for training the\nclassification model. Extensive experiments on eight few-shot text\nclassification datasets show that the proposed method outperforms\nstate-of-the-art baselines significantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Han Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Feng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaotong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Siyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fenglong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Ming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xianchao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not cool, calm or collected: Using emotional language to detect COVID-19 misinformation. (arXiv:2303.16777v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16777","description":"<p>COVID-19 misinformation on social media platforms such as twitter is a threat\nto effective pandemic management. Prior works on tweet COVID-19 misinformation\nnegates the role of semantic features common to twitter such as charged\nemotions. Thus, we present a novel COVID-19 misinformation model, which uses\nboth a tweet emotion encoder and COVID-19 misinformation encoder to predict\nwhether a tweet contains COVID-19 misinformation. Our emotion encoder was\nfine-tuned on a novel annotated dataset and our COVID-19 misinformation encoder\nwas fine-tuned on a subset of the COVID-HeRA dataset. Experimental results show\nsuperior results using the combination of emotion and misinformation encoders\nas opposed to a misinformation classifier alone. Furthermore, extensive result\nanalysis was conducted, highlighting low quality labels and mismatched label\ndistributions as key limitations to our study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asher_G/0/1/0/all/0/1\">Gabriel Asher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohlman_P/0/1/0/all/0/1\">Phil Bohlman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleyensteuber_K/0/1/0/all/0/1\">Karsten Kleyensteuber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assorted, Archetypal and Annotated Two Million (3A2M) Cooking Recipes Dataset based on Active Learning. (arXiv:2303.16778v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16778","description":"<p>Cooking recipes allow individuals to exchange culinary ideas and provide food\npreparation instructions. Due to a lack of adequate labeled data, categorizing\nraw recipes found online to the appropriate food genres is a challenging task\nin this domain. Utilizing the knowledge of domain experts to categorize recipes\ncould be a solution. In this study, we present a novel dataset of two million\nculinary recipes labeled in respective categories leveraging the knowledge of\nfood experts and an active learning technique. To construct the dataset, we\ncollect the recipes from the RecipeNLG dataset. Then, we employ three human\nexperts whose trustworthiness score is higher than 86.667% to categorize 300K\nrecipe by their Named Entity Recognition (NER) and assign it to one of the nine\ncategories: bakery, drinks, non-veg, vegetables, fast food, cereals, meals,\nsides and fusion. Finally, we categorize the remaining 1900K recipes using\nActive Learning method with a blend of Query-by-Committee and Human In The Loop\n(HITL) approaches. There are more than two million recipes in our dataset, each\nof which is categorized and has a confidence score linked with it. For the 9\ngenres, the Fleiss Kappa score of this massive dataset is roughly 0.56026. We\nbelieve that the research community can use this dataset to perform various\nmachine learning tasks such as recipe genre classification, recipe generation\nof a specific genre, new recipe creation, etc. The dataset can also be used to\ntrain and evaluate the performance of various NLP tasks such as named entity\nrecognition, part-of-speech tagging, semantic role labeling, and so on. The\ndataset will be available upon publication: https://tinyurl.com/3zu4778y.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sakib_N/0/1/0/all/0/1\">Nazmus Sakib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahariar_G/0/1/0/all/0/1\">G. M. Shahariar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Md. Mohsinul Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Md. Kamrul Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmud_H/0/1/0/all/0/1\">Hasan Mahmud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models Trained on Media Diets Can Predict Public Opinion. (arXiv:2303.16779v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16779","description":"<p>Public opinion reflects and shapes societal behavior, but the traditional\nsurvey-based tools to measure it are limited. We introduce a novel approach to\nprobe media diet models -- language models adapted to online news, TV\nbroadcast, or radio show content -- that can emulate the opinions of\nsubpopulations that have consumed a set of media. To validate this method, we\nuse as ground truth the opinions expressed in U.S. nationally representative\nsurveys on COVID-19 and consumer confidence. Our studies indicate that this\napproach is (1) predictive of human judgements found in survey response\ndistributions and robust to phrasing and channels of media exposure, (2) more\naccurate at modeling people who follow media more closely, and (3) aligned with\nliterature on which types of opinions are affected by media consumption.\nProbing language models provides a powerful new method for investigating media\neffects, has practical applications in supplementing polls and forecasting\npublic opinion, and suggests a need for further study of the surprising\nfidelity with which neural language models can predict human responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_E/0/1/0/all/0/1\">Eric Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ansolabehere_S/0/1/0/all/0/1\">Stephen Ansolabehere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1\">Deb Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Entailment of Leaderboards for Empirical AI Research. (arXiv:2303.16835v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16835","description":"<p>We present a large-scale empirical investigation of the zero-shot learning\nphenomena in a specific recognizing textual entailment (RTE) task category,\ni.e. the automated mining of leaderboards for Empirical AI Research. The prior\nreported state-of-the-art models for leaderboards extraction formulated as an\nRTE task, in a non-zero-shot setting, are promising with above 90% reported\nperformances. However, a central research question remains unexamined: did the\nmodels actually learn entailment? Thus, for the experiments in this paper, two\nprior reported state-of-the-art models are tested out-of-the-box for their\nability to generalize or their capacity for entailment, given leaderboard\nlabels that were unseen during training. We hypothesize that if the models\nlearned entailment, their zero-shot performances can be expected to be\nmoderately high as well--perhaps, concretely, better than chance. As a result\nof this work, a zero-shot labeled dataset is created via distant labeling\nformulating the leaderboard extraction RTE task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kabongo_S/0/1/0/all/0/1\">Salomon Kabongo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1\">Jennifer D&#x27;Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auer_S/0/1/0/all/0/1\">S&#xf6;ren Auer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks. (arXiv:2303.16839v1 [cs.CV])","link":"http://arxiv.org/abs/2303.16839","description":"<p>The development of language models have moved from encoder-decoder to\ndecoder-only designs. In addition, the common knowledge has it that the two\nmost popular multimodal tasks, the generative and contrastive tasks, tend to\nconflict with one another, are hard to accommodate in one architecture, and\nfurther need complex adaptations for downstream tasks. We propose a novel\nparadigm of training with a decoder-only model for multimodal tasks, which is\nsurprisingly effective in jointly learning of these disparate vision-language\ntasks. This is done with a simple model, called MaMMUT. It consists of a single\nvision encoder and a text decoder, and is able to accommodate contrastive and\ngenerative learning by a novel two-pass approach on the text decoder. We\ndemonstrate that joint training of these diverse-objective tasks is simple,\neffective, and maximizes the weight-sharing of the model. Furthermore, the same\narchitecture enables straightforward extensions to open-vocabulary object\ndetection and video-language tasks. The model tackles a diverse range of tasks,\nwhile being modest in capacity. Our model achieves the SOTA on image-text and\ntext-image retrieval, video question answering and open-vocabulary detection\ntasks, outperforming much larger and more extensively trained foundational\nmodels. It shows competitive results on VQA and Video Captioning, especially\nconsidering its size. Ablations confirm the flexibility and advantages of our\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuo_W/0/1/0/all/0/1\">Weicheng Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piergiovanni_A/0/1/0/all/0/1\">AJ Piergiovanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dahun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caine_B/0/1/0/all/0/1\">Ben Caine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogale_A/0/1/0/all/0/1\">Abhijit Ogale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Claire Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1\">Anelia Angelova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators. (arXiv:2303.16854v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16854","description":"<p>Many natural language processing (NLP) tasks rely on labeled data to train\nmachine learning models to achieve high performance. However, data annotation\ncan be a time-consuming and expensive process, especially when the task\ninvolves a large amount of data or requires specialized domains. Recently,\nGPT-3.5 series models have demonstrated remarkable few-shot and zero-shot\nability across various NLP tasks. In this paper, we first claim that large\nlanguage models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced\nannotator by providing them with sufficient guidance and demonstrated examples.\nTo make LLMs to be better annotators, we propose a two-step approach,\n'explain-then-annotate'. To be more precise, we begin by creating prompts for\nevery demonstrated example, which we subsequently utilize to prompt a LLM to\nprovide an explanation for why the specific ground truth answer/label was\nchosen for that particular example. Following this, we construct the few-shot\nchain-of-thought prompt with the self-generated explanation and employ it to\nannotate the unlabeled data. We conduct experiments on three tasks, including\nuser input and keyword relevance assessment, BoolQ and WiC. The annotation\nresults from GPT-3.5 surpasses those from crowdsourced annotation for user\ninput and keyword relevance assessment. Additionally, for the other two tasks,\nGPT-3.5 achieves results that are comparable to those obtained through\ncrowdsourced annotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xingwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhenghao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_A/0/1/0/all/0/1\">A-Long Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yiu_S/0/1/0/all/0/1\">Siu Ming Yiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Did You Mean...? Confidence-based Trade-offs in Semantic Parsing. (arXiv:2303.16857v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16857","description":"<p>We illustrate how a calibrated model can help balance common trade-offs in\ntask-oriented parsing. In a simulated annotator-in-the-loop experiment, we show\nthat well-calibrated confidence scores allow us to balance cost with annotator\nload, improving accuracy with a small number of interactions. We then examine\nhow confidence scores can help optimize the trade-off between usability and\nsafety. We show that confidence-based thresholding can substantially reduce the\nnumber of incorrect low-confidence programs executed; however, this comes at a\ncost to usability. We propose the DidYouMean system which better balances\nusability and safety.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stengel_Eskin_E/0/1/0/all/0/1\">Elias Stengel-Eskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End $n$-ary Relation Extraction for Combination Drug Therapies. (arXiv:2303.16886v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16886","description":"<p>Combination drug therapies are treatment regimens that involve two or more\ndrugs, administered more commonly for patients with cancer, HIV, malaria, or\ntuberculosis. Currently there are over 350K articles in PubMed that use the\n\"combination drug therapy\" MeSH heading with at least 10K articles published\nper year over the past two decades. Extracting combination therapies from\nscientific literature inherently constitutes an $n$-ary relation extraction\nproblem. Unlike in the general $n$-ary setting where $n$ is fixed (e.g.,\ndrug-gene-mutation relations where $n=3$), extracting combination therapies is\na special setting where $n \\geq 2$ is dynamic, depending on each instance.\nRecently, Tiktinsky et al. (NAACL 2022) introduced a first of its kind dataset,\nCombDrugExt, for extracting such therapies from literature. Here, we use a\nsequence-to-sequence style end-to-end extraction method to achieve an F1-Score\nof $66.7\\%$ on the CombDrugExt test set for positive (or effective)\ncombinations. This is an absolute $\\approx 5\\%$ F1-score improvement even over\nthe prior best relation classification score with spotted drug entities (hence,\nnot end-to-end). Thus our effort introduces a state-of-the-art first model for\nend-to-end extraction that is already superior to the best prior non end-to-end\nmodel for this task. Our model seamlessly extracts all drug entities and\nrelations in a single pass and is highly suitable for dynamic $n$-ary\nextraction scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuhang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavuluru_R/0/1/0/all/0/1\">Ramakanth Kavuluru</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance. (arXiv:2303.16894v1 [cs.CV])","link":"http://arxiv.org/abs/2303.16894","description":"<p>Understanding 3D scenes from multi-view inputs has been proven to alleviate\nthe view discrepancy issue in 3D visual grounding. However, existing methods\nnormally neglect the view cues embedded in the text modality and fail to weigh\nthe relative importance of different views. In this paper, we propose\nViewRefer, a multi-view framework for 3D visual grounding exploring how to\ngrasp the view knowledge from both text and 3D modalities. For the text branch,\nViewRefer leverages the diverse linguistic knowledge of large-scale language\nmodels, e.g., GPT, to expand a single grounding text to multiple\ngeometry-consistent descriptions. Meanwhile, in the 3D modality, a transformer\nfusion module with inter-view attention is introduced to boost the interaction\nof objects across views. On top of that, we further present a set of learnable\nmulti-view prototypes, which memorize scene-agnostic knowledge for different\nviews, and enhance the framework from two perspectives: a view-guided attention\nmodule for more robust text features, and a view-guided scoring strategy during\nthe final prediction. With our designed paradigm, ViewRefer achieves superior\nperformance on three benchmarks and surpasses the second-best by +2.8%, +1.2%,\nand +0.73% on Sr3D, Nr3D, and ScanRefer. Code will be released at\nhttps://github.com/ZiyuGuo99/ViewRefer3D.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Ziyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yiwen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhigang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ousiometrics and Telegnomics: The essence of meaning conforms to a two-dimensional powerful-weak and dangerous-safe framework with diverse corpora presenting a safety bias. (arXiv:2110.06847v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06847","description":"<p>We define `ousiometrics' to be the study of essential meaning in whatever\ncontext that meaningful signals are communicated, and `telegnomics' as the\nstudy of remotely sensed knowledge. From work emerging through the middle of\nthe 20th century, the essence of meaning has become generally accepted as being\nwell captured by the three orthogonal dimensions of evaluation, potency, and\nactivation (EPA). By re-examining first types and then tokens for the English\nlanguage, and through the use of automatically annotated histograms --\n`ousiograms' -- we find here that: 1. The essence of meaning conveyed by words\nis instead best described by a compass-like power-danger (PD) framework, and 2.\nAnalysis of a disparate collection of large-scale English language corpora --\nliterature, news, Wikipedia, talk radio, and social media -- shows that natural\nlanguage exhibits a systematic bias toward safe, low danger words -- a\nreinterpretation of the Pollyanna principle's positivity bias for written\nexpression. To help justify our choice of dimension names and to help address\nthe problems with representing observed ousiometric dimensions by bipolar\nadjective pairs, we introduce and explore `synousionyms' and `antousionyms' --\nousiometric counterparts of synonyms and antonyms. We further show that the PD\nframework revises the circumplex model of affect as a more general model of\nstate of mind. Finally, we use our findings to construct and test a prototype\n`ousiometer', a telegnomic instrument that measures ousiometric time series for\ntemporal corpora. We contend that our power-danger ousiometric framework\nprovides a complement for entropy-based measurements, and may be of value for\nthe study of a wide variety of communication across biological and artificial\nlife.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dodds_P/0/1/0/all/0/1\">P. S. Dodds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshaabi_T/0/1/0/all/0/1\">T. Alshaabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fudolig_M/0/1/0/all/0/1\">M. I. Fudolig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmerman_J/0/1/0/all/0/1\">J. W. Zimmerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovato_J/0/1/0/all/0/1\">J. Lovato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaulieu_S/0/1/0/all/0/1\">S. Beaulieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minot_J/0/1/0/all/0/1\">J. R. Minot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1\">M. V. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reagan_A/0/1/0/all/0/1\">A. J. Reagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danforth_C/0/1/0/all/0/1\">C. M. Danforth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A methodology to characterize bias and harmful stereotypes in natural language processing in Latin America. (arXiv:2207.06591v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.06591","description":"<p>Automated decision-making systems, especially those based on natural language\nprocessing, are pervasive in our lives. They are not only behind the internet\nsearch engines we use daily, but also take more critical roles: selecting\ncandidates for a job, determining suspects of a crime, diagnosing autism and\nmore. Such automated systems make errors, which may be harmful in many ways, be\nit because of the severity of the consequences (as in health issues) or because\nof the sheer number of people they affect. When errors made by an automated\nsystem affect a population more than others, we call the system\n\\textit{biased}.\n</p>\n<p>Most modern natural language technologies are based on artifacts obtained\nfrom enormous volumes of text using machine learning, namely language models\nand word embeddings. Since they are created by applying subsymbolic machine\nlearning, mostly artificial neural networks, they are opaque and practically\nuninterpretable by direct inspection, thus making it very difficult to audit\nthem.\n</p>\n<p>In this paper, we present a methodology that spells out how social\nscientists, domain experts, and machine learning experts can collaboratively\nexplore biases and harmful stereotypes in word embeddings and large language\nmodels. Our methodology is based on the following principles:\n</p>\n<p>* focus on the linguistic manifestations of discrimination on word embeddings\nand language models, not on the mathematical properties of the models * reduce\nthe technical barrier for discrimination experts%, be it social scientists,\ndomain experts or other * characterize through a qualitative exploratory\nprocess in addition to a metric-based approach * address mitigation as part of\nthe training process, not as an afterthought\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alemany_L/0/1/0/all/0/1\">Laura Alonso Alemany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benotti_L/0/1/0/all/0/1\">Luciana Benotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maina_H/0/1/0/all/0/1\">Hern&#xe1;n Maina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_L/0/1/0/all/0/1\">Luc&#xed;a Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajngewerc_M/0/1/0/all/0/1\">Mariela Rajngewerc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_L/0/1/0/all/0/1\">Lautaro Mart&#xed;nez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_J/0/1/0/all/0/1\">Jorge S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schilman_M/0/1/0/all/0/1\">Mauro Schilman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivetta_G/0/1/0/all/0/1\">Guido Ivetta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halvorsen_A/0/1/0/all/0/1\">Alexia Halvorsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rojo_A/0/1/0/all/0/1\">Amanda Mata Rojo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bordone_M/0/1/0/all/0/1\">Mat&#xed;as Bordone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busaniche_B/0/1/0/all/0/1\">Beatriz Busaniche</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation. (arXiv:2209.15323v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2209.15323","description":"<p>Recent advances in image captioning have focused on scaling the data and\nmodel size, substantially increasing the cost of pre-training and finetuning.\nAs an alternative to large models, we present SmallCap, which generates a\ncaption conditioned on an input image and related captions retrieved from a\ndatastore. Our model is lightweight and fast to train, as the only learned\nparameters are in newly introduced cross-attention layers between a pre-trained\nCLIP encoder and GPT-2 decoder. SmallCap can transfer to new domains without\nadditional finetuning and can exploit large-scale data in a training-free\nfashion since the contents of the datastore can be readily replaced. Our\nexperiments show that SmallCap, trained only on COCO, has competitive\nperformance on this benchmark, and also transfers to other domains without\nretraining, solely through retrieval from target-domain data. Further\nimprovement is achieved through the training-free exploitation of diverse\nhuman-labeled and web data, which proves to be effective for a range of\ndomains, including the nocaps benchmark, designed to test generalization to\nunseen visual concepts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramos_R/0/1/0/all/0/1\">Rita Ramos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_B/0/1/0/all/0/1\">Bruno Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1\">Desmond Elliott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kementchedjhieva_Y/0/1/0/all/0/1\">Yova Kementchedjhieva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Retrieval with Search Agents and Hybrid Environments. (arXiv:2209.15469v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.15469","description":"<p>Learning to search is the task of building artificial agents that learn to\nautonomously use a search box to find information. So far, it has been shown\nthat current language models can learn symbolic query reformulation policies,\nin combination with traditional term-based retrieval, but fall short of\noutperforming neural retrievers. We extend the previous learning to search\nsetup to a hybrid environment, which accepts discrete query refinement\noperations, after a first-pass retrieval step via a dual encoder. Experiments\non the BEIR task show that search agents, trained via behavioral cloning,\noutperform the underlying search system based on a combined dual encoder\nretriever and cross encoder reranker. Furthermore, we find that simple\nheuristic Hybrid Retrieval Environments (HRE) can improve baseline performance\nby several nDCG points. The search agent based on HRE (HARE) matches\nstate-of-the-art performance, balanced in both zero-shot and in-domain\nevaluations, via interpretable actions, and at twice the speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huebscher_M/0/1/0/all/0/1\">Michelle Chen Huebscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buck_C/0/1/0/all/0/1\">Christian Buck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciaramita_M/0/1/0/all/0/1\">Massimiliano Ciaramita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rothe_S/0/1/0/all/0/1\">Sascha Rothe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-lingual Evaluation of Code Generation Models. (arXiv:2210.14868v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.14868","description":"<p>We present new benchmarks on evaluation code generation models: MBXP and\nMultilingual HumanEval, and MathQA-X. These datasets cover over 10 programming\nlanguages and are generated using a scalable conversion framework that\ntranspiles prompts and test cases from the original Python datasets into the\ncorresponding data in the target language. Using these benchmarks, we are able\nto assess the performance of code generation models in a multi-lingual fashion,\nand discovered generalization ability of language models on out-of-domain\nlanguages, advantages of multi-lingual models over mono-lingual, the ability of\nfew-shot prompting to teach the model new languages, and zero-shot translation\nabilities even on mono-lingual settings. Furthermore, we use our code\ngeneration model to perform large-scale bootstrapping to obtain synthetic\ncanonical solutions in several languages, which can be used for other\ncode-related evaluations such as code insertion, robustness, or summarization\ntasks. Overall, our benchmarks represents a significant step towards a deeper\nunderstanding of language models' code generation abilities. We publicly\nrelease our code and datasets at https://github.com/amazon-research/mxeval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Athiwaratkun_B/0/1/0/all/0/1\">Ben Athiwaratkun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gouda_S/0/1/0/all/0/1\">Sanjay Krishna Gouda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaopeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuchen Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Ming Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_M/0/1/0/all/0/1\">Mingyue Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonugondla_S/0/1/0/all/0/1\">Sujan Kumar Gonugondla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Hantian Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fulton_N/0/1/0/all/0/1\">Nathan Fulton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farahani_A/0/1/0/all/0/1\">Arash Farahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Siddhartha Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giaquinto_R/0/1/0/all/0/1\">Robert Giaquinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Haifeng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanathan_M/0/1/0/all/0/1\">Murali Krishna Ramanathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nallapati_R/0/1/0/all/0/1\">Ramesh Nallapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_B/0/1/0/all/0/1\">Baishakhi Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_P/0/1/0/all/0/1\">Parminder Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Sudipta Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent Linguistic Structures in Neural Networks are Fragile. (arXiv:2210.17406v7 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.17406","description":"<p>Large Language Models (LLMs) have been reported to have strong performance on\nnatural language processing tasks. However, performance metrics such as\naccuracy do not measure the quality of the model in terms of its ability to\nrobustly represent complex linguistic structure. In this paper, focusing on the\nability of language models to represent syntax, we propose a framework to\nassess the consistency and robustness of linguistic representations. To this\nend, we introduce measures of robustness of neural network models that leverage\nrecent advances in extracting linguistic constructs from LLMs via probing\ntasks, i.e., simple tasks used to extract meaningful information about a single\nfacet of a language model, such as syntax reconstruction and root\nidentification. Empirically, we study the performance of four LLMs across six\ndifferent corpora on the proposed robustness measures by analysing their\nperformance and robustness with respect to syntax-preserving perturbations. We\nprovide evidence that context-free representation (e.g., GloVe) are in some\ncases competitive with context-dependent representations from modern LLMs\n(e.g., BERT), yet equally brittle to syntax-preserving perturbations. Our key\nobservation is that emergent syntactic representations in neural networks are\nbrittle. We make the code, trained models and logs available to the community\nas a contribution to the debate about the capabilities of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malfa_E/0/1/0/all/0/1\">Emanuele La Malfa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wicker_M/0/1/0/all/0/1\">Matthew Wicker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwiatkowska_M/0/1/0/all/0/1\">Marta Kwiatkowska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrated Interpretation: Confidence Estimation in Semantic Parsing. (arXiv:2211.07443v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07443","description":"<p>Sequence generation models are increasingly being used to translate language\ninto executable programs, i.e. to perform executable semantic parsing. The fact\nthat semantic parsing aims to execute actions in the real world motivates\ndeveloping safe systems, which in turn makes measuring calibration -- a central\ncomponent to safety -- particularly important. We investigate the calibration\nof common generation models across four popular semantic parsing datasets,\nfinding that it varies across models and datasets. We then analyze factors\nassociated with calibration error and release new confidence-based challenge\nsplits of two parsing datasets. To facilitate the inclusion of calibration in\nsemantic parsing evaluations, we release a library for computing calibration\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stengel_Eskin_E/0/1/0/all/0/1\">Elias Stengel-Eskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning. (arXiv:2212.01117v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.01117","description":"<p>The spread of rumors along with breaking events seriously hinders the truth\nin the era of social media. Previous studies reveal that due to the lack of\nannotated resources, rumors presented in minority languages are hard to be\ndetected. Furthermore, the unforeseen breaking events not involved in\nyesterday's news exacerbate the scarcity of data resources. In this work, we\npropose a novel zero-shot framework based on prompt learning to detect rumors\nfalling in different domains or presented in different languages. More\nspecifically, we firstly represent rumor circulated on social media as diverse\npropagation threads, then design a hierarchical prompt encoding mechanism to\nlearn language-agnostic contextual representations for both prompts and rumor\ndata. To further enhance domain adaptation, we model the domain-invariant\nstructural features from the propagation threads, to incorporate structural\nposition representations of influential community response. In addition, a new\nvirtual response augmentation method is used to improve model training.\nExtensive experiments conducted on three real-world datasets demonstrate that\nour proposed model achieves much better performance than state-of-the-art\nmethods and exhibits a superior capacity for detecting rumors at early stages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongzhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_P/0/1/0/all/0/1\">Pengyao Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Ziyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruifang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Editing Models with Task Arithmetic. (arXiv:2212.04089v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2212.04089","description":"<p>Changing how pre-trained models behave -- e.g., improving their performance\non a downstream task or mitigating biases learned during pre-training -- is a\ncommon practice when developing machine learning systems. In this work, we\npropose a new paradigm for steering the behavior of neural networks, centered\naround \\textit{task vectors}. A task vector specifies a direction in the weight\nspace of a pre-trained model, such that movement in that direction improves\nperformance on the task. We build task vectors by subtracting the weights of a\npre-trained model from the weights of the same model after fine-tuning on a\ntask. We show that these task vectors can be modified and combined together\nthrough arithmetic operations such as negation and addition, and the behavior\nof the resulting model is steered accordingly. Negating a task vector decreases\nperformance on the target task, with little change in model behavior on control\ntasks. Moreover, adding task vectors together can improve performance on\nmultiple tasks at once. Finally, when tasks are linked by an analogy\nrelationship of the form ``A is to B as C is to D\", combining task vectors from\nthree of the tasks can improve performance on the fourth, even when no data\nfrom the fourth task is used for training. Overall, our experiments with\nseveral models, modalities and tasks show that task arithmetic is a simple,\nefficient and effective way of editing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Marco Tulio Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1\">Suchin Gururangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harnessing the Power of Multi-Task Pretraining for Ground-Truth Level Natural Language Explanations. (arXiv:2212.04231v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2212.04231","description":"<p>Natural language explanations promise to offer intuitively understandable\nexplanations of a neural network's decision process in complex vision-language\ntasks, as pursued in recent VL-NLE models. While current models offer\nimpressive performance on task accuracy and explanation plausibility, they\nsuffer from a range of issues: Some models feature a modular design where the\nexplanation generation module is poorly integrated with a separate module for\ntask-answer prediction, employ backbone models trained on limited sets of\ntasks, or incorporate ad hoc solutions to increase performance on single\ndatasets. We propose to evade these limitations by applying recent advances in\nlarge-scale multi-task pretraining of generative Transformer models to the\nproblem of VL-NLE tasks. Our approach outperforms recent models by a large\nmargin, with human annotators preferring the generated explanations over the\nground truth in two out of three evaluated datasets. As a novel challenge in\nVL-NLE research, we propose the problem of multi-task VL-NLE and show that\njointly training on multiple tasks can increase the explanation quality. We\ndiscuss the ethical implications of high-quality NLE generation and other\nissues in recent VL-NLE research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pluster_B/0/1/0/all/0/1\">Bj&#xf6;rn Pl&#xfc;ster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambsdorf_J/0/1/0/all/0/1\">Jakob Ambsdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braach_L/0/1/0/all/0/1\">Lukas Braach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jae Hee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-aware Fine-tuning of Self-supervised Speech Models. (arXiv:2212.08542v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2212.08542","description":"<p>Self-supervised pre-trained transformers have improved the state of the art\non a variety of speech tasks. Due to the quadratic time and space complexity of\nself-attention, they usually operate at the level of relatively short (e.g.,\nutterance) segments. In this paper, we study the use of context, i.e.,\nsurrounding segments, during fine-tuning and propose a new approach called\ncontext-aware fine-tuning. We attach a context module on top of the last layer\nof a pre-trained model to encode the whole segment into a context embedding\nvector which is then used as an additional feature for the final prediction.\nDuring the fine-tuning stage, we introduce an auxiliary loss that encourages\nthis context embedding vector to be similar to context vectors of surrounding\nsegments. This allows the model to make predictions without access to these\nsurrounding segments at inference time and requires only a tiny overhead\ncompared to standard fine-tuned models. We evaluate the proposed approach using\nthe SLUE and Libri-light benchmarks for several downstream tasks: Automatic\nspeech recognition (ASR), named entity recognition (NER), and sentiment\nanalysis (SA). The results show that context-aware fine-tuning not only\noutperforms a standard fine-tuning baseline but also rivals a strong context\ninjection baseline that uses neighboring speech segments during inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shon_S/0/1/0/all/0/1\">Suwon Shon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_F/0/1/0/all/0/1\">Felix Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_K/0/1/0/all/0/1\">Kwangyoun Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sridhar_P/0/1/0/all/0/1\">Prashant Sridhar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are reasoners with Self-Verification. (arXiv:2212.09561v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2212.09561","description":"<p>When a large language model (LLM) performs complex reasoning by chain of\nthought (CoT), it can be highly sensitive to individual mistakes. We have had\nto train verifiers to address this issue. As we all know, after human inferring\na conclusion, they often check it by re-verifying it, which can avoid some\nmistakes. We propose a new method called self-verification that uses the\nconclusion of the CoT as a condition to build a new sample and asks the LLM to\nre-predict the original conditions which be masked. We calculate an explainable\nverification score based on the accuracy. This method can improve the accuracy\nof multiple arithmetics and logical reasoning datasets when using few-shot\nlearning. we have demonstrated that LLMs can conduct explainable\nself-verification of their own conclusions and achieve competitive reasoning\nperformance. Extensive experimentals have demonstrated that our method can help\nmultiple large language models with self-verification can avoid interference\nfrom incorrect CoT. Code is available at\n\\url{https://github.com/WENGSYX/Self-Verification}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Minjun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shizhu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does CLIP Bind Concepts? Probing Compositionality in Large Image Models. (arXiv:2212.10537v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2212.10537","description":"<p>Large-scale neural network models combining text and images have made\nincredible progress in recent years. However, it remains an open question to\nwhat extent such models encode compositional representations of the concepts\nover which they operate, such as correctly identifying ''red cube'' by\nreasoning over the constituents ''red'' and ''cube''. In this work, we focus on\nthe ability of a large pretrained vision and language model (CLIP) to encode\ncompositional concepts and to bind variables in a structure-sensitive way\n(e.g., differentiating ''cube behind sphere'' from ''sphere behind cube''). In\norder to inspect the performance of CLIP, we compare several architectures from\nresearch on compositional distributional semantics models (CDSMs), a line of\nresearch that attempts to implement traditional compositional linguistic\nstructures within embedding spaces. We find that CLIP can compose concepts in a\nsingle-object setting, but in situations where concept binding is needed,\nperformance drops dramatically. At the same time, CDSMs also perform poorly,\nwith best performance at chance level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Martha Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_N/0/1/0/all/0/1\">Nihal V. Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Peilin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qinan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merullo_J/0/1/0/all/0/1\">Jack Merullo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1\">Stephen H. Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective. (arXiv:2302.12095v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2302.12095","description":"<p>ChatGPT is a recent chatbot service released by OpenAI and is receiving\nincreasing attention over the past few months. While evaluations of various\naspects of ChatGPT have been done, its robustness, i.e., the performance to\nunexpected inputs, is still unclear to the public. Robustness is of particular\nconcern in responsible AI, especially for safety-critical applications. In this\npaper, we conduct a thorough evaluation of the robustness of ChatGPT from the\nadversarial and out-of-distribution (OOD) perspective. To do so, we employ the\nAdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart\nreview and DDXPlus medical diagnosis datasets for OOD evaluation. We select\nseveral popular foundation models as baselines. Results show that ChatGPT shows\nconsistent advantages on most adversarial and OOD classification and\ntranslation tasks. However, the absolute performance is far from perfection,\nwhich suggests that adversarial and OOD robustness remains a significant threat\nto foundation models. Moreover, ChatGPT shows astounding performance in\nunderstanding dialogue-related texts and we find that it tends to provide\ninformal suggestions for medical tasks instead of definitive answers. Finally,\nwe present in-depth discussions of possible research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xixu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1\">Wenxin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Runkai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haojun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_B/0/1/0/all/0/1\">Binxin Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential. (arXiv:2303.09038v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.09038","description":"<p>The large language model called ChatGPT has drawn extensively attention\nbecause of its human-like expression and reasoning abilities. In this study, we\ninvestigate the feasibility of using ChatGPT in experiments on using ChatGPT to\ntranslate radiology reports into plain language for patients and healthcare\nproviders so that they are educated for improved healthcare. Radiology reports\nfrom 62 low-dose chest CT lung cancer screening scans and 76 brain MRI\nmetastases screening scans were collected in the first half of February for\nthis study. According to the evaluation by radiologists, ChatGPT can\nsuccessfully translate radiology reports into plain language with an average\nscore of 4.27 in the five-point system with 0.08 places of information missing\nand 0.07 places of misinformation. In terms of the suggestions provided by\nChatGPT, they are general relevant such as keeping following-up with doctors\nand closely monitoring any symptoms, and for about 37% of 138 cases in total\nChatGPT offers specific suggestions based on findings in the report. ChatGPT\nalso presents some randomness in its responses with occasionally\nover-simplified or neglected information, which can be mitigated using a more\ndetailed prompt. Furthermore, ChatGPT results are compared with a newly\nreleased large model GPT-4, showing that GPT-4 can significantly improve the\nquality of translated reports. Our results show that it is feasible to utilize\nlarge language models in clinical education, and further efforts are needed to\naddress limitations and maximize their potential.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Josh Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zapadka_M/0/1/0/all/0/1\">Michael E. Zapadka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponnatapura_J/0/1/0/all/0/1\">Janardhana Ponnatapura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myers_K/0/1/0/all/0/1\">Kyle J. Myers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitlow_C/0/1/0/all/0/1\">Christopher T. Whitlow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trained on 100 million words and still in shape: BERT meets British National Corpus. (arXiv:2303.09859v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.09859","description":"<p>While modern masked language models (LMs) are trained on ever larger corpora,\nwe here explore the effects of down-scaling training to a modestly-sized but\nrepresentative, well-balanced, and publicly available English text source --\nthe British National Corpus. We show that pre-training on this carefully\ncurated corpus can reach better performance than the original BERT model. We\nargue that this type of corpora has great potential as a language modeling\nbenchmark. To showcase this potential, we present fair, reproducible and\ndata-efficient comparative studies of LMs, in which we evaluate several\ntraining objectives and model architectures and replicate previous empirical\nresults in a systematic way. We propose an optimized LM architecture called\nLTG-BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Samuel_D/0/1/0/all/0/1\">David Samuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutuzov_A/0/1/0/all/0/1\">Andrey Kutuzov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovrelid_L/0/1/0/all/0/1\">Lilja &#xd8;vrelid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velldal_E/0/1/0/all/0/1\">Erik Velldal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph. (arXiv:2303.13351v3 [cs.DL] UPDATED)","link":"http://arxiv.org/abs/2303.13351","description":"<p>In this work we create a question answering dataset over the DBLP scholarly\nknowledge graph (KG). DBLP is an on-line reference for bibliographic\ninformation on major computer science publications that indexes over 4.4\nmillion publications published by more than 2.2 million authors. Our dataset\nconsists of 10,000 question answer pairs with the corresponding SPARQL queries\nwhich can be executed over the DBLP KG to fetch the correct answer. DBLP-QuAD\nis the largest scholarly question answering dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1\">Debayan Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awale_S/0/1/0/all/0/1\">Sushil Awale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usbeck_R/0/1/0/all/0/1\">Ricardo Usbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biemann_C/0/1/0/all/0/1\">Chris Biemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TextMI: Textualize Multimodal Information for Integrating Non-verbal Cues in Pre-trained Language Models. (arXiv:2303.15430v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.15430","description":"<p>Pre-trained large language models have recently achieved ground-breaking\nperformance in a wide variety of language understanding tasks. However, the\nsame model can not be applied to multimodal behavior understanding tasks (e.g.,\nvideo sentiment/humor detection) unless non-verbal features (e.g., acoustic and\nvisual) can be integrated with language. Jointly modeling multiple modalities\nsignificantly increases the model complexity, and makes the training process\ndata-hungry. While an enormous amount of text data is available via the web,\ncollecting large-scale multimodal behavioral video datasets is extremely\nexpensive, both in terms of time and money. In this paper, we investigate\nwhether large language models alone can successfully incorporate non-verbal\ninformation when they are presented in textual form. We present a way to\nconvert the acoustic and visual information into corresponding textual\ndescriptions and concatenate them with the spoken text. We feed this augmented\ninput to a pre-trained BERT model and fine-tune it on three downstream\nmultimodal tasks: sentiment, humor, and sarcasm detection. Our approach,\nTextMI, significantly reduces model complexity, adds interpretability to the\nmodel's decision, and can be applied for a diverse set of tasks while achieving\nsuperior (multimodal sarcasm detection) or near SOTA (multimodal sentiment\nanalysis and multimodal humor detection) performance. We propose TextMI as a\ngeneral, competitive baseline for multimodal behavioral analysis tasks,\nparticularly in a low-resource setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Md Kamrul Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Saiful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangwu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_W/0/1/0/all/0/1\">Wasifur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naim_I/0/1/0/all/0/1\">Iftekhar Naim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mohammed Ibrahim Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoque_E/0/1/0/all/0/1\">Ehsan Hoque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP. (arXiv:2303.16166v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.16166","description":"<p>Despite its pivotal role in research experiments, code correctness is often\npresumed only on the basis of the perceived quality of the results. This comes\nwith the risk of erroneous outcomes and potentially misleading findings. To\naddress this issue, we posit that the current focus on result reproducibility\nshould go hand in hand with the emphasis on coding best practices. We bolster\nour call to the NLP community by presenting a case study, in which we identify\n(and correct) three bugs in widely used open-source implementations of the\nstate-of-the-art Conformer architecture. Through comparative experiments on\nautomatic speech recognition and translation in various language settings, we\ndemonstrate that the existence of bugs does not prevent the achievement of good\nand reproducible results and can lead to incorrect conclusions that potentially\nmisguide future research. In response to this, this study is a call to action\ntoward the adoption of coding best practices aimed at fostering correctness and\nimproving the quality of the developed software.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilzer_A/0/1/0/all/0/1\">Andrea Pilzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-03-29T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}