{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-11-16T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"UNcommonsense Reasoning: Abductive Reasoning about Uncommon Situations. (arXiv:2311.08469v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08469","description":"<p>Language technologies that accurately model the dynamics of events must\nperform commonsense reasoning. Existing work evaluating commonsense reasoning\nfocuses on making inferences about common, everyday situations. To instead\ninvestigate the ability to model unusual, unexpected, and unlikely situations,\nwe explore the task of uncommonsense abductive reasoning. Given a piece of\ncontext with an unexpected outcome, this task requires reasoning abductively to\ngenerate a natural language explanation that makes the unexpected outcome more\nlikely in the context. To this end, we curate and release a new English\nlanguage corpus called UNcommonsense. We characterize the differences between\nthe performance of human explainers and the best performing large language\nmodels, finding that model-enhanced human-written explanations achieve the\nhighest quality by trading off between specificity and diversity. Finally, we\nexperiment with several online imitation learning algorithms to train open and\naccessible language models on this task. When compared with the vanilla\nsupervised fine-tuning approach, these methods consistently reduce lose rates\non both common and uncommonsense abductive reasoning judged by human\nevaluators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenting Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_J/0/1/0/all/0/1\">Justin T Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_S/0/1/0/all/0/1\">Sanjiban Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Lorraine Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhr_A/0/1/0/all/0/1\">Alane Suhr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selecting Shots for Demographic Fairness in Few-Shot Learning with Large Language Models. (arXiv:2311.08472v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08472","description":"<p>Recently, work in NLP has shifted to few-shot (in-context) learning, with\nlarge language models (LLMs) performing well across a range of tasks. However,\nwhile fairness evaluations have become a standard for supervised methods,\nlittle is known about the fairness of LLMs as prediction systems. Further,\ncommon standard methods for fairness involve access to models weights or are\napplied during finetuning, which are not applicable in few-shot learning. Do\nLLMs exhibit prediction biases when used for standard NLP tasks? In this work,\nwe explore the effect of shots, which directly affect the performance of\nmodels, on the fairness of LLMs as NLP classification systems. We consider how\ndifferent shot selection strategies, both existing and new demographically\nsensitive methods, affect model fairness across three standard fairness\ndatasets. We discuss how future work can include LLM fairness evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aguirre_C/0/1/0/all/0/1\">Carlos Aguirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasse_K/0/1/0/all/0/1\">Kuleen Sasse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cachola_I/0/1/0/all/0/1\">Isabel Cachola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1\">Mark Dredze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Functionality learning through specification instructions. (arXiv:2311.08481v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08481","description":"<p>Test suites assess natural language processing models' performance on\nspecific functionalities: cases of interest involving model robustness,\nfairness, or particular linguistic capabilities. They enable fine-grained\nevaluations of model aspects that would otherwise go unnoticed in standard\nevaluation datasets, but they do not address the problem of how to fix the\nfailure cases. Previous work has explored functionality learning by fine-tuning\nmodels on suite data. While this improves performance on seen functionalities,\nit often does not generalize to unseen ones and can harm general performance.\n</p>\n<p>This paper analyses a fine-tuning-free approach to functionality learning.\nFor each functionality in a suite, we generate a specification instruction that\nencodes it. We combine the obtained specification instructions to create\nspecification-augmented prompts, which we feed to language models pre-trained\non natural instruction data to generate suite predictions. A core aspect of our\nanalysis is to measure the effect that including a set of specifications has on\na held-out set of unseen, qualitatively different specifications. Our\nexperiments across four tasks and models ranging from 80M to 175B parameters\nshow that smaller models struggle to follow specification instructions.\nHowever, larger models (&gt; 3B params.) can benefit from specifications and even\ngeneralize desirable behaviors across functionalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Araujo_P/0/1/0/all/0/1\">Pedro Henrique Luz de Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alignment is not sufficient to prevent large language models from generating harmful information: A psychoanalytic perspective. (arXiv:2311.08487v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08487","description":"<p>Large Language Models (LLMs) are central to a multitude of applications but\nstruggle with significant risks, notably in generating harmful content and\nbiases. Drawing an analogy to the human psyche's conflict between evolutionary\nsurvival instincts and societal norm adherence elucidated in Freud's\npsychoanalysis theory, we argue that LLMs suffer a similar fundamental\nconflict, arising between their inherent desire for syntactic and semantic\ncontinuity, established during the pre-training phase, and the post-training\nalignment with human values. This conflict renders LLMs vulnerable to\nadversarial attacks, wherein intensifying the models' desire for continuity can\ncircumvent alignment efforts, resulting in the generation of harmful\ninformation. Through a series of experiments, we first validated the existence\nof the desire for continuity in LLMs, and further devised a straightforward yet\npowerful technique, such as incomplete sentences, negative priming, and\ncognitive dissonance scenarios, to demonstrate that even advanced LLMs struggle\nto prevent the generation of harmful information. In summary, our study\nuncovers the root of LLMs' vulnerabilities to adversarial attacks, hereby\nquestioning the efficacy of solely relying on sophisticated alignment methods,\nand further advocates for a new training idea that integrates modal concepts\nalongside traditional amodal concepts, aiming to endow LLMs with a more nuanced\nunderstanding of real-world contexts and ethical considerations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jia Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Structured Chain-of-Thought: Integrating Multiple Sources of Knowledge for Improved Language Model Reasoning. (arXiv:2311.08505v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08505","description":"<p>An important open question pertaining to the use of large language models for\nknowledge-intensive tasks is how to effectively integrate knowledge from three\nsources: the model's parametric memory, external structured knowledge, and\nexternal unstructured knowledge. Most existing prompting methods either rely\nsolely on one or two of these sources, or require repeatedly invoking large\nlanguage models to generate similar or identical content. In this work, we\novercome these limitations by introducing a novel semi-structured prompting\napproach that seamlessly integrates the model's parametric memory with\nunstructured knowledge from text documents and structured knowledge from\nknowledge graphs. Experimental results on open-domain multi-hop question\nanswering datasets demonstrate that our prompting method significantly\nsurpasses existing techniques, even exceeding those which require fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1\">Xin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Tiep Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethard_S/0/1/0/all/0/1\">Steven Bethard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howard_P/0/1/0/all/0/1\">Phillip Howard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoRE-CoG: Conversational Recommendation of Entities using Constrained Generation. (arXiv:2311.08511v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08511","description":"<p>End-to-end conversational recommendation systems (CRS) generate responses by\nleveraging both dialog history and a knowledge base (KB). A CRS mainly faces\nthree key challenges: (1) at each turn, it must decide if recommending a KB\nentity is appropriate; if so, it must identify the most relevant KB entity to\nrecommend; and finally, it must recommend the entity in a fluent utterance that\nis consistent with the conversation history. Recent CRSs do not pay sufficient\nattention to these desiderata, often generating unfluent responses or not\nrecommending (relevant) entities at the right turn. We introduce a new CRS we\ncall CoRE-CoG. CoRE-CoG addresses the limitations in prior systems by\nimplementing (1) a recommendation trigger that decides if the system utterance\nshould include an entity, (2) a type pruning module that improves the relevance\nof recommended entities, and (3) a novel constrained response generator to make\nrecommendations while maintaining fluency. Together, these modules ensure\nsimultaneous accurate recommendation decisions and fluent system utterances.\nExperiments with recent benchmarks show the superiority particularly on\nconditional generation sub-tasks with close to 10 F1 and 4 Recall@1 percent\npoints gain over baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_H/0/1/0/all/0/1\">Harshvardhan Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruthi_K/0/1/0/all/0/1\">Kanav Pruthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1\">Soumen Chakrabarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLMs cannot find reasoning errors, but can correct them!. (arXiv:2311.08516v1 [cs.AI])","link":"http://arxiv.org/abs/2311.08516","description":"<p>While self-correction has shown promise in improving LLM outputs in terms of\nstyle and quality (e.g. Chen et al., 2023; Madaan et al., 2023), recent\nattempts to self-correct logical or reasoning errors often cause correct\nanswers to become incorrect, resulting in worse performances overall (Huang et\nal., 2023). In this paper, we break down the self-correction process into two\ncore components: mistake finding and output correction. For mistake finding, we\nrelease BIG-Bench Mistake, a dataset of logical mistakes in Chain-of-Thought\nreasoning traces. We provide benchmark numbers for several state-of-the-art\nLLMs, and demonstrate that LLMs generally struggle with finding logical\nmistakes. For output correction, we propose a backtracking method which\nprovides large improvements when given information on mistake location. We\nconstrue backtracking as a lightweight alternative to reinforcement learning\nmethods, and show that it remains effective with a reward model at 60-70%\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tyen_G/0/1/0/all/0/1\">Gladys Tyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansoor_H/0/1/0/all/0/1\">Hassan Mansoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peter Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mak_T/0/1/0/all/0/1\">Tony Mak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carbune_V/0/1/0/all/0/1\">Victor C&#x103;rbune</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer. (arXiv:2311.08526v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08526","description":"<p>Named Entity Recognition (NER) is essential in various Natural Language\nProcessing (NLP) applications. Traditional NER models are effective but limited\nto a set of predefined entity types. In contrast, Large Language Models (LLMs)\ncan extract arbitrary entities through natural language instructions, offering\ngreater flexibility. However, their size and cost, particularly for those\naccessed via APIs like ChatGPT, make them impractical in resource-limited\nscenarios. In this paper, we introduce a compact NER model trained to identify\nany type of entity. Leveraging a bidirectional transformer encoder, our model,\nGLiNER, facilitates parallel entity extraction, an advantage over the slow\nsequential token generation of LLMs. Through comprehensive testing, GLiNER\ndemonstrate strong performance, outperforming both ChatGPT and fine-tuned LLMs\nin zero-shot evaluations on various NER benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaratiana_U/0/1/0/all/0/1\">Urchade Zaratiana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomeh_N/0/1/0/all/0/1\">Nadi Tomeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holat_P/0/1/0/all/0/1\">Pierre Holat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charnois_T/0/1/0/all/0/1\">Thierry Charnois</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Processing for Financial Regulation. (arXiv:2311.08533v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08533","description":"<p>This article provides an understanding of Natural Language Processing\ntechniques in the framework of financial regulation, more specifically in order\nto perform semantic matching search between rules and policy when no dataset is\navailable for supervised learning. We outline how to outperform simple\npre-trained sentences-transformer models using freely available resources and\nexplain the mathematical concepts behind the key building blocks of Natural\nLanguage Processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Achitouv_I/0/1/0/all/0/1\">Ixandra Achitouv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorduza_D/0/1/0/all/0/1\">Dragos Gorduza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacquier_A/0/1/0/all/0/1\">Antoine Jacquier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extending Multilingual Machine Translation through Imitation Learning. (arXiv:2311.08538v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08538","description":"<p>Despite the growing variety of languages supported by existing multilingual\nneural machine translation (MNMT) models, most of the world's languages are\nstill being left behind. We aim to extend large-scale MNMT models to a new\nlanguage, allowing for translation between the newly added and all of the\nalready supported languages in a challenging scenario: using only a parallel\ncorpus between the new language and English. Previous approaches, such as\ncontinued training on parallel data including the new language, suffer from\ncatastrophic forgetting (i.e., performance on other languages is reduced). Our\nnovel approach Imit-MNMT treats the task as an imitation learning process,\nwhich mimicks the behavior of an expert, a technique widely used in the\ncomputer vision area, but not well explored in NLP. More specifically, we\nconstruct a pseudo multi-parallel corpus of the new and the original languages\nby pivoting through English, and imitate the output distribution of the\noriginal MNMT model. Extensive experiments show that our approach significantly\nimproves the translation performance between the new and the original\nlanguages, without severe catastrophic forgetting. We also demonstrate that our\napproach is capable of solving copy and off-target problems, which are two\ncommon issues existence in current large-scale MNMT models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1\">Wen Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hangya_V/0/1/0/all/0/1\">Viktor Hangya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Continual Pre-training for Building Domain Specific Large Language Models. (arXiv:2311.08545v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08545","description":"<p>Large language models (LLMs) have demonstrated remarkable open-domain\ncapabilities. Traditionally, LLMs tailored for a domain are trained from\nscratch to excel at handling domain-specific tasks. In this work, we explore an\nalternative strategy of continual pre-training as a means to develop\ndomain-specific LLMs. We introduce FinPythia-6.9B, developed through\ndomain-adaptive continual pre-training on the financial domain. Continual\npre-trained FinPythia showcases consistent improvements on financial tasks over\nthe original foundational model. We further explore simple but effective data\nselection strategies for continual pre-training. Our data selection strategies\noutperforms vanilla continual pre-training's performance with just 10% of\ncorpus size and cost, without any degradation on open-domain standard tasks.\nOur work proposes an alternative solution to building domain-specific LLMs from\nscratch in a cost-effective manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_K/0/1/0/all/0/1\">Karan Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1\">Aitzaz Ahmad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UT5: Pretraining Non autoregressive T5 with unrolled denoising. (arXiv:2311.08552v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08552","description":"<p>Recent advances in Transformer-based Large Language Models have made great\nstrides in natural language generation. However, to decode K tokens, an\nautoregressive model needs K sequential forward passes, which may be a\nperformance bottleneck for large language models. Many non-autoregressive (NAR)\nresearch are aiming to address this sequentiality bottleneck, albeit many have\nfocused on a dedicated architecture in supervised benchmarks. In this work, we\nstudied unsupervised pretraining for non auto-regressive T5 models via unrolled\ndenoising and shown its SoTA results in downstream generation tasks such as\nSQuAD question generation and XSum.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salem_M/0/1/0/all/0/1\">Mahmoud G. Salem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiayu Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chu-Cheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Frederick Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAgIC: Benchmarking Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration. (arXiv:2311.08562v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08562","description":"<p>Large Language Models (LLMs) have marked a significant advancement in the\nfield of natural language processing, demonstrating exceptional capabilities in\nreasoning, tool usage, and memory. As their applications extend into\nmulti-agent environments, a need has arisen for a comprehensive evaluation\nframework that captures their abilities in reasoning, planning, collaboration,\nand more. This work introduces a novel benchmarking framework specifically\ntailored to assess LLMs within multi-agent settings, providing quantitative\nmetrics to evaluate their judgment, reasoning, deception, self-awareness,\ncollaboration, coordination, and rationality. We utilize games such as\nChameleon and Undercover, alongside game theory scenarios like Cost Sharing,\nMulti-player Prisoner's Dilemma, and Public Good, to create diverse testing\nenvironments. Our framework is fortified with the Probabilistic Graphical\nModeling (PGM) method, enhancing the LLMs' capabilities in navigating complex\nsocial and cognitive dimensions. The benchmark evaluates seven multi-agent\nsystems powered by different LLMs, quantitatively highlighting a significant\ncapability gap over threefold between the strongest, GPT-4, and the weakest,\nLlama-2-70B. It also confirms that our PGM enhancement boosts the inherent\nabilities of all selected models by 50% on average. Our codes are released here\nhttps://github.com/cathyxl/MAgIC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Daquan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1\">See Kiong Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Multilingual Summarisation: An Empirical Study. (arXiv:2311.08572v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08572","description":"<p>With the increasing prevalence of Large Language Models, traditional full\nfine-tuning approaches face growing challenges, especially in memory-intensive\ntasks. This paper investigates the potential of Parameter-Efficient\nFine-Tuning, focusing on Low-Rank Adaptation (LoRA), for complex and\nunder-explored multilingual summarisation tasks. We conduct an extensive study\nacross different data availability scenarios, including full-data, low-data,\nand cross-lingual transfer, leveraging models of different sizes. Our findings\nreveal that LoRA lags behind full fine-tuning when trained with full data,\nhowever, it excels in low-data scenarios and cross-lingual transfer.\nInterestingly, as models scale up, the performance gap between LoRA and full\nfine-tuning diminishes. Additionally, we investigate effective strategies for\nfew-shot cross-lingual transfer, finding that continued LoRA tuning achieves\nthe best performance compared to both full fine-tuning and dynamic composition\nof language-specific LoRA modules.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Whitehouse_C/0/1/0/all/0/1\">Chenxi Whitehouse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huot_F/0/1/0/all/0/1\">Fantine Huot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastings_J/0/1/0/all/0/1\">Jasmijn Bastings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chu-Cheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Evaluating AI Systems for Moral Status Using Self-Reports. (arXiv:2311.08576v1 [cs.LG])","link":"http://arxiv.org/abs/2311.08576","description":"<p>As AI systems become more advanced and widely deployed, there will likely be\nincreasing debate over whether AI systems could have conscious experiences,\ndesires, or other states of potential moral significance. It is important to\ninform these discussions with empirical evidence to the extent possible. We\nargue that under the right circumstances, self-reports, or an AI system's\nstatements about its own internal states, could provide an avenue for\ninvestigating whether AI systems have states of moral significance.\nSelf-reports are the main way such states are assessed in humans (\"Are you in\npain?\"), but self-reports from current systems like large language models are\nspurious for many reasons (e.g. often just reflecting what humans would say).\nTo make self-reports more appropriate for this purpose, we propose to train\nmodels to answer many kinds of questions about themselves with known answers,\nwhile avoiding or limiting training incentives that bias self-reports. The hope\nof this approach is that models will develop introspection-like capabilities,\nand that these capabilities will generalize to questions about states of moral\nsignificance. We then propose methods for assessing the extent to which these\ntechniques have succeeded: evaluating self-report consistency across contexts\nand between similar models, measuring the confidence and resilience of models'\nself-reports, and using interpretability to corroborate self-reports. We also\ndiscuss challenges for our approach, from philosophical difficulties in\ninterpreting self-reports to technical reasons why our proposal might fail. We\nhope our discussion inspires philosophers and AI researchers to criticize and\nimprove our proposed methodology, as well as to run experiments to test whether\nself-reports can be made reliable enough to provide information about states of\nmoral significance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_R/0/1/0/all/0/1\">Robert Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph-Induced Syntactic-Semantic Spaces in Transformer-Based Variational AutoEncoders. (arXiv:2311.08579v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08579","description":"<p>The injection of syntactic information in Variational AutoEncoders (VAEs) has\nbeen shown to result in an overall improvement of performances and\ngeneralisation. An effective strategy to achieve such a goal is to separate the\nencoding of distributional semantic features and syntactic structures into\nheterogeneous latent spaces via multi-task learning or dual encoder\narchitectures. However, existing works employing such techniques are limited to\nLSTM-based VAEs. In this paper, we investigate latent space separation methods\nfor structural syntactic injection in Transformer-based VAE architectures\n(i.e., Optimus). Specifically, we explore how syntactic structures can be\nleveraged in the encoding stage through the integration of graph-based and\nsequential models, and how multiple, specialised latent representations can be\ninjected into the decoder's attention mechanism via low-rank operators. Our\nempirical evaluation, carried out on natural language sentences and\nmathematical expressions, reveals that the proposed end-to-end VAE architecture\ncan result in a better overall organisation of the latent space, alleviating\nthe information loss occurring in standard VAE setups, resulting in enhanced\nperformances on language modelling and downstream generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1\">Marco Valentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_D/0/1/0/all/0/1\">Danilo S. Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratt_Hartmann_I/0/1/0/all/0/1\">Ian Pratt-Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asking More Informative Questions for Grounded Retrieval. (arXiv:2311.08584v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08584","description":"<p>When a model is trying to gather information in an interactive setting, it\nbenefits from asking informative questions. However, in the case of a grounded\nmulti-turn image identification task, previous studies have been constrained to\npolar yes/no questions, limiting how much information the model can gain in a\nsingle turn. We present an approach that formulates more informative,\nopen-ended questions. In doing so, we discover that off-the-shelf visual\nquestion answering (VQA) models often make presupposition errors, which\nstandard information gain question selection methods fail to account for. To\naddress this issue, we propose a method that can incorporate presupposition\nhandling into both question selection and belief updates. Specifically, we use\na two-stage process, where the model first filters out images which are\nirrelevant to a given question, then updates its beliefs about which image the\nuser intends. Through self-play and human evaluations, we show that our method\nis successful in asking informative open-ended questions, increasing accuracy\nover the past state-of-the-art by 14%, while resulting in 48% more efficient\ngames in human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keh_S/0/1/0/all/0/1\">Sedrick Keh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_J/0/1/0/all/0/1\">Justin T. Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation. (arXiv:2311.08588v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08588","description":"<p>Large Language Models (LLMs) have demonstrated remarkable performance on\ncoding related tasks, particularly on assisting humans in programming and\nfacilitating programming automation. However, existing benchmarks for\nevaluating the code understanding and generation capacities of LLMs suffer from\nsevere limitations. First, most benchmarks are deficient as they focus on a\nnarrow range of popular programming languages and specific tasks, whereas the\nreal-world software development scenarios show dire need to implement systems\nwith multilingual programming environments to satisfy diverse requirements.\nPractical programming practices also strongly expect multi-task settings for\ntesting coding capabilities of LLMs comprehensively and robustly. Second, most\nbenchmarks also fail to consider the actual executability and the consistency\nof execution results of the generated code. To bridge these gaps between\nexisting benchmarks and expectations from practical applications, we introduce\nCodeScope, an execution-based, multilingual, multi-task, multi-dimensional\nevaluation benchmark for comprehensively gauging LLM capabilities on coding\ntasks. CodeScope covers 43 programming languages and 8 coding tasks. It\nevaluates the coding performance of LLMs from three dimensions (perspectives):\ndifficulty, efficiency, and length. To facilitate execution-based evaluations\nof code generation, we develop MultiCodeEngine, an automated code execution\nengine that supports 14 programming languages. Finally, we systematically\nevaluate and analyze 8 mainstream LLMs on CodeScope tasks and demonstrate the\nsuperior breadth and challenges of CodeScope for evaluating LLMs on code\nunderstanding and generation tasks compared to other benchmarks. The CodeScope\nbenchmark and datasets are publicly available at\nhttps://github.com/WeixiangYAN/CodeScope.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1\">Weixiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haitian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunkun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tingyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weishan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Li Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shuiguang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaram_H/0/1/0/all/0/1\">Hari Sundaram</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PEMA: Plug-in External Memory Adaptation for Language Models. (arXiv:2311.08590v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08590","description":"<p>Pre-trained language models (PLMs) have demonstrated impressive performance\nacross various downstream NLP tasks. Nevertheless, the resource requirements of\npre-training large language models in terms of memory and training compute pose\nsignificant challenges. Furthermore, due to the substantial resources required,\nmany PLM weights are confidential. Consequently, users are compelled to share\ntheir data with model owners for fine-tuning on specific tasks. To overcome the\nlimitations, we introduce Plug-in External Memory Adaptation (PEMA), a\nParameter-Efficient Fine-Tuning (PEFT) approach designed for fine-tuning PLMs\nwithout the need for all weights. PEMA can be integrated into the context\nrepresentation of test data during inference to execute downstream tasks. It\nleverages an external memory to store context representations generated by a\nPLM, mapped with the desired target word. Our method entails training\nLoRA-based weight matrices within the final layer of the PLM for enhanced\nefficiency. The probability is then interpolated with the next-word\ndistribution from the PLM to perform downstream tasks. To improve the\ngeneration quality, we propose a novel interpolation strategy named Gradual\nUnrolling. To demonstrate the effectiveness of our proposed method, we conduct\nexperiments to demonstrate the efficacy of PEMA with a syntactic dataset and\nassess its performance on machine translation and style transfer tasks using\nreal datasets. PEMA outperforms other PEFT methods in terms of memory and\nlatency efficiency for training and inference. Furthermore, it outperforms\nother baselines in preserving the meaning of sentences while generating\nappropriate language and styles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">HyunJin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bak_J/0/1/0/all/0/1\">JinYeong Bak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-powered Applications. (arXiv:2311.08592v1 [cs.SE])","link":"http://arxiv.org/abs/2311.08592","description":"<p>Adversarial testing of large language models (LLMs) is crucial for their safe\nand responsible deployment. We introduce a novel approach for automated\ngeneration of adversarial evaluation datasets to test the safety of LLM\ngenerations on new downstream applications. We call it AI-assisted Red-Teaming\n(AART) - an automated alternative to current manual red-teaming efforts. AART\noffers a data generation and augmentation pipeline of reusable and customizable\nrecipes that reduce human effort significantly and enable integration of\nadversarial testing earlier in new product development. AART generates\nevaluation datasets with high diversity of content characteristics critical for\neffective adversarial testing (e.g. sensitive and harmful concepts, specific to\na wide range of cultural and geographic regions and application scenarios). The\ndata generation is steered by AI-assisted recipes to define, scope and\nprioritize diversity within the application context. This feeds into a\nstructured LLM-generation process that scales up evaluation priorities.\nCompared to some state-of-the-art tools, AART shows promising results in terms\nof concept coverage and data quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Radharapu_B/0/1/0/all/0/1\">Bhaktipriya Radharapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_K/0/1/0/all/0/1\">Kevin Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aroyo_L/0/1/0/all/0/1\">Lora Aroyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahoti_P/0/1/0/all/0/1\">Preethi Lahoti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACID: Abstractive, Content-Based IDs for Document Retrieval with Language Models. (arXiv:2311.08593v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08593","description":"<p>Generative retrieval (Wang et al., 2022; Tay et al., 2022) is a new approach\nfor end-to-end document retrieval that directly generates document identifiers\ngiven an input query. Techniques for designing effective, high-quality document\nIDs remain largely unexplored. We introduce ACID, in which each document's ID\nis composed of abstractive keyphrases generated by a large language model,\nrather than an integer ID sequence as done in past work. We compare our method\nwith the current state-of-the-art technique for ID generation, which produces\nIDs through hierarchical clustering of document embeddings. We also examine\nsimpler methods to generate natural-language document IDs, including the naive\napproach of using the first k words of each document as its ID or words with\nhigh BM25 scores in that document. We show that using ACID improves top-10 and\ntop-20 accuracy by 15.6% and 14.4% (relative) respectively versus the\nstate-of-the-art baseline on the MSMARCO 100k retrieval task, and 4.4% and 4.0%\nrespectively on the Natural Questions 100k retrieval task. Our results\ndemonstrate the effectiveness of human-readable, natural-language IDs in\ngenerative retrieval with LMs. The code for reproducing our results and the\nkeyword-augmented datasets will be released on formal publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keung_P/0/1/0/all/0/1\">Phillip Keung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1\">Daniel Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are You Sure? Challenging LLMs Leads to Performance Drops in The FlipFlop Experiment. (arXiv:2311.08596v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08596","description":"<p>The interactive nature of Large Language Models (LLMs) theoretically allows\nmodels to refine and improve their answers, yet systematic analysis of the\nmulti-turn behavior of LLMs remains limited. In this paper, we propose the\nFlipFlop experiment: in the first round of the conversation, an LLM responds to\na prompt containing a classification task. In a second round, the LLM is\nchallenged with a follow-up phrase like \"Are you sure?\", offering an\nopportunity for the model to reflect on its initial answer, and decide whether\nto confirm or flip its answer. A systematic study of nine LLMs on seven\nclassification tasks reveals that models flip their answers on average 46% of\nthe time and that all models see a deterioration of accuracy between their\nfirst and final prediction, with an average drop of 17%. The FlipFlop\nexperiment illustrates the universality of sycophantic behavior in LLMs and\nprovides a robust framework to analyze model behavior and evaluate potential\nsolutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1\">Philippe Laban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murakhovska_L/0/1/0/all/0/1\">Lidiya Murakhovs&#x27;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DALA: A Distribution-Aware LoRA-Based Adversarial Attack against Pre-trained Language Models. (arXiv:2311.08598v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08598","description":"<p>Pre-trained language models (PLMs) that achieve success in applications are\nsusceptible to adversarial attack methods that are capable of generating\nadversarial examples with minor perturbations. Although recent attack methods\ncan achieve a relatively high attack success rate (ASR), our observation shows\nthat the generated adversarial examples have a different data distribution\ncompared with the original examples. Specifically, these adversarial examples\nexhibit lower confidence levels and higher distance to the training data\ndistribution. As a result, they are easy to detect using very simple detection\nmethods, diminishing the actual effectiveness of these attack methods. To solve\nthis problem, we propose a Distribution-Aware LoRA-based Adversarial Attack\n(DALA) method, which considers the distribution shift of adversarial examples\nto improve attack effectiveness under detection methods. We further design a\nnew evaluation metric NASR combining ASR and detection for the attack task. We\nconduct experiments on four widely-used datasets and validate the attack\neffectiveness on ASR and NASR of the adversarial examples generated by DALA on\nthe BERT-base model and the black-box LLaMA2-7b model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiangjue Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caverlee_J/0/1/0/all/0/1\">James Caverlee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Navigating the Ocean of Biases: Political Bias Attribution in Language Models via Causal Structures. (arXiv:2311.08605v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08605","description":"<p>The rapid advancement of Large Language Models (LLMs) has sparked intense\ndebate regarding their ability to perceive and interpret complex\nsocio-political landscapes. In this study, we undertake an exploration of\ndecision-making processes and inherent biases within LLMs, exemplified by\nChatGPT, specifically contextualizing our analysis within political debates. We\naim not to critique or validate LLMs' values, but rather to discern how they\ninterpret and adjudicate \"good arguments.\" By applying Activity Dependency\nNetworks (ADNs), we extract the LLMs' implicit criteria for such assessments\nand illustrate how normative values influence these perceptions. We discuss the\nconsequences of our findings for human-AI alignment and bias mitigation. Our\ncode and data at https://github.com/david-jenny/LLM-Political-Study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jenny_D/0/1/0/all/0/1\">David F. Jenny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Billeter_Y/0/1/0/all/0/1\">Yann Billeter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Generalizable SER: Soft Labeling and Data Augmentation for Modeling Temporal Emotion Shifts in Large-Scale Multilingual Speech. (arXiv:2311.08607v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08607","description":"<p>Recognizing emotions in spoken communication is crucial for advanced\nhuman-machine interaction. Current emotion detection methodologies often\ndisplay biases when applied cross-corpus. To address this, our study\namalgamates 16 diverse datasets, resulting in 375 hours of data across\nlanguages like English, Chinese, and Japanese. We propose a soft labeling\nsystem to capture gradational emotional intensities. Using the Whisper encoder\nand data augmentation methods inspired by contrastive learning, our method\nemphasizes the temporal dynamics of emotions. Our validation on four\nmultilingual datasets demonstrates notable zero-shot generalization. We publish\nour open source model weights and initial promising results after fine-tuning\non Hume-Prosody.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Osman_M/0/1/0/all/0/1\">Mohamed Osman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeem_T/0/1/0/all/0/1\">Tamer Nadeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khoriba_G/0/1/0/all/0/1\">Ghada Khoriba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XplainLLM: A QA Explanation Dataset for Understanding LLM Decision-Making. (arXiv:2311.08614v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08614","description":"<p>Large Language Models (LLMs) have recently made impressive strides in natural\nlanguage understanding tasks. Despite their remarkable performance,\nunderstanding their decision-making process remains a big challenge. In this\npaper, we look into bringing some transparency to this process by introducing a\nnew explanation dataset for question answering (QA) tasks that integrates\nknowledge graphs (KGs) in a novel way. Our dataset includes 12,102\nquestion-answer-explanation (QAE) triples. Each explanation in the dataset\nlinks the LLM's reasoning to entities and relations in the KGs. The explanation\ncomponent includes a why-choose explanation, a why-not-choose explanation, and\na set of reason-elements that underlie the LLM's decision. We leverage KGs and\ngraph attention networks (GAT) to find the reason-elements and transform them\ninto why-choose and why-not-choose explanations that are comprehensible to\nhumans. Through quantitative and qualitative evaluations, we demonstrate the\npotential of our dataset to improve the in-context learning of LLMs, and\nenhance their interpretability and explainability. Our work contributes to the\nfield of explainable AI by enabling a deeper understanding of the LLMs\ndecision-making process to make them more transparent and thereby, potentially\nmore reliable, to researchers and practitioners alike. Our dataset is available\nat: https://github.com/chen-zichen/XplainLLM_dataset.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zichen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianda Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidhani_M/0/1/0/all/0/1\">Mitali Gaidhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Ambuj Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sra_M/0/1/0/all/0/1\">Misha Sra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toucan: Token-Aware Character Level Language Modeling. (arXiv:2311.08620v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08620","description":"<p>Character-level language models obviate the need for separately trained\ntokenizers, but efficiency suffers from longer sequence lengths. Learning to\ncombine character representations into tokens has made training these models\nmore efficient, but they still require decoding characters individually. We\npropose Toucan, an augmentation to character-level models to make them\n\"token-aware\". Comparing our method to prior work, we demonstrate significant\nspeed-ups in character generation without a loss in language modeling\nperformance. We then explore differences between our learned dynamic\ntokenization of character sequences with popular fixed vocabulary solutions\nsuch as Byte-Pair Encoding and WordPiece, finding our approach leads to a\ngreater amount of longer sequences tokenized as single items. Our project and\ncode are available at https://nlp.jhu.edu/nuggets/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fleshman_W/0/1/0/all/0/1\">William Fleshman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiple-Question Multiple-Answer Text-VQA. (arXiv:2311.08622v1 [cs.CV])","link":"http://arxiv.org/abs/2311.08622","description":"<p>We present Multiple-Question Multiple-Answer (MQMA), a novel approach to do\ntext-VQA in encoder-decoder transformer models. The text-VQA task requires a\nmodel to answer a question by understanding multi-modal content: text\n(typically from OCR) and an associated image. To the best of our knowledge,\nalmost all previous approaches for text-VQA process a single question and its\nassociated content to predict a single answer. In order to answer multiple\nquestions from the same image, each question and content are fed into the model\nmultiple times. In contrast, our proposed MQMA approach takes multiple\nquestions and content as input at the encoder and predicts multiple answers at\nthe decoder in an auto-regressive manner at the same time. We make several\nnovel architectural modifications to standard encoder-decoder transformers to\nsupport MQMA. We also propose a novel MQMA denoising pre-training task which is\ndesigned to teach the model to align and delineate multiple questions and\ncontent with associated answers. MQMA pre-trained model achieves\nstate-of-the-art results on multiple text-VQA datasets, each with strong\nbaselines. Specifically, on OCR-VQA (+2.5%), TextVQA (+1.4%), ST-VQA (+0.6%),\nDocVQA (+1.1%) absolute improvements over the previous state-of-the-art\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_P/0/1/0/all/0/1\">Peng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Appalaraju_S/0/1/0/all/0/1\">Srikar Appalaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1\">R. Manmatha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yusheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahadevan_V/0/1/0/all/0/1\">Vijay Mahadevan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models. (arXiv:2311.08623v1 [cs.CV])","link":"http://arxiv.org/abs/2311.08623","description":"<p>Encoder-decoder transformer models have achieved great success on various\nvision-language (VL) tasks, but they suffer from high inference latency.\nTypically, the decoder takes up most of the latency because of the\nauto-regressive decoding. To accelerate the inference, we propose an approach\nof performing Dynamic Early Exit on Decoder (DEED). We build a multi-exit\nencoder-decoder transformer model which is trained with deep supervision so\nthat each of its decoder layers is capable of generating plausible predictions.\nIn addition, we leverage simple yet practical techniques, including shared\ngeneration head and adaptation modules, to keep accuracy when exiting at\nshallow decoder layers. Based on the multi-exit model, we perform step-level\ndynamic early exit during inference, where the model may decide to use fewer\ndecoder layers based on its confidence of the current layer at each individual\ndecoding step. Considering different number of decoder layers may be used at\ndifferent decoding steps, we compute deeper-layer decoder features of previous\ndecoding steps just-in-time, which ensures the features from different decoding\nsteps are semantically aligned. We evaluate our approach with two\nstate-of-the-art encoder-decoder transformer models on various VL tasks. We\nshow our approach can reduce overall inference latency by 30%-60% with\ncomparable or even higher accuracy compared to baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_P/0/1/0/all/0/1\">Peng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Pengkai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Appalaraju_S/0/1/0/all/0/1\">Srikar Appalaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahadevan_V/0/1/0/all/0/1\">Vijay Mahadevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1\">R. Manmatha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Formal Proofs as Structured Explanations: Proposing Several Tasks on Explainable Natural Language Inference. (arXiv:2311.08637v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08637","description":"<p>In this position paper, we propose a way of exploiting formal proofs to put\nforward several explainable natural language inference (NLI) tasks. The formal\nproofs will be produced by a reliable and high-performing logic-based NLI\nsystem. Taking advantage of the in-depth information available in the generated\nformal proofs, we show how it can be used to define NLI tasks with structured\nexplanations. The proposed tasks can be ordered according to difficulty defined\nin terms of the granularity of explanations. We argue that the tasks will\nsuffer with substantially fewer shortcomings than the existing explainable NLI\ntasks (or datasets).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abzianidze_L/0/1/0/all/0/1\">Lasha Abzianidze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multistage Collaborative Knowledge Distillation from Large Language Models. (arXiv:2311.08640v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08640","description":"<p>We study semi-supervised sequence prediction tasks where labeled data are too\nscarce to effectively finetune a model and at the same time few-shot prompting\nof a large language model (LLM) has suboptimal performance. This happens when a\ntask, such as parsing, is expensive to annotate and also unfamiliar to a\npretrained LLM. In this paper, we present a discovery that student models\ndistilled from a prompted LLM can often generalize better than their teacher on\nsuch tasks. Leveraging this finding, we propose a new distillation method,\nmultistage collaborative knowledge distillation from an LLM (MCKD), for such\ntasks. MCKD first prompts an LLM using few-shot in-context learning to produce\npseudolabels for unlabeled data. Then, at each stage of distillation, a pair of\nstudents are trained on disjoint partitions of the pseudolabeled data. Each\nstudent subsequently produces new and improved pseudolabels for the unseen\npartition to supervise the next round of student(s) with. We show the benefit\nof multistage cross-partition labeling on two constituency parsing tasks. On\nCRAFT biomedical parsing, 3-stage MCKD with 50 labeled examples matches the\nperformance of supervised finetuning with 500 examples and outperforms the\nprompted LLM and vanilla KD by 7.5% and 3.7% parsing F1, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiachen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenlong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdov_A/0/1/0/all/0/1\">Andrew Drozdov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozonoyer_B/0/1/0/all/0/1\">Benjamin Rozonoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sultan_M/0/1/0/all/0/1\">Md Arafat Sultan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jay-Yoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explore Spurious Correlations at the Concept Level in Language Models for Text Classification. (arXiv:2311.08648v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08648","description":"<p>Language models (LMs) have gained great achievement in various NLP tasks for\nboth fine-tuning and in-context learning (ICL) methods. Despite its outstanding\nperformance, evidence shows that spurious correlations caused by imbalanced\nlabel distributions in training data (or exemplars in ICL) lead to robustness\nissues. However, previous studies mostly focus on word- and phrase-level\nfeatures and fail to tackle it from the concept level, partly due to the lack\nof concept labels and subtle and diverse expressions of concepts in text. In\nthis paper, we first use the LLM to label the concept for each text and then\nmeasure the concept bias of models for fine-tuning or ICL on the test data.\nSecond, we propose a data rebalancing method to mitigate the spurious\ncorrelations by adding the LLM-generated counterfactual data to make a balanced\nlabel distribution for each concept. We verify the effectiveness of our\nmitigation method and show its superiority over the token removal method.\nOverall, our results show that there exist label distribution biases in\nconcepts across multiple text classification datasets, and LMs will utilize\nthese shortcuts to make predictions in both fine-tuning and ICL methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Paiheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1\">Bang An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_W/0/1/0/all/0/1\">Wei Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Furong Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Set Inoculation: Assessing Model Robustness Across Multiple Challenge Sets. (arXiv:2311.08662v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08662","description":"<p>Language models, given their black-box nature, often exhibit sensitivity to\ninput perturbations, leading to trust issues due to hallucinations. To bolster\ntrust, it's essential to understand these models' failure modes and devise\nstrategies to enhance their performance. In this study, we propose a framework\nto study the effect of input perturbations on language models of different\nscales, from pre-trained models to large language models (LLMs). We use\nfine-tuning to train a robust model to perturbations, and we investigate\nwhether exposure to one perturbation improves or degrades the model's\nperformance on other perturbations. To address multi-perturbation robustness,\nwe suggest three distinct training strategies. We also extend the framework to\nLLMs via a chain of thought(COT) prompting with exemplars. We instantiate our\nframework for the Tabular-NLI task and show that the proposed strategies train\nthe model robust to different perturbations without losing accuracy on a given\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vatsal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandya_P/0/1/0/all/0/1\">Pranshu Pandya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kataria_T/0/1/0/all/0/1\">Tushar Kataria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vivek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It Takes Two to Negotiate: Modeling Social Exchange in Online Multiplayer Games. (arXiv:2311.08666v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08666","description":"<p>Online games are dynamic environments where players interact with each other,\nwhich offers a rich setting for understanding how players negotiate their way\nthrough the game to an ultimate victory. This work studies online player\ninteractions during the turn-based strategy game, Diplomacy. We annotated a\ndataset of over 10,000 chat messages for different negotiation strategies and\nempirically examined their importance in predicting long- and short-term game\noutcomes. Although negotiation strategies can be predicted reasonably\naccurately through the linguistic modeling of the chat messages, more is needed\nfor predicting short-term outcomes such as trustworthiness. On the other hand,\nthey are essential in graph-aware reinforcement learning approaches to predict\nlong-term outcomes, such as a player's success, based on their prior\nnegotiation history. We close with a discussion of the implications and impact\nof our work. The dataset is available at\nhttps://github.com/kj2013/claff-diplomacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jaidka_K/0/1/0/all/0/1\">Kokil Jaidka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_H/0/1/0/all/0/1\">Hansin Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_L/0/1/0/all/0/1\">Lynnette Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Calibration for Multilingual Question Answering Models. (arXiv:2311.08669v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08669","description":"<p>Multilingual pre-trained language models are incredibly effective at Question\nAnswering (QA), a core task in Natural Language Understanding, achieving high\naccuracies on several multilingual benchmarks. However, little is known about\nhow well they are calibrated. In this paper, we study the calibration\nproperties of several pre-trained multilingual large language models (LLMs) on\na variety of question-answering tasks. We perform extensive experiments,\nspanning both extractive and generative QA model designs and diverse languages,\nspanning both high-resource and low-resource ones. We study different\ndimensions of calibration in in-distribution, out-of-distribution, and\ncross-lingual transfer settings, and investigate strategies to improve it,\nincluding post-hoc methods and regularized fine-tuning. We demonstrate\nautomatically translated data augmentation as a highly effective technique to\nimprove model calibration. We also conduct a number of ablation experiments to\nstudy the effect of model size on calibration and how multilingual models\ncompare with their monolingual counterparts for diverse tasks and languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yahan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dan_S/0/1/0/all/0/1\">Soham Dan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Insup Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Safer-Instruct: Aligning Language Models with Automated Preference Data. (arXiv:2311.08685v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08685","description":"<p>Reinforcement Learning from Human Feedback (RLHF) is a vital strategy for\nenhancing model safety in language models. However, annotating preference data\nfor RLHF is a resource-intensive and creativity-demanding process, while\nautomatic generation methods face limitations in data diversity and quality. In\nresponse, we present Safer-Instruct, a novel pipeline for semi-automatically\nconstructing large-scale preference datasets. Our approach leverages reversed\ninstruction tuning, instruction induction, and expert model evaluation to\nefficiently generate high-quality preference data without human annotators. We\nevaluate Safer-Instruct using LLaMA for instruction induction and GPT-4 as an\nexpert model, generating approximately 10K preference samples. Finetuning an\nAlpaca model on this dataset demonstrates improved harmlessness while\nmaintaining competitive performance on conversation and downstream tasks.\nSafer-Instruct addresses the challenges in preference data acquisition,\nadvancing the development of safer and more responsible AI systems. Our code\nand data are available at https://github.com/uscnlp-lime/safer-instruct\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1\">Taiwei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jieyu Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Eye on Clinical BERT: Investigating Language Model Generalization for Diabetic Eye Disease Phenotyping. (arXiv:2311.08687v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08687","description":"<p>Diabetic eye disease is a major cause of blindness worldwide. The ability to\nmonitor relevant clinical trajectories and detect lapses in care is critical to\nmanaging the disease and preventing blindness. Alas, much of the information\nnecessary to support these goals is found only in the free text of the\nelectronic medical record. To fill this information gap, we introduce a system\nfor extracting evidence from clinical text of 19 clinical concepts related to\ndiabetic eye disease and inferring relevant attributes for each. In developing\nthis ophthalmology phenotyping system, we are also afforded a unique\nopportunity to evaluate the effectiveness of clinical language models at\nadapting to new clinical domains. Across multiple training paradigms, we find\nthat BERT language models pretrained on out-of-distribution clinical data offer\nno significant improvement over BERT language models pretrained on non-clinical\ndata for our domain. Our study tempers recent claims that language models\npretrained on clinical data are necessary for clinical NLP tasks and highlights\nthe importance of not treating clinical language data as a single homogeneous\ndomain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harrigian_K/0/1/0/all/0/1\">Keith Harrigian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tina Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzales_A/0/1/0/all/0/1\">Anthony Gonzales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Cindy X. Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1\">Mark Dredze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models. (arXiv:2311.08692v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08692","description":"<p>The complementary potential of Large Language Models (LLM) assumes\noff-the-shelf LLMs have heterogeneous expertise in a wide range of domains and\ntasks so that an ensemble of LLMs can achieve consistently better performance.\nExisting ensemble methods for LLMs mainly focus on reward model ranking of\noutputs, leading to significant computation overhead. To combat this issue, we\nrevisit the complementary potential of LLMs and further elaborate it by mining\nlatent expertise with off-the-shelf reward models. We propose Zooter, a\nreward-guided routing method distilling rewards on training queries to train a\nrouting function, which can precisely distribute each query to the LLM with\nexpertise about it. We also integrate a tag-based label enhancement to mitigate\nnoise from uncertainty when using rewards as silver supervision. Zooter shows\ncomputation efficiency in inference as it introduces only a minor computation\noverhead of a routing function compared with reward model ranking methods. We\nevaluate Zooter on a comprehensive benchmark collection with 26 subsets on\ndifferent domains and tasks. Zooter outperforms the best single model on\naverage and ranks first on 44% of tasks, even surpassing multiple reward model\nranking methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Keming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongyi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1\">Runji Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attribute Diversity Determines the Systematicity Gap in VQA. (arXiv:2311.08695v1 [cs.LG])","link":"http://arxiv.org/abs/2311.08695","description":"<p>The degree to which neural networks can generalize to new combinations of\nfamiliar concepts, and the conditions under which they are able to do so, has\nlong been an open question. In this work, we study the systematicity gap in\nvisual question answering: the performance difference between reasoning on\npreviously seen and unseen combinations of object attributes. To test, we\nintroduce a novel diagnostic dataset, CLEVR-HOPE. We find that while increased\nquantity of training data does not reduce the systematicity gap, increased\ntraining data diversity of the attributes in the unseen combination does. In\nall, our experiments suggest that the more distinct attribute type combinations\nare seen during training, the more systematic we can expect the resulting model\nto be.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berlot_Attwell_I/0/1/0/all/0/1\">Ian Berlot-Attwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrell_A/0/1/0/all/0/1\">A. Michael Carrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_K/0/1/0/all/0/1\">Kumar Krishna Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_Y/0/1/0/all/0/1\">Yash Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1\">Naomi Saphra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Debate Helps Supervise Unreliable Experts. (arXiv:2311.08702v1 [cs.AI])","link":"http://arxiv.org/abs/2311.08702","description":"<p>As AI systems are used to answer more difficult questions and potentially\nhelp create new knowledge, judging the truthfulness of their outputs becomes\nmore difficult and more important. How can we supervise unreliable experts,\nwhich have access to the truth but may not accurately report it, to give\nanswers that are systematically true and don't just superficially seem true,\nwhen the supervisor can't tell the difference between the two on their own? In\nthis work, we show that debate between two unreliable experts can help a\nnon-expert judge more reliably identify the truth. We collect a dataset of\nhuman-written debates on hard reading comprehension questions where the judge\nhas not read the source passage, only ever seeing expert arguments and short\nquotes selectively revealed by 'expert' debaters who have access to the\npassage. In our debates, one expert argues for the correct answer, and the\nother for an incorrect answer. Comparing debate to a baseline we call\nconsultancy, where a single expert argues for only one answer which is correct\nhalf of the time, we find that debate performs significantly better, with 84%\njudge accuracy compared to consultancy's 74%. Debates are also more efficient,\nbeing 68% of the length of consultancies. By comparing human to AI debaters, we\nfind evidence that with more skilled (in this case, human) debaters, the\nperformance of debate goes up but the performance of consultancy goes down. Our\nerror analysis also supports this trend, with 46% of errors in human debate\nattributable to mistakes by the honest debater (which should go away with\nincreased skill); whereas 52% of errors in human consultancy are due to\ndebaters obfuscating the relevant evidence from the judge (which should become\nworse with increased skill). Overall, these results show that debate is a\npromising approach for supervising increasingly capable but potentially\nunreliable AI systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michael_J/0/1/0/all/0/1\">Julian Michael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdi_S/0/1/0/all/0/1\">Salsabila Mahdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rein_D/0/1/0/all/0/1\">David Rein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petty_J/0/1/0/all/0/1\">Jackson Petty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dirani_J/0/1/0/all/0/1\">Julien Dirani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_V/0/1/0/all/0/1\">Vishakh Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Large Language Models Follow Concept Annotation Guidelines? A Case Study on Scientific and Financial Domains. (arXiv:2311.08704v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08704","description":"<p>Although large language models (LLMs) exhibit remarkable capacity to leverage\nin-context demonstrations, it is still unclear to what extent they can learn\nnew concepts or facts from ground-truth labels. To address this question, we\nexamine the capacity of instruction-tuned LLMs to follow in-context concept\nguidelines for sentence labeling tasks. We design guidelines that present\ndifferent types of factual and counterfactual concept definitions, which are\nused as prompts for zero-shot sentence classification tasks. Our results show\nthat although concept definitions consistently help in task performance, only\nthe larger models (with 70B parameters or more) have limited ability to work\nunder counterfactual contexts. Importantly, only proprietary models such as\nGPT-3.5 and GPT-4 can recognize nonsensical guidelines, which we hypothesize is\ndue to more sophisticated alignment methods. Finally, we find that\nFalcon-180B-chat is outperformed by Llama-2-70B-chat is most cases, which\nindicates that careful fine-tuning is more effective than increasing model\nscale. Altogether, our simple evaluation method reveals significant gaps in\nconcept understanding between the most capable open-source language models and\nthe leading proprietary APIs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_M/0/1/0/all/0/1\">Marcio Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Shay B. Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Robustness of Dialogue Summarization Models in the Presence of Naturally Occurring Variations. (arXiv:2311.08705v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08705","description":"<p>Dialogue summarization task involves summarizing long conversations while\npreserving the most salient information. Real-life dialogues often involve\nnaturally occurring variations (e.g., repetitions, hesitations) and existing\ndialogue summarization models suffer from performance drop on such\nconversations. In this study, we systematically investigate the impact of such\nvariations on state-of-the-art dialogue summarization models using publicly\navailable datasets. To simulate real-life variations, we introduce two types of\nperturbations: utterance-level perturbations that modify individual utterances\nwith errors and language variations, and dialogue-level perturbations that add\nnon-informative exchanges (e.g., repetitions, greetings). We conduct our\nanalysis along three dimensions of robustness: consistency, saliency, and\nfaithfulness, which capture different aspects of the summarization model's\nperformance. We find that both fine-tuned and instruction-tuned models are\naffected by input variations, with the latter being more susceptible,\nparticularly to dialogue-level perturbations. We also validate our findings via\nhuman evaluation. Finally, we investigate if the robustness of fine-tuned\nmodels can be improved by training them with a fraction of perturbed data and\nobserve that this approach is insufficient to address robustness challenges\nwith current models and thus warrants a more thorough investigation to identify\nbetter solutions. Overall, our work highlights robustness challenges in\ndialogue summarization and provides insights for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ankita Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunasekara_C/0/1/0/all/0/1\">Chulaka Gunasekara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_H/0/1/0/all/0/1\">Hui Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganhotra_J/0/1/0/all/0/1\">Jatin Ganhotra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Sachindra Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danilevsky_M/0/1/0/all/0/1\">Marina Danilevsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning. (arXiv:2311.08711v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08711","description":"<p>Instruction tuning has remarkably advanced large language models (LLMs) in\nunderstanding and responding to diverse human instructions. Despite the success\nin high-resource languages, its application in lower-resource ones faces\nchallenges due to the imbalanced foundational abilities of LLMs across\ndifferent languages, stemming from the uneven language distribution in their\npre-training data. To tackle this issue, we propose pivot language guided\ngeneration (PLUG), an approach that utilizes a high-resource language,\nprimarily English, as the pivot to enhance instruction tuning in lower-resource\nlanguages. It trains the model to first process instructions in the pivot\nlanguage, and then produce responses in the target language. To evaluate our\napproach, we introduce a benchmark, X-AlpacaEval, of instructions in 4\nlanguages (Chinese, Korean, Italian, and Spanish), each annotated by\nprofessional translators. Our approach demonstrates a significant improvement\nin the instruction-following abilities of LLMs by 29% on average, compared to\ndirectly responding in the target language alone. Further experiments validate\nthe versatility of our approach by employing alternative pivot languages beyond\nEnglish to assist languages where LLMs exhibit lower proficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-Ho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1\">Mengzhao Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbieri_F/0/1/0/all/0/1\">Francesco Barbieri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling. (arXiv:2311.08718v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08718","description":"<p>Uncertainty decomposition refers to the task of decomposing the total\nuncertainty of a model into data (aleatoric) uncertainty, resulting from the\ninherent complexity or ambiguity of the data, and model (epistemic)\nuncertainty, resulting from the lack of knowledge in the model. Performing\nuncertainty decomposition for large language models (LLMs) is an important step\ntoward improving the reliability, trustworthiness, and interpretability of\nLLMs, but this research task is very challenging and remains unresolved. The\nexisting canonical method, Bayesian Neural Network (BNN), cannot be applied to\nLLMs, because BNN requires training and ensembling multiple variants of models,\nwhich is infeasible or prohibitively expensive for LLMs. In this paper, we\nintroduce an uncertainty decomposition framework for LLMs, called input\nclarifications ensemble, which bypasses the need to train new models. Rather\nthan ensembling models with different parameters, our approach generates a set\nof clarifications for the input, feeds them into the fixed LLMs, and ensembles\nthe corresponding predictions. We show that our framework shares a symmetric\ndecomposition structure with BNN. Empirical evaluations demonstrate that the\nproposed framework provides accurate and reliable uncertainty quantification on\nvarious tasks. Code will be made publicly available at\nhttps://github.com/UCSB-NLP-Chang/llm_uncertainty .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1\">Bairu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yujian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1\">Kaizhi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory. (arXiv:2311.08719v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08719","description":"<p>Memory-augmented Large Language Models (LLMs) have demonstrated remarkable\nperformance in long-term human-machine interactions, which basically relies on\niterative recalling and reasoning of history to generate high-quality\nresponses. However, such repeated recall-reason steps easily produce biased\nthoughts, \\textit{i.e.}, inconsistent reasoning results when recalling the same\nhistory for different questions. On the contrary, humans can keep thoughts in\nthe memory and recall them without repeated reasoning. Motivated by this human\ncapability, we propose a novel memory mechanism called TiM (Think-in-Memory)\nthat enables LLMs to maintain an evolved memory for storing historical thoughts\nalong the conversation stream. The TiM framework consists of two crucial\nstages: (1) before generating a response, a LLM agent recalls relevant thoughts\nfrom memory, and (2) after generating a response, the LLM agent post-thinks and\nincorporates both historical and new thoughts to update the memory. Thus, TiM\ncan eliminate the issue of repeated reasoning by saving the post-thinking\nthoughts as the history. Besides, we formulate the basic principles to organize\nthe thoughts in memory based on the well-established operations,\n(\\textit{i.e.}, insert, forget, and merge operations), allowing for dynamic\nupdates and evolution of the thoughts. Furthermore, we introduce\nLocality-Sensitive Hashing into TiM to achieve efficient retrieval for the\nlong-term conversations. We conduct qualitative and quantitative experiments on\nreal-world and simulated dialogues covering a wide range of topics,\ndemonstrating that equipping existing LLMs with TiM significantly enhances\ntheir performance in generating responses for long-term interactions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yue Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Binbin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjie Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guannan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Token Prediction as Implicit Classification to Identify LLM-Generated Text. (arXiv:2311.08723v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08723","description":"<p>This paper introduces a novel approach for identifying the possible large\nlanguage models (LLMs) involved in text generation. Instead of adding an\nadditional classification layer to a base LM, we reframe the classification\ntask as a next-token prediction task and directly fine-tune the base LM to\nperform it. We utilize the Text-to-Text Transfer Transformer (T5) model as the\nbackbone for our experiments. We compared our approach to the more direct\napproach of utilizing hidden states for classification. Evaluation shows the\nexceptional performance of our method in the text classification task,\nhighlighting its simplicity and efficiency. Furthermore, interpretability\nstudies on the features extracted by our model reveal its ability to\ndifferentiate distinctive writing styles among various LLMs even in the absence\nof an explicit classifier. We also collected a dataset named OpenLLMText,\ncontaining approximately 340k text samples from human and LLMs, including\nGPT3.5, PaLM, LLaMA, and GPT2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yutian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hao Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_V/0/1/0/all/0/1\">Vivian Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rita Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Method for Text Entity Linking in Power Distribution Scheduling Oriented to Power Distribution Network Knowledge Graph. (arXiv:2311.08724v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08724","description":"<p>The proposed method for linking entities in power distribution dispatch texts\nto a power distribution network knowledge graph is based on a deep\nunderstanding of these networks. This method leverages the unique features of\nentities in both the power distribution network's knowledge graph and the\ndispatch texts, focusing on their semantic, phonetic, and syntactic\ncharacteristics. An enhanced model, the Lexical Semantic Feature-based Skip\nConvolutional Neural Network (LSF-SCNN), is utilized for effectively matching\ndispatch text entities with those in the knowledge graph. The efficacy of this\nmodel, compared to a control model, is evaluated through cross-validation\nmethods in real-world power distribution dispatch scenarios. The results\nindicate that the LSF-SCNN model excels in accurately linking a variety of\nentity types, demonstrating high overall accuracy in entity linking when the\nprocess is conducted in English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Che Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sizhe Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty Estimation on Sequential Labeling via Uncertainty Transmission. (arXiv:2311.08726v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08726","description":"<p>Sequential labeling is a task predicting labels for each token in a sequence,\nsuch as Named Entity Recognition (NER). NER tasks aim to extract entities and\npredict their labels given a text, which is important in information\nextraction. Although previous works have shown great progress in improving NER\nperformance, uncertainty estimation on NER (UE-NER) is still underexplored but\nessential. This work focuses on UE-NER, which aims to estimate uncertainty\nscores for the NER predictions. Previous uncertainty estimation models often\noverlook two unique characteristics of NER: the connection between entities\n(i.e., one entity embedding is learned based on the other ones) and wrong span\ncases in the entity extraction subtask. Therefore, we propose a Sequential\nLabeling Posterior Network (SLPN) to estimate uncertainty scores for the\nextracted entities, considering uncertainty transmitted from other tokens.\nMoreover, we have defined an evaluation strategy to address the specificity of\nwrong-span cases. Our SLPN has achieved significant improvements on two\ndatasets, such as a 5.54-point improvement in AUPR on the MIT-Restaurant\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Linlin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Shuo Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chang-Tien Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Emergency Decision-making with Knowledge Graphs and Large Language Models. (arXiv:2311.08732v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08732","description":"<p>Emergency management urgently requires comprehensive knowledge while having a\nhigh possibility to go beyond individuals' cognitive scope. Therefore,\nartificial intelligence(AI) supported decision-making under that circumstance\nis of vital importance. Recent emerging large language models (LLM) provide a\nnew direction for enhancing targeted machine intelligence. However, the\nutilization of LLM directly would inevitably introduce unreliable output for\nits inherent issue of hallucination and poor reasoning skills. In this work, we\ndevelop a system called Enhancing Emergency decision-making with Knowledge\nGraph and LLM (E-KELL), which provides evidence-based decision-making in\nvarious emergency stages. The study constructs a structured emergency knowledge\ngraph and guides LLMs to reason over it via a prompt chain. In real-world\nevaluations, E-KELL receives scores of 9.06, 9.09, 9.03, and 9.09 in\ncomprehensibility, accuracy, conciseness, and instructiveness from a group of\nemergency commanders and firefighters, demonstrating a significant improvement\nacross various situations compared to baseline models. This work introduces a\nnovel approach to providing reliable emergency decision support.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Minze Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1\">Zhenxiang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Weitong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tingxin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Rui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chunli Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Thread of Thought Unraveling Chaotic Contexts. (arXiv:2311.08734v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08734","description":"<p>Large Language Models (LLMs) have ushered in a transformative era in the\nfield of natural language processing, excelling in tasks related to text\ncomprehension and generation. Nevertheless, they encounter difficulties when\nconfronted with chaotic contexts (e.g., distractors rather than long irrelevant\ncontext), leading to the inadvertent omission of certain details within the\nchaotic context. In response to these challenges, we introduce the \"Thread of\nThought\" (ThoT) strategy, which draws inspiration from human cognitive\nprocesses. ThoT systematically segments and analyzes extended contexts while\nadeptly selecting pertinent information. This strategy serves as a versatile\n\"plug-and-play\" module, seamlessly integrating with various LLMs and prompting\ntechniques. In the experiments, we utilize the PopQA and EntityQ datasets, as\nwell as a Multi-Turn Conversation Response dataset (MTCR) we collected, to\nillustrate that ThoT significantly improves reasoning performance compared to\nother prompting techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yucheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianbing Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Toeplitz Neural Network with Constant-time Inference Complexity. (arXiv:2311.08756v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08756","description":"<p>Toeplitz Neural Networks (TNNs) have exhibited outstanding performance in\nvarious sequence modeling tasks. They outperform commonly used\nTransformer-based models while benefiting from log-linear space-time\ncomplexities. On the other hand, State Space Models (SSMs) achieve lower\nperformance than TNNs in language modeling but offer the advantage of constant\ninference complexity. In this paper, we aim to combine the strengths of TNNs\nand SSMs by converting TNNs to SSMs during inference, thereby enabling TNNs to\nachieve the same constant inference complexities as SSMs. To accomplish this,\nwe formulate the conversion process as an optimization problem and provide a\nclosed-form solution. We demonstrate how to transform the target equation into\na Vandermonde linear system problem, which can be efficiently solved using the\nDiscrete Fourier Transform (DFT). Notably, our method requires no training and\nmaintains numerical stability. It can be also applied to any LongConv-based\nmodel. To assess its effectiveness, we conduct extensive experiments on\nlanguage modeling tasks across various settings. Additionally, we compare our\nmethod to other gradient-descent solutions, highlighting the superior numerical\nstability of our approach. The source code is available at\nhttps://github.com/OpenNLPLab/ETSC-Exact-Toeplitz-to-SSM-Conversion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Three Conjectures on Unexpectedeness. (arXiv:2311.08768v1 [cs.AI])","link":"http://arxiv.org/abs/2311.08768","description":"<p>Unexpectedness is a central concept in Simplicity Theory, a theory of\ncognition relating various inferential processes to the computation of\nKolmogorov complexities, rather than probabilities. Its predictive power has\nbeen confirmed by several experiments with human subjects, yet its theoretical\nbasis remains largely unexplored: why does it work? This paper lays the\ngroundwork for three theoretical conjectures. First, unexpectedness can be seen\nas a generalization of Bayes' rule. Second, the frequentist core of\nunexpectedness can be connected to the function of tracking ergodic properties\nof the world. Third, unexpectedness can be seen as constituent of various\nmeasures of divergence between the entropy of the world (environment) and the\nvariety of the observer (system). The resulting framework hints to research\ndirections that go beyond the division between probabilistic and logical\napproaches, potentially bringing new insights into the extraction of causal\nrelations, and into the role of descriptive mechanisms in learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sileno_G/0/1/0/all/0/1\">Giovanni Sileno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dessalles_J/0/1/0/all/0/1\">Jean-Louis Dessalles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented Instruction Tuning with Auxiliary Evaluation Aspects. (arXiv:2311.08788v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08788","description":"<p>Natural Language Generation (NLG) typically involves evaluating the generated\ntext in various aspects (e.g., consistency and naturalness) to obtain a\ncomprehensive assessment. However, multi-aspect evaluation remains challenging\nas it may require the evaluator to generalize to any given evaluation aspect\neven if it's absent during training. In this paper, we introduce X-Eval, a\ntwo-stage instruction tuning framework to evaluate the text in both seen and\nunseen aspects customized by end users. X-Eval consists of two learning stages:\nthe vanilla instruction tuning stage that improves the model's ability to\nfollow evaluation instructions, and an enhanced instruction tuning stage that\nexploits the connections between fine-grained evaluation aspects to better\nassess text quality. To support the training of X-Eval, we collect\nAspectInstruct, the first instruction tuning dataset tailored for multi-aspect\nNLG evaluation spanning 27 diverse evaluation aspects with 65 tasks. To enhance\ntask diversity, we devise an augmentation strategy that converts human rating\nannotations into diverse forms of NLG evaluation tasks, including scoring,\ncomparison, ranking, and Boolean question answering. Extensive experiments\nacross three essential categories of NLG tasks: dialogue generation,\nsummarization, and data-to-text coupled with 21 aspects in meta-evaluation,\ndemonstrate that our X-Eval enables even a lightweight language model to\nachieve a comparable if not higher correlation with human judgments compared to\nthe state-of-the-art NLG evaluators, such as GPT-4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minqian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_E/0/1/0/all/0/1\">Eunah Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vaibhav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanadan_R/0/1/0/all/0/1\">Reza Ghanadan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"German FinBERT: A German Pre-trained Language Model. (arXiv:2311.08793v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08793","description":"<p>This study presents German FinBERT, a novel pre-trained German language model\ntailored for financial textual data. The model is trained through a\ncomprehensive pre-training process, leveraging a substantial corpus comprising\nfinancial reports, ad-hoc announcements and news related to German companies.\nThe corpus size is comparable to the data sets commonly used for training\nstandard BERT models. I evaluate the performance of German FinBERT on\ndownstream tasks, specifically sentiment prediction, topic recognition and\nquestion answering against generic German language models. My results\ndemonstrate improved performance on finance-specific data, indicating the\nefficacy of German FinBERT in capturing domain-specific nuances. The presented\nfindings suggest that German FinBERT holds promise as a valuable tool for\nfinancial text analysis, potentially benefiting various applications in the\nfinancial domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scherrmann_M/0/1/0/all/0/1\">Moritz Scherrmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving. (arXiv:2311.08803v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08803","description":"<p>Most existing chain-of-thought (CoT) prompting methods suffer from the issues\nof generalizability and consistency, as they often rely on instance-specific\nsolutions that may not be applicable to other cases and lack task-level\nconsistency in their reasoning steps. To address these limitations, we propose\na comprehensive framework, StrategyLLM, harnessing the capabilities of LLMs to\ntackle various tasks. The framework improves generalizability by formulating\ngeneral problem-solving strategies and enhances consistency by producing\nconsistent solutions using these strategies. StrategyLLM employs four LLM-based\nagents: strategy generator, executor, optimizer, and evaluator, working\ntogether to generate, evaluate, and select promising strategies for a given\ntask automatically. The experimental results demonstrate that StrategyLLM\noutperforms the competitive baseline CoT-SC that requires human-annotated\nsolutions on 13 datasets across 4 challenging tasks without human involvement,\nincluding math reasoning (39.2% $\\rightarrow$ 43.3%), commonsense reasoning\n(70.3% $\\rightarrow$ 72.5%), algorithmic reasoning (51.7% $\\rightarrow$ 62.0%),\nand symbolic reasoning (30.0% $\\rightarrow$ 79.2%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAP's not dead yet: Uncovering true language model modes by conditioning away degeneracy. (arXiv:2311.08817v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08817","description":"<p>It has been widely observed that exact or approximate MAP (mode-seeking)\ndecoding from natural language generation (NLG) models consistently leads to\ndegenerate outputs (Stahlberg and Byrne, 2019, Holtzman et al., 2019). This has\ngenerally been attributed to either a fundamental inadequacy of modes in models\nor weaknesses in language modeling. Contrastingly in this work, we emphasize\nthat degenerate modes can even occur in the absence of any model error, due to\ncontamination of the training data. Specifically, we show that mixing even a\ntiny amount of low-entropy noise with a population text distribution can cause\nthe data distribution's mode to become degenerate, implying that any models\ntrained on it will be as well. As the unconditional mode of NLG models will\noften be degenerate, we therefore propose to apply MAP decoding to the model's\ndistribution conditional on avoiding specific degeneracies. Using exact-search,\nwe empirically verify that the length-conditional modes of machine translation\nmodels and language models are indeed more fluent and topical than their\nunconditional modes. For the first time, we also share many examples of exact\nmodal sequences from these models, and from several variants of the LLaMA-7B\nmodel. Notably, the modes of the LLaMA models are still degenerate, showing\nthat improvements in modeling have not fixed this issue. Because of the cost of\nexact mode finding algorithms, we develop an approximate mode finding approach,\nACBS, which finds sequences that are both high-likelihood and high-quality. We\napply this approach to LLaMA-7B, a model which was not trained for instruction\nfollowing, and find that we are able to elicit reasonable outputs without any\nfinetuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoshida_D/0/1/0/all/0/1\">Davis Yoshida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_K/0/1/0/all/0/1\">Kartik Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimpel_K/0/1/0/all/0/1\">Kevin Gimpel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Gender Bias in the Translation of Gender-Neutral Languages into English. (arXiv:2311.08836v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08836","description":"<p>Machine Translation (MT) continues to improve in quality and adoption, yet\nthe inadvertent perpetuation of gender bias remains a significant concern.\nDespite numerous studies into gender bias in translations from gender-neutral\nlanguages such as Turkish into more strongly gendered languages like English,\nthere are no benchmarks for evaluating this phenomenon or for assessing\nmitigation strategies. To address this gap, we introduce GATE X-E, an extension\nto the GATE (Rarrick et al., 2023) corpus, that consists of human translations\nfrom Turkish, Hungarian, Finnish, and Persian into English. Each translation is\naccompanied by feminine, masculine, and neutral variants for each possible\ngender interpretation. The dataset, which contains between 1250 and 1850\ninstances for each of the four language pairs, features natural sentences with\na wide range of sentence lengths and domains, challenging translation rewriters\non various linguistic phenomena. Additionally, we present an English gender\nrewriting solution built on GPT-3.5 Turbo and use GATE X-E to evaluate it. We\nopen source our contributions to encourage further research on gender\ndebiasing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rarrick_S/0/1/0/all/0/1\">Spencer Rarrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_R/0/1/0/all/0/1\">Ranjita Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poudel_S/0/1/0/all/0/1\">Sundar Poudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhary_V/0/1/0/all/0/1\">Vishal Chowdhary</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disinformation Capabilities of Large Language Models. (arXiv:2311.08838v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08838","description":"<p>Automated disinformation generation is often listed as one of the risks of\nlarge language models (LLMs). The theoretical ability to flood the information\nspace with disinformation content might have dramatic consequences for\ndemocratic societies around the world. This paper presents a comprehensive\nstudy of the disinformation capabilities of the current generation of LLMs to\ngenerate false news articles in English language. In our study, we evaluated\nthe capabilities of 10 LLMs using 20 disinformation narratives. We evaluated\nseveral aspects of the LLMs: how well they are at generating news articles, how\nstrongly they tend to agree or disagree with the disinformation narratives, how\noften they generate safety warnings, etc. We also evaluated the abilities of\ndetection models to detect these articles as LLM-generated. We conclude that\nLLMs are able to generate convincing news articles that agree with dangerous\ndisinformation narratives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vykopal_I/0/1/0/all/0/1\">Ivan Vykopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pikuliak_M/0/1/0/all/0/1\">Mat&#xfa;&#x161; Pikuliak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srba_I/0/1/0/all/0/1\">Ivan Srba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moro_R/0/1/0/all/0/1\">Robert Moro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macko_D/0/1/0/all/0/1\">Dominik Macko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielikova_M/0/1/0/all/0/1\">Maria Bielikova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Violet: A Vision-Language Model for Arabic Image Captioning with Gemini Decoder. (arXiv:2311.08844v1 [cs.CV])","link":"http://arxiv.org/abs/2311.08844","description":"<p>Although image captioning has a vast array of applications, it has not\nreached its full potential in languages other than English. Arabic, for\ninstance, although the native language of more than 400 million people, remains\nlargely underrepresented in this area. This is due to the lack of labeled data\nand powerful Arabic generative models. We alleviate this issue by presenting a\nnovel vision-language model dedicated to Arabic, dubbed \\textit{Violet}. Our\nmodel is based on a vision encoder and a Gemini text decoder that maintains\ngeneration fluency while allowing fusion between the vision and language\ncomponents. To train our model, we introduce a new method for automatically\nacquiring data from available English datasets. We also manually prepare a new\ndataset for evaluation. \\textit{Violet} performs sizeably better than our\nbaselines on all of our evaluation datasets. For example, it reaches a CIDEr\nscore of $61.2$ on our manually annotated dataset and achieves an improvement\nof $13$ points on Flickr8k.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alwajih_F/0/1/0/all/0/1\">Fakhraddin Alwajih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagoudi_E/0/1/0/all/0/1\">El Moatez Billah Nagoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inciarte_A/0/1/0/all/0/1\">Alcides Alcoba Inciarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient Large-scale Multilingual Continued Pretraining. (arXiv:2311.08849v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08849","description":"<p>Pretraining multilingual language models from scratch requires considerable\ncomputational resources and substantial training data. Therefore, a more\nefficient method is to adapt existing pretrained language models (PLMs) to new\nlanguages via vocabulary extension and continued pretraining. However, this\nmethod usually randomly initializes the embeddings of new subwords and\nintroduces substantially more embedding parameters to the language model, thus\nweakening the efficiency. To address these issues, we propose a novel\nframework: \\textbf{O}ne \\textbf{F}or \\textbf{A}ll (\\textbf{\\textsc{Ofa}}),\nwhich wisely initializes the embeddings of unseen subwords from target\nlanguages and thus can adapt a PLM to multiple languages efficiently and\neffectively. \\textsc{Ofa} takes advantage of external well-aligned multilingual\nword embeddings and injects the alignment knowledge into the new embeddings. In\naddition, \\textsc{Ofa} applies matrix factorization and replaces the cumbersome\nembeddings with two lower-dimensional matrices, which significantly reduces the\nnumber of parameters while not sacrificing the performance. Through extensive\nexperiments, we show models initialized by \\textsc{Ofa} are efficient and\noutperform several baselines. \\textsc{Ofa} not only accelerates the convergence\nof continued pretraining, which is friendly to a limited computation budget,\nbut also improves the zero-shot crosslingual transfer on a wide range of\ndownstream tasks. We make our code and models publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yihong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Peiqin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Llamas Know What GPTs Don't Show: Surrogate Models for Confidence Estimation. (arXiv:2311.08877v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08877","description":"<p>To maintain user trust, large language models (LLMs) should signal low\nconfidence on examples where they are incorrect, instead of misleading the\nuser. The standard approach of estimating confidence is to use the softmax\nprobabilities of these models, but as of November 2023, state-of-the-art LLMs\nsuch as GPT-4 and Claude-v1.3 do not provide access to these probabilities. We\nfirst study eliciting confidence linguistically -- asking an LLM for its\nconfidence in its answer -- which performs reasonably (80.5% AUC on GPT-4\naveraged across 12 question-answering datasets -- 7% above a random baseline)\nbut leaves room for improvement. We then explore using a surrogate confidence\nmodel -- using a model where we do have probabilities to evaluate the original\nmodel's confidence in a given question. Surprisingly, even though these\nprobabilities come from a different and often weaker model, this method leads\nto higher AUC than linguistic confidences on 9 out of 12 datasets. Our best\nmethod composing linguistic confidences and surrogate model probabilities gives\nstate-of-the-art confidence estimates on all 12 datasets (84.6% average AUC on\nGPT-4).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_V/0/1/0/all/0/1\">Vaishnavi Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ananya Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enabling Large Language Models to Learn from Rules. (arXiv:2311.08883v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08883","description":"<p>Large language models (LLMs) have shown incredible performance in completing\nvarious real-world tasks. The current knowledge learning paradigm of LLMs is\nmainly based on learning from examples, in which LLMs learn the internal rule\nimplicitly from a certain number of supervised examples. However, the learning\nparadigm may not well learn those complicated rules, especially when the\ntraining examples are limited. We are inspired that humans can learn the new\ntasks or knowledge in another way by learning from rules. That is, humans can\ngrasp the new tasks or knowledge quickly and generalize well given only a\ndetailed rule and a few optional examples. Therefore, in this paper, we aim to\nexplore the feasibility of this new learning paradigm, which encodes the\nrule-based knowledge into LLMs. We propose rule distillation, which first uses\nthe strong in-context abilities of LLMs to extract the knowledge from the\ntextual rules and then explicitly encode the knowledge into LLMs' parameters by\nlearning from the above in-context signals produced inside the model. Our\nexperiments show that making LLMs learn from rules by our method is much more\nefficient than example-based learning in both the sample size and\ngeneralization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenkai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jirong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIMB: Curriculum Learning for Infant-inspired Model Building. (arXiv:2311.08886v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08886","description":"<p>We describe our team's contribution to the STRICT-SMALL track of the BabyLM\nChallenge. The challenge requires training a language model from scratch using\nonly a relatively small training dataset of ten million words. We experiment\nwith three variants of cognitively-motivated curriculum learning and analyze\ntheir effect on the performance of the model on linguistic evaluation tasks. In\nthe vocabulary curriculum, we analyze methods for constraining the vocabulary\nin the early stages of training to simulate cognitively more plausible learning\ncurves. In the data curriculum experiments, we vary the order of the training\ninstances based on i) infant-inspired expectations and ii) the learning\nbehavior of the model. In the objective curriculum, we explore different\nvariations of combining the conventional masked language modeling task with a\nmore coarse-grained word class prediction task to reinforce linguistic\ngeneralization capabilities. Our results did not yield consistent improvements\nover our own non-curriculum learning baseline across a range of linguistic\nbenchmarks; however, we do find marginal gains on select tasks. Our analysis\nhighlights key takeaways for specific combinations of tasks and settings which\nbenefit from our proposed curricula. We moreover determine that careful\nselection of model architecture, and training hyper-parameters yield\nsubstantial improvements over the default baselines provided by the BabyLM\nchallenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_R/0/1/0/all/0/1\">Richard Diehl Martinez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goriely_Z/0/1/0/all/0/1\">Zebulon Goriely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGovern_H/0/1/0/all/0/1\">Hope McGovern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_C/0/1/0/all/0/1\">Christopher Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caines_A/0/1/0/all/0/1\">Andrew Caines</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buttery_P/0/1/0/all/0/1\">Paula Buttery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beinborn_L/0/1/0/all/0/1\">Lisa Beinborn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are legal but they are not: Making the case for a powerful LegalLLM. (arXiv:2311.08890v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08890","description":"<p>Realizing the recent advances in Natural Language Processing (NLP) to the\nlegal sector poses challenging problems such as extremely long sequence\nlengths, specialized vocabulary that is usually only understood by legal\nprofessionals, and high amounts of data imbalance. The recent surge of Large\nLanguage Models (LLMs) has begun to provide new opportunities to apply NLP in\nthe legal domain due to their ability to handle lengthy, complex sequences.\nMoreover, the emergence of domain-specific LLMs has displayed extremely\npromising results on various tasks. In this study, we aim to quantify how\ngeneral LLMs perform in comparison to legal-domain models (be it an LLM or\notherwise). Specifically, we compare the zero-shot performance of three\ngeneral-purpose LLMs (ChatGPT-20b, LLaMA-2-70b, and Falcon-180b) on the LEDGAR\nsubset of the LexGLUE benchmark for contract provision classification. Although\nthe LLMs were not explicitly trained on legal data, we observe that they are\nstill able to classify the theme correctly in most cases. However, we find that\ntheir mic-F1/mac-F1 performance is up to 19.2/26.8\\% lesser than smaller models\nfine-tuned on the legal domain, thus underscoring the need for more powerful\nlegal-domain LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jayakumar_T/0/1/0/all/0/1\">Thanmay Jayakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farooqui_F/0/1/0/all/0/1\">Fauzan Farooqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farooqui_L/0/1/0/all/0/1\">Luqman Farooqui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Transfer Learning with In-context Learning using Blackbox LLMs for Zero-shot Knowledge Base Question Answering. (arXiv:2311.08894v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08894","description":"<p>We address the zero-shot transfer learning setting for the knowledge base\nquestion answering (KBQA) problem, where a large volume of labeled training\ndata is available for the source domain, but no such labeled examples are\navailable for the target domain. Transfer learning for KBQA makes use of large\nvolumes of unlabeled data in the target in addition to the labeled data in the\nsource. More recently, few-shot in-context learning using Black-box Large\nLanguage Models (BLLMs) has been adapted for KBQA without considering any\nsource domain data. In this work, we show how to meaningfully combine these two\nparadigms for KBQA so that their benefits add up. Specifically, we preserve the\ntwo stage retrieve-then-generate pipeline of supervised KBQA and introduce\ninteraction between in-context learning using BLLMs and transfer learning from\nthe source for both stages. In addition, we propose execution-guided\nself-refinement using BLLMs, decoupled from the transfer setting. With the help\nof experiments using benchmark datasets GrailQA as the source and WebQSP as the\ntarget, we show that the proposed combination brings significant improvements\nto both stages and also outperforms by a large margin state-of-the-art\nsupervised KBQA models trained on the source. We also show that in the\nin-domain setting, the proposed BLLM augmentation significantly outperforms\nstate-of-the-art supervised models, when the volume of labeled data is limited,\nand also outperforms these marginally even when using the entire large training\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patidar_M/0/1/0/all/0/1\">Mayur Patidar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Avinash Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawhney_R/0/1/0/all/0/1\">Riya Sawhney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_I/0/1/0/all/0/1\">Indrajit Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HELLaMA: LLaMA-based Table to Text Generation by Highlighting the Important Evidence. (arXiv:2311.08896v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08896","description":"<p>Large models have demonstrated significant progress across various domains,\nparticularly in tasks related to text generation. In the domain of Table to\nText, many Large Language Model (LLM)-based methods currently resort to\nmodifying prompts to invoke public APIs, incurring potential costs and\ninformation leaks. With the advent of open-source large models, fine-tuning\nLLMs has become feasible. In this study, we conducted parameter-efficient\nfine-tuning on the LLaMA2 model. Distinguishing itself from previous\nfine-tuning-based table-to-text methods, our approach involves injecting\nreasoning information into the input by emphasizing table-specific row data.\nOur model consists of two modules: 1) a table reasoner that identifies relevant\nrow evidence, and 2) a table summarizer that generates sentences based on the\nhighlighted table. To facilitate this, we propose a search strategy to\nconstruct reasoning labels for training the table reasoner. On both the FetaQA\nand QTSumm datasets, our approach achieved state-of-the-art results.\nAdditionally, we observed that highlighting input tables significantly enhances\nthe model's performance and provides valuable interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Junyi Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiaolei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1\">Wuhe Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Mengzuo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weidong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models. (arXiv:2311.08921v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08921","description":"<p>Exploring the application of powerful large language models (LLMs) on the\nfundamental named entity recognition (NER) task has drawn much attention\nrecently. This work aims to investigate the possibilities of pushing the\nboundary of zero-shot NER with LLM via a training-free self-improving strategy.\nWe propose a self-improving framework, which utilize an unlabeled corpus to\nstimulate the self-learning ability of LLMs on NER. First, we use LLM to make\npredictions on the unlabeled corpus and obtain the self-annotated data. Second,\nwe explore various strategies to select reliable samples from the\nself-annotated dataset as demonstrations, considering the similarity, diversity\nand reliability of demonstrations. Finally, we conduct inference for the test\nquery via in-context learning with the selected self-annotated demonstrations.\nThrough comprehensive experimental analysis, our study yielded the following\nfindings: (1) The self-improving framework further pushes the boundary of\nzero-shot NER with LLMs, and achieves an obvious performance improvement; (2)\nIterative self-improving or naively increasing the size of unlabeled corpus\ndoes not guarantee improvements; (3) There might still be space for improvement\nvia more advanced strategy for reliable entity selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tingyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zuozhu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning over Description Logic-based Contexts with Transformers. (arXiv:2311.08941v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08941","description":"<p>One way that the current state of the art measures the reasoning ability of\ntransformer-based models is by evaluating accuracy in downstream tasks like\nlogical question answering or proof generation over synthetic contexts\nexpressed in natural language. However, most of the contexts used are in\npractice very simple; in most cases, they are generated from short first-order\nlogic sentences with only a few logical operators and quantifiers. In this\nwork, we seek to answer the question how well a transformer-based model will\nperform reasoning over expressive contexts. For this purpose, we construct a\nsynthetic natural language question-answering dataset, generated by description\nlogic knowledge bases. For the generation of the knowledge bases, we use the\nexpressive language $\\mathcal{ALCQ}$. The resulting dataset contains 384K\nexamples, and increases in two dimensions: i) reasoning depth, and ii) length\nof sentences. We show that the performance of our DeBERTa-based model,\nDELTA$_M$, is marginally affected when the reasoning depth is increased and it\nis not affected at all when the length of the sentences is increasing. We also\nevaluate the generalization ability of the model on reasoning depths unseen at\ntraining, both increasing and decreasing, revealing interesting insights into\nthe model's adaptive generalization abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poulis_A/0/1/0/all/0/1\">Angelos Poulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsalapati_E/0/1/0/all/0/1\">Eleni Tsalapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koubarakis_M/0/1/0/all/0/1\">Manolis Koubarakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Large-scale Deep Biasing with Phoneme Features and Text-only Data in Streaming Transducer. (arXiv:2311.08966v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08966","description":"<p>Deep biasing for the Transducer can improve the recognition performance of\nrare words or contextual entities, which is essential in practical\napplications, especially for streaming Automatic Speech Recognition (ASR).\nHowever, deep biasing with large-scale rare words remains challenging, as the\nperformance drops significantly when more distractors exist and there are words\nwith similar grapheme sequences in the bias list. In this paper, we combine the\nphoneme and textual information of rare words in Transducers to distinguish\nwords with similar pronunciation or spelling. Moreover, the introduction of\ntraining with text-only data containing more rare words benefits large-scale\ndeep biasing. The experiments on the LibriSpeech corpus demonstrate that the\nproposed method achieves state-of-the-art performance on rare word error rate\nfor different scales and levels of bias lists.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jin Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Lu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zejun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Linear Relational Concepts in Large Language Models. (arXiv:2311.08968v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08968","description":"<p>Transformer language models (LMs) have been shown to represent concepts as\ndirections in the latent space of hidden activations. However, for any given\nhuman-interpretable concept, how can we find its direction in the latent space?\nWe present a technique called linear relational concepts (LRC) for finding\nconcept directions corresponding to human-interpretable concepts at a given\nhidden layer in a transformer LM by first modeling the relation between subject\nand object as a linear relational embedding (LRE). While the LRE work was\nmainly presented as an exercise in understanding model representations, we find\nthat inverting the LRE while using earlier object layers results in a powerful\ntechnique to find concept directions that both work well as a classifier and\ncausally influence model outputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chanin_D/0/1/0/all/0/1\">David Chanin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hunter_A/0/1/0/all/0/1\">Anthony Hunter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speculative Contrastive Decoding. (arXiv:2311.08981v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08981","description":"<p>Large language models (LLMs) have shown extraordinary performance in various\nlanguage tasks, but high computational requirements hinder their widespread\ndeployment. Speculative decoding, which uses amateur models to predict the\ngeneration of expert models, has been proposed as a way to accelerate LLM\ninference. However, speculative decoding focuses on acceleration instead of\nmaking the best use of the token distribution from amateur models. We proposed\nSpeculative Contrastive Decoding (SCD), an accelerated decoding method\nleveraging the natural contrast between expert and amateur models in\nspeculative decoding. Comprehensive evaluations on four benchmarks show that\nSCD can achieve similar acceleration factors as speculative decoding while\nfurther improving the generation quality as the contrastive decoding. The\nanalysis of token probabilities further demonstrates the compatibility between\nspeculative and contrastive decoding. Overall, SCD provides an effective\napproach to enhance the decoding quality of LLMs while saving computational\nresources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongyi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Keming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SentAlign: Accurate and Scalable Sentence Alignment. (arXiv:2311.08982v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08982","description":"<p>We present SentAlign, an accurate sentence alignment tool designed to handle\nvery large parallel document pairs. Given user-defined parameters, the\nalignment algorithm evaluates all possible alignment paths in fairly large\ndocuments of thousands of sentences and uses a divide-and-conquer approach to\nalign documents containing tens of thousands of sentences. The scoring function\nis based on LaBSE bilingual sentence representations. SentAlign outperforms\nfive other sentence alignment tools when evaluated on two different evaluation\nsets, German-French and English-Icelandic, and on a downstream machine\ntranslation task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Steingrimsson_S/0/1/0/all/0/1\">Stein&#xfe;&#xf3;r Steingr&#xed;msson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loftsson_H/0/1/0/all/0/1\">Hrafn Loftsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Way_A/0/1/0/all/0/1\">Andy Way</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks. (arXiv:2311.08993v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08993","description":"<p>In-context learning (ICL) has become the default method for using large\nlanguage models (LLMs), making the exploration of its limitations and\nunderstanding the underlying causes crucial. In this paper, we find that ICL\nfalls short of handling specification-heavy tasks, which are tasks with\ncomplicated and extensive task specifications, requiring several hours for\nordinary humans to master, such as traditional information extraction tasks.\nThe performance of ICL on these tasks mostly cannot reach half of the\nstate-of-the-art results. To explore the reasons behind this failure, we\nconduct comprehensive experiments on 18 specification-heavy tasks with various\nLLMs and identify three primary reasons: inability to specifically understand\ncontext, misalignment in task schema comprehension with humans, and inadequate\nlong-text understanding ability. Furthermore, we demonstrate that through\nfine-tuning, LLMs can achieve decent performance on these tasks, indicating\nthat the failure of ICL is not an inherent flaw of LLMs, but rather a drawback\nof existing alignment methods that renders LLMs incapable of handling\ncomplicated specification-heavy tasks via ICL. To substantiate this, we perform\ndedicated instruction tuning on LLMs for these tasks and observe a notable\nimprovement. We hope the analyses in this paper could facilitate advancements\nin alignment methods enabling LLMs to meet more sophisticated human demands.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianhui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weikai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yunjia Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zimu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhili Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_K/0/1/0/all/0/1\">Kaisheng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output. (arXiv:2311.09000v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09000","description":"<p>The increased use of large language models (LLMs) across a variety of\nreal-world applications calls for mechanisms to verify the factual accuracy of\ntheir outputs. In this work, we present a holistic end-to-end solution for\nannotating the factuality of LLM-generated responses, which encompasses a\nmulti-stage annotation scheme designed to yield detailed labels concerning the\nverifiability and factual inconsistencies found in LLM outputs. We design and\nbuild an annotation tool to speed up the labelling procedure and ease the\nworkload of raters. It allows flexible incorporation of automatic results in\nany stage, e.g. automatically-retrieved evidence. We further construct an\nopen-domain document-level factuality benchmark in three-level granularity:\nclaim, sentence and document. Preliminary experiments show that FacTool,\nFactScore and Perplexity.ai are struggling to identify false claims with the\nbest F1=0.53. Annotation tool, benchmark and code are available at\nhttps://github.com/yuxiaw/Factcheck-GPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1\">Revanth Gangi Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mujahid_Z/0/1/0/all/0/1\">Zain Muhammad Mujahid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Arnav Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubashevskii_A/0/1/0/all/0/1\">Aleksandr Rubashevskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_J/0/1/0/all/0/1\">Jiahui Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afzal_O/0/1/0/all/0/1\">Osama Mohammed Afzal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liangming Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borenstein_N/0/1/0/all/0/1\">Nadav Borenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pillai_A/0/1/0/all/0/1\">Aditya Pillai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Similarity is Not Enough to Explain Language Model Performance. (arXiv:2311.09006v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09006","description":"<p>Large language models achieve high performance on many but not all downstream\ntasks. The interaction between pretraining data and task data is commonly\nassumed to determine this variance: a task with data that is more similar to a\nmodel's pretraining data is assumed to be easier for that model. We test\nwhether distributional and example-specific similarity measures (embedding-,\ntoken- and model-based) correlate with language model performance through a\nlarge-scale comparison of the Pile and C4 pretraining datasets with downstream\nbenchmarks. Similarity correlates with performance for multilingual datasets,\nbut in other benchmarks, we surprisingly find that similarity metrics are not\ncorrelated with accuracy or even each other. This suggests that the\nrelationship between pretraining data and downstream tasks is more complex than\noften assumed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yauney_G/0/1/0/all/0/1\">Gregory Yauney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1\">Emily Reif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mimno_D/0/1/0/all/0/1\">David Mimno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Task-oriented Dialogue: A Survey of Tasks, Methods, and Future Directions. (arXiv:2311.09008v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09008","description":"<p>End-to-end task-oriented dialogue (EToD) can directly generate responses in\nan end-to-end fashion without modular training, which attracts escalating\npopularity. The advancement of deep neural networks, especially the successful\nuse of large pre-trained models, has further led to significant progress in\nEToD research in recent years. In this paper, we present a thorough review and\nprovide a unified perspective to summarize existing approaches as well as\nrecent trends to advance the development of EToD research. The contributions of\nthis paper can be summarized: (1) \\textbf{\\textit{First survey}}: to our\nknowledge, we take the first step to present a thorough survey of this research\nfield; (2) \\textbf{\\textit{New taxonomy}}: we first introduce a unified\nperspective for EToD, including (i) \\textit{Modularly EToD} and (ii)\n\\textit{Fully EToD}; (3) \\textbf{\\textit{New Frontiers}}: we discuss some\npotential frontier areas as well as the corresponding challenges, hoping to\nspur breakthrough research in EToD field; (4) \\textbf{\\textit{Abundant\nresources}}: we build a public website\\footnote{We collect the related papers,\nbaseline projects, and leaderboards for the community at\n\\url{https://etods.net/}.}, where EToD researchers could directly access the\nrecent progress. We hope this work can serve as a thorough reference for the\nEToD research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Wenbo Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiguang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Lizi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Min Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Potential of Large Language Models in Computational Argumentation. (arXiv:2311.09022v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09022","description":"<p>Computational argumentation has become an essential tool in various fields,\nincluding artificial intelligence, law, and public policy. It is an emerging\nresearch field in natural language processing (NLP) that attracts increasing\nattention. Research on computational argumentation mainly involves two types of\ntasks: argument mining and argument generation. As large language models (LLMs)\nhave demonstrated strong abilities in understanding context and generating\nnatural language, it is worthwhile to evaluate the performance of LLMs on\nvarious computational argumentation tasks. This work aims to embark on an\nassessment of LLMs, such as ChatGPT, Flan models and LLaMA2 models, under\nzero-shot and few-shot settings within the realm of computational\nargumentation. We organize existing tasks into 6 main classes and standardise\nthe format of 14 open-sourced datasets. In addition, we present a new benchmark\ndataset on counter speech generation, that aims to holistically evaluate the\nend-to-end performance of LLMs on argument mining and argument generation.\nExtensive experiments show that LLMs exhibit commendable performance across\nmost of these datasets, demonstrating their capabilities in the field of\nargumentation. We also highlight the limitations in evaluating computational\nargumentation and provide suggestions for future research directions in this\nfield.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guizhen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1\">Luu Anh Tuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MELA: Multilingual Evaluation of Linguistic Acceptability. (arXiv:2311.09033v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09033","description":"<p>Recent benchmarks for Large Language Models (LLMs) have mostly focused on\napplication-driven tasks such as complex reasoning and code generation, and\nthis has led to a scarcity in purely linguistic evaluation of LLMs. Against\nthis background, we introduce Multilingual Evaluation of Linguistic\nAcceptability -- MELA, the first multilingual benchmark on linguistic\nacceptability with 48K samples covering 10 languages from a diverse set of\nlanguage families. We establish baselines of commonly used LLMs along with\nsupervised models, and conduct cross-lingual transfer and multi-task learning\nexperiments with XLM-R. In pursuit of multilingual interpretability, we analyze\nthe weights of fine-tuned XLM-R to explore the possibility of identifying\ntransfer difficulty between languages. Our results show that ChatGPT benefits\nmuch from in-context examples but still lags behind fine-tuned XLM-R, while the\nperformance of GPT-4 is on par with fine-tuned XLM-R even in zero-shot setting.\nCross-lingual and multi-task learning experiments show that unlike semantic\ntasks, in-language training data is crucial in acceptability judgements.\nResults in layerwise probing indicate that the upper layers of XLM-R become a\ntask-specific but language-agnostic region for multilingual acceptability\njudgment. We also introduce the concept of conflicting weight, which could be a\npotential indicator for the difficulty of cross-lingual transfer between\nlanguages. Our data will be available at https://github.com/sjtu-compling/MELA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziyin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yikang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Weifang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Junyu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hai Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models. (arXiv:2311.09048v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09048","description":"<p>This paper presents GRASP, a novel benchmark to evaluate the language\ngrounding and physical understanding capabilities of video-based multimodal\nlarge language models (LLMs). This evaluation is accomplished via a two-tier\napproach leveraging Unity simulations. The initial level tests for language\ngrounding by assessing a model's ability to relate simple textual descriptions\nwith visual information. The second level evaluates the model's understanding\nof 'Intuitive Physics' principles, such as object permanence and continuity. In\naddition to releasing the benchmark, we use it to evaluate several\nstate-of-the-art multimodal LLMs. Our evaluation reveals significant\nshortcomings in current models' language grounding and intuitive physics. These\nidentified limitations underline the importance of benchmarks like GRASP to\nmonitor the progress of future models in developing these competencies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jassim_S/0/1/0/all/0/1\">Serwan Jassim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holubar_M/0/1/0/all/0/1\">Mario Holubar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richter_A/0/1/0/all/0/1\">Annika Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolff_C/0/1/0/all/0/1\">Cornelius Wolff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohmer_X/0/1/0/all/0/1\">Xenia Ohmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruni_E/0/1/0/all/0/1\">Elia Bruni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Knowledge Editing in Language Models via Relation Perspective. (arXiv:2311.09053v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09053","description":"<p>Knowledge Editing (KE) for modifying factual knowledge in Large Language\nModels (LLMs) has been receiving increasing attention. However, existing\nknowledge editing methods are entity-centric, and it is unclear whether this\napproach is suitable for a relation-centric perspective. To address this gap,\nthis paper constructs a new benchmark named RaKE, which focuses on Relation\nbased Knowledge Editing. In this paper, we establish a suite of innovative\nmetrics for evaluation and conduct comprehensive experiments involving various\nknowledge editing baselines. We notice that existing knowledge editing methods\nexhibit the potential difficulty in their ability to edit relations. Therefore,\nwe further explore the role of relations in factual triplets within the\ntransformer. Our research results confirm that knowledge related to relations\nis not only stored in the FFN network but also in the attention layers. This\nprovides experimental support for future relation-based knowledge editing\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yifan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaoyan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Huanhuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_F/0/1/0/all/0/1\">Fangyu Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Ran Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Localization Methods Actually Localize Memorized Data in LLMs?. (arXiv:2311.09060v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09060","description":"<p>Large language models (LLMs) can memorize many pretrained sequences verbatim.\nThis paper studies if we can locate a small set of neurons in LLMs responsible\nfor memorizing a given sequence. While the concept of localization is often\nmentioned in prior work, methods for localization have never been\nsystematically and directly evaluated; we address this with two benchmarking\napproaches. In our INJ Benchmark, we actively inject a piece of new information\ninto a small subset of LLM weights and measure whether localization methods can\nidentify these \"ground truth\" weights. In the DEL Benchmark, we study\nlocalization of pretrained data that LLMs have already memorized; while this\nsetting lacks ground truth, we can still evaluate localization by measuring\nwhether dropping out located neurons erases a memorized sequence from the\nmodel. We evaluate five localization methods on our two benchmarks, and both\nshow similar rankings. All methods exhibit promising localization ability,\nespecially for pruning-based methods, though the neurons they identify are not\nnecessarily specific to a single memorized sequence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Ting-Yun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Self-Disclosures of Use, Misuse and Addiction in Community-based Social Media Posts. (arXiv:2311.09066v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09066","description":"<p>In the last decade, the United States has lost more than 500,000 people from\nan overdose involving prescription and illicit opioids\n(https://www.cdc.gov/drugoverdose/epidemic/index.html) making it a national\npublic health emergency (USDHHS, 2017). To more effectively prevent\nunintentional opioid overdoses, medical practitioners require robust and timely\ntools that can effectively identify at-risk patients. Community-based social\nmedia platforms such as Reddit allow self-disclosure for users to discuss\notherwise sensitive drug-related behaviors, often acting as indicators for\nopioid use disorder. Towards this, we present a moderate size corpus of 2500\nopioid-related posts from various subreddits spanning 6 different phases of\nopioid use: Medical Use, Misuse, Addiction, Recovery, Relapse, Not Using. For\nevery post, we annotate span-level extractive explanations and crucially study\ntheir role both in annotation quality and model development. We evaluate\nseveral state-of-the-art models in a supervised, few-shot, or zero-shot\nsetting. Experimental results and error analysis show that identifying the\nphases of opioid use disorder is highly contextual and challenging. However, we\nfind that using explanations during modeling leads to a significant boost in\nclassification accuracy demonstrating their beneficial role in a high-stakes\ndomain such as studying the opioid use disorder continuum. The dataset will be\nmade available for research on Github in the formal version.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenghao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hochstatter_K/0/1/0/all/0/1\">Karli R Hochstatter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slavin_M/0/1/0/all/0/1\">Melissa N Slavin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Bassel_N/0/1/0/all/0/1\">Nabila El-Bassel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Well Do Large Language Models Truly Ground?. (arXiv:2311.09069v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09069","description":"<p>Reliance on the inherent knowledge of Large Language Models (LLMs) can cause\nissues such as hallucinations, lack of control, and difficulties in integrating\nvariable knowledge. To mitigate this, LLMs can be probed to generate responses\nby grounding on external context, often given as input (knowledge-augmented\nmodels). Yet, previous research is often confined to a narrow view of the term\n\"grounding\", often only focusing on whether the response contains the correct\nanswer or not, which does not ensure the reliability of the entire response. To\naddress this limitation, we introduce a strict definition of grounding: a model\nis considered truly grounded when its responses (1) fully utilize necessary\nknowledge from the provided context, and (2) don't exceed the knowledge within\nthe contexts. We introduce a new dataset and a grounding metric to assess this\nnew definition and perform experiments across 13 LLMs of different sizes and\ntraining methods to provide insights into the factors that influence grounding\nperformance. Our findings contribute to a better understanding of how to\nimprove grounding capabilities and suggest an area of improvement toward more\nreliable and controllable LLM applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyunji Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_S/0/1/0/all/0/1\">Sejune Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chaeeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Joel Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+On_K/0/1/0/all/0/1\">Kyoung-Woon On</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Multilingual is Multilingual LLM?. (arXiv:2311.09071v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09071","description":"<p>Large Language Models (LLMs), trained predominantly on extensive English\ndata, often exhibit limitations when applied to other languages. Current\nresearch is primarily focused on enhancing the multilingual capabilities of\nthese models by employing various tuning strategies. Despite their\neffectiveness in certain languages, the understanding of the multilingual\nabilities of LLMs remains incomplete. This study endeavors to evaluate the\nmultilingual capacity of LLMs by conducting an exhaustive analysis across 101\nlanguages, and classifies languages with similar characteristics into four\ndistinct quadrants. By delving into each quadrant, we shed light on the\nrationale behind their categorization and offer actionable guidelines for\ntuning these languages. Extensive experiments reveal that existing LLMs possess\nmultilingual capabilities that surpass our expectations, and we can\nsignificantly improve the multilingual performance of LLMs by focusing on these\ndistinct attributes present in each quadrant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1\">Fei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Shuai Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Uli Dataset: An Exercise in Experience Led Annotation of oGBV. (arXiv:2311.09086v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09086","description":"<p>Online gender based violence has grown concomitantly with adoption of the\ninternet and social media. Its effects are worse in the Global majority where\nmany users use social media in languages other than English. The scale and\nvolume of conversations on the internet has necessitated the need for automated\ndetection of hate speech, and more specifically gendered abuse. There is,\nhowever, a lack of language specific and contextual data to build such\nautomated tools. In this paper we present a dataset on gendered abuse in three\nlanguages- Hindi, Tamil and Indian English. The dataset comprises of tweets\nannotated along three questions pertaining to the experience of gender abuse,\nby experts who identify as women or a member of the LGBTQIA community in South\nAsia. Through this dataset we demonstrate a participatory approach to creating\ndatasets that drive AI systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Arnav Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jinadoss_M/0/1/0/all/0/1\">Maha Jinadoss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1\">Cheshta Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+George_D/0/1/0/all/0/1\">Denny George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brindaalakshmi/0/1/0/all/0/1\">Brindaalakshmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_H/0/1/0/all/0/1\">Haseena Dawood Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_K/0/1/0/all/0/1\">Kirti Rawat</a>, Div, <a href=\"http://arxiv.org/find/cs/1/au:+Ritash/0/1/0/all/0/1\">Ritash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_S/0/1/0/all/0/1\">Seema Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_S/0/1/0/all/0/1\">Shivani Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shora_S/0/1/0/all/0/1\">Shehla Rashid Shora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raut_R/0/1/0/all/0/1\">Rie Raut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pawar_S/0/1/0/all/0/1\">Sumit Pawar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paithane_A/0/1/0/all/0/1\">Apurva Paithane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonia/0/1/0/all/0/1\">Sonia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vivek/0/1/0/all/0/1\">Vivek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priscilla_D/0/1/0/all/0/1\">Dharini Priscilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khairunnisha/0/1/0/all/0/1\">Khairunnisha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banu_G/0/1/0/all/0/1\">Grace Banu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_A/0/1/0/all/0/1\">Ambika Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakker_R/0/1/0/all/0/1\">Rishav Thakker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korra_R/0/1/0/all/0/1\">Rahul Dev Korra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaidya_A/0/1/0/all/0/1\">Aatman Vaidya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakar_T/0/1/0/all/0/1\">Tarunima Prabhakar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Social Bias Probing: Fairness Benchmarking for Language Models. (arXiv:2311.09090v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09090","description":"<p>Large language models have been shown to encode a variety of social biases,\nwhich carries the risk of downstream harms. While the impact of these biases\nhas been recognized, prior methods for bias evaluation have been limited to\nbinary association tests on small datasets, offering a constrained view of the\nnature of societal biases within language models. In this paper, we propose an\noriginal framework for probing language models for societal biases. We collect\na probing dataset to analyze language models' general associations, as well as\nalong the axes of societal categories, identities, and stereotypes. To this\nend, we leverage a novel perplexity-based fairness score. We curate a\nlarge-scale benchmarking dataset addressing drawbacks and limitations of\nexisting fairness collections, expanding to a variety of different identities\nand stereotypes. When comparing our methodology with prior work, we demonstrate\nthat biases within language models are more nuanced than previously\nacknowledged. In agreement with recent findings, we find that larger model\nvariants exhibit a higher degree of bias. Moreover, we expose how identities\nexpressing different religions lead to the most pronounced disparate treatments\nacross all models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manerba_M/0/1/0/all/0/1\">Marta Marchiori Manerba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanczak_K/0/1/0/all/0/1\">Karolina Sta&#x144;czak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guidotti_R/0/1/0/all/0/1\">Riccardo Guidotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization. (arXiv:2311.09096v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09096","description":"<p>Large Language Models (LLMs) continue to advance in their capabilities, yet\nthis progress is accompanied by a growing array of safety risks. While\nsignificant attention has been dedicated to exploiting weaknesses in LLMs\nthrough jailbreaking attacks, there remains a paucity of exploration into\ndefending against these attacks. We point out a pivotal factor contributing to\nthe success of jailbreaks: the inherent conflict between the goals of being\nhelpful and ensuring safety. To counter jailbreaking attacks, we propose to\nintegrate goal prioritization at both training and inference stages.\nImplementing goal prioritization during inference substantially diminishes the\nAttack Success Rate (ASR) of jailbreaking attacks, reducing it from 66.4% to\n2.0% for ChatGPT and from 68.2% to 19.4% for Vicuna-33B, without compromising\ngeneral performance. Furthermore, integrating the concept of goal\nprioritization into the training phase reduces the ASR from 71.0% to 6.6% for\nLLama2-13B. Remarkably, even in scenarios where no jailbreaking samples are\nincluded during training, our approach slashes the ASR by half, decreasing it\nfrom 71.0% to 34.0%. Additionally, our findings reveal that while stronger LLMs\nface greater safety risks, they also possess a greater capacity to be steered\ntowards defending against such attacks. We hope our work could contribute to\nthe comprehension of jailbreaking attacks and defenses, and shed light on the\nrelationship between LLMs' capability and safety. Our code will be available at\n\\url{https://github.com/thu-coai/JailbreakDefense_GoalPriority}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhexin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Junxiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1\">Pei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards A Unified View of Answer Calibration for Multi-Step Reasoning. (arXiv:2311.09101v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09101","description":"<p>Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have\nbroadened the scope for improving multi-step reasoning capabilities. Usually,\nanswer calibration strategies such as step-level or path-level calibration play\na vital role in multi-step reasoning. While effective, there remains a\nsignificant gap in our understanding of the key factors that drive their\nsuccess. In this paper, we break down the design of recent answer calibration\nstrategies and present a unified view which establishes connections between\nthem. We then conduct a thorough evaluation on these strategies from a unified\nview, systematically scrutinizing step-level and path-level answer calibration\nacross multiple paths. Our study holds the potential to illuminate key insights\nfor optimizing multi-step reasoning with answer calibration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oo_N/0/1/0/all/0/1\">Nay Oo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation. (arXiv:2311.09105v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09105","description":"<p>Understanding events in texts is a core objective of natural language\nunderstanding, which requires detecting event occurrences, extracting event\narguments, and analyzing inter-event relationships. However, due to the\nannotation challenges brought by task complexity, a large-scale dataset\ncovering the full process of event understanding has long been absent. In this\npaper, we introduce MAVEN-Arg, which augments MAVEN datasets with event\nargument annotations, making the first all-in-one dataset supporting event\ndetection, event argument extraction (EAE), and event relation extraction. As\nan EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive\nschema covering 162 event types and 612 argument roles, all with expert-written\ndefinitions and examples; (2) a large data scale, containing 98,591 events and\n290,613 arguments obtained with laborious human annotation; (3) the exhaustive\nannotation supporting all task variants of EAE, which annotates both entity and\nnon-entity event arguments in document level. Experiments indicate that\nMAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary\nlarge language models (LLMs). Furthermore, to demonstrate the benefits of an\nall-in-one dataset, we preliminarily explore a potential application, future\nevent prediction, with LLMs. MAVEN-Arg and our code can be obtained from\nhttps://github.com/THU-KEG/MAVEN-Argument.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yong Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_K/0/1/0/all/0/1\">Kaisheng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianhui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Ruobing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"We Demand Justice!\": Towards Grounding Political Text in Social Context. (arXiv:2311.09106v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09106","description":"<p>Social media discourse from US politicians frequently consists of 'seemingly\nsimilar language used by opposing sides of the political spectrum'. But often,\nit translates to starkly contrasting real-world actions. For instance, \"We need\nto keep our students safe from mass shootings\" may signal either \"arming\nteachers to stop the shooter\" or \"banning guns to reduce mass shootings\"\ndepending on who says it and their political stance on the issue. In this\npaper, we define and characterize the context that is required to fully\nunderstand such ambiguous statements in a computational setting and ground them\nin real-world entities, actions, and attitudes. To that end, we propose two\nchallenging datasets that require an understanding of the real-world context of\nthe text to be solved effectively. We benchmark these datasets against\nbaselines built upon large pre-trained models such as BERT, RoBERTa, GPT-3,\netc. Additionally, we develop and benchmark more structured baselines building\nupon existing 'Discourse Contextualization Framework' and 'Political Actor\nRepresentation' models. We perform analysis of the datasets and baseline\npredictions to obtain further insights into the pragmatic language\nunderstanding challenges posed by the proposed social grounding tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pujari_R/0/1/0/all/0/1\">Rajkumar Pujari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chengfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge Graph Completion?. (arXiv:2311.09109v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09109","description":"<p>Knowledge graphs (KGs) consist of links that describe relationships between\nentities. Due to the difficulty of manually enumerating all relationships\nbetween entities, automatically completing them is essential for KGs. Knowledge\nGraph Completion (KGC) is a task that infers unseen relationships between\nentities in a KG. Traditional embedding-based KGC methods, such as RESCAL,\nTransE, DistMult, ComplEx, RotatE, HAKE, HousE, etc., infer missing links using\nonly the knowledge from training data. In contrast, the recent Pre-trained\nLanguage Model (PLM)-based KGC utilizes knowledge obtained during pre-training.\nTherefore, PLM-based KGC can estimate missing links between entities by reusing\nmemorized knowledge from pre-training without inference. This approach is\nproblematic because building KGC models aims to infer unseen links between\nentities. However, conventional evaluations in KGC do not consider inference\nand memorization abilities separately. Thus, a PLM-based KGC method, which\nachieves high performance in current KGC evaluations, may be ineffective in\npractical applications. To address this issue, we analyze whether PLM-based KGC\nmethods make inferences or merely access memorized knowledge. For this purpose,\nwe propose a method for constructing synthetic datasets specified in this\nanalysis and conclude that PLMs acquire the inference abilities required for\nKGC through pre-training, even though the performance improvements mostly come\nfrom textual information of entities and relations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sakai_Y/0/1/0/all/0/1\">Yusuke Sakai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamigaito_H/0/1/0/all/0/1\">Hidetaka Kamigaito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_K/0/1/0/all/0/1\">Katsuhiko Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1\">Taro Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification. (arXiv:2311.09114v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09114","description":"<p>Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating fluent text. However, they often encounter the challenge of\ngenerating inaccurate or hallucinated content. This issue is common in both\nnon-retrieval-based generation and retrieval-augmented generation approaches,\nand existing post-hoc rectification methods may not address the accumulated\nhallucination errors that may be caused by the \"snowballing\" issue, especially\nin reasoning tasks. To tackle these challenges, we introduce a novel approach\ncalled Real-time Verification and Rectification (Ever). Instead of waiting\nuntil the end of the generation process to rectify hallucinations, Ever employs\na real-time, step-wise generation and hallucination rectification strategy. The\nprimary objective is to detect and rectify hallucinations as they occur during\nthe text generation process. When compared to both retrieval-based and\nnon-retrieval-based baselines, Ever demonstrates a significant improvement in\ngenerating trustworthy and factually accurate text across a diverse range of\ntasks, including short-form QA, biography generation, and multi-hop reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Haoqiang Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Juntong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Huaxiu Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces. (arXiv:2311.09117v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09117","description":"<p>This paper introduces Robust Spin (R-Spin), a data-efficient self-supervised\nfine-tuning framework for speaker and noise-invariant speech representations by\nlearning discrete acoustic units with speaker-invariant clustering (Spin).\nR-Spin resolves Spin's issues and enhances content representations by learning\nto predict acoustic pieces. R-Spin offers a 12X reduction in computational\nresources compared to previous state-of-the-art methods while outperforming\nthem in severely distorted speech scenarios. This paper provides detailed\nanalyses to show how discrete units contribute to speech encoder training and\nimproving robustness in diverse acoustic environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal NER: A Gold-Standard Multilingual Named Entity Recognition Benchmark. (arXiv:2311.09122v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09122","description":"<p>We introduce Universal NER (UNER), an open, community-driven project to\ndevelop gold-standard NER benchmarks in many languages. The overarching goal of\nUNER is to provide high-quality, cross-lingually consistent annotations to\nfacilitate and standardize multilingual NER research. UNER v1 contains 18\ndatasets annotated with named entities in a cross-lingual consistent schema\nacross 12 diverse languages. In this paper, we detail the dataset creation and\ncomposition of UNER; we also provide initial modeling baselines on both\nin-language and cross-lingual learning settings. We release the data, code, and\nfitted models to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mayhew_S/0/1/0/all/0/1\">Stephen Mayhew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blevins_T/0/1/0/all/0/1\">Terra Blevins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suppa_M/0/1/0/all/0/1\">Marek &#x160;uppa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonen_H/0/1/0/all/0/1\">Hila Gonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1\">Joseph Marvin Imperial</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_B/0/1/0/all/0/1\">B&#xf6;rje F. Karlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Peiqin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ljubesic_N/0/1/0/all/0/1\">Nikola Ljube&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miranda_L/0/1/0/all/0/1\">LJ Miranda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riabi_A/0/1/0/all/0/1\">Arij Riabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1\">Yuval Pinter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Social Meme-ing: Measuring Linguistic Variation in Memes. (arXiv:2311.09130v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09130","description":"<p>Much work in the space of NLP has used computational methods to explore\nsociolinguistic variation in text. In this paper, we argue that memes, as\nmultimodal forms of language comprised of visual templates and text, also\nexhibit meaningful social variation. We construct a computational pipeline to\ncluster individual instances of memes into templates and semantic variables,\ntaking advantage of their multimodal structure in doing so. We apply this\nmethod to a large collection of meme images from Reddit and make available the\nresulting \\textsc{SemanticMemes} dataset of 3.8M images clustered by their\nsemantic function. We use these clusters to analyze linguistic variation in\nmemes, discovering not only that socially meaningful variation in meme usage\nexists between subreddits, but that patterns of meme innovation and\nacculturation within these communities align with previous findings on written\nlanguage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_N/0/1/0/all/0/1\">Naitian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamman_D/0/1/0/all/0/1\">David Bamman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aligning Neural Machine Translation Models: Human Feedback in Training and Inference. (arXiv:2311.09132v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09132","description":"<p>Reinforcement learning from human feedback (RLHF) is a recent technique to\nimprove the quality of the text generated by a language model, making it closer\nto what humans would generate. A core ingredient in RLHF's success in aligning\nand improving large language models (LLMs) is its reward model, trained using\nhuman feedback on model outputs. In machine translation (MT), where metrics\ntrained from human annotations can readily be used as reward models, recent\nmethods using minimum Bayes risk decoding and reranking have succeeded in\nimproving the final quality of translation. In this study, we comprehensively\nexplore and compare techniques for integrating quality metrics as reward models\ninto the MT pipeline. This includes using the reward model for data filtering,\nduring the training phase through RL, and at inference time by employing\nreranking techniques, and we assess the effects of combining these in a unified\napproach. Our experimental results, conducted across multiple translation\ntasks, underscore the crucial role of effective data filtering, based on\nestimated quality, in harnessing the full potential of RL in enhancing MT\nquality. Furthermore, our findings demonstrate the effectiveness of combining\nRL training with reranking techniques, showcasing substantial improvements in\ntranslation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramos_M/0/1/0/all/0/1\">Miguel Moura Ramos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1\">Patrick Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farinhas_A/0/1/0/all/0/1\">Ant&#xf3;nio Farinhas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RRescue: Ranking LLM Responses to Enhance Reasoning Over Context. (arXiv:2311.09136v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09136","description":"<p>Effectively using a given context is paramount for large language models. A\ncontext window can include task specifications, retrieved documents, previous\nconversations, and even model self-reflections, functioning similarly to\nepisodic memory. While efforts are being made to expand the context window,\nstudies indicate that LLMs do not use their context optimally for response\ngeneration. In this paper, we present a novel approach to optimize LLMs using\nranking metrics, which teaches LLMs to rank a collection of\ncontextually-grounded candidate responses. Rather than a traditional full\nordering, we advocate for a partial ordering. This is because achieving\nconsensus on the perfect order for system responses can be challenging. Our\npartial ordering is more robust, less sensitive to noise, and can be acquired\nthrough human labelers, heuristic functions, or model distillation. We test our\nsystem's improved contextual understanding using the latest benchmarks,\nincluding a new multi-document question answering dataset. We conduct ablation\nstudies to understand crucial factors, such as how to gather candidate\nresponses, determine their most suitable order, and balance supervised\nfine-tuning with ranking metrics. Our approach, named RRescue, suggests a\npromising avenue for enhancing LLMs' contextual understanding via response\nranking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yikun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Rui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding or Guesswork? Large Language Models are Presumptive Grounders. (arXiv:2311.09144v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09144","description":"<p>Effective conversation requires common ground: a shared understanding between\nthe participants. Common ground, however, does not emerge spontaneously in\nconversation. Speakers and listeners work together to both identify and\nconstruct a shared basis while avoiding misunderstanding. To accomplish\ngrounding, humans rely on a range of dialogue acts, like clarification (What do\nyou mean?) and acknowledgment (I understand.). In domains like teaching and\nemotional support, carefully constructing grounding prevents misunderstanding.\nHowever, it is unclear whether large language models (LLMs) leverage these\ndialogue acts in constructing common ground. To this end, we curate a set of\ngrounding acts and propose corresponding metrics that quantify attempted\ngrounding. We study whether LLMs use these grounding acts, simulating them\ntaking turns from several dialogue datasets, and comparing the results to\nhumans. We find that current LLMs are presumptive grounders, biased towards\nassuming common ground without using grounding acts. To understand the roots of\nthis behavior, we examine the role of instruction tuning and reinforcement\nlearning with human feedback (RLHF), finding that RLHF leads to less grounding.\nAltogether, our work highlights the need for more research investigating\ngrounding in human-AI interaction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_O/0/1/0/all/0/1\">Omar Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gligoric_K/0/1/0/all/0/1\">Kristina Gligori&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khetan_A/0/1/0/all/0/1\">Ashna Khetan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerstgrasser_M/0/1/0/all/0/1\">Matthias Gerstgrasser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Knowledge Question Answering via Abstract Reasoning Induction. (arXiv:2311.09149v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09149","description":"<p>In this paper, we tackle the significant challenge of temporal knowledge\nreasoning in Large Language Models (LLMs), an area where such models frequently\nencounter difficulties. These difficulties often result in the generation of\nmisleading or incorrect information, primarily due to their limited capacity to\nprocess evolving factual knowledge and complex temporal logic. In response, we\npropose a novel, constructivism-based approach that advocates for a paradigm\nshift in LLM learning towards an active, ongoing process of knowledge synthesis\nand customization. At the heart of our proposal is the Abstract Reasoning\nInduction ARI framework, which divides temporal reasoning into two distinct\nphases: Knowledge-agnostic and Knowledge-based. This division aims to reduce\ninstances of hallucinations and improve LLMs' capacity for integrating abstract\nmethodologies derived from historical data. Our approach achieves remarkable\nimprovements, with relative gains of 29.7\\% and 9.27\\% on two temporal QA\ndatasets, underscoring its efficacy in advancing temporal reasoning in LLMs.\nThe code will be released at https://github.com/czy1999/ARI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongfang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Baotian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models. (arXiv:2311.09154v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09154","description":"<p>We are currently in an era of fierce competition among various large language\nmodels (LLMs) continuously pushing the boundaries of benchmark performance.\nHowever, genuinely assessing the capabilities of these LLMs has become a\nchallenging and critical issue due to potential data contamination, and it\nwastes dozens of time and effort for researchers and engineers to download and\ntry those contaminated models. To save our precious time, we propose a novel\nand useful method, Clean-Eval, which mitigates the issue of data contamination\nand evaluates the LLMs in a cleaner manner. Clean-Eval employs an LLM to\nparaphrase and back-translate the contaminated data into a candidate set,\ngenerating expressions with the same meaning but in different surface forms. A\nsemantic detector is then used to filter the generated low-quality samples to\nnarrow down this candidate set. The best candidate is finally selected from\nthis set based on the BLEURT score. According to human assessment, this best\ncandidate is semantically similar to the original contamination data but\nexpressed differently. All candidates can form a new benchmark to evaluate the\nmodel. Our experiments illustrate that Clean-Eval substantially restores the\nactual evaluation results on contaminated LLMs under both few-shot learning and\nfine-tuning scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenhong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_H/0/1/0/all/0/1\">Hongkun Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhiwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yunze Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yumeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hanxu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yiran Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongyuan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph. (arXiv:2311.09174v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09174","description":"<p>Cognitive research indicates that abstraction ability is essential in human\nintelligence, which remains under-explored in language models. In this paper,\nwe present AbsPyramid, a unified entailment graph of 221K textual descriptions\nof abstraction knowledge. While existing resources only touch nouns or verbs\nwithin simplified events or specific domains, AbsPyramid collects abstract\nknowledge for three components of diverse events to comprehensively evaluate\nthe abstraction ability of language models in the open domain. Experimental\nresults demonstrate that current LLMs face challenges comprehending abstraction\nknowledge in zero-shot and few-shot settings. By training on our rich\nabstraction knowledge, we find LLMs can acquire basic abstraction abilities and\ngeneralize to unseen events. In the meantime, we empirically show that our\nbenchmark is comprehensive to enhance LLMs across two previous abstraction\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haochen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1\">Tianqing Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Sehyun Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SiRA: Sparse Mixture of Low Rank Adaptation. (arXiv:2311.09179v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09179","description":"<p>Parameter Efficient Tuning has been an prominent approach to adapt the Large\nLanguage Model to downstream tasks. Most previous works considers adding the\ndense trainable parameters, where all parameters are used to adapt certain\ntask. We found this less effective empirically using the example of LoRA that\nintroducing more trainable parameters does not help. Motivated by this we\ninvestigate the importance of leveraging \"sparse\" computation and propose SiRA:\nsparse mixture of low rank adaption. SiRA leverages the Sparse Mixture of\nExpert(SMoE) to boost the performance of LoRA. Specifically it enforces the top\n$k$ experts routing with a capacity limit restricting the maximum number of\ntokens each expert can process. We propose a novel and simple expert dropout on\ntop of gating network to reduce the over-fitting issue. Through extensive\nexperiments, we verify SiRA performs better than LoRA and other mixture of\nexpert approaches across different single tasks and multitask settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wichers_N/0/1/0/all/0/1\">Nevan Wichers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chu-Cheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_L/0/1/0/all/0/1\">Lei Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Han Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Canoee Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Liangchen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jindong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lei Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PEARL: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers. (arXiv:2311.09180v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09180","description":"<p>Powerful large language models have facilitated the development of writing\nassistants that promise to significantly improve the quality and efficiency of\ncomposition and communication. However, a barrier to effective assistance is\nthe lack of personalization in LLM outputs to the author's communication style\nand specialized knowledge. In this paper, we address this challenge by\nproposing PEARL, a retrieval-augmented LLM writing assistant personalized with\na generation-calibrated retriever. Our retriever is trained to select historic\nuser-authored documents for prompt augmentation, such that they are likely to\nbest personalize LLM generations for a user request. We propose two key\nnovelties for training our retriever: 1) A training data selection method that\nidentifies user requests likely to benefit from personalization and documents\nthat provide that benefit; and 2) A scale-calibrating KL-divergence objective\nthat ensures that our retriever closely tracks the benefit of a document for\npersonalized generation. We demonstrate the effectiveness of PEARL in\ngenerating personalized workplace social media posts and Reddit comments.\nFinally, we showcase the potential of a generation-calibrated retriever to\ndouble as a performance predictor and further improve low-quality generations\nvia LLM chaining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mysore_S/0/1/0/all/0/1\">Sheshera Mysore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhuoran Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_M/0/1/0/all/0/1\">Mengting Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Longqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menezes_S/0/1/0/all/0/1\">Steve Menezes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baghaee_T/0/1/0/all/0/1\">Tina Baghaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_E/0/1/0/all/0/1\">Emmanuel Barajas Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neville_J/0/1/0/all/0/1\">Jennifer Neville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safavi_T/0/1/0/all/0/1\">Tara Safavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models. (arXiv:2311.09182v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09182","description":"<p>In recent times, large language models (LLMs) have shown impressive\nperformance on various document-level tasks such as document classification,\nsummarization, and question-answering. However, research on understanding their\ncapabilities on the task of self-contradictions in long documents has been very\nlimited. In this work, we introduce ContraDoc, the first human-annotated\ndataset to study self-contradictions in long documents across multiple domains,\nvarying document lengths, self-contradictions types, and scope. We then analyze\nthe current capabilities of four state-of-the-art open-source and commercially\navailable LLMs: GPT3.5, GPT4, PaLM2, and LLaMAv2 on this dataset. While GPT4\nperforms the best and can outperform humans on this task, we find that it is\nstill unreliable and struggles with self-contradictions that require more\nnuance and context. We release the dataset and all the code associated with the\nexperiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jierui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raheja_V/0/1/0/all/0/1\">Vipul Raheja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1\">Dhruv Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization. (arXiv:2311.09184v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09184","description":"<p>While large language models (LLMs) already achieve strong performance on\nstandard generic summarization benchmarks, their performance on more complex\nsummarization task settings is less studied. Therefore, we benchmark LLMs on\ninstruction controllable text summarization, where the model input consists of\nboth a source article and a natural language requirement for the desired\nsummary characteristics. To this end, we curate an evaluation-only dataset for\nthis task setting and conduct human evaluation on 5 LLM-based summarization\nsystems. We then benchmark LLM-based automatic evaluation for this task with 4\ndifferent evaluation protocols and 11 LLMs, resulting in 40 evaluation methods\nin total. Our study reveals that instruction controllable text summarization\nremains a challenging task for LLMs, since (1) all LLMs evaluated still make\nfactual and other types of errors in their summaries; (2) all LLM-based\nevaluation methods cannot achieve a strong alignment with human annotators when\njudging the quality of candidate summaries; (3) different LLMs show large\nperformance gaps in summary generation and evaluation. We make our collected\nbenchmark, InstruSum, publicly available to facilitate future research in this\ndirection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander R. Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yilun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Simeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Verifiable Text Generation with Symbolic References. (arXiv:2311.09188v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09188","description":"<p>Large language models (LLMs) have demonstrated an impressive ability to\nsynthesize plausible and fluent text. However they remain vulnerable to\nhallucinations, and thus their outputs generally require manual human\nverification for high-stakes applications, which can be time-consuming and\ndifficult. This paper proposes symbolically grounded generation (SymGen) as a\nsimple approach for enabling easier validation of an LLM's output. SymGen\nprompts an LLM to interleave its regular output text with explicit symbolic\nreferences to fields present in some conditioning data (e.g., a table in JSON\nformat). The references can be used to display the provenance of different\nspans of text in the generation, reducing the effort required for manual\nverification. Across data-to-text and question answering experiments, we find\nthat LLMs are able to directly output text that makes use of symbolic\nreferences while maintaining fluency and accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hennigen_L/0/1/0/all/0/1\">Lucas Torroba Hennigen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Shannon Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nrusimha_A/0/1/0/all/0/1\">Aniruddha Nrusimha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gapp_B/0/1/0/all/0/1\">Bernhard Gapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1\">David Sontag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PsyEval: A Comprehensive Large Language Model Evaluation Benchmark for Mental Health. (arXiv:2311.09189v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09189","description":"<p>Recently, there has been a growing interest in utilizing large language\nmodels (LLMs) in mental health research, with studies showcasing their\nremarkable capabilities, such as disease detection. However, there is currently\na lack of a comprehensive benchmark for evaluating the capability of LLMs in\nthis domain. Therefore, we address this gap by introducing the first\ncomprehensive benchmark tailored to the unique characteristics of the mental\nhealth domain. This benchmark encompasses a total of six sub-tasks, covering\nthree dimensions, to systematically assess the capabilities of LLMs in the\nrealm of mental health. We have designed corresponding concise prompts for each\nsub-task. And we comprehensively evaluate a total of eight advanced LLMs using\nour benchmark. Experiment results not only demonstrate significant room for\nimprovement in current LLMs concerning mental health but also unveil potential\ndirections for future model optimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Haoan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kenny Q. Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Role of Chain-of-Thought in Complex Vision-Language Reasoning Task. (arXiv:2311.09193v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09193","description":"<p>The study explores the effectiveness of the Chain-of-Thought approach, known\nfor its proficiency in language tasks by breaking them down into sub-tasks and\nintermediate steps, in improving vision-language tasks that demand\nsophisticated perception and reasoning. We present the \"Description then\nDecision\" strategy, which is inspired by how humans process signals. This\nstrategy significantly improves probing task performance by 50%, establishing\nthe groundwork for future research on reasoning paradigms in complex\nvision-language tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yifan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Wenhan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas Oguz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gee_J/0/1/0/all/0/1\">James C. Gee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yixin Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structural Priming Demonstrates Abstract Grammatical Representations in Multilingual Language Models. (arXiv:2311.09194v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09194","description":"<p>Abstract grammatical knowledge - of parts of speech and grammatical patterns\n- is key to the capacity for linguistic generalization in humans. But how\nabstract is grammatical knowledge in large language models? In the human\nliterature, compelling evidence for grammatical abstraction comes from\nstructural priming. A sentence that shares the same grammatical structure as a\npreceding sentence is processed and produced more readily. Because confounds\nexist when using stimuli in a single language, evidence of abstraction is even\nmore compelling from crosslingual structural priming, where use of a syntactic\nstructure in one language primes an analogous structure in another language. We\nmeasure crosslingual structural priming in large language models, comparing\nmodel behavior to human experimental results from eight crosslingual\nexperiments covering six languages, and four monolingual structural priming\nexperiments in three non-English languages. We find evidence for abstract\nmonolingual and crosslingual grammatical representations in the models that\nfunction similarly to those found in humans. These results demonstrate that\ngrammatical representations in multilingual language models are not only\nsimilar across languages, but they can causally influence text produced in\ndifferent languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michaelov_J/0/1/0/all/0/1\">James A. Michaelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnett_C/0/1/0/all/0/1\">Catherine Arnett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tyler A. Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1\">Benjamin K. Bergen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Never Lost in the Middle: Improving Large Language Models via Attention Strengthening Question Answering. (arXiv:2311.09198v1 [cs.CL])","link":"http://arxiv.org/abs/2311.09198","description":"<p>While large language models (LLMs) are equipped with longer text input\ncapabilities than before, they are struggling to seek correct information in\nlong contexts. The \"lost in the middle\" problem challenges most LLMs, referring\nto the dramatic decline in accuracy when correct information is located in the\nmiddle. To overcome this crucial issue, this paper proposes to enhance the\ninformation searching and reflection ability of LLMs in long contexts via\nspecially designed tasks called Attention Strengthening Multi-doc QA (ASM QA).\nFollowing these tasks, our model excels in focusing more precisely on the\ndesired information. Experimental results show substantial improvement in\nMulti-doc QA and other benchmarks, superior to state-of-the-art models by 13.7%\nabsolute gain in shuffled settings, by 21.5% in passage retrieval task. We\nrelease our model, Ziya-Reader to promote related research in the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junqing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_K/0/1/0/all/0/1\">Kunhao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoqun Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhuoyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuxin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qianguo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zejian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Causal Structure of Semantic Ambiguities. (arXiv:2206.06807v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.06807","description":"<p>Ambiguity is a natural language phenomenon occurring at different levels of\nsyntax, semantics, and pragmatics. It is widely studied; in Psycholinguistics,\nfor instance, we have a variety of competing studies for the human\ndisambiguation processes. These studies are empirical and based on eye-tracking\nmeasurements. Here we take first steps towards formalizing these processes for\nsemantic ambiguities where we identified the presence of two features: (1)\njoint plausibility degrees of different possible interpretations, (2) causal\nstructures according to which certain words play a more substantial role in the\nprocesses. The novel sheaf-theoretic model of definite causality developed by\nGogioso and Pinzani in QPL 2021 offers tools to model and reason about these\nfeatures. We applied this theory to a dataset of ambiguous phrases extracted\nfrom Psycholinguistics literature and their human plausibility judgements\ncollected by us using the Amazon Mechanical Turk engine. We measured the causal\nfractions of different disambiguation orders within the phrases and discovered\ntwo prominent orders: from subject to verb in the subject-verb and from object\nto verb in the verb object phrases. We also found evidence for delay in the\ndisambiguation of polysemous vs homonymous verbs, again compatible with\nPsycholinguistic findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daphne Wang</a> (University College London), <a href=\"http://arxiv.org/find/cs/1/au:+Sadrzadeh_M/0/1/0/all/0/1\">Mehrnoosh Sadrzadeh</a> (University College London)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Vectors: Subspace Representations for Set Operations of Embeddings. (arXiv:2210.13034v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13034","description":"<p>In natural language processing (NLP), the role of embeddings in representing\nlinguistic semantics is crucial. Despite the prevalence of vector\nrepresentations in embedding sets, they exhibit limitations in expressiveness\nand lack comprehensive set operations. To address this, we attempt to formulate\nand apply sets and their operations within pre-trained embedding spaces.\nInspired by quantum logic, we propose to go beyond the conventional vector set\nrepresentation with our novel subspace-based approach. This methodology\nconstructs subspaces using pre-trained embedding sets, effectively preserving\nsemantic nuances previously overlooked, and consequently consistently improving\nperformance in downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ishibashi_Y/0/1/0/all/0/1\">Yoichi Ishibashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokoi_S/0/1/0/all/0/1\">Sho Yokoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudoh_K/0/1/0/all/0/1\">Katsuhito Sudoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1\">Satoshi Nakamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deanthropomorphising NLP: Can a Language Model Be Conscious?. (arXiv:2211.11483v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.11483","description":"<p>This work is intended as a voice in the discussion over previous claims that\na pretrained large language model (LLM) based on the Transformer model\narchitecture can be sentient. Such claims have been made concerning the LaMDA\nmodel and also concerning the current wave of LLM-powered chatbots, such as\nChatGPT. This claim, if confirmed, would have serious ramifications in the\nNatural Language Processing (NLP) community due to wide-spread use of similar\nmodels. However, here we take the position that such a large language model\ncannot be sentient, or conscious, and that LaMDA in particular exhibits no\nadvances over other similar models that would qualify it. We justify this by\nanalysing the Transformer architecture through Integrated Information Theory of\nconsciousness. We see the claims of sentience as part of a wider tendency to\nuse anthropomorphic language in NLP reporting. Regardless of the veracity of\nthe claims, we consider this an opportune moment to take stock of progress in\nlanguage modelling and consider the ethical implications of the task. In order\nto make this work helpful for readers outside the NLP community, we also\npresent the necessary background in language modelling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shardlow_M/0/1/0/all/0/1\">Matthew Shardlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Przybyla_P/0/1/0/all/0/1\">Piotr Przyby&#x142;a</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.10405","description":"<p>Recently decades have witnessed the empirical success of framing Knowledge\nGraph (KG) embeddings via language models. However, language model-based KG\nembeddings are usually deployed as static artifacts, making them difficult to\nmodify post-deployment without re-training after deployment. To address this\nissue, we propose a new task of editing language model-based KG embeddings in\nthis paper. This task is designed to facilitate rapid, data-efficient updates\nto KG embeddings without compromising the performance of other aspects. We\nbuild four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and\nevaluate several knowledge editing baselines demonstrating the limited ability\nof previous models to handle the proposed challenging task. We further propose\na simple yet strong baseline dubbed KGEditor, which utilizes additional\nparametric layers of the hyper network to edit/add facts. Our comprehensive\nexperimental results reveal that KGEditor excels in updating specific facts\nwithout impacting the overall performance, even when faced with limited\ntraining resources. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKG/tree/main/deltaKG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1\">Bozhong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingbing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing. (arXiv:2304.02017v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.02017","description":"<p>Large language models have revolutionized the field of artificial\nintelligence and have been used in various applications. Among these models,\nChatGPT (Chat Generative Pre-trained Transformer) has been developed by OpenAI,\nit stands out as a powerful tool that has been widely adopted. ChatGPT has been\nsuccessfully applied in numerous areas, including chatbots, content generation,\nlanguage translation, personalized recommendations, and even medical diagnosis\nand treatment. Its success in these applications can be attributed to its\nability to generate human-like responses, understand natural language, and\nadapt to different contexts. Its versatility and accuracy make it a powerful\ntool for natural language processing (NLP). However, there are also limitations\nto ChatGPT, such as its tendency to produce biased responses and its potential\nto perpetuate harmful language patterns. This article provides a comprehensive\noverview of ChatGPT, its applications, advantages, and limitations.\nAdditionally, the paper emphasizes the importance of ethical considerations\nwhen using this robust tool in real-world scenarios. Finally, This paper\ncontributes to ongoing discussions surrounding artificial intelligence and its\nimpact on vision and NLP domains by providing insights into prompt engineering\ntechniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hariri_W/0/1/0/all/0/1\">Walid Hariri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs. (arXiv:2305.03111v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03111","description":"<p>Text-to-SQL parsing, which aims at converting natural language instructions\ninto executable SQLs, has gained increasing attention in recent years. In\nparticular, Codex and ChatGPT have shown impressive results in this task.\nHowever, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on\ndatabase schema with few rows of database contents leaving the gap between\nacademic study and real-world applications. To mitigate this gap, we present\nBird, a big benchmark for large-scale database grounded in text-to-SQL tasks,\ncontaining 12,751 pairs of text-to-SQL data and 95 databases with a total size\nof 33.4 GB, spanning 37 professional domains. Our emphasis on database values\nhighlights the new challenges of dirty database contents, external knowledge\nbetween NL questions and database contents, and SQL efficiency, particularly in\nthe context of massive databases. To solve these problems, text-to-SQL models\nmust feature database value comprehension in addition to semantic parsing. The\nexperimental results demonstrate the significance of database values in\ngenerating accurate text-to-SQLs for big databases. Furthermore, even the most\neffective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution\naccuracy, which is still far from the human result of 92.96%, proving that\nchallenges still stand. Besides, we also provide an efficiency analysis to\noffer insights into generating text-to-efficient-SQLs that are beneficial to\nindustries. We believe that BIRD will contribute to advancing real-world\napplications of text-to-SQL research. The leaderboard and source code are\navailable: https://bird-bench.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1\">Binyuan Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_G/0/1/0/all/0/1\">Ge Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaxi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Binhua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bailin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bowen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Rongyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_R/0/1/0/all/0/1\">Ruiying Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_N/0/1/0/all/0/1\">Nan Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xuanhe Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chenhao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin C.C. Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1\">Reynold Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge. (arXiv:2305.04978v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.04978","description":"<p>Comparative knowledge (e.g., steel is stronger and heavier than styrofoam) is\nan essential component of our world knowledge, yet understudied in prior\nliterature. In this paper, we study the task of comparative knowledge\nacquisition, motivated by the dramatic improvements in the capabilities of\nextreme-scale language models like GPT-4, which have fueled efforts towards\nharvesting their knowledge into knowledge bases. While acquisition of such\ncomparative knowledge is much easier from models like GPT-4, compared to their\nconsiderably smaller and weaker counterparts such as GPT-2, not even the most\npowerful models are exempt from making errors. We thus ask: to what extent are\nmodels at different scales able to generate valid and diverse comparative\nknowledge?\n</p>\n<p>We introduce NeuroComparatives, a novel framework for comparative knowledge\ndistillation overgenerated from language models such as GPT-variants and Llama,\nfollowed by stringent filtering of the generated knowledge. Our framework\nacquires comparative knowledge between everyday objects, producing a corpus of\nup to 8.8M comparisons over 1.74M entity pairs - 10X larger and 30% more\ndiverse than existing resources. Moreover, human evaluations show that\nNeuroComparatives outperform existing resources (up to 32% absolute\nimprovement). We also demonstrate the utility of our distilled\nNeuroComparatives on three downstream tasks. Our results show that\nneuro-symbolic manipulation of smaller models offer complementary benefits to\nthe currently dominant practice of prompting extreme-scale language models for\nknowledge distillation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Howard_P/0/1/0/all/0/1\">Phillip Howard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junlin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1\">Vasudev Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singer_G/0/1/0/all/0/1\">Gadi Singer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Schema-adaptable Knowledge Graph Construction. (arXiv:2305.08703v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08703","description":"<p>Conventional Knowledge Graph Construction (KGC) approaches typically follow\nthe static information extraction paradigm with a closed set of pre-defined\nschema. As a result, such approaches fall short when applied to dynamic\nscenarios or domains, whereas a new type of knowledge emerges. This\nnecessitates a system that can handle evolving schema automatically to extract\ninformation for KGC. To address this need, we propose a new task called\nschema-adaptable KGC, which aims to continually extract entity, relation, and\nevent based on a dynamically changing schema graph without re-training. We\nfirst split and convert existing datasets based on three principles to build a\nbenchmark, i.e., horizontal schema expansion, vertical schema expansion, and\nhybrid schema expansion; then investigate the schema-adaptable performance of\nseveral well-known approaches such as Text2Event, TANL, UIE and GPT-3.5. We\nfurther propose a simple yet effective baseline dubbed \\textsc{AdaKGC}, which\ncontains schema-enriched prefix instructor and schema-conditioned dynamic\ndecoding to better handle evolving schema. Comprehensive experimental results\nillustrate that AdaKGC can outperform baselines but still have room for\nimprovement. We hope the proposed work can deliver benefits to the community.\nCode and datasets available at https://github.com/zjunlp/AdaKGC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_H/0/1/0/all/0/1\">Honghao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining black box text modules in natural language with language models. (arXiv:2305.09863v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2305.09863","description":"<p>Large language models (LLMs) have demonstrated remarkable prediction\nperformance for a growing array of tasks. However, their rapid proliferation\nand increasing opaqueness have created a growing need for interpretability.\nHere, we ask whether we can automatically obtain natural language explanations\nfor black box text modules. A \"text module\" is any function that maps text to a\nscalar continuous value, such as a submodule within an LLM or a fitted model of\na brain region. \"Black box\" indicates that we only have access to the module's\ninputs/outputs.\n</p>\n<p>We introduce Summarize and Score (SASC), a method that takes in a text module\nand returns a natural language explanation of the module's selectivity along\nwith a score for how reliable the explanation is. We study SASC in 3 contexts.\nFirst, we evaluate SASC on synthetic modules and find that it often recovers\nground truth explanations. Second, we use SASC to explain modules found within\na pre-trained BERT model, enabling inspection of the model's internals.\nFinally, we show that SASC can generate explanations for the response of\nindividual fMRI voxels to language stimuli, with potential applications to\nfine-grained brain mapping. All code for using SASC and reproducing results is\nmade available on Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1\">Chandan Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_A/0/1/0/all/0/1\">Aliyah R. Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antonello_R/0/1/0/all/0/1\">Richard Antonello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shailee Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huth_A/0/1/0/all/0/1\">Alexander G. Huth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate Knowledge Distillation with n-best Reranking. (arXiv:2305.12057v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12057","description":"<p>We propose utilizing n-best reranking to enhance the Sequence-Level Knowledge\nDistillation (Kim and Rush, 2016) where we explore hypotheses beyond the top-1\nto acquire more accurate pseudo-labels. To accomplish this, we leverage a\ndiverse set of models with different inductive biases, objective functions or\narchitectures, including publicly-available large pretrained models. The\neffectiveness of our proposal is validated through experiments on the WMT'21\nGerman-English and Chinese-English translation tasks. Our results demonstrate\nthat utilizing the pseudo-labels generated by our n-best reranker leads to a\nsignificantly more accurate student model. In fact, our best student model\nachieves comparable accuracy to a large translation model from (Tran et al.,\n2021) with 4.7 billion parameters, while having two orders of magnitude fewer\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Setiawan_H/0/1/0/all/0/1\">Hendra Setiawan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT4Table: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study. (arXiv:2305.13062v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13062","description":"<p>Large language models (LLMs) are becoming attractive as few-shot reasoners to\nsolve Natural Language (NL)-related tasks. However, there is still much to\nlearn about how well LLMs understand structured data, such as tables. While it\nis true that tables can be used as inputs to LLMs with serialization, there is\na lack of comprehensive studies examining whether LLMs can truly comprehend\nsuch data. In this paper, we try to understand this by designing a benchmark to\nevaluate the structural understanding capabilities (SUC) of LLMs. The benchmark\nwe create includes seven tasks, each with its own unique challenges, \\eg, cell\nlookup, row retrieval, and size detection. We conduct a series of evaluations\non GPT-3.5 and GPT-4. We find that the performance varied depending on several\ninput choices, including table input format, content order, role prompting, and\npartition marks. Drawing from the insights gained through the benchmark\nevaluations, we propose \\textit{self-augmentation} for effective structural\nprompting, such as critical value / range identification using LLMs' internal\nknowledge. When combined with carefully chosen input choices, these structural\nprompting methods lead to promising improvements in LLM performance on a\nvariety of tabular tasks, \\eg, TabFact($\\uparrow2.31\\%$),\nHybridQA($\\uparrow2.13\\%$), SQA($\\uparrow2.72\\%$), Feverous($\\uparrow0.84\\%$),\nand ToTTo($\\uparrow5.68\\%$). We believe that our benchmark and proposed\nprompting methods can serve as a simple yet generic selection for future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1\">Yuan Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mengyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingjie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating and Modeling Attribution for Cross-Lingual Question Answering. (arXiv:2305.14332v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14332","description":"<p>Trustworthy answer content is abundant in many high-resource languages and is\ninstantly accessible through question answering systems, yet this content can\nbe hard to access for those that do not speak these languages. The leap forward\nin cross-lingual modeling quality offered by generative language models offers\nmuch promise, yet their raw generations often fall short in factuality. To\nimprove trustworthiness in these systems, a promising direction is to attribute\nthe answer to a retrieved source, possibly in a content-rich language different\nfrom the query. Our work is the first to study attribution for cross-lingual\nquestion answering. First, we collect data in 5 languages to assess the\nattribution level of a state-of-the-art cross-lingual QA system. To our\nsurprise, we find that a substantial portion of the answers is not attributable\nto any retrieved passages (up to 50% of answers exactly matching a gold\nreference) despite the system being able to attend directly to the retrieved\ntext. Second, to address this poor attribution level, we experiment with a wide\nrange of attribution detection techniques. We find that Natural Language\nInference models and PaLM 2 fine-tuned on a very small amount of attribution\ndata can accurately detect attribution. Based on these models, we improve the\nattribution level of a cross-lingual question-answering system. Overall, we\nshow that current academic generative cross-lingual QA systems have substantial\nshortcomings in attribution and we build tooling to mitigate these issues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muller_B/0/1/0/all/0/1\">Benjamin Muller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieting_J/0/1/0/all/0/1\">John Wieting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jonathan H. Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwiatkowski_T/0/1/0/all/0/1\">Tom Kwiatkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_L/0/1/0/all/0/1\">Livio Baldini Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1\">Roee Aharoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1\">Jonathan Herzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Schema-Driven Information Extraction from Heterogeneous Tables. (arXiv:2305.14336v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14336","description":"<p>In this paper, we explore the question of whether large language models can\nsupport cost-efficient information extraction from tables. We introduce\nschema-driven information extraction, a new task that transforms tabular data\ninto structured records following a human-authored schema. To assess various\nLLM's capabilities on this task, we develop a benchmark composed of tables from\nfour diverse domains: machine learning papers, chemistry literature, material\nscience journals, and webpages. Alongside the benchmark, we present an\nextraction method based on instruction-tuned LLMs. Our approach shows\ncompetitive performance without task-specific labels, achieving F1 scores\nranging from 74.2 to 96.1, while maintaining great cost efficiency. Moreover,\nwe validate the possibility of distilling compact table-extraction models to\nreduce API reliance, as well as extraction from image tables using multi-modal\nmodels. By developing a benchmark and demonstrating the feasibility of this\ntask using proprietary models, we aim to support future work on open-source\nschema-driven IE models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_F/0/1/0/all/0/1\">Fan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Junmo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitag_D/0/1/0/all/0/1\">Dayne Freitag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment. (arXiv:2305.14463v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14463","description":"<p>We present a systematic study and comprehensive evaluation of large language\nmodels for automatic multilingual readability assessment. In particular, we\nconstruct ReadMe++, a multilingual multi-domain dataset with human annotations\nof 9757 sentences in Arabic, English, French, Hindi, and Russian collected from\n112 different data sources. ReadMe++ offers more domain and language diversity\nthan existing readability datasets, making it ideal for benchmarking\nmultilingual and non-English language models (including mBERT, XLM-R, mT5,\nLlama-2, GPT-4, etc.) in the supervised, unsupervised, and few-shot prompting\nsettings. Our experiments reveal that models fine-tuned on ReadMe++ outperform\nthose trained on single-domain datasets, showcasing superior performance on\nmulti-domain readability assessment and cross-lingual transfer capabilities. We\nalso compare to traditional readability metrics (such as Flesch-Kincaid Grade\nLevel and Open Source Metric for Measuring Arabic Narratives), as well as the\nstate-of-the-art unsupervised metric RSRS (Martinc et al., 2021). We will make\nour data and code publicly available at: https://github.com/tareknaous/readme.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naous_T/0/1/0/all/0/1\">Tarek Naous</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryan_M/0/1/0/all/0/1\">Michael J. Ryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavrouk_A/0/1/0/all/0/1\">Anton Lavrouk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_M/0/1/0/all/0/1\">Mohit Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Large Language Models Robust Coreference Resolvers?. (arXiv:2305.14489v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14489","description":"<p>Recent work on extending coreference resolution across domains and languages\nrelies on annotated data in both the target domain and language. At the same\ntime, pre-trained large language models (LMs) have been reported to exhibit\nstrong zero- and few-shot learning abilities across a wide range of NLP tasks.\nHowever, prior work mostly studied this ability using artificial sentence-level\ndatasets such as the Winograd Schema Challenge. In this paper, we assess the\nfeasibility of prompt-based coreference resolution by evaluating\ninstruction-tuned language models on difficult, linguistically-complex\ncoreference benchmarks (e.g., CoNLL-2012). We show that prompting for\ncoreference can outperform current unsupervised coreference systems, although\nthis approach appears to be reliant on high-quality mention detectors. Further\ninvestigations reveal that instruction-tuned LMs generalize surprisingly well\nacross domains, languages, and time periods; yet continued fine-tuning of\nneural models should still be preferred if small amounts of annotated examples\nare available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Nghia T. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do prompt positions really matter?. (arXiv:2305.14493v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14493","description":"<p>Prompt-based models have gathered a lot of attention from researchers due to\ntheir remarkable advancements in the fields of zero-shot and few-shot learning.\nDeveloping an effective prompt template plays a critical role. However, prior\nstudies have mainly focused on prompt vocabulary selection or embedding\ninitialization within a predefined template with the prompt position fixed. In\nthis empirical study, we conduct the most comprehensive analysis to date of\nprompt position for diverse natural language process tasks. Our findings\nquantify the substantial impact prompt position has on model performance. We\nobserve that the prompt position used in prior studies is often sub-optimal.\nThese findings suggest prompt position optimisation as a valuable research\ndirection to fill the gap in existing prompt engineering methodologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Junyu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Middleton_S/0/1/0/all/0/1\">Stuart E. Middleton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niranjan_M/0/1/0/all/0/1\">Mahesan Niranjan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selectively Answering Ambiguous Questions. (arXiv:2305.14613v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14613","description":"<p>Trustworthy language models should abstain from answering questions when they\ndo not know the answer. However, the answer to a question can be unknown for a\nvariety of reasons. Prior research has focused on the case in which the\nquestion is clear and the answer is unambiguous but possibly unknown, but the\nanswer to a question can also be unclear due to uncertainty of the questioner's\nintent or context. We investigate question answering from this perspective,\nfocusing on answering a subset of questions with a high degree of accuracy,\nfrom a set of questions in which many are inherently ambiguous. In this\nsetting, we find that the most reliable approach to decide when to abstain\ninvolves quantifying repetition within sampled model outputs, rather than the\nmodel's likelihood or self-verification as used in prior work. We find this to\nbe the case across different types of uncertainty and model scales,and with or\nwithout instruction tuning. Our results suggest that sampling-based confidence\nscores help calibrate answers to relatively unambiguous questions, with more\ndramatic improvements on ambiguous questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cole_J/0/1/0/all/0/1\">Jeremy R. Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Michael J.Q. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillick_D/0/1/0/all/0/1\">Daniel Gillick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenschlos_J/0/1/0/all/0/1\">Julian Martin Eisenschlos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1\">Bhuwan Dhingra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluate What You Can't Evaluate: Unassessable Quality for Generated Response. (arXiv:2305.14658v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14658","description":"<p>LLMs (large language models) such as ChatGPT have shown remarkable language\nunderstanding and generation capabilities. Although reference-free evaluators\nbased on LLMs show better human alignment than traditional reference-based\nevaluators, there are many challenges in using reference-free evaluators based\non LLMs. Reference-free evaluators are more suitable for open-ended examples\nwith different semantics responses. But not all examples are open-ended. For\nclosed-ended examples with unique correct semantic response, reference-free\nevaluators will still consider it high quality when giving a response that is\ninconsistent with the facts and the semantic of reference. In order to\ncomprehensively evaluate the reliability of evaluators based on LLMs, we\nconstruct two adversarial meta-evaluation dialogue generation datasets\nKdConv-ADV and DSTC7-ADV based on KdConv and DSTC7-AVSD, respectively. Compared\nto previous meta-evaluation benchmarks, KdConv-ADV and DSTC7-ADV are much more\nchallenging since they requires evaluators to be able to reasonably evaluate\nclosed-ended examples with the help of external knowledge or even its own\nknowledge. Empirical results show that the ability of LLMs to identify\nunreasonable responses is insufficient. There are risks in using eference-free\nevaluators based on LLMs to evaluate the quality of dialogue responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongkang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Multidimensional Political Incivility on Social Media. (arXiv:2305.14964v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14964","description":"<p>The rise of social media has been argued to intensify uncivil and hostile\nonline political discourse. Yet, to date, there is a lack of clarity on what\nincivility means in the political sphere. In this work, we utilize a\nmultidimensional perspective of political incivility, developed in the fields\nof political science and communication, that differentiates between\nimpoliteness and political intolerance. We present state-of-the-art incivility\ndetection results using a large dataset of 13K political tweets, collected and\nannotated per this distinction. Applying political incivility detection at\nlarge-scale, we observe that political incivility demonstrates a highly skewed\ndistribution over users, and examine social factors that correlate with\nincivility at subpopulation and user-level. Finally, we propose an approach for\nmodeling social context information about the tweet author alongside the tweet\ncontent, showing that this leads to improved performance on the task of\npolitical incivility detection. We believe that this latter result holds\npromise for socially-informed text processing in general.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pendzel_S/0/1/0/all/0/1\">Sagi Pendzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotan_N/0/1/0/all/0/1\">Nir Lotan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoizner_A/0/1/0/all/0/1\">Alon Zoizner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minkov_E/0/1/0/all/0/1\">Einat Minkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MiniSUPERB: Lightweight Benchmark for Self-supervised Speech Models. (arXiv:2305.19011v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2305.19011","description":"<p>SUPERB was proposed to evaluate the generalizability of self-supervised\nlearning (SSL) speech models across various tasks. However, it incurs high\ncomputational costs due to the large datasets and diverse tasks. In this paper,\nwe introduce MiniSUPERB, a lightweight benchmark that efficiently evaluates SSL\nspeech models with comparable results to SUPERB but lower computational costs\nsignificantly. We carefully select representative tasks, sample datasets, and\nextract model representations offline. Our approach achieves a Spearman's rank\ncorrelation of 0.954 and 0.982 with SUPERB Paper and SUPERB Challenge,\nrespectively. Additionally, we reduce the computational cost by 97% in terms of\nMultiply-ACcumulate operations (MACs). Furthermore, we evaluate SSL speech\nmodels in few-shot scenarios and observe significant variations in their\nperformance. To our knowledge, this is the first study to examine both the\ncomputational cost of the model itself and the cost of evaluating it on a\nbenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Hsiang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Huang-Yu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsu_W/0/1/0/all/0/1\">Winston Hsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$FastDoc$: Domain-Specific Fast Pre-training Technique using Document-Level Metadata and Taxonomy. (arXiv:2306.06190v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.06190","description":"<p>As the demand for sophisticated Natural Language Processing (NLP) models\ncontinues to grow, so does the need for efficient pre-training techniques.\nCurrent NLP models undergo resource-intensive pre-training. In response, we\nintroduce $FastDoc$ (Fast Pre-training Technique using Document-Level Metadata\nand Taxonomy), a novel approach designed to significantly reduce computational\ndemands. $FastDoc$ leverages document metadata and domain-specific taxonomy as\nsupervision signals. It involves continual pre-training of an open-domain\ntransformer encoder using sentence-level embeddings, followed by fine-tuning\nusing token-level embeddings. We evaluate $FastDoc$ on six tasks across nine\ndatasets spanning three distinct domains. Remarkably, $FastDoc$ achieves\nremarkable compute reductions of approximately 1,000x, 4,500x, 500x compared to\ncompetitive approaches in Customer Support, Scientific, and Legal domains,\nrespectively. Importantly, these efficiency gains do not compromise performance\nrelative to competitive baselines. Furthermore, reduced pre-training data\nmitigates catastrophic forgetting, ensuring consistent performance in\nopen-domain scenarios. $FastDoc$ offers a promising solution for\nresource-efficient pre-training, with potential applications spanning various\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nandy_A/0/1/0/all/0/1\">Abhilash Nandy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapadnis_M/0/1/0/all/0/1\">Manav Nitin Kapadnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patnaik_S/0/1/0/all/0/1\">Sohan Patnaik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butala_Y/0/1/0/all/0/1\">Yash Parag Butala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_N/0/1/0/all/0/1\">Niloy Ganguly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Citation: A Key to Building Responsible and Accountable Large Language Models. (arXiv:2307.02185v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.02185","description":"<p>Large Language Models (LLMs) bring transformative benefits alongside unique\nchallenges, including intellectual property (IP) and ethical concerns. This\nposition paper explores a novel angle to mitigate these risks, drawing\nparallels between LLMs and established web systems. We identify \"citation\" -\nthe acknowledgement or reference to a source or evidence - as a crucial yet\nmissing component in LLMs. Incorporating citation could enhance content\ntransparency and verifiability, thereby confronting the IP and ethical issues\nin the deployment of LLMs. We further propose that a comprehensive citation\nmechanism for LLMs should account for both non-parametric and parametric\ncontent. Despite the complexity of implementing such a citation mechanism,\nalong with the potential pitfalls, we advocate for its development. Building on\nthis foundation, we outline several research problems in this area, aiming to\nguide future explorations towards building more responsible and accountable\nLLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models. (arXiv:2307.07705v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.07705","description":"<p>Parameter-efficient tuning (PET) has been widely explored in recent years\nbecause it tunes much fewer parameters (PET modules) than full-parameter\nfine-tuning (FT) while still stimulating sufficient knowledge from large\nlanguage models (LLMs) for downstream tasks. Moreover, when PET is employed to\nserve multiple tasks, different task-specific PET modules can be built on a\nfrozen LLM, avoiding redundant LLM deployments. Although PET significantly\nreduces the cost of tuning and deploying LLMs, its inference still suffers from\nthe computational bottleneck of LLMs. To address the above issue, we propose an\neffective PET framework based on compressed LLMs, named \"CPET\". In CPET, we\nevaluate the impact of mainstream LLM compression techniques on PET performance\nand then introduce knowledge inheritance and recovery strategies to restore the\nknowledge loss caused by these compression techniques. Our experimental results\ndemonstrate that, owing to the restoring strategies of CPET, collaborating\ntask-specific PET modules with a compressed LLM can achieve comparable\nperformance to collaborating PET modules with the original version of the\ncompressed LLM and outperform directly applying vanilla PET methods to the\ncompressed LLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuxiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Bias Amplification Paradox in Text-to-Image Generation. (arXiv:2308.00755v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2308.00755","description":"<p>Bias amplification is a phenomenon in which models exacerbate biases or\nstereotypes present in the training data. In this paper, we study bias\namplification in the text-to-image domain using Stable Diffusion by comparing\ngender ratios in training vs. generated images. We find that the model appears\nto amplify gender-occupation biases found in the training data (LAION)\nconsiderably. However, we discover that amplification can be largely attributed\nto discrepancies between training captions and model prompts. For example, an\ninherent difference is that captions from the training data often contain\nexplicit gender information while our prompts do not, which leads to a\ndistribution shift and consequently inflates bias measures. Once we account for\ndistributional differences between texts used for training and generation when\nevaluating amplification, we observe that amplification decreases drastically.\nOur findings illustrate the challenges of comparing biases in models and their\ntraining data, and highlight confounding factors that impact analyses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seshadri_P/0/1/0/all/0/1\">Preethi Seshadri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1\">Yanai Elazar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASPIRE: Language-Guided Augmentation for Robust Image Classification. (arXiv:2308.10103v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2308.10103","description":"<p>Neural image classifiers can often learn to make predictions by overly\nrelying on non-predictive features that are spuriously correlated with the\nclass labels in the training data. This leads to poor performance in real-world\natypical scenarios where such features are absent. Supplementing the training\ndataset with images without such spurious features can aid robust learning\nagainst spurious correlations via better generalization. This paper presents\nASPIRE (Language-guided data Augmentation for SPurIous correlation REmoval), a\nsimple yet effective solution for expanding the training dataset with synthetic\nimages without spurious features. ASPIRE, guided by language, generates these\nimages without requiring any form of additional supervision or existing\nexamples. Precisely, we employ LLMs to first extract foreground and background\nfeatures from textual descriptions of an image, followed by advanced\nlanguage-guided image editing to discover the features that are spuriously\ncorrelated with the class label. Finally, we personalize a text-to-image\ngeneration model to generate diverse in-domain images without spurious\nfeatures. We demonstrate the effectiveness of ASPIRE on 4 datasets, including\nthe very challenging Hard ImageNet dataset, and 9 baselines and show that\nASPIRE improves the classification accuracy of prior methods by 1% - 38%. Code\nsoon at: https://github.com/Sreyan88/ASPIRE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evuru_C/0/1/0/all/0/1\">Chandra Kiran Reddy Evuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sonal Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_U/0/1/0/all/0/1\">Utkarsh Tyagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sakshi Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Sanjoy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.15452","description":"<p>In the realm of embodied artificial intelligence, the reasoning capabilities\nof Large Language Models (LLMs) play a pivotal role. Although there are\neffective methods like program-of-thought prompting for LLMs which uses\nprogramming language to tackle complex reasoning tasks, the specific impact of\ncode data on the improvement of reasoning capabilities remains under-explored.\nTo address this gap, we propose complexity-impacted reasoning score (CIRS),\nwhich combines structural and logical attributes, to measure the correlation\nbetween code and reasoning abilities. Specifically, we use the abstract syntax\ntree to encode the structural information and calculate logical complexity by\nconsidering the difficulty and the cyclomatic complexity. Through an empirical\nanalysis, we find not all code data of complexity can be learned or understood\nby LLMs. Optimal level of complexity is critical to the improvement of\nreasoning abilities by program-aided prompting. Then we design an\nauto-synthesizing and stratifying algorithm, and apply it to instruction\ngeneration for mathematical reasoning and code data filtering for code\ngeneration tasks. Extensive results demonstrates the effectiveness of our\nproposed approach. Code will be integrated into the EasyInstruct framework at\nhttps://github.com/zjunlp/EasyInstruct.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yinuo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guozhou Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising \"Alignment\" in Large Language Models. (arXiv:2310.02457v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.02457","description":"<p>In this paper, we address the concept of \"alignment\" in large language models\n(LLMs) through the lens of post-structuralist socio-political theory,\nspecifically examining its parallels to empty signifiers. To establish a shared\nvocabulary around how abstract concepts of alignment are operationalised in\nempirical datasets, we propose a framework that demarcates: 1) which dimensions\nof model behaviour are considered important, then 2) how meanings and\ndefinitions are ascribed to these dimensions, and by whom. We situate existing\nempirical literature and provide guidance on deciding which paradigm to follow.\nThrough this framework, we aim to foster a culture of transparency and critical\nevaluation, aiding the community in navigating the complexities of aligning\nLLMs with human populations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Rose Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1\">Bertie Vidgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottger_P/0/1/0/all/0/1\">Paul R&#xf6;ttger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1\">Scott A. Hale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech Model. (arXiv:2310.02971v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2310.02971","description":"<p>Prompting and adapter tuning have emerged as efficient alternatives to\nfine-tuning (FT) methods. However, existing studies on speech prompting focused\non classification tasks and failed on more complex sequence generation tasks.\nBesides, adapter tuning is primarily applied with a focus on encoder-only\nself-supervised models. Our experiments show that prompting on Wav2Seq, a\nself-supervised encoder-decoder model, surpasses previous works in sequence\ngeneration tasks. It achieves a remarkable 53% relative improvement in word\nerror rate for ASR and a 27% in F1 score for slot filling. Additionally,\nprompting competes with the FT method in the low-resource scenario. Moreover,\nwe show the transferability of prompting and adapter tuning on Wav2Seq in\ncross-lingual ASR. When limited trainable parameters are involved, prompting\nand adapter tuning consistently outperform conventional FT across 7 languages.\nNotably, in the low-resource scenario, prompting consistently outperforms\nadapter tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1\">Ming-Hsin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1\">Yun-Ping Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsu_J/0/1/0/all/0/1\">Jing Neng Hsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_P/0/1/0/all/0/1\">Paul Kuo-Ming Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1\">Chien-yu Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Conversational Decoding via Isotropic and Proximal Search. (arXiv:2310.08130v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.08130","description":"<p>General-purpose text decoding approaches are usually adopted for dialogue\nresponse generation. Although the quality of the generated responses can be\nimproved with dialogue-specific encoding methods, conversational decoding\nmethods are still under-explored. Inspired by \\citet{wu2023learning} that a\ngood dialogue feature space should follow the rules of locality and isotropy,\nwe present a fine-grained conversational decoding method, termed\n\\textit{isotropic and proximal search (IPS)}. Our method is designed to\ngenerate the semantic-concentrated response, while still maintaining\ninformativeness and discrimination against the context. Experiments show that\nour approach outperforms existing decoding strategies in the dialogue field\nacross both automatic and human evaluation metrics. More in-depth analyses\nfurther confirm the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuxuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiling Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linqi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VIBE: Topic-Driven Temporal Adaptation for Twitter Classification. (arXiv:2310.10191v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.10191","description":"<p>Language features are evolving in real-world social media, resulting in the\ndeteriorating performance of text classification in dynamics. To address this\nchallenge, we study temporal adaptation, where models trained on past data are\ntested in the future. Most prior work focused on continued pretraining or\nknowledge updating, which may compromise their performance on noisy social\nmedia data. To tackle this issue, we reflect feature change via modeling latent\ntopic evolution and propose a novel model, VIBE: Variational Information\nBottleneck for Evolutions. Concretely, we first employ two Information\nBottleneck (IB) regularizers to distinguish past and future topics. Then, the\ndistinguished topics work as adaptive features via multi-task training with\ntimestamp and class label prediction. In adaptive learning, VIBE utilizes\nretrieved unlabeled data from online streams created posterior to training data\ntime. Substantial Twitter experiments on three classification tasks show that\nour model, with only 3% of data, significantly outperforms previous\nstate-of-the-art continued-pretraining methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eliminating Reasoning via Inferring with Planning: A New Framework to Guide LLMs' Non-linear Thinking. (arXiv:2310.12342v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.12342","description":"<p>Chain-of-Thought(CoT) prompting and its variants explore equipping large\nlanguage models (LLMs) with high-level reasoning abilities by emulating\nhuman-like linear cognition and logic. However, the human mind is complicated\nand mixed with both linear and nonlinear thinking. In this work, we propose\n\\textbf{I}nferential \\textbf{E}xclusion \\textbf{P}rompting (IEP), a novel\nprompting that combines the principles of elimination and inference in order to\nguide LLMs to think non-linearly. IEP guides LLMs to plan and then utilize\nNatural Language Inference (NLI) to deduce each possible solution's entailment\nrelation with context, commonsense, or facts, therefore yielding a broader\nperspective by thinking back for inferring. This forward planning and backward\neliminating process allows IEP to better simulate the complex human thinking\nprocesses compared to other CoT-based methods, which only reflect linear\ncognitive processes. We conducted a series of empirical studies and have\ncorroborated that IEP consistently outperforms CoT across various tasks.\nAdditionally, we observe that integrating IEP and CoT further improves the\nLLMs' performance on certain tasks, highlighting the necessity of equipping\nLLMs with mixed logic processes. Moreover, to better evaluate comprehensive\nfeatures inherent in human logic, we introduce \\textbf{M}ental-\\textbf{A}bility\n\\textbf{R}easoning \\textbf{B}enchmark (MARB). The benchmark comprises six novel\nsubtasks with a total of 9,115 questions, among which 1,685 are developed with\nhand-crafted rationale references. We believe both \\textsc{IEP} and\n\\textsc{MARB} can serve as a promising direction for unveiling LLMs' logic and\nverbal reasoning abilities and drive further advancements. \\textsc{MARB} will\nbe available at ~\\texttt{anonymity link} soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yongqi Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dawei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sizhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Simeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoMix: Automatically Mixing Language Models. (arXiv:2310.12963v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.12963","description":"<p>Large language models (LLMs) are now available in various sizes and\nconfigurations from cloud API providers. While this diversity offers a broad\nspectrum of choices, effectively leveraging the options to optimize\ncomputational cost and performance remains challenging. In this work, we\npresent AutoMix, an approach that strategically routes queries to larger LMs,\nbased on the approximate correctness of outputs from a smaller LM. Central to\nAutoMix is a few-shot self-verification mechanism, which estimates the\nreliability of its own outputs without requiring training. Given that\nverifications can be noisy, we employ a meta verifier in AutoMix to refine the\naccuracy of these assessments. Our experiments using LLAMA2-13/70B, on five\ncontext-grounded reasoning datasets demonstrate that AutoMix surpasses\nestablished baselines, improving the incremental benefit per cost by up to 89%.\nOur code and data are available at https://github.com/automix-llm/automix.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_P/0/1/0/all/0/1\">Pranjal Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1\">Ankit Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potharaju_S/0/1/0/all/0/1\">Srividya Pranavi Potharaju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aditya Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopal_D/0/1/0/all/0/1\">Dheeraj Rajagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kappaganthu_K/0/1/0/all/0/1\">Karthik Kappaganthu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1\">Shyam Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faruqui_M/0/1/0/all/0/1\">Manaal Faruqui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation. (arXiv:2310.14088v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.14088","description":"<p>Curated datasets for healthcare are often limited due to the need of human\nannotations from experts. In this paper, we present MedEval, a multi-level,\nmulti-task, and multi-domain medical benchmark to facilitate the development of\nlanguage models for healthcare. MedEval is comprehensive and consists of data\nfrom several healthcare systems and spans 35 human body regions from 8\nexamination modalities. With 22,779 collected sentences and 21,228 reports, we\nprovide expert annotations at multiple levels, offering a granular potential\nusage of the data and supporting a wide range of tasks. Moreover, we\nsystematically evaluated 10 generic and domain-specific language models under\nzero-shot and finetuning settings, from domain-adapted baselines in healthcare\nto general-purposed state-of-the-art large language models (e.g., ChatGPT). Our\nevaluations reveal varying effectiveness of the two categories of language\nmodels across different tasks, from which we notice the importance of\ninstruction tuning for few-shot usage of large language models. Our\ninvestigation paves the way toward benchmarking language models for healthcare\nand provides valuable insights into the strengths and limitations of adopting\nlarge language models in medical domains, informing their practical\napplications and future advancements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zexue He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_A/0/1/0/all/0/1\">An Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1\">Eric Y. Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gentili_A/0/1/0/all/0/1\">Amilcare Gentili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chun-Nan Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Translation for Nko: Tools, Corpora and Baseline Results. (arXiv:2310.15612v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.15612","description":"<p>Currently, there is no usable machine translation system for Nko, a language\nspoken by tens of millions of people across multiple West African countries,\nwhich holds significant cultural and educational value.\n</p>\n<p>To address this issue, we present a set of tools, resources, and baseline\nresults aimed towards the development of usable machine translation systems for\nNko and other languages that do not currently have sufficiently large parallel\ntext corpora available.\n</p>\n<p>(1) Fria$\\parallel$el: A novel collaborative parallel text curation software\nthat incorporates quality control through copyedit-based workflows.\n</p>\n<p>(2) Expansion of the FLoRes-200 and NLLB-Seed corpora with 2,009 and 6,193\nhigh-quality Nko translations in parallel with 204 and 40 other languages.\n</p>\n<p>(3) nicolingua-0005: A collection of trilingual and bilingual corpora with\n130,850 parallel segments and monolingual corpora containing over 3 million Nko\nwords.\n</p>\n<p>(4) Baseline bilingual and multilingual neural machine translation results\nwith the best model scoring 30.83 English-Nko chrF++ on FLoRes-devtest.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doumbouya_M/0/1/0/all/0/1\">Moussa Koulako Bala Doumbouya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diane_B/0/1/0/all/0/1\">Baba Mamadi Dian&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cisse_S/0/1/0/all/0/1\">Solo Farabado Ciss&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diane_D/0/1/0/all/0/1\">Djibrila Dian&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sow_A/0/1/0/all/0/1\">Abdoulaye Sow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doumbouya_S/0/1/0/all/0/1\">S&#xe9;r&#xe9; Moussa Doumbouya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bangoura_D/0/1/0/all/0/1\">Daouda Bangoura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayo_F/0/1/0/all/0/1\">Fod&#xe9; Moriba Bayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conde_I/0/1/0/all/0/1\">Ibrahima Sory 2. Cond&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diane_K/0/1/0/all/0/1\">Kalo Mory Dian&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piech_C/0/1/0/all/0/1\">Chris Piech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher Manning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation. (arXiv:2311.00684v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.00684","description":"<p>An ideal length-extrapolatable Transformer language model can handle\nsequences longer than the training length without any fine-tuning. Such\nlong-context utilization capability relies heavily on a flexible positional\nembedding design. Upon investigating the flexibility of existing large\npre-trained Transformer language models, we find that the T5 family deserves a\ncloser look, as its positional embeddings capture rich and flexible attention\npatterns. However, T5 suffers from the dispersed attention issue: the longer\nthe input sequence, the flatter the attention distribution. To alleviate the\nissue, we propose two attention alignment strategies via temperature scaling.\nOur findings show improvement on the long-context utilization capability of T5\non language modeling, retrieval, multi-document question answering, and code\ncompletion tasks without any fine-tuning. This suggests that a flexible\npositional embedding design and attention alignment can go a long way toward\nTransformer length extrapolation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_T/0/1/0/all/0/1\">Ta-Chung Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1\">Ting-Han Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudnicky_A/0/1/0/all/0/1\">Alexander I. Rudnicky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models. (arXiv:2311.04378v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2311.04378","description":"<p>Watermarking generative models consists of planting a statistical signal\n(watermark) in a model's output so that it can be later verified that the\noutput was generated by the given model. A strong watermarking scheme satisfies\nthe property that a computationally bounded attacker cannot erase the watermark\nwithout causing significant quality degradation. In this paper, we study the\n(im)possibility of strong watermarking schemes. We prove that, under\nwell-specified and natural assumptions, strong watermarking is impossible to\nachieve. This holds even in the private detection algorithm setting, where the\nwatermark insertion and detection algorithms share a secret key, unknown to the\nattacker. To prove this result, we introduce a generic efficient watermark\nattack; the attacker is not required to know the private key of the scheme or\neven which scheme is used. Our attack is based on two assumptions: (1) The\nattacker has access to a \"quality oracle\" that can evaluate whether a candidate\noutput is a high-quality response to a prompt, and (2) The attacker has access\nto a \"perturbation oracle\" which can modify an output with a nontrivial\nprobability of maintaining quality, and which induces an efficiently mixing\nrandom walk on high-quality outputs. We argue that both assumptions can be\nsatisfied in practice by an attacker with weaker computational capabilities\nthan the watermarked model itself, to which the attacker has only black-box\naccess. Furthermore, our assumptions will likely only be easier to satisfy over\ntime as models grow in capabilities and modalities. We demonstrate the\nfeasibility of our attack by instantiating it to attack three existing\nwatermarking schemes for large language models: Kirchenbauer et al. (2023),\nKuditipudi et al. (2023), and Zhao et al. (2023). The same attack successfully\nremoves the watermarks planted by all three schemes, with only minor quality\ndegradation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edelman_B/0/1/0/all/0/1\">Benjamin L. Edelman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francati_D/0/1/0/all/0/1\">Danilo Francati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venturi_D/0/1/0/all/0/1\">Daniele Venturi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ateniese_G/0/1/0/all/0/1\">Giuseppe Ateniese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barak_B/0/1/0/all/0/1\">Boaz Barak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation Extraction. (arXiv:2311.05922v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.05922","description":"<p>Few-shot relation extraction involves identifying the type of relationship\nbetween two specific entities within a text, using a limited number of\nannotated samples. A variety of solutions to this problem have emerged by\napplying meta-learning and neural graph techniques which typically necessitate\na training process for adaptation. Recently, the strategy of in-context\nlearning has been demonstrating notable results without the need of training.\nFew studies have already utilized in-context learning for zero-shot information\nextraction. Unfortunately, the evidence for inference is either not considered\nor implicitly modeled during the construction of chain-of-thought prompts. In\nthis paper, we propose a novel approach for few-shot relation extraction using\nlarge language models, named CoT-ER, chain-of-thought with explicit evidence\nreasoning. In particular, CoT-ER first induces large language models to\ngenerate evidences using task-specific and concept-level knowledge. Then these\nevidences are explicitly incorporated into chain-of-thought prompting for\nrelation extraction. Experimental results demonstrate that our CoT-ER approach\n(with 0% training data) achieves competitive performance compared to the\nfully-supervised (with 100% training data) state-of-the-art approach on the\nFewRel1.0 and FewRel2.0 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xilai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Textual Normalization for Hate Speech Detection. (arXiv:2311.06851v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.06851","description":"<p>Social media data is a valuable resource for research, yet it contains a wide\nrange of non-standard words (NSW). These irregularities hinder the effective\noperation of NLP tools. Current state-of-the-art methods for the Vietnamese\nlanguage address this issue as a problem of lexical normalization, involving\nthe creation of manual rules or the implementation of multi-staged deep\nlearning frameworks, which necessitate extensive efforts to craft intricate\nrules. In contrast, our approach is straightforward, employing solely a\nsequence-to-sequence (Seq2Seq) model. In this research, we provide a dataset\nfor textual normalization, comprising 2,181 human-annotated comments with an\ninter-annotator agreement of 0.9014. By leveraging the Seq2Seq model for\ntextual normalization, our results reveal that the accuracy achieved falls\nslightly short of 70%. Nevertheless, textual normalization enhances the\naccuracy of the Hate Speech Detection (HSD) task by approximately 2%,\ndemonstrating its potential to improve the performance of complex NLP tasks.\nOur dataset is accessible for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Thi-Hoang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dung Ha Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Nguyet Thi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_K/0/1/0/all/0/1\">Khanh Thanh-Duy Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-context Learning and Gradient Descent Revisited. (arXiv:2311.07772v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.07772","description":"<p>In-context learning (ICL) has shown impressive results in few-shot learning\ntasks, yet its underlying mechanism is still not fully understood. Recent works\nsuggest that ICL can be thought of as a gradient descent (GD) based\noptimization process. While promising, these results mainly focus on simplified\nsettings of ICL and provide only a preliminary evaluation of the similarities\nbetween the two methods. In this work, we revisit the comparison between ICL\nand GD-based finetuning and study what properties of ICL an equivalent process\nmust follow. We highlight a major difference in the flow of information between\nICL and standard finetuning. Namely, ICL can only rely on information from\nlower layers at every point, while finetuning depends on loss gradients from\ndeeper layers. We refer to this discrepancy as Layer Causality and show that a\nlayer causal variant of the finetuning process aligns with ICL on par with\nvanilla finetuning and is even better in most cases across relevant metrics. To\nthe best of our knowledge, this is the first work to discuss this discrepancy\nexplicitly and suggest a solution that tackles this problem with minimal\nchanges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Natan_T/0/1/0/all/0/1\">Tomer Bar Natan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deutch_G/0/1/0/all/0/1\">Gilad Deutch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magar_N/0/1/0/all/0/1\">Nadav Magar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dar_G/0/1/0/all/0/1\">Guy Dar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unlocking Science: Novel Dataset and Benchmark for Cross-Modality Scientific Information Extraction. (arXiv:2311.08189v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.08189","description":"<p>Extracting key information from scientific papers has the potential to help\nresearchers work more efficiently and accelerate the pace of scientific\nprogress. Over the last few years, research on Scientific Information\nExtraction (SciIE) witnessed the release of several new systems and benchmarks.\nHowever, existing paper-focused datasets mostly focus only on specific parts of\na manuscript (e.g., abstracts) and are single-modality (i.e., text- or\ntable-only), due to complex processing and expensive annotations. Moreover,\ncore information can be present in either text or tables or across both. To\nclose this gap in data availability and enable cross-modality IE, while\nalleviating labeling costs, we propose a semi-supervised pipeline for\nannotating entities in text, as well as entities and relations in tables, in an\niterative procedure. Based on this pipeline, we release novel resources for the\nscientific community, including a high-quality benchmark, a large-scale corpus,\nand a semi-supervised annotation pipeline. We further report the performance of\nstate-of-the-art IE models on the proposed benchmark dataset, as a baseline.\nLastly, we explore the potential capability of large language models such as\nChatGPT for the current task. Our new dataset, results, and analysis validate\nthe effectiveness and efficiency of our semi-supervised pipeline, and we\ndiscuss its remaining limitations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_B/0/1/0/all/0/1\">B&#xf6;rje F. Karlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okumura_M/0/1/0/all/0/1\">Manabu Okumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chin-Yew Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KTRL+F: Knowledge-Augmented In-Document Search. (arXiv:2311.08329v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.08329","description":"<p>We introduce a new problem KTRL+F, a knowledge-augmented in-document search\ntask that necessitates real-time identification of all semantic targets within\na document with the awareness of external sources through a single natural\nquery. This task addresses following unique challenges for in-document search:\n1) utilizing knowledge outside the document for extended use of additional\ninformation about targets to bridge the semantic gap between the query and the\ntargets, and 2) balancing between real-time applicability with the performance.\nWe analyze various baselines in KTRL+F and find there are limitations of\nexisting models, such as hallucinations, low latency, or difficulties in\nleveraging external knowledge. Therefore we propose a Knowledge-Augmented\nPhrase Retrieval model that shows a promising balance between speed and\nperformance by simply augmenting external knowledge embedding in phrase\nembedding. Additionally, we conduct a user study to verify whether solving\nKTRL+F can enhance search experience of users. It demonstrates that even with\nour simple model users can reduce the time for searching with less queries and\nreduced extra visits to other sources for collecting evidence. We encourage the\nresearch community to work on KTRL+F to enhance more efficient in-document\ninformation access.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1\">Hanseok Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_H/0/1/0/all/0/1\">Haebin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_M/0/1/0/all/0/1\">Miyoung Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyunji Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Transient Nature of Emergent In-Context Learning in Transformers. (arXiv:2311.08360v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2311.08360","description":"<p>Transformer neural networks can exhibit a surprising capacity for in-context\nlearning (ICL) despite not being explicitly trained for it. Prior work has\nprovided a deeper understanding of how ICL emerges in transformers, e.g.\nthrough the lens of mechanistic interpretability, Bayesian inference, or by\nexamining the distributional properties of training data. However, in each of\nthese cases, ICL is treated largely as a persistent phenomenon; namely, once\nICL emerges, it is assumed to persist asymptotically. Here, we show that the\nemergence of ICL during transformer training is, in fact, often transient. We\ntrain transformers on synthetic data designed so that both ICL and in-weights\nlearning (IWL) strategies can lead to correct predictions. We find that ICL\nfirst emerges, then disappears and gives way to IWL, all while the training\nloss decreases, indicating an asymptotic preference for IWL. The transient\nnature of ICL is observed in transformers across a range of model sizes and\ndatasets, raising the question of how much to \"overtrain\" transformers when\nseeking compact, cheaper-to-run models. We find that L2 regularization may\noffer a path to more persistent ICL that removes the need for early stopping\nbased on ICL-style validation tasks. Finally, we present initial evidence that\nICL transience may be caused by competition between ICL and IWL circuits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aaditya K. Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">Stephanie C.Y. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moskovitz_T/0/1/0/all/0/1\">Ted Moskovitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grant_E/0/1/0/all/0/1\">Erin Grant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxe_A/0/1/0/all/0/1\">Andrew M. Saxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChOiRe: Characterizing and Predicting Human Opinions with Chain of Opinion Reasoning. (arXiv:2311.08385v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.08385","description":"<p>Aligning language models (LMs) with human opinion is challenging yet vital to\nenhance their grasp of human values, preferences, and beliefs. We present\nChOiRe, a four-step solution framework to predict human opinion that\ndifferentiates between the user explicit personae (i.e. demographic or\nideological attributes) that are manually declared and implicit personae\ninferred from user historical opinions. Specifically, it consists of (i) an LM\nanalyzing the user explicit personae to filter out irrelevant attributes; (ii)\nthe LM ranking the implicit persona opinions into a preferential list; (iii)\nChain-of-Opinion (CoO) reasoning, where the LM sequentially analyzes the\nexplicit personae and the most relevant implicit personae to perform opinion\nprediction; (iv) and where ChOiRe executes Step (iii) CoO multiple times with\nincreasingly larger lists of implicit personae to overcome insufficient\npersonae information to infer a final result. ChOiRe achieves new\nstate-of-the-art effectiveness with limited inference calls, improving previous\nLLM-based techniques significantly by 3.22%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Do_X/0/1/0/all/0/1\">Xuan Long Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Min-Yen Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-11-15T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/"}}]}]}